<doc id="2399" url="https://es.wikipedia.org/wiki?curid=2399" title="Ponferrada">
Ponferrada

Ponferrada es un municipio y ciudad de España, capital de la comarca del Bierzo de la Provincia de León, comunidad autónoma de Castilla y León. Está situada en la confluencia de los ríos Sil y Boeza. Tiene una población de más de 65 000 habitantes, siendo la segunda ciudad en importancia de la provincia con un 

Aunque existen indicios de poblamiento tanto en el Neolítico (en las orillas del Sil), como en la Edad del Hierro y en la época romana, no es hasta el siglo XI cuando tenemos constancia documental. Es al final de ese siglo cuando el obispo Osmundo de Astorga ordena la construcción de un puente, en el año 1082, para los peregrinos del Camino de Santiago, debido a las dificultades que suponía el paso del río Sil en el anterior paso, a la altura del actual barrio de Compostilla. Se reforzó con hierro, y esto dará nombre, posteriormente, a la población que crece en sus alrededores, a las orillas del río Sil. Otra teoría sobre el nombre de Ponferrada proviene igualmente de Pons Ferrata pero con la traducción de puente fortificado.

Al poco se fundó la Iglesia de San Pedro, en el año 1086 y a su alrededor surgió "La Puebla de San Pedro" que es como se llamó primeramente a Ponferrada, para pasar a denominarse poco después Ponte Ferrato.Pasó por diversas manos, primero fue propiedad de los Templarios, que se encargaban precisamente de la defensa del camino.

En el año 1180 el rey leonés Fernando II concede a la villa los primeros fueros. Durante los siglos XIII y XIV, ya amurallada, comienza a crecer y desarrollarse, al aparecer tanto intramuros como en los alrededores, campesinos, comerciantes y artesanos, a la sombra del camino, y que originó un crecimiento rápido y sostenido. La muralla construida cuenta con cuatro puertas de entrada: El cristo, Paraisín, Las nieves y Las eras. En las afueras de la villa se asienta una comunidad judía.

Al desaparecer la orden en 1312, pasaron por las familias de Osorio, o del Conde de Lemos entre otros, hasta que los Reyes Católicos la reclamaron como propia, a raíz de una disputa entre el Conde de Lemos y su hijo, en el siglo XVI, que llevó a diversas batallas en el castillo, y la toma y recuperación sucesiva de la plaza por cada uno de ellos. Finalmente los Reyes Católicos decidieron que el castillo y la villa eran de su propiedad, acto que puso fin a las refriegas. Desde entonces y hasta el fin del Antiguo Régimen, procedieron a nombrar corregidor en la villa; el primero de ellos fue don Juan de Torres.

En el año 2008 se celebró el centenario de la concesión del título de ciudad a Ponferrada coincidiendo con el centenario de la coronación de la Virgen de la Encina como patrona de la comarca. La concesión del título de ciudad fue de la siguiente manera:

Ponferrada se encuentra en la confluencia de los ríos Sil y Boeza, en el extremo este de la llanura berciana, y en el noroeste de la provincia de León. La altitud media de la ciudad es de 512 msnm, la máxima altitud del municipio se sitúa en el monte Pajariel con 830 msnm . El término municipal cuenta con una superficie total de 283.17 km².

Está rodeada por los siguientes municipios:

Ponferrada tiene un clima mediterráneo continental subhúmedo que se da en las zonas o regiones consideradas de transición entre el clima mediterráneo continental y el clima oceánico o de montaña, contando con precipitaciones relativamente abundantes, aunque con la sequía estival característica del clima mediterráneo y adquiriendo en las regiones interiores la nieve mucha importancia durante el invierno. La nieve es un fenómeno relativamente frecuente en invierno y se suele registrar entre diciembre y marzo. Los inviernos son fríos y largos siendo normales temperaturas de -3ºC, por otro lado el verano es corto y cálido con temperaturas que fácilmente superan los 32ºC. 

El núcleo urbano de Ponferrada está bañado por dos ríos, el Sil y el Boeza.

El primero, atraviesa la ciudad de noreste a sur, hasta el barrio de la estación, donde une el caudal del Boeza, y el río comienza a seguir la falda del monte Pajariel, en dirección hacia el oeste. El río se encuentra canalizado a su paso por la ciudad, existiendo paseos y jardines junto a su ribera, que empiezan a la altura de "La Fuente del Azufre", una antigua central hidroeléctrica actualmente abandonada, y terminan en el barrio de Flores del Sil. Existen varios puentes y pasarelas para salvar el Sil a su paso por la ciudad, siendo los más recientes una pasarela que une al barrio de la estación con el monte Pajariel y el "Puente del Centenario", que permite una nueva unión entre la parte alta de Ponferrada y el barrio del Campo de los Judíos con la zona de Compostilla y las huertas del Sacramento. Hay que destacar que el Sil divide a la ciudad en dos partes, y que este hecho unido a la orografía ponferradina, hace distinguir a la zona antigua de la ciudad como la "Zona Alta".

El río Boeza delimita por el sur el núcleo urbano de la ciudad, separando al barrio del "Puente Boeza" del núcleo urbano. Este río es un afluente del Sil, y su desembocadura en él se produce en Ponferrada, en las faldas del Pajariel.

Además, el municipio es atravesado por otros ríos. Es el caso del río Oza, que nace en los Montes Aquilanos, atravesando varios pueblos del sur del municipio, que integran el Valle del Oza, como son Montes de Valdueza, San Clemente de Valdueza, Valdefrancos o San Esteban de Valdueza. El río vierte sus aguas al Sil tras pasar el pueblo de Toral de Merayo. Un afluente del río Oza es el río Valdueza, que pasa por el pueblo de Villanueva de Valdueza, desembocando en el Oza cerca de San Esteban.

De la "Fuente del Azufre" salen dos canales, uno de ellos destinado al riego, el "Canal Bajo del Bierzo", que lleva agua procedente del Sil hasta Carracedelo, pasando en Ponferrada por los barrios de Compostilla y Cuatrovientos. El otro canal, el "Canal de Cornatel", que lleva agua del Sil hasta la presa de Campañana, en Carucedo. El canal de Cornatel atraviesa la parte alta de Ponferrada soterrado, saliendo a la luz en un tramo muy próximo al casco antiguo, para luego volver a soterrarse, cruzar el Boeza y volver a desaparecer en las cercanías de Otero.

El embalse de Bárcena está ubicado también en el municipio de Ponferrada, además de otros como Congosto y Cubillos del Sil. Fue inaugurado en 1960 y ello supuso la inundación de pueblos como "Bárcena del Río" y "Posada del Río", siendo su población trasladada a pueblos de nueva creación (Bárcena del Bierzo, su nombre inicial fue "Bárcena del Caudillo" y Posada del Bierzo), o al barrio de Fuentesnuevas. Fue inaugurado en 1960, construido en el curso del Río Sil y tiene varios usos: Refrigerar la central de Compostilla II, principal motivo de su construcción; aprovechamiento para la generación de energía eléctrica mediante una central hidroeléctrica que posee, y recientemente, el abastecimiento de agua a Ponferrada, con la construcción e inauguración en 2009 de la nueva traída de agua desde el embalse, para evitar los problemas de suministro que sufría la ciudad debido a su crecimiento, ya que hasta 2009 se dependía del agua del río Oza y de la mancomunidad.

La Mancomunidad de Municipios del Agua del Bierzo es un ente público formado por los municipios de Arganza, Cabañas Raras, Cacabelos, Camponaraya, Carracedelo, Cubillos del Sil, y Sancedo (gran parte de los municipios que comprende el Bierzo Bajo excepto Villafranca del Bierzo, Toral de los Vados, Priaranza del Bierzo y Borrenes), pudiéndose adherirse otros municipios de acuerdo a lo recogido en sus Estatutos y al procedimiento indicado en la Ley 1/ 1998, del 4 de junio del Régimen Local de Castilla y León.

En el año 2008 el municipio de Ponferrada mostró su voluntad de abandonar la Mancomunidad, decisión que finalmente se llevó a cabo, procediéndose a cambiar la denominación de la entidad (fundada como Mancomunidad de municipios de la comarca de Ponferrada). A pesar de esta decisión se ha mantenido la sede de esta entidad en Ponferrada, situada en el barrio de Fuentesnuevas, avenida de Galicia número 369.

La ciudad de Ponferrada es la quinta ciudad más poblada de Castilla y León, superando en número de habitantes a cuatro capitales de provincia y al resto de municipios de la Comunidad Autónoma.


Se observa el espectacular despegue poblacional entre 1940 y 1970, periodo en el que Ponferrada triplica su población, principalmente debido al crecimiento industrial del municipio y convertirse en el centro administrativo de la comarca, que vivía un auge minero sin precedentes. Esto dio lugar a numerosos problemas en su ordenación urbana debido al crecimiento desproporcionado y muchas veces caótico de las distintas barriadas de la ciudad, problema arrastrado hasta bien entrada la década de 1990-2000.

El casco urbano de Ponferrada, excluyendo Toral de Merayo, Flores del Sil, Cuatrovientos, Fuentesnuevas, Dehesas y otros barrios, tiene una población de 55.000 habitantes/aprox, repartidos en 25.000 varones y 30.000 mujeres.

Las entidades de población que componen el término municipal de Ponferrada son las siguientes:

Actualmente, la Corporación Municipal de Ponferrada está integrada por 25 concejales, según lo dispuesto en la "Ley del Régimen Electoral General", que establece el número de concejales elegibles en función de la población del municipio.

Hasta la fecha, todos los alcaldes de la ciudad de Ponferrada han pertenecido al Partido Popular (PP) y el Partido Socialista Obrero Español (PSOE). El 8 de marzo de 2013 se produce una moción de censura presentada por PSOE-IAP, que desbancó a Carlos López Riesco para colocar en la alcaldía a Samuel Folgueral Arias, del PSOE. Hecho polémico que planteó a Folgueral la disyuntiva de seguir en el PSOE o dejar su nuevo cargo de alcalde si no rompía el pacto con IAP, partido liderado por Ismael Álvarez. El 10 de marzo, Folgueral y los otros siete concejales del PSOE optaron por marcharse del partido. Por su parte, Álvarez (IAP) renunció al cargo.

El municipio de Ponferrada comprende, además de la ciudad de Ponferrada, a otras 33 entidades singulares de población. Algunas de ellas, 17 en total, son, a su vez, entidades locales menores. El municipio anexionó los términos municipales de San Esteban de Valdueza en 1974 y de Los Barrios de Salas en 1980, ambos integrados por muchos pueblos.

Los núcleos de población que forman parte del municipio de Ponferrada son los siguientes:



El Castillo "templario" de Ponferrada se sitúa sobre una colina en la confluencia de los ríos Boeza y Sil. Se emplaza en lo que, probablemente, en origen fue un castro celta, en una posición similar a la de otros de la comarca. Posteriormente se cree que fue un emplazamiento romano y visigodo.

Hacia 1178 Fernando II permite que los templarios establezcan una encomienda en la actual Ponferrada. En 1180 el rey expide fuero para la repoblación de la villa que había surgido un siglo antes, documentándose la primera fortificación en 1187. La fortaleza fue reconstruida en numerosas ocasiones a lo largo de las Edades Media y Moderna.
Actualmente, toda la zona palacial y ciertas torres de su recinto del siglo XV han sido reformadas con el fin de instalar un centro cultural.

En estos momentos el castillo cuenta con la exposición permanente "templum libri" en la que se exponen libros facsímiles del medievo y del renacimiento, y la nueva "Biblioteca Templaria", que cuenta con 1380 volúmenes relacionados con la Orden del Temple y que la convierten en la mayor de sus características en todo el mundo.

De estilo renacentista, se comenzó a construir en 1572, bajo la dirección del arquitecto Juan de Alvear, que realizó la cabecera del templo, desapareciendo así la iglesia medieval. La esbelta torre que remata la basílica se construyó en 1614. Custodia en su interior la imagen de la Virgen de La Encina, patrona de El Bierzo.

Se intentó convertir en colegiata, pero la petición fue denegada el 6 de mayo de 1720 y posteriormente se denegó una nueva petición, el 21 de octubre de 1725

Es una de las joyas arquitectónicas del Arte mozárabe junto al también leonés monasterio de San Miguel de Escalada. Está enclavada en el corazón del Valle del Oza a 14 kilómetros de Ponferrada. La zona, llena de monasterios e iglesias eremíticas desde el Siglo VIII mereció en su época el sobrenombre de la "Tebaida berciana". Su situación en el centro de los Montes Aquilanos, facilitaba el aislamiento que buscaban los santos ascéticos de los Siglos IX y X, como San Fructuoso y San Genadio.

Fue construida en la primera mitad del siglo X por el abad Salomón, concretamente en el reinado de Ramiro II, Ramiro II hizo múltiples donaciones a la iglesia y al monasterio de la localidad. Entre ellas estuvo la llamada Cruz de Peñalba, símbolo del Bierzo. En esta cruz se puede ver la fuerte influencia de la orfebrería visigoda sobre los modelos mozárabes.

Situado en la Valdueza, fue junto con el Monasterio de Santa María de Carracedo, el más poderoso de los monasterios en cuanto a dominios de la comarca. Fundado por San Fructuoso hacia el año 635 a día de hoy solo se conservan ruinas y la iglesia del Monasterio debido a un incendio en el siglo XIX, la desamortización de Mendizábal y la desatención de las autoridades patrimoniales.

Situada en el pueblo de Santo Tomas de las Ollas construida siglo X, es una de las mejores muestras del estilo mozárabe. A destacar su bella capilla arqueada. El nombre de la ermita, tomado del pueblo Santo Tomás de las Ollas, proviene del oficio principal de esa localidad: la alfarería. Proveían de género a la zona del Valle del Oza. La ermita fue donada por el a la comunidad del Monasterio de San Pedro de Montes.

Su construcción original data del siglo X, después de la cual se efectuaron diversas modificaciones, siendo la última la Sacristía, del siglo XIX. La ermita consta de una nave rectangular con sólo un ábside el cual, a pesar de tener una planta ovalada, continúa la línea rectangular de la nave en sus muros exteriores. La techumbre es a dos aguas, de pizarra, y asentado sobre una armadura de madera.

Situada en el pueblo de Otero de Ponferrada. Consta de una sola nave. Fue construida en el siglo XI, siendo la iglesia más antigua de estilo románico, también se pueden observar influencias del mozárabe. Destacan sus arcos de herradura y el bello testero, en el que se encuentra una ventana geminada de gran belleza.

En las estribaciones del municipio, dentro de los Montes Aquilanos, se encuentra la estación de esquí del Morredero.
Creada en el año 2002, actualmente es muy pequeña, con proyectos de ampliación, destacando la calidad de la nieve y el bajo coste del forfait. De su gestión se encarga la Asociación de Amigos del Morredero.

Recientemente se ha presentado un proyecto desde el ayuntamiento de Ponferrada para la construcción de equipamientos turísticos y la ampliación del dominio esquiable al máximo de las posibilidades del entorno; lo que daría lugar a una estación algo inferior a la vecina San Isidro. Está previsto que esté en funcionamiento en 2014 coincidiendo con el mundial de ciclismo en Ponferrada.


En los últimos años se ha incrementado notablemente el número de zonas verdes en la ciudad. El pulmón verde de Ponferrada es el monte Pajariel, aunque ha sufrido recientemente varios incendios forestales, existiendo un proyecto para convertirlo en parque forestal. Todos los barrios ponferradinos cuentan con alguna zona verde. Los principales parques de la ciudad son el del Plantío, parque del Temple, parque de la Concordia y parque del Oeste.


Este parque está ubicado en la zona alta de la ciudad, cercano a la universidad, contando con una senda de acceso desde el barrio de los Judíos. También es denominado "Parque Gil y Carrasco". La especie predominante en el arbolado del parque es el pino, aunque también existen otras especies de árboles de jardín. Entre los diferentes servicios que dispone el parque está una cancha deportiva de cemento en la que se puede practicar fútbol o baloncesto, una cafetería, un centro de día y una gran zona destinada a juegos infantiles, existiendo otro espacio para realizar ejercicios denominadas "Zonas de 1 a 100 años", que también hay en otros parques ponferradinos. Además cuenta con un estanque en el que hay patos, gansos y cisnes, un recinto cubierto donde se encuentran pavos reales y loros entre otras especies, y un quiosco o templete de música. Anteriormente, la estatua de "La Carrasca", el personaje de la novela El Señor de Bembibre de Gil y Carrasco, se encontraba en el parque, pero actualmente se ha trasladado a la glorieta homónima, que se encuentra junto a una de las entradas del parque.


Este parque está ubicado entre la zona centro de la ciudad y el barrio de Flores del Sil, rodeado en el norte por la barriada del temple, en el este por el barrio de la estación, en el sur por el auditorio municipal y la ribera del Sil, y en el oeste por el barrio de Flores del Sil. Es uno de los mayores parque de la ciudad. También es conocido como el "Parque del Belga" porque hasta hace unos años este parque era propiedad de la MSP, ya que era una finca particular, que estaba vallada, y donde residía el primero director general, y posteriormente vicepresidente de la MSP, el ingeniero Marcelo Jorissen, oriundo de Bruselas.

Este parque, perfectamente integrado en la ciudad, albergaba el conservatorio de Ponferrada, aunque debido a su traslado a un nuevo edificio en la Rosaleda, en noviembre de 2014 se hizo efectivo el traslado la sede de la policía local a este edificio, el auditorio municipal donde se ofrecen conciertos y espectáculos durante las fiestas de la encina, y celebrándose también en el parque unos días antes del comienzo de las fiestas, la denominada "ciudad mágica", más popularmente conocida como "CIMA" en la que se desarrollan numerosas actividades para los más pequeños, con talleres de artesanía, teatro, manualidades, hinchables..., existiendo el punto de información juvenil CIMA en uno de los edificios del parque.

La mayoría del arbolado está compuesto por pino. En el parque hay, además de los servicios expuestos anteriormente, una cafetería, un espacio destinado a la práctica de petanca, realizándose eventualmente torneos de la misma; canchas deportivas de cemento para la práctica de deportes como fútbol y baloncesto, y un circuito para el aprendizaje de seguridad vial, en que la policía municipal imparte clases de seguridad vial a los alumnos de los colegios de Ponferrada. También hay varias zonas destinadas a juegos y atracciones infantiles, además de una fuente y un estanque.

Recientemente se ha ampliado el parque para unirlo totalmente al barrio de flores del Sil, elevando la superficie a 60.000 m².


Ubicado en la zona de las huertas del sacramento, en la parte baja de ponferrada, es un parque que se desarrolla junto a una parte de la ribera del Sil, desde los campos de fútbol "Ramón Martínez" hasta el conocido "Puente de Cubelos".Hace unos años se acometió una gran reforma en el parque. Cuenta con todos los caminos asfaltados, y los dos antiguos quioscos han sido restaurados, albergando uno de ellos un aula de interpretación del río Sil. Tiene un estanque, un área infantil, y otra área para realizar ejercicios de gimnasia. La especie predominante es el chopo. Junto al parque se encuentran los juzgados, los campos de fútbol "Ramón Martínez", la escuela de idiomas de Ponferrada, el colegio "Peñalba" y el instituto "Álvaro de Mendaña". El parque es atravesado por la avenida "Gran vía del Reino de León".


Es el último parque construido en Ponferrada, ubicado junto al nuevo barrio de la Rosaleda, y que une este barrio con Cuatrovientos. Es un importante espacio verde, en el que predomina el césped. En él se encuentra el nuevo conservatorio de música de Ponferrada. Tiene un carril bici que bordea todo el parque, y cuenta con varias zonas de merenderos, así como una zona de juegos infantiles.


Además de estos parques, existen más zonas verdes distribuidas por toda la ciudad:

En toda la comarca se producen vinos de gran calidad (Denominación de Origen Bierzo), exportados a diversos sitios como Alemania o Estados Unidos, en el municipìo de Ponferrada, concretamente en el barrio del Puente Boeza tiene su sede la Cooperativa Vitivinícola "Cepas del Bierzo" la de mayor producción en tonelaje de uvas de El Bierzo, que recoge la uva procedente de las viñas de los pueblos de alrededor de la ciudad como son San Lorenzo, Otero o Campo, por lo que los últimos fines de semana del mes de septiembre o el primero de octubre, según hayan sido las precipitaciones y las condiciones climáticas durante el verano, se realiza la vendimia, en la que se procede a recolectar la uva de las viñas de estos pueblos ubicados en el sur y sureste de Ponferrada.

También tienen un lugar muy destacado en la agricultura ponferradina y berciana las frutas y hortalizas (Denominación de origen Manzana Reineta del Bierzo o Pimiento del Bierzo); castañas, pera Conferencia,cerezas etcétera.

Aún se explotan numerosas huertas, bien para el consumo familiar o bien para una posterior venta en el mercado de abastos de Ponferrada o fruterías locales. Estas huertas todavía se pueden encontrar en la ciudad, por la zona de la ribera del Sil en el barrio de los Judíos, que son el vestigio de la zona de "Las Huertas del Sacramento", ubicada donde actualmente está la avenida homónima, y donde numerosos ponferradinos tenían tierras en las que cultivaban diferentes tipos de hortalizas. Otros lugares aún próximos a la zona urbana son las zonas de huertas existentes entre los barrios de Flores del Sil y La Placa, o entre Cuatrovientos y Columbrianos, además de en las zonas rurales. En septiembre, las familias que han plantado pimientos en sus huertas, todavía continúan con la tradición del asado de pimientos, comenzada en el siglo XVIII por las amas de casa bercianas, conocidas como "las pimenteras", y que consiste en recolecta los pimientos, almacenarlos y a los pocos días, se asan, y posteriormente se pelan y se eliminan las semillas, pasando a envasarlos en pequeños botes de vidrio.

La ganadería, aunque ha sufrido un retroceso en el municipio debido al crecimiento de la ciudad, sigue teniendo una cierta importancia tanto a nivel de explotaciones familiares en los pueblos del municipio, como en la cría de distintas especies para sacrificio.
Hasta 2009 Ponferrada contaba con un matadero municipal ubicado en la zona de La Martina, a la espera de la construcción de un matadero comarcal.

Sede de CUPA GROUP, líder mundial en la producción y comercialización de pizarras para cubiertas, con más de 2.000 trabajadores y filiales en 11 países. Cuenta con la mayor planta de producción de palas (LM Wind Power) y fustes (Grupo Comonor) para aerogeneradores eléctricos de Europa que emplearon en su día a unos 1.500 trabajadores encuadrándose por tanto dentro de la gran industria, aunque actualmente ha cesado la actividad en la segunda y se ha reducido la plantilla en la primera. Una empresa de siderurgia, Aceros Roldán S.A., que posee una planta de mediano tamaño dentro del grupo Acerinox y diversas pequeñas empresas. Otra empresa a destacar en la comarca es Vitro Cristalglass (transformación del vidrio) quien empleó a más 600 trabajadores y cuyas instalaciones son las de mayor fabricación de vidrio aislante de la península. Actualmente, debido a la crisis económica, esta empresa se encuentra en concurso de acreedores. Así mismo es la cuna tanto de la Minero Siderúrgica de Ponferrada, recientemente convertida en Coto Minero Cantábrico, la mayor empresa minera privada de España, como de ENDESA," Empresa Nacional de Electricidad", fundada en 1947, privatizada en el 2000 y que es la mayor eléctrica de España e Iberoamérica. "Compostilla I" fue su primera planta de producción, inaugurada a principio de los años 50. En la década de los 60 se crea la Central térmica de Compostilla II, en el cercano municipio de Cubillos del Sil (León).
A estas dos industrias, la MSP y ENDESA es a las que se les debe mayoritariamente el gran crecimiento de Ponferrada en el siglo XX.

Dentro del municipio tienen su sede distintas empresas dedicadas a la fabricación de productos cárnicos con gran producción y volumen de ventas, como es el caso de "Embutidos Pajariel". Su zona de influencia también cuenta con industrias agroalimentarias y empresas que se dedican a la producción de cemento y también de suma importancia la manufacturación de la pizarra.

En la antigua "Central Térmica de Compostilla I", tiene su sede la fundación "Ciudad de la Energía (CIUDEN)". A finales de 2008 se aprueba el proyecto que convertirá la antigua "Central Térmica de la MSP" y parte de la "Central Térmica de Compostilla I" en el "Museo Nacional de la Energía".

En Ponferrada existen varios polígonos industriales:





Actualmente está en construcción el Cylog, un centro logístico que se ubica en la zona de "los muelles", en las cercanías de Cuatrovientos, y que actuará como un puerto seco que contará con acceso ferroviario y por carretera. Existen acuerdos para transportar mercancías de los puertos de La Coruña y Vigo.

El sector servicios, que sigue creciendo como en el resto de Europa, representa actualmente el sector probablemente más en auge y el que mayores ingresos produce aunque centrado en el comercio generando un nulo valor añadido y empleo con una escasa demanda de cualificación, como el resto del país.

Ponferrada ha sido un destacado nudo de comunicaciones desde la primera mitad del siglo XII, formando parte del Camino de Santiago. Desde principios del siglo XX en el que se asfaltan las primeras carreteras, forma parte del itinerario de la N-VI, una de las seis carreteras nacionales radiales que vertebran España. En la actualidad, la ciudad se encuentra plenamente integrada en la mediante la autovía A-6, que es la principal entrada a Galicia.

En 2011, Ponferrada contaba con un total de 46.816 vehículos de motor, lo que resulta a 680 automóviles por cada 1.000 habitantes.

Existe una estación de ITV ubicada en la a la altura del barrio de Fuentesnuevas

Ponferrada cuenta con una estación de ferrocarril, gestionada por Adif que mantiene líneas con Vigo, La Coruña, Zaragoza, Madrid y Barcelona.

Está ubicada en el Barrio de la Estación y presta servicio "Renfe Operadora", mediante diversos trenes destinados a trayectos de Larga Distancia, como el Alvia, el Arco o Trenhotel, u orientados a relaciones de Media Distancia, en este caso, Intercity o Regional Exprés.

Permite numerosas conexiones a nivel nacional, puesto que enlaza diariamente con comunidades como Navarra, País Vasco, Aragón, Cataluña contando con varios trenes a lo largo del día.

Además de esta estación, destinada principalmente al transporte de viajeros, en el Polígono Industrial del Bierzo (PIB), existe una terminal para el transporte ferroviario de mercancías, gestionada por Adif y situada entre Ponferrada, Cuatrovientos y Flores del Sil. Es utilizada mayoritariamente para descargar trenes de carbón procedentes de los puertos de La Coruña, Avilés o de Gijón. También salen de ella trenes cargados con la ceniza producida por la central de Compostilla II. Dispone de 4 vías que están electrificadas en su cabecera, de las cuales dos tienen acceso al muelle de carga y descarga de la terminal.

Ponferrada era el origen del Ferrocarril Ponferrada - Villablino de vía métrica que conectaba las minas de Laciana con las centrales térmicas de Ponferrada y la línea de RENFE para trasbordar el carbón a los trenes de ésta, siendo la estación ponferradina actualmente la sede del Museo del Ferrocarril de Ponferrada. En él hay restauradas varias locomotoras y vagones que prestaron servicio en la línea. En el municipio se conservan todavía algunos restos de este ferrocarril que hoy día parte de Cubillos del Sil, pudiéndose ver en algunas zonas los raíles de la vía.

La estación de autobuses está situada en la avenida de la Libertad, y desde ella parten autobuses con destinos comarcales como Toral de los Vados, Carracedelo, Bembibre, Villafranca...;regionales como León o Salamanca y destinos nacionales a Madrid, Santiago de Compostela, La Coruña...

La gran mayoría de los destinos regionales y nacionales están operados por Alsa, mientras que los comarcales son prestados por empresas locales (Aupsa, Pelines y González de la Riva) además de Alsa

Actualmente la ciudad de Ponferrada cuenta con un servicio de transporte urbano de 15 líneas de las que podemos destacar la circular y búho.
En los días laborables y sábados por la mañana existen 9 líneas de las cuales 8 son diametrales y conectan los diversos barrios de Ponferrada. Tienen una frecuencia de 1 hora:









También existe la línea Circular que conecta la zona alta de la ciudad con diferentes puntos administrativos o de interés como el Centro Comercial El Rosal, el Barrio de la Rosaleda, la estación de autobuses, Hospital de la Reina, el ayuntamiento o los Juzgados. Cuenta con una frecuencia de aproximadamente 15 minutos.

Los fines de semana y los días festivos, debido al menor uso del autobús circulan únicamente 6 líneas:





Estas líneas tienen una frecuencia de 1 hora y circulan los sábados por la tarde, domingos y festivos.
Los domingos y festivos el servicio comienza a las 11:00.
Además también circulan:



Hay que destacar que la totalidad de las líneas pasan por la parada del Intercambiador, para permitir una mayor intermodalidad entre ellas y una mayor variedad de destinos.

En el verano de 2009 para evitar que siguiese creciendo el Déficit del TUP se realizó un plan de Ajuste en el cual se suprimieron las Líneas L13, C2 y C9, así como se redujeron las frecuencias de paso de algunas líneas por los pueblos de sus extremos, como Bárcena, San Lorenzo, Santo Tomás o Dehesas, y que vieron aumentado su tiempo de espera de 1 a 2 horas.

Toda la flota fue renovada en el 2007, así como además de la creación de nuevas líneas y bonos se renovaron las marquesinas y paradas y se introdujeron los paneles electrónicos para saber el tiempo de espera de los buses en las paradas más importantes.
Al TUP se puede acceder con la tarjeta ciudadana de Ponferrada. En 2013 el billete sencillo del TUP cuesta 1,15€ y da derecho a transbordo a otras líneas durante 45 minutos.

Además del servicio de autobuses urbanos (TUP), Ponferrada cuenta con un sistema de préstamo de bicicletas, contando con 10 estaciones distribuidas por diferentes barrios y zonas de la ciudad. En los últimos años se ha comenzado a expandir la puesta en servicio de tramos de carril bici en algunas zonas de Ponferrada, bien sea integrándolos en la acera o en carreteras existentes, así como limitando la velocidad de algunas calles del centro urbano para poder hacer compatible el uso de la bici por ellas. También se han distribuido aparcabicis junto a lugares públicos o deportivos para poder dejar la bicicleta en ellos.

Ponferrada cuenta con tres zonas en las que se restringe el acceso al tráfico, que son el Casco Antiguo, la Avenida de España y la zona comercial del centro de la ciudad que está acotada por las calles Gómez Núñez, Camino de Santiago y Ramón y Cajal, a las que únicamente pueden entrar los residentes, quedando como zonas semipeatonales, fomentando el turismo y el comercio, estableciéndose varios aparcamientos disuasorios ubicados en la Avenida de la Libertad, el Museo del Ferrocarril y el Albergue de Peregrinos para estacionar los vehículos. También existen tres aparcamientos subterráneos en la ciudad, construidos en los últimos años. El primero se construyó bajo el bulevar de Pérez Colino, el segundo, bajo la Plaza del Ayuntamiento, y el tercero, en la Avenida de Compostilla. Entre todos suman un total de más de 800 plazas.

Debido a la orografía ponferradina, desde el año 2003 para acceder a la zona alta de la ciudad desde la calle General Vives, se ha instalado un ascensor público panorámico de uso gratuito y en funcionamiento las 24 horas del día, que facilita el acceso al casco histórico y a la popular "Calle Ancha", una de las principales arterias de la zona alta de Ponferrada. En el año 2011, y con el aval del uso masivo con el que cuenta el primer ascensor, se procedió a inaugurar el segundo ascensor público de similares características al primero, y permite una mejor unión del Barrio de los Judíos con la zona alta y el parque del Plantío.

El Aeropuerto de León, que entró en servicio en el año 1999, es el aeropuerto más cercano, encontrándose a 102 kilómetros de Ponferrada. Según las estadísticas de Aena, en 2008 el aeropuerto movió 122.809 pasajeros, 5.700 operaciones y 15,9 toneladas de carga.

Mantiene vuelos con Madrid, Barcelona, Valencia, Tenerife y París todo el año, que se refuerzan en temporada estival con enlaces a Palma de Mallorca, Málaga-Costa del Sol, Ibiza, Gran Canaria y Menorca.

Ponferrada es la cabeza del Partido judicial de Ponferrada. Es el número 4 de los siete y su demarcación comprende al municipio de Ponferrada y el resto de municipios de El Bierzo a excepción del municipio de Palacios del Sil. El conjunto de organismos judiciales es el siguiente:


Ponferrada cuenta con los siguientes centros educativos:

Creado, según palabras del rector Ángel Penas de la universidad, como una «"apuesta de futuro"», el Campus de Ponferrada dependiente de la Universidad de León comenzó a impartir docencia en el curso 1996/1997 después de años de reivindicaciones para que Ponferrada se dotara de instituciones universitarias (ver: UNED). Para ello, se reaprovechó el edificio del antiguo Hospital "Camino de Santiago", vacío después de su traslado a un edificio de nueva planta.

La actividad en el Campus ponferradino se inició con solo 149 alumnos; hoy, supera los 1500, tras años en que el campus ha ido aumentando su abanico de titulaciones, al tiempo que se han ido dotando de nuevas y más modernas infraestructuras; entre ellas, nuevos edificios para servicios, institutos de investigación, cafetería, biblioteca universitaria, piscina climatizada...

En la actualidad, el abanico de titulaciones impartidas en Ponferrada son las siguientes:

Son diversas las fiestas universitarias a lo largo del año pero las de mayor importancia son sin duda la fiesta del magosto y la de Agrícolas (18 de marzo), adquiriendo también cierta importancia las fiesta de San Isidro, de Forestales.

La sanidad ponferradina está gestionada por el ente autonómico Sacyl (Sanidad de Castilla y León).La Ley 1/1993, de 6 de abril, de Ordenación del Sistema Sanitario, divide la atención sanitaria en tres niveles de atención: primaria, especializada y continuada.

La atención primaria está gestionada desde el Área de Salud de El Bierzo. Ponferrada cuenta con 4 centros de salud:
Además de estos centros, en los últimos años se están construyendo consultorios médicos en algunas pedanías para acercar la atención médica a los pueblos. En estos centros un médico especialista perteneciente generalmente a un centro de salud acude unos días determinados a la semana para prestar consulta. Actualmente tienen consultorio médico los pueblos de San Esteban de Valdueza, Campo, Fuentesnuevas, Dehesas, San Lorenzo, Toral de Merayo, Villanueva de Valdueza y San Andrés de Montejos.

En cuanto a la atención especializada, en Fuentesnuevas está ubicado el Hospital Comarcal de El Bierzo, que da servicio a todas las poblaciones del Bierzo así como a la comarca vecina de Laciana. El centro cuenta con un total de 450 camas y en él se imparten numerosas especialidades. Actualmente en algunas consultas concretas presenta índices de saturación. Este hospital se inauguró en 1994 en sustitución del antiguo "Hospital Camino de Santiago" ubicado en la zona alta que debido al aumento de la población se había quedado pequeño para atender a toda la demanda.

Además de las numerosas consultas privadas de especialistas que hay distribuidas por la ciudad (clínicas dentales, oftalmológicas, laboratorios de análisis, consultas de dermatólogos, cardiólogos, podólogos, fisioterapeutas...) existen dos grandes centros sanitarios privados en Ponferrada:

Actualmente Ponferrada cuenta con una amplia red de farmacias para acercar a los ponferradinos la compra de medicamentos. La distribución de farmacias es la siguiente:

Por la noche, hay una farmacia de guardia en la ciudad de manera rotatoria, es decir, cada noche le toca a una farmacia de Ponferrada abrir en horario nocturno. Además de esta farmacia de guardia rotatoria, existen dos farmacias, una ubicada en el centro comercial "El Rosal" y otra en el centro de la ciudad, que están abiertas las 24 horas

Ponferrada cuenta con un gran centro comercial ("C.C. El Rosal"), inaugurado en octubre de 2007, que integra un hipermercado Carrefour, 147 tiendas (Zara y otras tiendas del grupo Inditex, Cortefiel, Mango, Desigual, C&A, H&M, Benetton o Worten, The Phone House, Arenal, Tiger, entre otras), más de 15 restaurantes y multicine de 7 salas.

Ubicado en la zona de la Avenida de los Escritores/Aldama y cercano al Museo del Ferrocarril, está compuesto por establecimientos de grandes superficies como Decathlon Easy, Star Center (antes Darty), Sprinter, Lidl, Mercadona o el restaurante Mc Donalds.

En esta ubicación se encontraban también los 'Cines la Dehesa', trasladados en 2007 al Centro Comercial El Rosal.

Otro centro importante de comercio, es el remodelado Mercado de Abastos destinado a la alimentación, con productos autóctonos como carnes, frutas y hortalizas además de pescado. Actualmente se pretende potenciar con la apertura de gastrobares en las zonas que están en desuso del mismo. Destacan 5 gastrobares: Deleites Gourmet, Er Pescaíto, La crepería, Rosquilla Berciana y SaboreArte. 

A esta oferta hay que añadir la numerosa oferta comercial que se concentra principalmente en el entorno de la Plaza de Lazúrtegui donde se encuentran grandes firmas como: Zara, Bershka, Pull & Bear, Oysho, Stradivarius, Massimo Dutti, Springfield o Cortefiel entre otras, así como la presencia del grupo El Corte Inglés con tiendas Sfera, Telecor o viajes El Corte Inglés

Con la apertura del C.C. El Rosal en 2007 se creó el Centro Comercial Abierto La Cebra, más tarde, en 2015 Templarium toma el relevo, ampliando la zona de acción al resto del núcleo urbano, es una asociación de comerciantes, hosteleros, servicios de ocio y profesionales de Ponferrada que han decidido aunar fuerzas para poder competir con las grandes superficies. Entre otras iniciativas, realizan una feria de rebajas al finalizar el período de rebajas de invierno para dar salida al stock de las tiendas a unos precios muy económicos.

La zona alta de la ciudad podría decirse que se ha especializado más como zona de ronda de vinos y bares, concretamente los que están en el entorno de la plaza del Ayuntamiento o la plaza de la Encina.

La Virgen de la Encina es la patrona de Ponferrada, de El Bierzo. El Consejo Comarcal de El Bierzo pretende convertirla en festividad laboral en El Bierzo.

El día de la Encina es el 8 de septiembre, el 9 se celebra la Encinina. Los dos días festivos locales establecidos por decreto de la alcaldía. Trasladándose a fecha diferente si coincidiera en sábado o domingo. También se realiza una fiesta juvenil e infantil del 1 al 6 de septiembre llamada "Cima" en el Parque del temple, en el cual se instalan hinchables y un tren que recorre el parque así como talleres de manualidades y de aventura, representación de obras de teatro y actuaciones de baile etc... para los niños.

Las fiestas tienen lugar en los primeros días de septiembre, entre los días 5 y 9, según programa establecido por el Patronato de Fiestas, presidido por el Concejal de Cultura.
Según la tradición, la talla de la Virgen fue localizada en el interior de una encina por los Templarios en la Edad Media, donde habría sido depositada para protegerla de los musulmanes. Como curiosidad, se trata de una de las pocas vírgenes negras que existen, por lo que también es conocida popularmente como "La Morenica".

En estas fiestas se realiza una feria de Cerámica en el patio del CEIP Campo de la Cruz que cuenta con la representación de numerosos artistas y en la que también se imparten cursos sobre el manejo de la cerámica; exposiciones de pintores locales ubicadas en el casco antiguo y numerosas degustaciones gastronómicas ya sean de carácter local o nacional.

Otras actividades realizadas en estas fiestas son el Festival de la canción berciana, en el cual participan grupos corales de toda la comarca del Bierzo, así como diferentes competiciones deportivas y conciertos en el Auditorio Municipal, en el denominado ciclo de "Noches de la Encina", que ha traído a importantes artistas de nivel nacional. El día 8 de septiembre por la tarde se celebra el ya tradicional desfile de carrozas que cierra la reina de las fiestas con sus damas de honor. En 2012 a causa de la crisis económica se ha suspendido la elección de ésta

En la explanada situada entre la Avenida de la Libertad y la Avenida de la Lealtad se instalan carpas donde se realizan degustaciones gastronómicas, y también es en ese lugar donde se instalan las orquestas que amenizan las noches de verbena de las fiestas.

Magostos de mayor o menor importancia y afluencia de público se celebran en muchos barrios de la ciudad, donde realmente se muestra el carácter hospitalario y acogedor de sus habitantes.

Consiste en una tarde/noche en la que se comen castañas asadas y otras viandas, como el "bollo preñao". Suelen estar animados por bandas o grupos de música. Un magosto muy popular es el del Campus universitario en el cual se concentran cientos de jóvenes alrededor de las hogueras.

También estos magostos se suelen celebrar en los colegios de Ponferrada, organizados por las asociaciones de padres, madres y alumnos de los respectivos colegios, o por las asociaciones vecinales de cada barrio.

Celebrada la primera luna llena del mes de julio, la Noche Templaria recrea un acontecimiento medieval lleno de fantasía.

Se trata de la representación de como Frey Guido de Garda, Maestre de la Orden de los Caballeros Templarios, vuelve a la ciudad del Puente de Hierro para sellar con ella un pacto de eterna amistad y entregarle la custodia de los símbolos hallados en la tierra sagrada de Jerusalén: la sagrada Arca de la Alianza y el Santo Grial.
La comitiva Templaria es recibida por miles de ponferradinos ataviados con ropajes medievales que, en desfile y custodiando el Arca de la Alianza y el Santo Grial, se dirigen hacia el Castillo.

Llegados hasta aquí se realiza un Juicio a la Orden Templaria. “Yo, Guido de Garda, Maestre de la fortaleza de Pons Ferrata, comprometo a todo el pueblo de Ponferrada para que vuelva cada año a renovar este compromiso festivo con su historia y su leyenda hasta que el tiempo llegue a borrar la línea del horizonte.

La Noche Templaria es una de las fiestas más animadas del verano ponferradino, todo ello amenizado con música, fuegos artificiales, degustaciones gastronómicas (consolidándose con gran popularidad la cena medieval) o animaciones de calles entre otras cosas, además de un gran desfile templario.

En Ponferrada también se celebra al igual que en toda España la Semana Santa. Es uno de los principales acontecimientos religiosos de la ciudad a lo larga del año, y la Semana Santa Ponferradina está declarada de Interés Turístico Regional, y desde 2015 también de Interés turístico Nacional, solicitada en 2012.
Cuenta con cuatro hermandades:
Se realizan diferentes actos procesionales en los días de la Semana Santa comprendidos entre el Viernes de Dolores y el Domingo de Resurrección
Además de las procesiones, otro personaje típico de la semana Santa ponferradina es el Nazareno Lambrión chupacandiles, que sale por las calles de Ponferrada el día antes del pregón de Semana Santa anunciando la llegada de ésta.

Otras peculiaridades de la semana Santa en Ponferrada son la custodia de la llave de la basílica de la Encina por parte del Alcalde de Ponferrada el Jueves y Viernes Santo o la llamada ritual de timbales y clarinetes en la madrugada del Viernes Santo.

Gala que anualmente se celebra en Ponferrada un sábado de abril o mayo en la "Federación de Asociaciones de la Radio y la Televisión" entrega a diferentes personalidades del mundo del periodismo, política, cine, danza, tauromaquia, teatro, deporte o moda. Hasta 2012 se han realizado diez galas, siendo hasta 2010 celebrados en el "Pabellón del Toralín", donde se realizaba una gran cena-entrega de premios con más de 2000 invitados, pasándose a hacer desde la edición de 2011 la entrega de premios en el "Teatro Bérgidum" y una posterior cena en el Castillo de los Templarios. Como anécdota, los invitados llegan a la alfombra roja donde son recibidos por las autoridades en coches clásicos, realizando un recorrido por la ciudad.

Es patrocinada y organizado por el locutor de radio Luis del Olmo. A este acto han acudido diferentes personalidades como Vicente del Bosque, Antonio Banderas, Carmen Cervera o Rocío Jurado.

Conjuntamente a la celebración de la gala, durante esa semana se celebra la "semana de la radio", en la que se emiten programas radiofónicos nacionales desde Ponferrada y se realizan diferentes foros de debate sobre la situación y futuro de la radio española, entre otras actividades.

Se celebra del domingo al martes anterior al Miércoles de Ceniza. El domingo por la tarde se celebra el "Carnaval infantil", un desfile que se realiza por el centro de la ciudad y finaliza en el parque de Flores del Sil organizado por las AMPAS de los colegios de Ponferrada, disfrazándose cada colegio de un tema diferente. El lunes es el "Carnaval de los jóvenes", en el que se organiza por Cima un rompecabezas en el que los participantes integrados en grupos disfrazados tienen que buscar pistas distribuidas por toda la ciudad. El martes se realiza el tradicional desfile de carnaval, en el que participan tanto peñas como grupos, que optan por un premio económico por categorías. Este desfile comienza en Flores del Sil, atraviesa el centro de la ciudad y llega a la zona alta finalizando en la Plaza del Ayuntamiento, donde se realiza un baile popular a cargo de una orquesta.

El día 18 de julio se festeja el Corpus Christi, con solemne misa en la Ermita del Cristo (situada en el Camino de Santiago), en esta fiesta se suele merendar con ternera que se vende en el Bar del "Campo de la fiesta".

El 15 de agosto se celebra el Festival de la sardina. Donde se puede degustar una buena tapa de sardinas, acompañada de buen vino y amenizada con una orquesta al más puro estilo de una verbena. En honor de Nuestra Señora y San Roque. Hoy en día las nuevas actividades festivas han llevado a que los actos más concurridos sean La Procesión de la Luz (en plena noche y con alumbrado de velas) y la ronda de Bodegas (un acto popular de visitar la Calle Real, donde los vecinos sacan sus bodegas a la misma calle ofreciendo vino y comida gratis a todos los asistentes. Se ha puesto de moda entre los jóvenes del pueblo realizar, entre el grupo de "colegas", graciosas camisetas para animar el acto. Se espera que en años venideros la comisión de fiestas dedique un pequeño premio económico a las 3 mejores). En este barrio ponferradino se sitúan el hospital comarcal del Bierzo, el más grande de la comarca, un centro de asprona, un instituto, un colegio de educación especial y otro público -"la cogolla"- y un edificio del Proyecto Hombre.

Está comunicado con Ponferrada por las líneas , , , , , , y del TUP

Situado al Noroeste de Ponferrada, el barrio está dividido por el Canal Bajo del Bierzo, y dista 2,5 km del centro de la ciudad.

La creación del barrio surge por la llegada de obreros para trabajar en la construcción de la Central Térmica de Endesa de Compostilla I (hoy, ya desmantelada, sede del CIUDEN y futura sede del museo de la Energía) y también para trabajar en ella. Dicho barrio recibe la definición de barrio residencial debido a que la mayoría de sus edificaciones son unifamiliares.

Cuenta con instalaciones deportivas privadas, canchas de tenis, campos de baloncesto, campo de fútbol (usado, habitualmente, por la S.D.Ponferradina B) y canchas de pádel. También tiene unas piscinas de verano.

Este barrio está sufriendo cambios con la construcción en su entorno del nuevo Parque de Bomberos. Cuenta también con la sede de la Cruz Roja de la ciudad y el nuevo centro de Formación Profesional contiguo al Parque de Bomberos.

Podemos destacar la gran obra para la ciudad de Ponferrada que fue la retirada de la Montaña de Carbón que separaba el centro de la ciudad de dicho barrio. En este espacio está prevista la construcción de un proyecto para la juventud, que constará de un parque, zonas culturales y viviendas para jóvenes, así como el Museo de la Energía.

Está comunicado con Ponferrada por las líneas , y del TUP

Fiestas del barrio:


Este conocido barrio de Ponferrada al principio no era más que una hilera de casas bordeando la primera nacional VI que se construyó, allá por los años cincuenta. Los terrenos adyacentes, que básicamente eran zarzales o vegetación libre con alguna finca de manzanos, poco a poco fueron comprados por emigrantes, principalmente procedentes de Galicia pero también de muchos otros pueblos de El Bierzo, que en muchos casos se construyeron sus casas ellos mismos. Actualmente cuenta con un centro de salud el "Ponferrada IV" que posee una plantilla de seis médicos de familia, dos fisioterapeutas, un odontólogo, una matrona de área, un pediatra, un higienista dental y dos auxiliares de enfermería y servicios como el de Fisioterapia. También se encuentran en este barrio el colegio público Jesús Maestro y el centro concertado San José Obrero, además de un centro cívico y el parque Pablo Picasso.

Está comunicado con Ponferrada por las líneas , , , (Por el Canal), , y del TUP

En principio se llamaba San Dionisio. El nombre actual se debe a la ubicación de la placa donde giraban las locomotoras del ferrocarril, ya que en este barrio era donde estaban ubicados los talleres de RENFE en Ponferrada. Actualmente es un barrio en expansión. Se ubicará en un futuro la estación del AVE de Ponferrada, además de trasladarse aquí la estación de autobuses para configurar un enclave intermediario entre los diversos medios de transporte. Cuenta con su propio colegio, el CP Virgen del Carmen, además de con unas piscinas privadas y una iglesia, entre otros servicios.
A destacar la imagen, procedente de Ferradillo (traída al quedar deshabitado este pueblo) de San Bartolomé.

La Placa está comunicada con Ponferrada por las líneas , y del TUP

Es una barrio de reciente creación, que se extiende desde los límites que marca la vía del ferrocarril, al este y norte; el río Sil al sur y la pedanía de La Martina al oeste. Los primeros pobladores se instalaron en el barrio en los años 20. Hasta entonces, los terrenos donde se ubica, eran en su mayoría campos de cultivo especialmente de cereal, pertenecientes a los vecinos del cercano pueblo de Toral de Merayo. El barrio fue surgiendo en torno a la carretera Nacional 120, Logroño-Vigo, llamada en Ponferrada, carretera de Orense.
La fiesta se celebra siempre el segundo domingo de mayo en honor a Jesús Divino Obrero. El obispo de Astorga eligió este patrono, por consejo de la gente, en el año 1936 ya que la mayor parte de la población era obrera. El primer templo del barrio, sin embargo, está dedicada a Santiago Apóstol.
En 2006 la población de este populoso barrio de Ponferrada era de 9072 habitantes.

Actualmente cuenta con el centro de Salud Ponferrada III y el IES Europa, ubicados en la zona del Temple, que se encuentra situada al este del barrio; delimitada al norte y al este por las vías del ferrocarril de la línea Palencia - La Coruña, al sur por el río Sil y al oeste por la "Avenida de La Cemba", se considera como un barrio dentro de Flores del Sil, donde están establecidos los servicios citados anteriormente además de una residencia de ancianos en el antiguo campo de la Minero, gestionada por la Junta de Castilla y León, un centro de día para enfermos de Alzheimer,y el parque homónimo, uno de los más grandes de la ciudad y donde entre otras actividades, se realiza en las fiestas de la Encina la denominada "Ciudad Mágica" CIMA, consistente en atracciones hinchables, juegos y talleres para los niños, o zonas dedicadas para practicar la petanca.

Es digno de nombrar que el otro parque grande de Flores del Sil fue adquirido en la década de los 70 por los propios vecinos, en "cuestación" popular. Para ello compraron parte de los terrenos de la finca del Trinitario.
Se ha inaugurado recientemente un centro cívico ubicado en la avenida de la Martina. Como centros educativos cuenta con el colegio Ponferrada XII que nace de la fusión de los 2 anteriores colegios existentes y el colegio bilingüe concertado La Asunción, perteneciente a las religiosas de la Asunción.

El barrio está conectado a Ponferrada mediante las líneas L1, , , , y del TUP

Su nombre es debido al puente medieval (cerca de él existen restos de uno anterior de construcción romana) que cruza el Río Boeza a su paso por este barrio.

Recientemente el puente medieval, que hasta bien poco aún daba servicio a la circulación de vehículos de motor, paso a ser peatonal al construirse a corta distancia otro puente de nueva fábrica.

Este barrio dividido en dos por el río Boeza, da paso a la parte sur del municipio de Ponferrada y a la comarca tradicional del Valle del Oza o Valdueza, así como la salida hacía los Montes Aquilanos, El Morredero y la parte más oriental de La Cabrera.

De él parte la carretera de Sanabria, carretera proyectada en 1919 para comunicar El Bierzo con La Cabrera (Ponferrada-Santalavilla-Puebla de Sanabria) y que quedó sin terminar en lo alto del pico de la Aquiana en el llamado Campo de las Danzas.

Está conectado a Ponferrada por las líneas del TUP , , y 

En el barrio se ubica la cooperativa "Cepas del Bierzo" y cuenta con un gran prado en el que está delimitado un campo polivalente de Fútbol y Rugby. En la zona norte hay un polideportivo, conocido popularmente como "JT", y cuya denominación oficial es "Polideportivo Antonio Vecino".

Los eventos festivos de este barrio son:

Barrio ligado a la antigua carretera a Orense, situado entre el barrio de Flores del Sil y Dehesas. Dentro de sus límites se encuentran numerosas naves industriales. Las casas que lo forman suelen ser unifamiliares o de pocas alturas.

Está unido a Ponferrada por las líneas L1 y del TUP.

Nuevo barrio en constante crecimiento de la ciudad de Ponferrada cuenta con el edificio más alto de Castilla y León. La torre de la rosaleda "con cerca de 28 pisos", un centro comercial "El Rosal", un conservatorio y un barrio comercial con el bulevar Juan Carlos I como base. Se va a construir un centro de salud y un nuevo edificio para la junta de Castilla y León de 14 plantas. En él está ubicado el Parque del Oeste, que es el parque más grande de Ponferrada y que se prolongará con la creación del parque de la juventud, creando un gran pulmón verde para Ponferrada.
También se ha construido una nueva iglesia de diseño, la parroquia del buen Pastor, en el Bulevar Juan Carlos I, y próximamente se edificará una nueva residencia privada de mayores en el barrio.

Por el centro del barrio pasa la línea del TUP. Además, por los diferentes extremos de La Rosaleda se pueden coger otras líneas como la , , , , , , y en la avenida Galicia y la en la avenida de Asturias.

El ayuntamiento de Ponferrada cuenta con varias escuelas deportivas para potenciar el deporte entre los jóvenes; en total 41 escuelas de numerosos deportes, como son las de Aeromodelismo, Esquí, Aire Comprimido, Escalada, Salvamento Deportivo, Rugby, Béisbol, Trial, Tiro con arco, billar, buceo, capoeira, atletismo etc...

La Sociedad Deportiva Ponferradina es el equipo de la ciudad. Fue fundado en 1922 y juega en la Segunda División B de España.

Juega en el Estadio El Toralín; inaugurado el 5 de septiembre de 2000 fue ampliada su capacidad en la temporada 2006-07 que es, actualmente de 8300 espectadores, midiendo el terreno de juego 105x70 metros.

Este estadio sustituye al antiguo "Fuentesnuevas" (1975-2000) el cual sustituyó al estadio de "Santa Marta" que fue el estadio original.

La Sociedad Deportiva Ponferradina jugó contra el Sevilla Fútbol Club en los dieciseisavos de final de la Copa del Rey en el 29 de octubre de 2008, ganando por 1-0 con un gol del cántabro Cristian Portilla.

La ""Ponfe"" ha jugado en la Segunda División de España en dos temporadas, 2006/2007 y 2010/2011. En la primera ocasión, consiguió el ascenso frente al Alicante Club de Fútbol en el Estadio José Rico Pérez con un gol del jugador Francisco Domínguez. En el 2010 el ascenso fue consumando en El Toralín ante la Unió Esportiva Sant Andreu, tras una fatídica tanda de penaltis en la que el arquero blanquiazul Ian Mackay consiguió parar el lanzamiento del ex-deportivista Pedro Tarradellas.

El 13 de diciembre de 2011 la S.D.Ponferradina se enfrentó al Real Madrid en el encuentro de dieciseisavos de final de la Copa del Rey, el cual perdió por 0-2. Se contó con la presencia en El Toralín de jugadores como Cristiano Ronaldo, Sergio Ramos u Özil.

El 24 de junio de 2012 asciende de nuevo a Segunda División tras ganar en la final de los play off al Tenerife (1-0 en la ida y 1-2 en la vuelta).

El Promesas Ponferrada fue un equipo de fútbol de la ciudad de Ponferrada fundado en 1998, que jugaba sus partidos en Fuentesnuevas. Fue de los clubes más importantes de Ponferrada, aunque al final no consiguió ser lo que Tino Pérez había querido, un rival para la Ponferradina, ya que no consiguió ascender a 2.ªB.

Tras varios años del club "Jóvenes Trabajadores" en las categorías nacionales del baloncesto español, se declaró en quiebra, tomando su relevo el "Ciudad de Ponferrada", actualmente en Primera División Provincial Masculina de Baloncesto.

La ciudad cuenta con carriles Bici para practicar este deporte y recientemente se ha incrementado el número de estos.

La ciudad fue la sede del Campeonato Mundial de Ciclismo en Ruta en su edición de 2014. El Mundial, que se celebró entre los días 20 y 28 de septiembre de 2014, es el mayor hito deportivo de la historia de la comarca por su proyección nacional e internacional, y por la repercusión no sólo en imagen sino económica y social para la ciudad y El Bierzo, ya que movió miles de visitantes durante el tiempo que duró el evento.

El Mundial en cifras de la UCI: 350.000 espectadores "in situ" a lo largo de la semana de competición, 1.500 deportistas, 70 naciones, 6.000 acreditaciones oficiales, 73 cadenas de televisión, 400 horas de emisiones de televisión, 216 horas de emisión en vivo, una audiencia televisiva acumulada de 300 a 400 millones de espectadores.

La organización del mundial supuso preparar la ciudad para acoger el evento, con inversiones de las diferentes administraciones. Hasta ahora en España el Campeonato Mundial de Ciclismo en Ruta se había disputado en Madrid, Barcelona, San Sebastián y Benidorm.

La ciudad de Ponferrada participa en la iniciativa de hermanamiento de ciudades promovida, entre otras instituciones, por la Unión Europea. A partir de esta iniciativa se han establecido lazos con la siguiente localidad:

El elemento central del escudo de Ponferrada es un puente de oro, mazonado de sable (negro). El puente se encuentra situado sobre ondas de de azur y plata. En los adornos exteriores figuran un pergamino heráldico y una corona real antigua o abierta.

Popularmente se ha aceptado la canción tradicional ""A Ponferrada me voy"" como himno de Ponferrada, siendo una de las canciones más conocidas y populares. En 1950 se dio a conocer el himno de Ponferrada compuesto por Pedro Fernández Matachana: ""En el Bierzo, jardín del amor, floreció esta hermosa ciudad, Ponferrada, hidalga y señorial..."".







Originarios de Ponferrada son:



</doc>
<doc id="2400" url="https://es.wikipedia.org/wiki?curid=2400" title="Punk">
Punk

El punk, también llamado punk rock, es un género musical dentro del rock que emergió a mediados de los años 1970. Este género se caracteriza en la industria musical por su actitud independiente y contracultural. En sus inicios, el punk era una música muy simple y cruda, a veces descuidada: un tipo de rock sencillo, con melodías agresivas de duraciones cortas, sonidos de guitarras amplificadas poco controlados y ruidosos cargados de mucha distorsión, pocos arreglos e instrumentos y, por lo general, de compases y tempos rápidos.

Las líneas de guitarra se caracterizan por su sencillez y la crudeza del sonido amplificado y muy distorsionado, generalmente creando un ambiente sonoro ruidoso o agresivo heredado del garage rock. El bajo, por lo general, sigue solo la línea del acorde y no busca adornar con octavas ni arreglos la melodía. La batería por su parte lleva un tempo acelerado, con ritmos sencillos de rock and roll. Las voces varían desde expresiones fuertes e incluso violentas o desgarradas, mientras las letras se caracterizan por tratar temas como los problemas políticos y sociales (llevando con esto un mensaje de conciencia que se extiende, tratando de denunciar por medio de la música este tipo de problemas e incluso se llegan a tratar temas como el no a las drogas, amor, pacifismo, etc.). Aunque desde finales de los 70 e incluso antes (a excepción de la lírica) , estos aspectos fueron cambiando gracias a que las bandas que se formaron en ese tiempo (por general bandas del estilo hardcore y otras bandas más tradicionalistas) fueron agregando cambios de acorde muy rápidos en la guitarra, arreglos (para mayor complejidad) y solos de guitarra (heredados directamente del rock and roll y rockabilly), en el bajo se introdujeron los arreglos con octava, y la batería aceleraba más el ritmo y el tempo, las voces eran muchas fuertes. 
El punk rock explotó en la corriente musical a finales de los años 70 con bandas británicas como Sex Pistols; The Clash y The Damned, y bandas estadounidenses como Ramones, The Dead Boys y Blondie. Más adelante en la década de los 80, dentro de la escena independiente surgirían dos movimientos dentro del punk rock que se caracterizarían por seguir las ideas de la ética Hazlo tú mismo, y con una fórmula musical mucho más agresiva, rápida, y veloz; en Estados Unidos el hardcore (con bandas como Black Flag, Minor Threat, Dead Kennedys, Circle Jerks, Bad Brains, quienes ayudaron e influenciaron a músicos que más tarde conformarían la escena hardcore melódica, skate punk, nardcore e incluso straight edge/youth crew) y en el Reino Unido el UK '82 (representado por The Exploited, GBH, Chaos UK, Discharge, Varukers, estos últimos dos dieron continuación e influencia a formas más extremas del punk como d-beat, crust punk y grindcore). Muchos músicos influenciados por el punk rock (pero con distintos intereses musicales) más tarde formarían bandas de post-punk, heavy metal, grunge, emo, noise rock, ska, new wave, rock alternativo y gótico. En los 90, aparece el grunge con Nirvana; más tarde bandas californianas de skate punk y hardcore melódico seguirían vendiendo millones de discos como Offspring, Rancid y NOFX. A mediados de los 90. A finales de los 90 el mercado musical promovería a bandas de pop punk como Green Day o Blink-182; asimismo, bandas de punk hardcore y street punk siguieron formándose como The Casualties, Unseen, Blanks 77, Agnostic Front, Violent Society, Restarts, First Step, Battery, Good Clean Fun, entre muchas más bandas.

La forma originaria del punk era un tipo de rock sencillo y algo ruidoso para expresarse con sus propios medios y conceptos. Entre las primeras bandas musicales representantes del punk están The Ramones, Dead Boys, The Voidoids, Blondie, The Heartbreakers, Tuff Darts, The Cramps, The Misfits, Black Flag, The Gun Club, The Bags, Cherry Vanilla y Wayne County & the Electric Chairs y en el proto-punk The New York Dolls, The Velvet Underground (la banda de Lou Reed), Iggy Pop y The Stooges, MC5, la cantante Patti Smith, Television y The Dictators entre otros, todo esto en Estados Unidos.

En el Reino Unido proliferarían agrupaciones como Sex Pistols, The Clash, The Damned, Buzzcocks, The Adicts, The Pretenders, The Jam, The Stranglers, U.K. Subs y Adam & the Ants, además de David Bowie, Marc Bolan de T-Rex y The Who entre otros, quienes también influenciaron al punk igual que paralelamente lo hicieron las bandas "proto-punk" de garage rock de EE.UU.

Ramones se presentaban a sí mismos como banda de rock, sin pretensiones declaradas de mensaje directamente innovador o rompedor salvo en lo musical. Después surgiría la etiqueta punk y Ramones serían considerados los primeros en representarla. Por otro lado había una forma de transgresión, buscando liberarse de los estigmas sociales. Esta rama no daba explicaciones y buscaba incomodar a lo establecido chocando, ofendiendo y molestando al buen gusto, la moral y la tradición. Se buscaba básicamente la provocación a través de demostraciones de transgresión estética o giros de lenguaje contradictorios, absurdos o insolentes. Es el estilo que popularizaron Sex Pistols, relacionado ligeramente al nihilismo y otras formas de escepticismo. 

Más adelante, especialmente con la aparición del hardcore punk y marcado por la herencia de la actitud del colectivo Crass, se hizo presente todo un abanico de enfoques de crítica social, posicionamientos políticos y afinidad a campañas de protesta. El ejemplo musical más clásico son Crass y The Clash. La filosofía punk puede resumirse en: "Hazlo tú mismo" o "hazlo a tu manera". Rechaza los dogmas y cuestiona lo establecido. Desprecia las modas y la sociedad de masas (aunque su estética también puede considerarse una moda preestablecida por el punk). En sus canciones, estas bandas expresan un serio descontento con los sistemas e instituciones que organizan y controlan el mundo. En ocasiones también la música sirve de plataforma para propuestas filosóficas e ideológicas.

Durante la década de los 80, el punk en Estados Unidos estuvo permeado de contenidos políticos, principalmente progresistas, en oposición al gobierno conservador de la época. Ejemplos de esta época son las bandas Dead Kennedys y Bad Religion. En Europa, el punk es una música especialmente utilizada como medio de difusión por gente afín a movimientos políticos y sociales "outsiders", mayoritariamente de izquierda, aunque existe una corriente de derecha que lo utiliza, siendo criticada y denostada por la mayoría de los movimientos punk.

A principios de los 90, las publicaciones amateur del punk y las canciones de los grupos también sirvieron como vehículo de los planteamientos y denuncias de los movimientos antiglobalización.

A comienzos y mediados de los años 1960, las bandas de garage rock, que son reconocidas como las progenitoras del punk rock, comenzaron a aparecer en diferentes lugares de América del Norte. The Kingsmen, una banda de "garage rock" de Portland, Oregon, tuvo un éxito con su versión de "Louie, Louie" de 1963, citada como "el "urtext" que define al punk rock". El sonido minimalista de muchas bandas de "garage rock" fue influenciado por el ala más dura de la invasión británica. Los éxitos de The Kinks de 1964, "You Really Got Me" y "All Day and All of the Night", se han descrito como "precursores de todo el género de tres acordes de Ramones; por ejemplo, "I Don't Want You" de 1978, fue puro Kinks". 

En 1964, The Who rápidamente progresó desde su sencillo debut, "I Can't Explain" a "My Generation". A pesar de que tuvo poco impacto en las listas estadounidenses, el himno de The Who presagiaba una mezcla más cerebral de ferocidad musical y postura rebelde que caracterizó a gran parte del primer punk rock británico: John Reed describe la aparición de The Clash como una "ajustada bola de energía que recuerda tanto en imagen como en retórica a un joven Pete Townshend con obsesión por la velocidad, ropa pop-art y ambición de escuela de arte". The Who y The Small Faces fueron algunos de los pocos viejos roqueros conocidos por los Sex Pistols. 

En 1966, el "mod" ya estaba en declive. Las bandas de "garage rock" de Estados Unidos comenzaron a perder fuerza en un par de años, pero el sonido crudo y la actitud desconocida de bandas de rock psicodélico como The Seeds presagiaba el estilo de bandas que se conocerían como las figuras arquetípicas del proto-punk.

Uno de los antecedentes del estilo punk, y para algunos la primera muestra del género, ocurrió en la década de los años 1960 en Perú con la banda Los Saicos formada por jóvenes de 19 años. Aunque la banda ha indicado que ellos solo tocaban rock and roll, aceptan ser llamados los precursores del punk.

En 1969, aparecieron los álbumes debut de dos bandas de Míchigan que son conocidos comúnmente como las grabaciones principales del proto-punk. En enero, MC5 de Detroit lanzó "Kick Out the Jams". "Musicalmente el grupo es intencionalmente crudo y agresivo", escribió el crítico Lester Bangs en "Rolling Stone":

En agosto, The Stooges de Ann Arbor, lanzaron su álbum homónimo. De acuerdo con el crítico Greil Marcus, la banda, liderada por el cantante Iggy Pop, creó "el sonido del "Airmobile" de Chuck Berry después de que los ladrones lo desarmaran". El álbum fue producido por John Cale, un exmiembro del grupo de rock experimental The Velvet Underground. Habiendo ganado una "reputación como la primera banda de rock under", The Velvet Underground inspiró, directa o indirectamente, a muchos de los músicos que estuvieron involucrados en la creación del punk rock y el heavy rock.

Son también etiquetados como proto-punk las bandas New York Dolls, Dictators o incluso The Who; además de temas de hard rock como "Comunication Breakdown", de Led Zeppelin, "Paranoid" de Black Sabbath o "I´m Eighteen" de Alice Cooper.

A finales de los años 1960, una corriente de jóvenes del Reino Unido, Estados Unidos y otros países consideraban que el rock había pasado de ser un medio de expresión para los jóvenes, a una mera herramienta de mercado y escaparate para la grandilocuencia de los músicos de ese entonces, alejando la música de la gente común. El punk surgió como una burla a la rigidez de los convencionalismos que ocultaban formas de opresión social y cultural.

Las características del punk rock fueron precedidas por el garage rock (por ejemplo, "Pushin Too Hard" de 1966 de los californianos The Seeds), recrudeciendo más el sonido fuerte del rock y con composiciones menos profesionales, influenciados por el sonido de la invasión británica, como The Beatles, The Kinks o The Who, y cogiendo elementos del sonido ruidoso del "garage rock" de The Stooges o The Velvet Underground. También se recogen influencias del frenético surf rock. Estas variadas influencias se conocen ahora como proto-punk. Dentro de las mismas se puede incluir también entre las influencias tempranas a Bobby Fuller Four autor del sencillo "I Fought the Law", que fuera versionado por The Clash, y a Modern Lovers autores de "Roadrunner" dados sus ritmos acelerados, no tan cercanos al punk rock posterior pero si alejado del rock convencional y típico de la época, con acordes simples pero aún sin distorsión o volumen alto. The Dictators también fueron una banda crucial para el surgimiento del punk rock, empezaron como "garage rock" que posteriormente sería llamado "proto-punk" para luego emerger en el mismo punk rock que ellos mismos habían ayudado a formar. Dead Boys fue otra banda de punk estadounidense formada en 1976 y debutando en el mítico club CBGB.

Ramones hicieron composiciones sencillas, cuyas ácidas letras trataban temas como la discriminación de otros jóvenes, la anti-moda y las drogas, una pauta para las bandas por venir.

La visita de estos a Londres (actuaron en el mítico "Roundhouse" el 4 de julio, día de independencia estadounidense, paradójicamente) hizo que grupos ya existentes, como Sex Pistols, comenzaran a usar sus instrumentos como medios de expresión y provocación para mostrar su descontento hacia lo que consideraban una sociedad de mentalidad estrecha y represora.
The Damned es otra de las emblemáticas primeras bandas de punk rock de Inglaterra. A diferencia de The Clash, sus acordes de guitarra eran más trabajados, siendo considerados precursores del hardcore punk que se caracteriza precisamente, por tener canciones de punk con acordes más trabajados como los del thrash metal por ejemplo; su vocalista y líder Dave Vanian llevaba una estética vampírica que luego influyó a la estética de la subcultura gótica y a las bandas de rock gótico.

Blondie fue otra banda que sobresalió en la primera oleada del punk rock estadounidense. Empezaron sus conciertos en el CBGB y se iniciaron junto a otras bandas como Ramones y Television. Fueron de crucial importancia para lo que posteriormente sería el new wave, derivado del punk rock. 

Con el tiempo, el género tomaría diferentes caminos y en su paso evolucionaría en muchos subgéneros y recogería influencias de otros estilos musicales. Los subgéneros del punk se definen a veces por características musicales, y en otros casos por el contenido del mensaje o la ideología que lo inspira.

Tal como después sucedería en muchos otros países, en el Reino Unido pronto los grupos tomaron influencias de otros géneros. Una de las primeras fusiones del punk fue con el reggae y el ska de los emigrantes jamaicanos en el país. Como primer y más representativo ejemplo, se puede mencionar a la banda The Clash y sus temas "Police and Thieves" (una versión del jamaicano Junior Murvin), "The Guns of Brixton" y "(White Man) In Hammersmith Palais". 

Otra fusión en el punk fue con otro estilo negro, el funk, el que se puede oír en trabajos como "The Idiot" de Iggy Pop, y bandas como Ian Dury and The Blockheads, Gang Of Four, A Certain Ratio, como los mismos The Clash en su "Magnificient Seven".

En los inicios también surgió el estilo hardcore punk, que se caracterizó por ser una versión más rápida de la forma sucia de tocar el punk. The Damned fue la principal banda en influenciar a esta nueva rama del punk, seguidos por otras bandas como The Misfits, Bad Brains, Middle Class y Black Flag. Otra banda temprana en este estilo fueron Teen Idles.

El término inglés "punk" tiene un significado despectivo que suele variar, aplicándose a objetos (significando "basura", "suciedad") o a personas (significando "vago", "despreciable", "sucio" o, también, "basura" y "escoria"). Se utiliza de forma irónica como descripción del sustrato crítico o descontento que contiene esta música. Al utilizarlo como etiqueta propia, los "punkies" (o "punks") se desmarcan de la adecuación a los roles y estereotipos sociales. Debido al carácter de este significado, el punk a menudo se ha asociado a actitudes de descuido personal, se ha utilizado como medio de expresión de sentimientos de malestar y odio, y también ha dado cabida a comportamientos neuróticos o autodestructivos.

El término "punk" se utilizó como título de una revista fundada en 1976 en Nueva York por John Holmstrom, Ged Dunn y Legs McNeil que deseaban una revista que hablara de todo lo que les gustaba: las reposiciones por televisión, beber cerveza, el sexo, las hamburguesas con queso, los cómics, las películas de serie B, y el rock n' roll que sonaba en los garitos más mugrientos de la ciudad: The Velvet Underground, The Stooges y New York Dolls, entre otros.

Más tarde el significado también serviría para inspirar las corrientes izquierdistas del género, como etiqueta que deshace la condición de clase o rol social con deudas de reputación o apariencia.

Como movimiento creativo, el sonido punk dio aparición a numerosas vertientes. Muchos de los grupos se movían de uno a otro género, existiendo diferentes niveles de permeación, evolución y fusión, pudiendo hablarse de bandas que encajan en el perfil de más de dos subgéneros. Estos estilos individuales fueron popularizándose hasta formarse subgéneros musicales, categorías de obras y artistas que compartían un rasgo definido común.

Entre algunos de sus subgéneros se pueden identificar los siguientes:







</doc>
<doc id="2403" url="https://es.wikipedia.org/wiki?curid=2403" title="Provincias marítimas de Canadá">
Provincias marítimas de Canadá

Las llamadas Provincias Marítimas son una región de Canadá en la costa del océano Atlántico. Su economía tradicional tiene que ver con recursos naturales como la pesca, la agricultura y la minería.

Las provincias que componen esta región son:


</doc>
<doc id="2404" url="https://es.wikipedia.org/wiki?curid=2404" title="Palacio de los Guzmanes">
Palacio de los Guzmanes

El palacio de los Guzmanes es un palacio renacentista del siglo XVI situado en la plaza de San Marcelo junto a la Casa Botines en la ciudad de León, España. Su traza se debe al maestro Rodrigo Gil de Hontañón si bien se ocupó de su ejecución Juan de Ribero Rada. A pesar de quedar inconcluso se convirtió en el palacio más destacado de la ciudad. Ya en el siglo XX la Diputación Provincial de León se hizo cargo de terminarlo para adecuarlo a sus nuevas funciones, y actualmente alberga las oficinas de la Diputación.

Fue declarado Monumento histórico (BIC) en 1963.

Fue mandado construir por D. Ramiro Núñez de Guzmán, antiguo líder comunero, sobre los solares que ocupaban las casas señoriales de su linaje. La familia de los Guzmanes era uno de los linajes leoneses más antiguos. A principios del S. XVI eran señores de Guzmán, Aviados, Toral y Valle de Boñar. Asentados en la ciudad eran una de las casas nobles más influyentes y con mayor protagonismo en León donde estaban introducidos en los órganos de poder local.

D. Ramiro quiso aprovechar la situación de sus antiguas casas en una de las zonas principales de la ciudad para edificar un palacio de nueva planta que destacase y se diferenciase del entorno urbano por sus dimensiones y por adoptar la tipología y estética de la arquitectura “a lo romano” o renacentista. Para ello encargó el diseño de su traza a uno de los maestros más prestigiosos de Castilla en aquella época, Rodrigo Gil de Hontañón. Este diseñó un palacio rectangular con patio central, exento en sus cuatro costados, que precisaba estar enclavado entre calles alineadas de trazado regular y cuya fachada principal debía abrirse a la plaza existente para poder ser contemplado desde ella. En relación a su estructura y distribución interna el edificio tenía que conjugar el ámbito privado como residencia de la familia, con la esfera pública en la cual las partes nobles del palacio eran el reflejo de la categoría y nobleza de sus propietarios.

Inmediatamente el ambicioso proyecto tuvo que enfrentarse a las limitaciones existentes. Los solares disponibles eran de traza irregular al igual que las calles con las que limitaban, y su tamaño menor que el necesario para el nuevo edificio. Este necesitaba incorporar una zona ocupada por un tramo interior de la antigua muralla y por otras edificaciones. 

En el año 1559 el Consistorio leones autorizó el derribo de los cubos y lienzo de la muralla y la ocupación de ese terreno. Asimismo se le solicitó autorización para la ejecución de diversas obras encaminadas a conseguir la alineación de las calles. Este mismo año se iniciaron las obras. El encargado de su ejecución en calidad de aparejador fue el maestro Juan Ribero de Rada quien realizó aportaciones notables al diseño de Gil de Hontañón. 

En el año 1566 estaba levantada la fachada principal que da a la actual plaza de San Marcelo. En los años 1586 y 1587 se procedió a la adquisición y derribo de casas particulares para proseguir la obra y para ampliar el espacio de la plaza pública. Se buscaba que el palacio pudiese ser contemplado por entero desde ella como manifestación de la posición dominante que el linaje de los Guzmanes ocupaba en la ciudad. Con este fin se llegó a un acuerdo con el Consistorio para que esos terrenos quedasen en adelante libres de edificaciones y se incorporasen a la plaza existente.

A finales del siglo XVI se interrumpen las obras y el palacio queda incompleto. Se habían levantado dos de las cuatro alas, las que dan a la plaza y a la actual calle Ancha, y el patio central. A pesar de ello era la principal residencia de la ciudad y como tal hospedó en el año 1602, a Felipe III y a Margarita de Austria.

Pero en esta centuria el palacio dejará de estar habitado de forma regular al dejar de ser la residencia principal de la familia y comenzará su deterioro. Ya en los años 1654 y 1656 serán necesarias obras de reparación y reformas en los tejados, cornisas y en el patio entre otras. Sin uso continuado hubo que esperar al siglo XIX para que se empezase a limitar el proceso de decadencia en el que había entrado. En los años 40 de dicho siglo el Gobierno Provincial alquiló parte del edificio para instalar sus oficinas llevándose a cabo reparaciones parciales. Posteriormente en 1881 la Diputación Provincial de León compró el edificio a los propietarios de aquel entonces, los condes de Peñaranda de Bracamonte. 

Con su adquisición por parte del a Diputación leonesa se planteó una intervención en el edificio para adecuarlo a sus nuevas funciones y devolverle su primitivo aspecto ya que en los años 1840 el arquitecto Miguel Echano había desmochado las torres quitándoles una planta, cerrado los balcones del segundo piso de estas y apuntalado las ventanas angulares del tercero para garantizar su estabilidad.

A lo largo de los siguientes años se sucedieron los proyectos y las reformas marcados siempre por las restricciones económicas. Al igual que ocurría en esa época con la catedral leonesa, la restauración del palacio fue objeto de debate entre las escuela conservadora, que abogaba por que las intervenciones se limitasen a la recuperación de las zonas dañadas produciendo “alteraciones mínimas”, y la escuela restauradora que proponía recuperar y completar el palacio bajo el criterio de “unidad de estilo”, tal y como se suponía lo hubiese concluido Gil de Hontañón.

Las intervenciones más destacadas fueron las siguientes:





El palacio tiene planta trapezoidal articulada en torno a un patio interior y esta torreado en sus cuatro esquinas. Su fachada principal tiene un marcado desarrollo horizontal, es de tres alturas separadas por impostas, la inferior tiene ventanas enrejadas, el cuerpo central balcones de los cuales los cercanos a la portada y los situados en las esquinas se coronas con frontones triangulares y semicirculares, y el superior presenta una galería que recorre la fachada hasta las torres formada por arcos de medio punto separados por pilastras corintias. Sobre estas y sobresaliendo de la cornisa se disponen un conjunto de gárgolas.

Las torres tienen una altura más, la última reconstruida en la restauración de 1975 buscando devolverles el aspecto que tuvieron antes de ser desmochadas en 1840. Tres de ellas lucen ventanas angulares y la suroeste lleva adosada una escalera de caracol.

La fachada sur que da a la calle Ancha es de estilo más clasicista. Se relaciona con Juan del Ribero Rada al que también se le atribuyen las ventanas angulares de la torre sudeste decoradas con pilastras dóricas y columnas jónicas y corintias, y la portada abierta a la calle del Cid.

La portada principal se abre descentrada siguiendo la tradición medieval hispana. Su diseño es característico del estilo de Rodrigo Gil de Hontañón. Formada por un arco de medio punto, está enmarcada por columnas jónicas sobre las que se apoya el entablamento que sustenta un balcón rematado por un frontón triangular decorado. A sus lados siguiendo la vertical de las columnas, dos guerreros portan los escudos de armas de la familia.

Atravesando el zaguán se accede al patio columnado. Es de dos plantas, la baja formada por arcos escarzanos apoyados en columnas jónicas que presentan la particularidad de que sus capiteles se muestran de perfil. La superior tiene arcos carpaneles sobre columnas corintias. Entre estas los antepechos están labrados con los escudos de los Guzmanes. Los huecos se cubren con vidrieras. Rematan el conjunto gárgolas al igual que en la fachada. En cuanto a su autoría su atribución es dudosa descartándose que se deba a Gil de Hontañón.

En la zona sur del patio se sitúa la escalera claustral de tres tramos sobre bóvedas rampantes. Aunque su estructura es de la época de construcción del edificio lo que podemos ver debe su aspecto a las restauraciones de los siglos XIX y XX.
Respecto a las dependencias interiores han sido totalmente modificadas en las sucesivas obras llevadas a cabo en los pasados siglos. De los elementos originales ha sobrevivido la chimenea basada en modelos de Serlio que preside el salón principal. La decoración actual a base de cuadros, tapices y vidrieras de temas alegóricos de la historia leonesa corresponde a la etapa en que la diputación se ha hecho cargo del edificio.



</doc>
<doc id="2407" url="https://es.wikipedia.org/wiki?curid=2407" title="Piero della Francesca">
Piero della Francesca

Piero della Francesca ("Piero di Benedetto dei Franceschi"; llamado también "Pietro Borghese", Borgo del Santo Sepolcro, en el valle alto del Tíber, cerca de Arezzo, h. 1415 – Borgo del Santo Sepolcro, 12 de octubre de 1492) fue un pintor italiano del "Quattrocento" (siglo XV). Actualmente se le aprecia sobre todo como pintor especialista en frescos, pero en su época fue conocido también como un geómetra y matemático, maestro de la perspectiva y de la geometría euclidiana, temas en los que se concentró a partir del año 1470. Su pintura se caracterizó por su estilo sereno y el uso de las formas geométricas, particularmente en relación con la perspectiva y la luz. Es uno de los principales y fundamentales personajes del Renacimiento, aunque jamás trabajó para los Médicis y pasó poco tiempo en Florencia.

La reconstrucción biográfica de la vida de Piero es una empresa ardua a la que se han dedicado generaciones de estudiosos, confiando en los más débiles indicios, en la escasez general de documentos oficiales fiables que nos han llegado. Su propia obra ha llegado sólo de forma fragmentaria, con numerosas pérdidas de extrema importancia, entre las que destacan los frescos ejecutados para el Palacio Apostólico, sustituidos en el siglo XVI por los frescos de Rafael.

Piero nació en un año no precisado entre el 1406 y el 1420, en Sansepolcro, que Vasari llama «Borgo San Sepolcro», región de la Toscana. Este territorio fronterizo, a mediados del siglo XV, cambió en diversas ocasiones de soberanía: en un principio estaba en manos de Rímini, después fue de la República de Florencia y más tarde pasó a la posesión del Papado. La fecha de nacimiento se desconoce, porque un incendio en los archivos comunales de Sansepolcro destruyó las actas de nacimiento del registro civil. Un primer documento que menciona a Piero como testigo es un testamento datado el 8 de octubre de 1436, del cual se deduce que el artista debía tener ya al menos la edad prescrita de veinte años para un documento oficial. Según Giorgio Vasari en "Las vidas de los más excelentes arquitectos, pintores y escultores italianos desde Cimabue a nuestros tiempos", Piero, que murió en el año 1492, tenía 86 años en el momento de su muerte, lo que remontaría su fecha de nacimiento al año 1406 pero esta noticia se considera errónea, dado que sus padres se casaron en el año 1413.

Piero della Francesca procedía de una familia de mercaderes, de ahí que supiera matemáticas, cálculo, álgebra, geometría y contar con el ábaco. Su padre era el riquísimo comerciante de tejidos Benedetto de' Franceschi, y su madre Romana di Perino da Monterchi, noble de familia umbra. A esta aristocrática familia pertenecieron otros personajes famosos de la historia italiana; así, Francesco Franceschi (h. 1530-h.1599), importante editor literario y musical del Renacimiento; Angiolo Franceschi (1734 – 1806), arzobispo de Pisa y primado de Córcega y Cerdeña; y la escritora Caterina Franceschi Ferrucci (1803 – 1887), hija de Antonio Franceschi, médico y político, y de la condesa Maria Spada di Cesi.

Se ignora porqué, poco antes de su muerte, ya se le llamaba «della Francesca», en lugar de «di Benedetto» o «de' Franceschi», pero la conjetura de Vasari de que había tomado el apellido de su madre porque su marido murió cuando estaba ella embaraza y fue ella quien lo crio, no puede atenderse. Piero era el hijo primogénito de la pareja, que después tuvo otros cuatro hermanos (dos muertos a temprana edad) y una hermana.

Fue un artista itinerante, que trabajó en diversas localidades del centro y norte de Italia, en una actitud comparable a otros contemporáneos como la de Leon Battista Alberti.
Debió tener una primera educación dentro del negocio familiar, para después formarse como pintor, si bien no se sabe con seguridad cómo, aunque probablemente fue en el mismo Sansepulcro, ciudad de frontera cultural, entre las influencias florentinas, sienesas y aportes umbros. Pudo haber aprendido su arte de uno de los varios artistas sieneses que trabajaban en Sansepulcro durante su juventud. También se ha apuntado la posibilidad de una formación en Umbría, de donde le provendría el gusto por la pintura de paisajes y el uso de colores delicados. El primer artista con el que colaboró fue Antonio de Anghiari, socio de su padre en la fabricación de estandartes, activo y residente en Sansepulcro, como atestigua el 27 de mayo de 1430 un documento de pago a Piero por la pintura de estandartes y banderas con las insignias de la Comuna y del gobierno papal, puestos por encima de una puerta de las murallas. Con Antonio de Anghiari colaboraría entre el año 1432 y 1436. En 1438 está de nuevo documentado en Sansepolcro, donde se le menciona entre los otros ayudantes de Antonio de Anghiari, a quien se confió, en un primer momento, el encargo para el retablo de la iglesia de San Francisco (luego realizada por Sassetta). Saber si Piero se formó con Antonio como maestro es difícil de decir, dado que de este último no se conserva ninguna obra cierta.

En 1439 está documentado por primera vez en Florencia, donde quizá recibió su verdadera formación, puede ser que estuviera allí ya en torno al año 1435. Para entonces, Masaccio ya llevaba muerto una década. Estuvo de aprendiz con Domenico Veneziano, y se le cita el 7 de septiembre de 1439 entre sus ayudantes en un ciclo de frescos dedicados la "Vida de la Virgen" en el coro de San Gil (actualmente Santa María la Nuova), hoy perdidos. Conoció a Fra Angélico, gracias al cual tuvo acceso a la obra del difunto Masaccio y también a otros maestros de la época como Brunelleschi. La maestría del arte de la perspectiva, la pintura luminosa y la paleta clarísima y suntuosa de Domenico Veneziano influyeron en Piero, pero también la moderna y vigorosa de Masaccio, lo que dio forma a algunas de las características fundamentales de su obra posterior. Piero conoció las diversas soluciones que el Prerrenacimiento florentino daba a los problemas de la representación del cuerpo humano y de cómo reflejar el espacio tridimensional sobre una superficie bidimensional. Por un lado seguía vigente el linearismo y el lirismo de Fra Angélico, Benozzo Gozzoli o Filippo Lippi, y por otro, estaba el realismo geométrico de Paolo Uccello. Piero aprendió cómo lograr representar una luz atmosférica, añadiendo una gran proporción de aceite en las mezclas de color.

Probablemente ya había colaborado con Domenico en Perugia en 1437-1438 y, según Vasari, los dos trabajaron también en Loreto, en la iglesia de Santa María, esta obra la dejaron inacabada y la terminó Luca Signorelli.

La primera obra que se conserva es la "Virgen con Niño", actualmente en la florentina Colección Contini Bonacossi, atribuida por vez primera a Piero en el año 1942 por Roberto Longhi, que data en los años 1435-1440, cuando Piero aún trabajaba como colaborador de Domenico Veneziano. En la parte posterior de la tabla está pintado un vaso, como ejercicio de perspectiva.

Para el año 1442 Piero estaba de vuelta en Sansepolcro donde fue nombrado uno de los «consiglieri popolari» del consejo comunal. El 11 de enero de 1445 recibió de la Cofradía de la Misericordia local el encargo de un retablo para el altar de su iglesia: el contrato preveía la realización de la obra en tres años y su completa autografía, si bien se dilató a lo largo de los quince años siguientes y parte del mismo se debe a colaboradores de su taller. Todavía en el año 1462 la cofradía de Sansepolcro realizaba un pago a Marco di Benedetto de' Franceschi, hermano de Piero y su representante en su ausencia, a cuenta de este retablo. La parte más conocida de este retablo es la tabla central, posiblemente la última en pintarse, que representa a la "Virgen de la Misericordia". La cofradía le exigió que el fondo del retablo fuera dorado, rasgo arcaizante e inusual en Piero.

De esta primera época es muy posible que sea una de sus más famosas obras, el "Bautismo de Cristo", originariamente el panel central de un gran tríptico. Su datación es controvertida, hasta el punto de que algunos la consideran la primera obra de Piero. Algunos elementos iconográficos, como la presencia de dignatarios bizantinos en el fondo, hacen que se sitúe la obra en torno a 1439, año del Concilio de Basilea-Ferrara-Florencia en el que se reunificaron efímeramente las iglesias de Occidente y Oriente. Otros datan la obra más tarde, en torno al 1460.

Pronto fue solicitado por diversos príncipes. En los años cuarenta estuvo en varias cortes italianas: Urbino, Ferrara y probablemente Bolonia, realizando frescos que se han perdido por completo. En Ferrara trabajó entre el 1447 y el 1448 para Lionello de Este, marqués de Ferrara. En 1449 ejecutó varios frescos en el Castillo de los Este y la iglesia de San Andrés de Ferrara, también perdidos. Quizá tuviera aquí un primer contacto con la pintura flamenca, encontrando a Rogier van der Weyden directamente o a través de las obras que había dejado en la corte. Esta influencia flamenca es particularmente evidente si se piensa en su precoz uso de la pintura al óleo. Piero influyó en el posterior pintor ferrarés Cosme Tura.

El 18 de marzo de 1450 está documentado en Ancona, como testimonia el testamento (recuperado recientemente por Matteo Mazzalupi) de la viuda del conde Giovanni di messer Francesco Ferretti. En el documento el notario especifica que los testigos son todos «ciudadanos y habitantes de Ancona», por lo que Piero fue probablemente huésped por cierto tiempo de la importante familia anconetana y quizá para ellos pintase la tablilla de "San Jerónimo penitente", datada precisamente en 1450. De los mismos años procede el muy parecido "San Jerónimo y el donante Girolamo Amadi". En ambos se registra un interés por el paisaje y por la adecuada representación de los detalles, por las variaciones de los materiales y del "«lustro»" (esto es de reflejos de luces), que pueden ser explicadas sólo a través de un conocimiento directo de la pintura flamenca. Vasari recuerda también unos "Desposorios de la Virgen" sobre el altar de San José en la catedral, ya desaparecido en el 1821.

En 1451 fue a Rímini, llamado por Segismundo Pandolfo Malatesta. Entonces ejecutó, para el famoso Templo Malatestiano, su conocido fresco votivo monumental de "Pandolfo Malatesta a los pies de su santo patrón", del año 1451, en el que la escena se enmarca en un trampantojo. También hizo un retrato del condotiero. Aquí probablemente pudo conocer a otro famoso matemático y arquitecto del Renacimiento, Leon Battista Alberti.

En el año 1452, Piero della Francesca fue llamado a realizar, en sustitución de Bicci di Lorenzo la que acabaría conociéndose como su obra maestra y una de las más significativas del Renacimiento: los frescos de la basílica de San Francisco en Arezzo, dedicados a la "Leyenda de la Santa Cruz". Fue la familia Bacci, la más rica de Arezzo, la que decidió decorar el coro o capilla mayor de la iglesia dedicada a San Francisco. En el año 1447 contrataron para ello a Bicci di Lorenzo, de tradición tardogótica, pero sólo consiguió acabar el fresco de la bóveda, antes de fallecer. Contrataron entonces a Piero della Francesca para acabarlo, datándose su realización entre el año 1452 y 1466, aunque también se ha considerado como posible que acabase antes del año 1459. Es muy posible que trabajara en dos fases, una primera entre 1452 y 1458, y una segunda a su regreso de Roma. A finales del 1466 la cofradía aretina de la Anunciada le encargó un estandarte con la "Anunciación", citando en el contrato el éxito de los frescos de San Francisco como motivo del encargo por lo tanto, para aquella fecha, el ciclo tenía que estar acabado. En esta obra se pueden apreciar características que hacen de Piero un precursor del Alto Renacimiento, como la composición clara que emplea magistralmente la perspectiva geométrica, el tratamiento rico y novedoso de la luz (tomado de Domenico Veneziano) y su cromatismo admirable, delicado y claro.

La realización de la obra de Arezzo fue simultanea con la de otras obras y con su estancia en otras localidades. Así, en 1453, regresó a Sansepolcro donde, al año siguiente, firmó un contrato para un retablo con destino al altar mayor de la iglesia agustiniana, conocido como Retablo o Políptico de San Agustín. Trabajó en este proyecto desde 1454 y no se acabó hasta 1469, como evidencia el pago realizado, quizás el último, el 14 de noviembre de ese año. En estos paneles se pone en evidencia, nuevamente, su profundo interés en el estudio teórico de la perspectiva y su enfoque contemplativo. La obra es muy innovadora, careciendo de fondo de oro, sustituido por un cielo abierto entre balaustres clasicistas, y con las figuras de los santos de una linealidad y monumentalidad acentuadas. Actualmente sólo quedan cuatro paneles.

También estuvo en Roma, en al menos dos ocasiones. Una primera vez, llamado por el papa Nicolás V (m. 1455), en la que ejecutó frescos en la Basílica de Santa María la Mayor, de los que sólo quedan restos, en concreto un "San Lucas" pintado probablemente por su taller, mientras que nada se ha conservado de las obras enteramente autógrafas. La segunda vez, fue cuando lo llamó el papa Pío II, que acababa de ser elegido. Antes de marcharse de Sansepolcro, designó a su hermano Marco como representante, en previsión de una larga ausencia. Pío II le encargó pintar su habitación en el Palacio Apostólico; esta obra fue destruida en el siglo XVI para dejar sitio a la primera de las "Estancias Vaticanas" de Rafael. La tesorería papal emitió un documento, datado el 12 de abril de 1459 para el pago de 140 florines por «ciertas pinturas» en la «cámara de Su Santidad Nuestro Señor»

Otras obras de madurez son la "Virgen del parto" (1455-1465) y "La resurrección de Cristo" (1450-1463). La "Virgen del Parto" la realizó en tan sólo siete jornadas, para la capilla de la antigua iglesia de Santa María de Nomentana del cementerio de Monterchi, aldea vecina de Sansepolcro y de la que era originaria su madre. El modelo iconográfico, la Virgen del Parto, no era muy frecuente. Empleó materiales de alta calidad, como una cantidad considerable de azul marino que se obtenía a partir de lapislázuli importado. En esta obra puede apreciarse la obsesión de Piero por la simetría, que le lleva a colocar a dos ángeles idénticos, uno a cada lado de la Virgen, utilizando el mismo cartón. "La resurrección de Cristo", por su parte, es obra notable al utilizar diversas perspectivas. Fue pintada en Arezzo, cerca de su ciudad natal, al tiempo que trabajaba en los frescos de la "Leyenda de la Santa Cruz".

El 6 de noviembre de 1459 murió la madre de Piero y el 20 de febrero de 1464 su padre. En el año 1460 se encontraba en Sansepolcro, donde firmó y dató el fresco de "San Luis de Tolosa". Hay que recordar que en 1462 le hicieron un pago por el "Políptico de la Misericordia". En 1466 Piero pintó el fresco de una "Magdalena" en la Catedral de Arezzo, y le encargaron, como ya se señaló, el estandarte para la cofradía de la Anunciada, que entregó en Arezzo en el año 1468.
En el año 1467 en Perugia ejecutó por cuenta de las hermanas terciarias del convento de San Antonio un retablo, conocido como Políptico de San Antonio. Le encargaron una obra de inspiración tardogótica, pero en la parte superior tiene lo más destacado: la "Anunciación" del gablete es de clara estampa renacentista, mostrando su maestría con la perspectiva.

En 1468 está documentado en Bastia Umbra, donde se había refugiado para huir de la peste. Allí realizó al menos otro gonfalón pintado perdido.

Para el año 1469, acabados ya los frescos de Arezzo y el retablo de San Agustín, Piero se encontraba en Urbino, al servicio de Federico de Montefeltro. No están claros los períodos de estancia en Urbino, parece que con seguridad estuvo allí entre el 1469 y 1472, pero algunos autores retrasan su marcha hasta 1480. Fue una época en la que produjo cuadros de notable calidad. Piero está considerado como uno de los protagonistas y promotores del renacimiento en Urbino, y su propio estilo alcanza en esta ciudad un equilibrio no superado entre el uso de las rigurosas reglas geométricas y el aire serenamente monumental. En la corte de Urbino profundizó en el conocimiento de la pintura flamenca, tanto a través de la colección del duque como por la presencia de Justo de Gante, quien entre el año 1471 y el 1472 se asentó en Italia, primero en Roma pero luego, invitado por Federico de Montefeltro, en la corte de Urbino, donde estuvo hasta octubre del año 1475. No sería el único artista destacado a quien conoció en Urbino, pues allí entró en contacto también con Melozzo da Forlì y Luca Pacioli.
Aquí pintó el famoso retrato doble de Federico de Montefeltro y su esposa Battista Sforza (h. 1465-1472), hoy en la Galería de los Uffizi de Florencia, titulado "Triunfo de la Castidad". En él se aprecia la influencia de la pintura flamenca en el tratamiento del paisaje y en la minuciosidad y amor por el detalle.

En 1469 Piero está documentado en Urbino, donde la Cofradía del Corpus le encargó que pintase un estandarte procesional. En aquella ocasión al maestro se le propuso también la pintura del "Retablo del Corpus Domini", ya encargada a Fra Carnevale, luego a Paolo Uccello (1467), que pintó sólo la predela, y al final terminada por Justo de Gante en 1473-1474. En el año 1470 se documenta a Federico da Montefeltro en Sansepolcro, quizá en compañía de Piero.

A esta época de Urbino pertenece "La flagelación" (h. 1470, aunque otros lo datan en 1452), una de sus pinturas más conocidas. Al parecer, fue una creación personal que no dependió de encargo alguno y que pone de manifiesto que Piero era consciente de las innovaciones arquitectónicas de la época; es controvertido en cuanto a su significado exacto (véanse las Interpretaciones icónicas de este cuadro).

Se cree que fue en Urbino donde pintó la "Natividad" (1470-1485), que se encuentra actualmente en Londres. Es una de las últimas obras de Piero, cuando ya se estaba quedando ciego, creyéndose que por este motivo quedó inacabada, aunque su estado puede deberse también a las restauraciones de siglos pasados. Fue un encargo de su sobrino, con motivo de su matrimonio. Algunos críticos elaboran la hipótesis que el rostro de la Virgen fuera realizado por otra mano «flamenca». A este período se atribuye también la "Virgen con Niño y cuatro ángeles" del Instituto de Arte Clark en Williamstown, Massachusetts.

En el año 1473 se registra un pago, quizá aún del "Políptico de San Agustín". En 1474 le corresponde el último pago de una pintura perdida, destinada a la capilla de la Virgen de la abadía de Sansepolcro. Desde el 1 de julio de 1477 y hasta 1480 vivió, con algunas interrupciones, en Sansepolcro, donde formó parte regularmente del consejo comunal. En el año 1478 pintó un fresco perdido para la Capilla de la Misericordia, siempre en Sansepolcro. Entre 1480 y 1482 estuvo al frente de la Cofradía de San Bartolomé en su ciudad natal.

Piero della Francesca está documentado en Rímini el 22 de abril de 1482, donde alquiló «una mansión con un pozo». Aquí se dedicó a la escritura del "Libellus de quinque corporibus regularibus", terminado en el año 1485 y dedicado a Guidobaldo da Montefeltro. Otorgó testamento el 5 de julio de 1487, declarándose «sano de espíritu, de mente y de cuerpo». En sus últimos años, pintores como Perugino y Luca Signorelli visitaron frecuentemente su taller.

Aunque actualmente su obra matemática es poco menos que ignorada por completo, Piero fue, en vida, un matemático reputado. Según Giorgio Vasari, «…los artistas le otorgaron el título del mejor geómetra de sus tiempos, porque seguramente sus perspectivas tienen una modernidad, un mejor diseño y una mayor gracia que ninguna otra». Es Vasari también quien dice que en estos últimos años se vio afectado por una grave enfermedad de los ojos que le impidió trabajar. Por ello abandonó la pintura y se dedicó exclusivamente a su obra teórica, que escribió dictándola.

Murió en Sansepolcro, el mismo día en el que Cristóbal Colón pisó por vez primera América. Fue sepultado en la abadía de Sansepolcro, hoy el "Duomo".

Se conocen tres textos muy importantes escritos por Piero, de los más científicos del siglo XV: el "De prospectiva pingendi" («Sobre la perspectiva para la pintura»), "Libellus de quinque corporibus regularibus" («Librito de los cinco sólidos regulares») y un manual de cálculo titulado "Trattato dell’abaco" («Tratado del ábaco»).

Los temas tratados en estos escritos incluyen aritmética, álgebra, geometría y obras innovadoras tanto en geometría de los sólidos como perspectiva. En ellos se pone de manifiesto su contacto con Alberti. En estas tres obras matemáticas está presente una síntesis entre la geometría euclidiana, perteneciente a la escuela de los eruditos, y matemática con el ábaco, reservada a los técnicos.

La primera obra fue el "Libellus de quinque corporibus regularibus", un tratado dedicado a la geometría, que retomó temas antiguos de tradición platónico-pitagórica, estudiados siempre con la intención de que se puedan utilizar como elementos de diseño. Se inspira en las lecciones euclidianas para el orden lógico de las expresiones, para las referencias y el uso coordinado y complejo de los teoremas, mientras que se aproxima a las exigencias de los técnicos por la predictibilidad de las figuras tratadas, sólidas y poliédricas, y por la ausencia de demostraciones clásicas y por el uso de reglas aritméticas y algebraicas aplicadas a los cálculos. En el texto, en particular, por vez primera se dibujan los poliedros regulares y semiregulares, estudiando las relaciones que existen entre los cinco regulares.

En el segundo tratado, "De prospectiva pingendi" siguió en la misma línea de estudio, pero con notables novedades, hasta el punto de que se puede definir a Piero como uno de los padres del moderno dibujo técnico; de hecho, prefería la axonometría a la perspectiva, por considerarla más congruente con un modelo geométrico. Entre los problemas resueltos destaca el cálculo del volumen de la bóveda y la elaboración arquitectónica de las construcciones de las cúpulas.

El "Trattato d'abaco", sobre matemática aplicada (cálculo) fue escrito quizá ya en el año 1450, treinta años antes que el "Libellus". El título es de época moderna, porque el original carece de él. La parte geométrica y la algebraica resulta muy amplia en relación con las costumbres de su tiempo, así como la parte experimental sobre la que el autor ha explorado elementos no convencionales.

Gran parte de la obra de Piero fue posteriormente incluido en obras de otros, especialmente Luca Pacioli, un franciscano que era discípulo de Piero y a quien Vasari acusa directamente de copiar y plagiar a su maestro. La obra de Piero sobre geometría sólida aparece en la obra de Pacioli " De divina proportione (Divina Proporción)", un trabajo ilustrado por Leonardo da Vinci.

La crítica se encuentra dividida sobre la colaboración de varios artistas en su taller (entre otros Lorentino d'Arezzo, Luca Signorelli y el Perugino); por otro lado el único alumno que se ha documentado es Galeotto da Perugia. Entre sus colaboradores debe mencionarse a Giovanni da Piamonte, con el que trabajó en la ejecución de los frescos en San Francisco; es de dicho autor la tabla conservada cerca de la iglesia de Santa María de las Gracias de Città di Castello, en la que están seguramente presentes influencias de Piero della Francesca.

En vida fue muy famoso y su impacto se nota en las generaciones posteriores, aunque no fuera de pintores que trabajaran directamente con él. Dejó varios discípulos y seguidores: además de Luca Pacioli, Melozzo da Forli y Luca Signorelli.

Piero della Francesca es un pintor cuatrocentista, perteneciente a la segunda generación de pintores-renacentistas, intermedia entre Fra Angélico y Botticelli. Asumió los hallazgos de la primera escuela renacentista florentina de autores como Paolo Uccello, Masaccio y Domenico Veneziano. No viajó a Flandes, pero sí vio pintura flamenca, de manera que hizo una especie de simbiosis entre el Renacimiento italiano y la pintura flamenca.

Primó, como los otros grandes maestros de su tiempo, la creatividad. Trabajó técnicas nuevas, como el uso del lienzo como soporte pictórico y el óleo. Y también trató temas novedosos no sólo la omnipresente pintura religiosa, como el retrato y la representación de la Naturaleza. Tiene un estilo pictórico muy particular y por lo tanto es fácil de identificar. En su obra confluyen la perspectiva geométrica brunelleschiana, la plasticidad de Masaccio, la luz altísima que aclara las sombras y empapa los colores de Fra Angélico y Domenico Veneziano, así como la descripción precisa y atenta a la realidad de los flamencos. Otras características fundamentales de su expresión poética son la simplificación geométrica, tanto de la composición como de los volúmenes, la inmovilidad ceremonial de los gestos, la atención a la verdad humana.

Sus obras están admirablemente equilibradas entre el arte, la geometría y un complejo sistema de lectura a muchos niveles, donde se unen complejas cuestiones teológicas, filosóficas y de actualidad. Consiguió armonizar, tanto en su vida como en sus obras, los valores intelectuales y espirituales de su tiempo, condensando múltiples influencias y mediando entre la tradición y la modernidad, entre la religiosidad y las nuevas afirmaciones del Humanismo, entre la racionalidad y la estética.

Su actividad puede caracterizarse como un proceso que va de la práctica pictórica, a la matemática y a la especulación abstracta. Su producción artística, caracterizada por el extremo rigor de la búsqueda perspectivística, de la plástica monumentalidad de las figuras, del uso en función expresiva de la luz, influyó en el profundo la pintura renacentista de la Italia septentrional y, en particular, la escuela ferraresa y véneta.

Su obra se caracteriza por una dignidad clásica, similar a Masaccio. El término que mejor define su arte es el de «tranquilidad», lo que no impide que tenga un tratamiento técnico riguroso. Se percibe también la voluntad de construcción de un espacio racional y coherente. Piero se interesó mucho por los problemas del claroscuro y perspectiva, como su contemporáneo Melozzo da Forli. Piero della Francesca y Melozzo da Forlì son los más célebres maestros de la perspectiva del siglo XV, reconocidos como tales por Giorgio Vasari y Luca Pacioli. Destaca por sus conocimientos de perspectiva y composición, en lo que influyeron sus conocimientos matemáticos, fusionando el arte con la ciencia de la matemática, la geometría y la perspectiva. La perspectiva lineal era su característica principal a la hora de pintar, lo que se puede apreciar en todos sus cuadros, que se distinguen básicamente por sus coloridos luminosos y un suave pero firme trazo en las figuras. Sus composiciones son claras, equilibradas, reflejando con precisión matemática las arquitecturas. Sin ceder los efectos de trampantojo, Piero utilizó la perspectiva a fin de planificar las composiciones naturalistas grandiosas.

En estos paisajes serenos introducía las figuras de los personajes con un tratamiento muy volumétrico: se percibe un estudio anatómico, y una cierta monumentalidad. Ahora bien, son personajes muy estáticos, que permanecen como congelados y suspensos en sus propios movimientos, resultando un poco fríos, inexpresivos, monolíticos. Esta ausencia de nerviosismo es lo opuesto al resto de los pintores renacentistas de Florencia, que conforme avanzó el tiempo fueron haciendo figuras cada vez más dinámicas. Roberto Longhi, cuando habla de Piero della Francesca, dice que sus figuras son «columnas». El tratamiento de las figuras en volúmenes simples expresa un sentimiento de intemporalidad, lo mismo que la armonía de los tonos claros; todo ello expresa el sentido poético del arte de Piero della Francesca.

La luz atmosférica es otro de sus rasgos destacados, que adquirió de su maestro Domenico Veneziano, y que le servía para simbolizar la perfección de la Creación divina. Es muy diáfana, muy diurna, con un tratamiento uniforme, sin intensidades ni gradación lumínica (ligeramente arcaica, similar a la de Fra Angélico). Sus ensayos en este sentido llegan a dar la sensación de que sus figuras están modeladas en material dotado de luz propia, íntima, radiante. Los frescos como la "Leyenda de la Santa Cruz", en el ábside de la Iglesia de San Francisco, en Arezzo, son una obra de arte en luminosidad.

Lista de sus obras (pinturas sobre tabla y frescos) en orden cronológico.



Bohuslav Martinů escribió una obra en tres movimientos para gran orquesta titulado "Les Fresques de Piero della Francesca", H. 352 (1955). Dedicado a Rafael Kubelik, éste lo estrenó, junto con la Filarmónica de Viena en el Festival de Salzburgo del año 1956.






</doc>
<doc id="2417" url="https://es.wikipedia.org/wiki?curid=2417" title="Panacea universal">
Panacea universal

La panacea es un mítico medicamento que cura todas las enfermedades o, incluso, prolonga indefinidamente la vida. Fue buscada por los alquimistas durante siglos, especialmente en la Edad Media.

La palabra panacea proviene de la voz griega "panakos" y significa 'remedio para todo' ("pan": todo y "akos": remedio).



</doc>
<doc id="2421" url="https://es.wikipedia.org/wiki?curid=2421" title="Pársec">
Pársec

El pársec o parsec (símbolo pc) es una unidad de longitud utilizada en astronomía. Su nombre se deriva del inglés "parallax of one arc second" (paralaje de un segundo de arco o arcosegundo).

En sentido estricto "pársec" se define como la distancia a la que una unidad astronómica (UA) subtiende un ángulo de un segundo de arco (1"). En otras palabras, una estrella dista un pársec si su paralaje es igual a 1 segundo de arco entre el Sol y la Tierra.

De la definición resulta que:
Múltiplos del parsec:

La separación básica que usan los astrónomos para determinar el paralaje de las estrellas es el radio de la órbita de la Tierra. El paralaje se mide en segundos de arco (60 segundos de arco = 1 minuto de arco; 60 minutos de arco = 1 grado). Se basa en el método de el paralaje trigonométrica, el más antiguo y extendido para determinar la distancia a las estrellas.

Puesto que el pársec es una distancia relacionada con la unidad astronómica, se relaciona con la tangente del ángulo en P (ver el diagrama). Ahora bien, siendo β (léase "beta") un ángulo muy pequeño, del orden de hasta la milésima de segundo de arco, se comportará como una función lineal de proporcionalidad inversa respecto a Δ (léase "delta"). Es decir, a Δ doble, π se hace la mitad, pero si Δ es la mitad, π será el doble, y así sucesivamente, de tal forma que la relación entre distancia y paralaje se vuelve muy sencilla:
dónde formula_2 es la distancia en pársecs y formula_3 el paralaje en segundos de arco. Medida el paralaje de una estrella, no hay más que calcular su inversa para tener la distancia en pársecs.

Otra posibilidad es definir un pársec como la distancia a la que dos objetos, separados entre sí por una unidad astronómica, parecen estar separados por un ángulo de 1 segundo de arco.
Entonces:
El valor adoptado por la Unión Astronómica Internacional es: 1 pc = 3,0857 × 10 m.

Ejemplos de distancias en pársecs:

Los pársec se mencionan en diversas obras de ciencia ficción, como son libros, series de televisión y películas. En muchas de ellas, como son las novelas de Isaac Asimov o las series de televisión "Star Trek", se utiliza el término más o menos correctamente, sin embargo en ocasiones no es así.

En "La guerra de las galaxias" ("Episodio IV: Una nueva esperanza"), Han Solo se jacta de que su nave, el Halcón Milenario, es «la nave que corrió la carrera Kessel en menos de 12 pársecs». Esto es repetido en el primer capítulo de la tercera trilogía ("Episodio VII: El despertar de la Fuerza"), esta bajo la dirección de J. J. Abrams. Popularmente se cree que George Lucas utilizó el pársec como medida de tiempo y no de distancia, sin embargo está usada correctamente: la carrera consiste en encontrar el camino más corto y a eso se refiere la frase. Esto ha dado lugar a que se hagan parodias del supuesto "error", como por ejemplo en el episodio "Blue Harvest" de "Padre de Familia".

Sin embargo, el mismo Lucas, en el comentario de la película aparecida en DVD de 2004, aclara que el tiempo dado en unidades de distancia significa, en el universo de "La guerra de las galaxias", una referencia a la manera en que el ordenador de una nave calcula el camino a recorrer entre dos puntos del espacio; una distancia menor (en pársecs) quiere decir que el ordenador ha encontrado un camino que se puede recorrer en menos tiempo. Al parecer, el Halcón Milenario era muy eficiente en este sentido, gracias a las "mejoras" introducidas por Solo. Por otro lado, en el Episodio 2, de la misma saga, la senadora Padme utiliza el pársec como una unidad de distancia. (Nota: En la versión original doblada en España del episodio 4 se utilizó "parsec" como la unidad de velocidad del Halcón Milenario. En las siguientes versiones se tradujo la expresión por "parasegundo").

El planeta Melmac, en la serie "ALF", estaba localizado a seis pársecs más allá del supercúmulo Hydra-Centaurus o 19,56 años luz de allí.

En el videojuego para PC "Spore", al llegar al estado espacial, el pársec es utilizado para medir las distancias y como referencia para localizar un sistema en la galaxia utilizando el ángulo y la distancia al centro de la galaxia.

Es notable que el pársec se utiliza con naturalidad y universalidad en novelas de Asimov como las pertenecientes a las sagas del "Imperio galáctico" o "Fundación" donde el origen de la humanidad ha sido olvidado y es un tema central de discusión (a niveles científico y popular), y por lo tanto la Tierra y nuestro sol tienen tanta relevancia como todos los demás (o incluso menos).



</doc>
<doc id="2422" url="https://es.wikipedia.org/wiki?curid=2422" title="Plancton">
Plancton

Se denomina plancton (del griego πλαγκτός ["planctós"], ‘errantes’) al conjunto de organismos, principalmente microscópicos, que flotan en aguas saladas o dulces, más abundantes hasta los 200 metros de profundidad, aproximadamente. Se distingue del necton, palabra que denomina a todos los nadadores activos y del neuston, los que viven en la interfase o límite con el aire, es decir, en la superficie. Plancton (organismos que viven en suspensión en el agua), bentos (del fondo de ecosistemas acuáticos) y edafón (de la comunidad que habita los suelos).

Aunque tradicionalmente se ha subdividido el plancton en fitoplancton y zooplancton, según las clasificaciones más recientes esta distinción no parece apropiada, ya que entre los organismos autótrofos se incluyen los vegetales, algunos protistas y bacterias, y entre los heterótrofos están los animales, otros protistas y bacterias. No obstante, esta clasificación sigue utilizándose extensamente.

Se puede hacer una primera división entre holoplancton, que son aquellos organismos que pasan todo su ciclo vital perteneciendo al plancton y meroplancton, formado por organismos que sólo durante una parte de su vida integran la comunidad planctónica.

Constituido por todos los consumidores que constituyen en su gran mayoría a productores secundarios y terciarios. Este grupo está constituido por organismos generalmente microscópicos adultos y sus fases larvarias (holoplancton), y por las fases larvarias de otros organismos que en forma adulta habitan los fondos acuáticos o la columna de agua pero contrarrestando el movimiento de las corrientes. Algunos de los grupos de organismos más abundantes y característicos del zooplancton son los copépodos, cladóceros, rotíferos, cnidarios, quetognatos, eufáusidos y las larvas de los peces que por su relevancia socioeconómica de los organismos juveniles y adultos generalmente estudian y describen con el término “ictioplancton”. Al igual que el fitoplancton, dependiendo del ambiente en que se encuentren, ya sea dulceacuícola o marino, cada uno de los grupos o especies del zooplancton variará su diversidad y abundancia. Un componente del zooplancton relativamente menos estudiado son sus parásitos que constituyen una diversidad varios órdenes de magnitud mayor que los mismos organismos fitoplanctónicos y zooplanctónicos ya que cada organismos que existe en el planeta es propenso a infestarse o infectarse por múltiples parásitos.

El plancton vegetal, denominado fitoplancton, vocablo que deriva del griego φύτοπλαγκτον ["phytoplankton"] (φυτόν ["phyton"] significa planta), se desarrolla en las aguas costeras del mar con luz solar y sales minerales abundantes (aguas de hasta 30 m de profundidad), dado que elaboran su alimento por fotosíntesis.

Constituyen el alimento del zooplancton y producen el 50 % del oxígeno molecular necesario para la vida terrestre. Los organismos que más abundan en el fitoplancton son las cianobacterias y las diatomeas, unas algas doradas unicelulares. También encontramos a los dinoflagelados, responsables de las mareas rojas.

Base de la cadena trófica marina, el fitoplancton ha experimentado un significativo descenso debido al aumento de la radiación ultravioleta. Se ha observado que bajo el agujero de la capa de ozono en la Antártida la productividad del fitoplancton decreció entre el 6 % y el 12 %.

Diversos autores han realizado una clasificación del plancton por su tamaño, aunque es una división que puede considerarse “artificial”, pues en principio se basó en la luz de malla con la que se hacían las capturas, y no se ha llegado a un acuerdo definitivo. Una de las clasificaciones más utilizadas es la siguiente:

También se puede clasificar según su ubicación (horizontal o vertical)

El plancton vegetal está siempre cerca de la superficie del agua, pues necesita luz para realizar la fotosíntesis. En cambio el zooplancton está siempre en movimiento, de arriba hacia abajo, completando un ciclo diario con un recorrido de entre 100 a 500 metros, o más. Están casi siempre cerca de la superficie de noche para alimentarse, y más abajo durante el día para escapar de las fuertes radiaciones solares, aunque puede invertirse para algunos grupos.

La mayoría de las especies son transparentes con una cierta irisación, y presentan colores sólo al microscopio. Las especies superficiales son azuladas, y las otras rojizas. Algunas emiten luminiscencia, como la noctiluca.

La mayoría de las especies del plancton mide menos de un milímetro, otras, en cambio, son más grandes, como los sifonóforos, ctenóforos y medusas acalefas.

El fitoplancton es el alimento del zooplancton. Éste, sirve al mismo tiempo como alimento a equinodermos, crustáceos y peces en estado larvario. Estas larvas al crecer sirven como alimento a bancos de pequeños peces que a su vez alimentan a grandes planctívoros, como las ballenas o los tiburones ballena, y a peces mayores que alimentan, a veces, en varios eslabones sucesivos, a los grandes depredadores oceánicos, como son los cetáceos carnívoros, los tiburones, los atunes o los peces espada. En proporción, una tonelada de estos últimos habrá requerido, para su existencia y desarrollo, cinco mil toneladas de fitoplancton, como parte de lo que se denomina cadena trófica.

Conocidas normalmente como “mareas rojas” son las proliferaciones de dinoflagelados (fitoplancton) que crecen exponencialmente debido a las condiciones favorables para su desarrollo (temperaturas, calidad y cantidad de luz, nutrientes y pasividad de la columna de agua) . Su reproducción no para hasta que las condiciones sean desfavorables. Muchas veces estas floraciones algales pasan desapercibidas, mas es posible que la floración sea de algún tipo de fitoplancton tóxico, como "Alexandrium catenella", que provoca la muerte en vertebrados como los humanos. No todas las floraciones tornan el agua del color rojo que le da el nombre a este fenómeno, paradójicamente las floraciones más nocivas son incoloras, por lo que los expertos suelen referirse a ellas como “Floraciones algales nocivas” (o FAN).

Por analogía con el plancton, se denomina plancton aéreo al conjunto de invertebrados que hay en el aire y sirven de alimento a aves como las golondrinas o los vencejos.




</doc>
<doc id="2423" url="https://es.wikipedia.org/wiki?curid=2423" title="Pirekua">
Pirekua

La pirekua es uno de los géneros musicales propios del pueblo purépecha, del estado de Michoacán, originada del sincretismo de la música y cantos religiosos de los evangelizadores españoles con las reminiscencias de la música indígena (sonecitos de la Tierra y sones del Costumbre). Actualmente representa un medio de expresión de la lengua purépecha y constituyen una manera de exaltar su conciencia étnica a fin de salvaguardar la pindekua (tradición y costumbre). La pirekua, en sus ritmos abajeño (6/8) y son valseado (3/4), transmite mensajes de amor, de desamor, de la historia purépecha y de Michoacán, de la geografía michoacana y de la vida social de la comunidad.

La palabra "pirekua", en lengua p'urhépecha o purépecha, significa "canción", y por lo general las pirekuas se cantan en esta lengua nativa o en castellano, algunas incluso intercalan ambos idiomas y otras tantas se interpretan instrumentalmente. Se destaca por su carácter noble, nostálgico y sentimental. Algunas de las regiones donde se cultiva la pirekua son el Quinceo, Zacán, San Lorenzo, Comachuen, Nurio, Cherán, Ichan, Angahuan, Pátzcuaro y otras comunidades. Aunque este género no es muy conocido en otras regiones del país, en Michoacán es una parte de la cultura de los purépechas, incorrectamente llamados tarascos por sus enemigos aztecas y más tarde por los conquistadores españoles.

Habitualmente se canta una o dos voces masculinas, aunque pueden ser femeninas o mixtas y acompañadas por guitarra sexta, contrabajo, vihuela o arpa y violín. A diferencia del son abajeño purépecha, se utilizan pocos instrumentos (a menudo uno, dos o hasta tres), pero en algunas ocasiones se utiliza la misma instrumentación, sobre todo cuando un grupo se dedica a interpretar ambos géneros tradicionales.

Algunas pirekuas tradicionales:




</doc>
<doc id="2426" url="https://es.wikipedia.org/wiki?curid=2426" title="Pandora">
Pandora

En la mitología griega, "Pandora" (en griego antiguo: Πανδώρα) fue la primera mujer, 
hecha por Hefesto debido a una orden de Zeus, después de que Prometeo, yendo en contra de su voluntad, le otorgara el don del fuego a la humanidad. 

Según la versión del poeta Hesíodo, la creación de la primera mujer está ligada estrechamente con el incidente de Mecona. Cuando los mortales e inmortales se separaron, Prometeo urdió un engaño para que, en adelante, cuando los hombres sacrificaran para los dioses, solo les reservaran los huesos y pudieran aprovechar para sí mismos la carne y las vísceras. Zeus, irritado por el ardid, les negó el fuego a los hombres, pero Prometeo, hurtándolo, se los restituyó ("Teog." 535-570; "Trabajos y días", 47-58).

Zeus ordenó que Hefesto modelara una imagen con arcilla, con figura de encantadora doncella, semejante en belleza a las inmortales, y le infundiera vida. Pero, mientras que a Afrodita le mandó otorgarle gracia y sensualidad, y a Atenea concederle el dominio de las artes relacionadas con el telar y adornarla, junto a las Gracias y las Horas con diversos atavíos, a Hermes le encargó sembrar en su ánimo mentiras, seducción y un carácter inconstante. Ello, con el fin de configurar un "bello mal", un don tal que los hombres se alegren al recibirlo, aceptando en realidad un sinnúmero de desgracias.

Los poemas presentan de distinta forma la introducción de los males por Pandora. En "Teogonía", el poeta la presenta como la primera de entre las mujeres, que en sí mismas traen el mal: en adelante, el hombre debe optar por huir del matrimonio, a cambio de una vida sin carencias materiales, pero sin descendencia que lo cuide y que mantenga después de su muerte su hacienda; o bien casarse, y vivir constantemente en la penuria, corriendo el riesgo incluso de encontrar a una mujer desvergonzada, mal sin remedio ("Teog." 602-612).

En "Trabajos y días", Hesíodo indica que los hombres habían vivido hasta entonces libres de fatigas y enfermedades, pero Pandora abrió un ánfora que contenía todos los males (la expresión «caja de Pandora» en lugar de jarra o ánfora es una deformación renacentista) liberando todas las desgracias humanas. El ánfora se cerró justo antes de que la esperanza fuera liberada ("Trabajos y días" 90-105).

En esta última versión es cuando se menciona por primera vez el nombre de "Pandora", y su vínculo con Epimeteo: Prometeo le había advertido no aceptar ningún regalo de Zeus, de lo contrario les sobrevendría una gran desgracia a los mortales, tras un rechazo inicial que enfureció a Zeus este encadena a Prometeo en las montañas del Cáucaso. Epimeteo termina casándose con Pandora, dándose cuenta muy tarde de la astucia del padre de los dioses ("Los trabajos y los días" 83-89).

Otras versiones del mito relatan que en realidad la jarra contenía bienes y no males. La apertura de la jarra ocasionó que los bienes volaran regresando a las mansiones de los dioses, sustrayéndose de la vida de los hombres, que en adelante solo viven afligidos por males. Lo único que pudieron conservar de aquellos bienes es la esperanza.

La "Biblioteca mitológica" (I, VII, 2) menciona que Epimeteo y Pandora fueron padres de Pirra, esposa de Deucalión, hijo de Prometeo. Deucalión y Pirra son considerados por el mito como antepasados de la mayor parte de los pueblos de Grecia antigua.

Etimológicamente se ha dado a la palabra «Pandora» un significado con distintos matices: Paul Mazon y Willem Jacob Verdenius la han interpretado como "el regalo de todos"; sin embargo, para Robert Graves significa "la que da todo" e indica que con ese nombre (Pandora) se adoraba en Atenas y otros lugares a Rea. Según Graves, se estaría ante la precursora griega de la Eva bíblica, puesto que Pandora es quien, como aquella, trae la desgracia a la humanidad.

Para Jean-Pierre Vernant, el rol de mito de Pandora en el texto hesiódico (sobre todo referido a "Trabajos y días") es el de la justificación teológica de la presencia de fuerzas oscuras en el mundo humano. Al intentar Prometeo obtener para los hombres más de lo que debían recibir, arrastra a la humanidad a la desgracia: Zeus le da a los mortales un don ambiguo, mezcla de bien y mal, una peste difícil de tolerar pero de la que no se puede prescindir. Es el engaño mismo disfrazado de amante. Pandora es la responsable de comunicar al mundo humano los poderes representados por la estirpe de la Nyx: de ahora en adelante, toda abundancia convive con Ponos, a la juventud sigue Geras, y la justicia contrasta con Eris. La aparición de la mujer implica también la necesidad de un constante afán en las labores agrícolas, puesto que es presentada constantemente como un vientre hambriento, atenta a la hacienda de su prometido, al que acecha con encantos seductores (Apate), y una vez casada instala el hambre en el hogar.







</doc>
<doc id="2427" url="https://es.wikipedia.org/wiki?curid=2427" title="Prometeo">
Prometeo

En la mitología griega, Prometeo (en griego antiguo Προμηθεύς, ‘previsión’, ‘prospección’) es el Titán amigo de los mortales, honrado principalmente por robar el fuego de los dioses en el tallo de una cañaheja, darlo a los hombres para su uso y posteriormente ser castigado por Zeus por este motivo.

Como introductor del fuego e inventor del sacrificio, Prometeo es considerado el Titán protector de la civilización humana. 

En Atenas, se había dedicado un altar a Prometeo en la Academia de Platón. Desde allí partía una carrera de antorchas celebrada en su honor por la ciudad, en la que ganaba el primero que alcanzaba la meta con la antorcha encendida.

Prometeo era hijo de Jápeto y la oceánide Asia o de la también oceánide Clímene. Era hermano de Atlas, Epimeteo y Menecio, a los que superaba en astucia y engaños. No tenía miedo alguno a los dioses, y ridiculizó a Zeus y a su poca perspicacia. Sin embargo, Esquilo afirmaba en su "Prometeo encadenado" que era hijo de Gea o Temis. Según una versión minoritaria, el gigante Eurimedonte violó a Hera cuando esta era una niña y engendró a Prometeo, lo que causó la furia de Zeus.

Prometeo fue un gran benefactor de la humanidad. Urdió un primer engaño contra Zeus al realizar el sacrificio de un gran buey que dividió a continuación en dos partes: en una de ellas puso la piel, la carne y las vísceras, que ocultó en el vientre del buey y en la otra puso los huesos pero los cubrió de apetitosa grasa. Dejó entonces elegir a Zeus la parte que comerían los dioses. Zeus eligió la capa de grasa y se llenó de cólera cuando vio que en realidad había escogido los huesos. Desde entonces los hombres queman en los sacrificios los huesos para ofrecerlos a los dioses, y comen la carne.

Indignado por este engaño, Zeus prohibió a los hombres el fuego. Prometeo decidió robarlo, así que subió al monte Olimpo y lo cogió del carro de Helios o de la forja de Hefesto, y lo consiguió devolver a los hombres en el tallo de una cañaheja, que arde lentamente y resulta muy apropiado para este fin. De esta forma la humanidad pudo calentarse. 

En otras versiones (notablemente, el "Protágoras" de Platón), Prometeo robaba las artes de Hefesto y Atenea, se llevaba también el fuego porque sin él no servían para nada, y proporcionaba de esta forma al hombre los medios con los que ganarse la vida.

Para vengarse por esta segunda ofensa, Zeus ordenó a Hefesto que hiciese una mujer de arcilla llamada Pandora. Zeus le infundió vida y la envió por medio de Hermes al hermano de Prometeo: Epimeteo, en cuya casa se encontraba la jarra que contenía todas las desgracias (plagas, dolor, pobreza, crimen, etcétera) con las que Zeus quería castigar a la humanidad. Epimeteo se casó con ella para aplacar la ira de Zeus por haberla rechazado una primera vez (a causa de las advertencias de su hermano de no aceptar ningún regalo de los dioses; en castigo Prometeo sería encadenado). Pandora terminaría abriendo el ánfora, tal y como Zeus había previsto.
Tras vengarse así de la humanidad, Zeus se vengó también de Prometeo e hizo que lo llevaran al Cáucaso, donde fue encadenado por Hefesto con la ayuda de Bía y Kratos. Zeus envió un águila (hija de los monstruos Tifón y Equidna) para que se comiera el hígado de Prometeo. Siendo éste inmortal, su hígado volvía a crecer cada noche, y el águila volvía a comérselo cada día. Este castigo había de durar para siempre, pero Heracles pasó por el lugar de cautiverio de Prometeo de camino al jardín de las Hespérides y lo liberó disparando una flecha al águila. Esta vez no le importó a Zeus que Prometeo evitase de nuevo su castigo, ya que este acto de liberación y misericordia ayudaba a la glorificación del mito de Heracles, quien era hijo de Zeus. Prometeo fue así liberado, aunque debía llevar con él un anillo unido a un trozo de la roca a la que fue encadenado. 

Agradecido, Prometeo reveló a Heracles el modo de obtener las manzanas doradas de las Hespérides.

Sin embargo, en otra versión Prometeo fue liberado por Hefesto tras revelar a Zeus el destino de que si tenía un hijo con la nereida Tetis, este hijo llegaría a ser más poderoso que su padre, quien quiera que éste fuera. Por ello Zeus evitó tener a Tetis como consorte y el hijo que tuvo ésta con Peleo fue Aquiles quien, tal y como decía la profecía, llegó a ser más poderoso que su padre.

La "Biblioteca mitológica" recoge una versión según la cual Prometeo fue el creador de los hombres, modelándolos con barro. Prometeo se ofreció ante Zeus para cambiar su mortalidad por la inmortalidad de Quirón cuando éste fue herido accidentalmente por Heracles, lo que le produjo una herida incurable.

En la mitografía, se ha relacionado a Prometeo con Loki, de la mitología nórdica, quien análogamente es un gigante más que un dios, está asociado con el fuego y es castigado a ser encadenado a una roca y atormentado por un águila.

La historia de Prometeo ha inspirado a muchos autores a lo largo de la historia para referirse a la osadía de los hombres de hacer o poseer las cosas divinas, y los románticos vieron en él un prototipo del demon o genio natural. Algunas de las obras de dichos autores son:











</doc>
<doc id="2429" url="https://es.wikipedia.org/wiki?curid=2429" title="Papa (desambiguación)">
Papa (desambiguación)

Papa hace referencia a varios artículos:

Varias especies de plantas comestibles originarias de América del Sur:




</doc>
<doc id="2442" url="https://es.wikipedia.org/wiki?curid=2442" title="Quake (videojuego)">
Quake (videojuego)

Quake es un videojuego de disparos en primera persona publicado por id Software el 31 de mayo de 1996. Introdujo algunos de los mayores avances en el género de los videojuegos en 3D: utiliza modelos tridimensionales para los jugadores y los monstruos en vez de "sprites" bidimensionales; y el mundo donde el juego tiene lugar está creado como un verdadero espacio tridimensional, en vez de ser un mapa bidimensional con información sobre la altura representada en tres dimensiones. También incorporó la utilización de los mapas de luz y las fuentes de luz en tiempo real, descartando la iluminación estática basada en sectores de los juegos anteriores. Ofreció, en su tiempo, uno de los motores físicos más realistas programados para un videojuego hasta la fecha. Muchos creen que proporcionó la plataforma para la revolución de las tarjetas gráficas 3D independientes, «GLQuake» fue la primera aplicación que, en esos días, demostró la capacidad verdadera del chipset Voodoo Graphics de 3DFX. El impacto del motor Quake engine puede aún sentirse en nuestros días.

La mayoría de la programación del motor del Quake fue realizada por John Carmack. Michael Abrash, un especialista en optimización del rendimiento de los programas, fue contratado para ayudar a que el motor de representación por software fuera posible teniendo en cuenta la velocidad. La banda sonora y efectos de sonido fue compuesta por Trent Reznor.

Quake y sus secuelas "Quake II" y "Quake III Arena" han vendido más de 4 millones de copias juntos.

En un futuro no lejano, científicos militares dan sus primeros pasos en la tecnología de la teletransportación, creando los Slipgates (Portales-Pasadizos), artefactos que permiten materializar organismos desde y hasta puntos espaciales distantes. Sin que ellos pudieran prevenirlo, estos aparatos eran capaces —además— de unir dimensiones, produciendo que una de estas instalaciones construyera el puente entre este universo y una realidad plagada de demonios (alusiva al infierno), iniciándose una invasión a gran escala que amenaza con arrasar el planeta. Pronto, los soldados de las bases militares terrestres caen derrotados ante las gigantescas hordas.

En este punto, y sin una introducción gráfica, el jugador toma el rol del último superviviente de un batallón, un anónimo soldado (posteriormente llamado Ranger, en reseña a las fuerzas especiales militares) quien cruza el primer portal e inicia la odisea de derrotar al mayor de estos demonios, un enemigo sumamente poderoso sólo conocido por el nombre clave “Quake” (que posteriormente se revelará como Shub-Niggurath, oscura diosa pagana llena de maldad).

Armado sólo con una escopeta y un hacha, este soldado iniciará numerosos viajes interdimensionales entre nuestro mundo y el demoníaco, buscando las salidas de los lugares en que aparezca teletransportado, a fin de acercarse cada vez más a Shub-Niggurath y cumplir su misión de exterminarlo.

Cabe destacar que esta línea argumental —similar a la utilizada para el videojuego Doom— fue desechada posteriormente por ID Software para la realización de las secuelas de la serie, particularmente Quake II y Quake IV, cambiando radicalmente la historia hacia una trama no paranormal, sino alienígena. Entre los fanáticos de Quake, aún se ven solicitudes a esta empresa de unir ambas historias en algún punto, o bien desarrollar una secuela que siga la campaña del juego original, esta vez con un motor gráfico de última generación.














Enemigos no incluidos en la versión final del juego.

"El 24 de febrero de 1996, ID Software presentó" Qtest, "la primera prueba pública que exhibiría la tecnología utilizada en el videojuego. En dicho lanzamiento fueron introducidos tres monstruos que, en definitiva, no vieron la luz en la edición final de Quake. Sin embargo, sus modelos y scripts han sido utilizados en las numerosas modificaciones que los aficionados han efectuado al juego mismo".




Las armas en "Quake" (excepto el hacha) requieren de munición, la cual se encuentra dispersa por todo el nivel de juego, en ocasiones ubicada en lugares secretos. A diferencia de muchos juegos FPS posteriores a Quake, el jugador no necesita recargar su arma, la cual puede disparar en forma continua hasta agotar la munición respectiva. Algunos enemigos son particularmente débiles frente a un determinado tipo de arma, y otros son resistentes a la misma, lo cual entrega un toque de estrategia para el jugador.









En cada mapa puede encontrarse distinta maquinaria que el jugador usará a fin de encontrar la salida de cada nivel, o bien elementos ambientales con que debe interactuar. Muchos de estos objetos, a pesar de ser considerados además en el videojuego Doom, formaron un estándar a seguir por los futuros FPS, incluso hasta el día de hoy. Los más recurrentes son:













"Quake" incluye un modo multijugador para jugar a través de una red de área local o de Internet con o contra otros humanos. El juego en red utiliza un modelo cliente/servidor, donde el juego actual sólo se ejecuta en el servidor y todos los jugadores «acceden» a él para participar. Dependiendo de la ruta específica que el cliente tenga al servidor los diferentes clientes obtendrán diferentes tiempos de ping. Cuanto más baja sea la latencia (tiempo de ping), más suavemente será tus movimientos dentro del juego, y más fácil será apuntar correctamente y puntuar. Alguien jugando en el PC servidor obtiene una ventaja sustancial debido al lag prácticamente nulo.

Hay dos paquetes oficiales de expansión lanzados para Quake.

Los paquetes de expansión continuan donde el primer juego termino, incluyen todas las mismas armas, power-ups, monstruos y arquitectura / atmósfera gótica, y continúan / terminan la historia del primer juego y su protagonista. Un tercer paquete no oficial de expansión, Abyss of Pandemonium, fue desarrollado por Impel Development Team, publicado por Perfect Publishing, y publicado el 14 de abril de 1998; Una versión actualizada, versión 2.0, titulada Abyss of Pandemonium - The Final Mission fue lanzado como freeware. En honor del quinto aniversario de Quake, MachineGames, un estudio de ZeniMax Media, propietarios actuales de Quake ,lanzó en línea un nuevo paquete de expansión gratuito llamado Episode 5: Dimensión of the past.

"Quake Mission Pack No. 1: Scourge of Armagon" es el primer mission pack, lanzado el 28 de febrero de 1997. Desarrollado por Hipnotic Interactive, presenta tres episodios divididos en diecisiete nuevos niveles de un solo jugador (tres de los cuales son secretos), un episodio Nuevo nivel multijugador, una nueva banda sonora compuesta por Jeehun Hwang, y características de juego no presentes originalmente en Quake, incluyendo estructuras giratorias y paredes rompibles. A diferencia del juego Quake principal y Mission Pack No. 2, Scourge elimina el cubo del episodio, requiriendo que los tres episodios se reproduzcan secuencialmente. Los tres nuevos enemigos que se incluyen son los Centroides, grandes escorpiones cibernéticos con pistolas de clavos; Gremlins, pequeños goblins que pueden robar armas y multiplicarse alimentándose de cadáveres enemigos; Y Spike Mines, orbes flotantes que detonan cuando están cerca del jugador. Las tres nuevas armas incluyen el Mjolnir, un gran martillo emisor de relámpagos; El cañón láser, que dispara rebotes de energía; Y el lanzador de minas de proximidad, que dispara granadas que se adhieren a las superficies y detonar cuando un oponente se acerca. Los tres nuevos power-ups incluyen el Cuerno de Conjuración, que convoca a un enemigo para proteger al jugador; El Escudo de la Empatía, que reduce a la mitad el daño recibido por el jugador entre el jugador y el enemigo atacante; Y el Wetsuit, que hace que el jugador invulnerable a la electricidad y permite al jugador permanecer bajo el agua durante un período de tiempo mayor. 

"Quake Mission Pack No. 2: Dissolution of Eternity" es el segundo mission pack, lanzado el 31 de marzo 1997. Desarrollado por Rogue Entertainment, presenta dos episodios divididos en quince nuevos niveles de un solo jugador, un nuevo nivel multijugador, una nueva banda sonora , Y varios nuevos enemigos y jefes. En particular, el paquete carece de niveles secretos. Los ocho nuevos enemigos incluyen las anguilas eléctricas, los espadachines fantasma, los ogros de la Multi-Granada, el infierno Spawn, los Wraths, los guardianes (guerreros egipcios egipcios resucitados), las momias y las estatuas de varios enemigos que pueden venir a la vida. Los cuatro nuevos tipos de jefes incluyen a hombres de lava, Overlords, Wraths grandes, y un dragón que guarda el "convertidor de energía temporal". Los dos nuevos power-ups incluyen el Anti Grav Belt, que permite al jugador saltar más alto; Y el Power Shield, que reduce el daño que el jugador recibe. En lugar de ofrecer nuevas armas, el paquete de misión le da al jugador cuatro nuevos tipos de munición para las armas existentes, como "clavos de lava" para el Nailgun, cluster grenades para el lanzador de granadas, cohetes que se dividen en cuatro en una línea horizontal para el cohete Lanzador, y las células plasmáticas para el rayo, así como un gancho para ayudar con moverse alrededor de los niveles..

A finales de 1996, id Software lanzó VQuake, un port official del motor Quake para soportar la renderización acelerada de hardware en tarjetas gráficas usando el chipset Rendition Vérité. Aparte del beneficio esperado de un rendimiento mejorado, VQuake ofreció numerosas mejoras visuales sobre el original Quake. Cuenta con un completo color de 16 bits, filtrado bilineal (reducción de la pixelación), iluminación dinámica mejorada, anti-aliasing opcional y claridad de código fuente mejorada,Como su nombre implicaba, VQuake era un puerto propietario específicamente para el chipset Vérité; La aceleración 3D del consumidor estaba en su infancia en ese momento, y no había una API 3D estándar para el mercado de consumo. Después de completar VQuake, John Carmack prometió nunca volver a escribir un puerto propietario, citando su frustración con la API Speedy3D de Rendition.

Ir al Articulo: QuakeWorld

Para mejorar la calidad del juego en línea, Id Software lanzó QuakeWorld el 17 de diciembre de 1996, una versión de Quake que incluía un código de red ampliamente mejorado, incluyendo la adición de la predicción del lado del cliente. El código de red de Quake original no mostraría al jugador los resultados de sus acciones hasta que el servidor enviara una respuesta reconociéndolas. Por ejemplo, si el jugador intentaba avanzar, su cliente enviaría la solicitud para avanzar al servidor y el servidor determinaría si el cliente era realmente capaz de avanzar o si se topaba con un obstáculo, como una pared O de otro jugador. El servidor respondería entonces al cliente, y sólo entonces el cliente mostraría el movimiento al reproductor. Esto estaba bien para jugar en una LAN, con un ancho de banda alto, pero la latencia sobre una conexión a Internet de acceso telefónico es mucho mayor que en una LAN, y esto causó un retraso notable entre cuando un jugador trataba de actuar y Cuando esa acción era visible en la pantalla. Esto hizo que la jugabilidad fuera mucho más difícil, especialmente porque la naturaleza impredecible de Internet hacía que la cantidad de retraso variaba de un momento a otro.

El 22 de enero de 1997, id Software lanzó GLQuake. Esto fue diseñado para usar la API OpenGL 3D para acceder a las tarjetas de aceleración de gráficos 3D de hardware para rasterizar los gráficos(Modo Software), en lugar de tener la CPU de la computadora en cada píxel. Además de velocidades de fotogramas más altas para la mayoría de los jugadores, GLQuake proporciona modos de resolución más alta y filtrado de texturas. GLQuake también experimentó con reflejos, agua transparente e incluso sombras rudimentarias. GLQuake viene con un controlador que permite que el subconjunto de OpenGL utilizado por el juego funcione en la tarjeta gráfica 3Dfx Voodoo, la única tarjeta de nivel de consumidor en el momento capaz de ejecutar GLQuake. Anteriormente, John Carmack había experimentado con una versión de Quake específicamente escrita para el chip Rendition Vérité utilizado en la tarjeta PCI 3D Blaster de Creative Labs. Esta versión se había reunido con sólo un éxito limitado, y Carmack decidió escribir para APIs genéricos en el futuro en lugar de la adaptación de hardware específico.

El 11 de marzo de 1997, id Software lanzó WinQuake, una versión del motor que no utilizaba OpenGL (sino que utilizaba el modo Software) diseñado para funcionar bajo Microsoft Windows; El Quake original se había escrito para el DOS, permitiendo el lanzamiento de Windows 95, pero no podía funcionar bajo sistemas operativos basados Windows NT porque requería el acceso directo al hardware. WinQuake accedió a hardware a través de APIs basadas en Win32 como DirectSound, DirectInput y DirectDraw que se admitían en Windows 95, Windows NT 4.0 y versiones posteriores. Al igual que GLQuake, WinQuake también permite modos de video de mayor resolución. Esto eliminó la última barrera a la popularidad generalizada del juego. En 1998, LBE Systems y Laser-Tron lanzaron Quake: Arcade Tournament Edition en las arcades en cantidades limitadas

El 24 de junio de 2016, para celebrar el 20 aniversario de Quake, MachineGames desarrolló un paquete de misiones. Cuenta con 10 nuevos niveles de un solo jugador y un nuevo nivel multijugador, pero no utiliza nuevas incorporaciones de Scourge of Armagon y Dissolution of Eternity. Considerado el episodio que tiene lugar entre el juego principal y la expansiónes

Después de la salida de Sandy Peterseny la de los empleados ID software restantes decidieron cambiar sustancialmente la dirección de la temática para Quake II, haciendo el diseño más tecnológico y futurista, en lugar de mantener el foco en la fantasía Lovecraftiana. Quake 4 siguió los temas de diseño de Quake II, mientras que Quake III Arena mezcló estos estilos osea tenía un ajuste paralelo que albergaba varios "id all-stars" de varios juegos como personajes jugables. Los ajustes mixtos se produjeron porque Quake II originalmente comenzó como una línea de juego independiente aunque Los diseñadores de id se vieron obligados a recurrir al sobrenombre del proyecto de "Quake II" debido a la falta de obtener derechos sobre el título que querían. Ya que cualquier secuela del Quake original ya había sido vetada. En junio de 2011, John Carmack hizo un comentario extraño de que id Software estaba considerando un remake para el "... mezcló el mundo de "Cthulhu-ish" de Quake 1 y reinició en esa dirección". También hubo otro juego lanzado llamado "Quake Live", que es el último juego de la serie. En E3 2016, Quake Champions fue anunciado en la conferencia de prensa de Bethesda. El juego será un shooter multijugador en el estilo de Quake 3 Arena y será lanzado exclusivamente para Windows.

El 20 de julio de 2016, Axel Gneiting, un empleado de id Tech responsable de la implementación de la ruta de procesamiento de Vulkan al motor id Tech 6 utilizado en Doom (2016), lanzó un puerto con la api Vulkan para Quake 1 llamado vkQuake bajo la licencia GPLv2.

Cuando un cliente se conecta al servidor, este le envía el estado actual de todas las entidades que componen el juego: misiles, otros jugadores y objetos estáticos. Mientras el juego está corriendo, la máquina cliente suministra al servidor con la dirección en que está mirando el jugador, este computa esta dirección y ejecuta la lógica del juego, enviando al cliente una actualización delta de los cambios en las entidades. En cada cuadro se tienen que enviar todos los objetos que han cambiado con sus nuevos valores... así como los que no han cambiado con la información mínima. Como medida de optimización, en realidad sólo se envían aquellas entidades que entrarán dentro del corte de visualización del cliente, de modo que no tenga que viajar por la red datos sobre entidades que de todos modos no van a ser vistas por el cliente.

El juego se realiza en realidad sobre el servidor, y que los clientes son meros espectadores del resultado.

Más tarde, cuando los clientes tienen todos los datos del cuadro, el programa se limita a recoger la información de los polígonos, y pasárselo a las bibliotecas OpenGL (o a los representadores por software en las versiones originales) que se encargan de dibujar estos polígonos sobre pantalla con las texturas dadas.

En la actualidad, se está desviando este modelo a uno con más cálculos en el cliente, pues se ha encontrado que dada las malas calidades de las redes informáticas es muy difícil que este sistema funcione sin saltos bruscos debidos a retrasos en los paquetes de datos. Para ello se está tratando de añadir capacidades de predicción en el cliente de modo que este se pueda “adelantar” al mensaje del servidor y realizar por el mismo muchas de las cosas que se prevé que el servidor va a pedir. Esta técnica se llama predicción de movimiento.

Se llama "mods" a los módulos que permiten cambiar la apariencia o la naturaleza de los juegos corriendo en el motor de Quake. Originalmente los "Mods" se limitaban a modificar la lógica del juegos, los gráficos y el sonido, pero tras el lanzamiento del código fuente de Quake, fue posible también modificar el motor del juego. 
La mayor parte de los "Mods" parten de los diseños originales del juego, y los utilizan para crear otra experiencia de juego. Pueden añadir nuevas formas de juego (como Capturar la Bandera, Asalto a la Fortaleza o rol) o añadir pequeñas modificaciones como un arma nueva o sonidos de distingo gusto. Una pequeña minoría modifica la totalidad de la experiencia de juego, son las "Total Conversion" donde el juego del "Mods" es de otro género al del juego original.

El primer mod de Quake importante fue Threewave Capture the Flag (CTF, Captura la Bandera), principalmente desarrollado por Dave 'Zoid' Kirsch. Se basa en el típico juego en el que dos equipos (rojo y azul) deben competir intentando capturar la bandera del equipo contrario, aunque algunos mapas estaban preparados para soportar hasta a cuatro equipos diferentes (rojo, azul, verde y amarillo). Captura la Bandera se convirtió en un modo estándar de juego incluido en la mayoría de los juegos multijugador aparecidos después de Quake, a parte del modo Combate a Muerte (Deathmatch) introducido por primera vez por DOOM.

El popular mod «Asalto a la Fortaleza» TeamFortress para QuakeWorld consiste en un modo de juego Captura la Bandera, pero con un sistema de clases para los jugadores. Los jugadores adoptan un rol (la clase) y así obtienen unas habilidades (y restricciones) propias de ese papel que desempeña. Por ejemplo, la clase "Soldado" normal y corriente tiene una armadura media, velocidad media y una equilibrada selección de armas y granadas, mientras que la clase "Explorador" está débilmente protegido, es muy rápido, tiene un detector para los enemigos cercanos, pero tiene unas armas ofensivas muy débiles.

Junto con la gran capacidad de modificación de los parámetros de juego, se une la posibilidad de creación de mapas (escenarios del juego) para ser utilizados directamente como misiones individuales o unidos para formar una campaña, o bien escenarios multiplayer en que se desarrollan los llamados deathmatch (combates entre jugadores), ya sea en línea o contra los bots (jugadores virtuales controlados por la inteligencia artificial).
En un principio, crear mapas para Quake era tarea de gente especializada en el tema, al no existir entonces algún software que simplificara esta labor. Posteriormente, diversos programadores crearon utilidades que permitieron al público masivo dar rienda suelta a su imaginación, sin necesidad de conocimientos especializados de computación.

En este sentido, muchos programas obtuvieron popularidad entre los aficionados, como es el caso de Quark (Quake Army Knife). Sin embargo, el más conocido de todos es Worldcraft, creado por Ben Morris en el mes de septiembre de 1996. Este editor de mapas se convirtió en uno de los favoritos de los fanáticos no sólo por ser de los primeros en ver la luz, sino además por su simplicidad, intuitiva interfaz, compatibilidad con elementos creados a medida y rapidez de compilación. Tal nivel de genialidad consiguió este programa, que en julio de 1997 su creador fue contratado por la compañía de videojuegos Valve –adquiriendo asimismo los derechos intelectuales sobre el editor– para desarrollar el editor de mapas oficial de su masivamente famoso juego "Half-Life", pasando ahora a llamarse Hammer (Worldcraft en su versión 3.0). A pesar que esta nueva versión del editor ya no era compatible con Quake, programadores independientes lanzaron un parche que permite adaptar este juego con todas las mejoras que le fueron introducidas al otrora Worldcraft.

Junto con este famoso programa, es destacable también la gran utilidad que hasta hoy presenta el editor gráfico Wally. Este programa permite crear texturas para Quake (junto a otros juegos más, como el mismo Half-Life), pudiendo soportar la creación directa de los gráficos al más puro estilo Paint, como también la opción de importar archivos de imágenes (en numerosos formatos, incluyendo tga) que el mismo editor se encarga de traducir a la paleta de colores de Quake. Con esto –considerando las limitaciones colorativas del motor- los creadores de mapas pueden incluir en sus escenarios cuadros famosos, afiches publicitarios, murallas o decorados realistas, hasta incluso implementar fotografías propias que serán visibles dentro del juego.

Finalmente, dentro de la edición de Quake se pueden mencionar programas como Qped II, que presenta una amplia gama de utilidades, tales como compilar trabajos en archivos ".pak" (paquetes básicos del juego, que contienen desde la configuración hasta los mapas), observar los modelos tridimensionales de los personajes y extraer las texturas de los mapas, simplificando con creces la labor de cualquier aficionado a la edición de Quake.

Un detalle poco conocido de la historia de los juegos de PC: La primera mención a Quake estaba en el primer juego de id Software, "Commander Keen" para PC, que fue lanzado en diciembre de 1990. El siguiente texto (el original estaba en inglés, se presenta aquí una versión traducida) estaba incluido en el archivo "preview.ck1", que tiene fecha del 10 de diciembre de 1990:

Quake fue elegido como el título en el que id Software estaba trabajando poco después del lanzamiento de Doom 2. Las primeras noticias describían a Quake como un personaje parecido a Thor que portaba un martillo gigante y era capaz de derribar a las personas tirando el martillo (junto con cinemáticas inversas en tiempo real). Las primeras capturas de pantalla mostraban ambientes medievales y dragones. El plan era que el juego tuviera más elementos de los JDR. Sin embargo el trabajo en el motor era muy lento, puesto que Carmack no sólo desarrollaba un motor totalmente en 3D, sino también un sistema para redes TCP/IP (Carmack dijo posteriormente que debía haber desarrollado dos proyectos independientes para cada una de esas cosas). Al final el juego estaba muy lejos de sus intenciones originales, y presentaba un sistema de juego similar al del Doom 2. Adorado por la comunidad de jugadores, pronto destronó a los títulos FPS previos y revolucionó la manera en la que los juego multijugador fueron desarrollados. Las más importantes revistas de la época coincidieron en declarar que Quake marcaba "el año cero" en lo que se refiere a juegos asistidos por ordenador.

Antes del lanzamiento de Quake, el 24 de febrero de 1996, ID Software lanzó un Test solamente probado para DeathMatch llamado Qtest1 para así demostrar el magnífico motor gráfico 3D.
Aquel Qtest1 se pueden observar aquellas primitivas ideas del armamento de esta rara versión. La Shotgun, la SuperShotgun, o la RocketLauncher están intactas como la conoces en su versión final, pero las rarezas son la Nailgun y la SuperNailgun. Los niveles los clásicos DM: (DM1, DM2, DM3) aquí llamados: Test1, Test2, Test3. El Hud (barra de estado) con un diseño único,
enemigos los mismos pero con un diferentes "skin" y varios cambios como los ogros lanzaban Nails, el shalrath era un diferente modelo que caminaba en dos pies en vez de 3 de una araña, incluso 3 enemigos nuevos Vomitus (Monstruo que vomita tarbabys) Dragón (parecido a de Dissolution Of Eternity) Serpent (más que todo es como una serpiente voladora).

Un mes antes de la aparición definitiva de "Quake", apareció un prelanzamiento, casi todo era igual a excepción del último nivel que era diferente y los shalrath lanzaban bolas de lava.

Para mejorar la calidad del juego en línea, id Software lanzó "QuakeWorld" en 1996, una versión de Quake que incluía un código de red significativamente mejorado incluyendo predicción de movimiento. El código de red del Quake original fue diseñado para jugar en una LAN - una conexión de gran ancho de banda y baja latencia. Jugar a Quake en Internet a través de una conexión telefónica era terriblemente lento, con parones desconcertantes y retrasos mientras que el cliente esperaba por el servidor, y viceversa.

Con la ayuda de la predicción de movimiento, el código de red de QuakeWorld era mucho más adecuado para los jugadores con conexión telefónica con altos tiempos de ping. Los parámetros del código de red podía ser ajustados por el usuario, por lo que QuakeWorld ejecutaba bien para los usuarios con baja latencia (también conocidos como Low Ping Bastards (Bastardos con Ping Bajo) o LPB's) así como para jugadores con alta latencia (algunas veces llamados High Ping Weenies (salchichitas con Ping Alto) o HPB's). El popular mod Team Fortress está basado completamente es la plataforma QuakeWorld.

En 1996 apareció la conversión de Quake a Linux que incluyó un robo de código y algunos parches enviados a id Software antes de que se convirtiera en una conversión oficial. 1997 vio algunos esfuerzos de conversión más, con una conversión a IRIX, llamada SGI Quake (enlace) realizada por Ed Hutchins en el SGI O2. SGI Quake tenía sistemas de representación mediante OpenGL y mediante software. También en 1997 aparecieron una conversión a MacOS, realizada por MacSoft, y una conversión a Solaris sobre Sparc. Muchas más conversiones fueron realizadas después de que se lanzara el código fuente.

El 4 de agosto de 2007, Quake fue puesto a la venta a través de la plataforma de distribución digital de videojuegos Steam, junto con una colección de otros títulos clásicos de id Software.

El código fuente de los motores de "Quake" y "QuakeWorld" fueron licenciados mediante la GPL en 1999. Los mapas, objetos, texturas, sonidos y otros trabajos creativos de id Software permanecieron bajo su licencia original. La versión shareware de Quake es todavía libremente redistribuible y utilizable por el código del motor liberado mediante la GPL. Debes comprar una copia original del Quake para obtener la versión registrada del juego que incluye más episodios de un solo jugador y los mapas combate a muerte.

Basados en el éxito del primer Quake, id publicó posteriormente "Quake II", "Quake III Arena" y "Quake IV", este último fue desarrollado por Raven Software utilizando el motor Id Tech 4.

Es también interesante destacar que "Quake" es el juego responsable de la aparición del fenómeno machinima en el cual las películas son realizadas mediante los motores gráficos de los juegos gracias a demos de Quake editadas como Ranger Gone Bad y Blahbalicious.

En Quake hay varias maneras de hacer que el personaje se mueva mediante un salto. Algunas de ellas aprovechan errores de software en el motor de física, en vez de utilizar las características del juego. Se debe destacar que algunas de estas "características" han sido incluidas en algunos juegos de acción en primera persona posteriores, específicamente aquellos que utilizan el motor del Quake, como Half-Life.

Para realizar un "rocket jump" (abreviado "RJ"), el jugador utiliza un lanzador de cohetes, apunta hacia abajo cerca de sus pies, salta e inmediatamente dispara un cohete. La explosión del cohete propulsa al jugador hasta alturas y distancias increíbles. El verdadero efecto del "rocket jump" sólo se puede apreciar cuando el jugador no está en el suelo (es decir, que ha saltado antes de lanzar el cohete). Si el jugador estaba en el suelo cuando la explosión sucede el resultado es que el jugador no es impulsado tan lejos y además recibe bastante daño debido a la onda expansiva.

El "rocket jump" se puede realizar en cualquier juego de Quake. Los jugadores realizan el rocket jump para alcanzar los ítems más rápidamente, salvarse de la lava, evitar oponentes, o para encontrar lugares de camping. Algunos jugadores utilizan incluso el lanzagranadas (o el BFG en Quake II) para crear explosiones que intensifiquen el "rocket jump". Usar granadas para ayudar en el vuelo se denomina "grenade jumping".

Pueden realizarse "rocket jumps" increíblemente altos si el jugador está en posesión del "Quad Damage" (que provoca cuatro veces más daño) y el Pentagrama de Protección (que proporciona invulnerabilidad).

El "strafe jumping" permite que el jugador se mueva más rápido y salte más lejos. Se realiza saltando mientras uno se mueve hacia adelante (o hacia atrás) y desplazándose lateralmente a derecha o izquierda ("strafing").El "strafe jumping" se puede realizar en Quake, Quake II, Quake III Arena y Quake IV. Es un fallo relacionado con la aceleración aérea. IdSoftware no quiso integrarlo en Doom 3.

Para incrementar tu velocidad con el "strafe jumping" debes estar primero moviéndote hacia adelante o hacia atrás. Entonces, simultáneamente saltas y te desplazas lateralmente en una dirección, y giras la vista ligeramente en la misma dirección (para rotar avatar en el juego). Alternando entre los desplazamientos a la izquierda y a la derecha tiene como resultado un movimiento casi en línea recta a gran velocidad, y se ha convertido en una técnica utilizada ocasionalmente en los enfrentamientos de Quake.

Un lugar donde el "strafe jumping" puede ser útil es en el mapa de Quake dm2, donde puede hacer un "strafe jump" para coger la armadura roja a través de la lava. Normalmente, el jugador debe apretar un interruptor cercano para extender un puente sobre la lava, debido a que lava es exactamente una unidad más larga de lo que el jugador puede saltar normalmente. Sin embargo, con el aumento de velocidad proporcionado por el "strafe jump", un jugador experimentado puede saltar lo que supuestamente es una distancia imposible. Pero el "strafe jump" es de utilización limitada en el modo combate a muerte, ya que es más inseguro que correr y saltar y mucho menos efectivo que el "rocket jumping".

El "circle jumping" hace uso del hecho de que los jugadores pueden controlar su movimiento mientras están en el aire. Básicamente, un "circle jump" es simplemente un giro de 180 grados mientras estás en el aire. Este salto es utilizado principalmente en QuakeWorld, pero puede ser hecho en el Quake normal, a pesar de que es mucho más difícil.

Una versión diferente del circle jump es utilizado en Quake II donde los jugadores saltan en un arco manipulando la vista para saltar mayores distancias.

Un salto doble es un fallo que permite que el jugador salte dos veces seguidas en mitad del aire. Para realizar un doble salto el jugador debe haber saltado directamente desde un borde y entonces saltar de nuevo. El salto doble puede sólo ser realizado en Quake II en las últimas versiones, y en las modificaciones para QuakeWorld que soporten el "jawnmode". En el mapa Q2DM1, puedes realizar este salto donde recoges el megahealth. Puedes alcanzar el sitio desde el backpack realizando un doble salto y luego saltando normalmente por el megahealth.

El salto doble ha sido incluido intencionalmente en juegos posteriores, incluyendo "Unreal Tournament".

El bunnyhopping es un método para incrementar la velocidad de movimento mediante saltos continuos. Funciona utilizando un fallo en el motor de Quake. Normalmente los jugadores está limitados a una velocidad máxima mientras andan por el suelo. Sin embargo, este límite impuesto no tiene efecto mientras el jugador está en el aire. Además, girar mientras estás en movimiento hace que la entidad del jugador adquiera una aceleración. Estos dos hechos permiten mantener e incrementar la velocidad aérea en saltos sucesivos mientras se gira suavemente. Cuando vuelves a andar por el suelo de nuevo deceleras a la velocidad máxima de correr.

El fallo es que el acto de saltar no es considerado «tocar el suelo». Para ser más preciso, es posible iniciar el próximo salto mientras aún estás en el aire, y por lo tanto el estado no-en-el-suelo del jugador nunca se apaga. Si el jugador salta continuamente el motor no registrará que el jugador toca el suelo, y el movimiento del jugador se verá gobernado por la aceleración aérea (sin límites en su velocidad máxima).

Para empezar un bunnyhopping, realiza un "strafe jump" y salta continuamente mientras te mueves hacia adelante. Empezarás a acelerar más allá de la velocidad de correr normal. El secreto para mantener un bunny hop es pulsar el botón de salto (normalmente la barra espaciadora) mientras aún estás en el aire. El juego te hará saltar nada más aterrizar, y por lo tanto mantendrás tu velocidad aérea y no serás considerado fuera del estado no-en-el-suelo. El bunnyhopping es posible en QuakeWorld, Quake II y Quake III Arena.

En QuakeWorld puedes hacer uso del control aéreo para girar en las esquinas muy rápido, es parecido al "circle jump". En vez de correr alrededor de la esquina en el suelo lentamente el jugador salta y utiliza sus teclas de movimiento para rotarse un cuarto de circunferencia alrededor de la esquina en mitad del aire. En Quake II no hay prácticamente control aéreo, por lo que sólo puedes moverte hacia adelante. Es también útil en QuakeWorld cuando se realiza un "speed jump" (ver más abajo) para mantener tu velocidad de movimiento.

El speed jump es otro salto que permite al jugador moverse más rápidamente y, como el "rocket jump", se sirve de las fuerzas explosivas. Para realizar un speed jump, el jugador coge el lanzador de cohetes, se sitúa cerca de una pared, dispara el cohete hacia la pared, se gira rápidamente para acabar mirando en contra de la pared y salta hacia adelante con la ayuda de la onda expansiva del cohete. Muchos jugadores aderezan esto con un strafe jump o un bunnyhopping para mantener la velocidad ganada mediante esta acción. Los jugadores utilizan esta mejora de velocidad extrema para sorprender a sus oponentes o completar niveles de un solo jugador en tiempos récord. Este salto fue desarrollado por la comunidad de QuakeWorld y puede realizarse también en Quake II. Sin embargo, como no puedes controlar tu movimiento en el aire en Quake II, no puedes girar en las esquinas.

Este salto proviene del Quake III Arena. También es posible realizarlo en QuakeWorld bajo el "jawnmode" utilizando el "Super Nailgun". Dispara el SNG debajo de ti mientras estás muy cerca de una pared y salta para "trepar" por la pared.

En el juego Quake el "quad" proporciona cuatro veces más potencia de fuego. Esto te permite desmembrar a tus enemigos mucho más fácilmente. En Quake 3 el quad fue reducido a sólo 3 veces la potencia de fuego normal, convirtiéndose en un mal uso del nombre.

Un grupo de jugadores expertos de Quake grabaron demos de los niveles de Quake completándolo en nuevas marcas de tiempo y las editaron en una demo de velocidad de Quake continua de 19 minutos y 49 segundos llamada "Quake done Quick" (QdQ, Quake hecho Rápido). El récord fue más tarde mejorado en "Quake done Quicker" (QdQr, Quake hecho más Rápido) hasta 16:35 y por último el increíble "Quake done Quick with a Vengeance" (QdQwav, Quake hecho Rápido con una Venganza, los títulos parafrasean los títulos originales de la trilogía de Jungla de cristal) hasta 12:23 en el nivel Nightmare (Pesadilla, el más difícil de todos). Se han realizado speed runs parecidos para los conjuntos de misiones de "Quake", "Quake II", Ultimate Doom (16:05), Doom II (21:16) y Half-Life (31:00).



La popular «LAN party» estadounidense QuakeCon tiene sus raíces también en el juego. Esta convención para jugadores fue diseñada en un principio para que los fans pudieran reunirse cada año y competir en una red de área local en condiciones igualadas sin los retrasos de las conexiones a Internet y las pérdidas de paquetes que dificultan el juego.



</doc>
<doc id="2445" url="https://es.wikipedia.org/wiki?curid=2445" title="Quetzalcóatl">
Quetzalcóatl

Quetzalcóatl (nahuatl) : ‘serpiente emplumada; siendo "quetzal:" ‘pluma’; y "cōātl:" ‘serpiente’) es uno de los dioses de la cultura mesoamericana, llegando a considerarse como el dios principal del panteón prehispánico; entre otros. Se considera a Quetzalcóatl como la deidad principal de la cual se generan los demás a partir del desdoblamiento. Otros consideran a Tezcatlipoca como el dios principal (ensayo "Tezcatlipoca, dios principal"), mientras que otros más consideran a los dioses que le dieron origen como los principales, surgiéndose como el dios de la vida, de la luz, de la sabiduría, de la fertilidad y del conocimiento, patrón del día y de los vientos, el regidor del Oeste. 

Dios Quetzalcóatl, cuya traducción más popular es serpiente emplumada, va referida a Venus (planeta) y significa gemelo precioso, según Alfonso Caso, por creerlo una estrella gemela (lo es de sí misma, al aparecer en el firmamento en dos momentos distintos, como Lucero del alba y como Lucero vespertino).

Quetzalcóatl, considerado como "La Serpiente Emplumada", representa la dualidad inherente a la condición humana: la "serpiente" es cuerpo físico con sus limitaciones y las "plumas" son los principios espirituales. Otro nombre aplicado a esta deidad es Nahualpiltzintli, "príncipe de los nahuales". Quetzalcóatl es también el título de los sacerdotes supremos de la religión tolteca. Se lo identificó con al menos un personaje histórico, a saber: Ce Ácatl Topiltzin, rey de Tula, quien según el Memorial Breve de Colhuacan y la Historia de los Mexicanos por sus Pinturas, vivió entre los años 999 y 1051 de la era cristiana.

Las enseñanzas de Quetzalcóatl quedaron recogidas en ciertos documentos llamados Huehuetlahtolli (‘antiguas palabras’), transmitidos por tradición oral y puestos por escrito por los primeros cronistas españoles. Se han publicado traducciones parciales de los mismos.

Debido a que consideraban que todo el Universo tiene una naturaleza dual o polar, los toltecas creían que el Ser Supremo tiene una doble condición. Por un lado crea el mundo y por el otro lo destruye. La función destructora de Quetzalcóatl recibió el nombre de Tezcatlipoca, “espejo negro que humea”, cuya etimología es la siguiente: Tezcatl, “espejo”, tliltic, “negro”, Poca, “humo”. Los informantes del padre Motolinía describieron a esta deidad del siguiente modo: «Tezcatlipoca era el que sabía todos los pensamientos y estaba en todo lugar y conocía los corazones; por eso le llamaban Moyocoya (ni), que quiere decir que es Todopoderoso o que hace todas las cosas; y no le sabían pintar sino como aire.» (: "Teogonía e Historia de los Mexicanos")

Con un fin didáctico, el mito acentuaba la contradicción entre Quetzalcóatl y Tezcatlipoca. Sin embargo, su identidad esencial queda establecida en los códices y otros testimonios gráficos, donde ambas deidades comparten los mismos atributos.

Según la Cosmogonía Náhuatl, el dios Iztauhqui-tezcatlipoca (Quetzalcóatl) es uno de los cuatro hijos de los dioses primordiales llamados Ometecuhtli y Omecíhuatl, bajo el relato de la creación del universo, de los cuales representan las esencia masculina y femenina de la creación, por lo que Quetzalcóatl simboliza la vida, la luz, la sabiduría, la fertilidad y el conocimiento, así como patrón de los vientos y del día, es el regidor del Oeste con el nombre de Tezcatlipoca Blanco. Con el tiempo, otros mitos se vinieron integrando para pasar de ser un dios creador de la humanidad hasta un rey histórico de la ciudad de Tula, o bien como otro dios solar al lado de su hermano Huitzilopochtli, interpretándose así con este mito, el traslado que realiza el Sol a través de los cielos, desde el amanecer hasta el atardecer por sus regidores y hermanos Tlahuizcalpantecuhtli y Xólotl, que junto con ellos, es hijo de Mixcóatl y Chimalma.

Para la cultura azteca y otras civilizaciones mesoamericanas, el dios era hermano de Tezcatlipoca. Para los toltecas, también eran rivales. Sea como sea, ambos eran considerados como el Ser Supremo. La combinación Quetzal-coatl contiene los siguientes significados, todos relativos a las funciones de Quetzalcóatl en la teología tolteca: "serpiente con plumas", "doble precioso", "ave de las edades", "gema de los ciclos", "ombligo o centro precioso", "serpiente acuática fecundadora", "el de las barbas de serpiente", "el precioso aconsejador", "divina dualidad", "femenino y masculino", "pecado y perfección", "movimiento y quietud". Quetzalcóatl era también importante para la civilización teotihuacana.

El dios tiene varias etapas, primero como deidad olmeca, tolteca, pipil, maya (como Kukulcán) y más tarde en el grupo de los dioses aztecas. La cultura tolteca tomó la figura de este dios de la tradición religiosa de Teotihuacan en donde se encuentra una pirámide dedicada a la serpiente emplumada que data del siglo II de nuestra era. Sin embargo, tiene una raíz histórica más antigua. Los estudios recientes demuestran que este personaje se relaciona con la Mitología olmeca y con su visión de la serpiente emplumada. El arte y la iconografía de los olmecas demuestran claramente la importancia de la deidad de la Serpiente Emplumada en las cronologías de Mesoamérica, así como en el arte olmeca. En las grutas de Juxtlahuaca hay una representación de una serpiente emplumada de estilo olmeca. Incluso, desde lugares lejanos como la Laguna de Asososca, en Managua, Nicaragua se encuentran pinturas rupestres representativas de La Serpiente Emplumada, hasta Tula, hoy Estado de Hidalgo México. El nombre de Quetzalcóatl se compone de dos palabras de origen náhuatl: "quetzal", que es un ave de hermoso plumaje que habita la selva centroamericana, y "cóatl", que significa "serpiente". Existe otra versión científica según la cual es posible que este dios tenga raíces Chichimecas. Sus influencias culturales abarcaron gran parte de Mesoamérica, incluyendo a las culturas maya y mixteca. Los mayas retomaron a Quetzalcóatl como Kukulkán o Gucumatz, aunque como se ha dicho antes es más conocida la versión de la cultura tolteca. Los aztecas incorporaron esta deidad a su llegada al valle de México.

Los mexicas relacionaban a Quetzalcóatl con el planeta Venus, que se puede observar como si fuera una estrella al lado del volcán Popocatépetl durante ocho meses al año, y desaparece otros tres meses. La profecía indica que este astro y los dos solsticios en donde se dice que Quetzalcóatl viene a la tierra dos veces al año a traer fertilidad y cosecha, sucederán hasta la segunda venida carnal de Quetzalcóatl. Una de las representaciones de esta deidad era la de un hombre barbado, por lo que durante la conquista de la Nueva España (Mesoamérica) algunos pueblos identificaron a Hernán Cortés con Quetzalcóatl. Tal afirmación nace desde las primeras Cartas de Relación que Cortés preparó para ser entregadas al rey español Carlos V. Se considera que dichas cartas fueron una estrategia legal, ya que las conquistas de tierras solamente podían ser aprobadas por el rey de acuerdo con las leyes españolas (Las Siete Partidas). Cortés carecía de dicho permiso, y por lo tanto tenía una orden de aprehensión. Posteriormente, defendió su postura al decir que los mexicas se rindieron al confundirlo con el dueño de las tierras y Cortés le entregaba esta posesión al rey, argumentando inocencia. Durante la colonia, la ilegalidad de la conquista se fue olvidando y el mito de que los españoles fueron confundidos por Quetzalcóatl se fortaleció, en parte por la aculturación oficial de los mexicas para reafirmar la jerarquía colonial.

En tiempos recientes las religiones de origen neotolteca hablan en sus tradiciones y leyendas urbanas del renacimiento de este personaje, idea que aparece en el llamado "Códice de Quetzalcóatl".

Quetzalcóatl es también el nombre de un personaje tolteca legendario, Ce Acatl Topiltzin Quetzalcóatl. Hijo de Mixcóatl y Chimalma, fue el último rey de Tollan o Toílan, ciudad que algunos estudios han identificado con la de Tula. El significado de su nombre es como sigue: "Ce Acatl": "Uno Caña", inicio de la trecena y último día del cuarto mes Huei Tozoztli (Perforación de la Gran Ave) dedicado a el autosacrificio, "To": "Nuestro" y "Piltzin": "Joven Noble/Príncipe", el nombre con que se reconocía al gobernante. Su denominación como Quetzalcóatl se debe al culto al que pertenecía. Algunos autores creen que Tollan es hoy la ciudad de Tula, situada en el estado de Hidalgo, México. La leyenda dice que cayó por las tentaciones que los dioses presentaron al último rey de Tula y que están asociados a estados bélicos, no religiosos (precedentes al estado mexica). Teotihuacán, la ciudad de los dioses, es anterior a estas urbes.

La antropóloga Carmen Cook de Leonhardt promovió en los años ochenta la afirmación de que el pueblo María Magdalena Amatlán, o Amatlán de Quetzalcóatl (uno de los barrios de Tepoztlán), había sido la cuna del príncipe Ce Ácatl Quetzalcóatl. El presidente mexicano José López Portillo aceptó la propuesta y de alguna manera se "oficializó" la creencia de que el Quetzalcóatl histórico había nacido ahí. El novelista e investigador mexicano Fernando Zamora (respaldado por el Instituto de Investigaciones Estéticas de la UNAM) discute el hecho en la tesis: "Quetzalcóatl nació en Amatlán: Identidad y nación en un pueblo mesoamericano", publicado por la Universidad Iberoamericana.

La antropóloga Carmen Cook basó su afirmación en tres estelas, en las que se le representaba respectivamente como serpiente emplumada y como el planeta Venus. De acuerdo con Cook, en dichas estelas y con base en la forma en que Venus se mueve por el cielo, encontró que el padre del dios serpiente fue el rey tolteca Mixcóatl (representado en la Vía Láctea) y que su madre se llamaba Chimalma. Dos de los cerros que rodean el lugar llevan dichos nombres desde tiempos prehispánicos, lo cual condujo a Carmen Cook a la convicción de que Amatlán era el lugar de nacimiento de Quetzalcóatl, hecho que si bien no ha recibido aceptación por parte de la comunidad científica, suele ser aceptado como verdadero por la gente del estado de Morelos, y particularmente por el pueblo de Amatlán.

Según la leyenda, Quetzalcoátl llegó a la zona Maya (sureste del actual México) donde fue reconocido como un gran jefe guerrero, fundó la liga de Mayapán y conquistó la ciudad de Chichen Itzá donde fue conocido bajo el nombre de Kukulkán ("k'u uk'um", “pluma” y "kaan", “serpiente”) y donde se encuentra el templo que lleva su nombre.



</doc>
<doc id="2447" url="https://es.wikipedia.org/wiki?curid=2447" title="Publio Sulpicio Quirinio">
Publio Sulpicio Quirinio

Publio Sulpicio Quirinio, a veces llamado también Publio Sulpicio Quirino o Cirenio (en griego Κυρήνιος, c. 51 a. C. - 21) fue un aristócrata del Imperio romano, miembro del Senado y cónsul. 
Su periodo como gobernador de Siria es uno de los anclajes cronológicos del nacimiento de Jesús de Nazaret.

Nacido en el barrio de Lanuvio, población latina cercana a Roma, de familia nada distinguida, Quirinio recorrió el trayecto de servicio normal de un joven ambicioso de su clase social. Según el historiador romano Floro, Quirinio venció a los marmáridas, tribu de bandoleros del desierto procedente de Cirenaica, posiblemente cuando era gobernador de Creta y Cirene alrededor de 14 a. C., aunque no quiso aceptar el nombre honorífico que ameritaba por esa victoria militar, que habría sido Marmárico. En 12 a. C. fue nombrado cónsul, señal de que gozaba del favor de Augusto. Unos años más tarde encabezó una campaña contra los homonadenses, tribu sita en la región montañosa de Galacia y Cilicia, alrededor de 5 a. C. o 3 a. C., probablemente como legado de Galacia. Venció reduciendo los bastiones de su enemigo y matando de hambre a sus defensores. Esta victoria le valió un triunfo.

Para el año 1 d. C., Quirinio fue nombrado rector del nieto de Augusto, Gayo César, hasta que el joven murió de heridas que sufrió en campaña. Cuando el apoyo de Augusto pasó a su hijastro Tiberio, Quirinio se pasó al campo de seguidores de éste. Casado con Claudia Apia, de quien poco se sabe, se divorció de ella alrededor del año 3 d. C. y casó con Emilia Lépida, hija de Marco Emilio Lépido y hermana de Manio Emilio Lépido, que originalmente había estado comprometida con Lucio César. A los pocos años se divorciaron; en el año 20 d. C., Quirinio la acusó de alegar que era hijo del padre de ella y, más tarde, de intentar envenenarlo durante su matrimonio; Tácito afirma que Emilia gozaba de popularidad ante el pueblo, a cuyos ojos Quirinio la acusaba por despecho.

Tras la destitución de Arquelao, hijo de Herodes I el Grande, Quirinio llegó a Siria, enviado por César Augusto para hacer el censo de los bienes con vistas a establecer el impuesto. Con él fue enviado Coponio, para gobernar a los judíos. Como Judea había sido anexionada a Siria, Quirinio la incluyó en el censo.

El censo tuvo lugar "37 años después de que Octavio derrotó a Antonio en la batalla naval de Accio, el 2 de septiembre" (Flavio Josefo), lo que correspondería al año 6 de la era común.

La "Biblia" menciona el censo de Quirinio como referente del nacimiento de Jesús de Nazaret:

Mientras que la cita anterior del Evangelio de Lucas menciona el censo de Quirinio como previo al nacimiento de Jesús, el Evangelio de Mateo afirma que Jesús nació durante el reinado de Herodes I el Grande (Mateo 2:1). Una aparente contradicción resulta del hecho de que Herodes I el Grande fallece en el año 4 a. C., o sea, 10 años antes del censo de Qurinio. No se conoce de la existencia de otro censo en el período final del Reinado de Herodes, y el censo de Quirinio es llamado "el primero", por lo que se descartaría otro anterior. Sin embargo, para cuadrar ambos relatos algunos autores cristianos plantean si Qurirnio podría haber estado ya antes en Siria, hacia el año 6 a. C., gobernando conjuntamente con Saturnino o con Quintilio Varo, y si podría haber realizado entonces un "primer" censo. Pero en ese entonces Judea no era parte de Siria y no tendría sentido censarla.

Según Flavio Josefo (Ant. XVIII 1), este censo supuso una revuelta armada, dirigida por Sadoc y Judas el Galileo, natural de Gamala, y el propio Quirinio habría sofocado la revuelta, lo cual habría sido absurdo si aun viviera Herodes el Grande, pues como rey de Judea le habría correspondido sofocar ese levantamiento.

Quirinio se desempeñó como gobernador de Siria con autoridad nominal sobre Judea hasta 12 d. C., cuando volvió a Roma como allegado de Tiberio. Nueve años más tarde falleció, y Tiberio ordenó que se le diera funeral público.



</doc>
<doc id="2450" url="https://es.wikipedia.org/wiki?curid=2450" title="Quebec">
Quebec

Quebec (en inglés: «Quebec», y en francés y oficialmente: «Québec», pronunciado //) es una de las diez provincias que, junto con los tres territorios, conforman las trece entidades federales de Canadá. Su capital es la homónima Quebec y su ciudad más poblada, Montreal. Está ubicada al este del país, limitando al noroeste y norte con la bahía de Hudson y el estrecho de Hudson, respectivamente, —que la separan de Nunavut—, al noreste con Terranova y Labrador, al este con el golfo de San Lorenzo y Nuevo Brunswick, al sureste con el río San Lorenzo que la separa de Estados Unidos, y al sur y suroeste con Ontario. Con 7 744 530 habs. en 2008 es la segunda entidad más poblada —por detrás de Ontario— y con 1 542 056 km², la segunda más extensa, por detrás de Nunavut.

Por su idioma, su cultura y sus instituciones, forma una «nación dentro de Canadá». A diferencia de las demás provincias, Quebec tiene como única lengua oficial el francés, y es la única región mayoritariamente francófona de Norteamérica. El idioma francés goza de protección legal e incluso la provincia cuenta con inspectores lingüísticos que revisan y controlan su uso. El celo de los quebequeses por su lengua y su estatus de minoría lingüística en América del Norte ha llegado a ciertos extremos políticos, pero también en su historia el pueblo quebequés sufrió periodos de represión y asimilación inglesa.

El Referéndum de independencia de Quebec de 1980 tuvo lugar el 20 de mayo de ese mismo año y los independentistas liderados por René Lévesque obtuvieron el 40,5 % de los sufragios. En el Referéndum de independencia de Quebec de 1995, los independentistas se quedaron a menos de un punto porcentual de conseguirlo con el 49,4 % de los votos.

El 27 de noviembre de 2006 el parlamento canadiense, con el apoyo del partido gobernante, reconoció a los quebequeses como una nación dentro de Canadá unido en un intento de aplacar los deseos secesionistas de los partidos independentistas, aunque fue en un sentido cultural y social pero no legal.

En las elecciones generales de Quebec de 2012, el partido independentista Partido Quebequés, liderado por Pauline Marois, ganó la mayoría de los asientos de la Asamblea Nacional de Quebec, formando un gobierno minoritario. En el discurso del día de las elecciones, la ganadora planteó la posibilidad de convocar a un nuevo referéndum por la independencia al expresar su deseo de que Quebec se convierta en un país independiente y su convicción de que eso sucederá: «Queremos un país. Y lo tendremos.»

La provincia de Quebec se ubica al este de la provincia de Ontario y de la bahía de Hudson, al sur del Nunavut y del estrecho de Davis, al oeste de las Provincias Marítimas y de Labrador y al norte de varios estados de los Estados Unidos (Nueva York, Vermont, Nuevo Hampshire y Maine). Más del 90 % de la superficie de Quebec forma parte del llamado Escudo Canadiense.

Quebec comparte una frontera terrestre con cuatro estados en el noreste de Estados Unidos (Maine, Nuevo Hampshire, Nueva York y Vermont) y tres provincias canadienses (Nuevo Brunswick, Ontario y Terranova y Labrador). En el Golfo de San Lorenzo, la frontera es la línea de equidistancia entre las riberas de Quebec y la Isla del Príncipe Eduardo, Nuevo Brunswick, Nueva Escocia y Terranova y Labrador. Al norte y el noroeste, la frontera marítima con el territorio de Nunavut sigue las orillas de la península de Labrador.

Una disputa fronteriza sigue en relación con la propiedad de Labrador (la frontera no está expresamente reconocida en Quebec). Por otra parte, ya que los límites marítimos varían con las mareas, las islas costeras de la Bahía de Hudson y la Bahía de Ungava en Quebec sólo lo son durante la marea baja. Así, más del 80 % de las fronteras de Quebec siguen siendo inciertas.

El principal río es el San Lorenzo, arteria navegable que comunica la región de los Grandes Lagos con el Océano Atlántico. Atraviesa las ciudades de Montreal y Quebec, entre otras, y permanece helado desde noviembre hasta marzo. El clima es continental con temperaturas suaves en verano y muy frías en invierno, precipitaciones abundantes (en forma de nieve buena parte del año). En Montreal la temperatura media anual es de 6,1 °C (20,9 °C en julio, −10,4 °C en enero), en Quebec es de 4,0 °C (19,2 °C en julio, −12,8 °C en enero).

Quebec tiene tres regiones de clima principales:

Dentro de los mamíferos están el alce, lobo, puma, oso negro, venado, caribú, puercoespín, marmota, zorro, zorrillo, ardilla, carcayú. Aves como la guacharaca, lechuza montañera y colibrí. Reptiles como las serpientes son abundantes. Anfibios como ranas y sapos. Y entre los invertebrados abundan los insectos, arácnidos y escorpiones.

En el contorno de la bahía de Ungava y del estrecho de Hudson se encuentra la tundra, cuya flora se resume en una vegetación herbácea y arbustiva baja y de líquenes. Más al sur, el clima se vuelve propicio al crecimiento del bosque boreal, cuyo límite norte es la taiga.

La superficie del bosque quebequés se estima en 750 300 km². De Abitibi-Témiscamingue en la Costa-norte, este bosque esencialmente está compuesto por coníferas como el abeto baumier, el pino gris, la espineta (caponera) blanca, la espineta negra y el alerce laricio. Acercándose del río hacia el sur, se añaden gradualmente el abedul amarillo y otros hojosos. El valle del San Lorenzo está compuesto por el bosque laurentiano con coníferas como el pino blanco de América y la tuya de Occidente (cedro) así como de hojosos.

En 2016 la provincia de Quebec contaba con 8 164 361 habitantes, la mayoría de los cuales residían en el área metropolitana de Montreal, que cuenta con 4 098 927, segunda ciudad en habitantes de Canadá y la cuarta metrópoli francófona del mundo tras París, Kinsasa y Abiyán. Otras áreas metropolitanas de más de 100 .000 habitantes son las de Quebec (800 296), Sherbrooke (212 105), Saguenay (160 980) y Trois-Rivières (156 042). El área de Gatineau (332 057), frente a Ottawa, forma junto a ésta un área metropolitana de 1 323 783 habitantes. La población urbana en 2004 era de un 80,4 % y el porcentaje de inmigrantes se cifraba en un 12 % de la población total, destacando sobre todo la recepción de inmigración francófona, principalmente haitiana. La esperanza de vida es de 81,9 años para las mujeres y 76,3 para los hombres.

El idioma oficial de la provincia es el francés. Es la única provincia canadiense donde la mayoría de la población es francófona, y el inglés no es reconocido como lengua oficial por las leyes de la provincia. Sin embargo, según la ley constitucional de Canadá de 1867, tanto el francés como el inglés pueden ser usados en la Asamblea Nacional de Quebec y sus cortes judiciales, y algunos documentos oficiales también deben estar en ambos idiomas. Además, la minoría angloparlante tiene derecho a recibir la enseñanza en su idioma.

Según el censo de 2001, la lengua mayoritaria es el francés, hablada por el 81,2 % de la población. El 10 % habla una lengua no oficial (los llamados alófonos), el 8 % es anglófono y tan solo son bilingües el 0,8 % de la población. En el área metropolitana de Montreal el porcentaje de francófonos es del 68 %, siendo el 18,5 % alófonos, el 12,5 % anglófonos, y bilingües el 1 % de la población. En las demás ciudades el porcentaje de francófonos supera el 90 %.

Quebec es una de las dos provincias canadienses cuya población es mayoritariamente católica, junto a Nueva Brunswick. Éste es un legado de la época colonial, cuando solo a los católicos se les permitió establecerse en la Nueva Francia.

Los santos patronos de esta Provincia son San Juan Bautista (cuya festividad es la Fiesta Nacional de Quebec) y Santa Ana.

El censo de 2001 mostró que la población era en 83,2 % cristianos católicos; el 4,7 % cristianos protestantes (incluidos 1,2 % anglicanos, 0,7 % de la Iglesia Unida, y el 0,5 % bautistas); el 1,4 % cristianos ortodoxos (incluyendo el 0,7 % de griegos ortodoxos), y el 0,8 % otros cristianos, así como el 1,5 % musulmanes, 1,3 % judíos; 0,6 % budistas, 0,3 % hindúes y sijs el 0,1 %. Un 5,8 % de la población dijo que no tenía ninguna afiliación religiosa (incluido el 5,6 % que dijo que no tenían ninguna religión en absoluto).

Cabe destacar que la mayoría de las expresiones vulgares de la lengua cotidiana utilizan términos habituales de la Iglesia católica y considerados sagrados por esta: "calise" (cáliz), "tabarnac" (tabernáculo), "ciboire" (copón), "hostie" (hostia). Dichas expresiones son a veces consideradas como parte de la identidad del dialecto quebequés frente al francés europeo.

La provincia de Quebec está altamente industrializada y en el territorio abundan los recursos naturales, entre los que destacan los minerales, grandes bosques de coníferas que nutren una importante industria maderera o los lagos, ríos y otras corrientes de agua que producen energía hidroeléctrica no sólo para consumo interno sino también para su exportación a los Estados Unidos.

El valle de San Lorenzo es una región agrícola muy fértil. Al contar con una gran cabaña ganadera, produce lácteos variados y carne, y en sus campos se cosechan excelentes frutas y verduras. Destaca en gran medida la producción de azúcar de arce, del cual la provincia de Quebec es el primer productor mundial.

El jefe de gobierno es el Primer Ministro, quien es el presidente del partido que más escaños ocupa en la Asamblea Nacional de Quebec. Los partidos más fuertes son el federalista Partido Liberal de Quebec ("Parti libéral du Québec") de centro-derecha y el nacionalista Partido Quebequés ("Parti québécois") de centro-izquierda.
El Teniente Gobernador representa a la reina Isabel II del Reino Unido y actúa simbólicamente como jefe de estado.

Antes de la llegada de los franceses, Quebec estaba habitado por diferentes pueblos aborígenes, entre los cuales destacan los inuits (antiguos esquimales), los hurones, los algonquinos, los mohawks, los cree y los innus.

El primer explorador francés en Quebec fue Jacques Cartier, que en 1534 estableció en Gaspé una gran cruz de madera con tres flores de lis, tomando posesión de aquellas tierras en nombre de Francia. Cartier descubrió el río San Lorenzo.
En 1608, Samuel de Champlain dio nacimiento a la Nueva Francia fundando en la orilla norte del río San Lorenzo, en un lugar que los indios llamaban "kebek" (‘estrecho’), la ciudad de Quebec. La ciudad se volverá así el punto de partida de las exploraciones francesas en América del Norte. Después de 1627, el rey de Francia Luis XIII concedió el monopolio de la colonización a los católicos. La Nueva Francia se volvió una colonia real en 1663, bajo el reinado de Luis XIV.
Los franceses se alían con los indios hurones y otros contra los indios iroqueses que eran los aliados de los británicos. La Guerra de los Siete Años (1756-1763), entre Gran Bretaña y Francia, toma un cambio decisivo en Norteamérica con la derrota en 1759 del ejército de Louis-Joseph de Montcalm a manos del ejército británico del general James Wolfe en la batalla de los Llanos de Abraham, a las puertas de la ciudad de Quebec.

Entre 1755 y 1762 los pobladores de la zona llamada Acadia en las actuales provincias marítimas de Nueva Escocia y Nuevo Brunswick sufrieron una deportación masiva de sus tierras, entregadas a inmigrantes de Nueva Inglaterra. Las familias, separadas en varios navíos y deportadas a otros países —Estados Unidos (Luisiana, en particular), Francia, Gran Bretaña— sufrieron una fuerte mortalidad.

El Reino Unido tomó posesión de la Nueva Francia con el Tratado de París en 1763, cuando el rey Luis XV de Francia y sus consejeros eligieron conservar Guadalupe, por su azúcar, en lugar de Quebec, en ese entonces considerado como un extenso territorio de hielo sin importancia. A raíz de este Tratado, la mayoría de los aristócratas regresaron a Francia.

En 1774 con la Ley de Quebec, Londres daba reconocimiento oficial a los derechos del pueblo francés de Quebec: el uso de la lengua francesa, la práctica de la religión católica y el uso del Derecho Romano en lugar del Jurisprudencial anglosajón. Antes de esta fecha, la situación de la religión católica era muy frágil y las posibilidades para los católicos, muy limitadas.
En 1791 la Ley Constitucional de Canadá estableció dos provincias alrededor del río Ottawa: el Alto Canadá (la actual provincia de Ontario), de mayoría anglófona, y el Bajo Canadá (la actual provincia de Quebec), provincia mayoritariamente francófona.

En 1867, la firma de la Ley de América del Norte Británica consagró la federación de las provincias de Canadá, que constaba entonces de Quebec, Ontario, Nuevo Brunswick y Nueva Escocia.

La exclusión económica de los francoparlantes en Quebec fue considerada siempre un problema en Quebec hasta las reformas de los años 1960, la llamada «Revolución tranquila» ("Révolution Tranquille"). El primer ministro de Quebec en ese entonces, Jean Lesage, propuso la nacionalización de la producción de electricidad. El gobierno creó empresas y bancas nacionales, y después impuso legislación para reconocer el derecho de trabajar en francés.

En 1948 se aprobó la actual bandera de Quebec como oficial, y la lengua francesa sería cooficial, junto al inglés, en Canadá desde 1968. En 1976 ganó las elecciones el nacionalista Partido Quebequés, de René Lévesque, que promulgaría la "Ley 101," por la que el francés sería la única lengua oficial de Quebec. En 1980, el referéndum de independencia arrojó resultado negativo, con un 59,6 % de votos en contra. De nuevo en el poder en 1995, el Partido Quebequés convocó a un nuevo referéndum el 30 de octubre de 1995, donde el "no" a la independencia ganó por tan solo 54.000 votos y un 50,4 % de sufragios, con una participación que superó el 90 % del censo electoral. Según las encuestas, poco menos de la mitad de los quebequeses sigue deseando constituirse en estado independiente.

El 27 de noviembre de 2006, el parlamento canadiense, con el apoyo del partido en el gobierno, reconoció a los quebequeses ("Québécois", en francés) como «nación dentro de un Canadá unido», en un intento de aplacar los deseos secesionistas de los partidos independentistas, aunque en sentido cultural y social, no legal.

Asimismo, la provincia de Quebec tiene otras diez naciones de indios e inuit, reconocidos como tales por su Asamblea Nacional en la época de René Lévesque, y son también reconocidas por el gobierno federal (las Primeras Naciones). Otras naciones en la provincia de Quebec podrían eventualmente ser reconocidas, como los «métis» y los anglo-quebequeses.


La provincia de Quebec está dividida en 17 regiones administrativas:







</doc>
<doc id="2451" url="https://es.wikipedia.org/wiki?curid=2451" title="Quiromancia">
Quiromancia

La quiromancia o quiromancía es el intento de la adivinación a través de la lectura de las líneas de la mano. El término deriva del griego χείρ ("khéir", "mano") y μαντεία ("manteía", "adivinación"). Es una rama de la quirología y se centra en el estudio de las líneas y montes que se hallan en las palmas de las manos que, por medio de la observación, revelan supuestamente el perfil psicológico y fisiológico de una persona. Aunque suele ir íntimamente ligada a la adivinación y a las ciencias ocultas, siempre ha existido una cierta aceptación popular. Comúnmente, la práctica de la quiromancia se denomina "leer la mano" o "leer las manos", aunque también se conoce como "echar", "leer" o "decir la buenaventura". Científicamente se le considera una seudociencia donde históricamente se han utilizado técnicas de lectura en frío.

Desde hace algunas décadas, la comunidad científica ha corroborado la existencia de diversas relaciones químicas entre genes inconexos, vinculando así caracteres fenotípicos diferentes. Los quiromantes defienden de este modo la relación que pudiera existir entre los surcos y pliegues de las palmas de las manos con numerosos rasgos físicos y psíquicos, pudiendo así estudiar el perfil psicológico de una persona a través de su lectura palmar. Sin embargo, los defensores de esta hipótesis no presentan ningún estudio de relación entre los genes que determinan las líneas de la mano, actualmente desconocidos y los que determinan otros rasgos.

Se suele decir que en las manos se puede conocer el destino de una persona y adivinar sucesos pasados, presentes y futuros.

En su estudio sobre la historia de las técnicas adivinatorias, el ensayista italiano Giordano Berti cita las siguientes palabras de Leonardo da Vinci: 

El estudio de las líneas se lleva a cabo normalmente en la mano derecha, sea cual sea la mano dominante. La mano debe estar abierta con la palma a la vista. Dichas líneas se dividen en dos grupos: las líneas mayores y las menores. Existen 3 líneas mayores y numerosas líneas menores que no siempre aparecen en su totalidad.
Antiguamente, la quiromancia era un rito pagano, como todos los ritos de adivinación. Los que la practicaban eran acusados de brujería y perseguidos por la Santa Inquisición.

En la actualidad, la práctica de la quiromancia, o de la quirología en general, suele estar acompañada de la lectura del tarot y otras prácticas esotéricas. Se pueden encontrar quiromantes con consultas privadas y altos precios por el estudio de la mano, como quiromantes instalados en zonas céntricas de las ciudades con gran afluencia de peatones que practican la quiromancia y otras artes esotéricas a cambio de la voluntad del cliente.

La quiromancia es considerada como una pseudociencia por la comunidad científica al no cumplir con los requisitos básicos del método científico. Es decir, no ha superado las pruebas necesarias para ser considerada aceptable científicamente. Hay poca investigación que acepte la verificación de la precisión de la quiromancia como un sistema de análisis y mucha de ésta ha sido llevada a cabo por los propios quiromantes. Por otra parte, el mago y escéptico James Randi ofrece un premio de un millón de dólares a cualquiera que logre demostrar fehacientemente la existencia de un fenómeno o poderes paranormales —incluyendo el tipo de eventos como los que se ocupa la quiromancia— pero el premio está desierto desde que se ofreció.



</doc>
<doc id="2452" url="https://es.wikipedia.org/wiki?curid=2452" title="Quimera (mitología)">
Quimera (mitología)

En la mitología griega, Quimera (en griego antiguo Χίμαιρα "Khimaira" que significa "animal fabuloso"; latín "Chimæra") era un monstruo híbrido, hija de Tifón y de Equidna, que vagaba por las regiones de Asia Menor aterrorizando a las poblaciones y engullendo animales, y hasta rebaños enteros. De su unión con Ortro nacieron la Esfinge y el León de Nemea.

Las descripciones varían desde las que decían que tenía el cuerpo de una cabra, la cola de una serpiente o un dragón y la cabeza de un león, hasta las que afirmaban que tenía tres cabezas: una de león, otra de macho cabrío, que le salía del lomo, y la última de dragón o serpiente, que nacía en la cola. Todas las descripciones coinciden sin embargo en que escupía fuego por una o más de sus cabezas.Era sumamente rápida.

Quimera fue derrotada finalmente por Belerofonte con la ayuda de Pegaso, el caballo alado, a las órdenes del rey Iobates de Licia. Hay varias descripciones de su muerte: algunas dicen simplemente que Belerofonte la atravesó con su lanza, mientras que otras sostienen que la mató cubriendo la punta de la lanza con plomo que se fundió al ser expuesto a la ardiente respiración de Quimera.

"La quimera de Arezzo", de origen etrusco conservado en el Museo Arqueológico de Florencia, es un buen ejemplo.



</doc>
<doc id="2456" url="https://es.wikipedia.org/wiki?curid=2456" title="Rugby">
Rugby

El rugby es un deporte de contacto en equipo nacido en Inglaterra, donde tomó ese nombre a partir de las reglas del fútbol elaboradas en el colegio de la ciudad de Rugby (Rugby School) en el siglo XIX. Sobre la forma de denominar en español al jugador practicante del deporte, el "Diccionario panhispánico de dudas" informa que «se usa con frecuencia en los países del Río de la Plata la forma "rugbier" con el sufijo -"er" propio del inglés para crear este tipo de derivados (aunque en inglés se usa, en este caso, la expresión "rugby player"). La Real Academia Española utiliza la palabra "rugbi", como "adaptación gráfica" del término inglés y recomienda la expresión "rugbista" para referirse al jugador, utilizando el sufijo -"ista" ("futbolista, golfista, tenista", etc.).

El "rugby" es practicado a nivel internacional en todos los continentes, aunque es muy popular principalmente en las naciones que conforman las islas británicas (Escocia, Gales, Inglaterra, Irlanda e Irlanda del Norte), así como en países como Australia, Fiyi, Nueva Zelanda, Papúa Nueva Guinea, Samoa, Sudáfrica y Tonga, Argentina y en Francia.

En otros países tiene variados grados de popularidad y competitividad internacional. En África también es popular, por influencia inglesa, en Namibia, Kenia y Zimbabue, y por influencia francesa en Túnez, Costa de Marfil, Madagascar y Marruecos. En América se practica principalmente en Argentina —participante del Rugby Championship—, donde tiene gran arraigo y cuya selección ha logrado importantes logros internacionales; asimismo tiene cierta popularidad en otros países americanos, como Canadá, Uruguay y Estados Unidos. En Asia, el equipo más destacado es el de Japón. En el resto de Europa, se destaca Italia, que participa en el Torneo de las Seis Naciones. Igualmente está difundido en otros países del continente europeo, sobre todo en Portugal, España y en países del Este europeo, como Rumania, Georgia o Rusia. En Oceanía, por influencia australiana y neozelandesa, el "rugby" es un deporte muy popular en Fiyi, Tonga y Samoa, cuna de jugadores destacados en el ámbito internacional.

A nivel internacional, el rugby es regulado por World Rugby, asociación federativa que cuenta con 118 miembros (plenos y asociados). Luego de participar en cuatro ediciones de los Juegos Olímpicos a comienzos del siglo XX, el rugby fue reincorporado a los deportes olímpicos a partir de Río 2016 en su modalidad de "rugby 7".

Desde los orígenes mismos del "rugby" y el fútbol actual, a mediados del siglo XIX, se definieron como el álter ego del otro: fuerza contra habilidad; juego limpio contra juego desleal, etc. Un antiguo dicho británico dice que "el fútbol es un juego de caballeros jugado por bestias y el rugby es un juego de bestias jugado por caballeros". En el rugby es característico el respeto a las reglas que deben practicar tanto los jugadores como el público, y las decisiones del árbitro rara vez son discutidas por los jugadores. Además, se fomenta la sociabilidad, dándose generalmente entre compañeros de equipos y oponentes una cordial reunión después de los partidos, denominada "tercer tiempo", junto con los árbitros, entrenadores y parte del público, para hablar acerca del partido.

El rugby moderno, al igual que el fútbol moderno, es una evolución directa del "fútbol medieval británico", también llamado en español "fútbol de carnaval" (en inglés "mob football", equivalente a "fútbol multitudinario"), un juego de pelota violento y reiteradamente prohibido, de reglas sumamente variables, que se practicaba popularmente en las islas británicas durante el medievo europeo, en el que se usaban tanto las manos como los pies, así como la fuerza

La tradición atribuye la invención del rugby a William Webb Ellis, un estudiante de teología del Colegio de Rugby y el trofeo que se entrega a los ganadores de la Copa del Mundo de Rugby lleva su nombre. El hecho es recordado en una lápida mural de la "public school" de Rugby.

Las "Leyes del Juego de Rugby" son dictadas por World Rugby ("International Rugby Board" o IRB hasta 2014). Su cuerpo central son las 22 leyes que regulan el juego: el terreno, la pelota, número de jugadores, vestimenta, tiempo, oficiales, modo de jugar, ventaja, modo de marcar, juego sucio, "off side" (fuera de juego) y "on side" (en juego), pase forward (pase adelantado o "avant"), salidas, pelota al suelo sin tackle ("placaje"), tackle ("placaje"): portador de la pelota derribado, "ruck", "maul", "mark", "touch" y "line-out", "scrum" ("melé"), penales y "free kicks" y "tries" (ensayos).

Las "Leyes del Juego de Rugby" también están integradas por un Prólogo, una lista de definiciones de los términos utilizados en las reglas, un apartado de variaciones para menores de 19 años, otro apartado para ""seven a side"" (variante de siete jugadores por bando), señales de los árbitros, y un extracto de la Regulación 12, sobre vestimenta de los jugadores. La publicación oficial de World Rugby, asimismo, está acompañada de un «Documento del Juego», complementario de las leyes, que "cubre los principios básicos del Rugby".

En 2008, la IRB aprobó 13 modificaciones sustanciales, las "Experimental Law Variations" (Variaciones Experimentales de las Leyes), conocidas por su sigla en inglés, ELV al reglamento, y que fueron puestas en práctica en todos los torneos oficiales del mundo a partir del 1 de agosto de 2008. En 2009, la IRB incorporó 10 de las 13 ELV a las Leyes del Juego, dejando sin efecto las otras tres.

En español existen dos traducciones del reglamento:

Como las dos traducciones son distintas, la terminología reglamentaria en español varía según se trate de algún país hispanoamericano o España. Algunos de los principales términos en que se registran diferencias son los siguientes, según se trate de Hispanoamérica o España: try/ensayo, conversión/transformación, penal/transformación de golpe, scrum/melé, tackle/placaje, hooker/talonero, fullback/zaguero, ala/flanker, wing/ala, etc.

En el rugby se enfrentan dos equipos de quince jugadores cada equipo (aunque hay una variación para un juego de siete). El campo de juego tiene forma rectangular y es de césped (aunque puede ser de arena, tierra, nieve o césped artificial). Sus medidas son de un máximo de 95 metros de largo y 65 de ancho. Al campo de juego se le suman dos áreas, la zona de anotación (o "in-goal"), en cada uno de los extremos, de no más de 22 metros cada una, destinada a apoyar la pelota para obtener el "try" o "ensayo", principal anotación del juego.
En los dos extremos del campo, en el centro de la "línea de anotación", se encuentran instalados dos postes separados entre sí por 5,6 metros y unidos por un travesaño situado a 3 metros de altura. Los postes deben tener un mínimo de 3,4 metros de alto, lo que le da al conjunto de los tres palos una forma de H.

La pelota o balón es de forma ovalada, está construida con cuatro gajos de cuero o material sintético parecido y pesa algo menos de medio kilo. Los partidos, en la modalidad de quince jugadores, duran ochenta minutos, divididos en dos tiempos iguales (setenta minutos para las categorías juveniles menores de 19 años).

Un campo de juego de rugby es rectangular, y no debe exceder de 95 metros de largo por 65 metros de ancho. Las líneas laterales (denominadas "líneas de "touch"") del campo de juego no forman parte de este. A continuación de cada uno de los lados menores del rectángulo hay una zona de anotación (o de ensayo), denominada ""in-goal"", con una longitud de entre 10 y 22 m. Entre el campo de juego y estas zonas de anotación hay una línea continua, denominada "línea de "goal"" (de marca, o de anotación, o de gol), que es parte de las últimas y en cuyo centro se ubican los postes de gol. Estos postes verticales están separados entre sí por una distancia de 5,6 m y unidos a 3 m de altura por un travesaño. La altura de los postes depende del gusto del equipo local, aunque en cualquier caso debe sobrepasar los 3,4 metros. El conjunto del campo de juego y las áreas de gol se denomina "área de juego". El área de juego, las líneas no incluidas en ella (las líneas de "touch" y las líneas laterales y finales que limitan el "in-goal", denominadas líneas de "touch in-goal" y líneas de pelota muerta respectivamente), y un área perimetral de 5 m de ancho alrededor del conjunto anterior, se denomina "terreno de juego".

En mitad del campo, paralela a las líneas de gol, se ubica una línea continua denominada "línea de mitad de cancha". En el centro de esta, una línea perpendicular marca el centro del campo. A 10 m a cada lado de la línea de mitad de campo existe una línea discontinua paralela, la cual se utiliza como referencia para las salidas, ya que el balón debe superar dicha línea para considerarse en juego. De cada lado hay otra línea continua entre ambas líneas de banda, paralela a la línea de gol y a 22 m de esta hacia el centro del campo. El espacio delimitado por esta línea y la de gol (excluyendo a esta) se denomina "las 22" o "zona de 22". Entre ambas líneas de banda, a 5 m de las líneas de gol y paralelas a esta hay líneas discontinuas. Finalmente, hay líneas discontinuas entre las anteriores, paralelas a las líneas laterales, a los 5 y 15 m de estas. Estas líneas señalan los límites para la posición del jugador más avanzado y más retrasado en los saques de banda.

En el rugby, los jugadores de cada equipo se dividen en dos grandes grupos: los "forwards" o delanteros y los "backs", zagueros o defensores.

Los "forwards" o delanteros, también referidos como "pack de forwards", son ocho jugadores, ubicados en la zona delantera del equipo. En general son los jugadores más grandes y pesados del equipo. Tienen como función específica disputar el scrum (melé) y los saques de lateral ("line out"). Los "forwards" se ubican en tres líneas: la primera línea está integrada por dos pilares (1 y 3) y un "hooker" o talonero (2) en el medio; la segunda línea está integrada por dos jugadores denominados con ese nombre (4 y 5); la tercera línea está integrada por tres jugadores, con el octavo en el medio, flanqueado por dos alas o "flankers" (6 y 7).

Los "backs", zagueros o defensores, son siete jugadores que se ubican en la zona posterior del equipo. En general son los jugadores más ágiles y rápidos del equipo. Cinco "backs" forman "la línea" o los "tres cuartos", ubicados en diagonal -con el fin de lograr velocidad en el avance- sucesivamente a partir del medio scrum o medio melé (9), el apertura (10), dos centros (12 y 13) y un "wing" -o ala en España- (14). En el extremo opuesto de la línea se ubica otro "wing" o "wing" ciego (11) y detrás al centro se ubica el "fullback" o zaguero (15).

El objetivo fundamental consiste en obtener una mayor cantidad de puntos que el adversario. Los puntos se pueden obtener del siguiente modo:

Un jugador, siempre que se encuentre en juego ("on side"), puede:

Una de las reglas fundamentales del rugby es el placaje (llamado "tackle" en el reglamento publicado por la IRB), regulado en la ley 15:
Las leyes del juego hacen especial hincapié en evitar y sancionar severamente el juego peligroso, aun cuando no sea intencional. El tackle no puede realizarse mediante un golpe directo con el hombro o un brazo rígido. La Ley 10(4)(e), prohíbe explícitamente el tackle alto, que es aquel por el cual el jugador que lleva la pelota es tomado por encima de la línea de los hombros, aun cuando el tackle se haya iniciado por debajo. La World Rugby tiene una política de tolerancia cero respecto del contacto con la zona de la cabeza.

A partir del 3 de enero de 2017 la WR estableció una serie de medidas y definiciones para reducir la discrecionalidad y aumentar las sanciones ante los tackles altos. En las nuevas medidas la WR define dos tipos precisos de tackle alto: el "tackle atolondrado" ("reckless tackle") y el "tackle accidental". Se considera "tackle atolondrodado" cuando el jugador sabía o debería haber sabido que existía el riesgo de contacto con la cabeza y aun así siguió adelante y debe ser sancionado como mínimo con tarjeta amarilla (exclusión de la cancha durante un tiempo). Cuando el contacto con la cabeza sea accidental (tackle accidental), corresponde como mínimo cobrar penal.

Simultáneamente la WR definió una serie de normas educativas a tener en cuenta por todas las personas involucradas en el juego:


El juego se inicia con un puntapié de salida, que debe efectuarse de sobrepique, realizado desde el centro del campo. Todos los jugadores del equipo que efectúa la salida deben ubicarse por detrás de la pelota hasta que esta haya sido pateada, y los rivales a diez metros de distancia. El balón debe superar la distancia de diez metros sin salir del campo y botar o ser atrapado dentro de él. El juego general ha comenzado, y continuará hasta que se produzca una interrupción. El juego se interrumpe cuando la pelota ha quedado “muerta”: la pelota ha salido de los límites del área de juego, se ha marcado un tanto, se ha producido una anulada, se ha producido una infracción sin ventaja para el equipo no infractor, un jugador ha pedido una marca ("mark"), o se ha producido otra interrupción en el juego.

El silbato del árbitro marca los puntapiés de salida (no los de reinicio) y las pelotas muertas. El árbitro hace sonar su silbato también para indicar que ha detenido la cuenta del tiempo, por ejemplo para que un jugador lesionado sea atendido o para dar indicaciones al capitán de un equipo, y que la ha reiniciado.

Cuando el tiempo se agota, el juego continúa hasta que se haya producido una pelota muerta, salvo que esto fuera como consecuencia de una infracción castigada con penal, en cuyo caso deberá continuar bajo el mismo principio. El reinicio del juego en el segundo tiempo se produce también con un puntapié de salida, a cargo del equipo que no efectuó el del comienzo del partido.


Cuando el balón, o el jugador que lo lleva, salen del campo por la línea de "touch", el juego se reinicia mediante un saque de banda llamado "line out" que debe arrojarse recto y superando la línea ubicada a cinco metros campo adentro del "touch" entre dos hileras de jugadores, una de cada equipo y separadas por una distancia de un metro. Los jugadores deben saltar para obtener la pelota, pudiendo ser impulsados y sostenidos por sus compañeros. El lanzamiento le corresponde al equipo que no la envió afuera, salvo que haya sido consecuencia de un penal, en cuyo caso debe lanzar el equipo que pateó. El equipo que lanza la pelota decide también cuántos jugadores va a haber en la hilera (de 2 a 14), mientras que el otro equipo puede tener menos pero no más. Todos los demás jugadores, excepto el lanzador, un opuesto al lanzador del equipo rival, y un receptor por cada equipo, deben alejarse diez metros hacia su campo de la línea perpendicular al "touch" por donde se arrojará la pelota.

La posición en la que se efectuará el tiro no es necesariamente aquella en que el balón cruzó la línea de "touch". Si el balón fue pateado por un jugador por delante de su línea de 22 metros, o por detrás de ella cuando es el equipo defensor lo introdujo en su zona de 22 metros, entonces el lanzamiento debe efectuarse en línea con el lugar desde donde se pateó. En cualquier caso, el equipo que debe reponer la pelota en juego puede decidir efectuar un “tiro rápido” en cualquier lugar entre su línea de "goal" y la línea en que debe formarse el "line out"; pero para ello deben cumplirse ciertas condiciones: que se utilice el mismo balón que salió del campo, que no haya sido tocado por nadie excepto el jugador que lanza (y eventualmente por el jugador rival que salió fuera del campo con el balón), y que no haya principiado la formación del "line out". En el tiro rápido no se requiere que el balón sea arrojado paralelamente a las líneas de "goal": puede enviarse oblicuamente hacia la línea de "goal" del lanzador, pero debe superar la línea de cinco metros paralela al "touch".

La pelota no debe arrojarse intencionalmente con las manos al "touch". Esta acción es considerada juego sucio y castigada con un penal.

El scrum o la melé, una de las formaciones más reconocibles del rugby, es una puja frente a frente, de un grupo de cada equipo formado por un máximo de ocho y un mínimo de tres jugadores en tres líneas, que se enfrentan agazapados y asidos entre sí, para comenzar a empujar con el fin de obtener el balón que ha sido lanzado en medio de ellos y sin tocarlo con la mano. El grupo que haya obtenido el balón, debe sacarlo sin tocarlo con la mano por detrás de la formación, donde lo tomará un jugador (usualmente, pero no siempre, el "medio melé" o "medio scrum") y continuará el juego.

Tanto en el lanzado ("line out") como en el "scrum" (la melé), el sentido de las reglas es que exista disputa por la pelota. Esa es la diferencia con las infracciones mayores, que se penalizan con una patada de castigo (tiro a los postes, tiro afuera o puesta en juego), en la que el equipo infractor no puede intervenir.

Los "rucks" y los "mauls" son las formaciones grupales de lucha por la pelota que forman ambos equipos durante el desarrollo del juego. La diferencia entre ambos estriba en si la pelota se encuentra en poder de uno de los jugadores ("maul"), o si se encuentra en el suelo ("ruck").

El maul (ley 17) es una formación esencialmente ofensiva. Se produce cuando un jugador que lleva la pelota es asido por uno o más defensores, y hay uno o más compañeros del portador de la pelota asidos a este, todos ellos sobre sus pies (como mínimo deben ser dos atacantes y un defensor). Sus reglas son complejas, pero básicamente no debe dejar de moverse hacia la meta y con los defensores retrocediendo; si es detenido durante cinco segundos, algún jugador atacante debe abandonar el maul con la pelota o pasarla; en caso contrario, la acción del equipo atacante es castigada con un scrum a favor del defensor. Con un maul puede realizarse un try.

En la reglamentación vigente hasta agosto de 2008, esta formación no se podía derrumbar, por ser considerado juego peligroso, sancionándose en ese caso con penal. Entre el 1 de agosto de 2008 y el 1 de junio de 2009, se pusieron en vigencia trece Variaciones Experimentales Reglamentarias (ELVs), entre las que se incluyó una que permitía derrumbar el maul. Sin embargo, luego de ser examinadas durante la temporada 2008/2009 la IRB decidió no confirmar esta variación, volviendo a estar prohibido derribar el maul a partir del 1 de junio de 2009.
El ruck (ley 16) es una formación más orientada a la disputa de la pelota, pero cuando es ejecutada en serie, también se convierte en una herramienta ofensiva. El ruck se forma con la pelota en el suelo y con al menos un jugador de cada equipo chocando y pujando por la pelota, pero habitualmente son varios. El ruck lo forman los jugadores parados y enfrentados con sus contrincantes, que deben "ruquear" la pelota, esto es tratar de obtenerla. El ruck suele formarse cuando un jugador con la pelota es derribado; sus compañeros vienen entonces a proteger la posesión del balón, pasando un pie por encima de éste, tomando así posesión de la pelota y obligando al equipo contrario a pasar completamente por arriba del jugador derribado y correr a los jugadores contrarios para tomar la posición de la pelota. No se puede entrar lateralmente a esta formación ya que seria sancionado con un penal.

Cuando se forman un ruck o un maul, se forman también dos líneas imaginarias de fuera de juego. Estas líneas, paralelas a las de gol, pasan por detrás del pie más retrasado del último jugador de cada bando en el ruck o maul y van de una línea lateral a otra línea lateral. Cualquier jugador que esté delante de su respectiva línea de fuera de juego y no forme parte del ruck o maul se considera fuera de juego y puede ser penalizado si interviene directa o indirectamente en el juego. Al ruck y al maul solo se puede ingresar desde atrás de dichas líneas imaginarias.

El fuera de juego ("offside") es la infracción más común durante un encuentro. Si la penalización se otorga a una distancia razonable para el pateador del equipo no infractor, este puede decidir por patear hacia los postes para obtener tres puntos. El equipo infractor tiene que ubicarse a 10 m de distancia del equipo que patea y no puede hacer ningún movimiento ni ruido, ni siquiera levantar los brazos. Si la falta es convertida (transformada), el juego se reinicia en la línea de centro con un saque del equipo que cometió la infracción. Por el contrario, si la penalización no es convertida (transformada), normalmente se reinicia el encuentro desde los 22 m con una patada de botepronto ("drop") del equipo que cometió la infracción; esta patada se llama "salida de 22 metros". Otras penalizaciones frecuentes incluyen juego peligroso, interferencia, no soltar el balón en el suelo, y lanzarse sobre un ruck (montonera en el suelo). El equipo al que se le otorga la falta (pateador) puede reiniciar el juego con un pequeño toque con el pie (pasando la marca) para iniciar una jugada o con una patada a la línea de banda ("touch") para obtener un saque de banda. En este saque de banda, el equipo que pateó el balón tiene el derecho de lanzar el balón nuevamente en el line-out. Para infracciones menores (tales como adelantar el pie en el scrum), se otorga un tiro libre o "free kick". A diferencia del golpe de castigo ("penal"), este no puede patearse directamente a los postes para ganar puntos. Además, el equipo infractor puede cargar hacia el balón una vez el pateador haya hecho algún movimiento para patear el balón.

El rugby se ha caracterizado por una evolución dinámica de las leyes del juego. Regularmente se introducen modificaciones que tienen por intención agilizar el juego, hacerlo más atractivo para los espectadores y más seguro para los jugadores, y reducir los márgenes de error en los fallos arbitrales: el rugby ha sido el primer deporte colectivo en adoptar la revisión en video de las situaciones de difícil resolución, realizada por un cuarto árbitro a instancias del árbitro principal. En general, las modificaciones suelen probarse primero en un ámbito restringido (en los últimos años, la Universidad de Stellenbosch en Sudáfrica, para luego extenderse a un ámbito mayor (por ejemplo, algún torneo regional importante), para luego generalizarse como “variaciones experimentales”. La IRB analiza luego el resultado de este proceso, y finalmente se incorporan como leyes permanentes las variaciones experimentales que se hayan evaluado positivamente.

En 2008 la IRB aprobó una serie de modificaciones sustanciales al reglamento conocidas por su sigla en inglés, ELV (Experimental Law Variations), o Variaciones Reglamentarias Experimentales, que se pusieron en práctica en todos los torneos oficiales del mundo entre el 1 de agosto de 2008 y el 1 de junio de 2009. Tras de ser evaluadas globalmente durante la temporada 2008/2009, el IRB decidió confirmar 10 de las 13 variaciones e incorporarlas definitivamente a las Leyes del Juego, con excepción de las ELV 2, 3 y 6, que permitían derribar el "maul" y decidir libremente la cantidad de jugadores a colocar en el "line out" (saque de lateral).

Los cambios venían estudiándose desde 2004 y comenzaron a implementarse experimentalmente en 2006 en la universidad sudafricana de Stellenbosch, por lo que son referidas también como las "Reglas de Stellenbosch". De las muchas variaciones propuestas y ensayadas, la IRB decidió finalmente experimentar en todo el mundo trece reglas nuevas. De ellas, 10 fueran finalmente incluidas en las Leyes del Juego en 2009.

El 15 de mayo de 2012 la IRB sancionó once nuevas variaciones experimentales generales, a ser puestas en práctica a partir del 1º de septiembre de 2012 en los torneos del hemisferio norte, y del 1 de enero de 2013 en los del hemisferio sur, y una exclusivamente para la variante "seven a side" vigente a partir del 1 de junio de 2012.

Para la temporada 2013-2014 se produjo una modificación en lo referente a la entrada al "scrum" (ley 20.1 "Formación de un Scrum"), que comenzó a aplicarse al inicio de temporada en cada hemisferio. La variaciòn consiste en reemplazar el tiempo intermedio de la secuencia de entrada de tres tiempos ("cuclillas, tocar, ya") por "tomarse", de modo que los jugadores de las primeras lìneas estén efectivamente asidas entre sí y con la primera línea oponente, de modo de proporcionar mayor estabilidad a la formación antes de la introducciòn de la pelota.

La World Rugby decidió implementar a partir del 3 de enero de 2017 una serie de medidas con el fin de aumentar las precauciones para evitar el tackle alto, es decir el tackle en la que la persona tacleada es tomada o golpeada en la zona del cuello y la cabeza. Para ello la WR incorporó las figuras del "tackle atolondrado" ("reckless tackle") y del "tackle accidental" ("accidental tackle").

El rugby es un deporte de intenso contacto físico. Sin embargo, las reglas no permiten el uso de ninguna protección rígida, pues estas podrían causar lesiones a los jugadores. Solo se permiten protecciones acolchadas de hasta 5 mm de espesor en algunas zonas del cuerpo; estas protecciones deben ser aprobadas por el IRB. Normalmente se emplean un protector bucal de material siliconado; una camiseta elástica (usada por debajo de la camiseta del equipo) con protecciones para hombros y cuello, y a veces también para esternón, costillas, riñones, columna vertebral y bíceps; un casquete blando, destinado mayormente a reducir el efecto de los golpes en las orejas; y unas calzas cortas o medianas de contención. Se permite el uso de otras protecciones no rígidas y de espesor mínimo para prevenir lesiones, como rodilleras o tobilleras, o en algunos casos el uso de suspensorios para proteger los genitales de impactos dañinos.

La versión de este deporte más conocida es la del rugby jugado por equipos de quince jugadores, aunque no es la única. Es lo que se conoce en el mundo anglosajón como "rugby union", en referencia a la federación ("Union") de clubes que se rigen por unas mismas normas y que, tradicionalmente, habían sido universitarios o aficionados. Un partido dura 80 minutos, dividido en dos partes de 40 minutos, con un descanso de 15 minutos entre cada tiempo.

Sigue el modelo propuesto por William Webb Ellis. Por cada equipo juegan un total de 15 jugadores divididos en dos grupos: "forwards" o delanteros y "backs" o tres cuartos. Las denominaciones de los puestos, al igual que el resto de la terminología de juego, varía considerablemente entre España y los demás países hispanohablantes.

Los jugadores del 1 al 8 ("forwards") forman el "pack", la "delantera" o "paquete" para realizar el scrum (la melé):

"Primera línea": los jugadores que intentan llevar la pelota a su lado y que están en el choque; su función en los scrums es mantener el scrum estable, los pilares (números 1 y 3) suelen ser los más fuertes y pesados de entre todos los jugadores

"Segunda línea": generalmente los jugadores más altos del equipo y que se hacen cargo de empujar en los scrums, también suelen ser los encargados de ganar la pelota en los saques desde el lateral. ("touche", "line-out")

"Tercera línea": los jugadores que mantienen la formación equilibrada para que no se desarme cometiendo una falta.

"Línea de tres cuartos o "backs"": En los distintos países, estos jugadores reciben diferentes nombres de acuerdo con su propia tradición. Así, en Australia y Nueva Zelandia el apertura (nº 10) y el primer centro (nº 12) se denominan "first" y "second five eights", respectivamente.


Desde el siglo XIX existe en Inglaterra una variante cuyas reglas difieren en parte y en la que juegan equipos de 13 jugadores; éstos fueron profesionales prácticamente desde la implantación de esa modalidad. A ese juego se lo llamó "rugby league", en referencia al campeonato de liga en que se enfrentaban los clubes que remuneraban a sus jugadores. De Inglaterra pasó a algunos países de la esfera cultural y de influencia británica (Australia, Nueva Zelanda), así como a Francia.

Rugby League Football o Rugby a 13 es un deporte de equipo jugado por dos equipos de 13 jugadores, con 4 en el banco (reservas). El objetivo fundamental, como en el rugby de a 15, consiste en apoyar un balón ovalado en el suelo con las manos sobre o tras la línea de ensayo. Esto se denomina ensayo y tiene un valor, en Rugby League de 4 puntos. Tras el ensayo, el equipo anotador tiene el derecho de patear el balón hacia la portería adversaria, y si consigue pasarlo (transformación) entre los dos palos verticales y por encima del travesaño, anota 2 puntos más. También pueden conseguirse puntos tirando a palos tras un penalti, consistente en tirar a palos durante el juego abierto dejando previamente botar el balón en el suelo. En ambos casos su valor es de 1 punto. El equipo adversario intenta impedir al equipo de ataque realizar este gol obstaculizando al jugador con la pelota.

En áreas de Inglaterra donde el Rugby a 13 predomina - Yorkshire y el Noroeste - el uso del término rugby se refiere, por lo general, al rugby a 13, a diferencia de la mayor parte del país, donde este término se refiere al Rugby Union o Rugby a 15. En áreas de Australia y Nueva Zelanda donde predomina el Rugby a 13, el juego es comúnmente conocido como League o fútbol. En Francia, el juego es llamado el Rugby à Treize, que significa Rugby de a trece en francés. En Argentina, el nombre adoptado fue Rugby 13.

El Rugby a 13 fue jugado al principio por una facción que se escindió de la Federación Inglesa de Rugby (RFU) conocido como la Unión del Norte. Cuando se produjeron también escisiones similares en las federaciones de Rugby afiliadas a la RFU en Australia y Nueva Zelanda, en 1907 y 1908 formaron asociaciones conocidas como Rugby Leagues y usaron las reglas de la Unión del Norte modificadas. La Unión del Norte más tarde cambió su nombre a la Rugby Football League. Así, el juego se hizo conocido como la Rugby League.

El formato de rugby de 7 ("seven-a-side") se juega normalmente en torneos cortos (de un día o un fin de semana). Se utiliza el mismo campo que en la modalidad de 15 hombres, pero con solo 7 jugadores por equipo. Las variaciones respecto de las reglas del juego de quince son:
Actualmente existe un Campeonato del Mundo de Rugby a 7 y un circuito mundial, y ha sido aceptado como deporte olímpico para las Olimpiadas de 2016.

El rugby es un deporte en el que tradicionalmente se ha dado gran importancia a los valores morales. Las normas oficiales del juego están integradas por lo que se denomina "Documento del Juego", orientado a garantizar la conducta ética de todos los involucrados en el juego, "tanto dentro como fuera del campo". Una muestra de la importancia de los valores éticos en el rugby es la disposición referida al espíritu del juego que está incluida en el Documento:

Desde temprana edad a los jugadores de rugby se les enseñan una serie de cualidades positivas, como son el compañerismo, la honestidad, el respeto, la disciplina, la lealtad, el sacrificio y el altruismo.

A diferencia de otros deportes de equipo, en el rugby los jugadores no suelen discutir a los árbitros sus decisiones, ni tratan de engañarlos para sacar partido de sus decisiones. Los tantos son necesariamente consecuencia del esfuerzo de todos, por lo que no se producen las celebraciones individuales, tras la consecución de un try (ensayo) o una conversión (transformación), que se producen en otros deportes.

Al final del partido los jugadores de ambos equipos confraternizan juntos en el llamado «tercer tiempo», en el que beben y comen juntos por invitación del equipo local.

En el marco del cierre de la World Rugby Conference and Exhibition que se celebró en Londres el día 19 de noviembre de 2014, la IRB, la asociación madre del rugby mundial, presentó a la luz su nuevo programa de cambio de marca, y pasó oficialmente a llamarse World Rugby.
En el corazón de la marca hay un posicionamiento distinto, expresado visualmente a través de un logo más moderno y progresista que encarna la misión de World Rugby para hacer crecer el juego en todo el mundo, manteniendo un vínculo con el patrimonio de la organización a través de su combinación de colores azul y verde.
Esta nueva denominación es más inclusiva, ya que World Rugby abarca los fanes, jugadores aficionados, jugadores profesionales, árbitros y todos aquellos que de una u otra manera están relacionados al este deporte, unidos bajo los mismos valores e invita a nuevos públicos en todo el mundo.
El presidente de World Rugby, Bernard Lapasset, dijo: «El rugby ha crecido mucho en los últimos cuatro años, alcanzando una participación global de 6,6 millones de jugadores, impulsado por el éxito comercial de la Rugby World Cup, las estrategias de desarrollo de World Rugby y sus inversiones récord, la fortaleza de las Uniones y el regreso del rugby al Programa Olímpico».
El anuncio fue realizado en el cierre de una exitosa World Rugby Conference and Exhibition, que reunió a más de 700 delegados de 60 países durante más de dos días en los que participaron de talleres que invitaron a la reflexión y charlas que cubrieron los temas más importantes del rugby actual: el bienestar de jugador, la integridad para el futuro de la Copa del Mundo de Rugby y la realización de un excepcional evento de Seven en Río 2016.

Por lo general, y a diferencia de otros deportes, las selecciones nacionales de rugby tienen unos apodos afectivos por los que son conocidos sus equipos. Las de los diez primeros equipos según la clasificación del IRB a finales de octubre de 2007 son:


El rugby fue incluido como deporte olímpico a iniciativa del Barón Pierre de Coubertin, impulsor de las Olimpiadas modernas, quien había sido árbitro de la final de 1892, entre Stade Français y Racing Club de France. Estuvo presente en los Juegos Olímpicos de París 1900, Londres 1908, Amberes 1920 y París 1924. Las causas de su exclusión fueron la mínima cantidad de países participantes (solo 3 en 1924), el debut de las mujeres en los Juegos Olímpicos de Ámsterdam 1928, y el mayor énfasis del COI en los deportes individuales.


Rugby por país




</doc>
<doc id="2457" url="https://es.wikipedia.org/wiki?curid=2457" title="Red Hat">
Red Hat

Red Hat, Inc. es una multinacional americana de software que provee software de código abierto principalmente a empresas. Fundada en 1993, Red Hat tiene su sede corporativa en Raleigh, North Carolina, con oficinas satélite en todo el mundo.

Red Hat es conocida en gran medida por su sistema operativo empresarial Red Hat Enterprise Linux y por la adquisición del proveedor de middleware empresarial de código abierto JBoss. Red Hat también ofrece Red Hat Virtualization (RHV), un producto de virtualización empresarial. Red Hat proporciona almacenamiento, plataformas de sistemas operativos, middleware, aplicaciones, productos de administración y servicios de soporte, capacitación y consultoría.

Red Hat crea, mantiene y contribuye a muchos proyectos de software libre. Ha adquirido muchos productos de software propietario, a través de funciones y adquisiciones y ha liberado el código de estos aplicativos como código abierto. Red Hat es el segundo mayor contribuyente al kernel de Linux versión 4.14, tras Intel.

Sus principales productos son la distribución Red Hat Enterprise Linux, el servidor de aplicaciones libre JBoss, la herramienta de Mapeo objeto-relacional Hibernate y más soluciones en el ámbito de los servidores.

Por otra parte Red Hat patrocina y dirige la distribución Fedora, la cual usa para probar nuevas tecnologías. También participa en el proyecto "One Laptop per Child" y mantiene el sitio web Red Hat Magazine.

Red Hat Software Inc. fue fundada en 1994 por Bob Young y Marc Ewing. En agosto de 1999, Red Hat salió a bolsa y sus acciones obtuvieron la octava ganancia de primer día más grande en toda la historia de Wall Street. Cuatro años más tarde, las acciones de Red Hat se valuaron en alrededor de una centésima parte del máximo valor que llegaran a alcanzar antes de la crisis de las puntocom. Aun así, sus comienzos exitosos en el mercado de valores sirvieron para que Red Hat fuera portada en periódicos y revistas no directamente relacionadas con temas informáticos. En cualquier caso, parece ser que Red Hat ha sabido superar los problemas de otras compañías del mundo de los negocios en torno al software libre y anunció números negros por primera vez en su historia en el último cuarto del año 2002. 

Otro de los hechos históricos más importantes de Red Hat fue la adquisición en noviembre de 1999 de Cygnus Solutions, una empresa fundada una década antes y que ya había demostrado cómo con una estrategia integral basada en software libre se puede ganar dinero. 

En septiembre de 2003, Red Hat decidió concentrar sus esfuerzos de desarrollo en la versión corporativa de su distribución y delegó la versión común a Fedora Core, un proyecto abierto independiente de Red Hat, pero patrocinado por la empresa.




</doc>
<doc id="2459" url="https://es.wikipedia.org/wiki?curid=2459" title="Revisión por pares">
Revisión por pares

En los medios académicos (véase ciencia), la revisión por pares (en inglés: "peer review"), también denominada arbitraje, es un método usado para validar trabajos escritos y solicitudes de financiación con el fin de evaluar su calidad, originalidad, factibilidad, rigor científico, etcétera, antes de su publicación (véase revista científica) o aprobación.

Este método deja abierto el trabajo al escrutinio, y frecuentemente a la anotación o modificación, por autores de rango semejante o superior al del autor. Generalmente se considera válida una publicación científica sólo cuando ha pasado por un proceso de arbitraje como el de admisión para publicación en una revista arbitrada.

El arbitraje somete un trabajo o idea propuestos por los autores al escrutinio de uno o más expertos en el tema. Estos árbitros responden con una evaluación del trabajo, que comúnmente incluye sugerencias acerca de cómo mejorarlo (aunque está prevista la posibilidad de aceptarlo tal como está), la cual se envía al editor u otro intermediario. Generalmente, la mayoría de los comentarios de los árbitros se remiten a los autores.

Las evaluaciones habitualmente incluyen una recomendación explícita referente a lo que debe hacerse con el "manuscrito", la cual se escoge entre varias opciones propuestas por el editor, que generalmente representa una revista, una conferencia arbitrada, una agencia de financiamiento de programas de investigación o una editorial. Las opciones propuestas son generalmente las siguientes:


Durante el proceso de revisión, la función de los árbitros es consultiva. Su opinión no es vinculante (obligación formal) para el editor. Más aún, en las publicaciones científicas, los árbitros no actúan como grupo; no se comunican entre ellos. Generalmente no tienen conocimiento de la identidad ni de los resultados de sus colegas. En general no es necesario lograr consenso. Por ello, la dinámica del grupo es bien diferente a la de un jurado. En ocasiones su opinión no es unánime. En esos casos se pueden aplicar diferentes opciones para tomar una decisión.

Ha existido una posibilidad adicional para quien desea publicar su artículo en alguna revista: mediante una tarifa por página, pero se advierte que éste fue el procedimiento y que el trabajo no es arbitrado.

Tradicionalmente, la función de los árbitros ha sido anónima, pero poco a poco se ha ido abandonando este secretismo. En algunas ramas científicas muchas de las revistas arbitradas ofrecen ahora al árbitro la opción de mantenerse anónimo o de revelar su identidad. Por ello algunos trabajos pueden incluir una sección de agradecimientos, donde a los árbitros se les nombra según su contribución a mejorar el artículo.

En una revista o en una empresa de edición de libros, generalmente la tarea de selección de árbitros recae sobre el editor o en el consejo editorial. Al llegar un nuevo "manuscrito," el editor solicita los árbitros, seleccionados entre académicos u otros expertos en el tema. Los árbitros no necesariamente han escogido participar de esa manera en el proceso de publicación de la revista o casa editorial.

Algunas veces se les selecciona por ser conocidos en la materia de la publicación, o por haber publicado trabajos relativos a esa disciplina, o por recomendación de otros árbitros. Como paso previo a la recepción de solicitudes, las agencias de financiamiento generalmente seleccionan sus árbitros mediante un comité de revisión.

Habitualmente se evita seleccionar árbitros entre los investigadores cercanos o relacionados con los autores. Se espera que estos informen a los editores acerca de potenciales conflictos de interés para realizar la evaluación. Algunos editores o publicaciones solicitan a los autores una lista de posibles árbitros, así como de personas que ellos consideren inapropiadas para arbitrar su trabajo.

Generalmente se les pide incluir justificaciones de su elección. La razón de esto es que puede ocurrir que el tema de un trabajo sea tan especializado que los editores no puedan por ellos mismos ubicar especialistas en el tema. La selección de los autores se toma como sugerencia. No compromete en modo alguno a los editores.

El proceso que siguen las publicaciones científicas es casi siempre el mismo:

Sin embargo, algunas publicaciones en medicina han adoptado recientemente un modelo de acceso libre: en Internet publican el historial de los artículos, que incluye:

Luego de la revisión y de la resolución de empates ocurridos, el resultado del proceso de evaluación por pares puede ser la aceptación o el rechazo del "manuscrito". En algunos casos se propone una tercera opción: solicitud a los autores para que finalmente revisen el documento según los requerimientos específicos de cambios recomendados por los árbitros.

En algunas disciplinas, como la computación, se efectúan conferencias arbitradas. Para lograr la admisión a impartir una disertación, los científicos deben proponer un artículo, generalmente corto: de 15 páginas o menos. Un "comité de programa" (equivalente de un consejo editorial) revisa el artículo. Generalmente solicita la participación de árbitros para evaluar los artículos propuestos. Estos eventos suelen demandar fechas máximas de recepción, para permitir que el arbitraje sea oportuno en tiempo y que los autores de los artículos aceptados puedan planificar su participación en tales acontecimientos.

Debido a que generalmente los árbitros no reciben remuneración y a que toman tiempo de sus actividades principales, tales como sus propios trabajos de investigación, la selección y el reclutamiento de árbitros es un «arte político».

Una ventaja para convencer a los árbitros potenciales es que ellos mismos son también autores, o al menos lectores. Saben que el sistema de publicaciones requiere que los expertos donen parte de su tiempo. Los editores toman en cuenta que, por una parte, los autores que publican artículos en revistas arbitradas han demostrado su nivel, y por otra parte conocen la importancia del sistema de arbitraje. Por ello representan una fuente de árbitros potenciales.

Igualmente las agencias de financiamiento tienden a solicitar la participación como árbitros a investigadores a quienes ya se ha concedido tal suministro. Además, haber servido como árbitro es un elemento adicional en el currículo de un investigador o de un profesor.

Otro problema que surge para que un editor logre arbitrajes adecuados en algunos temas es poca disponibilidad de candidatos realmente calificados. En esos casos es difícil mantener el anonimato de los árbitros y se dificulta evitar conflictos de interés. También aumentan las posibilidades de que un editor no logre reclutar verdaderos expertos en esa materia: gente que haya realizado trabajos del nivel y del tema del evaluado, y que pueda «leer entre líneas». En general las publicaciones de menor prestigio y las agencias que otorgan menores medios de financiamiento afrontan mayores dificultades para atraer a verdaderos expertos como árbitros.

Otra dificultad es el anonimato de los árbitros. En los medios científicos es importante dar crédito a los trabajos realizados. Si bien se considera honorífico servir de árbitro para una publicación prestigiosa, no es posible acreditarse el arbitraje de una publicación en particular. Afortunadamente el medio más importante para obtener buena reputación en los medios científicos es la publicación de trabajos.

Aun cuando el arbitraje puede ser muy riguroso en términos cualitativos de un trabajo, al final la decisión de publicarlo o de financiarlo recae en el editor, y está sometida a algunas restricciones. Por ejemplo, si el espacio para publicar los artículos es limitado (como los de conferencias científicas) o si hay muchas solicitudes de financiamiento, puede ocurrir la no aceptación de trabajos con la calidad necesaria o negación de financiamiento a proyectos bien sustentados.

Inversamente, puede suceder que una publicación no haya recibido suficientes trabajos claramente publicables y decida aceptar mayor cantidad de artículos calificados «con aceptación condicionada».

En publicaciones como "Science" y "Nature" se dispone de un sistema de arbitraje muy restrictivo. A veces, cuando evalúan que un artículo no representa avance significativo en su ramo, ocurre que lo rechazan, aunque sea de buena calidad científica. Otras publicaciones, como el "Astrophysical Journal" y la "Physical Review," utilizan el arbitraje para eliminar trabajos con errores obvios o sin sentido.

La tasa de artículos aceptados denota este tipo de criterios. Por ejemplo, de los artículos sometidos a evaluación, en "Nature" se acepta sólo el 5%, y en "Astrophysical Journal" se publica cerca del 70%. Las diferentes tasas de aceptación también se notan en la cantidad de páginas de las publicaciones.

Con el fin de preservar la integridad del proceso de arbitraje, en algunas publicaciones los árbitros no conocen la identidad de los autores. De este modo se espera que en la decisión no influyan prejuicios por el prestigio autoral. Mediante esta modalidad de arbitraje, la versión enviada a revisión no debe contener referencias que revelen a los árbitros la identidad de los autores.

La razón por la cual no todas las publicaciones utilizan este estilo de arbitraje es que existen muchos indicios que de todos modos pueden revelar la identidad de los autores, tales como:

Una de las críticas graves al proceso de arbitraje se refiere a su lentitud. Por lo general entre la recepción del artículo y su publicación transcurren varios meses; en los relativos a algunas ciencias, varios años, por lo cual es posible que hayan perdido vigencia.

En la práctica, buena parte de la comunicación de nuevos resultados en ramas como la astronomía ya no se realiza mediante artículos sometidos a arbitraje, sino por prepublicaciones enviadas a servidores electrónicos tales como ArXiv. Otro error grave de tal procedimiento de revisión fue la explicación contradictoria y pseudocientífica de Eliyahu Rips acerca de la existencia de códigos criptográficos en la Biblia.

Además algunos estudios en Sociología de la Ciencia y la Tecnología argumentan que el arbitraje puede ejercer control en lo que se publica a las élites, y estar influida por intereses personales. Los árbitros tienden a ser especialmente críticos con las conclusiones cuando éstas contradicen sus propios puntos de vista, y más condescendientes con las que están en correspondencia con sus propias ideas.

Además la probabilidad de selección de los científicos más conocidos como árbitros es mayor, particularmente en las revistas más prestigiosas. Ello puede resultar en establecimiento de una especie de línea editorial que podría considerarse no neutral. Una consecuencia adversa de este método selectivo es que, según las observaciones de Thomas Kuhn acerca de las revoluciones científicas, se tiende a no publicar los artículos más revolucionarios en algunas «áreas», en grandes revistas, sino en otras más receptivas de ideas y trabajos innovadores.

Sin embargo, otros opinan que la cantidad de publicaciones científicas es suficientemente amplia de modo que se dificulta que una élite pueda controlar lo que se publica en cualquier área del conocimiento. Además, el proceso de toma de decisión fundamentado en el arbitraje, en el que cada árbitro da su opinión sin conocer a los restantes, ayuda a evitar estos problemas.

El arbitraje es parte integral del proceso de publicación de trabajos científicos sólo desde mediados del siglo XX . Anteriormente su aplicación era opcional. Por ejemplo, no se arbitraron los revolucionarios artículos de Albert Einstein "Annus Mirabilis," en el número de 1905 de "Annalen der Physik". Max Planck, jefe del consejo editorial de la publicación (y padre de la teoría cuántica), reconoció la importancia de los «manuscritos» recibidos y sencillamente los mandó a publicar.

Él o su coeditor Wilhelm Wien —ambos ciertamente «pares»— tomaron directamente la decisión de publicarlos. Posteriormente obtuvieron el de Física: Wien, en 1911; Planck, en 1918. Sin embargo, este proceso abreviado no corresponde con el método de arbitraje actualmente en vigencia. En aquella época existían criterios más favorables hacia autores que ya habían publicado trabajos. En un editorial reciente en la revista Nature se dice: «[...] en las revistas actuales, el peso de la prueba lo llevan los oponentes, en lugar de los proponentes de nuevas ideas».

Dado que la adopción del arbitraje es reciente, muchos de los trabajos científicos trascendentes no pasaron por el proceso de arbitraje. Asimismo existen trabajos posteriores muy importantes, que no pasaron por este proceso. Entre ellos destaca el siguiente:

Artículo de Watson y Crick's, de 1951, acerca de la estructura del ADN, publicado en "Nature". John Maddox afirmó: «el artículo de Watson y Crick no fue arbitrado en "Nature" [...] el artículo no tenía que ser arbitrado: su corrección era autoevidente. Ningún árbitro que trabajara en esa área podría mantener la confidencialidad una vez que viera la estructura» (Nature 426:119 (2003). Los editores aceptaron el artículo al recibir una carta de presentación del influyente físico William Lawrence Bragg, en la cual se sugería publicarlo.

En el proceso de arbitraje de las publicaciones científicas se supone honestidad de quien escribió el artículo en revisión, de manera que no está diseñado para detectar fraudes. Por lo general, los árbitros no disponen de acceso completo a los datos a partir de los cuales se obtuvieron los resultados del trabajo. Salvo quizás en temas como las matemáticas, esto predispone a aceptar como ciertos algunos resultados.

No se conoce la cantidad ni la proporción de artículos fraudulentos que logran publicación. En algunos casos los engaños (véase estafa y falsificación) se han descubierto solo cuando otros grupos de investigadores, mediante procedimientos iguales, no han logrado reproducir los resultados de trabajos publicados.

Un ejemplo de esto es el caso de Jan Hendrik Schön. Se cumplió el proceso de arbitraje en las prestigiosas revistas "Nature" y "Science," tras lo cual se aceptaron quince artículos para publicación. Todos resultaron fraudulentos. Luego se los retiró. Se descubrió el engaño después de su publicación, cuando otros grupos de investigadores no obtuvieron los resultados descritos en los artículos.

Un ejemplo de lo que puede ocurrir cuando una revista carece de un proceso de arbitraje adecuado es el caso de la publicación del profesor de física Alan Sokal, de la Universidad de Nueva York, en la revista "Social Text", titulada "Transgressing the Boundaries: Toward a Hermeneutics of Quantum Gravity". El resultado fue que el artículo consistía en una broma, ahora conocida como escándalo Sokal.

Otro caso famoso es el escándalo Bogdanov sobre el contenido de una serie de artículos sobre física teórica escritos por los hermanos gemelos franceses Igor y Grichka Bogdanov. Dichos artículos se publicaron en acreditadas revistas científicas y culminaron con la proposición de una teoría que describía lo ocurrido antes del "Big Bang". La controversia se inició en 2002 cuando en distintos grupos de noticias de Usenet se afirmó que dichos artículos eran un engaño ("hoax") dirigido contra la comunidad de físicos. Aunque los Bogdanov defendieron la veracidad de su trabajo, algunos físicos afirmaron que los artículos carecían de sentido y pusieron en duda el sistema de revisión por pares que aceptó publicar dichas investigaciones. Posteriormente alguna de las revistas retiró de sus repositorios artículos de los Bogdanov, y justificó arguyendo que el proceso de revisión no había sido adecuado.

En el 2017, el consejo editorial de la revista médica "Tumor Biology" rechazó 107 artículos de investigación al descubrir que los autores habían falsificado la revisión por pares.

Diferentes métodos de desarrollo de software incluyen etapas que comprenden arbitraje. Incluyen definición de requerimientos, diseño detallado y desarrollo de código. Uno de los enfoques muy rigurosos es el denominado inspección de software. En el «movimiento» de software libre se utiliza un procedimiento semejante al arbitraje, pues quien desee puede revisar, criticar y mejorar el software.

En este contexto, para la función del arbitraje existe una homóloga: la ley de Linus, que generalmente se expresa así: «Dados suficientes ojos, todo error es superficial». Esto se interpreta como «Con suficientes revisores, cualquier problema puede resolverse fácilmente». Eric S. Raymond, en su libro "La Catedral y el Bazar," reflexiona acerca de los beneficios de la aplicación del arbitraje en el desarrollo de software, en virtud de que permite encontrar defectos mucho más rápidamente que por "testing" o por informes de los usuarios acerca de errores. Esto minimiza tiempo, esfuerzo y los costos inherentes.






</doc>
<doc id="2463" url="https://es.wikipedia.org/wiki?curid=2463" title="Teoría de la relatividad especial">
Teoría de la relatividad especial

La teoría de la relatividad especial, también llamada teoría de la relatividad restringida, es una teoría de la física publicada en 1905 por Albert Einstein. Surge de la observación de que la velocidad de la luz en el vacío es igual en todos los sistemas de referencia inerciales y de obtener todas las consecuencias del principio de relatividad de Galileo, según el cual cualquier experimento realizado, en un sistema de referencia inercial, se desarrollará de manera idéntica en cualquier otro sistema inercial.

La teoría es "especial", ya que sólo se aplica en el caso especial donde la curvatura del espacio-tiempo debido a la gravedad es despreciable. Con el fin de incluir la gravedad, Einstein formuló la relatividad general en 1915. La relatividad general es capaz de manejar marcos de referencia acelerados, algo que no era posible con las teorías anteriores.

La Teoría de la relatividad especial estableció nuevas ecuaciones que facilitan pasar de un sistema de referencia inercial a otro. Las ecuaciones correspondientes conducen a fenómenos que chocan con el sentido común, como son la contracción espacial, la dilatación del tiempo, un límite universal a la velocidad, la equivalencia entre masa y energía o la relatividad de la simultaneidad entre otros, siendo la fórmula E=mc o la paradoja de los gemelos dos de los ejemplos más conocidos.

La relatividad especial tuvo también un impacto en la filosofía, eliminando toda posibilidad de existencia de un tiempo y de un espacio absoluto en el conjunto del universo.

A finales del siglo XIX los físicos pensaban que la mecánica clásica de Newton, basada en la llamada relatividad de Galileo Galilei (origen de las ecuaciones matemáticas conocidas como transformaciones de Galileo), describía los conceptos de velocidad y fuerza para todos los observadores (o sistemas de referencia). Sin embargo, Hendrik Lorentz y un poco antes Woldemar Voigt habían comprobado que las ecuaciones de Maxwell, que gobiernan el electromagnetismo, no cumplían las transformaciones de Galileo cuando el sistema de referencia inercial varía (por ejemplo, cuando se considera el mismo problema físico desde el punto de vista de dos observadores que se mueven uno respecto del otro). En particular las ecuaciones de Maxwell parecían requerir que la velocidad de la luz era constante (razón por la que se interpretó que esa velocidad se refería a la velocidad de la luz respecto al éter). Sin embargo, el experimento de Michelson y Morley sirvió para confirmar que la velocidad de la luz permanecía constante para cualquier velocidad y movimiento relativo al supuesto éter omnipresente y, además, independientemente del sistema de referencia en el cual se medía (contrariamente a lo esperado de aplicar las transformaciones de Galileo) . Por tanto la hipótesis del éter quedaba descartada y se abría un problema teórico grave asociado a las transformaciones de Galileo. Hendrik Lorentz ya había encontrado que las transformaciones correctas que garantizaban la invariancia no eran las de transformaciones de Galileo, sino las que actualmente se conocen como transformaciones de Lorentz.

Durante años las transformaciones de Lorentz y los trabajos de Henri Poincaré sobre el tema quedaron inexplicados hasta que Albert Einstein, un físico desconocido hasta 1905, sería capaz de darles una interpretación considerando el carácter relativo del tiempo y el espacio. Einstein también había sido influido por el físico y filósofo Ernst Mach. Einstein leyó a Ernst Mach cuando era estudiante y ya era seguidor suyo en 1902, cuando vivía en Zúrich y se reunía regularmente con sus amigos Conrad Habicht y Maurice Solovine (Véase Academia Olimpia). Einstein insistió para que el grupo leyese los dos libros que Mach había publicado hasta esa fecha: "El desarrollo de la mecánica" (título original, "Die Mechanik in ihrer Entwicklung", Leipzig, 1883) y "El análisis de las sensaciones" ("Die Analyse der Empfindungen und das Verhältnis des Physischen zum Psychischen", Jena, 1886). Einstein siempre creyó que Mach había estado en el camino correcto para descubrir la relatividad en parte de sus trabajos de juventud, y que la única razón por la que no lo había hecho fue porque la época no fue la propicia. El artículo de 1905 de Einstein, titulado "Zur Elektrodynamik bewegter Körper", cambió radicalmente la percepción del espacio y el tiempo que se tenía en ese entonces. En ese artículo Einstein introducía lo que ahora conocemos como teoría de la relatividad especial. Esta teoría se basaba en el principio de relatividad y en la constancia de la velocidad de la luz en cualquier sistema de referencia inercial. De ello Einstein dedujo las ecuaciones de Lorentz. También reescribió las relaciones del momento y de la energía cinética para que éstas también se mantuvieran invariantes.

La teoría permitió establecer la equivalencia entre masa y energía y una nueva definición del espacio-tiempo. De ella se derivaron predicciones y surgieron curiosidades. Como ejemplos, un observador atribuye a un cuerpo en movimiento una longitud más corta que la que tiene el cuerpo en reposo y la duración de los eventos que afecten al cuerpo en movimiento son más largos con respecto al mismo evento medido por un observador en el sistema de referencia del cuerpo en reposo.

En 1912, Wilhelm Wien, premio Nobel de Física de 1911, propuso a Lorentz y a Einstein para este galardón por la teoría de la relatividad, expresando Einstein no recibió el premio Nobel por la relatividad especial pues el comité, en principio, no otorgaba el premio a teorías puras. El Nobel no llegó hasta 1921, y fue por su trabajo sobre el efecto fotoeléctrico.


La fuerza del argumento de Einstein está en la forma en que se deducen de ella resultados sorprendentes y plausibles a partir de dos simples hipótesis y cómo estas predicciones las confirmaron las observaciones experimentales. Matemáticamente hablando, en ambos postulados, tomados en conjunto, implicaban que cualquier ley física debía ser invariante respecto a una transformación de Lorentz. Es decir, que en todos los sistemas inerciales la forma matemática de las ecuaciones debía ser forminvariante de Lorentz.

Cuando se aplican estos dos principios a las ecuaciones de Maxwell se ve que éstas sólo son invariantes bajo las transformaciones de Lorentz, lo que implica que el intervalo de tiempo entre dos sucesos o la distancia entre dos puntos deben ser relativos al observador. Es decir, no todos los observadores medirán el mismo intervalo de tiempo entre dos sucesos o la misma longitud para un mismo objeto. Ese carácter no absoluto, sino relativo del espacio y el tiempo, que es una consecuencia de requerir que las medidas tomadas por diferentes observadores dejen invariantes las ecuaciones de Maxwell es la fuente de todos los resultados sorprendentes de la teoría de la relatividad. Cuando se examinan las leyes de Newton y otras leyes del movimiento de la mecánica clásica se aprecia que estas deben ser modificadas para ser también invariantes según las mismas transformaciones que las ecuaciones de Maxwell.

Henri Poincaré, matemático francés, sugirió a finales del siglo XIX que el principio de relatividad establecido desde Galileo (la invariancia galileana) se mantiene para todas las leyes de la naturaleza. Joseph Larmor y Hendrik Lorentz descubrieron que las ecuaciones de Maxwell, la piedra angular del electromagnetismo, eran invariantes solo por una variación en el tiempo y una cierta unidad longitudinal, lo que produjo mucha confusión en los físicos, que en aquel tiempo estaban tratando de argumentar las bases de la teoría del éter, la hipotética substancia sutil que llenaba el vacío y en la que se transmitía la luz. El problema es que este éter era incompatible con el principio de relatividad.

En su publicación de 1905 en electrodinámica, Henri Poincaré y Albert Einstein explicaron que, con las transformaciones hechas por Lorentz, este principio se mantenía perfectamente invariable. La contribución de Einstein fue el elevar este axioma a principio y proponer las transformadas de Lorentz como primer principio. Además descartó la noción de tiempo absoluto y requirió que la velocidad de la luz en el vacío sea la misma para todos los observadores, sin importar si éstos se movían o no. Esto era fundamental para las ecuaciones de Maxwell, ya que éstas necesitan de una invarianza general de la velocidad de la luz en el vacío.

La teoría de la relatividad especial además busca formular todas las leyes físicas de forma que tengan validez para todos los observadores inerciales. Por lo que cualquier ley física debería tener una forma matemática invariante bajo unas transformaciones de Lorentz.

Como se ha mencionado, los físicos de la época habían encontrado una inconsistencia entre la completa descripción del electromagnetismo realizada por Maxwell y la mecánica clásica. Para ellos, la luz era una onda electromagnética transversal que se movía por un sistema de referencia privilegiado, al cual lo denominaban éter.

Hendrik Antoon Lorentz trabajó en resolver este problema y fue desarrollando unas transformaciones para las cuales las ecuaciones de Maxwell quedaban invariantes y sin necesidad de utilizar ese hipotético éter. La propuesta de Lorentz de 1899, conocida como la "Teoría electrónica de Lorentz", no excluía —sin embargo— al éter. En la misma, Lorentz proponía que la interacción eléctrica entre dos cuerpos cargados se realizaba por medio de unos corpúsculos a los que llamaba electrones y que se encontraban adheridos a la masa en cada uno de los cuerpos. Estos electrones interactuaban entre sí mediante el éter, el cual era contraído por los electrones acorde a transformaciones específicas, mientras estos se encontraban en movimiento relativo al mismo. Estas transformaciones se las conoce ahora como transformaciones de Lorentz. La formulación actual fue trabajo de Poincaré, el cual las presentó de una manera más consistente en 1905.

Se tiene un sistema S de coordenadas formula_1 y un sistema S' de coordenadas formula_2, de aquí las ecuaciones que describen la transformación de un sistema a otro son:
donde
es el llamado factor de Lorentz y formula_4 es la velocidad de la luz en el vacío.

Contrario a nuestro conocimiento actual, en aquel momento esto era una completa revolución, debido a que se planteaba una ecuación para transformar al tiempo, cosa que para la época era imposible. En la mecánica clásica, el tiempo era un invariante. Y para que las mismas leyes se puedan aplicar en cualquier sistema de referencia se obtiene otro tipo de invariante a grandes velocidades (ahora llamadas relativistas), la velocidad de la luz.

Directamente de los postulados expuestos arriba, y por supuesto de las transformaciones de Lorentz, se deduce el hecho de que no se puede decir con sentido absoluto que dos acontecimientos hayan ocurrido al mismo tiempo en diferentes lugares. Si dos sucesos ocurren simultáneamente en lugares separados espacialmente desde el punto de vista de un observador, cualquier otro observador inercial que se mueva respecto al primero los presencia en instantes distintos.

Matemáticamente, esto puede comprobarse en la primera ecuación de las transformaciones de Lorentz:

Dos eventos simultáneos verifican formula_5, pero si sucedieron en lugares distintos (con formula_6), otro observador con movimiento relativo obtiene formula_7. Sólo en el caso formula_5 y formula_9 (sucesos simultáneos "en el mismo punto") no ocurre esto.

El concepto de simultaneidad puede definirse como sigue. Dados dos eventos puntuales "E" y "E", que ocurre respectivamente en instantes de tiempo "t" y "t", y en puntos del espacio "P" = ("x", "y", "z") y "P" = ("x", "y", "z"), todas las teorías físicas admiten que estos sólo pueden darse una, de tres posibilidades mutuamente excluyentes:


Dado un evento cualquiera, el conjunto de eventos puede dividirse según esas tres categorías anteriores. Es decir, todas las teorías físicas permiten fijado un evento, clasificar a los demás eventos: en (1) pasado, (2) futuro y (3) resto de eventos (ni pasados ni futuros). En mecánica clásica esta última categoría está formada por los sucesos llamados simultáneos, y en mecánica relativista eventos no relacionados causalmente con el primer evento. Sin embargo, la mecánica clásica y la mecánica relativista difieren en el modo concreto en que esa división entre pasado, futuro y otros puede hacerse y en si dicho carácter es absoluto o relativo de dicha partición.

Como se dijo previamente, el tiempo en esta teoría deja de ser absoluto como se proponía en la mecánica clásica. O sea, el tiempo para todos los observadores del fenómeno deja de ser el mismo. Si tenemos un observador inmóvil haciendo una medición del tiempo de un acontecimiento y otro que se mueva a velocidades relativistas, los dos relojes no tendrán la misma medición de tiempo.

Mediante la transformación de Lorentz nuevamente llegamos a comprobar esto. Se coloca un reloj ligado al sistema S y otro al S', lo que nos indica que formula_10. Se tiene las transformaciones y sus inversas en términos de la diferencia de coordenadas:
y

Si despejamos las primeras ecuaciones obtenemos

De lo que obtenemos que los eventos que se realicen en el sistema en movimiento S' serán más largos que los del S. La relación entre ambos es esa formula_15. Este fenómeno se lo conoce como dilación del tiempo.

Si se dice que el tiempo varía a velocidades relativistas, la longitud también lo hace. Un ejemplo sería si tenemos a dos observadores inicialmente inmóviles, éstos miden un vehículo en el cual solo uno de ellos "viajará" a grandes velocidades, ambos obtendrán el mismo resultado. Uno de ellos entra al vehículo y cuando adquiera la suficiente velocidad mide el vehículo obteniendo el resultado esperado, pero si el que esta inmóvil lo vuelve a medir, obtendrá un valor menor. Esto se debe a que la longitud también se contrae.

Volviendo a las ecuaciones de Lorentz, despejando ahora a x y condicionando a formula_16 se obtiene:

de lo cual podemos ver que existirá una disminución debido al cociente. Estos efectos solo pueden verse a grandes velocidades, por lo que en nuestra vida cotidiana las conclusiones obtenidas a partir de estos cálculos no tienen mucho sentido.

Un buen ejemplo de estas contracciones y dilataciones fue propuesto por Einstein en su paradoja de los gemelos, y verificado experimentalmente por la anomalía en el tiempo de vida de los muones, producidos por los rayos cósmicos.

La composición de velocidades es el cambio en la velocidad de un cuerpo al ser medida en diferentes sistemas de referencia inerciales. En la física pre-relativista se calculaba mediante

donde "v"′ es la velocidad del cuerpo con respecto al sistema "S"′, "u" la velocidad con la que este sistema se aleja del sistema "en reposo" "S", y "v" es la velocidad del cuerpo medida en "S".

Sin embargo, debido a las modificaciones del espacio y el tiempo, esta relación no es válida en Relatividad Especial. Mediante las transformadas de Lorentz puede obtenerse la fórmula correcta:
</math>
Al observar con cuidado esta fórmula se nota que si tomamos para el cuerpo una velocidad en el sistema "S" igual a la de la luz (el caso de un fotón, por ejemplo), su velocidad en "S"′ sigue siendo "v"′="c", como se espera debido al segundo postulado. Además, si las velocidades son muy pequeñas en comparación con la luz, se obtiene que esta fórmula se aproxima a la anterior dada por Galileo.

El concepto de masa en la teoría de la relatividad especial tiene dos bifurcaciones: la masa invariante y la masa relativista aparente. La masa relativista aparente es la masa aparente que va a depender del observador y se puede incrementar dependiendo de su velocidad, mientras que la invariante es independiente del observador e invariante.

Matemáticamente tenemos que: formula_17 donde formula_18 es la masa relativista aparente, formula_19 es la invariante y formula_20 es el factor de Lorentz. Notemos que si la velocidad relativa del factor de Lorentz es muy baja, la masa relativa tiene el mismo valor que la masa invariante pero si ésta es comparable con la velocidad de la luz existe una variación entre ambas. Conforme la velocidad se vaya aproximando a la velocidad de la luz, la masa relativista tenderá a infinito.

Al existir una variación en la masa relativista aparente, la cantidad de movimiento de un cuerpo también debe ser redefinida. Según Newton, la cantidad de movimiento está definida por formula_21 donde formula_19 era la masa del cuerpo. Como esta masa ya no es invariante, nuestra nueva "cantidad de movimiento relativista" tiene el factor de Lorentz incluido así:

Sus consecuencias las veremos con más detenimiento en la sección posterior de fuerza.

La relatividad especial postula una ecuación para la energía, la cual inexplicablemente llegó a ser la ecuación más famosa del planeta, "E" = "mc". A esta ecuación también se la conoce como la equivalencia entre masa y energía. En la relatividad, la energía y el momento de una partícula están relacionados mediante la ecuación:

Esta relación de energía-momento formulada en la relatividad nos permite observar la independencia del observador tanto de la energía como de la cantidad de momento. Para velocidades no relativistas, la energía puede ser aproximada mediante una expansión de una serie de Taylor así
encontrando así la energía cinética de la mecánica de Newton. Lo que nos indica que esa 
mecánica no era más que un caso particular de la actual relatividad. El primer término de esta aproximación es lo que se conoce como la energía en reposo (energía potencial), ésta es la cantidad de energía que puede medir un observador en reposo de acuerdo con lo postulado por Einstein. Esta energía en reposo no causaba conflicto con lo establecido anteriormente por Newton, porque ésta es constante y además persiste la energía en movimiento. Einstein lo describió de esta manera: 

En mecánica newtoniana la fuerza no relativista puede obtenerse simplemente como la derivada temporal del momento lineal:
Pero contrariamente postula la mecánica newtoniana, aquí el momento no es simplemente la masa en reposo por la velocidad. Por lo que la ecuación formula_23 ya no es válida en relatividad. Si introducimos la definición correcta del momento lineal, usando la masa aparente relativista entonces obtenemos la expresión relativista correcta:
m\frac {d\gamma}{dt}\mathbf{v} + \gamma m\frac {d\mathbf{v}}{dt} </math>
donde formula_24 es la masa relativista aparente. Calculando la fuerza anterior se observa el hecho que la fuerza podría no tener necesariamente la dirección de la aceleración, como se deduce desarrollando la ecuación anterior:
Introduciendo las :

Existen dos casos particulares de movimiento de una partícula donde la fuerza es siempre paralela a la aceleración, que son el movimiento rectilíneo uniformemente acelerado y el movimiento circular uniforme; en el primer caso el factor de proporcionalidad es formula_25 y el en segundo formula_26

La relatividad especial usa tensores y cuadrivectores para representar un espacio pseudo-euclídeo. Este espacio, sin embargo, es similar al espacio euclídeo tridimensional en muchos aspectos y es relativamente fácil trabajar en él. El tensor métrico que da la distancia elemental ("ds") en un espacio euclídeo se define como:

donde formula_27 son diferenciales de las tres coordenadas cartesianas espaciales. En la geometría de la relatividad especial, se añade una cuarta dimensión imaginaria dada por el producto "ict", donde "t" es el tiempo, "c" la velocidad de la luz e "i" la unidad imaginaria: quedando el intervalo relativista, en forma diferencial, como:

El factor imaginario se introduce para mostrar el carácter pseudoeuclídeo de la geometría espacio-tiemporal. Si se reducen las dimensiones espaciales a 2, se puede hacer una representación física en un espacio tridimensional,
Se puede ver que las geodésicas con medida cero forman un cono dual definido por la ecuación

La ecuación anterior es la de círculo con formula_28. Si se extiende lo anterior a las tres dimensiones espaciales, las geodésicas nulas son esferas concéntricas, con radio = distancia = c por tiempo.

Este doble cono de distancias nulas representa el "horizonte de visión" de un punto en el espacio. Esto es, cuando se mira a las estrellas y se dice: "La estrella de la que estoy recibiendo luz tiene X años", se está viendo a través de esa línea de visión: una geodésica de distancia nula. Se está viendo un suceso a formula_29 metros, y formula_30 segundos en el pasado. Por esta razón, el doble cono es también conocido como cono de luz (El punto inferior de la izquierda del diagrama inferior representa la estrella, el origen representa el observador y la línea representa la geodésica nula, el "horizonte de visión" o "cono de luz"). Es importante notar que sólo los puntos interiores al cono de luz de un evento pueden estar en relación causal con ese evento.

Previo a esta teoría, el concepto de causalidad estaba determinado: "para una causa existe un efecto". Anteriormente, gracias a los postulados de Laplace, se creía que para todo acontecimiento se debía obtener un resultado que podía predecirse. La revolución en este concepto es que se "crea" un "cono de luz" de posibilidades (Véase gráfico adjunto).

Se observa este cono de luz y ahora un acontecimiento en el cono de luz del pasado no necesariamente nos conduce a un solo efecto en el cono de luz futuro. Desligando así la causa y el efecto. El observador que se sitúa en el vértice del cono ya no puede indicar qué causa del cono del pasado provocará el efecto en el cono del futuro.

Asumiendo el principio de causalidad e ingnorando ciertas posibilidades relacionadas con el movimiento superlumínico, obtenemos que ninguna partícula de masa positiva en reposo puede viajar más rápido que la luz. En particular, la relación entre la energía cinética "K" necesaria para acelerar rectilíneamente una partícula desde el reposo hasta una cierta velocidad "v" viene expresada por la ecuación:

Aquí puede verse claramente que para cualquier valor finito de "K" se cumplirá que "v" < "c". Otra manera de ver esta imposibilidad es usar el principio de causalidad, y aplicarlo al movimiento más rápido que el de la luz. Imagínese un cuerpo que experimenta una fuerza durante una cantidad infinita de tiempo. Tenemos entonces que para un movimiento rectilíneo:
</math>
De la expresión anterior se deduce que la "inercia efectiva", entendida como la resistencia que opone el cuerpo a ser acelerado "F / a", irá aumentando indefinidamente a medida que "v" se acerca a "c".

Por otra parte, esta conclusión depende críticamente de la asunción de causalidad. Así en mecánica cuántica esta asunción no se considera, por lo que algunas partículas virtuales no están sujetas a esa restricción. Además existen propuestas teóricas que postulan la existencia de partículas hipotéticas que podrían viajar más rápido que la luz, los taquiones, naturalmente en esas teorías no se asume el principio de causalidad en la forma planteada aquí.

La relatividad especial a pesar de poder ser descrita con facilidad por medio de la mecánica clásica y ser de fácil entendimiento, tiene una compleja matemática de por medio. Aquí se describe a la relatividad especial en la forma de la covariancia de Lorentz. La posición de un evento en el espacio-tiempo está dado por un vector contravariante cuatridimensional, sus componentes son:

esto es que formula_31, formula_32, formula_33 y formula_34. Los superíndices de esta sección describen contravarianza y no exponente a menos que sea un cuadrado o se diga lo contrario. Los superíndices son índices covariantes que tienen un rango de cero a tres como un gradiente del espacio tiempo del campo φ:

Habiendo reconocido la naturaleza cuatridimensional del espacio-tiempo, se puede empezar a emplear la métrica de Minkowski, η, dada en los componentes (válidos para cualquier sistema de referencia) así:
Su inversa es:
Luego se reconoce que las transformaciones coordenadas entre los sistemas de referencia inerciales están dadas por el tensor de transformación de Lorentz Λ. Para el caso especial de movimiento a través del eje x, se tiene:

que es simplemente la matriz de un boost (como una rotación) entre las coordenadas "x" y "t". Donde μ' indica la fila y ν la columna. También β y γ están definidos como:
</math>
Más generalmente, una transformación de un sistema inercial (ignorando la translación para simplificarlo) a otro debe satisfacer:

donde hay un sumatorio implícita de formula_35 y formula_36 de cero a tres en el lado derecho, de acuerdo con el Convenio de sumación de Einstein. El grupo de Poincaré es el grupo más general de transformaciones que preservan la métrica de Minkowski y ésta es la simetría física subyacente a la relatividad especial.

Todas las propiedades físicas cuantitativas son dadas por tensores. Así para transformar de un sistema a otro, se usa la muy conocida ley de transformación tensorial

donde formula_37 es la matriz inversa de formula_38. Para observar como esto es útil, transformamos la posición de un evento de un sistema de coordenadas "S" a uno "S"', se calcula

que son las transformaciones de Lorentz dadas anteriormente. Todas las transformaciones de tensores siguen la misma regla. El cuadrado de la diferencia de la longitud de la posición del vector formula_39 construido usando

es un invariante. Ser invariante significa que toma el mismo valor en todos los sistemas inerciales porque es un escalar (tensor de rango 0), y así Λ no aparece en esta transformación trivial. Se nota que cuando el elemento línea formula_40 es negativo formula_41 es el diferencial del tiempo propio, mientras que cuando formula_42 es positivo, formula_43 es el diferencial de la distancia propia.

El principal valor de expresar las ecuaciones de la física en forma tensorial es que éstas son luego manifestaciones invariantes bajo los grupos de Poincaré, así que no tenemos que hacer cálculos tediosos o especiales para confirmar ese hecho. También al construir tales ecuaciones encontramos usualmente que ecuaciones previas que no tienen relación, de hecho, están conectadas cercanamente al ser parte de la misma ecuación tensorial.

Ahora podemos definir igualmente la velocidad y la aceleración mediante simples leyes de transformación. La velocidad en el espacio-tiempo "U" está dada por

Reconociendo esto, podemos convertir buscando una ley sobre las composiciones de velocidades en un simple estado acerca de transformaciones de velocidades de cuatro dimensiones de una partícula de un sistema a otro. "U" también tiene una forma invariante:

Así la cuadrivelocidad tiene una magnitud de "c". Esta es una expresión del hecho que no hay tal cosa como la coordenada en reposo en relatividad: al menos, si se está siempre moviéndose a través del tiempo. Para la cuadriaceleración, ésta viene dada por formula_44. Dado esto, diferenciando la ecuación para "τ" produce

formula_45

así en relatividad, la aceleración y la velocidad en el espacio-tiempo son ortogonales.

El momento lineal y la energía se combinan en un cuadrivector covariante:

donde "m" es la masa invariante.

La magnitud invariante del cuadrimomento es:

Podemos trabajar con que este es un invariante por el argumento de que éste es primero un escalar, no interesa qué sistema de referencia se calcule y si la transformamos a un sistema donde el momento total sea cero.

Se observa que la energía en reposo es un invariante independiente. Una energía en reposo se puede calcular para partículas y sistemas en movimiento, por traslación de un sistema en que el momento es cero. La energía en reposo está relacionada con la masa de acuerdo con la ecuación antes discutida:

Nótese que la masa de un sistema de medida en su sistema de centro de momento (donde el momento total es cero) está dado por la energía total del sistema en ese marco de referencia. No debería ser igual a la suma de masas individuales del sistema medido en otros sistemas.

Al usar la tercera ley de Newton, ambas fuerzas deben estar definidas como la tasa de cambio del momentum respecto al mismo tiempo coordenado. Esto es, se requiere de las fuerzas definidas anteriormente. Desafortunadamente, no hay un tensor en cuatro dimensiones que contenga las componentes de un vector de fuerza en tres dimensiones entre sus componentes.

Si una partícula no está viajando a "c", se puede transformar en una fuerza de tres dimensiones del sistema de referencia de la partícula en movimiento entre los observadores de este sistema. A éstos se los suele llamar fuerza de cuatro dimensiones. Es la tasa de cambio del anterior vector de cuatro dimensiones de energía momento con respecto al tiempo propio. La versión covariante de esta fuerza es:
-{\text{d} E}/{\text{d} \tau} \\ {\text{d} p_x}/{\text{d} \tau} \\ {\text{d} p_y}/{\text{d} \tau} \\ {\text{d} p_z}/{\text{d} \tau} \end{pmatrix} </math>
donde formula_46 es el tiempo propio.

En el sistema en reposo del objeto, la componente del tiempo de esta fuerza es cero a menos que la masa invariante del objeto este cambiando, en ese caso la tasa de cambio es negativa y es "c" veces. En general, se piensa que las componentes de la fuerza de cuatro dimensiones no son iguales a las componentes de la fuerza de tres porque ésta de tres está definida por la tasa de cambio del momento con respecto al tiempo coordenado, así formula_47; mientras que la fuerza en cuatro dimensiones está definida por la tasa de cambio del momento respecto al tiempo propio, así formula_48.

En un medio continuo, la "densidad de fuerza" en tres dimensiones combinada con la "densidad de potencia" forma un vector de cuatro dimensiones covariante. La parte espacial es el resultado de dividir la fuerza en pequeñas células (en el espacio tridimensional) por el volumen de la célula. El componente del tiempo es negativo de la potencia transferida a la célula dividida para el volumen de la célula.

Investigaciones teóricas en el electromagnetismo clásico indicaron el camino para descubrir la propagación de onda. Las ecuaciones generalizando los efectos electromagnéticos encontraron que la velocidad de propagación finita de los campos E y B requiere comportamientos claros en partículas cargadas. El estudio general de cargas en movimiento forma un potencial de Liénard-Wiechert, que es un paso a través de la relatividad especial.

La transformación de Lorentz del campo eléctrico de una carga en movimiento por un observador en reposo en un sistema de referencia resulta en la aparición de un término matemático comúnmente llamado campo magnético. Al contrario, el campo magnético generado por las cargas en movimiento desaparece y se convierte en un campo electrostático en un sistema de referencia móvil. Las ecuaciones de Maxwell son entonces simplemente ajustes empíricos a los efectos de la relatividad especial en un modelo clásico del universo. Como los campos eléctricos y magnéticos son dependientes de los sistemas de referencia y así entrelazados, en el así llamado campo electromagnético. La relatividad especial provee las reglas de transformación de cómo los campos electromagnéticos en un sistema inercial aparecen en otro sistema inercial.

Las ecuaciones de Maxwell en la forma tridimensional son de por sí consistentes con el contenido físico de la relatividad especial. Pero debemos reescribirlas para hacerlas invariantes. La densidad de carga formula_49 y la densidad de corriente formula_50 son unificadas en el concepto de vector cuatridimensional:

La ley de conservación de la carga se vuelve:

El campo eléctrico formula_51 y la inducción magnética formula_52 son ahora unificadas en un tensor de campo electromagnético (de rango 2, antisimétrico covariante):

La densidad de la fuerza de Lorentz formula_53 ejercida en la materia por el campo electromagnético es:

La ley de Faraday de inducción y la ley de Gauss para el magnetismo se combinan en la forma:

A pesar de que se ven muchas ecuaciones, éstas se pueden reducir a solo cuatro ecuaciones independientes. Usando la antisimetría del campo electromagnético se puede reducir a la identidad o redundar en todas las ecuaciones excepto las que λ, μ, ν = 1,2,3 o 2,3,0 o 3,0,1 o 0,1,2.

Existe cierta confusión sobre los límites de la teoría especial de la relatividad. Por ejemplo, con frecuencia en textos de divulgación se repite que dentro de esta teoría sólo pueden tratarse sistemas de referencia inerciales, en los cuales la métrica toma la forma canónica. Sin embargo, como diversos autores se han encargado de demostrar la teoría puede tratar igualmente sistemas de referencia no inerciales.

Obviamente el tratamiento de sistemas no inerciales en la teoría de la relatividad especial resulta más complicado que el de los sistemas inerciales.

Einstein y otros autores consideraron antes del desarrollo de la relatividad general casi exclusivamente sistemas de coordenadas relacionados por transformaciones de Lorentz, razón por la cual se piensa que esta teoría es sólo aplicable a sistemas inerciales.

Actualmente se considera como relatividad general el estudio del espacio-tiempo deformado por campos gravitatorios, dejando el estudio de los sistemas de referencia acelerados en espacios planos dentro de la relatividad especial. Igualmente la relatividad general es una de las teorías más relevantes para la construcción de modelos cosmológicos sobre el origen del universo.

La teoría general de la relatividad fue introducida históricamente en conexión con el principio de equivalencia y el intento de explicar la identidad entre la masa inercial y la masa gravitatoria. En esta teoría se usaban explícitamente sistemas de coordenadas no relacionados entre sí por transformaciones de Lorentz o similares, con lo cual claramente en la resolución de muchos problemas se hacía patente el uso de sistemas de referencia no inerciales. Estos hechos condujeron a la confusión en muchos textos de divulgación de que los sistemas no inerciales requieren del desarrollo de la teoría general de la relatividad.





</doc>
<doc id="2464" url="https://es.wikipedia.org/wiki?curid=2464" title="Resedaceae">
Resedaceae

Las Resedáceas (Resedaceae) son una familia de plantas herbáceas o raramente subarbustivas. Hojas dispuestas helicoidalmente; enteras, tripartitas o pinnatífidas; estipulas glandulosas. Flores pequeñas, hermafroditas, cigomorfas; cáliz con 2-8 sépalos, a veces desiguales; corola con 2-8 pétalos (no tienen por qué tener el mismo número de piezas que el cáliz), a menudo laciniados, los posteriores mayores; androceo con 3 a muchos estambres; gineceo súpero, sincarpico, soldados siempre en la base y no siempre hasta arriba, con un número de carpelos variable entre 2 y 7. Inflorescencias abiertas, en racimo o en espiga. Fruto capsular (en "Reseda)" o polifolicular (en "Sesamoides"). 

Es una pequeña familia (c. 85 especies) cuyo conocimiento
taxonómico básico está relativamente bien establecido principalmente como fruto de dos
trabajos monográficos exhaustivos basados exclusivamente en datos morfológicos. Asimismo, el encuadramiento taxonómico general de la familia a nivel de orden es razonablemente consistente gracias a estudios recientes basados en caracteres morfológicos, bioquímicos y moleculares, si bien su relación con los grupos más próximos aún no está totalmente esclarecida.
Está compuesta por seis géneros (Caylusea A. St. Hil,
Ochradenus Del., Oligomeris Cambess., Randonia Coss., Reseda L. y Sesamoides All.; Fig.
1) y c. 85 especies, de las cuales más del 70% pertenecen al género Reseda. La familia se
distribuye principalmente por las regiones temperadas del Viejo Mundo, y presenta
su centro de diversidad en la cuenca mediterránea. Sus integrantes suelen encontrarse en
hábitats áridos y soleados, como estepas, desiertos y taludes, y muestran generalmente
preferencia por sustratos calcáreos. No obstante, cinco especies (Reseda alba L., R. lutea L.,
R. luteola L., R. odorata L. y R. phyteuma L.) son ruderales o arvenses en áreas alteradas por
la acción antrópica, y cuatro [R. attenuata (Ball) Ball, R. complicata Bory, R. glauca L. y R.
gredensis (Cutanda & Willk.) Müll. Arg.] se encuentran confinadas en zonas montañosas. En
cinco de los seis géneros de la familia, alguna de sus especies (en Caylusea y Reseda) o todas
ellas (en Ochradenus, Oligomeris y Randonia) habitan ambientes desérticos o subdesérticos.
Cuatro géneros están constituidos en su mayoría por hierbas anuales o perennes (Caylusea,
Oligomeris, Reseda y Sesamoides), mientras que los otros dos están formados por arbustos
(Ochradenus, Randonia). El hábito arbustivo también aparece en un pequeño grupo de unas
nueve especies del género Reseda, principalmente del cuerno de África.
Recientemente, se ha propuesto la primera hipótesis filogenética de la familia Resedaceae basada en regiones de ADN, con el principal objetivo de evaluar las clasificaciones taxonómicas tradicionales de la familia y discutir sus principales mecanismos evolutivos.



</doc>
<doc id="2467" url="https://es.wikipedia.org/wiki?curid=2467" title="Rosaceae">
Rosaceae

Las rosáceas (Rosaceae) son una familia de plantas dicotiledóneas pertenecientes al orden Rosales. Esta familia incluye la mayor parte de las especies de frutas de consumo masivo: manzana, pera, membrillo, melocotón (durazno), ciruela, cereza, fresa (frutilla), almendra, albaricoque, zarzamora, frambuesa, etc. También incluye muchas especies ornamentales, principalmente, las rosas, flores por excelencia, con importancia para la jardinería y la industria de la perfumería.

La familia de las rosáceas es muy amplia, con unos 90-130 géneros, en los que se reparten alrededor de 2000-2500 especies aceptadas, cuya distribución es casi mundial, originarias sobre todo de las regiones templadas y subtropicales del hemisferio boreal.

La familia Rosaceae incluye géneros con características muy heterogéneas, sin embargo, la característica común más importante es la presencia de un tálamo o receptáculo floral muy desarrollado, que varía desde una forma convexa (en "Rubus", "Fragaria"), hasta de forma cóncava (en "Rosa").

Estos individuos pueden poseer tallos leñosos o semileñosos. El hábito de estas plantas comprende: árboles, arbustos, trepadoras, sino hierbas perennes por rizomas o anuales.

Las hojas pueden ser simples o compuestas, (paripinnadas o imparipinnadas), casi siempre alternas y estipuladas, rara vez opuestas, con borde aserrado o dentado característico. Es frecuente la presencia de modificaciones: espinas, estípulas y aguijones, rara vez ausentes ("Spiraea").

Flores normalmente hermafroditas, actinomorfas (a veces zigomorfas por diferenciación de los sépalos en las plantas europeas, o también de los pétalos en las tropicales); periginas, epiginas o hipoginas; a menudo con un hipanto bien desarrollado; cáliz con 5 sépalos, a veces con epicáliz; corola generalmente con 5 pétalos libres; tetrámeras en "Sanguisorba"; androceo variable, con 4-5 estambres, y más frecuentemente diplostémono o polistémono; gineceo variable, con 1 a muchos carpelos, libres o soldados y estilos generalmente libres. Flores solitarias o en inflorescencias variadas (racimos, espigas y corimbos).

Frutos muy variables (aquenio, poliaquenio con receptáculo abombado (eterio) o cóncavo (cinorrodón), drupa, polidrupa (sorosis), folículo, pomo); semillas pequeñas, sin endosperma (las substancias de reserva en los cotiledones).

Rosaceae es una familia en la cual la delimitación de sus géneros (y así de sus especies también) es una de sus mayores problemáticas para consistencia sistemática del grupo.

Se consideran caracteres importantes en la sistemática de la familia: la forma del receptáculo, el tipo de concrescencia de los carpelos y su posición, el número y disposición de los rudimentos seminales en el carpelo, el tipo de dehiscencia y la histología de los frutos, el número básico de cromosomas y la distribución de los metabolitos secundarios.

El nombre Rosaceae aparece publicado por el botánico escocés M. Adanson en ; y se conserva para el taxón. También citado posteriormente por Antoine L. de Jussieu, Genera Plantarum: 334. 1789. 
La clasificación de la familia Rosaceae es muy variada y diversa en taxones subfamiliares, y ha cambiado mucho con el tiempo y según los diferentes criterios de clasificación propuestos por los autores. Este es un esquema posible en el que se listan en primer lugar las subfamilias, en segundo lugar las tribus y en tercer lugar los géneros en sendos grupos; además de la referencia bibliográfica de cada taxón según el IPNI.

Fruto indehiscente: aquenios o drupas en su mayoría agregados. En el gineceo, los carpelos son numerosos y libres, el ovario medio o súpero. 

Frutos dehiscentes tipo folículo o cápsula.


Fruto indehiscente característico: el pomo. En el gineceo, los carpelos son 2-5 y están unidos; el ovario es ínfero.








Ver el anexo de .

Una clasificación práctica —y tradicional— para la familia de las rosáceas está dada por la división en cuatro subfamilias en función de las características de floración y fructificación de cada género. A su vez, es posible agrupar varios géneros en tribus y subtribus. Las cuatro subfamilias clásicas son:
Gineceo con un solo carpelo:

También conocida como Amygdaloideae, incluye especies leñosas y arbóreas, que poseen estípulas generalmente pequeñas y caducas. Las flores son periginas: el perianto (cáliz + corola) y el androceo se insertan sobre un receptáculo (o tálamo) acopado rodeando al gineceo; Gineceo con ovario súpero unicarpelar y uniovulado (monómero) que genera un fruto drupáceo.

"Prunus" - "Maddenia" - "Oemleria" - "Prinsepia" - "Exochorda".
Gineceo compuesto por más de un carpelo (policarpo):

+ n carpelos libres (policarpo apocárpico): Flores periginas, con gineceo apocárpico y ovario súpero a semi-ínfero. Fruto, frecuentemente múltiple: poliaquenio, polidrupa, eterio (drupas múltiples sobre receptáculo hinchado), cinorrodón (aquenios encerrados en una urna) o frutos complejos secos, indehiscente, semejantes a un aquenio. numerosos carpelos uniovulados que producen drupelas o aquenios. Estípulas bien desarrolladas y persistentes.
Colurieae - Crataegeae - Dryadeae - Exochordeae - Gillenieae - Kerrieae - Neillieae - Potentilleae - Roseae - Rubeae - Sanguisorbeae - Ulmarieae - Incertae sedis (lugar incierto)

"Acaena" "Acomastylis" "Agrimonia" "Alchemilla" "Aphanes" "Aremonia" "Bencomia" "Chamaebatia" "Cliffortia" "Coluria" "Comarum" "Cowania" "Dalibarda" "Dendriopoterium" "Dryas" "Duchesnea" "Erythrocoma" "Fallugia" "Filipendula" "Fragaria" "Geum" "Hagenia" "Horkelia" "Ivesia" "Kerria" "Leucosidea" "Marcetella" "Margyricarpus" "Novosieversia" "Oncostylus" "Polylepis" "Potentilla" "Rosa" "Rubus" "Sanguisorba" "Sarcopoterium" "Sibbaldia" "Sieversia" "Spenceria" "Taihangia" "Tetraglochin" "Waldsteinia ".-

+ 5 carpelos unidos al hipanto: Especies leñosas y arbóreas, de estípulas caducas. Flores epiginas con gineceo apocárpico (2-5 carpelos) que se fusionan y ovario ínfero; receptáculo floral muy desarrollado. Fruto de tipo complejo conocido como pomo.

"Mespilus" _ "Pyracantha" - "Amelanchier"

"Aria" - "Aronia" - "Chamaemeles" - "Chamaemespilus" - "Cotoneaster" - "Cormus" - "Crataegus" - "Cydonia" - "Dichotomanthes" - "Docynia" - "Docyniopsis" - "Eriobotrya" - "Eriolobus" - "Heteromeles" - "Kageneckia" - "Lindleya" - "Malacomeles" - "Malus" - "Osteomeles" - "Peraphyllum" -"Photinia" - "Pseudocydonia" - "Pyrus" - "Rhaphiolepis" - "Sorbus" - "Stranvaesia" - "Torminalis" - "Vauquelinia" - "x Crataemespilus".

Se propone como la subfamilia que reúne a los géneros más primitivos de las rosáceas. Especies principalmente leñosas. Estípulas ausentes o presentes, caducas o persistentes. Flores periginas con receptáculo plano; ovario súpero. Flor epigina, gineceo formado por varios (5) carpelos apocárpicos pluriovulados (cada ovario con varios primordios seminales) con placentación marginal (a veces soldados) que originan folículos o polifolículos, excepcionalmente un fruto capsular o poliaquenio. Ref.: 

"Aruncus" - "Holodiscus" - "Kelseya" - "Luetkea" - "Petrophytum" - "Sibiraea" - "Spiraea" - "Xerospiraea"




</doc>
<doc id="2468" url="https://es.wikipedia.org/wiki?curid=2468" title="Rhizophoraceae">
Rhizophoraceae

Rhizophoraceae es una familia constituida por plantas tropicales o subtropicales. Entre los miembros más conocidos se encuentran los mangles, árboles del género "Rhizophora". Consta de alrededor de 120 especies distribuidas en 16 géneros, la mayoría nativos del Viejo Mundo.

Raíces modificadas para la obtención de aire (neumatóforos) o para la sujeción: "raíces zanco" (viven en medios muy móviles: fango). Flores tetrameras, periantio más o menos desarrollado en las especies africanas, y menos desarrollado en las americanas y asiáticas. Frutos vivíparos. Acuáticas, forma manglares en algunas zonas tropicales costeras.

Estas especies son a menudo hermafroditas, más raramente polígamas. Las especies de manglar son normalmente vivíparas, a diferencia de las de tierra.


Algunas especies producen madera para construcciones bajo el agua o pilotes. De la corteza se extrae tanino.



</doc>
<doc id="2469" url="https://es.wikipedia.org/wiki?curid=2469" title="Rafflesiales">
Rafflesiales

Las rafflesiales son un orden de plantas de la subclase Rosidae, clase Magnoliopsida.

Es un sinónimo de Malpighiales.

Pocas especies, una familia en península ibérica, con 1 especie, 2 especies importantes.


</doc>
<doc id="2470" url="https://es.wikipedia.org/wiki?curid=2470" title="Rafflesiaceae">
Rafflesiaceae

Rafflesiaceae, del orden Malpighiales, es una familia de hierbas perennes holoparásitas, monoicas, a veces con aspecto taloide. Tienen hojas reducidas a escamas o ausentes. Sus flores son unisexuales, homoclamídeas, generalmente tetrámeras, de ovario ínfero; dispuestas en espigas glomerulares o solitarias. Fruto en baya. Existen 9 géneros con unas 60 especies, sobre todo tropicales que se encuentran en el Este Sureste de Asia. Entre ellas, "Rafflesia arnoldii" es la especie con la flor de mayor tamaño entre todas las plantas.
La familia Rafflesiaceae "sensu lato" se divide en cuatro familias:


Las cuatro familias pueden distinguirse fácilmente por sus flores e inflorescencias:



</doc>
<doc id="2471" url="https://es.wikipedia.org/wiki?curid=2471" title="Rhamnales">
Rhamnales

En el sistema de Cronquist de clasificación científica de los vegetales, los Rhamnales eran un orden de plantas dicotiledóneas que incluía tres familias bien representadas en las zonas templadas. Generalmente leñosas, muchas trepadoras, algunas hierbas. Tiene siempre un disco nectarífero intraestaminal, que procede del verticilo interno de los estambres, los que quedan son epipétalos. La diferenciación entre familias por el número de óvulos por lóculo, "Vitaceae", con 2 óvulos por lóculo y "Rhamnaceae", con 1 óvulo por lóculo.

Las investigaciones filogenéticas modernas han separado a las ramnáceas, hoy consideradas parte del orden "Rosales", de las dos familias restantes; estas últimas se consideran las únicas en el orden "Vitales".


</doc>
<doc id="2472" url="https://es.wikipedia.org/wiki?curid=2472" title="Rhamnaceae">
Rhamnaceae

Las ramnáceas (Rhamnaceae) son una familia de plantas dicotiledóneas perteneciente al orden de las rosales. 

Se trata de árboles o arbustos, a veces trepadores con espinas, y matas. Hojas simples con frecuencia alternas, a veces opuestas, con estípulas caducas o transformadas en espinas; flores inconspicuas, hermafroditas o unisexuales (plantas dioicas o poliginas), pentámeras o tetrámeras, diclamideas o monoclamideas, de ovario súpero o ínfero, con un óvulo por lóculo. Inflorescencias en racimos o glomérulos. Frutos generalmente en drupa o secos. Unas 6.000 especies cosmopolitas la mayoría de países cálidos y templados, algunos con aplicaciones medicinales.













</doc>
<doc id="2473" url="https://es.wikipedia.org/wiki?curid=2473" title="Rutaceae">
Rutaceae

Las rutáceas son una familia de plantas angiospermas perteneciente al orden Sapindales. Agrupa alrededor de 160 géneros y 1.600 especies.

Plantas leñosas o raramente herbáceas, provistas de glándulas secretoras oleíferas. Hojas alternas u opuestas, simples o compuestas, sin estípulas, a veces con espinas axilares. Flores generalmente hermafroditas, actinomorfas o zigomorfas, pentámeras o tetrámeras, con piezas libres o soldadas en la base; androceo con un número variable de estambres, a menudo el mismo o el doble que el de pétalos, con disco nectarífero carnoso intraestaminal; gineceo súpero o semiínfero, pluricarpelar, generalmente sincárpico y plurilocular. Inflorescencias diversas. Frutos en cápsula, polifolículo, hesperidio, drupa o sámara. 

Tiene las siguientes subfamilias: Citroideae, Dictyolomatoideae, Flindersioideae, Rutoideae, Spathelioideae, Toddalioideae. En total son unas 1.600 especies, de las cuales la mayoría crecen en países tropicales y subtropicales.




</doc>
<doc id="2474" url="https://es.wikipedia.org/wiki?curid=2474" title="Rhoipteleaceae">
Rhoipteleaceae

Rhoipteleaceae, es una familia de plantas dicotiledóneas que comprende una sola especie "Rhoiptelea sinensis", con un único género "Rhoiptelea". Es nativa del sudoeste de China y norte de Vietnam donde vive en alturas de 700-1600 msnm en áreas montañosas.
Los árboles que alcanzan los 17 metros de altura y un tronco de 40 cm de diámetro, son polinizados por el viento y las flor]es se producen en panículas que alcanzan los 32 cm de longitud con parecido a una "cola de caballo". Su fruto es una pequeña nuez con alas redondeadas. Sus hojas son pinnadas.

"Rhoiptelea sinensis" fue descrita por Diels & Hand.-Mazz. y publicado en "Repertorium Specierum Novarum Regni Vegetabilis 30(791–798): 77–79, pl. 127 & 128", en el año 1932.



</doc>
<doc id="2475" url="https://es.wikipedia.org/wiki?curid=2475" title="Reyes Magos">
Reyes Magos

Los Reyes Magos de Oriente (o simplemente Reyes Magos) es el nombre por el que la tradición cristiana denomina a los «magos» —denominación que recibían los sacerdotes eruditos en el Antiguo Oriente— que, tras el nacimiento de Jesús de Nazaret, acudieron desde Oriente para rendirle homenaje y entregarle regalos de gran riqueza simbólica: oro, incienso y mirra.

En los evangelios canónicos solo el Evangelio de Mateo habla de estos "magos", sin precisar sus nombres, ni que fuesen reyes, ni que fueran tres. Fue en el cuando se estableció que pudieran ser reyes, ya que hasta entonces, por sus regalos y las iconografías que los representaban, tan solo se consideraba que eran personas pudientes. Fue también en ese siglo cuando se estableció su número en tres, uno por regalo, ya que hasta entonces había dibujos con dos, tres o cuatro magos, e incluso la Iglesia ortodoxa siria y la Iglesia apostólica armenia aseguraban que eran doce, como los apóstoles y las doce tribus de Israel.

Los nombres actuales de los tres reyes magos, Melchor, Gaspar y Baltasar, aparecen por primera vez en el famoso mosaico de San Apollinaire Nuovo (Rávena) que data del , en el que se distingue a los tres magos ataviados al modo persa con sus nombres encima y representando distintas edades. Aún tendrían que pasar varios siglos, hasta el , para que el rey Baltasar aparezca con la tez negra y los tres reyes, además de representar las edades, representen las tres razas de la Edad Media. Melchor encarnará a los europeos, Gaspar a los asiáticos y Baltasar a los africanos.

En España a partir del se inició la tradición de convertir la noche de Reyes (noche anterior a la Epifanía) en una fiesta infantil con regalos para los niños, a imitación de lo que se hacía en otros países el día de Navidad, en homenaje al santo oriental San Nicolás. Fue en 1866 cuando se celebró la primera cabalgata de Reyes Magos en Alcoy, tradición que se extendió al resto del país y posteriormente a otros países, especialmente a países de cultura hispana.

La palabra «mago», proviene del persa "ma-gu-u-sha", que significa sacerdote. Llegó al griego como μαγός ("magós", plural: μαγοι, "magoi"), refiriéndose a una casta de sacerdotes persas o babilonios, que estudiaban las estrellas en su deseo de buscar a Dios. Del griego pasó al latín como "magus", plural "magi", /mágui/ (cf. "magister", /maguíster/) de donde llegó al español "mago".

La figura católica de los Reyes Magos tiene su origen en los relatos del nacimiento de Jesús. Algunos de esos relatos fueron integrados en los evangelios canónicos que hoy conforman el Nuevo Testamento de la Biblia. Concretamente el Evangelio de Mateo es la única fuente bíblica que menciona a unos magos (aunque no especifica los nombres, el número ni el título de reyes) quienes, tras seguir una estrella, buscan al «rey de los judíos que ha nacido» en Jerusalén. Dicha estrella les guía hasta Jesús nacido en Belén, y a quien presentan ofrendas de oro, incienso y mirra.

Si bien parece contradictorio que practicantes de la magia (severamente amonestada tanto en el "Antiguo" como en el "Nuevo Testamento") sean admitidos como adoradores de Jesús, hay que tener en cuenta que el término griego "magós" no era utilizado únicamente para referirse a los hechiceros. Se utiliza, en este caso, para referirse a ‘hombres sabios’ (así se los llama en diversas versiones de la Biblia en inglés) o, más específicamente, "hombres de ciencia". De hecho, también poseían conocimiento de las Escrituras (Mateo 2:5-6) y, desde antiguo se ha sostenido que pertenecían al mazdeísmo.

Mateo no explicita que sean astrólogos que conocieran con precisión el movimiento de alguna estrella (2:7) a pesar de ser ésta la creencia general. Aunque bien intencionados, su visita es causa de turbación general y despierta la desconfianza de Herodes (2:3), pues veía al nuevo Mesías como un rival. A pesar de ser anciano y de haber reinado ya por más de treinta años, Herodes les ruega que averigüen el sitio preciso del nacimiento del Mesías (2:8) con el fin de poder, así, acabar con su potencial competidor. Los sabios, que no sospechan eso, encuentran al Niño, lo adoran y le obsequian oro, incienso y mirra (2:11). Un ángel previene a los magos de las intenciones que Herodes guardaba (2:12), así que no regresan donde él. Iracundo, el rey manda a matar a todos los niños menores de dos años. Para entonces, José ha sido avisado en sueños (2:13) de que debe huir a Egipto con los suyos.

A partir de ese relato, se han ido elaborando numerosas leyendas sobre los hechos y la personalidad de estas tres figuras.

Según la interpretación de José Luis Sicre, en el tiempo en que fue escrito el Evangelio de Mateo se estaba produciendo un incremento de conversiones paganas al cristianismo frente a las de los propios judíos. La incursión de estos fragmentos sobre los magos de Oriente en el Evangelio de Mateo subraya este hecho y lo utiliza como argumento de conversión: si los de fuera vienen y lo adoran (se convierten) ¿cómo no os dais cuenta los que lo tenéis entre vosotros?.

También existen otras interpretaciones astrológicas y cabalísticas sobre la figura de los Reyes Magos.

Según la interpretación ofrecida por Eric Rodríguez, se tiene lo siguiente:

Ya el término griego μάγος (literalmente “magos”) había caído en un uso peyorativo o deteriorado desde al menos el siglo III a. C. ("cf." versión Septuaginta) por la extracción de su origen y contexto cultural, y que es como se usa aún en la época del Nuevo Testamento ("cf." Hechos de los apóstoles, 8:9, 13:6, 19:13).
No obstante, en el texto original "koiné" (griego bíblico) de Mateo 2:1 dice:

A diferencia de los magos que ya se encontraban dispersos en tierra de Israel y todo el mundo helénico, el énfasis que se emplea al decir “de Oriente”, marca un cambio de connotación: el autor busca traer a la mente un personaje asociado con el Oriente, diferente a los sabios convencionales de Israel (rabinos), que conociera además las profecías mesiánicas y que fuera autoridad bíblica para el lector judío (ya que se acepta a nivel general que el Evangelio de Mateo fue escrito para hebreos y aún en lengua hebrea según el testimonio de casi todos los padres de la Iglesia).

Hay que tener en cuenta, además, que Oriente puede designar la región de Babilonia, y por lo tanto, para algunos autores, los magos podrían corresponder a los llamados en arameo מדנחאי (Medinja’ey, “doctores babilónicos de la tradición oral”) cuya escuela perduraría hasta entrado el siglo octavo de nuestra era en Babilonia, y quienes conociendo la interpretación de lo dicho en Números 24:17 ("cf". tárgum de Onqlós/Onkelos sobre este pasaje), habrían sido guiados por Dios hasta el Mesías. En este caso la estrella simbolizaría al mismo Mesías según el lenguaje midrásico contemporáneo.

Las tradiciones antiguas que no fueron recogidas en la Biblia ―como por ejemplo el llamado "Evangelio del Pseudo Tomás", o "Evangelio de la infancia", del siglo II― son sin embargo más ricas en detalles. En ese mismo evangelio apócrifo se dice que tenían algún vínculo familiar, y también que llegaron con tres legiones de soldados: una de Persia, otra de Babilonia y otra de Asia ("sic").

En el último libro escrito por el papa Benedicto XVI sobre Jesús de Nazaret, "La infancia de Jesús", se menciona de tal modo a los Reyes Magos que algunos han sostenido que probablemente no venían de Oriente, sino de Tartessos, una zona que los historiadores ubican entre Huelva, Cádiz y Sevilla (Andalucía, España). El texto, sin embargo, dice: «Así como la tradición de la Iglesia ha leído con toda naturalidad el relato de la Navidad sobre el trasfondo de Isaías 1:3, y de este modo llegaron al pesebre el buey y el asno, así también ha leído la historia de los Magos a la luz del Salmo 72:10 e Isaías 60. Y, de esta manera, los hombres sabios de Oriente se han convertido en reyes, y con ellos han entrado en el pesebre los camellos y los dromedarios». Eso relata Benedicto XVI y continúa: «La promesa contenida en estos textos extiende la proveniencia de estos hombres hasta el extremo Occidente (Tarsis, Tartessos en España), pero la tradición ha desarrollado ulteriormente este anuncio de la universalidad de los reinos de aquellos soberanos, interpretándolos como reyes de los tres continentes entonces conocidos: África, Asia y Europa». Al respecto, el secretario general de la Conferencia Episcopal, monseñor Juan Antonio Martínez Camino, recordó que en ningún momento el Santo Padre dice que «los Reyes Magos fueran andaluces, lo que explica el Papa es que los magos no eran otra cosa que buscadores de la verdad. Representaban a todos los hombres buscadores de Dios de todos los tiempos y de todos los lugares y eso incluía a todo el mundo hasta entonces conocido y cuyo límite occidental era Tartessos, en la península ibérica», explica. Al mencionar a Tartessos, Benedicto XVI se refiere a este límite geográfico que tenía el mundo en el siglo I a. C, «los Magos son de Oriente pero que en esa inquietud por buscar a Dios están representados los hombres buscadores de Dios de todos los lugares y de todos los tiempos».

Con respecto a los nombres de los reyes (Melchor, Gaspar y Baltasar) las primeras referencias parecen remontarse al siglo V por medio de dos textos, el primero titulado "Excerpta latina bárbari", en el que son llamados Melichior, Gathaspa y Bithisarea, y en otro evangelio apócrifo, el "Evangelio armenio de la infancia", donde se les llama Balthazar, Melkon y Gaspard.
Los nombres son además diferentes según la tradición siríaca: Larvandad, Gushnasaf y Hormisdas.

La historia narrada en el Evangelio de Mateo, cuenta que los magos vinieron de Oriente guiándose por una estrella, la cual los condujo hasta Belén (de ahí el nombre de estrella de Belén).

Antes de llegar, visitaron al rey Herodes el Grande en la ciudad de Jerusalén, a quien interrogaron por el nacimiento del "Rey de los Judíos". El monarca, después de consultar a los escribas versados en la Biblia, les aseguró que el niño debía nacer en la pequeña ciudad de Belén, como establecía la profecìa de Miqueas. Agregó, astutamente que, de regreso, hablaran con él para darle noticia del sitio exacto donde se encontraba dicho niño; y, así, poder ir él también a adorarle. En realidad, según el relato bíblico, su intención era darle muerte.

En Belén, los magos volvieron a ver la estrella, hallaron a Jesús recién nacido y lo adoraron; ofreciéndole oro (representando su naturaleza real, como presente conferido a los reyes), incienso (que representa su naturaleza divina, empleado en el culto) y mirra (un compuesto embalsamador para los muertos, representando el sufrimiento y muerte futura de Jesús). Parece ser que por el hecho de traer tres dones, se dio por sentado que eran tres los personajes que los traían. Aunque también en algún momento las distintas tradiciones han señalado que eran cuatro, siete y hasta doce magos.

Al regreso, advertidos los magos por un sueño de las intenciones del rey, no volvieron a Jerusalén. Herodes, entonces, ordenó dar muerte a todos los niños menores de dos años residentes en Belén, episodio conocido como la matanza de los inocentes. Un nuevo mensaje celestial, advirtió a José de la amenaza y éste, llevando a María y a Jesús, huyó a Egipto.

La primera vez que surge el nombre con que hoy conocemos a los Reyes Magos es en la iglesia de San Apolinar Nuovo, en Rávena (Italia). El friso de la imagen está decorado con mosaicos de mediados del siglo VI que representan la procesión de las vírgenes. Esta procesión está conducida por tres personajes vestidos a la moda persa, tocados con un gorro frigio y su actitud es la de ir a ofrecer lo que llevan en las manos a la Virgen que está sentada en un trono y tiene al Niño en su rodilla izquierda. Encima de sus cabezas se pueden leer tres nombres, de derecha a izquierda: "Gaspar, Melchior, Balthassar"...
Poco a poco la tradición ha ido añadiendo otros detalles a modo de simbología: se les ha hecho representantes de las tres razas conocidas en la antigüedad, representantes de las tres edades del hombre y representantes de los tres continentes (Asia, África y Europa).

La llegada de los Reyes Magos es un tema tratado también en los evangelios apócrifos. Según la tradición esotérica aplicada al cristianismo, estos personajes procedían del lugar donde se encontraba el Preste Juan.

Otra leyenda cuenta que, después de la resurrección de Jesús, el apóstol Tomás los halló en el reino de Saba, donde fueron por él bautizados y consagrados obispos. Después fueron martirizados en el año 70 y depositados en el mismo sarcófago. Allá fue Santa Elena a buscarlos, y halló tres cuerpos coronados, dando por sentado que se trataría de los Reyes Magos, por lo que los trasladó a Constantinopla. Posteriormente, Federico I Barbarroja, en el siglo XII, los trasladó a Colonia, Alemania, donde hoy reposan con las coronas que supuestamente llevaron durante su existencia (según la tradición, los relicarios con sus presentes se hallan en el monasterio de San Pablo, en el Monte Athos). Miles de peregrinos empezaron a llegar a Colonia, lo que propició que en 1248 se iniciara la construcción de la catedral de Colonia, que llevaría más de 600 años terminarla. Hoy día es uno de los monumentos góticos más impresionantes de Europa. Colonia se ha convertido junto con Roma y Santiago de Compostela en uno de los grandes centros de peregrinación. Igualmente, existen leyendas que hablan de un cuarto rey mago.

Según las diversas tradiciones de los reyes magos, el número de ellos varía; así se puede encontrar los siguientes reyes magos:


Los reyes magos son conocidos también como los Santos Reyes.

Con el tiempo, en países de tradición católica, se adoptó la costumbre de celebrar al mismo tiempo el día de la Epifanía (el 6 de enero) y la festividad de los Reyes Magos, conjugándose así la manifestación de Jesús al mundo no judío con la fiesta de estos personajes que representaban justamente ese mundo de gentiles. Poco a poco, se fue olvidando el significado verdadero de la palabra "epifanía" y la convirtió en un sinónimo de "adoración de los Magos".

El día 6 de enero es festivo en Cuba, España, México, Puerto Rico, República Dominicana, Paraguay, Uruguay, Colombia y Venezuela.

En algunos lugares, las autoridades organizan la llamada "Cabalgata de Reyes" el día 5 de enero, durante la cual los personajes suelen ir montados a caballo o en carrozas, vestidos con mantos y coronas, en lugar de la vestimenta frigia totalmente desconocida. En la mayoría de sitios donde sale la cabalgata, aparte de ir en ella los Reyes Magos, también hay carrozas de otros temas y distintos personajes, como pueden ser personajes infantiles y demás. El siguiente día, el 6 de enero, es festivo nacional. Ese día los niños disfrutan sus obsequios.
En España, la tradición dice que los regalos de Navidad a los niños los traen los Reyes Magos la noche del 5 al 6 de enero, compitiendo con la reciente introducción de Papá Noel en las costumbres navideñas debido a la influencia de otras culturas. Antes, los niños deben enviar una carta a los reyes enumerando los regalos que quieren y los méritos por los que merecen recibirlos. También es tradición que la noche del 5 de enero los niños dejen sus zapatos en algún lugar de la casa, junto a la puerta, en una ventana; incluso se dejan dulces para obsequiar a los Reyes Magos y agua o comida para los camellos. Al día siguiente se encuentran allí los regalos o, en el caso de haber sido malos, carbón en su lugar (se trata de un dulce de feo aspecto pero golosina, al fin y al cabo). El día 6 de enero es festivo en toda España. La escalada consumista ha conseguido que también reciban regalos los adultos, en ocasiones usando el juego del amigo invisible. Es típico desayunar el Roscón de Reyes que en muchos lugares puede comerse la víspera, para merendar o, como postre, en la cena. En España estos roscones suelen contener una figurilla, popularmente conocida como "la sorpresa".

En varios países de Hispanoamérica existe la costumbre adoptada de los españoles de que los niños reciban regalos de los Reyes Magos, bien en la víspera, es decir, a la medianoche del 5 de enero, o en la mañana del 6 de enero (Argentina, México, República Dominicana, Puerto Rico, Paraguay y Uruguay). También se han heredado las costumbres de la carta a los Reyes y el carbón dulce en vez de regalos. La mayoría de los servicios postales aceptan estas cartas.

Al igual que la costumbre anglosajona en torno a Santa Claus, es frecuente que los reyes magos aparezcan en tiendas de regalos y centros comerciales, donde los niños tienen la oportunidad de tomar una foto sentados en sus rodillas y entregar la carta con sus peticiones directamente. La representación consta normalmente de un escenario con tronos y los símbolos característicos, como figuras o dibujos de camellos, la estrella, un buzón y adornos de aspecto oriental. En los tronos es donde se sientan los reyes, habitualmente se trata de empleados caracterizados. En ocasiones se representan los tres reyes de la tradición, pero dependiendo de las circunstancias o el tamaño del escenario, puede incluirse únicamente uno. Además van acompañados de un paje, personaje característico que se encarga de conducir a los niños desde donde esperan con sus padres hasta los reyes y de recoger las cartas.

En Puerto Rico, la noche del 5 de enero los niños corren por el patio recogiendo grama. Ponen la grama en una caja de zapatos y colocan la caja junto a su cama. La grama es usada para alimentar a los camellos. Los reyes entonces dejan regalos en las cajas.

En Perú, ha caído en desuso el dar regalos a los niños en esta fecha. La celebración que se acostumbra es la llamada Bajada de Reyes, que consiste en que una familia o comunidad realiza una pequeña celebración mientras se va desmontando el "Nacimiento". Cuando se trata de una comunidad, es costumbre dejar dinero mientras se retiran los adornos y figuras. Esta tradición incluso ha llegado a empresas privadas, las cuales realizan dicha celebración entre sus miembros.

Es interesante hacer notar que, en tiempo de la colonización española, especialmente en Cuba, República Dominicana, Puerto Rico, México y Uruguay este día era de asueto para los esclavos negros que salían a las calles a bailar al ritmo de sus tambores. Esto origina el nombre de "Pascua de los Negros" con que el día es aún conocido en algunos países como en Chile o Paraguay donde la comunidad afroparaguaya celebra el día de su santo (San Baltasar).

Los países de habla inglesa dedican el día 6 de enero a desmontar los adornos de la Navidad. Esta costumbre también se ha extendido a países de América Latina, convirtiéndose el 6 de enero en el último día de la temporada navideña. Antiguamente se celebraban festejos con ese motivo y se cocinaba un pastel en el que se escondía un haba o una pequeña moneda de plata. La persona que encontraba el haba o la moneda era nombrada "rey judío" o "señor del desorden" y se veía obligada a encargarse de los festejos de esa noche. Con el tiempo, la fiesta fue evolucionando y se incluyeron bailes de máscaras y representaciones teatrales. Esta tradición dio origen en España al típico roscón de reyes (también llamado rosca de reyes) que se toma en ese día y que esconde una pequeña sorpresa en su interior. En México, dicha rosca tiene en su interior varios muñecos pequeños de plástico los cuales representan al niño Jesús; aquella persona que en el momento de partir la rosca encuentra alguno de ellos, es encargado de hacer o invitar tamales y atole el 2 de febrero, día de la Candelaria.

En el año 1601 los abogados de Londres encargaron a Shakespeare una obra de teatro que se tituló "Noche de Reyes" y fue representada ante la reina Isabel I.

Desde la antigüedad, el tema de los Reyes Magos ha sido motivo de representación por artistas, pintores y escultores y también en la literatura. Han sido retratados habitualmente en número de tres; otras veces, cuatro; y, excepcionalmente, en número de dos. Es un tema abundantemente tratado durante la historia.

Hasta finales del siglo XIV no se comenzó a representar a uno de los magos de color negro, y solo a partir del siglo XVI fue cuando se generalizó esta forma de representarlos.

En las arquivoltas que enmarcan el tímpano de la portada románica de la Iglesia de Santo Domingo de Soria, del siglo XIII, se encuentra una de las representaciones más inusual de los Reyes Magos en el arte. Se trata del llamado "sueño de los Reyes Magos". En la representación, labrada en piedra, se representa a tres hombres barbados, de iguales rasgos físicos y sin corona real, tumbados hacia arriba en representación de su sueño y, junto a ellos, el ángel que según el Evangelio de Mateo les advierte en sueños de la intención de Herodes de matar a Jesús y que desencadenó la llamada matanza de los inocentes (que se representa en la siguiente arquivolta).

Entre los pintores que representan la escena de la Adoración de los magos, pueden citarse Andrea Mantegna, Botticelli, Giotto, Leonardo da Vinci, el Bosco, Velázquez, Rubens, Durero.

Por supuesto el cine no es ajeno a la figura de los Reyes Magos. Desde "Vida y pasión de Jesucristo" (1907), de Ferdinand Zecca, hasta "La Natividad" (2006), numerosas películas han incluido a estos personajes en alguna escena.

En la plaza de los Reyes Magos de la localidad alicantina de Ibi se encuentra un monumento dedicado a la figura de los tres Reyes Magos de Oriente, tan entroncados con la industria juguetera y de fuerte implantación en la localidad desde principios del siglo XX. Dicho monumento, de 5,8 toneladas, fue inaugurado el 5 de enero de 1974 y es una obra en piedra caliza del escultor granadino D. Aurelio López Azauste.

Otra localidad que también ha honrado a la figura de los tres Reyes Magos de Oriente es Juana Díaz, en la isla caribeña de Puerto Rico. Este pueblo es sede de las más destacadas devociones en honor a los tres Reyes Magos, y como homenaje al arraigo de esta tradición entre los puertorriqueños hay dos monumentos dedicados a los Reyes Magos:

En Logroño el 5 de enero de 2009 fue inaugurada por personajes que interpretaban a los Reyes Magos una escultura que los representa en la rotonda situada junto al Estadio Las Gaunas, en el cual estos mismos aterrizan en helicóptero cada 5 de enero.




</doc>
<doc id="2476" url="https://es.wikipedia.org/wiki?curid=2476" title="Robert Schuman">
Robert Schuman

Jean-Baptiste Nicolas Robert Schuman, más conocido como Robert Schuman (Luxemburgo, 29 de junio de 1886-Scy-Chazelles, 4 de septiembre de 1963), fue un político francés de origen germano-luxemburgués. Es considerado como uno de los «padres de Europa» en referencia a su determinante participación en la creación de las Comunidades Europeas.
Como miembro fundador del Movimiento Republicano Popular (MRP), fue uno de los principales dirigentes de la Cuarta República Francesa, siendo ministro de Finanzas, presidente del Consejo de Francia, ministro de Asuntos Exteriores y ministro de Justicia. También se desempeñó como diputado de Mosela entre 1919 y 1962, con una pausa entre 1942 y 1946.

Su cargo como ministro de Asuntos Exteriores (1948-1952), lo llevó a ser el principal negociador francés de los tratados firmados entre el final de la Segunda Guerra Mundial y el principio de la Guerra Fría (Consejo de Europa, OTAN, CECA, etc.). Además, fue él quien propuso por primera vez, el 9 de mayo de 1950, un proyecto de integración europea, que daría lugar a la Comunidad Europea del Carbón y del Acero. Fue también el primer presidente de la Asamblea Parlamentaria Europea (1958-1960), precedente del actual Parlamento Europeo.

Robert Schuman nació en Clausen, un barrio de Luxemburgo. Su casa natal es ahora la sede del Centro de Estudios Europeos Robert Schuman.

Su madre era luxemburguesa, por lo que es en este país donde realizó la mayor parte de su formación escolar. Su padre, oriundo de Mosela, sirvió en el ejército francés durante la Guerra franco-prusiana (1870-1871), tras la cual adoptó la nacionalidad alemana, luego de que Alsacia-Lorena fuese anexionada por el Imperio Alemán. Después se mudó a Luxemburgo, donde fue considerado como alemán al igual que su esposa, nacionalizada alemana a causa de su vínculo matrimonial. La pareja y su hijo único conformaban una familia de clase media. El padre era propietario de un terreno dedicado a la explotación agrícola propia y al alquiler de parcelas.

La educación familiar de Schuman estuvo enmarcada en la práctica del catolicismo que profesaban sus progenitores. En su infancia asistió a la escuela comunal de Clausen antes de continuar sus estudios en l’Athénée Grand-Ducal, donde aprendió el francés. En 1900 murió su padre, cuatro años antes de que Robert Schuman hubiese finalizado sus estudios de secundaria en el Liceo Imperial de Metz.

Estudió Derecho en las universidades de Múnich, Bonn y Berlín. Finalmente se graduó en la Universidad de Estrasburgo, para luego abrir su propio bufete en Metz en junio de 1912, meses después de la muerte accidental de su madre, a quien lo unía una estrecha intimidad espiritual. En este periodo contempló la idea de iniciarse en el sacerdocio, pero finalmente optó por una vida a medio camino entre el clero y el trabajo como funcionario público. Gracias a la herencia dejada por sus padres, Schuman no tuvo dificultades económicas durante toda su vida.

En la universidad, Schuman formó parte de la Corporación Unitas, integrada principalmente por seminaristas y estudiantes de Teología. Sin embargo, su carácter reservado y su juventud hicieron que su paso por la organización no tuviera importancia significativa.

Schuman no prestó servicio militar por razones de salud. Pero al estallar la Primera Guerra Mundial, el reclutamiento se intensificó, por lo que fue empleado en la administración alemana debido a sus competencias jurídicas. Incorporado en el servicio auxiliar, fue radicado en Metz en una unidad de no combatientes. Allí prestó funciones de soldado secretario durante un año. Tras ser relevado del cargo, fue nombrado adjunto de la administración en Boulay, donde permaneció hasta el fin de la guerra en 1918, mientras continuaba paralelamente con su trabajo en el despacho de abogado de Metz. Hasta ese momento Schuman, quien nunca se casó y vivió siempre de manera austera, tenía una cultura esencialmente alemana.

Una vez terminada la guerra, Schuman se inscribió como abogado en Metz, estatuto que conservó durante toda su vida. Su formación de jurista y su conocimiento del francés hizo que las autoridades lo invitaran a participar en la reintegración de Alsacia-Mosela.

Los católicos de Lorena estaban preocupados por la integración al Estado francés, temerosos de esa república anticlerical. La disolución de las órdenes religiosas (1902-1904) en Francia y la separación de Estado e iglesia (1905), que suprimió toda subvención económica, no fueron aplicadas en Alsacia-Mosela en la época alemana. Estas regiones vivían aún bajo el concordato de 1801. Para defender esta particularidad, Schuman fue solicitado por los grupos católicos para presentarse como diputado y fue elegido para el cargo en 1919 como representante de Mosela por la circunscripción de Thionville, cargo que mantuvo durante toda su carrera.

En 1919 fue acusado en la asamblea de Lorena de haber formado parte del ejército alemán. Schuman respondió que esas acusaciones no iban a despertar aversiones por su parte y que los que las formulaban simplemente estaban manipulados. «Es mejor rezar por ellos que maldecirles», afirmó antes de salir de la cámara. En París, Schuman disfrutaba en ese momento de una relativa notoriedad en los círculos católicos. En esa época impulsó la formación de secciones departamentales de la CFTC (Confederación francesa de trabajadores cristianos) en Mosela. Espiritualmente se decía próximo a San Francisco de Asís.

Durante los años 1920 se asoció a los esfuerzos de paz y la acción del presidente Aristide Briand (1924-1932), que buscaba la aproximación entre la República de Weimar y la Francia del gobierno de Raymond Poincaré. En la década siguiente se mostró favorable a las sanciones impuestas a Italia, luego de que esta atacara Etiopía, pero por otra parte, se declaró partidario de los acuerdos de Múnich de 1938.

Su actividad parlamentaria fue modesta. De 1929 a 1939, fue miembro de la comisión de finanzas de la Asamblea nacional y permaneció en el Senado sin interrupción hasta 1940. Inicialmente estuvo inscrito en el partido Union Républicaine Lorraine, asociado al Bloc National (Poincaré), y después, en 1931, al Parti Démocrate Populaire, uno de los ancestros del Movimiento Republicano Popular. Schuman fue desde el principio adversario del Frente Popular (1936-1938), aunque personalmente apreciaba a Léon Blum.

En 1926 Schuman compró una casa en Scy-Chazelles en las afueras de Metz. Allí formó una colección de más de cuatro mil libros y autógrafos a partir de 1935 (entre ellos un autógrafo de Carlos I de España). Asimismo, durante varios años alquiló un apartamento en el 6.° piso sin ascensor en la rue du Bac de París.

Entre 1939 y 1945, Francia y Alemania se vieron enfrentadas en el marco de la Segunda Guerra Mundial. Schuman fue nombrado subsecretario de Estado para los refugiados en la administración de Paul Reynaud formada el 21 de marzo de 1940 —periodo de la drôle de guerre—, cargo que conservó durante el primer gobierno del mariscal Philippe Pétain. Más adelante votó a favor de otorgar plenos poderes a Petain, pero se negó a participar en dicho gobierno y regresó a Mosela que había sido nuevamente anexada a Alemania. Una vez en Metz, Schuman quemó su correspondencia misteriosamente. Después acudió a la policía para discutir la repatriación de los refugiados de Mosela y fue detenido por la Gestapo (policía secreta de la Alemania nazi) el 14 de septiembre de 1940, tras rechazar la solicitud de cooperación presentada por los nazis. Fue puesto bajo custodia en un hotel familiar de Neustadt (Renania-Palatinado) en abril de 1941. Allí le fue permitido recibir visitas y disfrutó de cierta libertad de desplazamiento, por lo que podía hacer trayectos en Alemania a excepción de Alsacia y Lorena que habían sido anexionadas por el gobierno nazi.

Persuadido de la posibilidad de ser trasladado a prisión o a un campo de prisioneros, Schuman logró evadirse en agosto de 1942 y llegó a la zona libre, para más tarde entrar en la clandestinidad el siguiente noviembre, cuando los nazis decidieron invadir el sur de Francia. Permaneció oculto en diferentes monasterios hasta que las fuerzas de ocupación fueron expulsadas.

No se conoce la reacción de Schuman al "Llamamiento del 18 de junio" en el que Charles de Gaulle convocaba al pueblo francés a mantener la resistencia ante el invasor, lo que suponía un rechazo a la petición de armisticio por parte del general Pétain. Por el contrario, Schuman fue claro en su respaldo a la posición inicial de Pétain durante la guerra y nunca formó parte de la Resistencia francesa.

Tras la invasión de Francia por los aliados y la consecuente expulsión de la Wehrmacht, el general De Lattre al mando del ejército francés, contactó a Schuman en septiembre de 1944 a fin de tener un consejero experimentado en asuntos de Alsacia-Lorena. Tres semanas después, el ministro de Guerra, André Diethelm, exigió que Schuman fuera arrestado. La sociedad de Metz acogió a Schuman, pero las autoridades lo trataron como a un exministro de Petain y como el parlamentario que votó a favor de los plenos poderes. Aunque fue considerado entonces como «indigno» e «ineligible», logró formar parte del comité departamental de liberación, donde trató de moderar la depuración.

Schuman escribió al general Charles de Gaulle el 24 de julio de 1945 y consiguió que hiciera archivar el expediente en su contra. Se proclamó su inocencia en el mes de septiembre siguiente, lo que le permitió retomar su lugar en la vida política del país. Rápidamente se convirtió en uno de los líderes principales de la Cuarta República Francesa y ocupó el cargo de ministro de Finanzas entre 1946 y 1947 en un momento crucial donde la inflación y el mercado negro tomaron fuerza.

El 24 de noviembre de 1947 asumió el cargo de presidente del Consejo de Francia (jefe de gobierno), cargo en el que estuvo hasta el 26 de julio de 1948, para volver brevemente en septiembre de ese mismo año durante apenas dos días, desde el día 5 hasta dimitir el 7 de septiembre. Durante su mandato nuevamente fue acusado de haber sido un oficial alemán, esta vez por el comunista Jacques Duclos. En julio de 1948, en el periodo que transcurrió entre sus dos gobiernos, fue nombrado ministro de Asuntos Exteriores, cargo que ocupó hasta 1952, a pesar de la inestabilidad que caracterizó al gobierno de la Cuarta República. Su bilingüismo le fue útil en las relaciones con Alemania, donde pronunció conferencias en el idioma local.
La opinión francesa vacilaba entre varias posibilidades con respecto al futuro inmediato de la Alemania derrotada. La mayoría prefería ejercer un control aliado en una Alemania descentralizada, un híbrido entre la Confederación Germánica (1815-1866) y la Confederación Alemana del Norte (1867-1871). Esta fue la posición de De Gaulle y sus sucesores hasta 1947. Otros políticos, entre los que se encontraba Schuman, prefirieron desmantelar las fuentes institucionales y culturales del militarismo alemán y fortalecer las relaciones intraeuropeas.

En marzo de 1948 fue creada la Unión Europea Occidental (Francia, Benelux y Reino Unido), como respuesta a la toma de poder de los comunistas en Praga. Esta unión se americanizó pronto y sirvió de base para la creación de la Organización del Tratado del Atlántico Norte (OTAN) en 1949.

La fusión de dos uniones aduaneras: el Benelux (surgida entre guerras) concretada en 1948 (sirvió de laboratorio a la CEE) y un proyecto de unión aduanera franco-italiana lanzado en 1947 que fracasó finalmente en 1951. Por su parte, el Consejo de Europa creado en 1949 fue destinado a preparar la confederación de estados europeos.

A comienzos de los años 1950, Schuman compró un apartamento en la rue de Verneuil, 6 en París, aunque siempre conservó su casa en Metz. A partir de 1953 no volvió a ser convocado para ningún cargo ministerial, a excepción de la cartera de Justicia en 1955. En 1962 no volvió a presentarse como candidato a diputado por Mosela.

Schuman no disponía de cualidades como orador. Ello no impidió que pronunciase uno de los discursos más trascendentales en la historia europea. El 9 de mayo de 1950, Schuman se dirigió a más de doscientos periodistas para presentar una declaración preparada junto a Jean Monnet (sentado a su derecha durante el discurso), que es considerada como la primera propuesta oficial para la construcción de una Europa integrada y que se conoce a partir de esa fecha como la "Declaración Schuman".

Ese día nació la Europa comunitaria, actualmente concretada en la Unión Europea. El plan de Schuman fue la base en la que se asentó la UE, una especie de primera piedra de las instituciones siguientes. En su discurso, Schuman proponía la creación de una comunidad franco-alemana para aprovechar conjuntamente el carbón y el acero de los dos países (en ese momento Alemania producía el doble de acero que Francia) bajo una Alta Autoridad común, independiente de los gobiernos y con poder para imponer sus decisiones. Una vez en funcionamiento, se ampliaría la comunidad a otros países europeos para formar un espacio de libre circulación de personas, mercancías y capital. Este sistema cruzado de intereses evitaría la posibilidad de una nueva guerra. Este proyecto de cooperación europea se presentó solo cinco años después de la capitulación de la Alemania nazi.

En la Cumbre de Milán de 1985 los Jefes de Estado y de gobierno decidieron establecer el 9 de mayo como el "Día de Europa" en conmemoración de esta declaración.

Schuman firmó el Tratado de París, del 18 de abril de 1951, que constituyó la Comunidad Europea del Carbón y del Acero (CECA) entre Alemania, Bélgica, Francia, Italia, Luxemburgo y los Países Bajos. Así, con 160 millones de habitantes, 210 millones de toneladas de carbón y 33 millones de toneladas de acero producidas, la CECA se convirtió en un interlocutor de peso en las relaciones económicas internacionales.

Impulsó el plan para la formación de un ejército europeo denominado "Comunidad Europea de Defensa", que fue rechazado por los franceses en 1954.

Planeó junto a Adenauer el Estatuto del Sarre. Esta iniciativa buscaba dotar a este estado alemán, que se encontraba ocupado por el ejército francés tras el final de la guerra, de un estatuto europeo que haría del Sarre la sede de las instituciones europeas y lo convertiría la capital de la CECA. Los dos líderes decidieron aprobar la medida por referendo, confiados de su resultado positivo. Sin embargo, los ciudadanos del Sarre se pronunciaron en contra de dicho estatuto y a favor de la reintegración en la Alemania occidental.

Schuman presidió el Movimiento Europeo entre 1955 y 1961 y se convirtió así en el primer presidente de la Asamblea Parlamentaria Europea (1958-1960), que le da al fin de su mandato el título de «padre de Europa». En ese período presidió en conjunto la CEE-CECA-CEEA donde los miembros eran designados por sus estados de origen. Su función fue consultiva. Entre tanto, en 1958, le fue otorgado el Premio Carlomagno.

Entre 1955 y 1956, Schuman ocupó la cartera de Justicia que fue su último cargo ministerial. En octubre de 1959, durante una visita oficial a Italia, comenzó a manifestar los primeros síntomas de la enfermedad que afectó progresivamente su salud, cuando en medio de una conferencia de prensa perdió la lucidez.

Tras retirarse definitivamente de la Asamblea parlamentaria europea, Schuman inició la redacción de su único libro, "Por Europa", con la idea de hacer descubrir sus intuiciones europeas. Pocos meses después cedió el escrito a un amigo cercano y le dio la libertad de publicarlo después de su muerte.

En 1959 le fue diagnosticada esclerosis múltiple, por lo que se le prohibieron las caminatas y la lectura.

En enero de 1961, Schuman sufrió nuevamente ataque de esclerosis en Scy-Chazelles. Durante su caminata cotidiana, sufrió mareos y cayó al suelo sin perder totalmente el conocimiento. Su sirvienta pensaba que había sido invitado a cenar en casa de algún amigo y por ello no se inquietó de su tardanza. Al amanecer, Schuman fue encontrado inmóvil al borde de un camino por un guardia, bajo la lluvia.
En el verano de 1963, Schuman ya no podía hablar y solo era capaz de mirar y tomar la mano de su interlocutor. Algunos días antes de su muerte, el obispo de Metz, tras administrarle la unción de los enfermos, le hizo la lectura de una carta que el papa Pablo VI había escrito para Schuman. El 4 de septiembre de 1963, Robert Schuman falleció, tras una noche de agonía, a las nueve horas y treinta minutos.

Después de las exequias solemnes en la catedral de Saint-Etienne de Metz, su cuerpo fue inhumado en el cementerio municipal de Scy-Chazelles. Solamente un representante del Estado francés, el vicepresidente de la Asamblea nacional, asistió a la ceremonia. La radio y la televisión de Francia dedicaron una corta emisión a la cobertura de su fallecimiento. En 1966 los restos de Schuman fueron trasladados a la pequeña iglesia fortificada de St. Quentin de Scy-Chazelles, frente a la casa Maison de Robert Schuman, perteneciente al Consejo General del Departamento del Mosela.

La vida privada de Schuman, sus fuentes de inspiración y sus gustos siguen siendo en gran medida desconocidos.

Lo que distingue a Schuman de sus predecesores en el Ministerio de Relaciones exteriores es su enfoque de la cuestión alemana. Buscaba no repetir los errores del Tratado de Versalles de 1919, fundándose en que "la paz solamente puede basarse en la igualdad".No fue partidario del desmantelamiento de las fábricas alemanas, pero negoció los intereses de Francia en la industria del carbón. Se inclinó ante la necesidad de crear un estado alemán en el oeste, pero apoyó la creación de un estatuto del Sarre (separación de la Sarre y apego económico a Francia) y se opuso el rearme de Alemania. Schuman supo ganarse la confianza de los líderes belgas, británicos y estadounidenses; construyó una relación positiva con los dirigentes de la Democracia Cristiana italiana y cuidó sus vínculos con los líderes Luxemburgueses. Su actividad le valió que el canciller de Alemania Konrad Adenauer lo considerase como el "padre de la amistad entre los dos países".

La Declaración Schuman exponía que la reconciliación franco-alemana representaría el preludio de la integración europea. Desde entonces, la Unión Europea se ha construido por medio de sucesivas adiciones de funciones en un sistema que no corresponde a ningún modelo; es una especie de "federalismo a la inversa", en el que se ha realizado primero la transferencia de las competencias económicas y luego la de los poderes políticos, al contrario de lo que ocurre tradicionalmente en los estados federales. Más de medio siglo después de presentarse esta propuesta, la Declaración Schuman sigue siendo un patrón para medir los progresos de la Unión Europea. En este sentido, la orientación que Schuman dio a la integración europea se mantiene vigente.

La obra "Por Europa" (en francés "Pour l’Europe") es el único libro de la autoría de Robert Schuman y. Fue publicado originalmente en 1963. Este libro es su testamento político y en él profundiza la relación entre la formación de la democracia moderna y el cristianismo:

Durante la presidencia de François Mitterrand en la década de 1980, se planteó la posibilidad de trasladar los restos de Schuman al Panteón de París en reconocimiento a su obra, pero la propuesta fue rechazada por autoridades locales, que prefirieron conservarlos en Scy-Chazelles. Asimismo, desde hace varios años está abierta la causa de beatificación de Schuman. Una vez finalizada la fase diocesana, la documentación se encuentra en la Santa Sede. La causa fue apoyada por parlamentarios europeos, especialmente franceses y alemanes y está promovida por el "l'Institut Saint Benoît, Patron de l'Europe". Actualmente está considerado Siervo de Dios, la primera de las etapas para ser canonizado.




</doc>
<doc id="2477" url="https://es.wikipedia.org/wiki?curid=2477" title="Rodopsina">
Rodopsina

La rodopsina es una proteína transmembranal que, en humanos, se encuentra en los discos de los bastones de la retina. Consta de una parte proteica, opsina, y una no proteica que es un derivado de la vitamina A que es el 11-cis-retinal. Es inestable y se altera fácilmente con la energía lumínica, se decolora y descompone por exposición a la luz y se regenera con la oscuridad.

Una mayoría microorganismos marinos no fotosintéticos captan energía de la luz solar mediante rodopsina. La proteína permite a estos organismos utilizar la energía del sol para moverse, crecer y sobrevivir ante la falta de nutrientes. La rodopsina está altamente conservada y presente en los tres grandes dominios (arqueas, bacterias y eucariotas), lo que sugiere una aparición temprana y un papel fundamental en la evolución.

La opsina es una cadena polipeptídica formada por unos 348 aminoácidos. La opsina se distribuye en siete tramos de hélice alfa que se sitúan perpendiculares a la membrana unidos por partes proteicas sin estructura.

También se le llama púrpura visual, debido a su color y a que la encontró por primera vez en la retina de las ranas Franz Boll.

El carboxilo terminal se sitúa en la parte citosólica y el amino en posición intradiscal.

El 11-cis-retinal se sitúa unido a una de las hélices alfa en el centro de la molécula y colocado perpendicularmente. Esta colocación hace que cuando llegue luz incida en el 11-cis-retinal y este se transforme produciendo reacciones que llevan a un impulso nervioso.

Los bastones, que contienen rodopsina, son los responsables de la visión en condiciones de baja luminosidad. 


</doc>
<doc id="2479" url="https://es.wikipedia.org/wiki?curid=2479" title="Resina">
Resina

La resina es una secreción orgánica que producen muchas plantas, particularmente los árboles del tipo conífera. Sirve como un recubrimiento natural de defensa contra insectos u organismos patógenos. Es muy valorada por sus propiedades químicas y sus usos asociados, como por ejemplo la producción de barnices, adhesivos y aditivos alimenticios. También es un constituyente habitual de perfumes o incienso.
En muchos países, entre ellos España, es frecuente referirse a la "resina" como "resina de pino" ya que esta conífera es su principal fuente.

No existe acuerdo en la denominación de la resina y sus derivados. En este artículo se utilizará la aceptada por la Academia de la Lengua Española. Cuando pueda dar origen a confusión se incluyen los sinónimos utilizados con más frecuencia.
El término incluye también sustancias sintéticas con propiedades similares a las resinas naturales. De esta forma las resinas se dividen en: resinas naturales y resinas sintéticas.



La resina es una mezcla compleja de terpenos, ácidos resínicos, ácidos grasos y otros componentes complejos: alcoholes, ésteres...
La proporción de cada componente es función de la especie arbórea y el origen geográfico. Los valores típicos son:
Por destilación a presión ambiente, es posible separar dos fracciones:

El oficio de resinero era muy común entre los pueblos de montaña durante gran parte del siglo pasado. De los extensos pinares se extraía la resina que era vendida a buen precio en el mercado, puesto que su utilización en la industria era muy variada. Las nuevas técnicas de producción, y los nuevos materiales han relegado este oficio al olvido.

La provincia de Segovia, por estar encuadrada dentro de la comarca Tierra de Pinares, ha sido la mayor productora de resina en España, destacando la villa de Cuéllar, que su alta producción permitía el abastecimiento de parte de Castilla y Andalucía. Además, en el año 1958, la imagen de la Virgen del Henar, patrona de la Comunidad de Villa y Tierra de Cuéllar fue proclamada patrona de los resineros de España por el sumo pontífice Pío XII. Otro importante foco de producción en la provincia fue la villa de Coca, y ambas volvieron a restablecer la industria en la zona en el siglo XXI.

En Molinicos (Albacete) la industria resinera extraía grandes cantidades de este material de los extensos pinares existentes en el municipio. Aún hoy podemos observar la huella de esta industria en los troncos de los pinos.



</doc>
<doc id="2480" url="https://es.wikipedia.org/wiki?curid=2480" title="Resina verdadera">
Resina verdadera

La resina verdadera es una resina dura, quebradiza, parecida exteriormente a la goma, pero insoluble y que no se reblandece en agua. La resina verdadera más típica es la colofonia, no es una resina natural, puesto que se obtiene de la destilación seca de la oleorresina del pino (la trementina).


</doc>
<doc id="2484" url="https://es.wikipedia.org/wiki?curid=2484" title="Ruppiaceae">
Ruppiaceae

Las rupiáceas (nombre científico Ruppiaceae) son una familia de plantas monocotiledóneas, herbáceas, perennes, acuáticas marinas sumergidas. La familia es reconocida por sistemas de clasificación modernos como el sistema de clasificación APG III (2009) y el APWeb (2001 en adelante), donde posee un solo género, Ruppia, con 7 especies. La familia puede ser reconocida por sus hojas dísticas, serruladas, con una vena media de fácil reconocimiento y base envainadora, y entrenudos bien desarrollados.

Las hojas no son rizomatosas, finas. Normalmente son anuales pero pueden ser perennes, no tienen agregaciones ni basales ni terminales de las hojas. El crecimiento del tallo puede ser spmpodial. Puede ser halófilo e hidrofítico, adaptados tanto a la vida marina como al agua dulce. Las hojas pueden estar sumergidas o no. No son heterófilas. Las hojas son pequeñas o medianas opuestas, alternadas o verticiladas. Las vainas foliares tiene márgenes libres. La lamina es entera, setosa o lineal. Presentan una vena, sin vénulas cruzadas. Se observan escamas axilares. No presentan estomas. El mesófilo carece de cristales de oxalato. 

La familia fue reconocida por el APG III (2009), el Linear APG III (2009) le asignó el número de familia 41. La familia ya había sido reconocida por el APG II (2003).

En los sistemas de clasificación modernos como el sistema de clasificación APG III (2009) y el APWeb (2001 en adelante) el género está en su propia familia Ruppiaceae. Anteriormente estaba en Potamogetonaceae, pero debe ser escindido para que Potamogetonaceae se mantenga monofilética.

Según el Real Jardín Botánico de Kew (visitado en enero de 2009), la familia posee 7 especies:
Cosmopolita.
México, Puerto Rico. 



</doc>
<doc id="2486" url="https://es.wikipedia.org/wiki?curid=2486" title="Rhodophyta">
Rhodophyta

Las algas rojas o rodófitas (filo Rhodophyta, del griego ῥόδον, "rosa" y φυτόν, "planta") son un importante grupo de algas que comprende unas 7000 especies de una gran diversidad de formas y tamaños. Forman parte del clado Archaeplastida junto a Glaucophyta (glaucofitas) y Viridiplantae (plantas verdes). Según el criterio que se tome se incluyen entre las plantas o entre los protistas.

Se caracterizan por su inmovilidad debido a la carencia o pérdida evolutiva de flagelos en todas las etapas de su ciclo vital. Sus plastos presentan dos membranas, clorofila "a" y pigmentos accesorios ficobiliproteínas y carotenoides, los cuales enmascaran el color de la clorofila y le dan el color rojo distintivo de estas algas. Están bien representadas en aguas profundas. 

Rhodophyta se origina en el Mesoproterozoico y se divide filogenéticamente en dos clados: Cyanidiophytina y Rhodophytina, los cuales divergieron hace unos 1200 millones de años.

Son un grupo primitivo de pequeñas algas unicelulares esféricas que presentan una coloración similar a las algas glaucófitas que va de verde a verde azulada (o verde cian). Se caracterizan por ser extremófilas debido a su condición termoacidófila (son termófilas e hiperacidófilas), habitando en aguas termales, calderas volcánicas y algunos entornos ácidos producto de la actividad humana, mostrando además resistencia a la presencia de metales tóxicos. 

Su reproducción es asexual por división mitótica, formando en ocasiones endosporas en número 2, 4, 8 y a veces hasta 16 células hijas. Son organismos muy simples, y en la mayoría de ocasiones poseen un solo cloroplasto y una sola mitocondria, los cuales se reproducen por división sincronizada entre la célula y estos mismos organelos.

La simplicidad de estas algas estaría en relación con la pérdida evolutiva de genes. Se estima que Cyanidiophytina habría perdido 3 veces más genes que Rhodophytina.

El clado Rhodophytina está conformado por las algas rojas propiamente dicho, con una coloración característica roja dada por el pigmento ficoeritrina. Son prácticamente marinas (solo 164 especies son de agua dulce) y se pueden encontrar en todo tipo de mares. Su ecosistema va desde zonas intermareales hasta zonas muy profundas, dependiendo de la transparencia del agua. Se pueden encontrar a 100 metros de profundidad, alcanzando hasta los 250 metros en casos excepcionales. Son las algas más abundantes en lugares profundos, pues sus pigmentos les permiten captar las longitudes de onda de la luz del Sol que penetran más profundamente en el agua. Estos pigmentos, que absorben la luz azul y reflejan la roja, les dan su característico color rojizo.

Algunas algas rojas, por ejemplo la dulse o el nori, son utilizadas como alimento y usadas para producir agar, carragenanos y otros aditivos alimenticios.

La extensa mayoría de las algas rojas son marinas, aunque existen algunas especies que viven en agua dulce o en el suelo. Son de vida libre, epífitas o epizoicas y se conocen algunas formas parásitas. Algunas especies crecen utilizando como sustrato las algas pardas (que suelen ser más grandes) o sobre las conchas de mejillones y gasterópodos. Algunas son simbiontes de foraminíferos bentónicos, mientras que otras tienen plastos vestigiales y se ven obligadas a parasitar otras rodofitas. En el grupo se incluye muchas macroalgas notables, entre las que se encuentran la mayoría de las algas coralinas que secretan carbonato de calcio y cumplen un papel crucial en la formación de los arrecifes de coral. Estas formas coralinas pueden ser difíciles de distinguir de los corales.

Las algas rojas suelen vivir en la zona litoral relativamente estrecha que bordea la placa continental. Como consecuencia de los pigmentos que presentan, pueden captar la luz a profundidades mayores que otros tipos de algas. Como ejemplo extremo se han encontrado rodofitas viviendo en las laderas de una montaña submarina a la profundidad de 268 m, donde solo es capaz de penetrar el 0,001% de la luz de la superficie. Se las encuentra en todas las latitudes, preferentemente en aguas tropicales y templadas, donde constituyen las algas más abundantes. En las aguas polares y subpolares hay pocas especies y dominan las algas verdes y pardas.

Algunas de las especies de algas rojas son unicelulares, pero la mayoría son pluricelulares creciendo en forma de filamentos o láminas membranosas, que suelen tener algún tipo de consolidación, alcanzado el grado de organización seudoparenquimatoso. Pueden existir como cilindros muy finos, ramificados en forma de arbolillo o como láminas enteras o divididas. El talo normalmente se construye mediante la agregación de numerosos filamentos, dando lugar a estructuras cilíndricas o laminares de hasta 1 m de longitud, pero que nunca alcanzan la complejidad de las algas pardas. Su tamaño es también menor al de las algas pardas más grandes. El crecimiento normalmente se produce mediante la división de la célula apical, que puede ser multinucleada. 

Adicionalmente como estructuras especializadas presentan rizoides que van desde filamentos simples hasta discos formados por la agrupación convergente de los filamentos en la base del alga. Los zarcillos son transformaciones de las ramas terminales cuya función es la adhesión a otras ramas de la misma alga, a algas mayores o a otros elementos. Como estructuras reproductoras se presentan esporangios (que forman esporas) y gametangios (que forman los gametos). Existen también células vesiculares o secretoras de sustancias que presentan una gran diversidad en su composición química.

En la mayoría de las algas rojas existen poros septales o de conexión (plasmodesmos) entre las células. Puesto que el crecimiento apical es la norma en las algas rojas, la mayoría de las células tienen dos poros primarios, uno a cada célula adyacente. Los poros que no se derivan de una división celular se denominan secundarias y se producen cuando se fusionan células adyacentes. Después de la formación del poro, la conexión citoplasmática es bloqueada mediante la formación de un tapón de proteínas que puede funcionar tanto como refuerzo estructural como de vía de comunicación de célula a célula. En el refuerzo estructural también tienen importancia los mucílagos que se acumulan en las paredes celulares, compactando tanto los filamentos axiales con los contiguos.

Las rodofitas son fotosintéticas y sus cloroplastos (llamados rodoplastos) presentan algunas características específicas de su grupo. Están rodeados por dos membranas y se supone procedentes de la endosimbiosis primaria de una cianobacteria, al igual que los de las plantas verdes y las glaucofitas. Sin embargo, presentan características diferenciadas, por lo que las rodofitas se clasifican como grupo aparte. 

Los rodoplastos contienen clorofila "a", además de los pigmentos accesorios ficobiliproteínas (ficoeritrina y ficocianina) y carotenoides. Las rodofitas son los únicos protistas fotosintéticos que presentan ficoeritrina. Antiguamente se creía que algunas especies contenían también clorofila "d", pero recientemente se ha descubierto que esta clorofila procede de una cianobacteria, "Acaryochloris marina", que vive epifita sobre estas algas. Los tilacoides son solitarios, sin apilar y algunas veces hay uno o dos tilacoides periféricos. Se caracterizan por la presencia de ficobilisomas que contienen ficobiliproteínas en la superficie de los tilacoides y ausencia de almidón en los cloroplastos, usando como material de reserva almidón de florídeas (de estructura semejante a la amilopectina) extraplastidial, en gránulos en el citoplasma, próximos al cloroplasto. Pueden presentar o no pirenoides, usualmente en los grupos basales en el primer caso. Los pigmentos les confieren un color rojo o rojizo, generalmente. 

Las rodofitas son organismos siempre inmóviles, pues carecen de células flageladas en todas las etapas de su ciclo vital. Sus células también carecen de centriolos, centrosomas y de cualquier otra estructura que implique una organización 9+2 de microtúbulos. Las células pueden ser multinucleadas. Las algas rojas tienen paredes celulares dobles. Las paredes externas contienen los polisacáridos agarosa y agaropectina que se puede extraer de las paredes celulares por ebullición para obtener agar. Las paredes internas, adheridas al citoplasma, son en su mayoría de celulosa.

Aunque en algunas especies puede faltar la sexualidad, usualmente presentan alternancia de generaciones, que pueden ser dos o tres. Son organismos en los que predomina la fase haploide, a la que se añaden una o dos fases diploides durante su ciclo vital. La reproducción sexual es por oogamia, con células especializadas, carpogonios y espermacios. Las células que hacen la función de espermatozoides carecen de flagelos y no puede nadar, por lo que son llevadas por las corrientes de agua.

El ciclo vital puede ser de dos tipos:



En la siguiente galería se muestran las tres generaciones de "Polysiphonia".

Los diversos eucariotas que componen las algas rojas han sido foco de numerosas investigaciones recientes y queda una rica fuente de especies pequeñas aún no descritas por la taxonomía tradicional. Los estudios moleculares ubican a las algas en Archaeplastida (= Primoplantae, Plantae sensu lato); sin embargo, la clasificación supraordinal se ha limitado al debate del nivel clase vs. subclase para los dos subgrupos reconocidos, uno de los cuales es ampliamente reconocido como parafilético. Este limitado foco generalmente ha ocultado en gran medida la necesidad de modificación de la clasificación algal.

Abajo hay dos taxonomías de algas rojas publicadas que son válidas, aunque necesariamente ninguna de las dos tiene que ser usada, ya que la taxonomía algal está todavía en continuo desarrollo. Nótese también que hay un debate científico continuo de si las rodofitas deberían ser incluidas en el reino Protista o en el reino Plantae. Estos dos sistemas de clasificación, en los cuales las algas se ubican en el reino Plantae, son mostrados en la tabla.


</doc>
<doc id="2487" url="https://es.wikipedia.org/wiki?curid=2487" title="Ratoncito Pérez">
Ratoncito Pérez

El Ratoncito Pérez es un personaje (Ficticio) muy popular entre los niños españoles e hispanoamericanos. La tradición sigue el mismo ritual que en el caso del hada de los dientes de los países germanos: cuando a un niño se le cae un diente de leche, lo pondrá debajo de la almohada y, mientras duerme estos personajes mágicos, duendes, hadas o ratones se lo cambiarán por dulces o monedas.

Se le reconoce como «Ratoncito Pérez» en los países hispanohablantes, con la excepción de algunas regiones de México, Chile y Perú, donde se le llama «el Ratón de los dientes»; en cambio, en Colombia, Ecuador, Panamá y Venezuela, simplemente "El Ratón Pérez". Algunas versiones del Ratoncito Pérez le han añadido como nombre de pila: Odón.

En Francia se le llama «Ratoncito» ("la petite souris"), en Italia se le conoce como "Topolino", "Topino" («Ratoncito») o "Fatina" («Hadita») y en los países germanos, el «Hada de los dientes» ("Tooth Fairy"). En España se llama Ratoncito Perez y en otras partes de España, como en Cataluña es "l'Angelet" («el Angelito»), en el País Vasco – sobre todo Vizcaya –, es "Maritxu teilatukoa" (Mari la del tejado) o en Cantabria es "L’Esquilu de los dientis" (La Ardilla de los dientes). En algunos lugares es tradición tirar los dientes de los niños a los tejados de las casas.

El origen más probable del ratoncito y su enlace con un hada proviene de un cuento francés del siglo XVIII de la baronesa d'Aulnoy: "La Bonne Petite Souris" (El Buen Ratoncito). Habla de un hada que se transforma en un ratón para ayudar a derrotar a un malvado rey, ocultándose bajo la almohada del mismo, tras lo cual se le caen todos los dientes.

En España, su introducción a la mitología infantil se ha atribuido a Luis Coloma (autor también de "Pequeñeces" o "Jeromín"), cuando hacia 1894 pidieron al jesuita que escribiera un cuento para el futuro rey Alfonso XIII, que entonces tenía 8 años, y al que se le cayó un diente. Sin embargo, en la novela de Benito Pérez Galdós "La de Bringas", escrita en 1884 y ambientada en 1868, el autor compara a un personaje, Francisco Bringas, avaro y tacaño, con el "ratoncito Pérez", luego debía ser popular para el público ya antes del cuento del padre Coloma.

El Ayuntamiento de Madrid rescató la memoria del Ratoncito -el primer personaje ficticio al que el Ayuntamiento homenajea con una placa del Plan Memoria de Madrid- instalando una placa en la calle del Arenal, número 8, domicilio donde el Luis Coloma situó la vivienda del roedor, en la entonces popular confitería Prast. En la placa puede leerse: «Aquí vivía, dentro de una caja de galletas en la confitería Prast el Ratón Pérez, según el cuento que el padre Coloma escribió para el niño rey Alfonso XIII.»

Existe en Madrid un Museo del Ratón Pérez dedicado al personaje.

En 2006 la historia fue llevada al cine en una coproducción hispano-argentina, bajo la dirección de Juan Pablo Buscarini, bajo el título "Pérez, el ratoncito de tus sueños".

En enero de 2009, Disney estrenó en Hispanoamérica la película "El Ratón Pérez 2".

En 2012 el Ratoncito Pérez tuvo un breve cameo en la película "El origen de los guardianes" de DreamWorks Animation. Durante la recolección de los dientes, una minihada de los dientes encuentra al ratón llevándose un diente y se pelea con él, pero ella le explica que es parte de la división latina mientras el ratón demuestra su enfado gritando y arrojando su sombrero; sin embargo, el ratón de la película parece ser un ratón ayudante que trabaja para el Ratoncito Pérez, parecido a las que trabajan para el Hada de los Dientes.

En 2005 se estrenó en Buenos Aires "El Ratón Pérez, tu primer musical", de Cibrian Mahler (que se repondría en 2011). En 2007 se presenta un nuevo espectáculo teatral en el Teatro El Nacional "El Ratón Pérez y el cofre perdido", y en abril de 2010, en el Teatro Gran Rex, "El Ratón Pérez Superpoderoso".. En 2017 se prepara un nuevo musical

En algunos países asiáticos, como Corea, India, Japón y Vietnam, cuando un niño pierde un diente, es costumbre que lo lance al techo si viniera de la mandíbula inferior, o en el espacio debajo del piso si viniera de la mandíbula superior. Mientras se hace esto, el niño expresa un deseo de que el diente se sustituya por el diente de un ratón. Esta tradición se basa en el hecho de que los dientes de ratones crecen durante toda su vida, una característica de todos los roedores. En Japón, una variación indica que los dientes superiores se lancen directamente hacia abajo a la tierra y los dientes inferiores hacia arriba al aire, la idea es que los dientes entrantes crezcan derechos.

En países del Cercano Oriente (incluyendo Irak, Jordania, Palestina, Egipto y Sudán) existe una tradición de lanzar un diente de leche al cielo hacia el Sol o hacia Allah. Esta tradición puede tener su origen en una oferta pre-islámica y se remonta al menos al siglo XIII. También se menciona por Izz bin Hibat Allah Al Hadid en el siglo XIII.




</doc>
<doc id="2488" url="https://es.wikipedia.org/wiki?curid=2488" title="Rhizodiniales">
Rhizodiniales

Rhizodiniales, organismos unicelulares de la división "Dinophyta", clase "Dinophyceae", subclase "Dinophycidae", con dos flagelos heterocontos en el sulco y el cíngulo. Son formas ameboides.



</doc>
<doc id="2490" url="https://es.wikipedia.org/wiki?curid=2490" title="Red punto a punto">
Red punto a punto

Los tipos de redes (multipunto) según tecnología:

Las redes punto a punto son aquellas que responden a un tipo de arquitectura de red en las que cada canal de datos se usa para comunicar únicamente dos nodos, en clara oposición a las redes multipunto, en las cuales cada canal de datos se puede usar para comunicarse con diversos nodos.

En una red punto a punto, los dispositivos en red actúan como socios iguales, o pares entre sí. Como pares, cada dispositivo puede tomar el rol de emisor o la función de receptor. En un momento, el dispositivo A, por ejemplo, puede hacer una petición de un mensaje / dato del dispositivo B, y este es el que le responde enviando el mensaje / dato al dispositivo A. El dispositivo A funciona como receptor, mientras que B funciona como emisor. Un momento después los dispositivos A y B pueden revertir los roles: B, como receptor, hace una solicitud a A, y A, como emisor, responde a la solicitud de B. A y B permanecen en una relación recíproca o par entre ellos.








</doc>
<doc id="2491" url="https://es.wikipedia.org/wiki?curid=2491" title="RFC">
RFC

El término RFC puede referirse:


</doc>
<doc id="2494" url="https://es.wikipedia.org/wiki?curid=2494" title="Radio">
Radio

Radio hace referencia a varios artículos:










</doc>
<doc id="2502" url="https://es.wikipedia.org/wiki?curid=2502" title="Robótica">
Robótica

La robótica es la rama de la Ingeniería mecatrónica, de la Ingeniería eléctrica, de la Ingeniería electrónica, de la Ingeniería mecánica, de la Ingeniería biomédica y de las ciencias de la computación que se ocupa del diseño, construcción, operación, disposición estructural, manufactura y aplicación de los robots. 

La robótica combina diversas disciplinas como son: la mecánica, la electrónica, la informática, la inteligencia artificial, la ingeniería de control y la física. Otras áreas importantes en robótica son el álgebra, los autómatas programables, la animatrónica y las máquinas de estados.

El término robot se popularizó con el éxito de la obra "R.U.R. (Robots Universales Rossum)", escrita por Karel Čapek en 1920. En la traducción al inglés de dicha obra la palabra checa "robota," que significa "trabajos forzados o trabajador," fue traducida al inglés como "robot".

La robótica va unida a la construcción de "artefactos" que trataban de materializar el deseo humano de crear seres a su semejanza y que al mismo tiempo lo descargasen de trabajos tediosos. El ingeniero español Leonardo Torres Quevedo (que construyó el primer mando a distancia para su automóvil mediante telegrafía, el ajedrecista automático, el primer transbordador aéreo y otros muchos ingenios), acuñó el término "automática" en relación con la teoría de la automatización de tareas tradicionalmente asociadas. 

Karel Čapek, un escritor checo, acuñó en 1923 el término "robot" en su obra dramática "Rossum's Universal Robots / R.U.R.", a partir de la palabra checa robota, que significa servidumbre o trabajo forzado. El término robótica es acuñado por Isaac Asimov, definiendo a la ciencia que estudia a los robots. Asimov creó también las tres leyes de la robótica. En la ciencia ficción el hombre ha imaginado a los robots visitando nuevos mundos, haciéndose con el poder o, simplemente, aliviando de las labores caseras.

La que a continuación se presenta es la clasificación más común:

Robots manipuladores. Son sistemas mecánicos multifuncionales con un sencillo sistema de control, bien manual, de secuencia fija o de secuencia variable.

Robots de aprendizaje. Repiten una secuencia de movimientos que ha sido ejecutada previamente por un operador humano. El modo de hacerlo es a través de un dispositivo mecánico. El operador realiza los movimientos requeridos mientras el robot le sigue y los memoriza.

Robots con control sensorizado. El controlador es una computadora que ejecuta las órdenes de un programa y las envía al manipulador para que realice los movimientos necesarios.

Robots inteligentes. Son similares a los anteriores, pero además poseen sensores que envían información a la computadora de control sobre el estado del proceso. Esto permite una toma inteligente de decisiones y el control del proceso en tiempo real.

La estructura es definida por el tipo de configuración general del robot, puede ser metamórfica. El concepto de metamorfismo, de reciente aparición, se ha introducido para incrementar la flexibilidad funcional de un robot a través del cambio de su configuración por el propio robot. El metamorfismo admite diversos niveles, desde los más elementales (cambio de herramienta o de efecto terminal), hasta los más complejos como el cambio o alteración de algunos de sus elementos o subsistemas estructurales. Los dispositivos y mecanismos que pueden agruparse bajo la denominación genérica del robot, tal como se ha indicado, son muy diversos y es por tanto difícil establecer una clasificación coherente de los mismos que resista un análisis crítico y riguroso. La subdivisión de los robots, con base en su arquitectura, se hace en los siguientes grupos: poliarticulados, móviles, androides, zoomórficos e híbridos. 

En este grupo se encuentran los robots de muy diversa forma y configuración, cuya característica común es la de ser básicamente sedentarios (aunque excepcionalmente pueden ser guiados para efectuar desplazamientos limitados) y estar estructurados para mover sus elementos terminales en un determinado espacio de trabajo según uno o más sistemas de coordenadas, y con un número limitado de grados de libertad. En este grupo se encuentran los robots manipuladores, los robots industriales y los robots cartesianos, que se emplean cuando es preciso abarcar una zona de trabajo relativamente amplia o alargada, actuar sobre objetos con un plano de simetría vertical o reducir el espacio ocupado en el suelo. 

Son Robots con gran capacidad de desplazamiento, basados en carros o plataformas y dotados de un sistema locomotor de tipo rodante. Siguen su camino por telemando o guiándose por la información recibida de su entorno a través de sus sensores. Estos robots aseguran el transporte de piezas de un punto a otro de una cadena de fabricación. Guiados mediante pistas materializadas a través de la radiación electromagnética de circuitos empotrados en el suelo, o a través de bandas detectadas fotoeléctricamente, pueden incluso llegar a sortear obstáculos y están dotados de un nivel relativamente elevado de inteligencia. 


Son los tipos de robots que intentan reproducir total o parcialmente la forma y el comportamiento cinemático del ser humano. Actualmente, los androides son todavía dispositivos muy poco evolucionados y sin utilidad práctica, y destinados, fundamentalmente, al estudio y experimentación. 
Uno de los aspectos más complejos de estos robots, y sobre el que se centra la mayoría de los trabajos, es el de la locomoción bípeda. En este caso, el principal problema es controlar dinámica y coordinadamente en el tiempo real el proceso y mantener simultáneamente el equilibrio del Robot. Vulgarmente se los suele llamar "marionetas" cuando se les ven los cables que permiten ver cómo realiza sus procesos. 

Los robots zoomórficos, que considerados en sentido no restrictivo podrían incluir también a los androides, constituyen una clase caracterizada principalmente por sus sistemas de locomoción que imitan a los diversos seres vivos. 
A pesar de la disparidad morfológica de sus posibles sistemas de locomoción es conveniente agrupar a los Robots zoomórficos en dos categorías principales: caminadores y no caminadores. El grupo de los robots zoomórficos no caminadores está muy poco evolucionado. Los experimentos efectuados en Japón basados en segmentos cilíndricos biselados acoplados axialmente entre sí y dotados de un movimiento relativo de rotación. Los Robots zoomórficos caminadores multípedos son muy numerosos y están siendo objeto de experimentos en diversos laboratorios con vistas al desarrollo posterior de verdaderos vehículos terrenos, pilotados o autónomos, capaces de evolucionar en superficies muy accidentadas. Las aplicaciones de estos robots serán interesantes en el campo de la exploración espacial y en el estudio de los volcanes. 

Estos robots corresponden a aquellos de difícil clasificación, cuya estructura se sitúa en combinación con alguna de las anteriores ya expuestas, bien sea por conjunción o por yuxtaposición. Por ejemplo, un dispositivo segmentado articulado y con ruedas es, al mismo tiempo, uno de los atributos de los robots móviles y de los robots zoomórficos.



</doc>
<doc id="2505" url="https://es.wikipedia.org/wiki?curid=2505" title="Robot">
Robot

Un robot es una entidad virtual o mecánica artificial. En la práctica, esto es por lo general un sistema electromecánico que normalmente es conducido por un programa de una computadora o por un circuito eléctrico. Este sistema electromecánico, por su apariencia o sus movimientos, ofrece la sensación de tener un propósito propio.
La independencia creada en sus movimientos hace que sus acciones sean la razón de un estudio razonable y profundo en el área de la ciencia y tecnología.
La limpieza y el mantenimiento del hogar son cada vez más comunes en los hogares. No obstante, existe una cierta incertidumbre sobre el impacto económico de la programación y la amenaza del equipamiento robótico, una ansiedad que se ve reflejada en el retrato a menudo perverso y malvado de robots presentes en obras de la cultura popular. Comparados con sus colegas de ficción, los robots reales siguen siendo limitados.

El gran público conoció la palabra robot a través de la obra "R.U.R. (Robots Universales Rossum)" del dramaturgo checo Karel Čapek, que se estrenó en 1920. La palabra se escribía como "robotnik".

Sin embargo, no fue este autor Čapek quien inventó la palabra. En una breve carta escrita a la editorial del Diccionario Oxford, atribuye a su hermano Josef la creación del término. En un artículo publicado en la revista checa "Lidové noviny" en 1933, explicó que originalmente los quiso llamar "laboři" (del latín "labor", trabajo). Sin embargo, no le gustaba la palabra y pidió consejo a su hermano Josef, que le sugirió "roboti". La palabra "robota" significa literalmente trabajo o labor y figuradamente "trabajo duro" en checo y muchas lenguas eslavas. Tradicionalmente robota era el periodo de trabajo que un siervo debía otorgar a su señor, generalmente 6 meses del año. La servidumbre se prohibió en 1848 en Bohemia, por lo que cuando Čapek escribió "R.U.R.", el uso del término "robota" ya se había extendido a varios tipos de trabajo, pero el significado obsoleto de "servidumbre" seguiría reconociéndose.

La palabra robótica, usada para describir este campo de estudio, fue acuñada por el escritor de ciencia ficción Isaac Asimov.
La robótica concentra 3 áreas de estudio: la mecatrónica, la física y las matemáticas como ciencias básicas.

En el siglo IV antes de Cristo, el matemático griego Arquitas de Tarento construyó un ave mecánica que funcionaba con vapor y a la que llamó «La paloma». También el ingeniero Herón de Alejandría (10-70 d. C.) creó numerosos dispositivos automáticos que los usuarios podían modificar, y describió máquinas accionadas por presión de aire, vapor y agua. Por su parte, el estudioso chino Su Sung levantó una torre de reloj en 1088 con figuras mecánicas que daban las campanadas de las horas.

Al Jazarí (1136–1206), un inventor musulmán de la dinastía Artuqid, diseñó y construyó una serie de máquinas automatizadas, entre las que había útiles de cocina, autómatas musicales que funcionaban con agua, y en 1206 los primeros robots humanoides programables. Las máquinas tenían el aspecto de cuatro músicos a bordo de un bote en un lago, entreteniendo a los invitados en las fiestas reales. Su mecanismo contenía un tambor programable con clavijas que chocaban con pequeñas palancas que accionaban instrumentos de percusión. Podían cambiarse los ritmos y patrones que tocaba el tamborilero moviendo las clavijas.

El artesano japonés Hisashige Tanaka (1799–1881), conocido como el «Edison japonés», creó una serie de juguetes mecánicos extremadamente complejos, algunos de los cuales servían té, disparaban flechas sacadas de un carcaj e incluso trazaban un "kanji" (caracteres utilizados en la escritura japonesa).

Por otra parte, desde la generalización del uso de la tecnología en procesos de producción con la Revolución Industrial se intentó la construcción de dispositivos automáticos que ayudasen o sustituyesen al hombre. Entre ellos destacaron los Jaquemarts, muñecos de dos o más posiciones que golpean campanas accionados por mecanismos de relojería china y japonesa.

Robots equipados con una sola rueda fueron utilizados para llevar a cabo investigaciones sobre conducta, navegación y planeo de ruta. Cuando estuvieron listos para intentar nuevamente con los robots caminantes, comenzaron con pequeños hexápodos y otros tipos de robots de múltiples patas. Estos robots imitaban insectos y artrópodos en funciones y forma. Como se ha mencionado anteriormente, la tendencia se dirige hacia ese tipo de cuerpos que ofrecen gran flexibilidad y han demostrado ser adaptables a cualquier ambiente. Con más de 4 piernas, estos robots son estáticamente estables, lo que hace que el trabajar con ellos sea más sencillo. Recientemente se han hecho progresos hacia los robots con locomoción bípeda.

En el sentido común de un autómata, el mayor robot en el mundo tendría que ser el "Maeslantkering", una barrera para tormentas del Plan Delta en los Países Bajos construida en los años 1990, la cual se cierra automáticamente cuando es necesario. Sin embargo, esta estructura no satisface los requerimientos de movilidad o generalidad.

En 2002 Honda y Sony comenzaron a vender comercialmente robots humanoides como «mascotas». Los robots con forma de perro o de serpiente se encuentran, sin embargo, en una fase de producción muy amplia; el ejemplo más notorio ha sido Aibo de Sony.

En la actualidad, los robots comerciales e industriales se utilizan ampliamente y realizan tareas de forma más exacta o más barata que los humanos. También se emplean en trabajos demasiado sucios, peligrosos o tediosos para los humanos. Los robots se usan en plantas de manufactura, montaje y embalaje, en transporte, en exploraciones en la Tierra y en el espacio, cirugía, armamento, investigación en laboratorios y en la producción en masa de bienes industriales o de consumo.

Otras aplicaciones incluyen la limpieza de residuos tóxicos, minería, búsqueda y rescate de personas y localización de minas terrestres.
Existe una gran esperanza, especialmente en Japón, de que el cuidado del hogar para la población de edad avanzada pueda ser desempeñado por robots.

Los robots parecen estar abaratándose y reduciendo su tamaño, una tendencia relacionada con la miniaturización de los componentes electrónicos que se utilizan para manejarlos. Además, muchos robots son diseñados en simuladores mucho antes de construirse y de que interactúen con ambientes físicos reales. Un buen ejemplo de esto es el equipo "Spiritual Machine", un equipo de 5 robots desarrollado totalmente en un ambiente virtual para jugar al fútbol en la liga mundial de la "F.I.R.A."

Además de los campos mencionados, hay modelos trabajando en el sector educativo, servicios (por ejemplo, en lugar de recepcionistas humanos o vigilancia) y tareas de búsqueda y rescate.

Recientemente se ha logrado un gran avance en los robots dedicados a la medicina, con dos compañías en particular, "Computer Motion" e "Intuitive Surgical", que han recibido la aprobación regulatoria en América del Norte, Europa y Asia para que sus robots sean utilizados en procedimientos de cirugía invasiva mínima. Desde la compra de Computer Motion (creador del robot Zeus) por Intuitive Surgical, se han desarrollado ya tres modelos de robot Da Vinci por esta última. En la actualidad, existen más de 2.300 robots quirúrgicos Da Vinci en el mundo, con aplicaciones en Urología, Ginecología, Cirugía general, Cirugía Pediátrica, Cirugía Torácica, Cirugía Cardíaca y ORL.
También la automatización de laboratorios es un área en crecimiento. Aquí, los robots son utilizados para transportar muestras biológicas o químicas entre instrumentos tales como incubadoras, manejadores de líquidos y lectores. Otros lugares donde los robots están reemplazando a los humanos son la exploración del fondo oceánico y exploración espacial. Para esas tareas se suele recurrir a robots de tipo artrópodo.

En fases iniciales de desarrollo hay robots alados experimentales y otros ejemplos que explotan el biomimetismo. Se espera que los así llamados nanomotores y cables inteligentes simplifiquen drásticamente el poder de locomoción, mientras que la estabilización en vuelo parece haber sido mejorada substancialmente por giroscopios extremadamente pequeños.

Un impulsor muy significativo de este tipo de investigaciones es el desarrollo de equipos de espionaje militar.
A fin de proteger a aquellos que ponen su vida en peligro, los robots de seguridad y defensa aptos para el combate pueden realizar numerosas misiones para ayudar a los profesionales de la seguridad pública y del ejército.

Existen diferentes tipos y clases de robots, entre ellos con forma humana, de animales, de plantas o incluso de elementos arquitectónicos pero todos se diferencian por sus capacidades y se clasifican en 4 formas:

En esta última se puede clasificar según su morfología en: Robots angulares o antropomórficos, robots cilíndricos, robots esféricos o polares, robots tipo SCARA, robots paralelos, robots cartesianos, entre otros.

El robot de fabricación más común es el robot industrial y de entre los robots industriales, el más común es el brazo articulado también llamado brazo robótico. Un brazo robótico típico se compone de siete segmentos metálicos, unidos por seis articulaciones. Una computadora controla el robot girando motores de pasos individuales conectados a cada junta (los brazos más grandes utilizan la hidráulica o neumática). A diferencia de los motores eléctricos de movimiento continuo, los motores de pasos pueden moverse en incrementos exactos. Esto permite que el ordenador pueda mover el brazo de manera muy precisa, repitiendo exactamente el mismo movimiento una y otra vez. El robot utiliza sensores de movimiento para hacer que se mueva la cantidad justa.

Un robot industrial con seis articulaciones se asemeja mucho a un brazo humano - tiene el equivalente de un hombro, un codo y la muñeca. Típicamente, el hombro está montado en una estructura de base estacionaria en lugar de a un cuerpo móvil. Este tipo de robot tiene seis grados de libertad, lo que significa que puede pivotar en seis formas diferentes. Un brazo humano, en comparación, tiene siete grados de libertad.

El trabajo del brazo humano es mover la mano de un lugar a otro. Del mismo modo, el trabajo del brazo robótico es mover un efector final de un lugar a otro. Se puede equipar brazos robóticos con todo tipo de efectores de extremo, que están adaptados a una aplicación particular. Un efector final común es una versión simplificada de la mano, que puede captar y transportar objetos diferentes. Las manos robóticas a menudo han incorporado sensores de presión que le dicen a la computadora que tan fuerte el robot está sujetando un objeto en particular. Esto evita que el robot tire o rompa lo que lleva. Otros efectores finales incluyen sopletes, los soldadores por puntos, taladros y la pintura por
aire a presión entre otros.

Los robots industriales están diseñados para hacer exactamente lo mismo, en un ambiente controlado, una y otra vez. Por ejemplo, un robot podría cerrar las tapas de frascos de mantequilla que salen de una línea de montaje. Para enseñar a un robot cómo hacer su trabajo, el programador guía el brazo a través de los movimientos utilizando un controlador de mano (Teachpendant). El robot almacena la secuencia exacta de los movimientos en su memoria, y lo hace una y otra vez cada vez que una nueva unidad viene por la línea de montaje.

Existen diferentes técnicas para programar robots industriales. Entre ellas se encuentran las técnicas de programación gestual y las de programación textual. En la programación gestual un operario guía al robot, manualmente o mediante controles remotos, enseñándole la tarea que este debe realizar. El robot va almacenando los pasos a seguir y luego puede repetirlos de manera autónoma. En la programación textual, en cambio, se realizan primero los cálculos de las posiciones y trayectorias que el robot debe recorrer y, con esta información, se crean las instrucciones del programa que el robot deberá ejecutar. Una vez transferido el programa al robot, este puede comenzar a realizar la tarea de manera autónoma.

La mayoría de los robots industriales trabajan en cadenas de montaje de automóviles, poniendo los coches juntos. Los robots pueden hacer este trabajo más eficientemente que los seres humanos gracias a su precisión, que les permite por ejemplo perforar siempre en el mismo lugar o apretar siempre los tornillos con la misma cantidad de fuerza, sin importar las horas que trabaje (cosa que no sucede con los humanos). Los robots de fabricación son también muy importantes en la industria electrónica, ya que se necesita un control increíblemente preciso para armar un microchip.


Muchas mitologías antiguas tratan la idea de los humanos artificiales.
En la mitología clásica, se dice que Cadmo sembró dientes de dragón que se convertían en soldados, y Galatea, la estatua de Pigmalión, cobró vida. También el dios griego de los herreros, Hefesto (Vulcano para los romanos) creó sirvientes mecánicos inteligentes, otros hechos de oro e incluso mesas que se podían mover por sí mismas.
Algunos de estos autómatas ayudan al dios a forjar la armadura de Aquiles, según la Ilíada
Aunque, por supuesto, no se describe a esas máquinas como "robots" o como "androides", son en cualquier caso dispositivos mecánicos de apariencia humana.

Una leyenda hebrea habla del Golem, una estatua animada por la magia cabalística. Por su parte, las leyendas de los Inuit describen al Tupilaq (o Tupilak), que un mago puede crear para cazar y asesinar a un enemigo. Sin embargo, emplear un Tupilaq para este fin puede ser una espada de doble filo, ya que la víctima puede detener el ataque del Tupilaq y reprogramarlo con magia para que busque y destruya a su creador.

Ya en 1817, en un cuento de Hoffmann llamado "El hombre de arena", aparece una mujer que parecía una muñeca mecánica, y en la obra de Edward S. Ellis de 1865 "El Hombre de Vapor de las Praderas" se expresa la fascinación americana por la industrialización.

Como se indicaba más arriba, la primera obra en utilizar la palabra "robot" fue la obra teatral R.U.R. de Čapek,(escrita en colaboración con su hermano Josef en 1920; representada por primera vez en 1921; escenificada en Nueva York en 1922. La edición en inglés se publicó en 1923).

La obra comienza en una fábrica que construye personas artificiales llamadas robots, pero están más cerca del concepto moderno de androide o clon, en el sentido de que se trata de criaturas que pueden confundirse con humanos. Pueden pensar por sí mismos, aunque parecen felices de servir. En cuestión está si los robos están siendo explotados, así como las consecuencias por su tratamiento.

El autor más prolífico de historias sobre robots fue Isaac Asimov (1920-1992), que colocó los robots y su interacción con la sociedad en el centro de muchos de sus libros. Este autor consideró seriamente la serie ideal de instrucciones que debería darse a los robots para reducir el peligro que estos representaban para los humanos. Así llegó a formular sus Tres Leyes de la Robótica: Ningún robot causará daño a un ser humano o permitirá, con su inacción, que un ser humano sufra daño; todo robot obedecerá las órdenes que le den los seres humanos, a menos que esas órdenes entren en conflicto con la primera ley; y todo robot debe proteger su propia existencia, siempre que esa protección no entre en conflicto con la primera o la segunda ley.

Esas tres leyes se introdujeron por primera vez en su relato corto de 1942 "Círculo Vicioso", aunque habían sido esbozadas en algunos textos anteriores. Más tarde, Asimov añadió la ley de Cero: "Ningún robot causará daño a la humanidad ni permitirá, con su inacción que la humanidad sufra daño". El resto de las leyes se modificaron para ajustarse a este añadido.

Según el Oxford English Dictionary, el principio del relato breve "¡Mentiroso!" de 1941 contiene el primer uso registrado de la palabra robótica. El autor no fue consciente de esto en un principio, y asumió que la palabra ya existía por su analogía con mecánica, hidráulica y otros términos similares que se refieren a ramas aplicadas del conocimiento.

El tono económico y filosófico iniciado por R.U.R. sería desarrollado más tarde por la película "Metrópolis", y las populares "Blade Runner" (1982) o "The Terminator" (1984).

Existen muchas películas sobre robots, entre las cuales cabe destacar:


En televisión, existen series muy populares como "Robot Wars" y "BattleBots". En la serie "Futurama" y Doraemon, de Matt Groening y Fujiko F. Fujio, los robots poseen una identidad propia, como ciudadanos. También, en la serie "Almost Human" aparecen robots-policías con conciencia propia, llamados DRN, los cuales funcionan con un programa de “alma sintética”.

Existe la preocupación de que los robots puedan desplazar o competir con los humanos. Las leyes o reglas que pudieran o debieran ser aplicadas a los robots u otros “entes autónomos” en cooperación o competencia con humanos si algún día se logra alcanzar la tecnología suficiente como para hacerlos inteligentes y conscientes de sí mismos, han estimulado las investigaciones macroeconómicas de este tipo de competencia, notablemente construido por Alessandro Acquisti basándose en un trabajo anterior de John von Neumann.

Actualmente, no es posible aplicar las Tres leyes de la robótica, dado que los robots no tienen capacidad para comprender su significado, evaluar las situaciones de riesgo tanto para los humanos como para ellos mismos o resolver los conflictos que se podrían dar entre estas leyes.

Entender y aplicar lo anteriormente expuesto requeriría verdadera inteligencia y consciencia del medio circundante, así como de sí mismo, por parte del robot, algo que a pesar de los grandes avances tecnológicos de la era moderna no se ha alcanzado.

Muchas grandes empresas, como Intel, Sony, General Motors, Dell, han implementado en sus líneas de producción unidades robóticas para desempeñar tareas que por lo general hubiesen desempeñado trabajadores de carne y hueso en épocas anteriores.

Esto ha causado una agilización en los procesos realizados, así como un mayor ahorro de recursos, al disponer de máquinas que pueden desempeñar las funciones de cierta cantidad de empleados a un costo relativamente menor y con un grado mayor de eficiencia, mejorando notablemente el rendimiento general y las ganancias de la empresa, así como la calidad de los productos ofrecidos.

Pero, por otro lado, ha suscitado y mantenido inquietudes entre diversos grupos por su impacto en la tasa de empleos disponibles, así como su repercusión directa en las personas desplazadas. Dicha controversia ha abarcado el aspecto de la seguridad, llamando la atención de casos como el ocurrido en Jackson, Míchigan, el 21 de julio de 1984 donde un robot aplastó a un trabajador contra una barra de protección en la que aparentemente fue la primera muerte relacionada con un robot en los EE. UU.

Debido a esto se ha llamado la atención sobre la ética en el diseño y construcción de los robots, así como la necesidad de contar con lineamientos claros de seguridad que garanticen una correcta interacción entre humanos y máquinas.

El empleo de robots para labores de manufactura pudiera aún abaratar costos, ya que a diferencia de un operario humano no acarrearía pago de sueldos/salarios ni reivindicaciones laborales. No obstante, por tratarse de una máquina requeriría de servicio técnico (mantenimiento y reparación), lo cual conlleva un gasto monetario.

El relator especial de la ONU sobre ejecuciones extrajudiciales, sumarias o arbitrarias, Christof Heyns, está tratando de detener la creación y el esparcimiento de los robots autónomos letales (LAR), conocidos también como robots asesinos, hacia otros países de manera general. Heyns realizó un informe en el que se menciona de manera muy significativa la necesidad de realizar una legislación o protocolo mundial que describa el compromiso serio y significativo de poner un límite al desarrollo de esta tecnología que, probablemente en un futuro no muy lejano, a los robots se les confiera el poder y el permiso para matar a los seres humanos.

Asimismo, mencionó que, mientras los drones sigan teniendo a un ser humano como su controlador que les dirija a quién matar y a quien no, también sería muy probable que los robots asesinos, debido a su programación centrada en el ataque contra los seres humanos y a la destrucción de los mismos, puede ser un grave peligro. En su informe plantea su idea sobre las preocupaciones de las posibles consecuencias del uso de los LAR, y es que, según Heyns, los robots podrían desequilibrar la balanza entre la guerra y la paz, y no solo eso: tienen una estructura que les permite tener un largo alcance cuando son utilizados.

El relator propuso que el desarrollo de esta tecnología no dejaría nada bueno para nadie, quizá para las grandes potencias que tienen planeado una guerra o el aprovechamiento de algunos recursos. Pero la cuestión aquí es la programación que pueden tener estos robots, porque ¿será posible que un robot pueda distinguir entre los combatientes y los civiles?

Para finalizar su informe menciona que, a pesar de que su implementación, sería inaceptable. De igual modo, sería muy importante que se elaboren una serie de reglas que permitan un manejo en el desarrollo de esta tecnología que no beneficiaría para nada a la sociedad, y en cambio estaría representando un grave problema para todo el mundo, porque si los robots pudieran tomar por sí mismos algunas decisiones, estaríamos en un gran riesgo que no se debe permitir o por lo menos lograr controlar el manejo de estos robots.

"Así que tal vez es necesario formalizar de una vez por todas en las restricciones del uso y la construcción de esta tecnología, así como tener la capacidad moral y ética de realizar robots en beneficio de la sociedad, que contribuyan en todo momento con los seres humanos. Esta tecnología abrirá camino a un futuro prometedor donde resplandezca paz y una buena interacción entre robots y humanos. Sólo pido a las grandes potencias que no tengan en mente destruir este hermoso planeta que nos ha dotado de mucha vida y felicidad a todos nosotros y sobre todo a las futuras generaciones."



</doc>
<doc id="2506" url="https://es.wikipedia.org/wiki?curid=2506" title="Rottboellia">
Rottboellia

Rottboellia es un género de plantas herbáceas perteneciente a la familia de las poáceas. Es originario de las regiones tropicales y subtropicales de Asia y África.

El nombre del género fue otorgado en honor de Christen Friis Rottbøll.



</doc>
<doc id="2507" url="https://es.wikipedia.org/wiki?curid=2507" title="Richardsiella eruciformis">
Richardsiella eruciformis

Richardsiella es un género monotípico de plantas herbáceas, perteneciente a la familia de las poáceas. Su única especie: Richardsiella eruciformis Elffers & Kenn.-O'Byrne, es originaria de Zambia.

El nombre del género fue otorgado en honor de H.M.Richards, recolector de plantas.



</doc>
<doc id="2509" url="https://es.wikipedia.org/wiki?curid=2509" title="Rechazo múltiple">
Rechazo múltiple

Rechazo múltiple es un tipo de respuesta ARQ en la cual no se dejan de enviar paquetes hasta que se recibe un NACK. En ese momento se interrumpe la transmisión y se empieza la transmisión continua a partir del paquete que tenía errores desperdiciando así toda la información transmitida entre el primer envío y la detección del error.

Este tipo de ARQ exige una memoria en el transmisor que sea capaz de almacenar tantos datos como los que puedan enviarse en un timeout, ya que será el tiempo máximo de espera y esos datos deben reenviarse tras detectar un error.

Otra de las exigencias de este tipo de ARQ es la numeración de los ACK's para poder distinguir a qué paquete de información están asintiendo.


</doc>
<doc id="2511" url="https://es.wikipedia.org/wiki?curid=2511" title="Reduced instruction set computing">
Reduced instruction set computing

En arquitectura computacional, RISC (del inglés "Reduced Instruction Set Computer", en español "Computador con Conjunto de Instrucciones Reducidas") es un tipo de diseño de CPU generalmente utilizado en microprocesadores o microcontroladores con las siguientes características fundamentales:


Además estos procesadores suelen disponer de muchos registros de propósito general.

El objetivo de diseñar máquinas con esta arquitectura es posibilitar la segmentación y el paralelismo en la ejecución de instrucciones y reducir los accesos a memoria.
Las máquinas RISC protagonizan la tendencia actual de construcción de microprocesadores. PowerPC, DEC Alpha, MIPS, ARM, SPARC son ejemplos de algunos de ellos.

RISC es una filosofía de diseño de CPU para computadora que está a favor de conjuntos de instrucciones pequeñas y simples que toman menor tiempo para ejecutarse. El tipo de procesador más comúnmente utilizado en equipos de escritorio, el x86, está basado en CISC en lugar de RISC, aunque las versiones más nuevas traducen instrucciones basadas en CISC x86 a instrucciones más simples basadas en RISC para uso interno antes de su ejecución.

La idea fue inspirada por el hecho de que muchas de las características que eran incluidas en los diseños tradicionales de CPU para aumentar la velocidad estaban siendo ignoradas por los programas que eran ejecutados en ellas. Además, la velocidad del procesador en relación con la memoria de la computadora que accedía era cada vez más alta. Esto conllevó la aparición de numerosas técnicas para reducir el procesamiento dentro del CPU, así como de reducir el número total de accesos a memoria.

Terminología más moderna se refiere a esos diseños como arquitecturas de carga-almacenamiento.

Uno de los principios básicos de diseño para todos los procesadores es añadir velocidad al proveerles alguna memoria muy rápida para almacenar información temporalmente, estas memorias son conocidas como registros. Por ejemplo, cada CPU incluye una orden para sumar dos números. La operación básica de un CPU sería cargar esos dos números en los registros, sumarlos y almacenar el resultado en otro registro, finalmente, tomar el resultado del último registro y devolverlo a la memoria principal.

Sin embargo, los registros tienen el inconveniente de ser algo complejos para implementar. Cada uno está representado por transistores en el chip, en este aspecto la memoria principal tiende a ser mucho más simple y económica. Además, los registros le añaden complejidad al cableado, porque la unidad central de procesamiento necesita estar conectada a todos y cada uno de los registros para poder utilizarlos por igual.

Como resultado de esto, muchos diseños de CPU limitan el uso de registros de alguna u otra manera. Algunos incluyen pocos registros, aunque esto limita su velocidad. Otros dedican sus registros a tareas específicas para reducir la complejidad; por ejemplo, un registro podría ser capaz de hacer operaciones con uno o más de los otros registros, mientras que el resultado podría estar almacenado en cualquiera de ellos.

En el mundo de la microcomputación de los años setenta, éste era un aspecto más de las CPU, ya que los procesadores eran entonces demasiado lentos –de hecho había una tendencia a que el procesador fuera más lento que la memoria con la que se comunicaba-. En esos casos tenía sentido eliminar casi todos los registros, y entonces proveer al programador de una buena cantidad de maneras de tratar con la memoria para facilitar su trabajo.

Dado el ejemplo de la suma, la mayoría de los diseños de CPU se enfocaron a crear una orden que pudiera hacer todo el trabajo automáticamente: llamar los dos números que serían sumados, sumarlos, y luego almacenarlos fuera directamente. Otra versión podría leer los dos números de la memoria, pero almacenaría el resultado en un registro. Otra versión podría leer uno de la memoria y otro desde un registro y almacenarlo en la memoria nuevamente. Y así sucesivamente.

La meta en general en aquel tiempo era proveer cada posible modo de direccionamiento para cada instrucción, un principio conocido como ortogonalidad. Esto llevó a un CPU complejo, pero en teoría capaz de configurar cada posible orden individualmente, haciendo el diseño más rápido en lugar de que el programador utilizara órdenes simples.

La última representación de este tipo de diseño puede ser vista en dos equipos, el MOS 6502 por un lado, y el VAX en el otro. El chip 6502 de $25 USD efectivamente tenía solamente un registro, y con la configuración cuidadosa de la interfaz de memoria fue capaz de sobrepasar diseños corriendo a velocidades mayores (como el Zilog Z80 a 4MHz). El VAX era un minicomputador que en una instalación inicial requería 3 gabinetes de equipo para un solo CPU, y era notable por la sorprendente variedad de estilos de acceso a memoria que soportaba, y el hecho de que cada uno de éstos estaba disponible para cada instrucción.

A finales de los setenta, investigaciones en IBM (y otros proyectos similares en otros lugares), demostraron que la mayoría de esos modos de direccionamiento "ortogonal" eran ignorados por la mayoría de los programas. Esto fue un efecto colateral en el incremento en el uso de compiladores para generar los programas, algo opuesto a escribirlos en lenguaje ensamblador. Los compiladores tendían a ser demasiado tontos en términos de las características que usaban, un efecto colateral del intento por hacerlos pequeños. El mercado se estaba moviendo hacia un uso más generalizado de los compiladores, diluyendo aún más la utilidad de los modelos ortogonales.

Otro descubrimiento fue que debido a que esas operaciones eran escasamente utilizadas, de hecho tendían a ser más lentas que un número pequeño de operaciones haciendo lo mismo. Esta paradoja fue un efecto colateral del tiempo que se utilizaba diseñando los CPU, los diseñadores simplemente no tenían tiempo de optimizar cada instrucción posible, y en vez de esto sólo optimizaban las más utilizadas. Un famoso ejemplo de esto era la instrucción codice_1, que se ejecutaba más lentamente que un bucle que implementara el mismo código.

Casi al mismo tiempo, las CPU comenzaron a correr a velocidades mayores que las de la memoria con la que se comunicaban. Aún a finales de los setenta, era aparente que esta disparidad continuaría incrementándose al menos durante la siguiente década, para entonces los CPU podrían ser cientos de veces más rápidos que la memoria. Esto significó que los avances para optimizar cualquier modo de direccionamiento serían completamente sobrepasados por las velocidades tan lentas en las que se llevaban a cabo.

Otra parte del diseño RISC llegó desde las medidas prácticas de los programas en el mundo real. Andrew Tanenbaum reunió muchos de éstos, demostrando así que la mayoría de los procesadores estaban sobredimensionados. Por ejemplo, él demostró que el 98 % de todas las constantes en un programa podían acomodarse en 13 bits, aun cuando cada diseño de CPU dedicaba algunos múltiplos de 8 bits para almacenarlos, típicamente 8, 16 o 32, una palabra entera. Tomando este hecho en cuenta sugiere que una máquina debería permitir que las constantes fuesen almacenadas en los bits sin utilizar de otras instrucciones, disminuyendo el número de accesos a memoria. En lugar de cargar números desde la memoria o los registros, éstos podrían estar "ahí mismo" para el momento en el que el CPU los necesitara, y por lo tanto el proceso sería mucho más rápido. Sin embargo, esto requería que la instrucción misma fuera muy pequeña, de otra manera no existiría suficiente espacio libre en los 32 bits para mantener constantes de un tamaño razonable.

Fue el pequeño número de modos y órdenes que dio lugar al término "conjunto reducido de instrucciones". Ésta no es una definición correcta, ya que los diseños RISC cuentan con una vasta cantidad de conjuntos de instrucciones para ellos. La verdadera diferencia es la filosofía para hacer todo en registros y llamar y guardar los datos hacia ellos y en ellos mismos. Ésta es la razón por la que la forma más correcta de denominar este diseño es "cargar-almacenar". Con el paso del tiempo las técnicas de diseño antiguas se dieron a conocer como "Computadora con Conjunto de Instrucciones Complejo", CISC por sus siglas en inglés, aunque esto fue solamente para darles un nombre diferente por razones de comparación.

Por esto la filosofía RISC fue crear instrucciones pequeñas, implicando que había pocas, de ahí el nombre "conjunto reducido de instrucciones". El código fue implementado como series de esas instrucciones simples, en vez de una sola instrucción compleja que diera el mismo resultado. Esto hizo posible tener más espacio dentro de la instrucción para transportar datos, resultando esto en la necesidad de menos registros en la memoria. Al mismo tiempo la interfaz con la memoria era considerablemente simple, permitiendo ser optimizada.

Sin embargo RISC también tenía sus desventajas. Debido a que una serie de instrucciones son necesarias para completar incluso las tareas más sencillas, el número total de instrucciones para la lectura de la memoria es más grande, y por lo tanto lleva más tiempo. Al mismo tiempo no estaba claro dónde habría o no una ganancia neta en el desempeño debido a esta limitación, y hubo una batalla casi continua en el mundo de la prensa y del diseño sobre los conceptos de RISC.

Debido a lo redundante de las microinstrucciones, los sistemas operativos diseñados para estos microprocesadores, contemplaban la capacidad de subdividir un microprocesador en varios, reduciendo el número de instrucciones redundantes por cada instancia del mismo. Con una arquitectura del software optimizada, los entornos visuales desarrollados para estas plataformas, contemplaban la posibilidad de ejecutar varias tareas en un mismo ciclo de reloj. Así mismo, la paginación de la memoria RAM era dinámica y se asignaba una cantidad suficiente a cada instancia, existiendo una especie de 'simbiosis' entre la potencia del microprocesador y la RAM dedicada a cada instancia del mismo.

La multitarea dentro de la arquitectura CISC nunca ha sido real, tal como en los RISC sí lo es. En CISC, el microprocesador en todo su conjunto está diseñado en tantas instrucciones complejas y diferentes, que la subdivisión no es posible, al menos a nivel lógico. Por lo tanto, la multitarea es aparente y por órdenes de prioridad. Cada ciclo de reloj trata de atender a una tarea instanciada en la RAM y pendiente de ser atendida. Con una cola de atención por tarea FIFO para los datos generados por el procesador, y LIFO para las interrupciones de usuario, trataban de dar prioridad a las tareas que el usuario desencadenara en el sistema. La apariencia de multitarea en un CISC tradicional, viene de la mano de los modelos escalares de datos, convirtiendo el flujo en un vector con distintas etapas y creando la tecnología pipeline.

Los microprocesadores actuales, al ser híbridos, permiten cierta parte de multitarea real. La capa final al usuario es como un CISC tradicional, mientras que las tareas que el usuario deja pendientes, dependiendo del tiempo de inactividad, el sistema traducirá las instrucciones (el software ha de ser compatible con esto) CISC a RISC, pasando la ejecución de la tarea a bajo nivel, en donde los recursos se procesan con la filosofía RISC. Dado que el usuario solo atiende una tarea por su capacidad de atención, el resto de tareas que deja pendientes y que no son compatibles con el modelo de traducción CISC/RISC, pasan a ser atendidas por el tradicional pipeline, o si son tareas de bajo nivel, tal como desfragmentaciones de disco, chequeo de la integridad de la información, formateos, tareas gráficas o tareas de cálculo matemático intenso.

En vez de tratar de subdividir a un solo microprocesador, se incorporó un segundo microprocesador gemelo, idéntico al primero. El inconveniente es que la RAM debía de ser tratada a nivel hardware y los módulos diseñados para plataformas monoprocesador no eran compatibles o con la misma eficiencia, que para las plataformas multiprocesador. Otro inconveniente, era la fragmentación del BYTE de palabra. En un RISC tradicional, se ocupan los BYTES de la siguiente forma: Si la palabra es de 32 BITS (4 BYTES de palabra de 8 BITS cada una, o dos de 16 o una de 32), dependiendo de la profundidad del dato portado, dentro del mismo BYTE, se incluían partes de otras instrucciones y datos. Ahora, al ser dos microprocesadores distintos, ambos usaban registros independientes, con accesos a la memoria propios (en estas plataformas, la relación de RAM por procesador es de 1/1). En sus orígenes, las soluciones se parecían a las típicas ñapas de albañil, cada placa base incorporaba una solución solamente homologada por la chip set usada y los drivers que la acompañaban. Si bien la fragmentación siempre ha sido como ese mosquito que zumba en el oído, pero que por pereza permitimos que nos pique, llegó un momento que era imposible evadir el zumbido. Esta época llegó con las plataformas de 64 BITS.

Mientras la filosofía de diseño RISC se estaba formando, nuevas ideas comenzaban a surgir con un único fin: incrementar drásticamente el rendimiento de la CPU.

Al principio de la década de los ochenta se pensaba que los diseños existentes estaban alcanzando sus límites teóricos. Las mejoras de la velocidad en el futuro serían hechas con base en "procesos" mejorados, esto es, pequeñas características en el chip. La complejidad del chip podría continuar como hasta entonces, pero un tamaño más pequeño podría resultar en un mejor rendimiento del mismo al operar a más altas velocidades de reloj. Se puso una gran cantidad de esfuerzo en diseñar chips para computación paralela, con vínculos de comunicación interconstruidos. En vez de hacer los chips más rápidos, una gran cantidad de chips serían utilizados, dividiendo la problemática entre estos. Sin embargo, la historia mostró que estos miedos no se convirtieron en realidad, y hubo un número de ideas que mejoraron drásticamente el rendimiento al final de la década de los ochenta.

Una idea era la de incluir un canal por el cual se pudieran dividir las instrucciones en pasos y trabajar en cada paso muchas instrucciones diferentes al mismo tiempo. Un procesador normal podría leer una instrucción, decodificarla, enviar a la memoria la instrucción de origen, realizar la operación y luego enviar los resultados. La clave de la canalización es que el procesador pueda comenzar a leer la siguiente instrucción tan pronto como termine la última instrucción, significando esto que ahora dos instrucciones se están trabajando (una está siendo leída, la otra está comenzando a ser decodificada), y en el siguiente ciclo habrá tres instrucciones. Mientras que una sola instrucción no se completaría más rápido, la "siguiente" instrucción sería completada enseguida. La ilusión era la de un sistema mucho más rápido. Esta técnica se conoce hoy en día como Segmentación de cauce.

Otra solución más era utilizar varios elementos de procesamiento dentro del procesador y ejecutarlos en paralelo. En vez de trabajar en una instrucción para sumar dos números, esos procesadores superescalares podrían ver la siguiente instrucción en el canal y tratar de ejecutarla al mismo tiempo en una unidad idéntica. Esto no era muy fácil de hacer, sin embargo, ya que algunas instrucciones dependían del resultado de otras instrucciones.

Ambas técnicas se basaban en incrementar la velocidad al añadir complejidad al diseño básico del CPU, todo lo opuesto a las instrucciones que se ejecutaban en el mismo. Siendo el espacio en el chip una cantidad finita, para poder incluir todas esas características algo más tendría que ser eliminado para hacer hueco. RISC se encargó de tomar ventaja de esas técnicas, esto debido a que su lógica para el CPU era considerablemente más simple que la de los diseños CISC. Aun con esto, los primeros diseños de RISC ofrecían una mejora de rendimiento muy pequeña, pero fueron capaces de añadir nuevas características y para finales de los ochenta habían dejado totalmente atrás a sus contrapartes CISC. Con el tiempo esto pudo ser dirigido como una mejora de proceso al punto en el que todo esto pudo ser añadido a los diseños CISC y aun así caber en un solo chip, pero esto tomó prácticamente una década entre finales de los ochenta y principios de los noventa.

En pocas palabras esto significa que para cualquier nivel de desempeño dado, un chip RISC típicamente tendrá menos transistores dedicados a la lógica principal. Esto permite a los diseñadores una flexibilidad considerable; así pueden, por ejemplo:


Las características que generalmente son encontradas en los diseños RISC son:

Los diseños RISC también prefieren utilizar como característica un modelo de memoria Harvard, donde los conjuntos de instrucciones y los conjuntos de datos están conceptualmente separados; esto significa que el modificar las direcciones donde el código se encuentra pudiera no tener efecto alguno en las instrucciones ejecutadas por el procesador (porque la CPU tiene separada la instrucción y el caché de datos, al menos mientras una instrucción especial de sincronización es utilizada). Por otra parte, esto permite que ambos cachés sean accedidos separadamente, lo que puede en algunas ocasiones mejorar el rendimiento.

Muchos de esos diseños RISC anteriores también compartían una característica no muy amable, el slot de salto retardado (Delay Slot). Un slot de salto retardado es un espacio de instrucción siguiendo inmediatamente un salto. La instrucción en este espacio es ejecutada independientemente de si el salto se produce o no (en otras palabra el salto es retardado). Esta instrucción mantiene la ALU de la CPU ocupada por el tiempo extra normalmente necesario para ejecutar una brecha. Para utilizarlo, recae en el compilador la responsabilidad de reordenar las instrucciones de manera que el código sea coherente para ejecutar con esta característica. En nuestros días el slot de salto retardado se considera un desafortunado efecto colateral de la estrategia particular por implementar algunos diseños RISC. Es por esto que los diseños modernos de RISC, tales como ARM, PowerPC, y versiones más recientes de SPARC y de MIPS, generalmente eliminan esta característica.

El primer sistema que pudiera ser considerado en nuestros días como RISC no lo era así en aquellos días; era la supercomputadora CDC 6600, diseñada en 1964 por Seymour Cray.

Cray la diseñó como un CPU para cálculos a gran escala (con 74 códigos, comparada con un 8086 400, además de 12 computadores simples para manejar los procesos de E/S (la mayor parte del sistema operativo se encontraba en uno de éstos).

El CDC 6600 tenía una arquitectura de carga/almacenamiento con tan solo dos modos de direccionamiento. Había once unidades de canalización funcional para la aritmética y la lógica, además de cinco unidades de carga y dos unidades de almacenamiento (la memoria tenía múltiples bancos para que todas las unidades de carga/almacenamiento pudiesen operar al mismo tiempo). El nivel promedio de operación por ciclo/instrucción era 10 veces más rápido que el tiempo de acceso a memoria.

Los diseños RISC que más se dieron a conocer sin embargo, fueron aquellos donde los resultados de los programas de investigación de las universidades eran ejecutados con fondos del programa DARPA VLSI. El programa VLSI prácticamente desconocido hoy en día, llevó a un gran número de avances en el diseño de chips, la fabricación y aún en las gráficas asistidas por computadora.

Una de las primeras máquinas de carga/almacenamiento fue la minicomputadora Data General Nova, diseñado en 1968 por Edson de Castro. Había un conjunto de instrucciones RISC casi puro, muy similar a la de los procesadores ARM de hoy, sin embargo no ha sido citado como haber influido en los diseñadores del ARM, aunque estas máquinas estaban en uso en la Universidad de Cambridge ComputerLaboratory en la década de 1980.

El proyecto RISC de la Universidad de Berkeley comenzó en 1980 bajo la dirección de David A. Patterson, basándose en la obtención de rendimiento a través del uso de la canalización y un agresivo uso de los registros conocido como ventanas de registros. En una CPU normal se tienen un pequeño número de registros, un programa puede usar cualquier registro en cualquier momento. En una CPU con ventanas de registros, existen un gran número de registros (138 en el RISC-I), pero los programas solo pueden utilizar un pequeño número de estos (32 en el RISC-I) en cualquier momento.

Un programa que se limita asimismo a 32 registros por procedimiento puede hacer llamadas a procedimientos muy rápidas: la llamada, y el regreso, simplemente mueven la ventana de 32 registros actual para limpiar suficiente espacio de trabajo para la subrutina, y el regreso "restablece" esos valores.

El proyecto RISC entregó el procesador RISC-I en 1982. Consistiendo de solo 44.420 transistores (comparado con promedios de aproximadamente 100.000 en un diseño CISC de esa época) RISC-I solo tenía 32 instrucciones, y aun así sobrepasaba el desempeño de cualquier otro diseño de chip simple. Se continuó con esta tendencia y RISC-II en 1983 tenía 40.760 transistores y 39 instrucciones, con los cuales ejecutaba 3 veces más rápido que el RISC-I.

Casi al mismo tiempo, John Hennessy comenzó un proyecto similar llamado MIPS en la Universidad de Stanford en 1981. MIPS se centraba casi completamente en la segmentación, asegurándose de que ejecutara tan "lleno" como fuera posible. Aunque la segmentación ya había sido utilizada en otros diseños, varias características del chip MIPS hacían su segmentación mucho más rápida. Lo más importante, y quizá molesto de estas características era el requisito de que todas las instrucciones fueran capaces de completarse en un solo ciclo. Este requisito permitía al canal ser ejecutado a velocidades más altas (no había necesidad de retardos inducidos) y es la responsable de la mayoría de la velocidad del procesador. Sin embargo, también tenía un efecto colateral negativo al eliminar muchas de las instrucciones potencialmente utilizables, como una multiplicación o una división.

El primer intento por hacer una CPU basada en el concepto RISC fue hecho en IBM el cual comenzó en 1975, precediendo a los dos proyectos anteriores. Nombrado como proyecto RAN, el trabajo llevó a la creación de la familia de procesadores IBM 801, la cual fue utilizada ampliamente en los equipos de IBM. El 801 fue producido eventualmente en forma de un chip como "ROMP" en 1981, que es la abreviatura de "Research Office Products Division Mini Processor". Como implica el nombre, esta CPU fue diseñada para "tareas pequeñas", y cuando IBM lanzó el diseño basado en el IBM RT-PC en 1986, el rendimiento no era aceptable. A pesar de esto, el 801 inspiró varios proyectos de investigación, incluyendo algunos nuevos dentro de IBM que eventualmente llevarían a su sistema IBM POWER.

En los primeros años, todos los esfuerzos de RISC eran bien conocidos, pero muy confinados a los laboratorios de las universidades que los habían creado. El esfuerzo de Berkeley se dio a conocer tanto que eventualmente se convirtió en el nombre para el proyecto completo. Muchos en la industria de la computación criticaban el que los beneficios del rendimiento no se podían traducir en resultados en el mundo real debido a la eficiencia de la memoria de múltiples instrucciones, y ésa fue la razón por la que nadie los estaba utilizando. Pero a comienzos de 1986, todos los proyectos de investigación RISC comenzaron a entregar productos. De hecho, casi todos los procesadores RISC modernos son copias directas del diseño RISC-II.

La investigación de Berkeley no fue comercializada directamente, pero el diseño RISC-II fue utilizado por Sun Microsystems para desarrollar el SPARC, por Pyramid Technology para desarrollar sus máquinas de multiprocesador de rango medio, y por casi todas las compañías unos años más tarde. Fue el uso de RISC por el chip de SUN en las nuevas máquinas el que demostró que los beneficios de RISC eran reales, y sus máquinas rápidamente desplazaron a la competencia y esencialmente se apoderaron de todo el mercado de estaciones de trabajo.

John Hennessy dejó Stanford para comercializar el diseño MIPS, comenzando una compañía conocida como MIPS Computer Systems Inc. Su primer diseño fue el chip de segunda generación MIPS-II conocido como el "R2000". Los diseños MIPS se convirtieron en uno de los chips más utilizados cuando fueron incluidos en las consolas de juego Nintendo 64 y PlayStation. Hoy son uno de los procesadores integrados más comúnmente utilizados en aplicaciones de alto nivel por Silicon Graphics.

IBM aprendió del fallo del RT-PC y tuvo que continuar con el diseño del RS/6000 basado en su entonces nueva arquitectura IBM POWER. Entonces movieron sus computadoras centrales S/370 a los chips basados en IBM POWER, y se sorprendieron al ver que aun el conjunto de instrucciones muy complejas (que era parte del S/360 desde 1964) corría considerablemente más rápido. El resultado fue la nueva serie System/390 que aún hoy en día es comercializada como zSeries. El diseño IBM POWER también se ha encontrado moviéndose hacia abajo en escala para producir el diseño PowerPC, el cual eliminó muchas de las instrucciones "solo IBM" y creó una implementación de chip único. El PowerPC fue utilizado en todas las computadoras Apple Macintosh hasta 2006, y está comenzando a ser utilizado en aplicaciones automotrices (algunos vehículos tienen más de 10 dentro de ellos), las consolas de videojuegos de última generación (PlayStation 3, Wii y Xbox 360) están basadas en PowerPC.

Casi todos los demás proveedores se unieron rápidamente. De los esfuerzos similares en el Reino Unido resultó el INMOS Trasputer, el Acorn Archimedes y la línea Advanced RISC Machine, la cual tiene un gran éxito hoy en día. Las compañías existentes con diseños CISC también se unieron a la revolución. Intel lanzó el i860 y el i960 a finales de los ochenta, aunque no fueron muy exitosos. Motorola construyó un nuevo diseño pero no le vio demasiado uso y eventualmente lo abandonó, uniéndose a IBM para producir el PowerPC. AMD lanzó su familia 29000 la cual se convirtió en el diseño RISC más popular a principios de los noventa.

Hoy en día los microcontroladores y CPU RISC representan a la vasta mayoría de todos los CPU utilizados. La técnica de diseño RISC ofrece poder incluso en medidas pequeñas, y esto ha venido a dominar completamente el mercado de CPU integrados de bajo consumo de energía. Los CPU integrados son por mucho los procesadores más comunes en el mercado: considera que una familia completa con una o dos computadoras personales puede poseer varias docenas de dispositivos con procesadores integrados. RISC se apoderó completamente del mercado de estación de trabajo. Después del lanzamiento de la SUN SPARCstation los otros proveedores se apuraron a competir con sus propias soluciones basadas en RISC. Aunque hacia 2006-2010 las estaciones de trabajo pasaron a la arquitectura x86-64 de Intel y AMD. Incluso el mundo de las computadoras centrales está ahora basado completamente en RISC.

Esto es sorprendente en vista del dominio del Intel x86 y x86 64 en el mercado de las computadoras personales de escritorio (ahora también en el de estaciones de trabajo), ordenadores portátiles y en servidores de la gama baja. Aunque RISC fue capaz de avanzar en velocidad muy rápida y económicamente.

Los diseños RISC han llevado a un gran número de plataformas y arquitecturas al éxito, algunas de las más grandes:






Uso Actualmente




</doc>
<doc id="2512" url="https://es.wikipedia.org/wiki?curid=2512" title="Receta de cocina">
Receta de cocina

Una receta de cocina (o simplemente receta), en gastronomía, es una descripción ordenada de un procedimiento culinario. Suele consistir primero en una lista de ingredientes necesarios, seguido de una serie de instrucciones con la cual se elabora un plato o una bebida específicos. Suele incluir en algunos casos una lista de los utensilios de cocina adecuados para su realización. Ocasionalmente incluye una descripción social, histórica que motiva la receta.

Las recetas pueden transmitirse a lo largo de la historia de los pueblos, de generación en generación, mediante tradición oral, o escritas mediante su recopilación en libros de cocina o recetarios culinarios. Este conocimiento compilado forma parte importante de la cultura de un grupo humano, su evolución permite conocer los cambios a los que se ve sometida una cultura. Su empleo en estudios sociológicos y antropológicos, o en historia permite conocer las condiciones culinarias, los gustos, e influencias de un periodo. En el siglo XXI las recetas culinarias aparecen frecuentemente en medios de comunicación como programas de televisión, revistas, periódicos y blogs.

Una de las primeras evidencias documentales de recetas conocidas procede de 1600 en forma de tablilla de barro procedente del sur de Babilonia con escritura cuneiforme y expresada en idioma acadio. Los griegos tuvieron escritores culinarios dedicados como el poeta Arquestrato, el prolífico escritor culinario Timáquides de Rodas. Ninguno de sus recetarios ha llegado a nuestros días. Uno de los primeros libros de recetas conocidos en la cocina occidental fue de "De re coquinaria" escrito por el cocinero romano Marco Gavio Apicio.

El nombre «receta» proviene del latín "recipere" que indica por igual 'dar'/'recibir'. Inicialmente en los textos de recetas, los procesos culinarios se describían como una secuencia de instrucciones. El primer recetario medieval fue un manuscrito alemán del siglo XIII. La cocina española tiene en el "Libre del Sent Soví", 1324 uno de sus recetarios más antiguos.

En Europa los cocineros franceses Antonin Carême y Georges Auguste Escoffier comienzan a definir las técnicas de la cocina, y entre su tarea la de recopilar y sistematizar los procesos de cocina. En América del Norte Isabella Beeton escribe a finales del siglo XIX su "Book of Household Management" como uno de los primeros recetarios modernos. El fenómeno antropológico de las recetas culinarias, como transmisión de cultura fue estudiado por Claude Lévi-Strauss en su obra "Les mythologiques 3: L'origine des manières de table" (El origen de las maneras en la mesa). En los años sesenta aparecen los programas de televisión mostrando formas de cocinar.

Las recetas culinarias anteriores al siglo XX poseían más una estructura narrativa que permitía cierta creación literaria paralela. Es precisamente en las primeras décadas del siglo XX cuando aparece una estructura separada de ingredientes/procesos en la descripción de las recetas culinarias. Si la tradición oral se cristalizó en una literatura culinaria en forma de libros de cocina escritos desde el siglo XVIII, en el siglo XX las recetas se describen en programas de televisión, en revistas (especializadas o no), siendo además muy populares en diversos blogs especializados.
Las recetas tienen unas normas y reglas precisas para su escritura. Si la receta se dirige al público en general, debería estar escrita en lenguaje llano. En muchos casos presuponen un conocimiento básico de las técnicas de cocina. Las recetas aparecen generalmente categorizadas en familias que se agrupan por ingrediente principal, tipo de preparación, país, etc. forman parte de un libro de cocina.

Las recetas formales incluyen como elementos:


En recetas editadas en libros, o recetarios culinarios, usualmente se incluye una fotografía del plato ya montado, y generalmente ya decorado para su muestra a los comensales. A veces incluye una ilustración secuenciada de los procesos más notables. Para mejorar la didáctica de la receta suele incluirse indicaciones acerca de como elegir un buen ingrediente, detalles sobre la calidad de los mismos. Si el ingrediente indicado no es habitual al lector, proporcionar consejos acerca de donde poder encontrarlo. La inclusión sobre ciertos detalles empleados en las técnicas culinarias no habituales puede ayudar a un lector a reproducción con éxito la receta. En las recetas se incluye a veces el cómputo total de calorías que supone su ingesta, así como cualquier indicación nutricional. En las recetas culinarias generalmente es menos deseable conocer el origen histórico, sociológico del plato.

Históricamente no se ha considerado que las recetas culinarias posean derechos de autor, debido en parte a que se ha considerado a las preparaciones gastronómicas como un proceso generalmente útil a la humanidad bajo los principios del "Utilitarismo". Con el devenir de la innovación en la cocina, así como la industrialización de algunas preparaciones, a lo largo del siglo XXI se ha replanteado la cuestión. Existen casos de autoría confusa, de disputa en el reconocimiento. Algunas recetas poseen usos privados que producen beneficios a ciertos restaurantes y cocineros, y es por esta razón que algunas recetas (generalmente salsas) se han protegido con una marcas registradas y poseen derechos de autor. En algunas ocasiones se pretende proteger la preparación de un plato con el objeto de preservar una cultura, y es por esta razón por la que su elaboración se supervisa por un "organismo competente". En otras ocasiones las recetas culinarias están libres de licencias, y su elaboración no está regulada por organismo alguno. En Estados Unidos las leyes de Copyright (como la "Copyright Act of 1976") protegen invenciones, ideas y dispositivos, excepto las recetas culinarias.



</doc>
<doc id="2514" url="https://es.wikipedia.org/wiki?curid=2514" title="Rhytachne">
Rhytachne

Rhytachne es un género de plantas herbáceas perteneciente a la familia de las poáceas. Es originario de Sudamérica tropical, Madagascar y África tropical.



</doc>
<doc id="2515" url="https://es.wikipedia.org/wiki?curid=2515" title="Rhombolytrum">
Rhombolytrum

Rhombolytrum es un género de plantas herbáceas perteneciente a la familia de las poáceas. Es originario de Sudamérica. Comprende 5 especies descritas y de estas, solo 2 aceptadas. 

El género fue descrito por Heinrich Friedrich Link y publicado en "Hortus Regius Botanicus Berolinensis" 2: 296. 1833. La especie tipo es: "Rhombolytrum rhomboideum" Link
Rhombolytrum: nombre genérico 
A continuación se brinda un listado de las especies del género "Rhombolytrum" aceptadas hasta enero de 2014, ordenadas alfabéticamente. Para cada una se indica el nombre binomial seguido del autor, abreviado según las convenciones y usos.



</doc>
<doc id="2516" url="https://es.wikipedia.org/wiki?curid=2516" title="Rhipidocladum">
Rhipidocladum

Rhipidocladum es un género de bambúes nativos de América. Pertenece a la subfamilia de las Bambusóideas, dentro de la familia de las gramíneas o Poáceas.
Rizomas paquimorfos, culmos de 7 a 22 m de largo, generalmente decumbentes en la vegetación aledaña; 1 hasta 1,5 cm de diámetro. Las ramas por nudo pueden variar entre 30 y 200 según la especie, no verticiladas sino creciendo en forma de abanico en la base. Las ramas primarias de hasta 35 cm de largo. Cúlmeas u hojas caulinares lisas y brillantes, no persistentes en los entrenudos inferiores. Presentan de 12 a 32 espiguillas por racimo de acuerdo a la especie.

Ramas primarias creciendo en forma de abanico y en número que varía de 35 a 200 según la especie. Culmo hueco y quebradizo.
Ramas secundarias de hasta 35 cm de largo.

El género "Rhipidocladum" se distribuye naturalmente desde México hasta Argentina y Chile. 

Bosques de galería, riparia. Selvas medianas. Ecotonos. 200 a 900 msnm
El género fue descrito por Floyd Alonzo McClure y publicado en "Smithsonian Contributions to Botany" 9: 101, f. 42. 1973. La especie tipo es: "Rhipidocladum harmonicum" (Parodi) McClure

Dentro del género se reconocen tres secciones:

y un total de 19 especies: 



</doc>
<doc id="2520" url="https://es.wikipedia.org/wiki?curid=2520" title="Raddiella">
Raddiella

Raddiella, es un género de plantas herbáceas perteneciente a la familia de las poáceas. Es originario de América tropical.




</doc>
<doc id="2521" url="https://es.wikipedia.org/wiki?curid=2521" title="Raddia">
Raddia

Raddia, es un género de plantas herbáceas perteneciente a la familia de las poáceas. Es originario del centro y sur de América.

Las especies descritas de "Raddia" son 9:



</doc>
<doc id="2525" url="https://es.wikipedia.org/wiki?curid=2525" title="Runrig">
Runrig

Runrig es un grupo escocés de música rock con toques de música celta tradicional. Cantan en inglés y en Gaélico escocés.

La en origen Banda de Baile Runrig, debutó como tal en el Glasgow Kelvin Hall en 1973. La formaban Rory MacDonald a la guitarra, Calum MacDonald a la batería y Blair Douglas al acordeón.

Donnie Munro se unió al año siguiente para darle más aporte vocal. Más tarde Blair se fue y se les unió como acordeonista un viejo amigo de la escuela, Robert MacDonald, quien falleció tristemente en 1986, tras una larga batalla contra el cáncer.

Hasta 1978 la banda había sido una ocupación parcial de estudiantes y fue en éste formato como se grabó su primer álbum "Play Gaelic" en la discográfica escocesa Limor Recordigns.

Tras este primer paso sintieron que podrían y debían llevar adelante su propia discográfica para dotarles de libertad artística y financiera para grabar su más ambicioso segundo álbum. 

Eran tiempos de gran riesgo y retos, si bien ciertamente éstos han rodeado toda la historia de Runrig. Ridge Records culminó con éxito y dotó a la banda de estatus profesional completo. Malcom fue persuadido para abandonar una brillante carrera universitaria, Rory diseñador gráfico, realizó su última obra maestra para las agencias publicitarias y Donnie and Calum, profesores, ya no podían salir de las clases lo suficientemente rápido como deseaban.
En 1979 entraron en el estudio y grabaron The HighLand Connection.

El sonido de la banda iba enriqueciéndose y había necesidad de extender sus parámetros musicales. Reforzaron la sección rítmica con la unión de un “Fifer” (fife:tipo de flauta), Iain Bayne, haciéndose cargo de la batería, pasando Calum a la percusión. Ésta fue la formación que grabó el clásico "Recovery" en 1981.
Recovery fue un álbum conceptual, tratando de la historia social del Gaélico y les introdujo en la lucha por el idioma y culturas Gaélicos, parte de la cual la banda siempre ha sido. Es desde este ambiente desde el que el núcleo de la banda ha hecho su razón de ser musical, física, emocional y espiritual.

Tras "Recovery", la banda sintió la necesidad de extender sus alas desde las Tierras Altas Gaélicas para alcanzar más vastas audiencias que parecían tener interés a su música.

Aunque ciertamente, como banda de Folk, sus canciones reflejaban la tradición, era gratificante alcanzar a nuevas audiencias que se interesaban por la música de la banda. Parecía ahora que el Gaélico podía cruzar y ser aceptado por un entorno totalmente diferente.
Una vez más se dio la necesidad de extender sus parámetros musicales, para entonces convertidos ya en sexteto, empleando los servicios de un inglés, Richard Chern a los teclados, Tenían el dilema de permanecer en Ridge Records o salir en busca de contratos mayores.
Iniciaron, pues, este último camino pero no fue el momento adecuado y la compañía de grabación en cuestión no resultó ser lo más adecuado para la banda. Así experimentaron plenamente la hegemonía de los dictados comerciales sobre los artísticos con una compañía londinesa.

Finalmente la banda volvió a Ridge Records pero sólo tras un prolongado retraso, tras el cual, en 1985, se grabó finalmente el 4º álbum, "Heartland". Richard se fue en 1986 para trabajar en el teatro y su vacante fue cogida por otro “Fifer”, el trabajador social ex BigCountry Peter Wishart; formación, la cual, perduró en la siguiente década. 
1987 fue claramente el año de la explosión. Los puntos más llamativos incluyen un exitoso viaje a Canadá, la primera salida al Telón de Acero para tocar en un festival en Berlín Este, un concierto en vivo emitido en ITV una colaboración con los recién erigidos reyes de Rock’n’Roll U2 en Murrayfield Stadium, Edimburgo y la publicación de "The Cutter and the Clan".
El álbum fue un increíble éxito para el sello Ridge, llevando a Runrig de la industria rural al ámbito nacional. Era tiempo de firmar con una de las compañías de discos en auge que más interés había mostrado por el grupo; Chrysalis Records con su carácter de independencia e integridad musical se convirtió en compañera inseparable. En el verano de 1987 firmó un importante contrato internacional con la banda y se convirtió en la estrella de un total acercamiento a su vida y trabajo.
"The Cutter and the Clan" fue inmediatamente reeditado con Chrysalis seguido rápidamente por el largamente esperado "Once in a Lifetime". 1989 vio la publicación de "Searchlight", el cual entró directamente en le puesto 11º de las listas nacionales, y tuvo su continuación con una gira internacional con 50 citas en Reino Unido y Europa, culminando en las Barrowlands Escocesas, con un concierto grabado por la STV para un futuro vídeo.

La nueva década se inició con un bombazo: apenas 30 escasos minutos tomados de la banda en el Glasgow George Square bastaron para liderar el Show de Hogmanay en la BBC. La cadena STV realizó un documental de una hora grabado en los conciertos de las BarrowLands que emitió en mayo. La respuesta fue abrumadora. La centralita de STV entonces se saturó durante horas y el programa tuvo cifras de audiencia improcedentes para la cadena. En 1990 vio también la luz el EP "Capture the Hearth", el cual entró directamente en las listas de singles con el número 49. La apertura en el Royal Concert Hall en Glasgow presentó la oportunidad de tocar en múltiples noches. El resultado fue de 5 conciertos con el cartel de “completo”. El esperado vídeo "City of Lights" fue publicado en noviembre, entrando el las listas nacionales con el número 7. Ese año se completó con el tour de "Alba". Un año más ocupado de lo esperado, pero aún quedaba más por venir.

Hay años llenos de trabajo y años “a tope”, y así fue 1991. Listar todos los eventos no sería suficiente para relatar todo el trabajo de todos aquellos conectados con la banda sino sólo una ligera idea.
El octavo y más exitoso álbum, "The Big Wheel", entró directamente en las Listas Nacionales en el puesto 4º. El concierto al aire libre en el Balloch Country Park en Loch Lomond, seguido por 50,000 personas fue una inmejorable ocasión para probar la sin duda brillante carrera de Runrig. El tour de las HighLands y las Islas les llevó de nuevo a casa con una gigantesca carpa. El single "Hearthammer" irrumpió en el Top 40 en el puesto 25. Dos conciertos más en la explanada del Castillo de Edimburgo atrajeron enorme interés de los medios, convirtiendo el Tour de "The Big Wheel" en internacional.

Otro single, "Flower of the West", fue publicado y el libro de Tom Morton "Going Home", acabó el año en la cima, convirtiéndose inmediatamente en un best seller escocés.
1992 vio a Runrig trabajando duro en los estudios de nuevo así como un buen número de apariciones en festivales por toda Europa. Los fans vieron 2 veces a la banda talonear a Génesis; primero Hockenheim, Alemania y de nuevo en Roundhay Park, Leeds. Los fans del otro lado del Atlántico, en Canadá tuvieron la oportunidad de verles tocar en Toronto y Montreal, y pisaron EE.UU., por primera vez para filmar en Nueva York un documental para la STV titulado "Air an Oir".
El 24 de agosto un nuevo video, "Wheel in Motion", fue publicado con imágenes en vivo del memorable concierto de Loch Lomond, del concierto de las HighLands y las Islas, el Castillo de Edimburgo y otras citas por Europa a lo largo de 1991.Al mismo tiempo, continuaron trabajando en Air an Oir con Graeme Strong para la Televisión Escocesa. Esta película fue emitida en Año Nuevo, despidiendo justamente 1993 en la dirección correcta.

Noviembre de 1992 vio a Runring retornar a los estudios Castle Sound, en Pencaitland, para grabar su siguiente álbum que estaba completado para enero de 1993. El single "Wonderful", se publicó primero y a continuación el álbum "Amazing Things". El mismo alcanzó la más alta posición nunca conseguida en la Listas, entrando en la Clasificación Gallup con el número 2, a sólo un punto de la cima. Wonderful y el segundo single "Greatest Flame", fueron ambos reproducidos en el Top of the Pops debido a su éxito en las Listas.
El resto de 1993 se empleó en promocionar el álbum retornando al directo. El tour de "Amazing Things" fue el más exitoso hecho nunca y para el último show en Barrowlands el 22 de diciembre, la banda había tocado 99 conciertos. Visitaron por segunda vez los teatros de Alemania e Inglaterra amen de otras notables actuaciones en Irlanda, Londres y las Fleadh Escocesas.

Finlandia se convirtió en un nuevo territorio cuando tocaron el en Festival de Turku en agosto de vuelta a casa entraron de nuevo en el Top e iniciaron un pequeño tour por Castillos y Lienzos culminando con un agradable retorno a la explanada del Castillo de Edimburgo en septiembre. Un punto álgido del año de "Amazing Things" fue cuando el álbum ganó el galardón de mejor disco del año por el Bristish Enviroment & Media.
Tras tomar un poco de aire y descansar, 1994 fue tomado como un año para apartarse un poco, escribir y planificar el nuevo álbum. Un álbum en vivo salió a la luz a finales de año y el siguiente álbum de estudio a principios de 1995. Para la grabación del álbum en directo la banda buscó sitios imaginativos al aire libre en los que tocar durante el verano, tales como Tarlair en el Nordeste de Escocia, Colonia en Alemania y 2 noches en el Castillo de Stirling cuyas actuaciones fueron cuidadosamente grabadas. La última noche del tour del "Amazing Things" fue también incluida.

Tras una breve visita a Canadá, "Transmitting Live" fue publicado en noviembre de 1994 y fue seguido por un tour escocés, culminado en diciembre con la transmisión en vivo en Hogmanay TV el show desde Princess Street Garden, en Edimburgo.
El nuevo año de 1995 vio a Runrig de nuevo en el entramado de las grabaciones. Tras un periodo de composición y ensayos, se recluyeron en los Studios Chapel de Lincolnshire para iniciar en abril la grabación de su siguiente disco. Las sesiones de Mara continuaron durante la primavera y el verano en los más cercanos estudios de Castle Sound, cerca de Edimburgo. En un pequeño interludio de las grabaciones, se publicó el single "Uhbal As Airde". Usándose como banda sonora de una anuncio televisivo de Carlsberg, atrajo la atención del gran público, consiguiendo el más alto puesto conseguido nunca de las Listas de Singles, el puesto 18º y otra aparición en el Top of the Pops. El hecho más reconfortante del éxito de la canción fue el hecho que fue la primera canción Gaélica en hacer el puesto 20º.
En junio una escapada del estudio fue aprovechada para tocar en una serie de festivales y conciertos en Europa. En particular, el primer gran concierto al aire libre ante 20.000 personas en Alemania en Loreley, a las riberas de Rhin. 

Antes de los conciertos europeos la banda tocó para Rod Steward en el Pittodrie Stadium en Aberdeen, y hacia el final de agosto, les llegó por sorpresa una invitación para talonear a los Rolling Stones, en Schuttorf, Alemania. Ese día fue particularmente significativo puesto que fueron requeridos para tocar 2 veces en la misma tarde, viajando posteriormente para hacer su propio concierto en Jubek, apoyados por Mike and the Mechanics, lo que requirió una cuidada planificación amen de un rápido vuelo en avión.
El nuevo álbum "Mara" fue publicado en otoño seguido otra mini gira por Europa y Reino Unido. El primer single "Things that Are" entró en el Top 40 y volvió a su ya habitual puesto en el Tops of the Pops. El tour de "Mara" fue el más ambicioso hasta la fecha, a lo que a producción se refiere y para muchos de los fans fueron los shows más fantásticos hechos nunca.
Culminado y aparcado el exitoso "Mara" todo el mundo pensaba que la banda estaba en una encrucijada y era tiempo de pensar en el futuro y en sus aspiraciones individuales y colectivas. Donnie cada vez más involucrado en la Política veía su cada vez más involucración en la mismo como un proceso delante de Runrig.
Aunque todo el mundo lo suponía, nadie estaba lo suficientemente preparado para el anuncio de abandonar la banda en breve. La banda entró en el más dudoso y discontinuo periodo de su historia. No quería hacer ninguna declaración pública durante cierto período por si había cambio de planes y hasta que todo el mundo lo hubiera reflexionado lo suficiente. De alguna manera, todos creían que estaban ante el fin de la formación que Runrig había tenido desde 1986 y cuando pusieran una fecha sería una decisión consensuada por todos.
Mientras tanto la práctica del trabajo diario tenía que continuar. No importaba que vendría por delante. Estaban llegando al final de una era de la banda y junto con la compañía discográfica decidieron que era hora de lanzar un grandes éxitos: "Best of Runrig".
Habría sido imposible para la banda decidir el listado final de canciones luego pensaron que sería mejor dejarlo en manos de los fans, cosa que hicieron a través del club de fans. Un fascinante ejercicio de mercado: un disco que era para los fans decidido por ellos mismos. "Long Distance" fue publicado el 7 de octubre, entrando en el puesto 13 y dando pie a un largo y gustoso tour a lo largo del otoño de 1996 y la primavera de 1997, parando a tocar una vez más en el ya considerado como el mayor concierto callejero en Princess Street Garden, Edimburgo. El primer single del álbum era una versión de del "Rhythm of My Heart" de Rod Stewart, grabado hacia el final de la sesiones de "Mara" para una posible inclusión en el film Loch Ness.

El último show de "Long Distance" fue en Bielefeld el 12 de marzo y de ahí todo el mundo retornó a su casa para poner en marcha sus aspiraciones individuales y relejar sus individualidades pospuestas. Donnie estaba fuera en la campaña de las Elecciones Generales, habiendo aceptado la nominación por el Partido Liberal donde tomó el segundo puesto de Charles Kennedy, quien casualmente era fan de Runrig.
Malcom se tomó un respiro de la música de Runrig para enfrascarse en proyectos propios mientras Rory y Calum disfrutaban del relativo lujo de trabajar con nuevo material sin la presión de fechas y objetivos específicos.Todos estaban esperando hasta que se celebraran las Elecciones Generales y se hiciera público el futuro de Donnie y la banda sintió que los primeros en ser informados debían ser los fans y así lo hicieron a través de la revista de su club de fans. 
Desafortunadamente con todo el interés en la prensa que había acumulado Donnie durante la campaña electoral, los medios desbordaron de especulaciones y rumores y eventualmente el Castillo Aberdeen no tuvo otra opción que bajar la verja para refugiarse del acoso mediático y se hizo público.

Eran tiempos prácticos. La salida de Donnie tenía que marcar de alguna manera y todos estaban expectantes ante el largamente prometido concierto al aire libre en verano en Escocia. Stirling fue considerado como el sitio ideal, junto con una serie de conciertos para sus fans europeos en Dinamarca y un par de noches para sus seguidores ingleses en Mánchester. El concierto de Stirling se alargó durante 3 noches y se añadió un concierto alemán de retorno a Tanzbrunnen, Colonia donde se grabó parte del álbum en vivo "Transmitting". Los shows finales con Donnie fueron emocionantes para todos y fue la mejor manera de celebrar el final de una era en la existencia de la banda y para que Donnie diera su personal adiós.
La segunda noche fue grabada en vivo para su publicación en vídeo a finales del otoño y aunque fue muy popular no pudo capturar toda la emoción y el espíritu del show. El último show del sábado a la noche fue sin duda el más emocionante y significativo concierto de la banda hasta entonces. El invierno de 1997 llegó con la banda reducida a 5 miembros y con la ardua tarea de audicionar a cantantes para la nueva vacante creada con la gira anual de Navidades en puertas y sin nadie que les emocionara particularmente para la siguiente etapa del viaje.

1998 fue el año que marcó el 25º aniversario de la banda, y para celebrarlo la ocasión comenzaron a trabajar en un proyecto que muchos fans habían estado pidiendo durante mucho tiempo. Una colección de temas Gaélicos en CD. Las grabaciones fueron recopiladas, remasterizadas digitalmente y publicadas en "The Gaelic Colection" con el sello Ridge Records en mayo.
La primera parte de 1998 fue empleada en más audiciones aunque con constante decepción y sin ningún resultado positivo, lo que supuso que la banda se percatara que los resultados positivos tardarían en llegar. Para entonces ya habían comenzado la grabación del siguiente álbum de estudio cuyo trabajo, desde su inicio parecía controlado y excitante. Tras todo el estrés y las dificultades de todos los involucrados en el mismo las cosas iban retornando poco a poco a la normalidad e iban encarando los cambios de manera positiva.

La larga búsqueda de un nuevo cantante llegaba por fin a su final y para mediados de julio la banda pudo anunciar que Bruce Guthro, de la isla Cape Breton, en Nueva Escocia, Canadá, iba a ser el nuevo miembro de Runrig. Bruce había llamado la atención de todos hacia el final del proceso de audición y tras oírle por primera vez supieron instantáneamente que tenían algo especial entre manos. La nueva era Runrig había comenzado. Bruce cruzó de nuevo el Atlántico para grabar 6 canciones del álbum y aunque no fuera publicado hasta marzo de 1999, los fans estaban desesperando por oír la nueva formación en vivo. El primer concierto se confirmó para ser en el Tonder Music Festival en Dinamarca a finales de agosto, en una corta aparición. El tour "Next Stage" se planeó para otoño e invierno. Increíblemente sonaba como si, tras mucha incertidumbre, la banda pudiera entrar en el milenio recargada, fresca y con mucho más entusiasmo que nunca antes.

El primer show de Bruce fue un poco nervioso pero su popularidad y aceptación sería completa durante los conciertos del corto "Next Stage". Guthro fue totalmente arrollado por el calor y el apoyo recibido por los fans de Runrig. El álbum, "In Search of Angles" se publicó a primeros de marzo de 1999 y vio gustosamente el retorno de Runrig al sello Ridge Records en Reino Unido y copó el puesto 26 en las lista nacionales. El single "The Message" se publicó una semana antes del álbum y aunque fue pinchado mucho fue relegado de las listas de singles porque el rejuvenecido sello Ridge Records no se había percatado que las reglas de la industria en cuanto al formato single habían cambiado. Se habían incluido demasiadas caras-B.

El tour del álbum se inició con 2 noches en el Glasgow Royal Concert Hall antes de emprender el circuito habitual. El segundo single "Maymorning" se hizo coincidir con las elecciones generales de l nuevo parlamento escocés el 6 de mayo y fue elegido por la televisión escocesa para ilustrar la cobertura informativa a dichas elecciones durante la campaña. El tour del álbum continuó durante todo el verano con una serie de conciertos al aire libre y festivales en Dinamarca y Alemania.
Por entonces el milenio que se avecinaba parecía estar exclusivamente en las mentes de la gente. La banda parecía ser el candidato natural como amenizador de Hogmanay. Se consideraron muchas ofertas pero ésta parecía ser la más apropiada de la audiencia de Inverness, capital de las Highland.

Enero vio a la banda tocar por primera vez en el ya prestigioso festival de Folk Celtic Connection". El show fue todo un éxito y fue grabado en vivo para publicarse en un CD: "Live at Celtic Connection. Ésta, junto con la publicación de un vídeo directo en Bonn, dio a muchos fans la ocasión de ver a Bruce con la banda en vivo y la oportunidad de ver y oír el nuevo sonido del grupo. Para entonces la nueva era de Bruce en Runrig ya se había establecido firmemente y la era previa se pudo evocar en diciembre con la publicación de "Flower of the West", "The Runrig Songbook": una completa colección de todas las canciones de Calum y Rory que habían aparecido en los álbumes de Runrig en los anteriores 25 años.

El camino estaba libre para la próxima grabación: el disco número 16, 9º en estudio. Puesto que Bruce había arribado a la mitad de la grabación del disco anterior, "Angels", era bueno comenzar un proyecto desde el principio. "The Stamping Ground", era un retorno a las raíces, un toque tradicional, reafirmando que la banda siempre intentaba tomar caminos musicales contemporáneos y que pudiera entrar en la radio del siglo XXI.
El proceso comenzó con una sesión de tambores étnicos en Ca.Va., en Glasgow seguido de una quincena de reclusión en los estudios Lundgaard, en Dinamarca, volviendo de nuevo a los estudios madre en Escocia en los siguientes meses.
Rompiendo el carácter del grupo aceptaron la entrada desde Brasil del fusionista céltico Paul Mounsey, y con el trabajo de producción del ingeniero de sonido danés Kristian Gislason. El álbum fue publicado en la primavera del 2001 con un éxito improcedente entre sus fans; para la opinión de muchos, el mejor álbum de Runrig hecho nunca.

Fue aquí que la banda dio otra vuelta de tuerca en lo personal resultado de una nueva incursión en la Política. El teclista Peter Wishart, se presentó como candidato por el Partido Nacional Escocés por North Tayside en las Elecciones Generales. Tuvo éxito y dejó la banda una vez más enredada en audiciones para sustituirle. Afortunadamente no fue tan traumático en esta ocasión. Un joven y avezado músico llamado Brian Hurren fue descubierto justo al término de su graduación en la Perth Music College. 
El tour de "Stamping Ground" se inició con 2 noches en el marco de la Isla de Skye y continuó por 6 semanas. En Alemania la banda tocó en el legendario show televisivo Geld Oder Lieben atrayendo la que sin duda sería la más alta audiencia conseguida nunca por la banda ante 5 millones de espectadores. Como resultado de esta actuación, el tour alemán agotó todas sus entradas y el álbum alcanzó el puesto 20 en las listas alemanas.

De vuelta a Reino Unido, los singles "Book of Golden Stories" y "Wall of China" consiguieron más tiempo en la radio que cualquiera de sus anteriores grabaciones. A lo largo del verano el tour "Stamping Ground" continuó con varios festivales y volvió a Alemania en diciembre con el tour Whisky and Gluhwein Christmas donde se filmó un concierto en Colonia para la serie Rockpalast TV transmitido luego en televisión e Internet.
Con la nueva banda ganando confianza en vivo estaba claro que la nueva era de Runrig había evolucionado. Con la mente puesta en el 2003 y el 30 aniversario de la banda, se decidió entrar rápidamente en el estudio. Partiendo de la esencia del éxito de "Stamping Ground" con el ingeniero Kristian Gislason en el estudio Arhus, Dinamarca, comenzó el proceso en los inicios del 2002. Éste iba a ser un pilar esencial y mucho se movía alrededor, máxime reconociendo los 30 años del grupo. Esto les remitió de nuevo a la influencia del músico brasileño Paul Monsey. Tras el éxito de "Running to the Light", en "Stamping Ground", Paul retornó para poner su sello en los arreglos y producción. Como parte del proyecto rearregló 2 canciones clásicas de la banda del "Recovery" de 1981.

"Proterra" se grabó en 12 estudios diferentes a los largo del mundo entre el 2002 y la primavera del 2003. Una excitante anécdota durante el proceso de grabación sucedió cuando en febrero de 2003 les fue presentado a la banda un CD recuperado intacto del accidente de la misión espacial Columbia. La tragedia conmocionó a los componentes de Runrig ya que la astronauta Dr. Lauren Clark había sido durante mucho tiempo fan del grupo y había contribuido al diario de la misión puesto que ella había usado la canción Runnig to the Light para despertar a sus compañeros en el centro de control de Houston.
Fue el marido de Lauren quien se presentó a la banda con el CD enmarcado. Fue una emocionante experiencia que apartó a todos por un rato de la música. 

Con el aniversario del 2003 en puertas, todas las esferas se concentraban en dos pilares: la publicación de "Proterra" y el concierto aniversario en la explanada del Castillo de Stirling, antes de finales de agosto. A lo largo de todo el circuito de festivales veraniegos la banda tocó un gran número de canciones antiguas: todo llevaba a Stirling. Iba a ser la más grande ocasión con significado personal para todo aquel involucrado en la marcha del grupo. Concierto que se grabaría en un nuevo disco "Day of Days", junto con un vídeo que se publicarían conjuntamente. Ültimo disco por el momento.




</doc>
<doc id="2527" url="https://es.wikipedia.org/wiki?curid=2527" title="Retórica">
Retórica

La retórica es la disciplina transversal a distintos campos de conocimiento (ciencia de la literatura, ciencia política, publicidad, periodismo, ciencias de la educación, ciencias sociales, derecho, estudios bíblicos, etc.) que se ocupa de estudiar y de sistematizar procedimientos y técnicas de utilización del lenguaje, puestos al servicio de una finalidad persuasiva o estética, añadida a su finalidad comunicativa.

Históricamente, la retórica tiene su origen en la Grecia clásica, donde se entendía, en palabras de los tratadistas clásicos, como el "ars bene dicendi", esto es, la técnica de expresarse de manera adecuada para lograr la persuasión del destinatario (etimológicamente, la palabra es un helenismo que proviene del griego ρητορική [τέχνη], «rhetorikè (téchne)»).

La retórica se configura como un "sistema de procesos y recursos" que actúan en distintos niveles en la construcción de un discurso. Tales elementos están estrechamente relacionados entre sí y todos ellos repercuten en los distintos ámbitos discursivos.

En principio, la retórica se ocupó de la lengua hablada, pero su saber trascendió al discurso escrito e influyó poderosamente en la literatura cuando la palabra escrita ganó prestigio en el régimen imperial en Roma, si bien el discurso escrito suele considerarse como una transcripción limitada o imitación estrecha del discurso oral, en la actualidad, la retórica ha vivido un gran resurgimiento y sus enseñanzas se utilizan en publicidad, la academia, la política, así como en la defensa de puntos de vista durante los juicios civiles. Por otro lado, gracias a las nuevas tecnologías audiovisuales podemos hablar de una retórica de la imagen, ya que mediante una imagen o vídeo podemos hablar sobre algo utilizando figuras retóricas (metáfora, metonimia, prosopopeya, personificación, etc.).

La retórica ocupó un lugar importante en el sistema educativo antiguo y medieval, y hasta el romanticismo su significación fue crucial dentro de las disciplinas humanísticas.

Son tres procesos complementarios los que conformaban el aprendizaje de la retórica: el "estudio de los preceptos", la "imitación de modelos" y la "práctica personal".

La elaboración del discurso verbal y su exposición ante un auditorio son aspectos que exigen la atención a cinco dimensiones que se complementan entre sí:

La finalidad de esta fase es establecer los contenidos del discurso. El término "inventio" procede del latín "invenire" que a su vez procede del griego εὒρεσις que significa «hallazgo», pues de lo que se trata es que el orador seleccione, halle, en un repertorio prefijado de temas aquellos que son los más adecuados a su exposición. Se trata, mentalmente hablando, de "invenire" («hallar») en la memoria, llena de "topoi" o "loci" («tópicos» o «lugares» comunes) las ideas propias o heredadas de la sociedad en general, susceptibles de ser utilizadas en el discurso.

La tipología del "tópico retórico" incluye los siguientes elementos: persona, cosa, lugar, instrumento, causa, modo, tiempo, comparación y argumentación, a los que habrá que añadirse el "tópico literario", en el caso de obras literarias.

Este término latino es una traducción del concepto de la retórica griega conocido como τἀξις que quiere decir «disposición». La finalidad de esta parte de la preparación discursiva es la "organización" de los elementos de la "inventio" en un todo estructurado. Son relevantes a este respecto el número de "partes del discurso" y su "orden" de aparición.


La estructuración tripartita, la más frecuente, consta de un "exordium" o parte inicial que tiene por objeto captar la atención (el interés o favor) del oyente ("captatio benevolentiae") e indicar a este la estructuración del discurso; una parte media con "narratio" (exposición del asunto y tesis del orador al respecto) y "argumentatio" (con las razones que sustentan dicha tesis); y, finalmente, una "peroratio" o recapitulación de lo dicho con apelaciones al auditorio.

El exordio busca hacer al auditorio benévolo, atento y dócil. Su función es señalizar que el discurso comienza, atraer la atención del receptor, disipar animosidades, granjear simpatías, fijar el interés del receptor y establecer el tema, tesis u objetivo.

La proposición es una enunciación breve y clara del tema que se va a tratar.

La división es la enumeración de las partes de que va a tratar el discurso.

La narración, desarrollo o exposición es la parte más extensa del discurso y cuenta los hechos necesarios para demostrar la conclusión que se persigue. Si el tema presenta subdivisiones, es preciso adoptar un orden conveniente ("partitio" o "divisio"). En la "partitio" tenemos que despojar al asunto de los elementos que no conviene mencionar y desarrollar y amplificar aquellos que sí conviene.

La argumentación es la parte donde se aducen las pruebas que confirman la propia posición revelada en la tesis de la exposición ("confirmatio" o "probatio") y se refutan las de la tesis que sostiene la parte contraria ("refutatio" o "reprehensio"), dos partes que Quintiliano considera independientes, de forma que para él el discurso forense tendría cinco. La confirmación del empleo de argumentos lógicos y de las figuras estilísticas del énfasis. También es un lugar apropiado para el postulado o enunciado sin prueba, siempre que no debilite nuestra credibilidad, para lo cual hay que recurrir al postulado no veraz pero plausible (hipótesis), a fin de debilitar al adversario desorientando su credibilidad; lo mejor en ese caso es sugerirlo y no decirlo. Se recurre a una «lógica retórica» o dialéctica que no tiene que ver con la lógica científica, pues su cometido no es hallar la verdad sino convencer. Se funda más en lo verosímil que en lo verdadero, de ahí su vinculación con la demagogia. Para los discursos monográficos enfocados a la persuasión, convienen las estructuras gradativas ascendentes. En el caso del discurso periodístico, la tendencia del lector a abandonar al principio recomienda el uso de la estructura opuesta: colocar lo más importante al principio. La retórica clásica recomienda para los discursos argumentativos monográficos el orden nestoriano, el 2,1,3: esto es, en primer lugar los argumentos medianamente fuertes, en segundo lugar los más flacos y débiles y en último lugar los más fuertes.

La peroración es la parte destinada a inclinar la voluntad del oyente suscitando sus afectos, recurriendo a móviles éticos o pragmáticos y provocando su compasión ("conquestio" o "conmiseratio") y su indignación ("indignatio") para atraer la piedad del público y lograr su participación emotiva, mediante recursos estilísticos patéticos; incluye lugares de casos de fortuna: enfermedad, mala suerte, desgracias, etc. Resume y sintetiza lo que fue desarrollado para facilitar el recuerdo de los puntos fuertes y lanzar la apelación a los afectos; es un buen lugar para lanzar un elemento nuevo, inesperado e interesante, el argumento-puñetazo que refuerce todos los demás creando en el que escucha una impresión final positiva y favorable.

Existen tres tipos de argumentos que pueden ser empleados en un discurso: los relativos al ethos, al pathos y al logos.

El orden de las partes puede ser "naturalis" o "artificialis". El "ordo naturalis" es el que respeta la propia naturaleza del discurso sin alteraciones intencionadas o el que sigue la tradición; el "ordo artificialis", por el contrario, altera el orden habitual de las partes (por ejemplo, empezar una historia no por el principio sino en un momento ya avanzado de la misma, esto es, "in medias res").

La "elocutio" afecta al modo de expresar verbalmente de manera adecuada los materiales de la "inventio" ordenados por la "dispositio". En la actualidad, la "elocutio" es lo que se denomina "estilo".

La "elocutio" se manifiesta a través de dos aspectos: las cualidades y los registros.

La "compositio" analiza la estructura sintáctica y fónica de los enunciados, esto es, sus constituyentes y sus distintas posibilidades de distribución en el discurso. Así, se distinguen la "compositio sintáctica" (centrada en la oración y sus partes) y la "compositio fonética" (centrada en la combinación de palabras en la oración por razones fonéticas).

La primera diferencia entre ambos es de tipo estructural y lógico-semántica: en el "periodo" existe una estructura periódica que presenta varias partes con autonomía argumentativa para cada una de ellas; en cambio, en el "estilo suelto" no existe esa estructuración, de forma que las ideas se suceden hasta llegar a la conclusión.

La segunda diferencia es de orden rítmico: en el "periodo" hay que tener en cuenta el "numerus" (el correlato en latín del metro en poesía, que se basaba en las cantidades vocálicas), mientras que en el "estilo suelto" esto es irrelevante.

La memorización del discurso elaborado depende de dos tipos de memoria según los tratadistas clásicos: la "memoria naturalis" (la innata) y la "memoria artificiosa", que implica una serie de procedimientos mnemotécnicos para facilitar el recuerdo.

También llamada "pronuntiatio", se ocupa de la declamación del discurso, prestando atención a la modulación de la voz y de los gestos, que debe estar en consonancia con el contenido del mismo.

Existen tres géneros de discursos de oratoria: el "genus iudiciale" (género judicial), el "genus deliberativum" (género deliberativo o forense) y el "genus demonstrativum" (género demostrativo o epidíctico).

Además de estos tres géneros, existen siete especies (εἲδη): la suasoria (προτρεπτικόν), disuasoria (ἀποτρεπτικόν), laudatoria (ἐγκωμιαστικόν), vituperadora (ψητικόν), acusatoria (κατηγορικόν), exculpatoria (ἀπολιγικόν) y la indagatoria (ἐξεταστικόν). Estas especies están presentes en los tres géneros. En el deliberativo, puesto que se busca convencer al auditoriο de una determinada tesis, las más frecuentes son la suasoria y la disuasoria. En el judicial, en el que hay que defenderse de acusaciones o realizarlas, predominan las especies acusatoria y exculpatoria y en el epidíctico, que sirve para reforzar los valores de una comunidad, la laudatoria y la vituperadora.
Aunque predοminen más en determinados discursos, las siete especies están en los tres géneros. En un discurso deliberativo se puede utilizar la especie acusatoria y la vituperadora, por ejemplo, el político que propone una ley puede acusar a su rival de algo o hacerle un vituperio con el fin de desacreditarlo. De la misma manera, en el discurso judicial son frecuentes las especies vituperadora y laudatoria. Un caso muy conocido es el discurso de Cicerón "Pro Archia Poeta" en el que hay un extenso elogio de la poesía.

En la Edad Media se añadieron a los anteriores las llamadas artes: "ars praedicandi" (sobre la técnica de elaborar sermones), "ars dictandi" (o "ars dictaminis", sobre el arte de escribir cartas) y las "ars poetriae" (preceptos gramaticales, métricos y retóricos para escribir poesía).

Podemos conocer la retórica ateniense a través de los discursos que dejaron grandes oradores como Demóstenes, Lisias o Isócrates. Heródoto y Tucídides en sus obras sobre historia, además de sucesos, también escribieron discursos pronunciados por personajes históricos como Alcibíades, Jerjes o Pericles.

Desde el punto de vista teórico las fuentes más importantes son la "Retórica a Alejandro" escrita por Anaxímenes de Lámpsaco y la "Retórica" de Aristóteles. La primera obra consiste en una serie de preceptos sobre como hablar elocuentemente. La segunda obra tiene un planteamiento más filosófico. Frente a la "Retórica a Alejandro" que es de carácter práctico, la "Retórica" de Aristóteles es de carácter teórico.

En la Atenas Clásica no existe una distinción clara entre la retórica y la filosofía. Por este motivo, hay que tener muy en cuenta esta última disciplina. La tragedia y la comedia, muy ligadas a lo político, son también importantes para conocer la retórica en la Atenas Clásica.

La retórica nació en la antigua Grecia alrededor del año 485 a. C. en la ciudad siciliana de Siracusa, cuando Gelón y su sucesor Hierón I, expropiaron las tierras a sus ciudadanos para adjudicárselas a miembros de su ejército personal. Más tarde, con la llegada de la democracia y el derrocamiento de los tiranos, los perjudicados pretendieron recuperar sus propiedades, y esta situación provocó una serie de pleitos en los que se manifestó la importancia de la elocuencia o arte de hablar bien y persuasivamente para conseguir las recuperaciones pretendidas. Así pues, su origen no está vinculado a lo literario sino a lo judicial, y estrechamente relacionado con lo político: la palabra pública y libre se relaciona con la retórica.
Ante la eficacia de la argumentación oral adecuada, Córax de Siracusa, en el siglo V a. C. (hacia el año 450) elaboró un sistema de comunicación para hablar ante la asamblea política o ante los tribunales con fines claramente persuasivos, que se puede considerar el primer tratado de retórica. Un discípulo suyo, Tisias, lo divulgó por Grecia. Así nacieron dos de los tres géneros clásicos de la retórica ya en su génesis: el judicial y el deliberativo. Y pronto se unió un tipo de discurso de elogio funerario en el que se trataba de alabar las virtudes del difunto, lo que se puede considerar el inicio del tercer género retórico, el demostrativo o epidíctico que, más adelante, se referiría a cualquier persona no necesariamente fallecida, o a diferentes aspectos de la vida o de la sociedad, desde un punto de vista positivo o negativo.

Las figuras de estos dos primeros maestros de retórica son bastante oscuras. Ningún escrito de ellos ha llegado hasta nuestros días, y se conoce su existencia por menciones de rétores posteriores. Hay una teoría que defiende que Tisias y Córax eran una sola persona y no dos. Según esta teoría, el primer rétor de la antigüedad se llamaría Tisias, el Corax o dicho de otra forma, Tisias el cuervo (κόραξ,κόρακος significaba en griego antiguo cuervo). Esa elocuencia vino a transformarse rápidamente en objeto de enseñanza, y se transmitió al Ática por comerciantes que comunicaban Siracusa y Atenas.

La retórica demostró pronto su utilidad como instrumento político en el régimen democrático, siglo V a. C., divulgada por profesores conocidos como sofistas, entre los cuales los más conocidos fueron Protágoras de Abdera y Gorgias. Para estos maestros de retórica que fueron también filósofos, no existe una única verdad y con el lenguaje sólo se pueden expresar cosas verosímiles (τὸ εἰκός). Valoraban mucho el poder que tenía la palabra (λὀγος) que según Gorgias es un gran soberano que con un cuerpo muy pequeño e imperceptible realiza obras de naturaleza divina.

Esta filosofía fue muy criticada por Platón. Tanto para Platón como para su maestro Sócrates, la esencia de la filosofía reposaba en la dialéctica: la razón y la discusión conducen poco a poco al descubrimiento de importantes verdades. Platón pensaba que los sofistas no se interesaban por la verdad, sino solamente por la manera de convencer, así que rechazó la palabra escrita y buscó la interlocución personal, y el método fundamental del discurso pedagógico que adoptó fue el del diálogo entre maestro y alumno.
Pero el gran maestro de la retórica griega fue Isócrates. Pensaba que la retórica era un plan de formación integral de la persona que servía para crear ciudadanos modélicos; con su sistema de enseñanza, precursor del humanismo, pretendía la regeneración ética y política de la sociedad ateniense.

Aristóteles, por otra parte, sistematizó la mayor parte de estos conocimientos sobre el arte de hablar y argumentar en una obra que consagró al efecto, su "Retórica". La gran aportación de la Retórica de Aristóteles es su enfoque filosófico. Los manuales anteriores, de los cuales el único ejemplar que se conserva es la Retórica a Alejandro, consistían en consejos prácticos sobre cómo persuadir. La Retórica de Aristóteles en cambio, realiza reflexiones teóricas sobre el lenguaje persuasivo.

Como Solón estableció que cada persona debía defenderse en persona ante un tribunal, llegaron a crearse los llamados "logógrafos", unos artesanos que se dedicaban a confeccionar discursos para quienes no sabían hacerlos a cambio de estipendio: autores como Antifonte, Lisias, que destacó por su naturalidad y aticismo, Iseo, famoso por su habilidad en la argumentación, y el más famoso de todos ellos, Isócrates, fueron logógrafos. Estos poseían también una preocupación estilística y procuraban que el estilo del discurso se ajustara a la personalidad y condición social de quien debía memorizarlo y pronunciarlo. También existía la figura del "sunégoros" (συνήγορος) cuya función era similar a la de un abogado. Demóstenes actuó como συνήγορος cuando pronunció su famoso discurso Sobre la Corona.

En los siglos V y IV a.C., el sistema político ateniense era la democracia radical que consistía en que todo ciudadano ateniense mayor de edad y varón podía exponer en la Asamblea (ἐκλεσία) sus puntos de vista sobre los asuntos de la polis. Para poder hablar en la Asamblea era necesario ser un orador excelente. Por este motivo se desarrolló en Atenas la retórica deliberativa.

El tercer género retórico que se desarrolló en Atenas fue el epidíctico que abarca los discursos que tienen lugar en ocasiones especiales, por ejemplo, en un funeral y cuyo principal objetivo es reforzar los valores de una comunidad. El discurso Epidíctico más importante de la Atenas Clásica es el Discurso Fúnebre de Pericles.

Ya en Roma la retórica se perfeccionó sumamente por medio de las investigaciones y esfuerzos de hombres de letras como Cicerón, quien dedicó al tema una parte sustancial de su obra e hizo de la retórica el eje de sus preocupaciones. De este autor son fundamentales los textos De Oratore y La invención retórica. De los manuales de retórica republicana se conserva la "Retorica ad Herennium" de autor anónimo. En época imperial, en el marco de la segunda sofística, se destaca Marco Fabio Quintiliano, cuyos doce libros de "Instituciones oratorias" suponen la culminación de los estudios sobre la materia en el mundo romano. Hay que notar que en la época republicana florece el género deliberativo, mientras que en la época imperial es el género epidíctico el que se desarrolla con mucha fuerza. Notable es el trabajo de Menandro el Rétor y sus Dos tratados de Retórica epidíctica. Dado que la retórica era parte fundamental de la eduación de todo romano ilustrado, es menester tenerla en cuenta a la hora de leer toda la producción intelectual de la época.

Durante la Edad Media, de los tres géneros oratorios, el judicial, el deliberativo y el epidíctico, entraron en decadencia el género deliberativo y el epidíctito, es decir, la oratoria política y la artística, ya que la militarización del imperio hacía inútil los conocimientos de la oratoria; sin embargo sus conocimientos fueron transvasados a la literatura en general, que se retorizó notablemente perdiendo bastante de su inspiración originaria y su frescura. Así lo vino a concluir el gran estudioso de la literatura medieval Ernst Robert Curtius en su "Literatura europea y Edad Media latina", traducido al castellano en 1955.

La retórica contemporánea ha prescindido del discurso oral y, por tanto, de entre las cinco fases de elaboración del discurso (invención, disposición, elocución, memoria y acción) de las dos últimas de índole práctica, la memoria y la acción. Se considera actualmente que es útil para actores, abogados, psicólogos, políticos, publicitarios, escritores, vendedores y, en general, quienes quieren persuadir o convencer de algo.

Sin embargo, la retórica ha vivido un gran renacimiento en la segunda mitad del siglo XX como disciplina científica con el surgir de varias corrientes de pensamiento que han redescubierto su valor para distintas disciplinas; comenzó Heinrich Lausberg realizando una gran labor de clasificación de la disciplina con sus "Elemente der literarischen Rhetorik", traducido como "Elementos de retórica literaria" en 1975; y su impagable "Manual de retórica literaria", publicado en español entre 1966 y 1970 en tres volúmenes; Chaïm Perelman y Lucie Ollbrechts-Tyteca publicaron en 1958 un fundamental "Tratado de la argumentación", traducido al castellano en 1994; la disciplina creada a raíz de este libro se denomina desde entonces Retórica de la argumentación o, a veces, Neorretórica; por otra parte, y al lado de esta llamada retórica de la argumentación, ha surgido una nueva neorretórica, la retórica contemporánea de las figuras, ilustrada por Roman Jakobson, el Grupo µ (o Grupo de Lieja), Lakoff y Johnson, etc. que permitió a la lingüística y a la semiótica desarrollarse en una orientación social y cognitivista. El estudio de la retórica como un fenómeno cultural ha sido profundamente renovado por el historiador francés de la cultura, Marc Fumaroli (Collège de France).

La invención, sola o conjuntamente con la disposición, es a menudo llamada argumentación; la elocución se subdivide, como habían determinado ya los teóricos de la antigüedad, en un gran número de puntos de vista sobre el discurso a hacer (arte de la retórica) o sobre el discurso ya hecho (retórica como ciencia): sobre el vocabulario (registros de la lengua), sobre los ritmos y las sonoridades, sobre la forma y la estructura de las frases (sintaxis, parataxis, hipotaxis, tipo de progresión remática, periodo, estilo comático, etc.).







</doc>
<doc id="2529" url="https://es.wikipedia.org/wiki?curid=2529" title="Rayuela">
Rayuela

Rayuela puede referirse a:


</doc>
<doc id="2532" url="https://es.wikipedia.org/wiki?curid=2532" title="Richard Stallman">
Richard Stallman

Richard Matthew Stallman (nacido en Manhattan, Nueva York, 16 de marzo de 1953), con frecuencia abreviado como «rms», es un programador estadounidense y fundador del movimiento por el software libre en el mundo.

Entre sus logros destacados como programador se incluye la realización del editor de texto GNU Emacs, el compilador GCC, y el depurador GDB, bajo la rúbrica del Proyecto GNU. Sin embargo, es principalmente conocido por el establecimiento de un marco de referencia moral, político y legal para el software libre, un modelo de desarrollo y distribución alternativo al software propietario. Es también inventor del concepto de copyleft (aunque no del término), un método para licenciar software de tal forma que su uso y modificación permanezcan siempre libres y queden en la comunidad de usuarios y desarrolladores.

Richard Matthew Stallman nació en la Ciudad de Nueva York en el año 1953, hijo de Alice Lippman y Daniel Stallman. Su primera experiencia con computadoras fue en el Centro Científico de IBM en Nueva York cuando cursaba la preparatoria. Fue contratado durante un verano para escribir un programa de análisis numérico en Fortran. Completó el trabajo después de un par de semanas, y dedicó el resto del verano escribiendo un editor de textos en el lenguaje de programación APL. Stallman invirtió el verano de su graduación de la preparatoria en escribir otro programa, un preprocesador para el lenguaje de programación PL/1 en el IBM S/360.

Durante este tiempo, Stallman fue también asistente voluntario de laboratorio en el departamento de biología de la Universidad Rockefeller. Aunque ya estaba ingresando a la carrera de física o matemáticas, su maestro tutor en el Rockefeller pensó que él podría ser biólogo en un futuro.

En 1971, siendo estudiante de primer año de Física en la Universidad Harvard, Stallman se convirtió en un "hacker" del Laboratorio de Inteligencia Artificial del Instituto Tecnológico de Massachusetts (MIT). En los años 1980, la cultura "hacker" que constituía la vida de Stallman empezó a disolverse bajo la presión de la comercialización en la industria del software. En particular, otros hackers del laboratorio de IA fundaron la compañía Symbolics, la cual intentaba activamente reemplazar el software libre del Laboratorio con su propio software privativo.

Entre 1982 y 1983, Stallman por sí solo duplicó los esfuerzos de los programadores de Symbolics para impedir que adquirieran un monopolio sobre los ordenadores del laboratorio. Por ese entonces, sin embargo, él era el último de su generación de hackers en el laboratorio. Se le pidió que firmara un acuerdo de no divulgación ("non-disclosure agreement") y que llevara a cabo otras acciones que él consideró traiciones a sus principios. El 27 de septiembre de 1983 Stallman anunció en varios grupos de noticias de Usenet el inicio del proyecto GNU, que perseguía crear un sistema operativo completamente libre.

Al anuncio inicial del proyecto GNU siguió, en 1985, la publicación del Manifiesto GNU, en el cual Stallman declaraba sus intenciones y motivaciones para crear una alternativa libre al sistema operativo Unix, al que denominó GNU ("GNU No es Unix"), pronunciado de forma parecida a "ñu", en inglés (de ahí los dibujos-logotipos que lo representan). Poco tiempo después fundó la organización sin ánimo de lucro Free Software Foundation para coordinar el esfuerzo. Inventó el concepto de copyleft, que fue utilizado en la Licencia Pública General GNU (conocida generalmente como la «GPL») en 1989. La mayor parte del sistema GNU, excepto el núcleo, se completó aproximadamente al mismo tiempo. En 1991, Linus Torvalds liberó el núcleo Linux bajo los términos de la GPL, completando un sistema GNU completo y operativo, el sistema operativo GNU/Linux.

Stallman insiste en la utilización del término «software libre», y no código abierto, porque lo importante es la libertad del usuario que se pierde. También busca que se utilice GNU/Linux y no solamente Linux cuando se habla del sistema operativo. Para Stallman el software libre no quiere decir gratis y considera que en la enseñanza no debería utilizarse software propietario.

Las motivaciones políticas y morales de Richard Stallman, lo han convertido en una figura controvertida. Muchos programadores influyentes que se encuentran de acuerdo con el concepto de compartir el código, difieren con las posturas morales, filosofía personal o el lenguaje que utiliza Stallman para describir sus posiciones. Un resultado de estas disputas condujo al establecimiento de una alternativa al movimiento del software libre, el movimiento de código abierto.

Stallman tiene una posición no remunerada como investigador en el MIT. Ha declarado ser "un ateo de ascendencia judía" y a menudo lleva un botón que dice "Acuse a Dios". Niega ser un anarquista a pesar de su cautela de algunas leyes y el hecho de que ha "defendido enérgicamente la privacidad de los usuarios y su propia visión de la libertad del software".

Stallman se refiere a los teléfonos móviles como "dispositivos portátiles de vigilancia y seguimiento" y dice que se niega a poseer un teléfono celular hasta que exista uno que se ejecute por completo en software libre. También evita el uso de una tarjeta-llave para entrar en su oficina, ya que estos sistemas llevan un seguimiento de cada lugar y hora en que alguien entra en el edificio. A excepción de unos pocos sitios web relacionados con su trabajo en GNU y la FSF, por lo general no navega por la web directamente desde su ordenador personal con el fin de evitar ser conectado con su historial de navegación. En su lugar, usa wget o programas similares que recuperan el contenido de los servidores web y luego envían el contenido a su correo electrónico.

Su imagen descuidada y sus manías extravagantes le han hecho ser blanco de numerosos chistes y bromas, llegando a aparecer en tiras cómicas.

En la actualidad, Stallman se encarga de difundir la ideología del Software Libre en todo el mundo mediante charlas y conferencias. Habla español de manera fluida y ha dado numerosas conferencias en países de habla hispana. Durante sus charlas aparece con una segunda personalidad que corresponde a "San Ignucio" con la que suele bendecir las computadoras de las personas como parte de una broma

Stallman ha escrito numerosos ensayos sobre la libertad del software y ha sido un orador político en favor del movimiento del software libre desde principios de los 1990s. Los discursos que ha dado regularmente se titulan El Proyecto GNU y el movimiento del software libre, Los peligros de las patentes de software, Copyright y globalización en la era de las redes informáticas.

La firme defensa de Stallman por el software libre inspiró la creación del "Virtual Richard M. Stallman" (vrms), un software que analiza los paquetes instalados en un sistema Debian GNU/Linux, e informa de aquellos que no son libres. Stallman no está de acuerdo con parte de la definición de software libre de este proyecto.

En 1999 promovió la creación de una enciclopedia libre, la GNUPedia, considerada como un antecedente directo de Wikipedia. El proyecto resultante se retiró finalmente en favor de esta última, que tenía objetivos similares y estaba contando con un éxito mayor.

Stallman es un viajero del mundo y ha visitado al menos 65 países, sobre todo para hablar sobre el software libre y el proyecto GNU. Según Stallman, el movimiento del software libre tiene mucho en común con aquel liderado por Mahatma Gandhi.

En abril de 2006, Stallman llevó un cartel de protesta contra el software propietario -"No compre ATI, enemigo de su libertad"- en un discurso pronunciado por el representante de esa firma, dando lugar a un llamado a la policía. ATI se fusionó luego con AMD Corporation y tomó medidas para que su documentación de hardware esté disponible para uso por la comunidad de software libre.

En agosto de 2006, en reuniones con el gobierno del estado indio de Kerala, convenció a los funcionarios de desprenderse del software propietario en escuelas estatales. Esto dio lugar a la decisión histórica de migrar todas las computadoras en 12.500 escuelas secundarias de Windows a un sistema operativo de software libre.

En Venezuela, Stallman ha pronunciado discursos públicos y promovido la adopción de software libre en la petrolera estatal PDVSA, el gobierno municipal y el ejército de la nación. En reuniones con Hugo Chávez y en discursos públicos, Stallman criticó algunas políticas sobre la radiodifusión televisiva, derechos de libertad de expresión y privacidad. Stallman estuvo en el Consejo Asesor de la cadena de televisión Telesur desde su lanzamiento, pero renunció en febrero de 2011 criticando la propaganda pro-Gadafi durante la primavera árabe.

Luego de reuniones personales, Stallman obtuvo declaraciones en favor del movimiento del software libre del presidente de la India, Dr. APJ Abdul Kalam, la candidata presidencial francesa en 2007 Ségolène Royal, y el entonces presidente de Ecuador Rafael Correa.

El 30 de noviembre de 2012, Stallman dio la conferencia de apertura en el Foro Goiano Software Libre en Brasil, hablando de casos exitosos de migración a software libre en el gobierno, los negocios y las universidades.

Stallman ha participado en protestas contra el uso de patentes de software, DRMs, y software propietario. Linus Torvalds ha criticado a Stallman por lo que considera un pensamiento "de blanco o negro".

Stallman ha brindado regularmente una conferencia titulada "Derechos de autor "versus" comunidad", donde revisa el estado de los DRMs y se pronuncia en contra de ciertos productos y empresas que los utilizan. Su visión sobre los DRMs se resume en la campaña de la FSF Defectuoso por Diseño. En sus charlas se manifiesta en favor de un "copyright reducido" y sugiere un límite de 10 años para los derechos de autor. Sugiere que, en lugar de las restricciones al intercambio, se apoye a los autores utilizando un impuesto y se distribuyan los ingresos sobre la base de las raíces cúbicas de su popularidad. Esto aseguraría que "los exitosos no sean estrellas" y reciban, sin embargo, una recompensa mayor comparada con el gravamen por copia privada asociada a los defensores del copyright. Otra opción sería un sistema de micropago anónimo para que la gente apoye de manera directa a los autores.

Stallman se pronuncia por que ninguna forma de intercambio no-comercial de copias sea considerada una violación a los derechos de autor. Ha abogado por la desobediencia civil en un comentario sobre la Ley Sinde.

También ha apoyado al Proyecto Biblioteca Internacional de Partituras Musicales, para volver a estar en línea luego de haber sido retirado el 19 de octubre de 2007, tras una orden de cese y desista de Universal Edition.

Stallman destaca los peligros que algunos e-libros tienen en comparación con los libros de papel. Un ejemplo es el lector electrónico Kindle de Amazon, que impide la copia de libros electrónicos y permite a la empresa la eliminación automática de contenido. Considera que tales desarrollos presentan un gran paso atrás con respecto a los libros de papel, por ser menos fáciles de usar, copiar, prestar a los demás o vender, además de que los Kindle no se pueden comprar de forma anónima. Su cuento "El derecho a leer" ofrece una imagen de un futuro distópico en el que se impide el derecho a compartir los libros. Se opone a muchos de los términos dentro de los acuerdos de licencia de usuario final que acompañan a tales dispositivos.

Stallman desalienta el uso de varias tecnologías de almacenamiento, como discos de vídeo DVD o Blu-ray debido a que su contenido está cifrado. Considera el uso de cifrado de datos por parte de los fabricantes (para obligar al usuario a ver cierto material promocional) como una conspiración.

Reconoció el escándalo de protección de copia rootkit de Sony BMG como un acto criminal. Stallman apoya un boicot a Sony por sus acciones legales contra George Hotz.

Stallman ha recibido numerosos premios y reconocimientos por su trabajo, entre ellos:







Artículos en revistas científicas:

Manuales:

Recopilaciones de ensayos:

Biografía:




</doc>
<doc id="2536" url="https://es.wikipedia.org/wiki?curid=2536" title="Rosa">
Rosa

El género Rosa está compuesto por un conocido grupo de arbustos espinosos y floridos representantes principales de la familia de las rosáceas. Se denomina rosa a la flor de los miembros de este género y rosal a la planta.

El número de especies ronda las 100, la mayoría originarias de Asia y un reducido número nativas de Europa, Norteamérica y África noroccidental. Tanto especies como cultivares e híbridos se cultivan como ornamentales por la belleza y fragancia de su flor; pero también para la extracción de aceite esencial, utilizado en perfumería y cosmética, usos medicinales (fitoterapia) y gastronómicos.

Existe una enorme variedad de (más de 30 000) a partir de diversas hibridaciones, y cada año aparecen otros nuevos. Las especies progenitoras mayormente implicadas en los cultivares son: "Rosa moschata", "Rosa gallica", "Rosa damascena", "Rosa wichuraiana", "Rosa californica" y "Rosa rugosa". Los cultivadores de rosas o del siglo XX se centraron en el tamaño y el color, para producir flores grandes y atractivas, aunque con poco o ningún aroma. Muchas rosas silvestres y «pasadas de moda», por el contrario, tienen una fragancia dulce y fuerte.

Las rosas están entre las flores más comunes vendidas por los floristas. El rosal es una de las plantas más populares de los jardines, incluso existen jardines específicos llamados rosaledas o rosedales, donde se exponen únicamente los miembros del género, cuya variedad es tan extensa que comprende desde rosales miniatura de 10 o 15 cm de altura, hasta grandes arbustos, trepadores que alcanzan varios metros de altura o rastreros utilizados como cubre suelos.

En español —y en otras lenguas romances también—, el término «rosa» proviene directamente y sin cambios del latín "rosa", con el significado que conocemos: «la rosa» o «la flor del rosal»; devenido del vocablo previo "rodia" [ródja] —por cambio similar como en: Clausus por Claudius—. Este último arcaísmo latino es, a su vez, prestado —a través del osco— del griego antiguo "ρόδον" [rhódon] «la rosa», «la flor del rosal» o mejor "rhodéa", «el tallo de la rosa», «el sostén de la flor».

A partir del griego antiguo se alude al posible significado de "rhódon" como «efluvio oloroso», «lo que es fragante», o «lo que desprende olor»; originado como término compuesto: por "ροήdon" o sino también de "wrodion" [bródion] en el antiguo dialecto eólico, raíces correspondientes con el persa antiguo "vereda" o "v'reda" (y sus dialectos: avéstico "warda", sogdiano "ward" y parto "wâr"), como una voz irania traspasada desde el sur de Armenia a Frigia y de ahí a Grecia. Y previamente de un origen tan antiguo como el arameo "wurrdā" y hasta del asirio "wurtinnu".

En cuanto a la base, el núcleo deriva de una raíz indoeuropea "vardh-" [wardh], "vradh-" [wradh], «crecer», «erguir(se)»; donde en sánscrito "wardh-as", significa «germinante», y wardhati, «elevar(se)», «prosperar».

Por otra parte, puede ser un derivado de una raíz grecolatina "vrad"-, «plegarse», «hacerse flexible». Y por ahí también del griego "rodanós", "rádinos", y el eólico "bradinós", «blando» o «flexible». Color claro.

"Rosa" también es un término coincidente con varios nombres germánicos que tienen la raíz "hrod", con el significado de «gloria».

Los rosales son arbustos o trepadoras (a veces colgantes), generalmente espinosos, que alcanzan de 2 a 5 metros de altura, en ocasiones, pueden llegar a los 20 m trepando sobre otras plantas.

Tienen tallos semileñosos, casi siempre erectos (a veces rastreros), algunos de textura rugosa y escamosa. Presentan notables formaciones epidérmicas persistentes, bien desarrolladas y de formas variadas, conocidas como espinas o aguijones.
Las hojas pueden ser perennes o caducas, pecioladas e imparipinnadas con 5 a 9 folíolos de borde aserrado y estípulas basales. Es frecuente la presencia de glándulas anexas, odoríferas o no, sobre los márgenes.

Las flores, generalmente aromáticas, se agrupan en inflorescencias racimosas, formando corimbos. Son flores completas, hermafroditas, regulares, con simetría radial (actinomorfas). El perianto está bien desarrollado. El hipanto o receptáculo floral prominente en forma de urna (tálamo cóncavo y profundo). El cáliz es dialisépalo, de 5 piezas de color verde. Los sépalos pueden ser simples o, a veces, de forma compleja con lobulaciones laterales estilizadas.
Corola dialipétala, simétrica, formada por 5 pétalos regulares (o múltiplos de 5), a veces escotados, y de variados colores llamativos o sólo blancos. La corola suele ser "doble" o "plena" por transformación de los estambres en pétalos, esto ocurre mayoritariamente en cultivares.

El androceo está compuesto por numerosos estambres dispuestos en espiral (varios verticilos), generalmente en número múltiplo de los pétalos (5x).
El gineceo apocárpico (compuesto por varios pistilos separados). Nectario presente, que atrae insectos para favorecer la polinización predominantemente entomófila. Perigina (ovario medio), numerosos carpelos uniovulados (un primordio seminal por cada carpelo), así cada carpelo produce un aquenio.

El fruto es conocido como escaramujo, que corresponde a un tipo de infrutescencia denominada cinorrodón. Está compuesto por múltiples frutos secos pequeños o aquenios (poliaquenio), separados y encerrados en un receptáculo carnoso (hipantio) y de color rojizo vistoso cuando está maduro.

El aceite esencial de "Rosa damascena" se compone de terpenos y derivados de ácidos grasos, tales como citronelol (30.31 %), geraniol (16.96 %), alcohol fenetílico (12.60 %), nerol (8.46%), hexacosano (3.70 %), nonadecano (2.7 %), linalol (2.15 %), β-Ionona (1.00 %), eicosano (1.65 %), docosano, (1.27 %), farnesol (1.36%), acetato de nerilo (1.41 %), propionato de citronelilo (1.38 %), geranial (1.35 %), α-pineno (0.60 %), mirceno (0.46 %), óxido "cis" rosa (0.55 %), decanal (0.51 %), terpinen-4-ol (0.55 %), β-cariofileno (0.81 %), isoborneol (0.57 %), y heptadecano (0.92 %).

El fruto del rosal, el escaramujo, tiene un alto contenido en Vitamina C: entre 1700-2000 mg por cada 100 g de producto seco, lo que lo convierte en una de las fuentes vegetales más ricas de esta vitamina. También contiene vitaminas A, D y E, y flavonoides antioxidantes. El alto contenido de taninos es causante de estreñimiento.

Desde el punto de vista de la práctica de la jardinería, y esquemáticamente, los rosales se clasifican en 4 grupos:

Algunas de las especies silvestres más representativas del género "Rosa":




Los rosales florecen continuamente durante todo el año desde primavera hasta principios de invierno (o más en climas cálidos). Para que esto ocurra hay que cortar las rosas marchitas. Una técnica popular consiste en seguir el tallo de la rosa seca hasta encontrar la primera rama con cinco hojas y cortar inmediatamente por encima de ella. Luego, entrado el invierno, se hace la poda radical, dejando nada más que cuatro o cinco ramas de un palmo desde el tronco principal. También se puede hacer media poda en medio de la temporada para mantener el rosal en un tamaño mediano. Esta no es necesaria para la salud de la planta ni para que florezca más.

Los cortes deben hacerse con tijera bien afilada para que resulten limpios, es decir, sin "picotazos". Deben ser sesgados, evitando los cortes rectos y no se deberán dejar fibras en ellos. Se debe cortar medio centímetro por encima de la yema exterior, en forma sesgada hacia adentro (inclinada) para que cuando llueva o se riegue la planta el agua corra y no se concentre en la yema perjudicando el crecimiento floral.

Al rosal de pie se le deberá dar forma de copa de vino para permitir un buen acceso a la luz a toda la planta.

Las rosas deben podarse cuando terminan de brotar las hojas.

Los rosales se pueden reproducir en el otoño de cuatro formas:


La mayoría suelen ser comunes a otras plantas de jardín y están en relación a la zona geográfica.

Algunas de ellas son:
Una solución para la plaga seria emplear clorpirifós como insecticida.


Evidentemente, ya desde la antigüedad, el cultivo de rosales estaba muy difundido, ya sea como plantas ornamentales como también para provecho de sus propiedades medicinales y aromáticas (perfumería y cosmética).

Los primeros datos de su utilización ornamental se remontan a Creta (siglo XVII a. C.). La rosa era considerada como símbolo de belleza por babilonios, sirios, egipcios, romanos y griegos. En Egipto y Grecia tuvo una especial relevancia, y mucho más en Roma. Los romanos cultivaron la rosa intensamente, siendo utilizados sus pétalos para ornamento, así como la planta en los jardines en una zona denominada Rosetum. Tras la Edad Media, donde su cultivo se restringió a Monasterios, vuelve a surgir la pasión por el cultivo del Rosal. Un ejemplo de esta pasión fue la emperatriz Josefina que a partir de 1802 en su Palacio de la Malmaison llegó a poseer una colección de 650 rosales. Las colecciones de rosas se han multiplicado desde entonces.

A fines de 1700, fue introducida en Europa, "R. semperflorens", conocida como "Rosa de Bengala", con flores pequeñas agrupadas. Para el comienzo de 1800, fue introducida en Europa, "R. indica var. fragans", conocida con el nombre de Rosa de Té, originaria de la China (conocida también como "R. chinensis").

La era moderna de las rosas se inicia a partir de 1867 con la creación del primer ejemplar híbrido de té por el productor francés Guillot, quien la llamó: «La France». El invento surgió por casualidad, cuando Guillot estaba intentando mejorar una rosa naranja. El resultado fue una flor muy olorosa y con una larga floración, distinta en tamaño y características a las rosas que había hasta entonces. La rosa de té original, anterior a la creación de los híbridos que sucedieron a la invención de Guillot de Francia, era más pequeña, casi sin olor y se producía en una escasa paleta cromática: blanco, rosa y rojo.

Durante el siglo XIX empiezan a llegar variedades del extremo oriente, donde su cultivo fue también muy relevante por los antiguos jardineros chinos (existen datos del cultivo de rosales 3000 a. C.).

En el catolicismo la rosa es un componente simbólico del Santo Rosario; Se dice que el Beato Angelico mientras rezaba el rosario en la calle vio a la Santísima Virgen con un grupo de ángeles que están ofreciendo canciones y alabanzas al componer una corona de rosas. Sorprendido con la visión interrumpió la oración y los ángeles se detuvieron; empezando a rezar nuevamente vio a los ángeles componer de la corona de rosas para ofrecer a María.

La rosa ha sido celebradísima en todo tiempo por los poetas y prestado materia a las mitologías y leyendas desde Salomón que veía una rosa en la esposa del "Cantar de los cantares", Safo y Anacreonte hasta la delicada comparación de Malherbe:

En la "Novela de la rosa", esta es el premio del amor y del valor. En "El asno de oro" de Apuleyo, el borrico se vuelve hombre al comer rosas y los poetas han representado a porfía a la Aurora como una joven que esparce rosas. En la mitología indiana, la rosa representa ya el Sol, ya la Aurora, ya el Crespúsculo vespertino.

Una de las tres gracias en Grecia llevaba una rosa en la mano y se decía que la rosa había brotado del pie de Venus al salir algunas gotas de sangre de una picadura que se había causado con una espina. La fábula decía también que la rosa era al principio blanca y se había vuelto encarnada al teñirse con la sangre de Adonis (alusión al paso de la luz blanca "alba" a la luz rosada "aurora"). De igual manera que a Venus y Flora, cuyas estatuas se adornaban con guirnaldas de rosas, pertenecía esta flor a Baco y en uno de sus ditirambos invita Píndaro a coronarse de rosas en honor a Dionisos. Muchos pueblos eslavos denominan a la fiesta de la primavera "rusdija" o "fiesta de las rosas".

En algunas leyendas italianas, la rosa es símbolo de virginidad. Contrariamente, las cortesanas de Roma celebraban su fiesta el día 23 de abril consagrado a Venus Ericina y se mostraban adornadas de rosas y mirtos; en el día de San Jorge en Barcelona, también es costumbre regalar rosas y libros. En los grandes banquetes romanos, los convidados iban coronados de rosas, creyéndose que preservaban de la embriaguez. En otros países, la rosa es un símbolo funerario y de ahí, según algunos, que se planten cipreses y rosales en los cementerios.
Las rosas son símbolos antiguos del amor y de la belleza. La rosa era sagrada para un número considerable de diosas (deidades femeninas) de la antigüedad, y se utiliza a menudo como símbolo de la Virgen María. Las rosas son tan importantes que de ellas derivan términos como color rosa o rojo en una considerable variedad de idiomas.

Las rosas vienen en una variedad de colores, cada uno con un diverso significado simbólico:

Además hay ciertas expresiones:
La rosa también es el símbolo de dos dinastías reales inglesas: la Casa de Lancaster (rosa roja) y la Casa de York (rosa blanca) que se vieron enfrentadas en la conocida como Guerra de las Dos Rosas.

También es el emblema de la Selección de rugby de Inglaterra, que es conocida como «el XV de la rosa».

La rosa roja (generalmente asida con el puño izquierdo) es el símbolo del Socialismo democrático, en recuerdo de Rosa de Luxemburgo, pensadora y mártir del pensamiento socialista. Es empleada por la mayoría de colectivos de esta ideología, como el Partido Socialista Obrero Español.

Su principal productor y exportador es Ecuador. La situación geográfica del país permite contar con microclimas y una luminosidad que proporciona características únicas a las flores como son: tallos largos, gruesos y totalmente verticales, botones grandes y colores vivos. Sus principales mercados: Estados Unidos, Holanda (importa flores para luego re-exportarlas a otros países de la Unión Europea), Italia, Alemania, Rusia, Canadá, Argentina, España, Francia, Suiza y Ucrania. También Chile, China y Brasil. La superficie total de plantaciones es de 3300 ha, con una disponibilidad de 85 000 toneladas por año. El 98 % de la producción se exporta.



</doc>
<doc id="2539" url="https://es.wikipedia.org/wiki?curid=2539" title="Reserva nacional Tambopata">
Reserva nacional Tambopata

La Reserva Nacional Tambopata es un área natural protegida del Perú, ubicada en el departamento de Madre de Dios, provincia de Tambopata y se extiende en los distritos de Tambopata e Inambari. La Reserva Nacional Tambopata fue creada el 4 de septiembre de 2000, mediante el decreto supremo DS Nº 048-2000-AG, con una extensión de 274,690 ha, esta área natural protegida cuenta con una gradiente altitudinal promedio de 300msnm en un rango de 200-400msnm. Dentro de sus objetivos de creación de la reserva se han establecido 3 ejes de acción: primero proteger a la flora, la fauna y los procesos ecológicos de una muestra de la selva sur amazónica del Perú; segundo generar procesos de conservación con la población en el ámbito de la reserva, con la finalidad de usar sosteniblemente los recursos como los castañales y el paisaje para la recreación y tercero contribuir al desarrollo sostenible de la región y del país, a partir del conocimiento de la diversidad biológica y del manejo de los diversos recursos naturales renovables. 

El clima de la zona es del tipo bosque subtropical húmedo o muy húmedo, en donde la temperatura media es de 26°C, llegando a fluctuar entre los 10°C y 38°C. Estos límites inferiores se explican por los vientos antárticos que ingresan irregularmente a la cuenca del Amazonas provenientes desde los andes generalmente en las temporadas de vientos fríos en junio y julio. En el caso de sus temperaturas máximas, estas se registran durante los meses de septiembre a octubre. 

Las precipitaciones en la zona están en el rango de 1600 a 2400mm anual. Los registros de las precipitaciones a nivel mensual varían de acuerdo a las temporadas, en donde la temporada de máxima precipitación es entre diciembre y marzo, a este periodo de altas lluvias tenemos los meses de transición que son octubre, noviembre, abril y mayo, finalmente la temporada de baja precipitación entre los meses de junio a septiembre.

La Reserva Nacional Tambopata tiene como principales cuencas a los ríos Tambopata y Heath, en la reserva también podemos encontrar a los ríos Azul y Malinowsquillo los cuáles desembocan en la margen derecha del río Malinowski.

El río Tambopata nace en el altiplano peruano boliviano, cuenta con una extensión de 402Km y su principal afluente es el Malinowski. Este río atraviesa la Reserva Nacional Tambopata de este a oeste, siendo este uno de los principales accesos a los atractivos de la reserva.

El río Heath nace en los andes en la región de Puno y tiene su desembocadura en el río Madre de Dios con un recorrido de 200 km. A lo largo de su recorrido cuenta con los afluentes Bravo y Wiener.

El río Malinowski o también conocido como río Carama, nace a las afueras de la reserva, en la comunidad nativa de Kotsimba en el distrito de Inambari, sus afluentes son los ríos Pamahuaca, Azul, Malinowsquillo y Agua Negra por la margen derecha y el río Manuani por la margen izquierda.

La cuenca del río Tambopata presenta uno de los mayores índices de diversidad biológica en el mundo. La Reserva Nacional Tambopata se ubica en la zona media y baja de esta cuenca, vecina a la ciudad de Puerto Maldonado. Entre sus ecosistemas más comunes se encuentran los aguajales, los pantanos, los pacales y los bosques ribereños, cuyas características físicas permiten a los pobladores locales el aprovechamiento de los recursos naturales. La Reserva Nacional Tambopata alberga hábitats principalmente acuáticos que son usados como paraderos de más de 40 especies de aves migratorias transcontinentales. En la reserva nacional se protegen importantes especies consideradas en vías de extinción y le ofrece al turismo un destino privilegiado para la observación de la diversidad de flora y fauna silvestre.

En el Diagnóstico del Proceso de Elaboración del Plan Maestro de la Reserva Nacional Tambopata 2011-2016 se reportaron 1 713 especies, pertenecientes a 654 géneros de 145 familias. Para las angiospermas (plantas con flores) se consideró la clasificación propuesta por el Angyosperm Phylogeny Group (APG III) y para los pteridofitos (helechos) se consideró la clasificación propuesta por Smith et al. (2006). Las angiospermas registran 1 637 especies agrupadas en 127 familias y 622 géneros, siendo las familias más diversas Fabaceae (158 especies), Rubiaceae (104 especies) y Moraceae (66 especies). Los pteridofitos registran 76 especies de 32 géneros y 18 familias, siendo las familias más diversas: Polypodiaceae (16 especies), Pteridaceae (11 especies) y Thelypteridaceae (9 especies). Es así que se pueden apreciar en llanuras de sedimentación a los aguajales ("Mauritia flexuosa"), así como otras especies con valor comercial como la caoba ("Swietenia macrophylla"), tornillo ("Cedrelinga catenaeformis"), cedro ("Cedrela odorata"), lupuna (Ceiba spp.), shiringa ("Hevea brasilensis"), caucho ("Castilla elastica") y la castaña ("Bertholletia excelsa"), de esta última especie hay que resaltar que es el recurso forestal no maderable con mayor potencial económico desarrollado en la Reserva Nacional Tambopata, el cual se aprovecha bajo planes de manejo aprobados y controlados por el Servicio Nacional de Áreas Naturales Protegidas por el Estado (SERNANP). 
En la Reserva Nacional Tambopata se ha reportado la presencia de más de 632 especies de aves, 1,200 de mariposas, 103 de anfibios, 180 de peces, 169 de mamíferos y 103 de reptiles. En su interior se encuentra hábitats saludables para la recuperación y refugio de poblaciones amenazadas de especies como el lobo de río ("Pteronura brasiliensis"), la nutria ("Lontra longicaudis") y felinos como el yaguarundi ("Herpailurus yagouaroundi"), el puma ("Puma concolor"), el jaguar ("Panthera onca"), el ocelote o tigrillo ("Leopardus pardalis") y el margay ("Leopardus wiedii").

Entre las especies de primates se encuentra el maquisapa ("Ateles chamek"), el pichico ("Saguinus fuscicollis"), el pichico emperador ("Saguinus imperator"), el coto mono ("Alouatta seniculus"), el mono cabecinegro ("Aotus nigriceps"), el mono choro ("Lagothrix lagotricha"), el fraile ("Saimiri boliviensis"), el mono ardilla ("Saimiri sciureus"), el machín blanco ("Cebus albifrons") y el machín negro ("Cebus apella").

En cuanto a las aves destaca la presencia del águila harpía ("Harpia harpyja"), del águila crestada ("Morphnus guianensis"), del paujil común ("Mitu tuberosa"), del paujil unicornio ("Pauxi unicornis") y del paujil carunculado ("Crax globulosa"). En la Reserva Nacional Tambopata se encuentra casi la totalidad de especies de guacamayos ( Ara spp.)que habitan en el Perú.

Otras especies comunes son los reptiles: boa esmeralda ("Corallus caninus"), el loro machaco (""), la boa constrictora ("Boa constrictor") y la shushupe ("Lachesis muta"). También es común observar al caimán negro ("Melanosuchus niger"), al caimán blanco ("Caiman crocodylus") y a la taricaya ("Podocnemis unifilis").

Los peces también presentan una gran variedad, entre ellos destaca el boquichico ("Prochilodus nigricans"), el zúngaro saltón ("Brachyplatystoma filamentosum"), el yahuarachi ("Potamorhyna latior"), el dorado ("") y el paco ("Piaractus brachipomun"). Entre los peces no comerciales están el sábalo (Brycon spp.), la lisa ("Schizodon fasciatus") y el bagre (Pimelodus sp.).

La Reserva Nacional Tambopata es uno de los principales destinos dentro del Sistema Nacional de Áreas Naturales Protegidas por el Estado (SINANPE). Debido a su gran biodiversidad y hábitats naturales protegidos, esta reserva es un sitio privilegiado para el contacto con la naturaleza en lo que respecta a flora, fauna y paisajes.

El destino turístico más visitado es el lago Sandoval, este es calificado como un espejo de agua de 127 ha, en donde abundan poblaciones de guacamayos en la vegetación que rodea al lago, además de permitir el avistamiento de garzas, martín pescador, caimanes y nutrias. Esta zona turística cuenta con alojamientos para los visitantes.

A través de la cuenca del río Tambopata hay acceso a los lagos Cocococha y Sachavacayoc, ambos son puntos en donde abunda la vida la vida silvestre. En Sachavacayoc hay una zona para campamento para que los turistas puedan pasar la noche. Además de los lagos también se tienen las Collpas, estos son lugares en donde los animales acuden a ingerir arcilla de los barrancos de los ríos. Entre 5:30 y 9:00 am se genera una aglomeración de guacamayos y loros que forman un espectáculo de color y vida silvestre para los turistas. Las principales collpas son Chuncho y Colorado, ambas están en las márgenes del río Tambopata.



</doc>
<doc id="2540" url="https://es.wikipedia.org/wiki?curid=2540" title="Reino">
Reino

Reino puede referirse a:


</doc>
<doc id="2541" url="https://es.wikipedia.org/wiki?curid=2541" title="Reino de Dios">
Reino de Dios

El Reino de Dios (o en griego βασιλεία τοῦ θεοῦ "basileia tou theou") es un concepto importante en el judaísmo, el cristianismo y el islamismo. Se refiere al reinado o soberanía de Dios que está sobre todas las cosas, y no es parecido al reinado de los poderes terrenales.

El Reino de Dios es mencionado frecuentemente en el Tanaj. Está unido al entendimiento judío de que Dios habría de intervenir directamente para restaurar la nacionalidad de Israel y luego reinar sobre ella.
Luego fue interpretado como que de la descendencia de David saldría el Mesías de Israel, que se sentaría en el trono de David y gobernaría por la eternidad. Por lo tanto los judíos esperan la intervención divina, en lo político y en lo espiritual.

El Reino de Dios fue expresamente prometido al Rey David, haciéndose un pacto entre él y Dios y prometiéndole que reinaría siempre alguien en el trono de su «casa» — la de David —.

El Catecismo de la Iglesia Católica indica que en el Nuevo Testamento se utilizan varias expresiones para caracterizar la bienaventuranza a la que Dios llama al hombre: la llegada del Reino de Dios; "«Dichosos los limpios de corazón porque ellos verán a Dios» —Mt 5, 8—; «la entrada en el gozo del Señor» — Mt 25; 21.23 —; «la entrada en el descanso de Dios» — Hb 4, 7-11—.
La idea del Reino de Dios se encuentra predominantemente en el Nuevo Testamento, especialmente en los Evangelios.

El Reino de Dios es un término usado indistintamente con el de «Reino de los Cielos». En el Evangelio según Mateo se utiliza esta última expresión, mientras que en el Lucas, en el de Marcos y en el de Juan se utiliza «Reino de Dios». La explicación habitual es que el evangelio de Mateo está destinado a los judíos quienes prefieren evitar el uso directo del nombre de Dios. Marcos y Lucas están dirigidos a una audiencia más general y menos familiarizada con el término «Reino de los Cielos».

Algunos intérpretes premilenaristas piensan que el «Reino de los Cielos» se refiere al reino milenario de Dios, mientras que el «Reino de Dios» se refiere a su reinado universal. Otros opinan que no hay base para tal distinción.

El historiador, escritor y filósofo británico H. G. Wells escribió:

El pensamiento cristiano del Reino de Dios agrupa distintos conceptos según el entendimiento de cada denominación, entre las que destacan las siguientes.

Los evangelios describen a Jesús de Nazaret proclamando el Reino como algo que ya está cerca, que está llegando en el presente, no como una realidad futura. Las actividades narradas de Jesús, al sanar enfermedades, expulsar demonios, enseñar una nueva ética de vida y ofrecer una nueva esperanza en Dios al más pobre, se entienden como una demostración que el Reino está en acción. Tener al Mesías, el Rey de los judíos, entre ellos, es un aspecto de este Reino: el Rey había llegado para representar su Reino. Por su vida sin pecado y mediante sus milagros estaba demostrando a los judíos como era el Reino.

"El Reino de Dios" es un genitivo, el cual nos indica que es Dios mismo desde un punto de vista concreto, su actuación en este mundo y en nuestra historia. La cuestión planteada a los contemporáneos de Jesús (especialmente a los imbuidos en la mentalidad apocalíptica) es si Dios actúa en este mundo y en esta historia, o no; y si actúa, cuándo lo hace o lo va a hacer y bajo qué condiciones. Jesús nos predica que esto es inminente, y que la esperada acción de Dios en este mundo empieza ya.

Jesús dio mucha importancia a este tema, como se puede ver en el Padrenuestro, donde es el segundo asunto más importante en esa oración.

El Reino de Dios también se refiere al cambio de corazón o mente (metanoia) por parte de los cristianos, dando énfasis a la naturaleza espiritual de su Reino al decir que "«el Reino de los Cielos está dentro de vosotros mismos»". Esta frase puede también traducirse, sin embargo, "«el reino de los cielos está en medio de vosotros.»"

Jesús usó el lenguaje del "Reino de Dios" de una forma que se contrapone con los revolucionarios judíos del siglo I, llamados zelotes, que creían que el Reino era una realidad política que llegaría con una revuelta violenta contra la dominación romana, reemplazada por una teocracia judía.

En la los Evangelios canónicos, Jesús de Nazaret invita a todos los hombres a entrar en el Reino de Dios; aun el peor de los pecadores es llamado a convertirse y aceptar la infinita misericordia del Padre. El Reino pertenece, ya aquí en la tierra, a quienes lo acogen con corazón humilde. A ellos les son revelados los misterios del Reino de Dios. La Iglesia (católica) se considera a sí misma como "el inicio sobre la tierra" del Reino de Dios y que la plenitud de éste se alcanzará después del juicio final, cuando el universo entero, liberado de la esclavitud de la corrupción, participará de la gloria de Cristo, inaugurando «los nuevos cielos y la tierra nueva» (2 P 3, 13). Así se alcanzará el Reino de Dios pleno, es decir, la realización definitiva del designio salvífico de Dios de «hacer que todo tenga a Cristo por Cabeza, lo que está en los cielos y lo que está en la tierra» (Ef 1, 10). Dios será entonces «todo en todos» (1 Co 15, 28), en la vida eterna.

Los protestantes, por otra parte, tienden a creer que la Iglesia es el instrumento en el cual el Reino se manifiesta, no un sinónimo del Reino en sí.

Según el teólogo protestante Dietrich Bonhoeffer el Reino de Dios en la tierra se configura en dos aspectos, en los que se manifiesta escindido: milagro y orden. «El aspecto bajo el cual el Reino de Dios se manifiesta como milagro lo llamamos iglesia; y el aspecto bajo el cual el Reino de Dios se manifiesta como orden lo llamamos estado. El Reino de Dios en nuestro mundo no es otra cosa que la dualidad de iglesia y estado… El Reino de Dios se configura en la iglesia en la medida en que ésta da testimonio del milagro de Dios… El Reino de Dios se configura en el estado en la medida en que éste reconoce y preserva el orden del mantenimiento de la vida…».

La manifestación presente del Reino fue expresada por Jesús como evidencia provisional de una realidad más amplia en un futuro inminente.

Este aspecto futuro del Reino es la creencia en una implementación post-apocalíptica del gobierno de Dios, (teocracia), especialmente en la interpretación premilenarista del protestantismo fundamentalista.

La tensión entre los aspectos futuros y presentes del Reino se han llamado "el ahora y el no todavía" del Reino de Dios. 

Típicamente, en el Catolicismo, el protestantismo liberal y entre los pentecostales, entre otros, se ha enfatizado el aspecto presente, mientras que protestantes fundamentalistas y evangélicos han enfatizado el aspecto futuro.

La enseñanza budista no reconoce a un Dios ni a una Persona, acentúa la naturaleza no individualizada de la Deidad y se niega a personalizar a la Divinidad. Buddha preservó en el pensamiento humano el concepto de la Deidad trascendente, separada de la triplicidad, dualidad y multiplicidad de la manifestación. Para él sólo existía la vida amorfa sin individualidad y desconocida.

En la enseñanza occidental fue preservado el concepto de un Dios inmanente, Dios presente en todas las formas. En la síntesis de las enseñanzas de oriente y occidente y en la fusión de las dos grandes escuelas de pensamiento, de la filosofía kantiana, puede comprenderse algo de ese quinto reino. El Concepto de Dios aparece trascendente para la razón pura e inmanente para la razón práctica.





</doc>
<doc id="2542" url="https://es.wikipedia.org/wiki?curid=2542" title="Roger de Flor">
Roger de Flor

Roger de Flor (Brindisi, 1266-Adrianópolis, 1305) fue un caballero templario y caudillo mercenario al servicio de la Corona de Aragón. Ejerció como uno de los capitanes de los almogávares y también fue conocido como Roger von Blume y Rutger Blume.

Roger fue hijo de un oficial de cetrería del emperador Federico II llamado Ricardo y de una burguesa de Brindisi, donde él nació. Cuando se arruinó la familia, su madre le confió a un caballero de la Orden del Temple y allí fue Hermano sargento al mando del navío "Halcón". 

Participó en la última cruzada a Tierra Santa, donde se distinguió en la defensa de San Juan de Acre (1291). Sin embargo, los templarios le acusaron de haberse apropiado de tesoros de la orden en la confusión en la que se desarrolló el desalojo de la ciudad, por lo que fue expulsado de la orden. Aprovechando su experiencia militar, se hizo mercenario, entrando al servicio del rey Federico II de Sicilia (hijo de Pedro III el Grande de Aragón).

Federico puso a Roger de Flor al mando de las compañías de almogávares, mercenarios que habían sido empleados por la Corona de Aragón en la conquista de Valencia y Mallorca, y más tarde para consolidar sus dominios de Sicilia frente a las pretensiones de la Casa de Anjou. Participó en la defensa de Mesina en 1302 demostrando dotes de auténtico líder. 

Tras la Paz de Caltabellota (1302) entre Carlos II de Anjou y Federico de Sicilia, en 1303 se puso al servicio del emperador bizantino Andrónico II Paleólogo para ayudarle contra el peligro otomano, al mando de una expedición de 4.000 almogávares, 1.500 soldados de caballería y 39 naves enviada por Federico (la Gran Compañía Catalana). Desfiló al mando de los almogávares, los cuales le tenían gran estima, ante el emperador bizantino en la ciudad de Constantinopla. Al mando de los almogávares aniquiló a los genoveses de Constantinopla, acto que agradeció el emperador. Harto de su tutela, pasó a Anatolia y tomó las ciudades de Filadelfia, Magnesia y Éfeso, rechazando a los turcos hasta Cilicia y los Tauro (1304), siempre en batallas en inferioridad numérica. 

Durante la primavera de 1304 tuvo lugar también una batalla entre los almogávares e invasores escitas procedentes del norte del mar Negro (alanos), que fueron derrotados. En recompensa por los servicios al Imperio, Andrónico le concedió el título de megaduque (comandante de la flota) y la mano de María, su sobrina e hija del zar de Bulgaria. Las batallas anteriores habían sido cortas y provocaron mayor número de víctimas, sobre todo en la retirada de los turcos del campo de batalla. Fueron de menor intensidad comparadas con la que se produjo cerca de las Puertas Cilicias. Roger de Flor y 8.000 almogávares derrotaron a un Ejército turco compuesto por 30 000 soldados, en su mayoría jenízaros, causando 18 000 bajas al enemigo. Después de esta gran victoria, los turcos se pensaron dos veces atacar de nuevo al Imperio Bizantino durante varios años, y Roger fue proclamado césar del Imperio, concediéndole aquél en feudo los territorios bizantinos en Asia Menor, con excepción de las ciudades. 
En la batalla destacó Berenguer de Entenza, que había apoyado a Roger con 1.000 almogávares. A éste se le concedió el título de megaduque a petición de Roger.

Estratégicamente, la posición de Roger de Flor y Berenguer de Entenza en Bizancio favorecía el proyecto Rex Bellator de Ramón Llull, que proponía en su "Liber de Fine" la ruta del sur (Almería-Granada-Norte de África-Egipto) para proseguir la Cruzada con ventaja de los reyes de la Corona de Aragón en caso de que hubiesen conseguido encabezar las órdenes militares unidas.

Sin embargo, la situación de los almogávares en el Imperio no era cómoda. Por una parte, al parecer cometieron excesos con la población local griega. Por otra, parece que la ambición de Roger de Flor era grande y pretendía erigirse en soberano de los territorios conquistados. Finalmente, su creciente ambición e influencia despertaron la hostilidad del emperador Miguel IX, hijo de Andrónico II y asociado al gobierno imperial. Así, éste le hizo asesinar en Adrianópolis durante un banquete junto con más de un centenar de jefes almogávares el 5 de abril de 1305, y atacó posteriormente a las tropas almogávares. Sin embargo, no sólo no pudieron acabar con ellos, sino que los supervivientes, bajo el mando de Berenguer de Entenza, contraatacaron y arrasaron todo cuanto encontraron a su paso en Tracia y Macedonia (hechos conocidos como "Venganza catalana"). Finalmente se creó un ducado (Atenas y Neopatria) nominalmente dependiente de la Corona de Aragón.

La figura de Roger de Flor alcanzó difusión entre sus contemporáneos gracias a la "Crónica de Muntaner", inspirando la obra "Tirante el Blanco", de Joanot Martorell. Una de las unidades de la BRIPAC (Brigada Paracaidista) del Ejército Español lleva su nombre.



</doc>
<doc id="2543" url="https://es.wikipedia.org/wiki?curid=2543" title="Real Academia Española">
Real Academia Española

La Real Academia Española (RAE) es una institución cultural con sede en Madrid (España). Esta y otras veintitrés academias de la Lengua correspondientes a cada uno de los países donde se habla el español conforman la Asociación de Academias de la Lengua Española (ASALE).

Se dedica a la regularización lingüística mediante la promulgación de normativas dirigidas a fomentar la unidad idiomática entre o dentro de los diversos territorios que componen el llamado mundo hispanohablante; garantizar una norma común, en concordancia con sus estatutos fundacionales: «velar por que los cambios que experimente [...] no quiebren la esencial unidad que mantiene en todo el ámbito hispánico».

Fue fundada en 1713 por iniciativa del ilustrado Juan Manuel Fernández Pacheco, VIII marqués de Villena y duque de Escalona, a imitación de la Academia Francesa. Al año siguiente, el rey Felipe V aprobó su constitución y la colocó bajo su protección.

Las directrices lingüísticas que propone se recogen en diversas obras. Las prioritarias son el diccionario, abreviado DRAE (art. 2.º de sus estatutos), editado periódicamente veintitrés veces desde 1780 hasta hoy; y la gramática (4.º), editada entre 2009 y 2011.

Desempeña sus funciones en la sede principal, inaugurada en 1894, en la calle Felipe IV, 4, en el barrio de Los Jerónimos, y en el Centro de Estudios de la Real Academia Española y de la ASALE, en la calle Serrano 187-189, en 2007.

En 1711, España, a diferencia de Francia, Italia y Portugal, no tenía un gran diccionario. El núcleo inicial de la futura Academia lo formaron ese mismo año los ocho "novatores" que se reunían en la biblioteca del palacio madrileño de Juan Manuel Fernández Pacheco, situado en la plaza de las Descalzas Reales en Madrid.

La Real Academia Española fue fundada en 1713 por iniciativa de Juan Manuel Fernández Pacheco, VIII marqués de Villena y duque de Escalona, con el propósito de «fijar las voces y vocablos de la lengua castellana en su mayor propiedad, elegancia y pureza»". "
El objetivo era fijar el idioma en el estado de plenitud que había alcanzado durante el siglo XVI y que se había consolidado en el XVII. Se tomaron como modelo para su creación la Accademia della Crusca italiana (1582) y la Academia Francesa (1635). La primera sesión oficial de la nueva corporación se celebró en la propia casa del marqués de Villena el 6 de julio de 1713, acontecimiento que se registra en el libro de actas, iniciado el 3 de agosto de 1713. Su creación, con veinticuatro sillas, fue aprobada el 3 de octubre de 1714 por Real Cédula de Felipe V, quien la acogió bajo su «amparo y Real Protección». Esto significaba que los académicos gozaban de las preeminencias y exenciones concedidas a la servidumbre de la Casa Real. Tuvo su primera sede en el número 26 de la calle de Valverde, de donde se trasladó a la de Alarcón esquina a Felipe IV, su sede definitiva.

En la conciencia, según la visión de la época, de que la lengua española había llegado a un momento de suma perfección, fue propósito de la Real Academia «fijar las voces y vocablos de la lengua castellana en su mayor propiedad, elegancia y pureza». Se representó tal finalidad con un emblema formado por un crisol puesto al fuego, con la leyenda: "Limpia, fija y da esplendor". Nació, por tanto, la institución como un centro de trabajo eficaz, según decían los fundadores, «al servicio del honor de la nación».

Esta vocación de utilidad colectiva se convirtió en la principal seña de identidad de la Academia Española, diferenciándola de otras academias que habían proliferado en los siglos de oro y que estaban concebidas como meras tertulias literarias de carácter ocasional.

En 1723 se le concedieron al marqués 60 000 reales anuales para sus publicaciones. Fernando VI le permitió publicar sus obras y las de sus miembros sin censura previa.

En 1726 se publica el primer volumen del gran diccionario de la época, y en 1741 el de ortografía. Y después, una gramática.

En 1784, María Isidra de Guzmán y de la Cerda, primera mujer doctora por la Universidad de Alcalá, fue admitida como académica honoraria y, aunque pronunció su discurso de agradecimiento, no volvió a comparecer más. Se cuenta entre las primeras mujeres académicas del mundo. No volvió a haber otra fémina hasta la elección como académica de número de Carmen Conde en 1978.

En 1842 solicitaron un crédito de ochenta mil reales por dos años para financiar el nuevo Diccionario a José Nicasio Gallego quien era secretario de la propia Real Corporación. Mediante dicho préstamo la Academia hipotecó todos sus bienes. En 1847 se pudo saldar la hipoteca.

En 1848 la Academia reformó su organización por medio de unos nuevos estatutos, aprobados por Real Decreto. Sucesivos reales decretos (1859, 1977, 1993) aprobaron nuevas reformas.

Tras la independencia de los países americanos, la Real Academia Española promovió el nacimiento de academias correspondientes en cada una de las jóvenes repúblicas hispanoamericanas. Esta decisión estuvo motivada por la idea central del movimiento llamado panhispanismo o hispanoamericanismo, según la cual los ciudadanos de todas las naciones de matriz española tienen por patria común una misma lengua (el español) y comparten el patrimonio de una misma literatura. A pesar de que hubo precedentes de academias nacionales creadas con independencia de la Española, como la Academia de la Lengua de México (1835), que se disolvió para dar paso a la correspondiente Academia Mexicana de la Lengua (1875), y de que alguna de las academias americanas, como la Academia Argentina de Letras (1931), no tuvo vinculación estatutaria con la RAE hasta fundarse la ASALE, desde 1870 se establecieron en América diversas academias hispanoamericanas subordinadas estatutariamente a la RAE, a las que se llamó "correspondientes" por mantener con la academia matriz una relación por correspondencia postal. A ellas se añadieron la Academia Argentina de Letras, la Academia Filipina de la Lengua Española, la Academia Norteamericana de la Lengua Española y la Academia Ecuatoguineana de la Lengua Española. Estas veintidós academias, que tienen igual rango y condiciones que la RAE, constituyen con ella la Asociación de Academias de la Lengua Española (ASALE), fundada en 1951 en el marco del I Congreso de Academias celebrado en México.

La ASALE es el órgano de colaboración de todas ellas en la promoción de una política lingüística panhispánica. Esta política, plasmada en numerosos proyectos de trabajo conjunto, fue galardonada en el año 2000 con el Premio Príncipe de Asturias de la Concordia, concedido a la Real Academia Española, junto con la Asociación de Academias de la Lengua Española.

El 20 de octubre de 1993 se constituyó la Fundación pro Real Academia Española, entidad que tiene como finalidad atraer recursos económicos para la financiación de las actividades e iniciativas de la Academia. Está regida por un patronato, cuya presidencia de honor corresponde al rey de España, la presidencia al gobernador del Banco de España y la vicepresidencia al director de la Real Academia Española. Las vocalías corresponden a otros académicos, presidentes de las comunidades autónomas y de empresas privadas, como socios fundadores.

En los últimos estatutos aprobados en 1993, se consideró necesario supeditar el antiguo lema fundacional -"Limpia, fija y da esplendor"- al objetivo superior de trabajar al servicio de la unidad idiomática. El artículo primero establece, en tal sentido, que la Academia “tiene como misión principal velar por que los cambios que experimente la lengua española en su constante adaptación a las necesidades de sus hablantes no quiebren la esencial unidad que mantiene en todo el ámbito hispánico”. De esa forma quedaba sancionado un compromiso que la Academia había asumido ya desde el siglo XIX.

La Fundación está abierta a la participación de particulares mediante la correspondiente cuota económica, miembros benefactores, y entre las actividades subvencionadas se encuentran la realización del banco de datos, el "Diccionario del estudiante", el "Diccionario panhispánico de dudas", la "Gramática normativa" y otras obras en proyecto o desarrollo como el CORPES (Corpus del Español del Siglo XXI) o el "Diccionario histórico".

El artículo primero de los estatutos de la RAE dice:

Los 46 miembros de número de la Real Academia Española son elegidos por cooptación por el resto de los académicos. Las plazas de académico de número se denominan «sillas», que tradicionalmente se han distribuido de acuerdo a letras del alfabeto latino de uso para el castellano, tanto mayúsculas como minúsculas (excepción hecha de las plazas de las secciones especiales o regionales). De acuerdo a una norma de respeto, la provisión de la plaza para un nuevo académico se inicia a partir del sexto mes desde el fallecimiento del anterior ocupante de la silla correspondiente.

Los académicos de número actualmente son:

Como dato de interés, el único Premio Nobel de Literatura español que no ingresó como académico en la RAE fue Juan Ramón Jiménez (galardonado en 1956 y fallecido dos años después).

En 1784, tal vez por presiones de la Corte, María Isidra de Guzmán y de la Cerda fue admitida como Académica honoraria. Sin embargo, a lo largo de la historia la Real Academia Española ha rechazado a personalidades del mundo de las letras por el hecho de ser mujeres.

En 1853 Gertrudis Gómez de Avellaneda solicitó su ingreso lo que planteó un largo debate tras el cual se tomó el acuerdo de no aceptar mujeres como académicas de número, resolución que la Academia utilizó hasta principios del siglo XX y que le valdría la consideración de antifeminista.En 1912 la petición de Emilia Pardo Bazán fue rechazada, a pesar de los apoyos de diferentes instituciones, en virtud del acuerdo de 1853.

La candidatura de Concha Espina fue igualmente rechazada en dos ocasiones (1928 y 1930),si bien en 1928 la Academia admite la de Blanca de los Ríos, candidatura que llegó a someterse a votación aunque no resultó elegida.También fue aceptada y sometida a votación la candidatura de María Moliner en 1972, aunque en esta ocasión la votación fue ganada por amplia mayoría por el lingüista Emilio Alarcos Llorach.

En 1978, casi 300 años después de su fundación, fue aceptada la presencia femenina en la Real Academia, siendo Carmen Conde la primera mujer que ejerció como Académica de número, ocupando la silla K.Al ingreso de esta escritora se han sucedido los de otras mujeres de reconocido prestigio en el mundo de las letras: Elena Quiroga de Abarca (1983), Ana María Matute (1998), María del Carmen Iglesias Cano (2002), Margarita Salas (2003),Soledad Puértolas (2010), Inés Fernández-Ordóñez (2011), Carme Riera (2013), Aurora Egido (2014), Clara Janés (2016) y Paz Battaner (2017).

Según sus estatutos, la RAE está compuesta por:


Una junta de gobierno rige la Academia y supervisa todos los asuntos relativos a su buena operación, tanto en lo relacionado con su funcionamiento interno como con sus relaciones con los organismos del estado, y las demás Academias. Esta junta la preside el director de la Academia y está constituida por el vicedirector, el secretario, el censor, el bibliotecario, el tesorero, el vicesecretario y dos vocales adjuntos. Todos estos cargos son electivos y, a excepción de los vocales, que se eligen cada dos años, pueden ejercerse durante cuatro años, prorrogables sólo una vez.

La Academia funciona en Pleno y en Comisiones que se reúnen semanalmente. Las Comisiones tienen la misión de elaborar las propuestas que posteriormente examinará el Pleno para decidir sobre su aprobación. En la actualidad existen las siguientes comisiones: Delegada del Pleno y para el Diccionario, Instituto de Lexicografía, Diccionario Histórico de la Lengua Española, Publicaciones y Boletín, Armonización de las Obras Académicas, Armonización de Terminología Lingüística, Comisión Conservadora de la Casa Museo Lope de Vega, Comisión para el III Centenario de la RAE, Ciencias Sociales, Vocabulario Científico y Técnico, Ciencias Humanas, Cultura I y Cultura II. Además, existe una Comisión encargada de la conservación de la casa del Museo de Lope de Vega.

El Pleno, formado por todos los académicos, se reúne durante el curso académico los jueves por la tarde. Una vez aprobada las actas de la sesión anterior y de debatir cualquier tema general, los asistentes presentan enmiendas y adiciones al Diccionario. Acto seguido se examinan las propuestas formuladas por las diversas Comisiones. Las resoluciones, en el caso de que se produzca disparidad de criterio, se adoptan mediante votación.

Al servicio de los trabajos que la Academia desarrolla en Pleno o en Comisiones, funciona el Instituto de Lexicografía, integrado por filólogos y lexicógrafos que realizan las tareas de apoyo para la elaboración de los diccionarios académicos.

Desde su creación la RAE ha tenido treinta directores. También hubo algunos casos de directores temporales, como Vicente García de Diego, director accidental (1965-1968), y Rafael Lapesa, director interino (1988).

El cargo de director de la Real Academia Española conlleva el cargo de presidente de la Asociación de Academias de la Lengua Española (ASALE).

Publicaciones conjuntas de la RAE y la Asociación de Academias de la Lengua Española (integrada por las 23 academias de la lengua española existentes en el mundo).
















Todas las obras son publicadas por la RAE y ASALE.








</doc>
<doc id="2551" url="https://es.wikipedia.org/wiki?curid=2551" title="Soda">
Soda

Soda hace referencia a varios artículos:


</doc>
<doc id="2552" url="https://es.wikipedia.org/wiki?curid=2552" title="Sistema operativo">
Sistema operativo

Un sistema operativo (SO o, frecuentemente, OS —del inglés "operating system"—) es el "software" principal o conjunto de programas de un sistema informático que gestiona los recursos de "hardware" y provee servicios a los programas de aplicación de "software", ejecutándose en modo privilegiado respecto de los restantes (aunque puede que parte de él se ejecute en espacio de usuario).

Nótese que es un error común muy extendido denominar al conjunto completo de herramientas sistema operativo,es decir, la inclusión en el mismo término de programas como el explorador de ficheros, el navegador web y todo tipo de herramientas que permiten la interacción con el sistema operativo. Otro ejemplo para comprender esta diferencia se encuentra en la plataforma Amiga, donde el entorno gráfico de usuario se distribuía por separado, de modo que, también podía reemplazarse por otro, como era el caso de directory Opus o incluso manejarlo arrancando con una línea de comandos y el sistema gráfico. De este modo, comenzaba a funcionar con el propio sistema operativo que llevaba incluido en una ROM, por lo que era cuestión del usuario decidir si necesitaba un entorno gráfico para manejar el sistema operativo o simplemente otra aplicación. Uno de los más prominentes ejemplos de esta diferencia, es el núcleo Linux, usado en las llamadas distribuciones Linux, ya que al estar también basadas en Unix, proporcionan un sistema de funcionamiento similar. Este error de precisión, se debe a la modernización de la informática llevada a cabo a finales de los 80, cuando la filosofía de estructura básica de funcionamiento de los grandes computadores se rediseñó a fin de llevarla a los hogares y facilitar su uso, cambiando el concepto de computador multiusuario, (muchos usuarios al mismo tiempo) por un sistema monousuario (únicamente un usuario al mismo tiempo) más sencillo de gestionar. Véase AmigaOS, beOS o Mac OS como los pioneros de dicha modernización, cuando los Amiga fueron bautizados con el sobrenombre de "Video Toasters" por su capacidad para la Edición de vídeo en entorno multitarea round robin, con gestión de miles de colores e interfaces intuitivos para diseño en 3D.

En ciertos textos, el sistema operativo es llamado indistintamente como núcleo o kernel, pero debe tenerse en cuenta que la diferencia entre "kernel" y sistema operativo solo es aplicable si el núcleo es monolítico, lo cual fue muy común entre los primeros sistemas. En caso contrario, es incorrecto llamar al sistema operativo núcleo.

Uno de los propósitos del sistema operativo que gestiona el núcleo intermediario consiste en gestionar los recursos de localización y protección de acceso del "hardware", hecho que alivia a los programadores de aplicaciones de tener que tratar con estos detalles. La mayoría de aparatos electrónicos que utilizan microprocesadores para funcionar, llevan incorporado un sistema operativo (teléfonos móviles, reproductores de DVD, computadoras, radios, enrutadores, etc.). En cuyo caso, son manejados mediante una interfaz gráfica de usuario, un gestor de ventanas o un entorno de escritorio, si es un celular, mediante una consola o control remoto si es un DVD y, mediante una línea de comandos o navegador web si es un enrutador.

Los primeros sistemas (1945-1954) eran grandes máquinas operadas desde la consola maestra por los programadores. Durante la década siguiente (1955-1965) se llevaron a cabo avances en el "hardware": lectoras de tarjetas, impresoras, cintas magnéticas, etc. Esto a su vez provocó un avance en el "software": compiladores, ensambladores, cargadores, manejadores de dispositivos, etc.

A finales de los años 1980, una computadora Commodore Amiga equipada con una aceleradora Video Toaster era capaz de producir efectos comparados a sistemas dedicados que costaban el triple. Un Video Toaster junto a Lightwave ayudó a producir muchos programas de televisión y películas, entre las que se incluyen Babylon 5, SeaQuest DSV y .

El problema principal de los primeros sistemas era su baja utilización, la primera solución fue poner un operador profesional que lo manejase, con lo que se eliminaron las hojas de reserva, se ahorró tiempo y se aumentó la velocidad.

Para ello, los trabajos se agrupaban de forma manual en lotes mediante lo que se conoce como procesamiento por lotes (batch) sin automatizar.

Según fue avanzando la complejidad de los programas, fue necesario implementar soluciones que automatizaran la organización de tareas sin necesidad de un operador. Debido a ello se crearon los monitores residentes: programas que residían en memoria y que gestionaban la ejecución de una cola de trabajos.

Un monitor residente estaba compuesto por un cargador, un Intérprete de comandos y un controlador ("drivers") para el manejo de entrada/salida.

Los avances en el "hardware" crearon el soporte de interrupciones y posteriormente se llevó a cabo un intento de solución más avanzado: solapar la E/S de un trabajo con sus propios cálculos, por lo que se creó el sistema de búfers con el siguiente funcionamiento:


Los problemas surgen si hay muchas más operaciones de cálculo que de E/S (limitado por la CPU) o si por el contrario hay muchas más operaciones de E/S que de cálculo (limitado por la E/S).

Hace aparición el disco magnético con lo que surgen nuevas soluciones a los problemas de rendimiento. Se eliminan las cintas magnéticas para el volcado previo de los datos de dispositivos lentos y se sustituyen por discos (un disco puede simular varias cintas). Debido al solapamiento del cálculo de un trabajo con la E/S de otro trabajo se crean tablas en el disco para diferentes tareas, lo que se conoce como Spool (Simultaneous Peripherial Operation On-Line).

Surge un nuevo avance: el "hardware" con protección de memoria, ofreciendo nuevas soluciones a los problemas de rendimiento:


Con los cambios anteriores el monitor residente debe abordar nuevas tareas, naciendo los Sistemas Operativos multiprogramados con las siguientes funciones:


Cuando desempeña esas tareas, el monitor residente se transforma en un sistema operativo multiprogramado.

Definición breve: llamadas que ejecutan los programas de aplicación para pedir algún servicio al SO.

Cada SO implementa un conjunto propio de llamadas al sistema. Ese conjunto de llamadas es la interfaz del SO frente a las aplicaciones. Constituyen el lenguaje que deben usar las aplicaciones para comunicarse con el SO. Por ello si cambiamos de SO, y abrimos un programa diseñado para trabajar sobre el anterior, en general el programa no funcionará, a no ser que el nuevo SO tenga la misma interfaz. Para ello:


Las aplicaciones no deben poder usar todas las instrucciones de la CPU. No obstante el Sistema Operativo, tiene que poder utilizar todo el conjunto de instrucciones del CPU. Por ello, una CPU debe tener (al menos) dos modos de operación diferentes:


Una aplicación, normalmente no sabe dónde está situada la rutina de servicio de la llamada. Por lo que si ésta se codifica como una llamada de función, cualquier cambio en el S.O. haría que hubiera que reconstruir la aplicación.

Pero lo más importante es que una llamada de función no cambia el modo de ejecución de la CPU. Con lo que hay que conseguir llamar a la rutina de servicio, sin tener que conocer su ubicación, y hacer que se fuerce un cambio de modo de operación de la CPU en la llamada (y la recuperación del modo anterior en el retorno).

Esto se hace utilizando instrucciones máquina diseñadas específicamente para este cometido, distintas de las que se usan para las llamadas de función.

Las llamadas al sistema no siempre tienen una expresión sencilla en los lenguajes de alto nivel, por ello se crean las bibliotecas de interfaz, que son bibliotecas de funciones que pueden usarse para efectuar llamadas al sistema. Las hay para distintos lenguajes de programación.

La aplicación llama a una función de la biblioteca de interfaz (mediante una llamada normal) y esa función es la que realmente hace la llamada al sistema.

El SO ocupa una posición intermedia entre los programas de aplicación y el "hardware". No se limita a utilizar el "hardware" a petición de las aplicaciones ya que hay situaciones en las que es el "hardware" el que necesita que se ejecute código del SO. En tales situaciones el "hardware" debe poder llamar al sistema, pudiendo deberse estas llamadas a dos condiciones:


En ambos casos, la acción realizada no está ordenada por el programa de aplicación, es decir, no figura en el programa.

Según los dos casos anteriores tenemos las interrupciones y las excepciones:


Una interrupción se trata en todo caso, después de terminar la ejecución de la instrucción en curso.

El tratamiento depende de cuál sea el dispositivo de E/S que ha causado la interrupción, ante la cual debe poder identificar el dispositivo que la ha causado.

La ventaja de este procedimiento es que no se tiene que perder tiempo ejecutando continuamente rutinas para consultar el estado del periférico. El inconveniente es que el dispositivo debe tener los circuitos electrónicos necesarios para acceder al sistema de interrupciones del computador.

El mecanismo de tratamiento de las interrupciones permite al SO utilizar la CPU en servicio de una aplicación, mientras otra permanece a la espera de que concluya una operación en un dispositivo de E/S.

El "hardware" se encarga de avisar al SO cuando el dispositivo de E/S ha terminado y el SO puede intervenir entonces, si es conveniente, para hacer que el programa que estaba esperando por el dispositivo, se continúe ejecutando.

En ciertos intervalos de tiempo puede convenir no aceptar señales de interrupción. Por ello las interrupciones pueden inhibirse por programa (aunque esto ellas no deben poder hacerlo).

Un ejemplo de sincronismo por interrupción es el almacenamiento de caracteres introducidos mediante el teclado. Cuando se introduce un carácter, se codifica en el registro de datos del dispositivo y además se activa un bit del registro de estado quien crea una interrupción en el "hardware". El procesador deja temporalmente la tarea que estaba completando y ejecuta la rutina de atención a la interrupción correspondiente. El teclado almacena el carácter en el vector de memoria intermedia (también llamado "buffer") asociada al teclado y despierta el proceso que había en el estado de espera de la operación de entrada/salida.

Cuando la CPU intenta ejecutar una instrucción incorrectamente construida, la unidad de control lanza una excepción para permitir al SO ejecutar el tratamiento adecuado. Al contrario que en una interrupción, la instrucción en curso es abortada. Las excepciones al igual que las interrupciones deben estar identificadas.

Las instrucciones de un programa pueden estar mal construidas por diversas razones:


El mecanismo de tratamiento de las excepciones es esencial para impedir, junto a los modos de ejecución de la CPU y los mecanismos de protección de la memoria, que las aplicaciones realicen operaciones que no les están permitidas. En cualquier caso, el tratamiento específico de una excepción lo realiza el SO.

Como en el caso de las interrupciones, el "hardware" se limita a dejar el control al SO, y éste es el que trata la situación como convenga.

Es bastante frecuente que el tratamiento de una excepción no retorne al programa que se estaba ejecutando cuando se produjo la excepción, sino que el SO aborte la ejecución de ese programa. Este factor depende de la pericia del programador para controlar la excepción adecuadamente.

Un proceso es simplemente, un programa en ejecución que necesita recursos para realizar su tarea: tiempo de CPU, memoria, archivos y dispositivos de E/S. El SO es el responsable de lo siguiente:


La gestión de procesos podría ser similar al trabajo de oficina. Se puede tener una lista de tareas a realizar y a estas fijarles prioridades: alta, media, baja, por ejemplo. Debemos comenzar haciendo las tareas de prioridad alta primero y cuando se terminen seguir con las de prioridad media y después las de baja. Una vez realizada la tarea se tacha.

Esto puede traer un problema que las tareas de baja prioridad pueden que nunca lleguen a ejecutarse y permanezcan en la lista para siempre. Para solucionar esto, se puede asignar alta prioridad a las tareas más antiguas.

La memoria es una gran tabla de palabras o bytes que se referencia cada una mediante una dirección única. Este almacén de datos de rápido acceso es compartido por la CPU y los dispositivos de E/S, es volátil y pierde su contenido ante fallos del sistema. El SO es el responsable de:


Un sistema de almacenamiento secundario es necesario, ya que la memoria principal (almacenamiento primario) es volátil y además muy pequeña para almacenar todos los programas y datos. También es necesario mantener los datos que no convenga mantener en la memoria principal. El SO se encarga de:


Consiste en un sistema de almacenamiento temporal (caché), una interfaz de manejadores de dispositivos y otra para dispositivos concretos. El sistema operativo debe gestionar el almacenamiento temporal de E/S y servir las interrupciones de los dispositivos de E/S.

Los archivos son colecciones de información relacionada, definidas por sus creadores. Estos almacenan programas (en código fuente y objeto) y datos tales como imágenes, textos, información de bases de datos, etc. El SO es responsable de:


Existen diferentes sistemas de archivos, es decir, existen diferentes formas de organizar la información que se almacena en las memorias (normalmente discos) de los ordenadores. Por ejemplo, existen los sistemas de archivos FAT, FAT32, ext3, NTFS, XFS, etc.

Desde el punto de vista del usuario estas diferencias pueden parecer insignificantes a primera vista, sin embargo, existen diferencias muy importantes. Por ejemplo, los sistemas de ficheros FAT32 y NTFS, que se utilizan fundamentalmente en sistemas operativos de Microsoft, tienen una gran diferencia para un usuario que utilice una base de datos con bastante información ya que el tamaño máximo de un fichero con un sistema de archivos FAT32 está limitado a 4 gigabytes, sin embargo, en un sistema NTFS el tamaño es considerablemente mayor.

Mecanismo que controla el acceso de los programas o los usuarios a los recursos del sistema. El SO se encarga de:


Para mantener las comunicaciones con otros sistemas es necesario poder controlar el envío y recepción de información a través de las interfaces de red. También hay que crear y mantener puntos de comunicación que sirvan a las aplicaciones para enviar y recibir información, y crear y mantener conexiones virtuales entre aplicaciones que están ejecutándose localmente y otras que lo hacen remotamente.

Son aplicaciones de utilidad que se suministran con el SO pero no forman parte de él. Ofrecen un entorno útil para el desarrollo y ejecución de programas, siendo algunas de las tareas que realizan:


Como gestor de recursos, el sistema operativo administra:








</doc>
<doc id="2554" url="https://es.wikipedia.org/wiki?curid=2554" title="San Sebastián">
San Sebastián

San Sebastián (en euskera Donostia y oficialmente Donostia/San Sebastián) es una ciudad y municipio español situado en la costa del golfo de Vizcaya y a 20 kilómetros de la frontera con Francia. La ciudad es la capital de la provincia de Guipúzcoa, en la comunidad autónoma del País Vasco. La población del municipio es de 186064 habitantes (2016), y su área metropolitana alcanza los 436 500 (2010). Es la cabecera de la Eurociudad Vasca Bayona-San Sebastián, una conurbación de más de 620 000 habitantes.

Sus principales actividades económicas son el comercio y el turismo, constituyendo en el pasado uno de los más famosos destinos turísticos de España. Su paisaje, dominado por la bahía de La Concha, así como su desarrollo arquitectónico moderno iniciado en la segunda mitad del siglo XIX, que configuró una ciudad de corte francés y aburguesado, propiciaron el desarrollo de la actividad turística a escala europea. Todo ello, unido a eventos internacionales como el Festival Internacional de Cine de San Sebastián, el Festival de Jazz de San Sebastián, la Quincena Musical o el Festival de Cine de Terror, ha dado proyección exterior a la ciudad, a pesar de sus pequeñas dimensiones. San Sebastián fue Capital Europea de la Cultura en 2016 junto con Breslavia, Polonia.

San Sebastián tiene diversas denominaciones:


El escudo de San Sebastián muestra, en campo de azur, sobre ondas de azur y plata, un bergantín de oro, de tres palos, habillado de plata y acompañado de las letras SS, de plata, una en cada cantón. Bordura de plata con la leyenda "Ganadas por fidelidad, nobleza y lealtad", en letras de sable. Al timbre corona real.

La bandera de la ciudad es blanca con un cantón de color azul, en una proporción de tres partes de largo por dos de ancho. Se corresponde con la contraseña de la Provincia marítima de San Sebastián.

San Sebastián se asienta a orillas del mar Cantábrico, teniendo varias playas (siendo la más conocida la de La Concha, en la bahía homónima) y un pequeño puerto al abrigo del monte Urgull. Posee además otras montañas, tanto promontorios costeros como tierra adentro, estando su cima más alta, Urdaburu (), en un exclave homónimo, si bien la cima no pertenece al término municipal, dándose la máxima altitud del municipio de unos 585 metros en la cara sur de la misma montaña, cerca de la cima. Aunque el relieve es accidentado se encuentran algunas zonas llanas de cierta amplitud en los valles, planicies donde se concentra buena parte del núcleo urbano.

La ciudad tiene tres playas urbanas: Ondarreta, La Concha y la Zurriola, las dos primeras situadas en la bahía de La Concha y la tercera al otro lado del río Urumea. Las tres están englobadas en un mismo Sistema de Gestión Medioambiental, que trata de garantizar un uso sostenible de las mismas.


Además de estas tres playas, también es utilizable la pequeña playa que se forma en la isla de Santa Clara, a la que se puede acceder en barco en los meses de verano, o a nado, pues se encuentra a escasos 500 m de Ondarreta en marea baja.

San Sebastián tiene un clima oceánico de tipo Cfb de acuerdo a la clasificación climática de Köppen y es una de las ciudades más lluviosas de España, con una media anual de unos 1.500 mm. Las lluvias son abundantes en todas las estaciones del año, especialmente en otoño, habiendo un mínimo en verano poco destacable. En 2007, San Sebastián fue la ciudad con más lluvia de España, con 1536,1 milímetros, según se desprende de los datos de los que dispone el Instituto Nacional de Estadística, recogidos en su anuario estadístico. Las precipitaciones en forma de nieve son escasas (entre 1 y 3 días al año, aunque hay inviernos como el 2004-05 y en el 2009-10 en los que el número de días fue superior a 10). A su vez, el número de heladas suele variar entre 5 y 10 anuales. A finales de septiembre y principios de octubre suelen darse «mareas vivas», pleamares más altas y bajamares más bajas de lo normal.

Las temperaturas son suaves y templadas (con una media de 15°C), aunque en verano e invierno la gran humedad (en torno a un 70-80 % la mayor parte de los días del año) provoca sensaciones térmicas de mayor calor/frío. Los días en los que sopla el viento del sur (que provoca el efecto foehn) eleva las temperaturas hasta los 20 °C en pleno invierno y hasta los 37 o 38 °C en verano: la humendad desciende considerablemente (aunque esta situación de altas temperaturas en verano suele durar unos pocos días o incluso unas horas, al interrumpirse con un giro de viento a componente NO, el cual proviene del mar Cantábrico; este fenómeno es la galerna y viene acompañado de un brusco descenso de las temperaturas y en ocasiones de nubes, tormentas o incluso niebla marina).

En situación de invasión de aire frío procedente de Europa (vientos del NE), San Sebastián es una de las primeras ciudades en notar el frío y suele ser una de las capitales costeras españolas más afectadas, dada su proximidad a Francia. Esto se debe a que los vientos no tienen recorrido marítimo y por lo tanto se templan menos que en otras ciudades, por lo que no es raro ver la playa de la Concha cubierta de nieve. Las temperaturas extremas registradas en el observatorio meteorológico de San Sebastián son de 38,6 °C y de –12,1 °C.

Estos son los valores medios y extremos de temperaturas y precipitaciones para San Sebastián:

Si bien se desconoce de manera exacta su fundación, el primer dato lo aporta un documento —considerado falso por la mayoría de los historiadores— del año 1014 de Sancho el Mayor de Navarra, según el cual el monasterio de San Sebastián se pone en manos del abad de Leyre y obispo de Pamplona. Dicho documento será confirmado, en 1101, por el rey Pedro Ramírez (Pedro I de Aragón, rey de Navarra y Aragón). Las primeras noticias escritas de San Sebastián hacen referencia a un monasterio, situado en el barrio que aún hoy se denomina San Sebastián El Antiguo. A aquel lugar se le conoció primitivamente, según algunos historiadores, como Izurum. El término español San Sebastián y la palabra vasca Donostia surge etimológicamente de la evolución de la palabra "Donebastian" (de Domine o "Done" = Santo, y Sebastián).

En los siglos XI y XII, el monasterio de San Sebastián El Antiguo, al mismo tiempo que centro espiritual, lo era de la naciente vida social y administrativa de la población de esta zona, que, con el tiempo, de no ser por diversos avatares que tendrán lugar posteriormente, habría cristalizado en un municipio.

San Sebastián fue fundada hacia 1180 por Sancho el Sabio, rey de Navarra, para ser puerto marítimo de Navarra, e inicialmente cumplió su misión como tal. Guipúzcoa a partir del año 1200 rindió vasallaje al rey castellano Alfonso VIII, enemigo de Sancho el Fuerte. Tradicionalmente, se ha tendido a creer que ese cambio de un reino a otro se dio a través de una negociación o pacto. Sin embargo, a tenor de la relectura de fuentes históricas conocidas, parece que San Sebastián pasó a Castilla mediante conquista militar.En cualquier caso, los comerciantes de San Sebastián se acostumbraron rápidamente al cambio, puesto que pasó de ser el puerto de un pequeño estado sin posibilidades de expansión territorial (Navarra), a servir de salida al mar de una monarquía, la castellana, mucho mayor, más rica y en plena expansión.

Los Reyes de Castilla contaron en 1248 por primera vez con fuerzas navales de San Sebastián, que tomaron parte en inutilizar la escuadra de moros y el puente de Triana, cuyo resultado fue la rendición de la ciudad de Sevilla.

Alfonso VIII juró los fueros e inició la larga serie de privilegios otorgados a San Sebastián, tendentes a mantener unos vivo el tráfico navarro y otros una situación privilegiada de los comerciantes donostiarras en el mercado español. Esta prosperidad es la que la hizo resurgir de los múltiples incendios que padeció a partir de 1266, llegando a arder por completo seis veces en dos siglos y cuarto.

La guerra de los Cien Años, las guerras de bandos y la evolución de Navarra en dirección francesa por motivos dinásticos trajeron para San Sebastián, en la segunda mitad del siglo XIV, una consecuencia grave: el desplazamiento de las principales líneas de tráfico hacia Bilbao, sustituyendo a San Sebastián como centro de gravedad del tráfico comercial. En enero de 1489 un incendio redujo a cenizas la villa. Este desgraciado acontecimiento tuvo como medida la construcción en piedra de la villa. Este incendio sería el último de la época medieval de San Sebastián.

A partir del último cuarto del siglo XV, San Sebastián pasó de ser un emporio mercantil gracias su situación estratégica, a ser plaza militar y su puerto principal, Pasajes, de ser esencialmente comercial a cumplir las funciones de base naval.

Tras la catástrofe de 1489, más que de una reconstrucción de la villa hay que hablar de una nueva forma de vida de la colectividad donostiarra. A partir del último cuarto del siglo XV, San Sebastián pasará, de ser un emporio mercantil por su situación estratégica, a ser plaza militar; y su puerto principal, Pasajes, pasará de ser esencialmente comercial, a cumplir las funciones de base naval de la Escuadra Cantábrica, fuerza marítima que mantendrá durante siglos (hasta el XIX) la lucha contra las escuadras francesa, holandesa y británica.

Este nuevo papel de San Sebastián como fortaleza, encargada de frenar las acometidas de los franceses, dará lugar a que la villa tome nuevos derroteros, por los cuales ganó los títulos de Noble y Leal. En el período entre los Reyes Católicos y Felipe V, trescientos años aproximadamente, la villa sufrió numerosos sitios. Este continuo estado de guerra supuso para San Sebastián un fuerte deterioro de su economía, motivado por los gastos en las fortificaciones, el mantenimiento de la guarnición y la continua caída del comercio marítimo, que, a partir de 1573, se agravó aún más, pues Sevilla adquirió el monopolio de las transacciones con América.

Después de llevar dos siglos cumpliendo heroicamente su misión bélica, Felipe IV le concedió en 1662 el título de Ciudad. Hasta su fundación sólo había pequeñas zonas residenciales en el barrio del Antiguo, en la Parte Vieja y en el valle del Urumea, emprendiendo hasta el siglo XV un lento proceso de crecimiento.
En 1719 San Sebastián fue tomada, por primera vez, por un poderoso ejército francés mandado por el duque de Berwick, quien se encontró una ciudad débil en fortificaciones y una pequeña guarnición con escasez de víveres y munición. La ciudad estuvo ocupada por una guarnición de 2000 soldados franceses hasta el 25 de agosto de 1721 en que fue evacuada por el Tratado de La Haya.

Durante la guerra de la Independencia, San Sebastián fue ocupada en 1808 por las tropas napoleónicas. Nombrado José I (José Bonaparte) soberano de España, entró el 9 de junio en San Sebastián y recorrió la calle Narrica, en la que permanecieron todas las ventanas cerradas. En junio de 1813, los aliados (las tropas anglo-portuguesas, bajo el mando directo de "sir" Thomas Graham y teniendo por generalísimo al duque de Wellington, con un fuerte contingente de tropas y armas), sitiaron la ciudad. Después de varios días de intenso bombardeo y un primer asalto fallido, el 31 de agosto tuvo lugar el asalto definitivo, realizado a través de la brecha abierta en las murallas, lo que obligó a las tropas francesas a replegarse hacia el Castillo, donde capitularon el 8 de septiembre.

El saqueo de las tropas anglo-portuguesas causó un gran incendio, del que solo se salvaron treinta y cinco casas, que servían de alojamiento para los oficiales británicos y portugueses, situadas en la misma calle, que hoy en día lleva el nombre de 31 de agosto en honor a ser la única calle que se salvó del incendio. Las tropas también iniciaron el ataque al Castillo, así como las edificaciones situadas al norte de la calle de la Trinidad (iglesias de Santa María y de San Vicente y conventos de San Telmo y de Santa Teresa).

Tras la guerra, los vecinos más representativos se reunieron en las afueras, en Zubieta, y decidieron reconstruir la ciudad.

La división del reino en cincuenta y dos provincias establece la capitalidad de Guipúzcoa en San Sebastián; hasta entonces ésta se había turnado entre San Sebastián, Tolosa, Azpeitia y Azcoitia, en función de dónde se realizaban las reuniones de Juntas y residiera el corregidor (representante del rey en la provincia). Tras un nuevo traslado a Tolosa (1844), en 1854 se declara San Sebastián capital de la provincia. Se decide el retroceso de las aduanas al Ebro y el cierre de San Sebastián como puerto habilitado para el comercio con América.

En la provincia se formaron dos bandos, carlistas y liberales, estos últimos partidarios de la Constitución. Ambos defendían los fueros, pero de diferente manera. San Sebastián optó por el liberalismo frente a la mayor parte de la Guipúzcoa rural.

En 1863, y tras un intenso debate, se derribaron las murallas, que limitaban el desarrollo de la ciudad. El 4 de mayo, a los acordes de una marcha expresamente realizada para tal acontecimiento, se procedió a quitar la primera piedra que, hecha pedazos, se repartió entre los invitados de primera fila.

San Sebastián cambió de orientación: terminada su etapa como fortaleza, pasó a cumplir la función de capital de la provincia, comenzando su expansión reflejada en el plan de Antonio Cortázar para la nueva ciudad.

A la muerte del rey Alfonso XII de España, en 1885, su viuda, la Reina Regente María Cristina, traslada todos los veranos la corte a San Sebastián, residiendo en el Palacio de Miramar. El Ayuntamiento de San Sebastián, en reconocimiento a la gran labor en favor de la ciudad, la nombró alcaldesa honoraria. Más adelante, ya en pleno desarrollo del Ensanche Cortázar, que dotó a la ciudad de su actual atractivo arquitectónico, la construcción del Casino en 1887 aumentó el número de veraneantes.

De esta etapa son todos los edificios reseñables de la ciudad (aparte de los presentes en la Parte Vieja, los más antiguos), como la Catedral del Buen Pastor de San Sebastián, la Escuela de Artes y Oficios (actual sede de Correos) y el Instituto Peñaflorida (luego ocupado por la Escuela de Ingenieros Industriales y hoy en día por el Centro Cultural Koldo Mitxelena), el Palacio de Miramar, el Teatro Victoria Eugenia, el Hotel María Cristina, las villas del Paseo de Francia o la estación del Norte, así como el resto de edificios del "Área Romántica", todos ellos con un marcado estilo francés que hizo acreedora a San Sebastián del sobrenombre de "Pequeña París" o "París del Sur".

En 1914, y con el inicio de la I Guerra Mundial, San Sebastián se convirtió en la ciudad más cosmopolita de Europa. En su Casino se dieron cita todos los personajes de la vida europea, Mata Hari, León Trotsky, Maurice Ravel, Romanones, Pastora Imperio, el torero de fama, el banquero ostentoso...; fueron los tiempos de la "Belle Époque" donostiarra, y en San Sebastián actuaron la compañía francesa de opereta, los ballets rusos, cantantes de ópera y muchos otros artistas famosos.

En 1930, la ciudad acogió la reunión de políticos republicanos que se dio en llamar Pacto de San Sebastián, que tuvo una gran trascendencia en el posterior advenimiento de la II República el 14 de abril de 1931; de hecho, el primer gobierno republicano estuvo formado, en gran medida, por el núcleo de políticos participantes en el «pacto». La elección de la capital donostiarra se debió, por una parte, a la proximidad de la ciudad con la República Francesa y al hecho de que San Sebastián fuera la capital de verano de la Corte. Fernando Sasiaín, anfitrión del Pacto, fue el alcalde de San Sebastián durante la República.

Al comenzar la Guerra Civil, el nacionalista vasco Telesforo Monzón se hizo cargo de la Comisión de Orden Público, creada por la Junta de Defensa de Guipúzcoa, en la que se reunían nacionalistas vascos, republicanos, comunistas y socialistas. Constituido el Gobierno de Euskadi, el 7 de octubre de 1936, Telesforo Monzón ocupó asimismo el Ministerio de la Gobernación.

Al poco de estallar la Guerra Civil Española, San Sebastián cayó en manos de los golpistas el 13 de septiembre de 1936. La dictadura mantuvo a San Sebastián en el papel de Ciudad Capital de Veraneo. Franco residió durante los meses de agosto desde 1940 hasta 1975 en el palacio de Ayete, que, comprado por el ayuntamiento, fue ofrecido al general. Durante ese período se celebraron en dicho lugar los Consejos de Ministros.

En 1946, durante el mandato de Rafael Lataillade Aldecoa, se llevó a cabo la recuperación del Gran Casino para reconvertirlo en Casa Consistorial.

En 1953, y a iniciativa de un grupo de comerciantes de la ciudad, nació el Festival Internacional de Cine de San Sebastián, con el doble objetivo de alargar el veraneo en la capital donostiarra y de devolver a San Sebastián la actividad cultural y el "glamour" perdidos desde la Guerra Civil. El éxito de la primera edición llevó a la dictadura a hacerse cargo del evento, que progresivamente fue ganando peso y prestigio hasta convertirse en uno de los eventos culturales más importantes y con mayor proyección exterior de España, y en uno de los mejores festivales de cine del mundo, escenario de algunos estrenos cinematográficos históricos y punto de encuentro de buena parte de las más importantes estrellas del séptimo arte.

En 1955 se inició el segundo y más importante proceso de ensanche de la ciudad, en lo que se denominó "Ensanche de Amara", dando lugar a un barrio del mismo nombre (que aludía a las marismas que había en dicho terreno antes de su construcción). Uno de los primeros pasos en la construcción del ensanche fue el traslado de la Escuela de Artes y Oficios y Comercio, situada en el centro, a unas escuelas de nueva construcción, así como el del Instituto Peñaflorida, pasando a denominarse Instituto Usandizaga en su sección femenina. Puede considerarse que el proceso de consolidación del barrio de Amara finalizó en 1993, con la construcción del Estadio de Anoeta y la renovación total de la ciudad deportiva de la ciudad (situada en Amara).

Tras ambos ensanches la ciudad consolidó su eje principal, alrededor del cual continúa expandiéndose aunque a un ritmo mucho menor. Hoy las prioridades de la ciudad son la mejora de las infraestructuras (potenciación del aeropuerto, mejores comunicaciones ferroviarias, mejora de las carreteras), la regeneración de los barrios de la periferia, el mantenimiento y potenciamiento del turismo, principal fuente de ingresos, y hasta hace unos años la lucha contra el terrorismo de ETA y la violencia callejera, que castigaron a la ciudad con intensidad. El crecimiento urbanístico pretende combinarse con el cuidado del medio ambiente, la lucha a escala municipal contra el cambio climático y la sostenibilidad. Fruto de los esfuerzos realizados en dicha dirección, San Sebastián fue premiada en 2008 por la Federación Española de Municipios y Provincias como la ciudad más sostenible de España.

A la muerte del dictador Francisco Franco se constituyó, en 1978, una gestora presidida por el socialista Ramón Jáuregui encargada de dirigir las instituciones municipales hasta las primeras elecciones municipales de la democracia, en 1979. En dichos comicios electorales resultó vencedor el PNV y el primer alcalde de la nueva etapa democrática fue Jesús María Alkain. Le sucedió, en 1983, Ramón Labayen, también del PNV, quien a su vez fue sustituido por el nacionalista Xabier Albistur, de Eusko Alkartasuna, en 1987. En el marco de la fuerte reconversión industrial que vivió el País Vasco en la década de 1980 y el clima de tensión interno, algunos informes de la época situaron a San Sebastián como la ciudad con mayor proporción de adictos a la droga del mundo. El socialista Odón Elorza, del Partido Socialista de Euskadi, alcanzó la alcaldía en 1991 a pesar de ser el candidato de la tercera fuerza más votada, gracias al apoyo del PNV y el PP. El 23 de enero de 1995, en vísperas de las elecciones municipales de mayo, la banda terrorista ETA asesinó al teniente de alcalde, Gregorio Ordóñez, candidato del Partido Popular. Ordóñez había mejorado progresivamente sus resultados electorales en el País Vasco. Tras su asesinato, el candidato del Partido Socialista, Elorza, revalidó su cargo al ser el candidato más votado y ostentó la alcaldía de la ciudad ininterrumpidamente desde entonces hasta su derrota en las elecciones municipales del 22 de mayo de 2011.

En 1991 Odón Elorza (PSE-EE) se convirtió en alcalde, con el apoyo del PP vasco y de EAJ-PNV. Con diversos pactos (con EAJ-PNV y EA en 1995; con PP en 1999) se mantuvo al frente del consistorio hasta las elecciones locales de 2011. En 2007, formó gobierno con el grupo municipal de Ezker Batua-Berdeak/Aralar.

En 2011 el candidato de Bildu, Juan Carlos Izagirre, fue elegido alcalde con los votos de los 8 concejales de su coalición; el PSE y el PP votaron por el candidato socialista, Ernesto Gasco (en total 13 votos) y el PNV votó por su candidato Eneko Goia (6 votos). Al no tener ningún candidato mayoría absoluta (14), fue elegido alcalde el candidato de la lista más votada por los ciudadanos. Por barrios, el PSE-EE fue la fuerza más votada en Bidebieta, Alza y Loyola; el PP en Ayete, Centro y Amara, y EAJ-PNV en Ibaeta y Antiguo. La coalición Bildu fue la fuerza más votada en la Parte Vieja, Añorga, Igueldo, Inchaurrondo, Eguía, Gros, Ulía y Martutene.

En 2015 el candidato del PNV Eneko Goia fue elegido alcalde con los votos de su partido y del PSE-EE.

La población de San Sebastián creció de manera progresiva a lo largo del siglo XX. Entre 1900 y 1930 el crecimiento fue regular, pasando a duplicarse en los apenas 35 años que separan 1930 de 1965. Este repunte en el crecimiento demográfico se vio atenuado por un menor crecimiento a partir de la década de los años 70, llegando a reducirse la población por primera vez en el siglo a finales de los años 80, como consecuencia de la caída generalizada de la natalidad en todo el país.
El crecimiento actual de la población es lento, si bien el fenómeno de la inmigración, aún incipiente en la ciudad (los inmigrantes, a 2006, llegan al 5 % de los empadronados, según la Sociedad de Fomento del Ayuntamiento de San Sebastián), puede incidir en un repunte del crecimiento demográfico. Según los últimos datos, a 1 de enero de 2009, la población total es de 185 357 habitantes, de los cuales 97 192 son mujeres (53 %) y 86 116 hombres (47 %).

El 59,86 % de la población de Guipúzcoa (sin contar la población de Hendaya) se concentra en su área metropolitana, que cuenta con 436 500 habitantes.

Desde 2003, el Ayuntamiento de San Sebastián divide la ciudad oficialmente en 17 barrios:

















Otros barrios tradicionalmente identificados por los donostiarras son considerados oficialmente por el ayuntamiento como parte del "Centro" de la ciudad.



San Sebastián posee tres enclaves:

Además de dichos exclaves, el Ayuntamiento de San Sebastián posee la finca de Articuza, situada en territorio navarro, dentro del término municipal de Goizueta. En él hay un embalse (es el punto más lluvioso de la península ibérica) y tiene un gran valor ecológico. Su superficie es de 37 km² (equivalente a más de la mitad del término municipal de San Sebastián).

A pesar de ciertas incursiones en el mundo de la banca en la segunda mitad del siglo XIX, con la creación del Banco de San Sebastián (que posteriormente se integraría en el Banco Hispano-Americano) o el Banco Guipuzcoano, la ciudad no destacará por su actividad bancaria, sino que lo hará en el sector del turismo. La elección de la ciudad como lugar de descanso y veraneo por parte de la Casa Real española fue la catalizadora del desarrollo de la actividad turística y de su consiguiente configuración arquitectónica afrancesada a partir del derribo de las murallas que limitaban la expansión de la ciudad. Algunos organismos fueron creados ya a comienzos del siglo XX para atraer al turismo, entre los que destacan la Sociedad de Fomento de San Sebastián, creada por iniciativa privada para la construcción de un hotel de lujo (el Hotel María Cristina) y de un teatro (el Teatro Victoria Eugenia). Aún hoy el turismo sigue siendo la principal actividad económica de San Sebastián, que sigue la misma estrategia de atracción de los turistas mediante reclamos como los festivales de verano.

También es importante el sector del comercio, una constante a lo largo de la historia de la ciudad. La actividad comercial es intensa en el Centro, sobre todo en la Avenida de la Libertad, con una gran concentración de entidades bancarias y comercios de importancia. Los comercios familiares del centro están siendo relegados, progresivamente, por grandes multinacionales, algunas de las cuales poseen varios locales en la ciudad. La proximidad con Francia atrae a numerosos visitantes, que llenan los comercios y las grandes superficies locales. En lo que a las últimas se refiere, en la ciudad existen cuatro, una en el barrio de Amara, dos en el centro y otra cuarta, la más grande, situada entre los barrios de Alza e Inchaurrondo. En cualquier caso, el fenómeno de las grandes superficies fue tardío, ya que no se abrió la primera de ellas hasta 1996.

La industria, por su parte, tiene poca presencia en la ciudad y se concentra en otros puntos de la provincia de Guipúzcoa.

Durante los años 2010 y 2011, y debido a la construcción de nuevas infraestructuras viarias como la GI-40, GI-41 y del Segundo cinturón de San Sebastián, la Diputación Foral de Guipúzcoa se vio obligada a renombrar los accesos y las circunvalaciones del área metropolitana de la ciudad.

Los accesos por carretera a la ciudad son los siguientes:

La ciudad consta de las siguientes circunvalaciones:

El carril bici, también llamado "bidegorri" ("camino rojo" en euskera, por ser ese el color del carril), es un medio de transporte que está creciendo mucho en el municipio. La red de carriles bici de San Sebastián supera los 56 kilómetros y está previsto ampliar dicha red hasta alcanzar una extensión suficiente como para poder recorrer en bicicleta toda la ciudad. El proyecto no ha tenido una acogida unánime: junto a quienes se felicitan por ella, hay quienes la critican porque dificulta el aparcamiento en la ciudad y crea en algunos lugares conflictos con los peatones. La red de carriles bici llega hasta los municipios colindantes (Lasarte, Pasaia y Astigarraga), enlazando con sus propias redes ciclistas.

El autobús urbano es el principal medio de transporte público municipal de San Sebastián. De dicho servicio se encarga, desde 1886, la Compañía del Tranvía de San Sebastián, que opera bajo el nombre comercial d·bus. En San Sebastián, el uso del autobús urbano por habitante es el más alto de España, dándose en 2015 un índice de 153 viajes por habitante al año. El servicio ofrece más de 30 líneas que abarcan toda la ciudad y un servicio de taxi bus para los barrios altos o a los que no pueden llegar los autobuses convencionales. También dispone de 9 líneas nocturnas para los viernes y sábados de madrugada, y líneas de refuerzo para los días de partido de fútbol y baloncesto.

Para llegar a San Sebastián desde otras localidades de la provincia, existen numerosas líneas de autobuses interurbanos integradas en Lurraldebus, la sociedad dependiente del Departamento de Movilidad y Ordenación del Territorio de la Diputación Foral de Guipúzcoa, entre las que se encuentran Autobuses Garayar, Autobuses Interurbanos Interbus, Autobuses La Guipuzcoana, EuskoTren, Herribus, Hijos de Antonio Areizaga, Transportes PESA y TSST. Las líneas enlazan la capital con el resto de la provincia y con otras ciudades del País Vasco como Bilbao, Lequeitio o Vitoria.

Las líneas interurbanas, nacionales e internaciones de autobuses tenían como destino la Estación de Atotxa, que se encuentra abajo y junto la Estación del Norte de Renfe.

En la ciudad existen dos redes diferenciadas: la de ancho métrico (dependiente de ETS) y la de ancho ibérico+UIC (dependiente de Adif).

Actualmente prestan servicio dos compañías: EuskoTren y Renfe .

La compañía estatal Renfe tiene la Estación del Norte como estación principal, y además tiene estaciones en Martutene, Loiola, Gros, Ategorrieta, Intxaurrondo y Herrera. De la Estación del Norte parten dos Alvia diarios a Madrid (por Valladolid), además de diversos servicios diarios de Intercity y otros nocturnos, a Madrid (por Pamplona), a Barcelona o a A Coruña. También paran aquí los trenes de Renfe Cercanías, que enlazan distintos puntos de la ciudad con diversos pueblos de Guipúzcoa. Está previsto que a esta estación llegue el futuro Tren de alta velocidad, en 2023. 

Sin embargo, en 2019 la estación será renovada para acomodar los trenes de alta velocidad procedentes de París; que podrán llegar hasta el municipio una vez se termine de renovar la vía a ancho mixto, lo que también permitirá trenes regionales a Bayona o Burdeos.

Además, a partir del 2020 se liberalizará el sector ferroviario para recorridos de larga distancia, por lo que habrá más compañías que operen en la ciudad.

La red de ancho métrico es operada por EuskoTren, que centraliza sus servicios en la Estación de Amara, situada en la Plaza Easo, y que además tiene también las estaciones de Errekalde, Añorga, Lugaritz, Anoeta, Loiola, Intxaurrondo, Herrera y Altza que forman parte de la línea del servicio de transporte metropolitano Metro Donostialdea, popularmente también llamado 'topo' debido a que gran parte de su trazado es subterráneo. Metro Donostialdea supone la modernización y ampliación de la línea E2 de EuskoTren. De la estación de Amara parten trenes en dirección a Lasarte-Oria, Irún y Francia. La última estación del metro es Hendaia, con lo que es habitual tomar este servicio para enlazar con el TGV dirección París.

EuskoTren, el Gobierno Vasco y la corporación local también han anunciado que, antes de 2019, se procederá a la construcción de una estación intermodal entre el Cercanías de San Sebastián operado por RENFE y el metro; así como a prolongar la línea hasta el centro de la ciudad para 2022, con la creación de las nuevas estaciones Centro-La Concha y Benta Berri, así como la renovación y sustitución de la actual estación central de Amara.

También parten desde dicha estación los servicios de cercanías-regionales hasta Éibar y Bilbao, haciendo paradas en muchos de los pueblos de la costa.

En total, tres líneas ofrecen sus servicios en San Sebastián:

El funicular de Igueldo, inaugurado en 1912, enlaza la playa de Ondarreta con el parque de atracciones en la cima del monte Igueldo.

Así mismo, se está construyendo un funicular moderno para subir al barrio de Aiete, que conectará el paseo de Morlans con la rotonda de Melodi. El proyecto también incluye la construcción de un ascensor para conectar el paseo Pío Baroja y el Paseo de Aiete. Está prevista su apertura el próximo invierno (2017-2018).

El municipio cuenta con numerosos ascensores y escaleras y rampas mecánicas en la ciudad, para facilitar el desplazamiento a los vecinos de los barrios altos, que representan al 50% de la población. Se trata de la 4ª ciudad de España con más infraestructura de movilidad vertical (Por detrás de Barcelona, Bilbao y Eibar). El plan de movilidad vertical del ayuntamiento tiene, asimismo, localizadas y ordenadas por prioridad futuras actuaciones para construir hasta 43 nuevos ascensores por la ciudad.



El Aeropuerto de San Sebastián, que se encuentra en la localidad fronteriza de Fuenterrabía, dispone de vuelos diarios a Madrid y Barcelona, además de otros destinos ocasionales. Según AENA el número de pasajeros en 2011 fue de 248.054, hubo 9.562 operaciones y se transportaron 31.966 kg de carga. La ausencia de líneas de bajo coste, así como sus reducidas dimensiones y la existencia de dos aeropuertos cercanos, limitan las posibilidades de uso del aeropuerto. La Diputación de Guipúzcoa y el Ayuntamiento de San Sebastián mantienen negociaciones con el Gobierno central para la ampliación de la pista. Actualmente operan Iberia y Vueling.

Los festivales son una de las principales características de la ciudad. Los certámenes de cine y música son numerosos en la ciudad, y algunos de ellos tienen gran prestigio internacional.

San Sebastián es Capital Europea de la Cultura en 2016. Este título le fue concedido oficialmente el 28 de junio de 2011, tras la evaluación del proyecto definitivo por parte del jurado español y europeo encargado de la deliberación. Culminaba así con éxito un proceso de tres años iniciado en 2008 cuando todos los grupos políticos de la ciudad apoyaron la iniciativa del entonces alcalde socialista Odón Elorza de presentar oficialmente la candidatura de la ciudad. En septiembre de 2010 la ciudad pasó la primera fase de selección, y finalmente alcanzó el título en junio de 2011. La capitalidad donostiarra explorará el rol de la cultura para la regeneración de la convivencia y la resolución de los problemas sociales derivados del terrorismo y de la división política y cultural de la sociedad donostiarra y, en general, de la sociedad vasca.


En lo que se ha dado en llamar Donosti Sound se engloba a los exitosos grupos de indie-pop/pop-rock surgidos en la ciudad durante las décadas de los 80 y los 90. Le Mans, La Buena Vida o Family, a los que a menudo se suma a Duncan Dhu (Mikel Erentxun y Diego Vasallo), 21 Japonesas, Álex Ubago o La Oreja de Van Gogh, son algunos de los grupos surgidos en San Sebastián en torno a este estilo musical, inequívocamente marcado por la climatología gris y la fisonomía aburguesada de la ciudad.




Se dice que San Sebastián es la ciudad del mundo con mayor número de estrellas Michelin por metro cuadrado. De hecho, es la única ciudad del mundo, junto con París, que posee tres restaurantes con tres estrellas, la máxima calificación. Así pues, la gastronomía es uno de los principales atractivos turísticos de la ciudad. Como representantes de la Nueva Cocina Vasca, prestigiosos cocineros como Juan Mari Arzak, Pedro Subijana o Martín Berasategui, los tres con las respectivas tres estrellas Michelín ya comentadas, tienen sus restaurantes en San Sebastián. También son muy populares los bares de "pintxos" de la Parte Vieja, obras de arte culinarias en miniatura.

Actualmente el restaurante Mugaritz está considerado como el cuarto mejor del mundo. Se encuentra entre Astigarraga y Rentería a escasos 10 km del centro de San Sebastián.

La principal fiesta de la ciudad es la Tamborrada, que se celebra el 20 de enero, día de San Sebastián. La noche del 19 al 20 de enero la Plaza de la Constitución de la Parte Vieja se llena de donostiarras alrededor del tablado en el que se sitúa la tamborrada de la Sociedad Gaztelubide, para realizar la izada de la bandera y comienzo de las fiestas. A lo largo de la mañana del día 20 desfila la Tamborrada Infantil, con más de medio centenar de compañías infantiles de centros escolares de San Sebastián y a lo largo de las 24 horas que dura la fiesta circula una centena de tamborradas de adultos. El día 20 a las doce de la noche, la Unión Artesana arría la bandera en la Plaza de la Constitución como fin de fiesta. Se trata de una fiesta con raíces históricas surgida a finales del XIX. La música que se interpreta, que incluye el himno de la ciudad (Marcha de San Sebastián, cuya letra fue obra de Serafín Baroja, padre del también donostiarra Pío Baroja), fue escrita por Raimundo Sarriegui, originalmente para piano, siendo adaptadas posteriormente para banda).

Entre la citada fiesta de San Sebastián y los carnavales existen distintas fiestas culturales y populares, entre las que cabe destacar la de los Caldereros, que se celebra el sábado más cercano a la Virgen de la Candelaria, que trata de recordar el paso de tribus nómadas por San Sebastián. Otra de estas festividades se celebra un día después de Caldereros, bajo el nombre de Iñudes eta Artzaiak, una fiesta completamente carnavelesca, donde se disfrazan, de alcalde, obispo, panadero, mikelete, cuidadoras, pastores...

En agosto, durante la semana del día 15 (la Asunción), se celebra la Semana Grande donostiarra, la gran fiesta veraniega de la ciudad. Entre las diversas actividades que se organizan, destacan el Concurso Internacional de Fuegos Artificiales y los desfiles de la comparsa de Gigantes y Cabezudos.

A finales de agosto y principio de septiembre se celebran las "Euskal Jaiak "(Fiestas Vascas), que se han celebrado bajo diferentes formas y no sin interrupciones desde la década de 1920. Son una suma de eventos culturales, deportivos y festivos relacionados con la cultura vasca que se programan a lo largo del último mes del verano. El plato fuerte de las mismas es la celebración de la Bandera de La Concha, la principal competición de traineras disputada en el Cantábrico. Esta excede el ámbito estrictamente deportivo, ya que la ciudad se llena de decenas de miles de seguidores de los equipos participantes en un ambiente de fiesta. Las tandas clasificatorias se celebran un jueves y la Bandera propiamente dicha los siguientes dos domingos en dos tandas. En el programa de las Euskal Jaiak se incluye la celebración del 31 de agosto en la que se recuerda el incendio que arrasó la ciudad en 1813, durante la Guerra de la Independencia, que dejó en pie una sola calle, la más antigua de la ciudad: la calle 31 de Agosto, de la Parte Vieja. Esta efeméride se conmemora con un conmovedor desfile de antorchas que se realiza por dicha calle. Otro punto fuerte es el <nowiki>"Sagardo Eguna"</nowiki> (Día de la Sidra) que suele celebrarse el sábado anterior al segundo domingo de regatas y que a la feria propiamente dicha une un gran número de actividades festivas paralelas.

El 21 de diciembre es el día de Santo Tomás. Durante este día se pueden ver por toda la ciudad puestos de productos artesanales entre los que destacan, por ser la comida típica del día, el talo, la chistorra (o txistorra, en euskera) y la sidra. Los puestos en cuestión suelen estar colocados en lugares como la Plaza de Guipúzcoa o la Plaza de la Constitución de la ciudad, y suelen estar atendidos por organizaciones o grupos de escolares.

El ocio nocturno de la ciudad se centra en varios puntos: la Parte Vieja, el entorno de la calle de los Reyes Católicos junto a la Catedral del Buen Pastor y las discotecas de la Bahía de la Concha y de la playa de la Zurriola.

En la Parte Vieja se concentran grupos variados de jóvenes de todas las edades, muchos de los cuales se desplazan a las discotecas de La Concha tras el cierre de los bares de dicha zona. Las tres discotecas situadas en la bahía acogen a todo tipo de público, si bien son consideradas discotecas de público con alto poder adquisitivo. Estas tres discotecas, principalmente las situadas en el edificio del Real Club Náutico de San Sebastián y junto al balneario de La Perla, acogen las fiestas del Festival Internacional de Cine. Otras opciones de ocio nocturno son el café situado en el Teatro Victoria Eugenia, la discoteca a orillas de la playa de la Zurriola, el centro de Ocio Illumbe formado por la plaza de toros y un centro comercial con pubs, restaurantes y discotecas o los mencionados bares del centro de la ciudad en el entorno de la Catedral.

Según datos de la Sociedad de Fomento del Ayuntamiento, casi el 70% de los donostiarras tiene estudios similares o superiores al bachillerato. El 26,6% dispone de algún título universitario o de estudios técnicos.

Al margen de los numerosos colegios privados de carácter religioso y laico y de las escuelas e institutos públicos, dependientes del Gobierno Vasco, la tradición musical de la ciudad pone de relieve el Conservatorio de Música Francisco Escudero, creado en 1879 y que posee una notable biblioteca musical con uno de los fondos históricos más importantes del país.

En lo que a la enseñanza superior universitaria se refiere, en la ciudad tienen presencia cuatro universidades y un conservatorio superior.


La investigación científica se está desarrollando de forma considerable principalmente en dos núcleos. En el campus donostiarra de la Universidad del País Vasco, donde tienen su sede la Facultad de Ciencias Químicas, la Escuela Universitaria Politécnica y cerca del cual se halla el Tecnun, Escuela Técnica Superior de Ingeniería de la Universidad de Navarra, se encuentra el Nanogune, centro de investigación de nanotecnología, y un centro de investigación mixto del Consejo Superior de Investigaciones Científicas (CSIC) y de la Universidad del País Vasco. Asimismo, en el campus se encuentra la sede del Donostia International Physics Center.

El otro núcleo principal de investigación es el Parque Tecnológico de Miramón, situado cerca del barrio de Ayete en un entorno natural. En él tienen su sede diversas empresas dedicadas a la investigación científica y tecnológica y el Centro de Estudios e Investigaciones Técnicas de Guipúzcoa (CEIT), y está previsto que nuevas empresas se vayan instalando en Miramón en los próximos años. Además de esta vertiente científica, el entorno de Miramón acoge también la sede de las Juntas Generales de Guipúzcoa y próximamente la nueva sede y auditorio del Orfeón Donostiarra.

La ciudad deportiva de San Sebastián, situada en el barrio de Amara, es Anoeta, donde se encuentra el Estadio de Anoeta (que sustituyó, en 1993, al Campo de Fútbol de Atocha), el Velódromo de Anoeta, una piscina olímpica, una piscina de ocio, varias canchas polideportivas polivalentes, un polideportivo, tres frontones, gimnasio, miniestadio, el Palacio de Hielo, un hotel, así como un circuito para la práctica del skate. Se trata de uno de los complejos deportivos más completos de España. Asimismo, existen otros polideportivos repartidos por los respectivos barrios (Antiguo, Ibaeta, Gros, Altza...). El Hipódromo de San Sebastián, también llamado Hipódromo de Lasarte, está situado en el barrio de Zubieta. Se trata del hipódromo más importante de España junto con el Hipódromo de la Zarzuela, en Madrid. Fue creado en 1916.

La Real Sociedad o «La Real», fundada en 1909 como sucesora directa del Club Ciclista de San Sebastián, es el equipo de fútbol de San Sebastián. Fue campeón de la Liga en dos ocasiones, ha ganado otros trofeos como la Copa del Rey y ha estado en varias ocasiones a las puertas de ganar otros campeonatos nacionales e internacionales. Después de unas décadas de fútbol en 1ª división, el club de la ciudad jugó en 2ª división desde la temporada 2007–-2008 hasta que consiguió el ascenso, quedando campeona de 2ª división en la temporada 2009-2010, por lo que desde la temporada 2010-2011 milita de nuevo en 1ª división.El campo de fútbol de La Real es el Estadio de Anoeta, pero los entrenamientos se desarrollan en las Instalaciones de Zubieta que el club posee en el barrio de Zubieta.

De la misma forma, la Sociedad Deportiva Lengokoak Kirol Elkartea es uno de los clubes convenidos con la Real Sociedad que mayor aportación de jugadores realiza a dicho club, entidad que cumplirá en 2013 su quincuagésimo aniversario y que ha estado desde su fundación fielmente ligada al club de referencia de Guipúzcoa. Entre los deportistas de renombre forjados en el Lengokoak podemos destacar a Luis Miguel Arconada y Javier Urruticoechea, llegando a convertirse ambos en figuras internacionales de la portería. David Zurutuza, actual centrocampista blanquiazul, también tuvo su origen en la S. D. Lengokoak K. E.

En el ámbito del baloncesto, el principal club es el San Sebastián GBC. El 23 de mayo de 2006, en el Polideportivo José Antonio Gasca de Anoeta, el Bruesa GBC, equipo profesional del San Sebastián Gipuzkoa Basket Club, aseguró su ascenso de la liga LEB-1 (de la que se proclamó campeón) a la liga ACB, con lo que el baloncesto donostiarra volvió a situar a un equipo local en la principal liga española y segunda más importante del mundo. Ese año, el equipo se mudó a la Plaza de Toros de Illumbe, recinto con una capacidad de 11.000 espectadores, que fue reacondicionada como cancha de baloncesto. Tras consumar su descenso a la liga LEB-1 en abril de 2007, volvió de nuevo a la máxima categoría en junio de 2008. Con el patrocinio de Seguros Lagun Aro el club donostiarra se ha asentado en la Liga ACB, en la que compite en la temporada 2011-2012 por cuarta temporada consecutiva, habiendo disputado por primera vez la Copa del Rey.

El C.D. Fortuna K.E. es un club deportivo fundado en 1911 dedicado a la promoción del deporte de base. Cuenta con numerosas secciones deportivas y organiza la popular carrera pedestre Behobia-San Sebastián. En el 2003 fue galardonado con la medalla al mérito ciudadano.

El Atlético San Sebastián es un club polideportivo fundado en 1958 de gran tradición e implantación social en la ciudad. Cuenta con equipos de hockey hierba masculino y atletismo femenino situados entre la élite del deporte español. En el pasado sus secciones de baloncesto masculino y rugby estuvieron también a primer nivel.

El Bera Bera Rugby Taldea es un club polideportivo fundado en 1986. Destacan sus equipos de rugby masculino: el Pegamo Bera Bera que juega en la Liga Española de rugby y el equipo de balonmano femenino, el Balonmano Bera Bera, que está en la Liga española de balonmano femenino y que ha sido dos veces campeona de la División de Honor de balonmano femenino y cuatro veces de la Copa de la Reina.

El Club Deportivo Egia Balonmano, que tiene su sede en el barrio de Egia, es el club de balonmano con más tradición de la ciudad. Tiene equipos masculinos en varias categorías, desde cadete hasta senior.

El remo donostiarra actualmente tiene como mayor representación la trainera del club Donostiarra, aunque en otras modalidades siguen en total actividad los históricos Ur-Kirolak y Donostia Arraun Lagunak.

El Real Golf Club de San Sebastián, fundado en 1910, tenía sus instalaciones en Lasarte-Oria y en 1969 se trasladaron a Fuenterrabía.


San Sebastián constituye un importante destino turístico tanto en el ámbito español como en el europeo. Son habituales las referencias periodísticas internacionales a las bondades turísticas de la ciudad. De hecho, y a modo de ejemplo, San Sebastián fue elegida por el periódico inglés "The Guardian" como «una de las cinco mejores ciudades de veraneo» del mundo, junto con Berlín, Estocolmo, Nueva York y Ámsterdam.

Uno de los principales atractivos turísticos de la ciudad es la gastronomía. También lo son los festivales de verano (Jazz, Quincena Musical y Cine). La bahía de La Concha, bordeada por su característica barandilla, es el símbolo turístico de San Sebastián. En el centro de la bahía se encuentra "la perla de La Concha", que es la isla de Santa Clara.
Junto a la playa de Ondarreta, y siguiendo hasta el final el paseo que bordea la bahía, se llega al "Peine del Viento", un conjunto escultórico elaborado por Eduardo Chillida y convertido en otro de los símbolos de la ciudad. Avanzando en sentido oeste por el paseo de La Concha se encuentra el singular Palacio de Miramar, construido en estilo inglés por la Casa Real española en 1893 y vendido al Ayuntamiento en los años setenta. Los jardines del palacio, abiertos al público, ofrecen unas espectaculares vistas a la bahía, al igual que el Parque de Atracciones Monte Igueldo, pequeño parque de atracciones de principios del siglo XX desde el que se obtienen unas vistas espléndidas de la bahía. Desde el "Peine del Viento" hasta Mompás, una salida de tierra al mar bajo el monte Ulía en el extremo oriental de la ciudad, recorriendo la bahía de La Concha, el pequeño puerto, el paseo Nuevo, la desembocadura del río y el paseo de la playa de la Zurriola, puede recorrerse un paseo marítimo de unos siete kilómetros de longitud sin cruzar un solo semáforo.

Los paseos por el centro de la ciudad, la denominada Área Romántica de la Belle Époque, cuyas calles principales están totalmente peatonalizadas, y junto al río Urumea, son otro de los puntos fuertes de la oferta turística de San Sebastián. Son reseñables los edificios de la Diputación Foral de Guipúzcoa (inspirado en el edificio de la Ópera de París), la Catedral del Buen Pastor y los edificios de Correos y el Centro Cultural Koldo Mitxelena, situados en la misma plaza, o el Ayuntamiento (antiguo casino). En la parte vieja son destacables el Museo San Telmo, la iglesia de Santa María y la parroquia de San Vicente. Junto a la desembocadura del río se encuentran el Teatro Victoria Eugenia y el Hotel María Cristina, que configuran uno de los conjuntos monumentales más atractivos de la ciudad. Cruzando el río por el puente de María Cristina, el más vistoso de los puentes donostiarras, se encuentran la estación del Norte y las villas de estilo puramente francés situadas al borde del río.

La ciudad de San Sebastián está hermanada con 9 ciudades.




</doc>
<doc id="2555" url="https://es.wikipedia.org/wiki?curid=2555" title="Software">
Software

Se conoce como software al soporte lógico de un sistema informático, que comprende el conjunto de los componentes lógicos necesarios que hacen posible la realización de tareas específicas, en contraposición a los componentes físicos que son llamados "hardware". La interacción entre el software y el hardware hace operativo un ordenador (u otro dispositivo), es decir, el Software envía instrucciones que el Hardware ejecuta, haciendo posible su funcionamiento.

Los componentes lógicos incluyen, entre muchos otros, las aplicaciones informáticas, tales como el procesador de texto, que permite al usuario realizar todas las tareas concernientes a la edición de textos; el llamado "software" de sistema, tal como el sistema operativo, que básicamente permite al resto de los programas funcionar adecuadamente, facilitando también la interacción entre los componentes físicos y el resto de las aplicaciones, y proporcionando una interfaz con el usuario. 

El software en su gran mayoría, está escrito en lenguajes de programación de alto nivel, ya que son más fáciles y eficientes para que los programadores los usen, porque son más cercanos al lenguaje natural respecto del lenguaje de máquina. Los lenguajes de alto nivel se traducen a lenguaje de máquina utilizando un compilador o un intérprete, o bien una combinación de ambos. El software también puede estar escrito en lenguaje ensamblador , que es de bajo nivel y tiene una alta correspondencia con las instrucciones de lenguaje máquina; se traduce al lenguaje de la máquina utilizando un ensamblador.

El anglicismo "software" es el más ampliamente difundido al referirse a este concepto, especialmente en la jerga técnica; en tanto que el término sinónimo «logicial», derivado del término francés "logiciel", es utilizado mayormente en países y zonas de influencia francesa. Su abreviatura es Sw.

"Software" (pronunciación AFI:) es una palabra proveniente del inglés, que en español no posee una traducción adecuada al contexto, por lo cual se la utiliza asiduamente sin traducir y así fue admitida por la Real Academia Española (RAE). Aunque puede no ser estrictamente lo mismo, suele sustituirse por expresiones tales como "programas (informáticos)" o "aplicaciones (informáticas)" o "soportes lógicos".

"Software" es lo que se denomina "producto" en ingeniería de "software".

Existen varias definiciones similares aceptadas para "software", pero probablemente la más formal sea la siguiente:

Considerando esta definición, el concepto de "software" va más allá de los programas de computación en sus distintos estados: código fuente, binario o ejecutable; también su documentación, los datos a procesar e incluso la información de usuario forman parte del "software": es decir, "abarca todo lo intangible", todo lo «no físico» relacionado.

El término "software" fue usado por primera vez en este sentido por John W. Tukey en 1957. En la ingeniería de "software" y las ciencias de la computación, el "software" es toda la información procesada por los sistemas informáticos: programas y datos.

El concepto de leer diferentes secuencias de instrucciones (programa) desde la memoria de un dispositivo para controlar los cálculos fue introducido por Charles Babbage como parte de su máquina diferencial. La teoría que forma la base de la mayor parte del "software" moderno fue propuesta por Alan Turing en su ensayo de 1936, «Los números computables», con una aplicación al problema de decisión.

Si bien esta distinción es, en cierto modo, arbitraria, y a veces confusa, a los fines prácticos se puede clasificar al "software" en tres tipos:


Se define como «proceso» al conjunto ordenado de pasos a seguir para llegar a la solución de un problema u obtención de un producto, en este caso particular, para lograr un producto "software" que resuelva un problema específico.

El proceso de creación de "software" puede llegar a ser muy complejo, dependiendo de su porte, características y criticidad del mismo. Por ejemplo la creación de un sistema operativo es una tarea que requiere proyecto, gestión, numerosos recursos y todo un equipo disciplinado de trabajo. En el otro extremo, si se trata de un sencillo programa (por ejemplo, la resolución de una ecuación de segundo orden), éste puede ser realizado por un solo programador (incluso aficionado) fácilmente. Es así que normalmente se dividen en tres categorías según su tamaño (líneas de código) o costo: de «pequeño», «mediano» y «gran porte». Existen varias metodologías para estimarlo, una de las más populares es el sistema COCOMO que provee métodos y un "software" (programa) que calcula y provee una aproximación de todos los costos de producción en un «proyecto "software"» (relación horas/hombre, costo monetario, cantidad de líneas fuente de acuerdo a lenguaje usado, etc.).

Considerando los de gran porte, es necesario realizar complejas tareas, tanto técnicas como de gerencia, una fuerte gestión y análisis diversos (entre otras cosas), la complejidad de ello ha llevado a que desarrolle una ingeniería específica para tratar su estudio y realización: es conocida como ingeniería de "Software".

En tanto que en los de mediano porte, pequeños equipos de trabajo (incluso un avezado analista-programador solitario) pueden realizar la tarea. Aunque, siempre en casos de mediano y gran porte (y a veces también en algunos de pequeño porte, según su complejidad), se deben seguir ciertas etapas que son necesarias para la construcción del "software". Tales etapas, si bien deben existir, son flexibles en su forma de aplicación, de acuerdo a la metodología o proceso de desarrollo escogido y utilizado por el equipo de desarrollo o por el analista-programador solitario (si fuere el caso).

Los «procesos de desarrollo de "software"» poseen reglas preestablecidas, y deben ser aplicados en la creación del "software" de mediano y gran porte, ya que en caso contrario lo más seguro es que el proyecto no logre concluir o termine sin cumplir los objetivos previstos, y con variedad de fallos inaceptables (fracasan, en pocas palabras). Entre tales «procesos» los hay ágiles o livianos (ejemplo XP), pesados y lentos (ejemplo RUP), y variantes intermedias. Normalmente se aplican de acuerdo al tipo y porte del "software" a desarrollar, a criterio del líder (si lo hay) del equipo de desarrollo. Algunos de esos procesos son Programación Extrema (en inglés "eXtreme Programming" o XP), Proceso Unificado de Rational (en inglés Rational Unified Process o RUP), Feature Driven Development (FDD), etc.

Cualquiera sea el «proceso» utilizado y aplicado al desarrollo del "software" (RUP, FDD, XP, etc), y casi independientemente de él, siempre se debe aplicar un «modelo de ciclo de vida».

Se estima que, del total de proyectos "software" grandes emprendidos, un 28 % fracasan, un 46 % caen en severas modificaciones que lo retrasan y un 26 % son totalmente exitosos.

Cuando un proyecto fracasa, rara vez es debido a fallas técnicas, la principal causa de fallos y fracasos es la falta de aplicación de una buena metodología o proceso de desarrollo. Entre otras, una fuerte tendencia, desde hace pocas décadas, es mejorar las metodologías o procesos de desarrollo, o crear nuevas y concientizar a los profesionales de la informática a su utilización adecuada. Normalmente los especialistas en el estudio y desarrollo de estas áreas (metodologías) y afines (tales como modelos y hasta la gestión misma de los proyectos) son los ingenieros en "software", es su orientación. Los especialistas en cualquier otra área de desarrollo informático (analista, programador, Lic. en informática, ingeniero en informática, ingeniero de sistemas, etc.) normalmente aplican sus conocimientos especializados pero utilizando modelos, paradigmas y procesos ya elaborados.

Es común para el desarrollo de "software" de mediano porte que los equipos humanos involucrados apliquen «metodologías propias», normalmente un híbrido de los procesos anteriores y a veces con criterios propios.

El proceso de desarrollo puede involucrar numerosas y variadas tareas, desde lo administrativo, pasando por lo técnico y hasta la gestión y el gerenciamiento. Pero, casi rigurosamente, siempre se cumplen ciertas etapas mínimas; las que se pueden resumir como sigue:


En las anteriores etapas pueden variar ligeramente sus nombres, o ser más globales, o contrariamente, ser más refinadas; por ejemplo indicar como una única fase (a los fines documentales e interpretativos) de «análisis y diseño»; o indicar como «implementación» lo que está dicho como «codificación»; pero en rigor, todas existen e incluyen, básicamente, las mismas tareas específicas.

En el apartado 4 del presente artículo se brindan mayores detalles de cada una de las etapas indicadas.

Para cada una de las fases o etapas listadas en el ítem anterior, existen sub-etapas (o tareas).
El modelo de proceso o modelo de ciclo de vida utilizado para el desarrollo, define el orden de las tareas o actividades involucradas, también define la coordinación entre ellas, y su enlace y realimentación. Entre los más conocidos se puede mencionar: modelo en cascada o secuencial, modelo espiral, modelo iterativo incremental. De los antedichos hay a su vez algunas variantes o alternativas, más o menos atractivas según sea la aplicación requerida y sus requisitos.

Este, aunque es más comúnmente conocido como modelo en cascada es también llamado «modelo clásico», «modelo tradicional» o «modelo lineal secuencial».

El modelo en cascada puro «difícilmente se utiliza tal cual», pues esto implicaría un previo y "absoluto" conocimiento de los requisitos, la no volatilidad de los mismos (o rigidez) y etapas subsiguientes libres de errores; ello sólo podría ser aplicable a escasos y pequeños sistemas a desarrollar. En estas circunstancias, el paso de una etapa a otra de las mencionadas sería sin retorno, por ejemplo pasar del diseño a la codificación implicaría un diseño exacto y sin errores ni probable modificación o evolución: «codifique lo diseñado sin errores, no habrá en absoluto variantes futuras». Esto es utópico; ya que intrínsecamente «el "software" es de carácter evolutivo», cambiante y difícilmente libre de errores, tanto durante su desarrollo como durante su vida operativa.

Algún cambio durante la ejecución de una cualquiera de las etapas en este modelo secuencial implicaría reiniciar desde el principio todo el ciclo completo, lo cual redundaría en altos costos de tiempo y desarrollo. La Figura 2 muestra un posible esquema del modelo en cuestión.

Sin embargo, el modelo cascada en algunas de sus variantes es uno de los actualmente "más utilizados", por su eficacia y simplicidad, más que nada en "software" de pequeño y algunos de mediano porte; pero nunca (o muy rara vez) se lo usa en su "forma pura", como se dijo anteriormente. En lugar de ello, siempre se produce alguna realimentación entre etapas, que no es completamente predecible ni rígida; esto da oportunidad al desarrollo de productos "software" en los cuales hay ciertas incertezas, cambios o evoluciones durante el ciclo de vida. Así por ejemplo, una vez capturados y especificados los requisitos (primera etapa) se puede pasar al diseño del sistema, pero durante esta última fase lo más probable es que se deban realizar ajustes en los requisitos (aunque sean mínimos), ya sea por fallas detectadas, ambigüedades o bien por que los propios requisitos han cambiado o evolucionado; con lo cual se debe retornar a la primera o previa etapa, hacer los reajustes pertinentes y luego continuar nuevamente con el diseño; esto último se conoce como realimentación. Lo normal en el modelo cascada es entonces la aplicación del mismo con sus etapas realimentadas de alguna forma, permitiendo retroceder de una a la anterior (e incluso poder saltar a varias anteriores) si es requerido.

De esta manera se obtiene el «modelo cascada realimentado», que puede ser esquematizado como lo ilustra la Figura 3.

Lo dicho es, a grandes rasgos, la forma y utilización de este modelo, uno de los más usados y populares. El modelo cascada realimentado resulta muy atractivo, hasta ideal, si el proyecto presenta alta rigidez (pocos cambios, previsto no evolutivo), los requisitos son muy claros y están correctamente especificados.

Hay más variantes similares al modelo: refino de etapas (más etapas, menores y más específicas) o incluso mostrar menos etapas de las indicadas, aunque en tal caso la faltante estará dentro de alguna otra. El orden de esas fases indicadas en el ítem previo es el lógico y adecuado, pero adviértase, como se dijo, que normalmente habrá realimentación hacia atrás.

El modelo lineal o en cascada es el paradigma más antiguo y extensamente utilizado, sin embargo las críticas a él (ver desventajas) han puesto en duda su eficacia. Pese a todo, tiene un lugar muy importante en la ingeniería de "software" y continúa siendo el más utilizado; y siempre es mejor que un enfoque al azar.

Desventajas del modelo cascada:


El "software" evoluciona con el tiempo. Los requisitos del usuario y del producto suelen cambiar conforme se desarrolla el mismo. Las fechas de mercado y la competencia hacen que no sea posible esperar a poner en el mercado un producto absolutamente completo, por lo que se aconseja introducir una versión funcional limitada de alguna forma para aliviar las presiones competitivas.

En esas u otras situaciones similares, los desarrolladores necesitan modelos de progreso que estén diseñados para acomodarse a una evolución temporal o progresiva, donde los requisitos centrales son conocidos de antemano, aunque no estén bien definidos a nivel detalle.

En el modelo cascada y cascada realimentado no se tiene demasiado en cuenta la naturaleza evolutiva del "software", se plantea como estático, con requisitos bien conocidos y definidos desde el inicio.

Los evolutivos son modelos iterativos, permiten desarrollar versiones cada vez más completas y complejas, hasta llegar al objetivo final deseado; incluso evolucionar más allá, durante la fase de operación.

Los modelos «iterativo incremental» y «espiral» (entre otros) son dos de los más conocidos y utilizados del tipo evolutivo.

En términos generales, se puede distinguir, en la figura 4, los pasos generales que sigue el proceso de desarrollo de un producto "software". En el modelo de ciclo de vida seleccionado, se identifican claramente dichos pasos. La descripción del sistema es esencial para especificar y confeccionar los distintos incrementos hasta llegar al producto global y final. Las actividades concurrentes (especificación, desarrollo y validación) sintetizan el desarrollo pormenorizado de los incrementos, que se hará posteriormente.

El diagrama de la figura 4 muestra en forma muy esquemática, el funcionamiento de un ciclo iterativo incremental, el cual permite la entrega de versiones parciales a medida que se va construyendo el producto final. Es decir, a medida que cada incremento definido llega a su etapa de operación y mantenimiento. Cada versión emitida incorpora a los anteriores incrementos las funcionalidades y requisitos que fueron analizados como necesarios.

"El incremental es un modelo de tipo evolutivo que está basado en varios ciclos cascada realimentados aplicados repetidamente, con una filosofía iterativa."
En la figura 5 se muestra un refino del diagrama previo, bajo un esquema temporal, para obtener finalmente el esquema del modelo de ciclo de vida iterativo incremental, con sus actividades genéricas asociadas. Aquí se observa claramente cada ciclo cascada que es aplicado para la obtención de un incremento; estos últimos se van integrando para obtener el producto final completo. Cada incremento es un ciclo cascada realimentado, aunque, por simplicidad, en la figura 5 se muestra como secuencial puro.

Se observa que existen actividades de desarrollo (para cada incremento) que son realizadas en paralelo o concurrentemente, así por ejemplo, en la Figura, mientras se realiza el diseño detalle del primer incremento ya se está realizando en análisis del segundo. La Figura 5 es sólo esquemática, un incremento no necesariamente se iniciará durante la fase de diseño del anterior, puede ser posterior (incluso antes), en cualquier tiempo de la etapa previa. Cada incremento concluye con la actividad de «operación y mantenimiento» (indicada como «Operación» en la figura), que es donde se produce la entrega del producto parcial al cliente. El momento de inicio de cada incremento es dependiente de varios factores: tipo de sistema; independencia o dependencia entre incrementos (dos de ellos totalmente independientes pueden ser fácilmente iniciados al mismo tiempo si se dispone de personal suficiente); capacidad y cantidad de profesionales involucrados en el desarrollo; etc.

Bajo este modelo se entrega "software" «por partes funcionales más pequeñas», pero reutilizables, llamadas incrementos. En general cada incremento se construye sobre aquel que ya fue entregado.

Como se muestra en la Figura 5, se aplican secuencias Cascada en forma escalonada, mientras progresa el tiempo calendario. Cada secuencia lineal o Cascada produce un incremento y a menudo el primer incremento es un sistema básico, con muchas funciones suplementarias (conocidas o no) sin entregar.

El cliente utiliza inicialmente ese sistema básico, intertanto, el resultado de su uso y evaluación puede aportar al plan para el desarrollo del/los siguientes incrementos (o versiones). Además también aportan a ese plan otros factores, como lo es la priorización (mayor o menor urgencia en la necesidad de cada incremento en particular) y la dependencia entre incrementos (o independencia).

Luego de cada integración se entrega un producto con mayor funcionalidad que el previo. El proceso se repite hasta alcanzar el "software" final completo.

Siendo iterativo, "con el modelo incremental se entrega un producto parcial pero completamente operacional en cada incremento", y no una parte que sea usada para reajustar los requisitos (como si ocurre en el modelo de construcción de prototipos).

El enfoque incremental resulta muy útil cuando se dispone de baja dotación de personal para el desarrollo; también si no hay disponible fecha límite del proyecto por lo que se entregan versiones incompletas pero que proporcionan al usuario funcionalidad básica (y cada vez mayor). También es un modelo útil a los fines de versiones de evaluación.

Nota: Puede ser considerado y útil, en cualquier momento o incremento incorporar temporalmente el paradigma MCP como complemento, teniendo así una mixtura de modelos que mejoran el esquema y desarrollo general.

Ejemplo:

Como se dijo, el iterativo incremental es un modelo del tipo evolutivo, es decir donde se permiten y esperan probables cambios en los requisitos en tiempo de desarrollo; se admite cierto margen para que el "software" pueda evolucionar. Aplicable cuando los requisitos son medianamente bien conocidos pero no son completamente estáticos y definidos, cuestión esa que si es indispensable para poder utilizar un modelo Cascada.

El modelo es aconsejable para el desarrollo de "software" en el cual se observe, en su etapa inicial de análisis, que posee áreas bastante bien definidas a cubrir, con suficiente independencia como para ser desarrolladas en etapas sucesivas. Tales áreas a cubrir suelen tener distintos grados de apremio por lo cual las mismas se deben priorizar en un análisis previo, es decir, definir cual será la primera, la segunda, y así sucesivamente; esto se conoce como «definición de los incrementos» con base en la priorización. Pueden no existir prioridades funcionales por parte del cliente, pero el desarrollador debe fijarlas de todos modos y con algún criterio, ya que basándose en ellas se desarrollarán y entregarán los distintos incrementos.

El hecho de que existan incrementos funcionales del "software" lleva inmediatamente a pensar en un esquema de desarrollo modular, por tanto este modelo facilita tal paradigma de diseño.

En resumen, un modelo incremental lleva a pensar en un desarrollo modular, con entregas parciales del producto "software" denominados «incrementos» del sistema, que son escogidos según prioridades predefinidas de algún modo. El modelo permite una implementación con refinamientos sucesivos (ampliación o mejora). Con cada incremento se agrega nueva funcionalidad o se cubren nuevos requisitos o bien se mejora la versión previamente implementada del producto "software".

Este modelo brinda cierta flexibilidad para que durante el desarrollo se incluyan cambios en los requisitos por parte del usuario, un cambio de requisitos propuesto y aprobado puede analizarse e implementarse como un nuevo incremento o, eventualmente, podrá constituir una mejora/adecuación de uno ya planeado. Aunque si se produce un cambio de requisitos por parte del cliente que afecte incrementos previos ya terminados (detección/incorporación tardía) "se debe evaluar la factibilidad y realizar un acuerdo con el cliente, ya que puede impactar fuertemente en los costos."

La selección de este modelo permite realizar entregas funcionales tempranas al cliente (lo cual es beneficioso tanto para él como para el grupo de desarrollo). Se priorizan las entregas de aquellos módulos o incrementos en que surja la necesidad operativa de hacerlo, por ejemplo para cargas previas de información, indispensable para los incrementos siguientes.

El modelo iterativo incremental no obliga a especificar con precisión y detalle absolutamente todo lo que el sistema debe hacer, (y cómo), antes de ser construido (como el caso del cascada, con requisitos congelados). Sólo se hace en el incremento en desarrollo. Esto torna más manejable el proceso y reduce el impacto en los costos. Esto es así, porque en caso de alterar o rehacer los requisitos, solo afecta una parte del sistema. Aunque, lógicamente, esta situación se agrava si se presenta en estado avanzado, es decir en los últimos incrementos. "En definitiva, el modelo facilita la incorporación de nuevos requisitos durante el desarrollo."

Con un paradigma incremental se reduce el tiempo de desarrollo inicial, ya que se implementa funcionalidad parcial. También provee un impacto ventajoso frente al cliente, que es la entrega temprana de partes operativas del "software".

El modelo proporciona todas las ventajas del modelo en cascada realimentado, reduciendo sus desventajas sólo al ámbito de cada incremento.

El modelo incremental no es recomendable para casos de sistemas de tiempo real, de alto nivel de seguridad, de procesamiento distribuido, o de alto índice de riesgos.

El modelo espiral fue propuesto inicialmente por Barry Boehm. Es un modelo evolutivo que conjuga la naturaleza iterativa del modelo MCP con los aspectos controlados y sistemáticos del Modelo Cascada. Proporciona potencial para desarrollo rápido de versiones incrementales. En el modelo espiral el "software" se construye en una serie de versiones incrementales. En las primeras iteraciones la versión incremental podría ser un modelo en papel o bien un prototipo. En las últimas iteraciones se producen versiones cada vez más completas del sistema diseñado.

El modelo se divide en un número de Actividades de marco de trabajo, llamadas «regiones de tareas». En general existen entre tres y seis regiones de tareas (hay variantes del modelo). En la figura 6 se muestra el esquema de un modelo espiral con seis regiones. En este caso se explica una variante del modelo original de Boehm, expuesto en su tratado de 1988; en 1998 expuso un tratado más reciente.

Las regiones definidas en el modelo de la figura son:


Las actividades enunciadas para el marco de trabajo son generales y se aplican a cualquier proyecto, grande, mediano o pequeño, complejo o no. Las regiones que definen esas actividades comprenden un «conjunto de tareas» del trabajo: ese conjunto sí se debe adaptar a las características del proyecto en particular a emprender. Nótese que lo listado en los ítems de 1 a 6 son conjuntos de tareas, algunas de las ellas normalmente dependen del proyecto o desarrollo en si.

Proyectos pequeños requieren baja cantidad de tareas y también de formalidad. En proyectos mayores o críticos cada región de tareas contiene labores de más alto nivel de formalidad. En cualquier caso se aplican actividades de protección (por ejemplo, gestión de configuración del "software", garantía de calidad, etc.).

Al inicio del ciclo, o proceso evolutivo, el equipo de ingeniería gira alrededor del espiral (metafóricamente hablando) comenzando por el centro (marcado con ๑ en la figura 6) y en el sentido indicado; el primer circuito de la espiral puede producir el desarrollo de una especificación del producto; los pasos siguientes podrían generar un prototipo y progresivamente versiones más sofisticadas del "software".

Cada paso por la región de planificación provoca ajustes en el plan del proyecto; el coste y planificación se realimentan en función de la evaluación del cliente. El gestor de proyectos debe ajustar el número de iteraciones requeridas para completar el desarrollo.

El modelo espiral puede ir adaptándose y aplicarse a lo largo de todo el Ciclo de vida del "software" (en el modelo clásico, o cascada, el proceso termina a la entrega del "software").

Una visión alternativa del modelo puede observarse examinando el «eje de punto de entrada de proyectos». Cada uno de los circulitos (๏) fijados a lo largo del eje representan puntos de arranque de los distintos proyectos (relacionados); a saber:


Cuando la espiral se caracteriza de esta forma, está operativa hasta que el "software" se retira, eventualmente puede estar inactiva (el proceso), pero cuando se produce un cambio el proceso arranca nuevamente en el punto de entrada apropiado (por ejemplo, en «mejora del producto»).

El modelo espiral da un enfoque realista, que evoluciona igual que el "software"; se adapta muy bien para desarrollos a gran escala.

El Espiral utiliza el MCP para reducir riesgos y permite aplicarlo en cualquier etapa de la evolución. Mantiene el enfoque clásico (cascada) pero incorpora un marco de trabajo iterativo que refleja mejor la realidad.

Este modelo "requiere considerar riesgos técnicos" en todas las etapas del proyecto; aplicado adecuadamente debe reducirlos antes de que sean un verdadero problema.

El Modelo evolutivo como el Espiral es particularmente apto para el desarrollo de Sistemas Operativos (complejos); también en sistemas de altos riesgos o críticos (Ej. navegadores y controladores aeronáuticos) y en todos aquellos en que sea necesaria una fuerte gestión del proyecto y sus riesgos, técnicos o de gestión.

Desventajas importantes:


Este modelo no se ha usado tanto, como el Cascada (Incremental) o MCP, por lo que no se tiene bien medida su eficacia, es un paradigma relativamente nuevo y difícil de implementar y controlar.

Una variante interesante del Modelo Espiral previamente visto (Figura 6) es el «Modelo espiral Win-Win» (Barry Boehm). El Modelo Espiral previo (clásico) sugiere la comunicación con el cliente para fijar los requisitos, en que simplemente se pregunta al cliente qué necesita y él proporciona la información para continuar; pero esto es en un contexto ideal que rara vez ocurre. Normalmente cliente y desarrollador entran en una negociación, se negocia coste frente a funcionalidad, rendimiento, calidad, etc.

"«Es así que la obtención de requisitos requiere una negociación, que tiene éxito cuando ambas partes ganan»."

Las mejores negociaciones se fuerzan en obtener «Victoria & Victoria» (Win & Win), es decir que el cliente gane obteniendo el producto que lo satisfaga, y el desarrollador también gane consiguiendo presupuesto y fecha de entrega realista. Evidentemente, este modelo requiere fuertes habilidades de negociación.

El modelo Win-Win define un conjunto de actividades de negociación al principio de cada paso alrededor de la espiral; se definen las siguientes actividades:


<nowiki>*</nowiki> Directivo: Cliente escogido con interés directo en el producto, que puede ser premiado por la organización si tiene éxito o criticado si no.

El modelo Win & Win hace énfasis en la negociación inicial, también introduce 3 hitos en el proceso llamados «puntos de fijación», que ayudan a establecer la completitud de un ciclo de la espiral, y proporcionan hitos de decisión antes de continuar el proyecto de desarrollo del "software".

Al inicio de un desarrollo (no de un proyecto), esta es la primera fase que se realiza, y, según el modelo de proceso adoptado, puede casi terminar para pasar a la próxima etapa (caso de Modelo Cascada Realimentado) o puede hacerse parcialmente para luego retomarla (caso Modelo Iterativo Incremental u otros de carácter evolutivo).

En simple palabras y básicamente, durante esta fase, se adquieren, reúnen y especifican las características funcionales y no funcionales que deberá cumplir el futuro programa o sistema a desarrollar.

Las bondades de las características, tanto del sistema o programa a desarrollar, como de su entorno, parámetros no funcionales y arquitectura dependen enormemente de lo bien lograda que esté esta etapa. Esta es, probablemente, la de mayor importancia y una de las fases más difíciles de lograr certeramente, pues no es automatizable, no es muy técnica y depende en gran medida de la habilidad y experiencia del analista que la realice.

Involucra fuertemente al usuario o cliente del sistema, por tanto tiene matices muy subjetivos y es difícil de modelar con certeza o aplicar una técnica que sea «la más cercana a la adecuada» (de hecho no existe «la estrictamente adecuada»). Si bien se han ideado varias metodologías, incluso "software" de apoyo, para captura, elicitación y registro de requisitos, no existe una forma infalible o absolutamente confiable, y deben aplicarse conjuntamente buenos criterios y mucho sentido común por parte del o los analistas encargados de la tarea; es fundamental también lograr una fluida y adecuada comunicación y comprensión con el usuario final o cliente del sistema.

El artefacto más importante resultado de la culminación de esta etapa es lo que se conoce como especificación de requisitos "software" o simplemente documento ERS.

Como se dijo, la habilidad del analista para interactuar con el cliente es fundamental; lo común es que el cliente tenga un objetivo general o problema que resolver, no conoce en absoluto el área (informática), ni su jerga, ni siquiera sabe con precisión qué debería hacer el producto "software" (qué y cuantas funciones) ni, mucho menos, cómo debe operar. En otros casos menos frecuentes, el cliente «piensa» que sabe precisamente lo que el "software" tiene que hacer, y generalmente acierta muy parcialmente, pero su empecinamiento entorpece la tarea de elicitación. El analista debe tener la capacidad para lidiar con este tipo de problemas, que incluyen relaciones humanas; tiene que saber ponerse al nivel del usuario para permitir una adecuada comunicación y comprensión.

Escasas son las situaciones en que el cliente sabe con certeza e incluso con completitud lo que requiere de su futuro sistema, este es el caso más sencillo para el analista.

Las tareas relativas a captura, elicitación, modelado y registro de requisitos, además de ser sumamente importante, puede llegar a ser dificultosa de lograr acertadamente y llevar bastante tiempo relativo al proceso total del desarrollo; al proceso y metodologías para llevar a cabo este conjunto de actividades normalmente se las asume parte propia de la ingeniería de "software", pero dada la antedicha complejidad, actualmente se habla de una ingeniería de requisitos, aunque ella aún no existe formalmente.

Hay grupos de estudio e investigación, en todo el mundo, que están exclusivamente abocados a idear modelos, técnicas y procesos para intentar lograr la correcta captura, análisis y registro de requisitos. Estos grupos son los que normalmente hablan de la ingeniería de requisitos; es decir se plantea ésta como un área o disciplina pero no como una carrera universitaria en si misma.

Algunos requisitos no necesitan la presencia del cliente, para ser capturados o analizados; en ciertos casos los puede proponer el mismo analista o, incluso, adoptar unilateralmente decisiones que considera adecuadas (tanto en requisitos funcionales como no funcionales). Por citar ejemplos probables: Algunos requisitos sobre la arquitectura del sistema, requisitos no funcionales tales como los relativos al rendimiento, nivel de soporte a errores operativos, plataformas de desarrollo, relaciones internas o ligas entre la información (entre registros o tablas de datos) a almacenar en caso de bases o bancos de datos, etc. Algunos funcionales tales como opciones secundarias o de soporte necesarias para una mejor o más sencilla operatividad; etc.

La obtención de especificaciones a partir del cliente (u otros actores intervinientes) es un proceso humano muy interactivo e iterativo; normalmente a medida que se captura la información, se la analiza y realimenta con el cliente, refinándola, puliéndola y corrigiendo si es necesario; cualquiera sea el método de ERS utilizado. EL analista siempre debe llegar a conocer la temática y el problema que resolver, dominarlo, hasta cierto punto, hasta el ámbito que el futuro sistema a desarrollar lo abarque. Por ello el analista debe tener alta capacidad para comprender problemas de muy diversas áreas o disciplinas de trabajo (que no son específicamente suyas); así por ejemplo, si el sistema a desarrollar será para gestionar información de una aseguradora y sus sucursales remotas, el analista se debe compenetrar en cómo ella trabaja y maneja su información, desde niveles muy bajos e incluso llegando hasta los gerenciales. Dada a gran diversidad de campos a cubrir, los analistas suelen ser asistidos por especialistas, es decir gente que conoce profundamente el área para la cual se desarrollará el "software"; evidentemente una única persona (el analista) no puede abarcar tan vasta cantidad de áreas del conocimiento. En empresas grandes de desarrollo de productos "software", es común tener analistas especializados en ciertas áreas de trabajo.

Contrariamente, no es problema del cliente, es decir él no tiene por qué saber nada de "software", ni de diseños, ni otras cosas relacionadas; sólo se debe limitar a aportar objetivos, datos e información (de mano propia o de sus registros, equipos, empleados, etc) al analista, y guiado por él, para que, en primera instancia, defina el «Universo de Discurso», y con posterior trabajo logre confeccionar el adecuado documento ERS.

Es bien conocida la presión que sufren los desarrolladores de sistemas informáticos para comprender y rescatar las necesidades de los clientes/usuarios. Cuanto más complejo es el contexto del problema más difícil es lograrlo, a veces se fuerza a los desarrolladores a tener que convertirse en casi expertos de los dominios que analizan.

Cuando esto no sucede es muy probable que se genere un conjunto de requisitos erróneos o incompletos y por lo tanto un producto de "software" con alto grado de desaprobación por parte de los clientes/usuarios y un altísimo costo de reingeniería y mantenimiento. "Todo aquello que no se detecte, o resulte mal entendido en la etapa inicial provocará un fuerte impacto negativo en los requisitos, propagando esta corriente degradante a lo largo de todo el proceso de desarrollo e incrementando su perjuicio cuanto más tardía sea su detección" (Bell y Thayer 1976)(Davis 1993).

Siendo que la captura, elicitación y especificación de requisitos, es una parte crucial en el proceso de desarrollo de "software", ya que de esta etapa depende el logro de los objetivos finales previstos, se han ideado modelos y diversas metodologías de trabajo para estos fines. También existen herramientas "software" que apoyan las tareas relativas realizadas por el ingeniero en requisitos.

El estándar IEEE 830-1998 brinda una normalización de las «Prácticas recomendadas para la especificación de requisitos "software"».

A medida que se obtienen los requisitos, normalmente se los va analizando, el resultado de este análisis, con o sin el cliente, se plasma en un documento, conocido como ERS o Especificación de requisitos "software", cuya estructura puede venir definida por varios estándares, tales como CMMI.

Un primer paso para realizar el relevamiento de información es el conocimiento y definición acertada lo que se conoce como «Universo de Discurso» del problema, que se define y entiende por:

Universo de Discurso (UdeD): es el contexto general en el cual el "software" deberá ser desarrollado y deberá operar. El UdeD incluye todas las fuentes de información y todas las personas relacionadas con el "software". Esas personas son conocidas también como actores de ese universo. El UdeD es la realidad circunstanciada por el conjunto de objetivos definidos por quienes demandaron el "software".

A partir de la extracción y análisis de información en su ámbito se obtienen todas las especificaciones necesarias y tipos de requisitos para el futuro producto "software".

El objetivo de la ingeniería de requisitos (IR) es sistematizar el proceso de definición de requisitos permitiendo elicitar, modelar y analizar el problema, generando un compromiso entre los ingenieros de requisitos y los clientes/usuarios, ya que ambos participan en la generación y definición de los requisitos del sistema. La IR aporta un conjunto de métodos, técnicas y herramientas que asisten a los ingenieros de requisitos (analistas) para obtener requisitos lo más seguros, veraces, completos y oportunos posibles, permitiendo básicamente:


Si bien existen diversas formas, modelos y metodologías para elicitar, definir y documentar requisitos, no se puede decir que alguna de ellas sea mejor o peor que la otra, suelen tener muchísimo en común, y todas cumplen el mismo objetivo. Sin embargo, lo que si se puede decir sin dudas es
que es indispensable utilizar alguna de ellas para documentar las especificaciones del futuro producto "software". Así por ejemplo, hay un grupo de investigación argentino que desde hace varios años ha propuesto y estudia el uso del LEL (Léxico Extendido del Lenguaje) y Escenarios como metodología, aquí se presenta una de las tantas referencias y bibliografía sobre ello. Otra forma, más ortodoxa, de capturar y documentar requisitos se puede obtener en detalle, por ejemplo, en el trabajo de la Universidad de Sevilla sobre «Metodología para el Análisis de Requisitos de Sistemas Software».

En la Figura 7 se muestra un esquema, más o menos riguroso, aunque no detallado, de los pasos y tareas a seguir para realizar la captura, análisis y especificación de requisitos "software". También allí se observa qué artefacto o documento se obtiene en cada etapa del proceso. En el diagrama no se explicita metodología o modelo a utilizar, sencillamente se pautan las tareas que deben cumplirse, de alguna manera.

Una posible lista, general y ordenada, de tareas recomendadas para obtener la definición de lo que se debe realizar, los productos a obtener y las técnicas a emplear durante la actividad de elicitación de requisitos, en fase de Especificación de requisitos "software" es:


Algunos principios básicos a tener en cuenta:


Se pueden identificar dos formas de requisitos:


Es decir, ambos son lo mismo, pero con distinto nivel de detalle.

Ejemplo de requisito de usuario: El sistema debe hacer préstamos
Ejemplo de requisito de sistema: Función préstamo: entrada código socio, código ejemplar; salida: fecha devolución; etc.

Se clasifican en tres los tipos de requisitos de sistema:

Los requisitos funcionales describen:


Los requisitos no funcionales son restricciones de los servicios o funciones que ofrece el sistema (ej. cotas de tiempo, proceso de desarrollo, rendimiento, etc.)


Los requisitos del dominio se derivan del dominio de la aplicación y reflejan características de dicho dominio.

Pueden ser funcionales o no funcionales.

Ej. El sistema de biblioteca de la Universidad debe ser capaz de exportar datos mediante el Lenguaje de Intercomunicación de Bibliotecas de España (LIBE).
Ej. El sistema de biblioteca no podrá acceder a bibliotecas con material censurado.

En ingeniería de "software", el diseño es una fase de ciclo de vida del "software". Se basa en la especificación de requisitos producido por el análisis de los requisitos (fase de análisis), el diseño define "cómo" estos requisitos se cumplirán, la estructura que debe darse al sistema de "software" para que se haga realidad.

El diseño sigue siendo una fase separada del la programación o codificación, esta última corresponde a la traducción en un determinado lenguaje de programación de las premisas adoptadas en el diseño.

Las distinciones entre las actividades mencionadas hasta ahora no siempre son claras cómo se quisiera en las teorías clásicas de ingeniería de "software". El diseño, en particular, puede describir el funcionamiento interno de un sistema en diferentes niveles de detalle, cada una de ellos se coloca en una posición intermedia entre el análisis y codificación.

Normalmente se entiende por "diseño de la arquitectura" al diseño de "muy alto nivel", que sólo define la estructura del sistema en términos de la módulos de "software" de que se compone y las relaciones macroscópicas entre ellos. A este nivel de diseño pertenecen fórmulas como cliente-servidor o “tres niveles”, o, más generalmente, las decisiones sobre el uso de la arquitectura de hardware especial que se utilice, el sistema operativo, DBMS, Protocolos de red, etc.

Un nivel intermedio de detalle puede definir la descomposición del sistema en módulos, pero esta vez con una referencia más o menos explícita al modo de descomposición que ofrece el particular lenguaje de programación con el que el desarrollo se va a implementar, por ejemplo, en un diseño realizado con la tecnología de objetos, el proyecto podría describir al sistema en términos de clases y sus interrelaciones.

El diseño detallado, por último, es una descripción del sistema muy cercana a la codificación (por ejemplo, describir no sólo las clases en abstracto, sino también sus atributos y los métodos con sus tipos).

Debido a la naturaleza "intangible" del "software", y dependiendo de las herramientas que se utilizan en el proceso, la frontera entre el diseño y la codificación también puede ser virtualmente imposible de identificar. Por ejemplo, algunas herramientas CASE son capaces de generar código a partir de diagramas UML, los que describen gráficamente la estructura de un sistema "software".

Durante esta etapa se realizan las tareas que comúnmente se conocen como programación; que consiste, esencialmente, en llevar a código fuente, en el lenguaje de programación elegido, todo lo diseñado en la fase anterior. Esta tarea la realiza el programador, siguiendo por completo los lineamientos impuestos en el diseño y en consideración siempre a los requisitos funcionales y no funcionales (ERS) especificados en la primera etapa.

Es común pensar que la etapa de programación o codificación (algunos la llaman implementación) es la que insume la mayor parte del trabajo de desarrollo del "software"; sin embargo, esto puede ser relativo (y generalmente aplicable a sistemas de pequeño porte) ya que las etapas previas son cruciales, críticas y pueden llevar bastante más tiempo. Se suele hacer estimaciones de un 30% del tiempo total insumido en la programación, pero esta cifra no es consistente ya que depende en gran medida de las características del sistema, su criticidad y el lenguaje de programación elegido.En tanto menor es el nivel del lenguaje mayor será el tiempo de programación requerido, así por ejemplo se tardaría más tiempo en codificar un algoritmo en lenguaje ensamblador que el mismo programado en lenguaje C.

Mientras se programa la aplicación, sistema, o "software" en general, se realizan también tareas de depuración, esto es la labor de ir liberando al código de los errores factibles de ser hallados en esta fase (de semántica, sintáctica y lógica). Hay una suerte de solapamiento con la fase siguiente, ya que para depurar la lógica es necesario realizar pruebas unitarias, normalmente con datos de prueba; claro es que no todos los errores serán encontrados sólo en la etapa de programación, habrá otros que se encontrarán durante las etapas subsiguientes. La aparición de algún error funcional (mala respuesta a los requisitos) eventualmente puede llevar a retornar a la fase de diseño antes de continuar la codificación.

Durante la fase de programación, el código puede adoptar varios estados, dependiendo de la forma de trabajo y del lenguaje elegido, a saber:




Entre las diversas pruebas que se le efectúan al "software" se pueden distinguir principalmente:


Las pruebas normalmente se efectúan con los llamados datos de prueba, que es un conjunto seleccionado de datos típicos a los que puede verse sometido el sistema, los módulos o los bloques de código. También se escogen: Datos que llevan a condiciones límites al "software" a fin de probar su tolerancia y robustez; datos de utilidad para mediciones de rendimiento; datos que provocan condiciones eventuales o particulares poco comunes y a las que el "software" normalmente no estará sometido pero pueden ocurrir; etc. Los «datos de prueba» no necesariamente son ficticios o «creados», pero normalmente sí lo son los de poca probabilidad de ocurrencia.

Generalmente, existe un fase probatoria final y completa del "software", llamada Beta Test, durante la cual el sistema instalado en condiciones normales de operación y trabajo es probado exhaustivamente a fin de encontrar errores, inestabilidades, respuestas erróneas, etc. que hayan pasado los previos controles. Estas son normalmente realizadas por personal idóneo contratado o afectado específicamente a ello. Los posibles errores encontrados se transmiten a los desarrolladores para su depuración. En el caso de "software" de desarrollo «a pedido», el usuario final (cliente) es el que realiza el Beta Test, teniendo para ello un período de prueba pactado con el desarrollador.

La instalación del "software" es el proceso por el cual los programas desarrollados son transferidos apropiadamente al computador destino, inicializados, y, eventualmente, configurados; todo ello con el propósito de ser ya utilizados por el usuario final. Constituye la etapa final en el desarrollo propiamente dicho del "software". Luego de ésta el producto entrará en la fase de funcionamiento y producción, para el que fuera diseñado.

La instalación, dependiendo del sistema desarrollado, puede consistir en una simple copia al disco rígido destino (casos raros actualmente); o bien, más comúnmente, con una de complejidad intermedia en la que los distintos archivos componentes del "software" (ejecutables, bibliotecas, datos propios, etc.) son descomprimidos y copiados a lugares específicos preestablecidos del disco; incluso se crean vínculos con otros productos, además del propio sistema operativo. Este último caso, comúnmente es un proceso bastante automático que es creado y guiado con herramientas "software" específicas (empaquetado y distribución, instaladores).

En productos de mayor complejidad, la segunda alternativa es la utilizada, pero es realizada o guiada por especialistas; puede incluso requerirse la instalación en varios y distintos computadores (instalación distribuida).

También, en "software" de mediana y alta complejidad normalmente es requerido un proceso de configuración y chequeo, por el cual se asignan adecuados parámetros de funcionamiento y se testea la operatividad funcional del producto.

En productos de venta masiva las instalaciones completas, si son relativamente simples, suelen ser realizadas por los propios usuarios finales (tales como sistemas operativos, paquetes de oficina, utilitarios, etc.) con herramientas propias de instalación guiada; incluso la configuración suele ser automática. En productos de diseño específico o «a medida» la instalación queda restringida, normalmente, a personas especialistas involucradas en el desarrollo del "software" en cuestión.

Una vez realizada exitosamente la instalación del "software", el mismo pasa a la fase de producción (operatividad), durante la cual cumple las funciones para las que fue desarrollado, es decir, es finalmente utilizado por el (o los) usuario final, produciendo los resultados esperados.

El mantenimiento de "software" es el proceso de control, mejora y optimización del "software" ya desarrollado e instalado, que también incluye depuración de errores y defectos que puedan haberse filtrado de la fase de pruebas de control y beta test.
Esta fase es la última (antes de iterar, según el modelo empleado) que se aplica al ciclo de vida del desarrollo de "software". La fase de mantenimiento es la que viene después de que el "software" está operativo y en producción.

De un buen diseño y documentación del desarrollo dependerá cómo será la fase de mantenimiento, tanto en costo temporal como monetario. Modificaciones realizadas a un "software" que fue elaborado con una documentación indebida o pobre y mal diseño puede llegar a ser tanto o más costosa que desarrollar el "software" desde el inicio. Por ello, es de fundamental importancia respetar debidamente todas las tareas de las fases del desarrollo y mantener adecuada y completa la documentación.

El período de la fase de mantenimiento es normalmente el mayor en todo el ciclo de vida. Esta fase involucra también actualizaciones y evoluciones del "software"; no necesariamente implica que el sistema tuvo errores. Uno o más cambios en el "software", por ejemplo de adaptación o evolutivos, puede llevar incluso a rever y adaptar desde parte de las primeras fases del desarrollo inicial, alterando todas las demás; dependiendo de cuán profundos sean los cambios.
El modelo cascada común es particularmente costoso en mantenimiento, ya que su rigidez implica que cualquier cambio provoca regreso a fase inicial y fuertes alteraciones en las demás fases del ciclo de vida.

Durante el período de mantenimiento, es común que surjan nuevas revisiones y versiones del producto; que lo liberan más depurado, con mayor y mejor funcionalidad, mejor rendimiento, etc. Varias son las facetas que pueden ser alteradas para provocar cambios deseables, evolutivos, adaptaciones o ampliaciones y mejoras.

Básicamente se tienen los siguientes tipos de cambios:


El "software" es el producto derivado del proceso de desarrollo, según la ingeniería de "software". Este producto es intrínsecamente evolutivo durante su ciclo de vida: en general, evoluciona generando versiones cada vez más completas, complejas, mejoradas, optimizadas en algún aspecto, adecuadas a nuevas plataformas (sean de "hardware" o sistemas operativos), etc.

Cuando un sistema deja de evolucionar, eventualmente cumplirá con su ciclo de vida, entrará en obsolescencia e inevitablemente, tarde o temprano, será reemplazado por un producto nuevo.

El "software" evoluciona sencillamente por que se debe adaptar a los cambios del entorno, sean funcionales (exigencias de usuarios), operativos, de plataforma o arquitectura "hardwar"e.

La dinámica de evolución del "software" es el estudio de los cambios del sistema. La mayor contribución en esta área fue realizada por Meir M. Lehman y , comenzando en los años 70 y 80. Su trabajo continuó en la década de 1990, con Lehman y otros investigadores de relevancia en la realimentación en los procesos de evolución (Lehman, 1996; Lehman et al., 1998; lehman et al., 2001). A partir de esos estudios propusieron un conjunto de leyes (conocidas como leyes de Lehman) respecto de los cambios producidos en los sistemas. Estas leyes (en realidad son hipótesis) son invariantes y ampliamente aplicables.

Lehman y Belady analizaron el crecimiento y la evolución de varios sistemas "software" de gran porte; derivando finalmente, según sus medidas, las siguientes ocho leyes:




</doc>
<doc id="2556" url="https://es.wikipedia.org/wiki?curid=2556" title="Sociología">
Sociología

La sociología es la ciencia social que se encarga del análisis científico de la estructura y funcionamiento de la sociedad humana o población regional. Estudia los fenómenos colectivos producidos por la actividad social de los seres humanos, dentro del contexto histórico-cultural en el que se encuentran inmersos.

En la sociología se utilizan múltiples técnicas de investigación interdisciplinarias para el análisis e interpretación desde diversas perspectivas teóricas las causas, significados e influencias culturales que motivan la aparición de diversas tendencias de comportamiento en el ser humano especialmente cuando se encuentra en convivencia social y dentro de un hábitat o "espacio-temporal" compartido.

Al ser una disciplina dedicada al estudio de las relaciones sociales humanas, siendo estas de carácter heterogéneo, la sociología ha producido diversas y en ocasiones opuestas corrientes. Tal situación ha enriquecido, mediante la confrontación de conocimientos, el cuerpo teórico de esta ciencia.

Los orígenes de la sociología están asociados a los nombres de Karl Marx, Henri de Saint-Simon, Auguste Comte, Herbert Spencer, Émile Durkheim, Georg Simmel, Talcott Parsons, Ferdinand Tönnies, Vilfredo Pareto, Max Weber, Alfred Schütz, Harriet Martineau, Beatrice Webb y Marianne Weber.

Algunos de los sociólogos más destacados del siglo XX han sido Talcott Parsons, Erving Goffman, Walter Benjamin, Herbert Marcuse, Wright Mills, Michel Foucault, Pierre Bourdieu, Niklas Luhmann y Jürgen Habermas. En la actualidad, los análisis y estudios más innovadores de los comportamientos sociales corren a cargo de autores como George Ritzer, Anthony Giddens, Zygmunt Bauman, Ulrich Beck, Alain Touraine, Manuel Castells, Slavoj Žižek, entre otros.

El razonamiento sociológico es preexistente a la fundación de la disciplina. El análisis social tiene su origen en el conocimiento y la filosofía occidental, desarrollados desde la antigua Grecia por filósofos como Platón, e incluso otros anteriores. El origen de la encuesta, es decir, la obtención de información a partir de una muestra de individuos, se remonta a por lo menos el Libro Domesday en 1086. El antiguo filósofo oriental Confucio escribió sobre la importancia de los roles sociales. Hay pruebas de la sociología temprana en el Islam medieval. Algunos consideran que Ibn Jaldún, un erudito musulmán del norte de África (Túnez), ha sido el primer sociólogo y padre de la sociología. Su Muqaddima fue quizás el primer trabajo para avanzar en el razonamiento científico-social de la cohesión social y el conflicto social.

Durante la época de la Ilustración y después de la Revolución Francesa, lo social y las actividades del hombre ganaron creciente interés. Escritores como Voltaire, Montesquieu, y Giambattista Vico, se interesaron por analizar las instituciones sociales y políticas europeas. Y Lord Kames inició el análisis de las causas del cambio social, y tras él, surgió una corriente conservadora, muy interesada en saber las razones de los cambios y de la estabilidad existentes en la sociedad, liderada por Joseph de Maistre y Edmund Burke, quienes criticaron muchas de las premisas de la Ilustración.

La voluntad de crear una "física social", esto es, un conocimiento indiscutible de la sociedad, de forma análoga a como se establece en la Física, surgió con el positivismo del siglo XIX. El primero en defender una teoría e investigación científica de los fenómenos sociales fue Henri de Saint-Simon (1760-1825) a mediados del siglo XIX. Auguste Comte, quien fue secretario de Saint-Simon entre 1817 y 1823, desarrolló sus teorías bajo las premisas del positivismo. Comte acuñó la palabra "sociología" en 1824 (del latín: socius, "socio, compañero"; y el sufijo griego -logía, "el estudio de"). La primera vez que apareció impresa esta palabra fue en su "Curso de filosofía positiva" de 1838.

Casi en simultáneo, en Alemania, Von Stein (1815-1890), introdujo el concepto de sociología como ciencia (Die Wissenschaft der Gesellschaft) incorporando a su estudio lo que él llamó "Movimientos sociales" y la dialéctica hegeliana. De esta manera logró darle a la disciplina una visión dinámica. Von Stein es considerado como el fundador de las ciencias de la Administración Pública.

Alexis de Tocqueville (1805-1859) por su parte, es también reconocido como uno de los precursores de la sociología, por sus estudios sobre la Revolución francesa y sobre los Estados Unidos (La democracia en América, publicada entre 1835-1840). El citado analizó a las sociedades en general e hizo una comparación entre las sociedades americanas y las sociedades europeas.

La sociología continuó con un desarrollo intenso y regular a principio del siglo XX. Émile Durkheim, quien se inspiró en algunas teorías de Auguste Comte para renovar la sociología, quería en particular "estudiar los hechos sociales como si fueran cosas". Uno de los retos de la sociología era desarrollarse como una ciencia autónoma. Durkheim buscó distinguir a la sociología de la filosofía por un lado y de la psicología por el otro. Por ello, se le considera como uno de los padres fundadores de la sociología.

El citado postuló las bases de una metodología científica para la sociología, en particular en la obra "Las reglas del método sociológico" (1895), y en "La división del trabajo social" (1893), libro que además es su tesis. Su método reposa esencialmente en la comparación de estadísticas y características cuantitativas, buscando liberarse de todo subjetivismo ligado a toda interpretación cualitativa, y a desembarazarse de todos los prejuicios morales o moralizadores "a priori" para comprender los hechos sociales como en su obra: "El Suicidio".

Karl Marx es otro científico que ha tenido una profunda influencia en el pensamiento social y la crítica del siglo XIX. Fue principalmente en Alemania donde desarrollara una teoría mayor de la sociología, influenciando posteriormente, entre otros, en la Escuela de Frankfurt.

Max Weber, contemporáneo de Durkheim, tomó un camino diferente: empleó la Ciencia política, la Economía política, la Filosofía de la cultura y del derecho, los estudios religiosos que son, según él, todo como la sociología, las "ciencias de la cultura". De acuerdo a toda una tradición de la filosofía alemana (sobre todo Wilhelm Dilthey), estas ciencias son diferentes de las ciencias naturales ya que tienen su propio método. Ellas proponen una comprensión de los fenómenos colectivos antes que la búsqueda de leyes (es el método comprensivo).

La investigación cualitativa requiere un profundo entendimiento del comportamiento humano y las razones que lo gobiernan. A diferencia de la investigación cuantitativa, la investigación cualitativa busca explicar las razones de los diferentes aspectos de tal comportamiento. En otras palabras, investiga el por qué y el cómo se tomó una decisión, en contraste con la investigación cuantitativa la cual busca responder preguntas tales como cuál, dónde, cuándo. La investigación cualitativa se basa en la toma de muestras pequeñas, esto es la observación de grupos de población reducidos, como salas de clase, etc.

Este método consiste en descripciones detalladas de situaciones, eventos, personas, interacciones y comportamientos que son observables. Incorpora lo que los participantes dicen, sus experiencias, actitudes, creencias, pensamientos y reflexiones tal como son expresadas por ellos mismos. Cook y Reichardt consideran entre los métodos cualitativos a la etnografía, los estudios de caso, las entrevistas a profundidad, la observación participante y la investigación-acción.

Una primera característica de estos métodos se manifiesta en su estrategia para tratar de conocer los hechos, procesos, estructuras y personas en su totalidad, y no a través de la medición de algunos de sus elementos. La misma estrategia indica ya el empleo de procedimientos que dan un carácter único a las observaciones. La segunda característica es el uso de procedimientos que hacen menos comparables las observaciones en el tiempo y en diferentes circunstancias culturales, es decir, este método busca menos la generalización y se acerca más a la fenomenología y al interaccionismo simbólico. Una tercera característica estratégica importante para este trabajo (ya que sienta bases para el método de la investigación participativa), se refiere al papel del investigador en su trato -intensivo- con las personas involucradas en el proceso de investigación, para entenderlas.

Cook y Reichardt apuntan que "cuando se aplican métodos cuantitativos se miden características o variables que pueden tomar valores numéricos y deben describirse para facilitar la búsqueda de posibles relaciones mediante el análisis estadístico". Aquí se utilizan las técnicas experimentales aleatorias, cuasi-experimentales, cuestionarios, encuestas, entre otros.

Dentro de todos los análisis de los métodos cuantitativos podemos encontrar unas características basadas en el positivismo como fuente epistemológica: el énfasis en la precisión de los procedimientos para la medición, el uso de técnicas de muestreo, así como la relación entre los conceptos y los indicadores con los que se miden (para evitar las confusiones que genera el uso de un lenguaje oscuro, que pese a ser seductor, es difícil de comprobar su veracidad). Otra característica predominante de los métodos cuantitativos es la selección subjetiva e intersubjetiva de indicadores (a través de conceptos y variables) de ciertos elementos de procesos, hechos, estructuras y personas. Estos elementos no conforman en su totalidad, los procesos o las personas (de allí se deriva el debate entre los cuantitativistas que nunca ven un fenómeno integrado, sino siempre conjuntos de partículas de los fenómenos relacionados con la observación, y los cualitativistas que pueden percibir los elementos generados que comparten los fenómenos). Sin embargo, las nuevas técnicas cuantitativas, como el análisis de redes sociales, o la historia de acontecimientos, consiguen en cierta medida superar estas limitaciones.

El método comparativo estudia la correlación que existe entre uno o más fenómenos que se cotejan. Cuando se estudia, por ejemplo, la relación directa que existe entre el desarrollo del urbanismo y la relajación de las costumbres, o entre la extensión de la educación y la democracia, se hace uso del método comparativo.

Distintas corrientes han nutrido el cuerpo teórico de la sociología, entre las que destacan, la Escuela Francesa, la Escuela Inglesa, la Escuela de Chicago y la Escuela de Fráncfort. Las perspectivas generalmente usadas son el interaccionismo simbólico, el socioconstruccionismo, la teoría del conflicto, la fenomenología y la teoría funcionalista, no siendo las únicas. Muchos sociólogos se han abocado al estudio de la sociología crítica, el posestructuralismo, y otras tantas basadas en la comprensión del sujeto desde una perspectiva amplia, basada en disciplinas como la historia, la filosofía, entre otras, obteniendo así una teoría sociológica compleja y cuyos conocimientos son más profundos que en los primeros casos. Además de las expuestas, entre el grupo de las grandes escuelas se encuentran la teoría neomarxiana y la fenomenología, en su vertiente sociológica.

La teoría está asociada a Émile Durkheim y más recientemente a Talcott Parsons, además de autores como Robert K. Merton. El funcionalismo estructuralista ve a la sociedad como un sistema complejo, cuyas partes trabajan juntas para promover la solidaridad y estabilidad. Este enfoque analiza la sociedad desde un nivel macro, que es un enfoque amplio en las estructuras sociales que la conforman en su conjunto. Cree que la sociedad evoluciona de manera gradual, como parte de un proceso de adaptación y complejización, de modo análogo a los organismos vivientes. El funcionalismo se preocupa tanto por las estructuras como las funciones sociales. Se interesa por sus elementos constitutivos, a saber: normas, costumbres, tradiciones e instituciones.

A pesar de la indiscutible hegemonía que ostentó durante las dos décadas posteriores a la Segunda Guerra Mundial, el funcionalismo estructural ha perdido importancia como teoría sociológica.

El interaccionismo simbólico es una corriente de pensamiento microsociológica. Partiendo de un método de estudio participante, capaz de dar cuenta del sujeto, concibe lo social como el marco de la interacción simbólica de individuos, y concibe la comunicación como el proceso social por antonomasia, a través del cual, se constituyen simultánea y coordinadamente, los grupos y los individuos. Este analiza el sentido de la acción social desde la perspectiva de los participantes. Algunos interaccionistas simbólicos destacados son Herbert Blumer, Erving Goffman o Nikolas Rose.

Según Mead, el individuo no nace siendo persona. La persona se forma socialmente cuando logra observarse a sí misma como un objeto, es decir, cuando logra un pensamiento reflexivo sobre sí mismo. Los otros, las demás personas, son un espejo en el cual se observa la propia persona.

En un sentido similar, Goffman, basado en un modelo interpretativo dramatúrgico, estudia los ritos de interacción comunicativa que aprendemos y ponemos en juego en nuestra vida cotidiana. Define el rol como un conjunto organizado de expectativas de comportamiento en torno a una función o posición social (por ejemplo "padre", "jefe", "profesor").

La etnometodología es una corriente sociológica surgida en los años sesenta a través de los trabajos de Harold Garfinkel. Se basa en el supuesto de que todos los seres humanos tienen un sentido práctico con el cual adecuan las normas de acuerdo con una racionalidad práctica que utilizan en la vida cotidiana. En términos más sencillos, se trata de una perspectiva sociológica que toma en cuenta los métodos que los seres humanos utilizan en su vida diaria para levantarse, ir al trabajo, tomar decisiones, entablar una conversación con los otros. En la antropología también se suele seguir esta línea sociológica, sobre todo los antropólogos que se especializan en los estudios de la sociedad.
La teoría del conflicto es una de las grandes escuelas de la teoría sociológica moderna, es considerada como desarrollo que se produjo en reacción a la estática del funcionalismo estructural. Durante las décadas de 1950 y 1960 la teoría del conflicto proporcionó una alternativa al funcionalismo estructural, pero ha sido superada recientemente por las teorías neomarxianas. La teoría del conflicto está íntimamente vinculada a la teoría de los juegos y a los estudios y escuelas sobre negociación.

Entre los más prominentes pensadores con enfoque sociológico de los últimos tiempos hay que tener en cuenta al pensador francés Michel Foucault (1926-1984) y al autor alemán Jürgen Habermas (nacido en 1929). Al igual que los clásicos de la disciplina, estos autores no sólo han sido sociólogos sino que se han ocupado ampliamente de la filosofía y de la historia. Foucault se ocupó de materias similares a las analizadas por Weber en sus estudios de la burocracia: el desarrollo de las prisiones, hospitales, escuelas y otras organizaciones a gran escala. Por ejemplo, consideraba que la sexualidad siempre está vinculada al poder social y cuestionaba la idea de que un mayor conocimiento conduzca a una mayor libertad, porque lo concebía como una forma de "etiquetar" a las personas y de controlarlas.

El desarrollo de la teoría del intercambio tiene sus raíces en el conductismo.

El conductismo está más vinculado a la psicología, pero en sociología tiene una influencia directa en la sociología conductista y una influencia indirecta en la teoría del intercambio. El sociólogo conductista se ocupa de la relación entre los efectos de la conducta de un actor sobre su entorno y su influencia sobre la conducta posterior del actor. Los conductistas se interesan mucho por las recompensas y los costes de las acciones. Las recompensas se definen por su capacidad de reforzar la conducta, mientras los costes reducen la probabilidad de la conducta. En este sentido, el conductismo en general, y la idea de recompensas y costes en particular, han influido poderosamente en la primera teoría del intercambio.

La teoría del intercambio de Peter Blau se diferencia en distintas facetas con la de Homans, la meta de Blau era contribuir a una comprensión de la estructura social sobre la base de un análisis de los procesos sociales que rigen las relaciones entre los individuos y los grupos. La cuestión básica es cómo se llega a organizar la vida social en estructuras cada vez más complejas de asociaciones entre personas.

Walter Buckley (1967) aborda una cuestión de importancia central: las ventajas de la teoría de sistemas para la sociología. En primer lugar, dado que la teoría de sistemas se deriva de las ciencias naturales y dado que, al menos a los ojos de sus exponentes, es aplicable a todas las ciencias sociales y conductistas, ofrece un vocabulario que las unifica. En segundo lugar, la teoría de sistemas incluye varios niveles de análisis y puede aplicarse igualmente a los aspectos macro más objetivos y a los aspectos micro más subjetivos de la vida social. En tercer lugar, la teoría de sistemas se interesa por las diversas relaciones entre los numerosos aspectos del mundo social, y por tanto, milita contra los análisis parciales del mundo social.

La dicotomía entre estructura y acción, a veces referida como determinismo contra voluntarismo, forma parte de un debate ontológico duradero en la teoría social: ¿determinan las estructuras sociales el comportamiento de un individuo o lo hace la acción humana? En este contexto, se entiende por agencia a la capacidad de las personas para actuar de forma independiente y tomar decisiones libres, mientras que la "estructura" se refiere a los factores que limitan o afectan las decisiones y acciones de los individuos (como la clase social, religión, género, origen étnico, entre otras). Las discusiones sobre la primacía de la estructura o la acción se relacionan con el núcleo de la epistemología sociológica (¿de qué está hecho el mundo social, ¿qué es una causa en el mundo social y qué es un efecto?).

Una pregunta permanente dentro de este debate es acerca de la "reproducción social": ¿cómo son las estructuras (en especial, las estructuras que producen desigualdad) reproducidas a través de las elecciones de los individuos?. Diferentes respuestas han sido planteadas a este respecto por la sociología contemporánea. Entre ellas podemos mencionar a Pierre Bourdieu, con su teoría constructivista genética; Jürgen Habermas, con su distinción entre racionalidad instrumental y comunicativa -sistema y mundo de la vida-, y Anthony Giddens, con su teoría de la estructuración social.

Se entiende como un "dinamismo social" el fluir de las costumbres y creencias de una sociedad. El cambio se evidencia a través de las interacciones de cada persona con el resto social y cómo el conjunto afecta al individuo, marcando un comportamiento de comunicación global de sujetos relacionados entre sí. Las formas y convenciones de la dinámica social están marcadas por la historia y sujetas, por tanto, a un cambio permanente.

La interacción social resultante de la dinámica, expresa grados sociales, estableciendo campos de acción que se expresan mediante la diferenciación del "status quo" social. En la interacción social, habría primero que establecer la capa o campo social sobre el que se va a observar a los individuos y cómo éstos influyen mutuamente y adaptan su comportamiento frente a los demás.

La sociología en la región latinoamericana se desarrollaría a lo largo del siglo XX, con posterioridad a Europa y los Estados Unidos. Su creación se vincula a diferentes intentos de apropiación del corpus teórico de la disciplina, sumado al desafío de producir y legitimar un ideario conceptual propio, que reflejara la realidad del conjunto de estos países. Esta se nutriría además de aportes intelectuales locales variados. Los desarrollos más significativos elaborados desde la región se refieren a lecturas críticas del imperialismo y los procesos de colonización, teorías vinculadas a la modernización de la matriz económica, social y cultural, así como teorías de la dependencia, con énfasis en la subordinación de la región a escala mundial. Estas últimas se vinculan a la teología de la liberación, pedagogía del oprimido y un conjunto de estudios realizados desde la CEPAL.

Más recientemente, encontramos estudios sobre democracia, democratización y derechos humanos, aportes críticos al neoliberalismo y la globalización económica, así como estudios sobre participación política, acción colectiva y conflicto social.

Algunas organizaciones que consolidaron la institucionalización de la disciplina en la región son: ALAS, CLACSO y FLACSO.



</doc>
<doc id="2558" url="https://es.wikipedia.org/wiki?curid=2558" title="Slackware">
Slackware

Slackware Linux es una distribución del sistema operativo GNU/Linux creada en 1993 por Patrick Volkerding. Basada originalmente en SLS Linux, Slackware es la distribución de GNU/Linux más antigua aún en mantenimiento.

Su versión actual es la versión 14.2, publicada el primero de julio de 2016. Contiene un programa de instalación sencillo de utilizar, aunque está basado en texto, a diferencia de otros entornos de instalación basados en ambientes gráficos. También cuenta con extensa documentación en inglés y un sistema de gestión de paquetes basado en menús. Lo que diferencia a Slackware Linux de otras distribuciones Linux es que la misma se asemeja en alto grado a los sistemas operativos Unix. A tal efecto, incluye software que normalmente no se encuentra en otras distribuciones Linux, tal como la última versión del entorno de comandos Korn shell.

Una instalación completa incluye una implementación de X Window System para el sistema de ventanas X.Org ; entornos de escritorio como KDE (4.8.5) (hasta la versión 10.1 estuvo incluido GNOME) y Xfce (4.10); entornos de desarrollo para C/C++, Perl, Python, Java, LISP y Ruby; utilidades de red, servidores de correo, de noticias (INN), HTTP (Apache) o FTP; programas de diseño gráfico como The GIMP; navegadores web como Konqueror, Firefox y Mozilla SeaMonkey, entre otras muchas aplicaciones.

La página informativa oficial describe a Slackware como "un sistema operativo avanzado, diseñado con el doble objetivo de facilidad de uso y estabilidad como prioridades principales" y describe algunas de las funciones que se distribuyen con él: "web, ftp y servidores de correo vienen por defecto, así como una amplia selección de entornos de escritorio populares. Se incluye una gama completa de herramientas de desarrollo, editores y librerías actuales, para los usuarios que deseen desarrollar o compilar software adicional".

Slackware actualmente proporciona soporte para la arquitectura x86 de 64 bits.

La distribución de paquetes en Slackware se hace principalmente con archivos tar comprimidos. Hace uso del programa rpm2txz y rpm2tgz respectivamente para convertir paquetes RPM a formatos tgz y txz nativos. La interfaz del programa de instalación es por texto, y requiere un mayor conocimiento de GNU/Linux que otras distribuciones. Esto puede ser una desventaja para usuarios principiantes, pero no representa mayor dificultad para usuarios intermedios o avanzados de GNU/Linux.

La primera versión oficial de Slackware, la 1.00, fue publicada el 16 de julio de 1993 por Patrick Volkerding, fundador y líder de desarrollo. Estaba basada en la distribución SLS Linux y se distribuía en discos flexibles de 3½ e imágenes que estaban disponibles en servidores FTP anónimos. Slackware es la distribución más antigua entre las que siguen activamente mantenidas. 

Así Patrick J. Volkerding decía en el newsgroups comp.os.linux:
El nombre "Slackware" deriva del término "slack", tal y como lo define la Iglesia de los SubGenios.

En las primeras versiones de Slackware, la distribución tenía tres cuentas de usuario, "satan", "gonzo" y "snake". Estas eran incluidas sólo como ejemplos, pero fueron eliminadas posteriormente debido a que significaban un potencial riesgo computacional. 

En 1999, el número de versión de Slackware se incrementó de 4 a 7, para demostrar que Slackware estaba actualizado al igual que otras distribuciones de Linux, muchas de las cuales tenían como número de publicación en ese momento el 6.

En 2004, Patrick Volkerding enfermó seriamente y el futuro desarrollo de Slackware se volvió incierto. Afortunadamente, se recuperó, y el desarrollo de Slackware ha continuado.

En 2005, el escritorio GNOME fue eliminado de la distribución, lo que creó una gran polémica superada en parte por el hecho de que sigue habiendo proyectos dedicados a ofrecer dicho escritorio a los usuarios de Slackware, como Freerock GNOME o dropline GNOME. 

En 2007, incluye la serie 2.6.x del núcleo Linux como estable. 

En el transcurso de la historia de Slackware, han nacido otras distribuciones y LiveCD basadas en ella. Algunas de las más populares incluyen College Linux, SLAX, Vector Linux y Zenwalk.

El 13 de agosto de 2008 Slackware incluía a KDE 4 en la rama de pruebas ("Slackware -current") en el directorio codice_1. 

El 19 de mayo de 2009 Volkerding anunció el comienzo del soporte oficial para la arquitectura de 64 bits, la cual se inició en la rama en desarrollo ("current").

El 9 de julio de 2009 Volkerding anuncia en el sitio oficial de Slackware el soporte para arquitecturas ARM, un port oficial denominado ARMedslack, tanto para la versión 12.2 como para la que está en desarrollo ("current").

El 26 de agosto de 2009 el proyecto Slackware lanzó la versión 13.0, que destacó dos importantes anuncios, el primero es el reemplazo de KDE 3 por KDE 4, y el segundo fue el lanzamiento de la primera versión oficial de Slackware para la arquitectura de 64 bits, la cual hasta ese momento otros proyectos, como Slamd64, desarrollaban ports no oficiales de Slackware para esa arquitectura.

El 24 de mayo de 2010 se lanzó la versión 13.1, que tenía como principales mejoras la versión SC de KDE 4.4.3, el kernel Linux 2.6.33.4, bibliotecas y aplicaciones actualizadas tales como Firefox y Thunderbird.

El 27 de abril de 2011 se lanzó la versión 13.37, el kernel Linux 2.6.37.6 , Kernel Linux 2.6.35.12 y 2.6.39-rc4 en testing , mejoras en el sistema X (incluye nouveau para las tarjetas gráficas nvidia) , navegador web firefox 4 , KDE SC 4.5.5 y las acostumbradas mejoras.

El 23 de marzo de 2013 se anunció que se quitaba MySQL y se agregaba MariaDB como servidor de base de datos. Este cambio está aplicado a la versión de desarrollo ("Slackware -current") por el momento y va a estar disponible en la próxima versión estable.

El 30 de junio de 2016 se anunció la versión 14.2 como estable, con el kernel Linux 4.4.14, bibliotecas y aplicaciones actualizadas tales como: XFCE 4.12.1 y KDE 4.14.21 (KDE 4.13.3 con kdelibs-4.14.21), X11 a la versión X11R7.7, la cual incluye mejoras en términos de rendimiento y soporte de hardware; gcc-5.3.0 por defecto para C, C++, Objective-C; la versión x86_64 es compatible con la instalación y arranque en máquinas que utilizan el firmware UEFI.

"Mantenlo Simple, Estúpido" (de sus siglas en inglés KISS que significan ""Keep It Simple Stupid""), es un concepto que explica muchas de las opciones en el diseño de Slackware. En este contexto, ""simple"" se refiere a un punto de vista de diseño, en vez de ser fácil de utilizar. Esta es la razón por la cual existen muy pocas herramientas GUI para configurar el sistema. Las herramientas GUI son (según nos dice la teoría) más complejas, y por lo tanto más propensas a tener problemas que una simple línea de órdenes. El resultado general sobre este principio es que Slackware es muy rápido, estable y seguro con el costo de no ser tan amigable al usuario. Los críticos mencionan que esto hace que las cosas sean difíciles de aprender y consuman mucho tiempo. Los seguidores dicen que la flexibilidad y transparencia, así como, la experiencia ganada en el proceso son más que suficientes.

Según la página oficial de Slackware el término KISS se refiere a "keep it simple stable", que traducido sería "manténgalo simple y estable".

Slackware utiliza scripts de inicio init de BSD, mientras que la mayoría de las distribuciones utilizan el estilo de scripts System V. Básicamente, con el estilo System V cada nivel de ejecución tiene un subdirectorio para sus scripts init, mientras que el estilo BSD ofrece un solo script init para cada nivel de ejecución. Los fieles del estilo BSD mencionan que es mejor ya que con este sistema es más fácil encontrar, leer, editar y mantener los scripts. Mientras que los seguidores de System V dicen que la estructura de System V para los scripts lo convierte en más poderoso y flexible. 

Cabe mencionar que la compatibilidad para los scripts init de System V han sido incorporados en Slackware, a partir de la versión 7.0.

La aproximación de Slackware para el manejo de paquetes es único. Su sistema de manejo de paquetes puede instalar, actualizar y eliminar paquetes tan fácilmente como en otras distribuciones. Pero no hace el intento por rastrear o manejar las "dependencias" referidas (por ejemplo: asegurándose de que el sistema tiene todas las bibliotecas y programas que el nuevo paquete "esperaría" estuvieran presentes en el sistema). Si los requisitos no se encuentran, no habrá indicaciones de falla hasta que el programa sea ejecutado.

Los paquetes son comprimidos en un tarball en donde los nombres de archivos terminan con .txz (El formato .tgz fue utilizado hasta la versión 12.2) en vez de .tar.gz. Son construidos de tal manera que al ser extraídos en el directorio raíz, los archivos se copien a sus lugares de instalación. Es por lo tanto posible (pero no aconsejable) instalar paquetes sin las herramientas de Slackware para paquetes, usando solamente tar's y gzip's y asegurándose de ejecutar los scripts doinst.sh en caso de ser incluidos en el paquete. 

En contraste Red Hat Linux tiene paquetes RPM los cuales son archivos CPIO, y los .deb de Debian son archivos ar. Estos contienen información detallada de las dependencias y las utilerías que se pueden utilizar para encontrar e instalar esas dependencias. Se negarán a instalarse a menos que los requisitos sean encontrados (aunque esto puede omitirse).

A pesar de que Slackware por sí mismo no incorpora herramientas para resolver dependencias automáticamente descargando e instalándolas, existen algunas herramientas externas que proveen de esta funcionalidad de forma similar a APT.

Algunas de estas herramientas determinan las dependencias analizando los paquetes instalados, determinando qué bibliotecas se necesita, y después descubriendo qué paquetes están disponibles. Este proceso automático, muy similar al APT de Debian y produce generalmente resultados satisfactorios. 


Slackware es una distribución que no se centra en tener las últimas versiones de los programas, sino que su foco es tener un sistema estable. Los nuevos paquetes se ponen a prueba y no son entregados hasta que no sean estables (esto no implica que sea la última versión disponible del programa), por ejemplo no se incluyó el núcleo Linux 2.6.* sino hasta el año 2007, habiendo sido lanzada la versión 2.6.0 en el año 2003. Pero cuando algún paquete tiene una actualización por bugs o mejoras de seguridad, éstas son incorporadas a los paquetes de Slackware y se anuncia a través de una lista de correo de dichas actualizaciones y en el log de cambios ("changelog") que se encuentra en el sitio web. Slackware incluye dentro del directorio /extra del CD de instalación el programa Slackpkg que ayuda a mantener actualizado el sistema.




</doc>
<doc id="2561" url="https://es.wikipedia.org/wiki?curid=2561" title="Segundo (desambiguación)">
Segundo (desambiguación)

La palabra segundo se puede referir a:

</doc>
<doc id="2562" url="https://es.wikipedia.org/wiki?curid=2562" title="Sistema solar">
Sistema solar

El sistema solar es el sistema planetario en el que se encuentran la Tierra y otros objetos astronómicos que giran directa o indirectamente en una órbita alrededor de una única estrella conocida como el Sol.

La estrella concentra el 99,75% de la masa del sistema solar, y la mayor parte de la masa restante se concentra en ocho planetas cuyas órbitas son prácticamente circulares y transitan dentro de un disco casi llano llamado plano eclíptico. Los cuatro planetas más cercanos, considerablemente más pequeños Mercurio, Venus, Tierra y Marte, también conocidos como los planetas terrestres, están compuestos principalmente por roca y metal. Mientras que los cuatro más alejados, denominados gigantes gaseosos o "planetas jovianos", más masivos que los terrestres, están compuesto de hielo y gases. Los dos más grandes, Júpiter y Saturno, están compuestos principalmente de helio e hidrógeno. Urano y Neptuno, denominados los gigantes helados, están formados mayoritariamente por agua congelada, amoniaco y metano.

El Sol es el único cuerpo celeste del sistema solar que emite luz propia, debido a la fusión termonuclear del hidrógeno y su transformación en helio en su núcleo. 
El sistema solar se formó hace unos 4600 millones de años a partir del colapso de una nube molecular. El material residual originó un disco circunestelar protoplanetario en el que ocurrieron los procesos físicos que llevaron a la formación de los planetas.
El sistema solar se ubica en la actualidad en la nube Interestelar Local que se halla en la Burbuja Local del brazo de Orión, de la galaxia espiral Vía Láctea, a unos 28 000 años luz del centro de esta.

El sistema solar es también el hogar de varias regiones compuestas por objetos pequeños. El cinturón de asteroides, ubicado entre Marte y Júpiter, es similar a los planetas terrestres ya que está constituido principalmente por roca y metal. En este cinturón se encuentra el planeta enano Ceres.
Más allá de la órbita de Neptuno están el cinturón de Kuiper, el disco disperso y la nube de Oort, que incluyen objetos transneptunianos formados por agua, amoníaco y metano principalmente. En este lugar existen cuatro planetas enanos Haumea, Makemake, Eris y Plutón, el cual fue considerado el noveno planeta del sistema solar hasta 2006. Este tipo de cuerpos celestes ubicados más allá de la órbita de Neptuno son también llamados plutoides, los cuales junto a Ceres, poseen el suficiente tamaño para que se hayan redondeado por efectos de su gravedad, pero que se diferencian principalmente de los planetas porque no han vaciado su órbita de cuerpos vecinos.

Adicionalmente a los miles de objetos pequeños de estas dos zonas, algunas docenas de los cuales son candidatos a planetas enanos, existen otros grupos como cometas, centauros y polvo cósmico que viajan libremente entre regiones. Seis planetas y cuatro planetas enanos poseen satélites naturales. El viento solar, un flujo de plasma del Sol, crea una burbuja de viento estelar en el medio interestelar conocido como heliosfera, la que se extiende hasta el borde del disco disperso. La nube de Oort, la cual se cree que es la fuente de los cometas de período largo, es el límite del sistema solar y su borde está ubicado a un año luz desde el Sol.

A principios del año 2016 se publicó un estudio según el cual puede existir un noveno planeta en el sistema Solar, al que dieron el nombre provisional de Phattie.

Algunas de las más antiguas civilizaciones concibieron al universo desde una perspectiva geocéntrica, como en Babilonia en donde su visión del mundo estuvo representada de esta forma.
En Occidente, el griego presocrático Anaximandro declaró a la Tierra como centro del universo, imaginó a esta como un pilar en forma de tambor equilibrado en sus cuatro puntos más distantes lo que, en su opinión, le permitió tener estabilidad.
Pitágoras y sus seguidores hablaron por primera vez del planeta como una esfera, basándose en la observación de los eclipses; y en el siglo IV a. C. Platón junto a su estudiante Aristóteles escribieron textos del modelo geocéntrico de Anaximandro, fusionándolo con el esférico pitagórico.
Pero fue el trabajo del astrónomo heleno Claudio Ptolomeo, especialmente su publicación llamada Almagesto expuesta en el siglo II de nuestra era, el cual sirvió durante un período de casi 1300 años como la norma en la cual se basaron tanto astrónomos europeos como islámicos.

Si bien el griego Aristarco presentó en el siglo siglo III a. C. a la teoría heliocéntrica y más adelante el matemático hindú Aryabhata hizo lo mismo, ningún astrónomo desafió realmente el modelo geocéntrico hasta la llegada del polaco Nicolás Copérnico el cual causó una verdadera revolución en esta rama a nivel mundial, por lo cual es considerado el padre de la astronomía moderna.
Esto debido a que, a diferencia de sus antecesores, su obra consiguió una amplia difusión pese a que fue concebida para circular en privado; el papa Clemente VII pidió información de este texto en 1533 y Lutero en el año 1539 lo calificó de "astrólogo advenedizo que pretende probar que la Tierra es la que gira".
La obra de Copérnico otorga dos movimientos a la tierra, uno de rotación en su propio eje cada 24 horas y uno de traslación alrededor del Sol cada año, con la particularidad de que este era circular y no elíptico como lo describimos hoy.

En el siglo XVII el trabajo de Copérnico fue impulsado por científicos como Galileo Galilei, quien ayudado con un nuevo invento, el telescopio, descubre que alrededor de Júpiter rotan satélites naturales que afectaron en gran forma la concepción de la teoría geocéntrica ya que estos cuerpos celestes no orbitaban a la Tierra; lo que ocasionó un gran conflicto entre la iglesia y los científicos que impulsaban esta teoría, el cual culminó con el apresamiento y sentencia del tribunal de la inquisición a Galileo por herejía al estar su idea contrapuesta con el modelo clásico religioso.
Su contemporáneo Johannes Kepler, a partir del estudio de la órbita circular intentó explicar la traslación planetaria sin conseguir ningún resultado, por lo que reformuló sus teorías y publicó, en el año 1609, las hoy conocidas Leyes de Kepler en su obra Astronomia Nova, en la que establece una órbita elíptica la cual se confirmó cuando predijo satisfactoriamente el tránsito de Venus del año 1631.
Junto a ellos el científico británico Isaac Newton formuló y dio una explicación al movimiento planetario mediante sus leyes y el desarrollo del concepto de la gravedad.

En el año 1704 se acuñó el término sistema solar. El científico británico Edmund Halley dedicó sus estudios principalmente al análisis de las órbitas de los cometas. El mejoramiento del telescopio durante este tiempo permitió a los científicos de todo el mundo descubrir nuevas características de los cuerpos celestes que existen. A mediados del siglo XX, el 12 de abril de 1961, el cosmonauta Yuri Gagarin se convirtió en el primer hombre en el espacio; la misión estadounidense Apolo 11 al mando de Neil Armstrong llega a la Luna. En la actualidad, el sistema solar se estudia con ayuda de telescopios terrestres, observatorios espaciales y misiones espaciales.

Los planetas y los asteroides orbitan alrededor del Sol, aproximadamente en un mismo plano y siguiendo órbitas elípticas (en sentido antihorario, si se observasen desde el Polo Norte del Sol); aunque hay excepciones, como el cometa Halley, que gira en sentido horario. El plano en el que gira la Tierra alrededor del Sol se denomina plano de la eclíptica, y los demás planetas orbitan aproximadamente en el mismo plano. Aunque algunos objetos orbitan con un gran grado de inclinación respecto de este, como Plutón que posee una inclinación con respecto al eje de la eclíptica de 17º, así como una parte importante de los objetos del cinturón de Kuiper.

Según sus características, los cuerpos que forman parte del sistema solar se clasifican como sigue:
El espacio interplanetario en torno al Sol contiene material disperso procedente de la evaporación de cometas y del escape de material proveniente de los diferentes cuerpos masivos. El polvo interplanetario (especie de polvo interestelar) está compuesto de partículas microscópicas sólidas. El gas interplanetario es un tenue flujo de gas y partículas cargadas que forman un plasma que es expulsado por el Sol en el viento solar. El límite exterior del sistema solar se define a través de la región de interacción entre el viento solar y el medio interestelar originado de la interacción con otras estrellas. La región de interacción entre ambos vientos se denomina heliopausa y determina los límites de influencia del Sol. La heliopausa puede encontrarse a unas 100 UA (15 000 millones de kilómetros del Sol).

Los sistemas planetarios detectados alrededor de otras estrellas parecen muy diferentes del sistema solar, si bien con los medios disponibles solo es posible detectar algunos planetas de gran masa en torno a otras estrellas. Por tanto, no parece posible determinar hasta qué punto el sistema solar es característico o atípico entre los sistemas planetarios del Universo.

Las órbitas de los planetas mayores se encuentran ordenadas a distancias del Sol crecientes, de modo que la distancia de cada planeta es aproximadamente el doble que la del planeta inmediatamente anterior, aunque esto no se ajusta a todos los planetas. Esta relación se expresa mediante la ley de Titius-Bode, una fórmula matemática aproximada que indica la distancia de un planeta al Sol, en Unidades Astronómicas (UA):

Donde la órbita de Mercurio se encuentra en k = 0 y semieje mayor 0,4 UA, la órbita de Marte es k = 4 a 1,6 UA, y Ceres (el mayor asteroide) es k = 8. En realidad las órbitas de Mercurio y Marte se encuentran en 0,38 y 1,52 UA. Esta ley no se ajusta a todos los planetas, por ejemplo Neptuno está mucho más cerca de lo que predice esta ley. No hay ninguna explicación de la ley de Titius-Bode y muchos científicos consideran que se trata tan solo de una coincidencia.

El sistema solar se formó hace 4568 millones de años por el colapso gravitatorio de una parte de una nube molecular gigante. Esta nube primigenia tenía varios años luz de diámetro y probablemente dio a luz a varias estrellas. Como es normal en las nubes moleculares, consistía principalmente de hidrógeno, algo de helio y pequeñas cantidades de elementos pesados surgidos de previas generaciones estelares. A medida que la región —conocida como nebulosa protosolar— se convertía en el sistema solar, colapsaba y la conservación del momento angular hizo que rotase más deprisa. El centro, donde se acumuló la mayor parte de la masa, se volvió cada vez más caliente que el disco circundante. A medida que la nebulosa en contracción rotaba más deprisa, comenzó a aplanarse en un disco protoplanetario con un diámetro de alrededor de 200 ua  y una densa y caliente protoestrella en el centro. Los planetas se formaron por acreción a partir de este disco  en el que el gas y el polvo atraídos gravitatoriamente entre sí se unen para formar cuerpos cada vez más grandes. En este escenario, cientos de protoplanetas podrían haber surgido en el temprano sistema solar que acabaron fusionándose o fueron destruidos dejando los planetas, los planetas enanos y el resto de cuerpos menores.

Gracias a sus puntos de ebullición más altos, solo los metales y silicatos podían existir en forma sólida cerca del Sol, en el cálido sistema solar interior; estos fueron finalmente los componentes de Mercurio, Venus, la Tierra y Marte: los planetas rocosos. Debido a que los metales solo eran una pequeña parte de la nebulosa solar, los planetas terrestres no se podían hacer muy grandes. Los planetas gigantes (Júpiter, Saturno, Urano y Neptuno) se formaron más lejos, más allá de la línea de congelación: el límite entre las órbitas de Marte y Júpiter donde las temperaturas son lo suficientemente bajas como para que los compuestos volátiles permanezcan sólidos. Los hielos que forman estos planetas eran más abundantes que los metales y silicatos que formaron los planetas terrestres interiores, por lo que los permitió crecer hasta ser lo suficientemente masivos como para capturar grandes atmósferas de hidrógeno y helio: los elementos más ligeros y abundantes. Los residuos restantes que no llegaron a convertirse en planetas se agruparon en regiones como el cinturón de asteroides, el cinturón de Kuiper y la nube de Oort. El modelo de Niza explica la aparición de estas regiones y propone que los planetas exteriores se podrían haber formado en sitios diferentes de los actuales a los que habrían llegado tras múltiples interacciones gravitatorias.

Tras cincuenta millones de años, la densidad del hidrógeno y la presión en el centro de la protoestrella se hicieron tan grandes que comenzó la fusión termonuclear. La temperatura, la velocidad de reacción, la presión y la densidad aumentaron hasta alcanzar el equilibrio hidrostático: la presión térmica igualó a la fuerza de la gravedad. En ese momento, el Sol entró en la secuencia principal. El tiempo que estará en la secuencia principal será de unos diez mil millones de años; en comparación, todas las fases previas al encendido termonuclear duraron unos dos mil millones de años. El viento solar formó la heliosfera que barrió los restos de gas y polvo del disco protoplanetario (y los expulsó al espacio interestelar), con lo que terminó el proceso de formación planetaria. Desde entonces, el Sol se ha ido haciendo cada vez más brillante; en la actualidad es un 70% más brillante que a su entrada en la secuencia principal.

El sistema solar continuará más o menos como lo conocemos hasta que todo el hidrógeno del núcleo del Sol se haya convertido en helio, situación que tendrá lugar dentro de cinco mil millones de años. Esto marcará el final de la estancia del Sol en la secuencia principal. En ese momento el núcleo colapsará y la producción de energía será mucho mayor que en el presente. Las capas exteriores se expandirán unas doscientas sesenta veces su diámetro actual, por lo que se convertirá en una gigante roja. El gran aumento de su superficie hará que esté muchísimo más frío (del orden de 2600 K). Se espera que el Sol en expansión vaporice Mercurio y Venus y vuelva la Tierra inhabitable al mover la zona de habitabilidad más allá de la órbita de Marte. Por último, el núcleo estará lo bastante caliente para fusionar el helio; el Sol quemará helio durante una fracción del tiempo que estuvo quemando hidrógeno. El Sol no tiene la suficiente masa para comenzar la fusión de elementos pesados, por lo que las reacciones nucleares en el núcleo disminuirán. Las capas exteriores se perderán en el espacio en forma de nebulosa planetaria, devolviendo parte del material con el que se formó el Sol —enriquecido con elementos pesados como el carbono— al medio interestelar y dejando atrás una enana blanca con la mitad de la masa original del Sol y el tamaño de la Tierra (un objeto extraordinariamente denso).

Los principales objetos del sistema solar son:

El Sol es la estrella única y central del sistema solar; por tanto, es la estrella más cercana a la Tierra y el astro con mayor brillo aparente. Su presencia o su ausencia en el cielo terrestre determinan, respectivamente, el día y la noche. La energía radiada por el Sol es aprovechada por los seres fotosintéticos, que constituyen la base de la cadena trófica, y es por ello la principal fuente de energía de la vida. También aporta la energía que mantiene en funcionamiento los procesos climáticos. El Sol es una estrella que se encuentra en la fase denominada secuencia principal, con un tipo espectral G2, que se formó hace unos 5000 millones de años, y permanecerá en la secuencia principal aproximadamente otros 5000 millones de años.

A pesar de ser una estrella mediana, es la única cuya forma circular se puede apreciar a simple vista, con un diámetro angular de 32' 35" de arco en el perihelio y 31' 31" en el afelio, lo que da un diámetro medio de 32' 03". Casualmente, la combinación de tamaños y distancias del Sol y la Luna respecto a la Tierra, hace que se vean aproximadamente con el mismo tamaño aparente en el cielo. Esto permite una amplia gama de eclipses solares distintos (totales, anulares o parciales).

Se han descubierto sistemas planetarios que tienen más de una estrella central (sistema estelar).

Los ocho planetas que componen el sistema solar son, de menor a mayor distancia respecto al Sol, los siguientes:

Los planetas son cuerpos que giran formando órbitas alrededor de la estrella, tienen suficiente masa para que su gravedad supere las fuerzas del cuerpo rígido, de manera que asuman una forma en equilibrio hidrostático (prácticamente esférica), y han limpiado la vecindad de su órbita de planetesimales (dominancia orbital).

Los planetas interiores son Mercurio, Venus, la Tierra y Marte y tienen la superficie sólida. Los planetas exteriores son Júpiter, Saturno, Urano y Neptuno, también se denominan planetas gaseosos porque contienen en sus atmósferas gases como el helio, el hidrógeno y el metano, y no se conoce con certeza la estructura de su superficie.

El 24 de agosto de 2006, la Unión Astronómica Internacional (UAI) excluyó a Plutón como planeta del sistema solar, y lo clasificó como planeta enano.

A principios del año 2016 se publicó un estudio según el cual puede existir un noveno planeta en el sistema Solar, al que dieron el nombre provisional de Phattie. Dicho estudio se centró en la explicación de las órbitas de muchos de los objetos en el cinturón de Kuiper, que difieren mucho con las órbitas que se calculan, incluidos objetos muy conocidos Sedna. Por tanto se surgió originalmente la idea de la existencia de un objeto no conocido perturbando dichas órbitas. Utilizando modelos matemáticos se realizaron simulaciones en computadora, y se determinó que el posible planeta tendría una órbita excéntrica a una distancia de unas entre 700 y 200UA del Sol, y tardaría unos diez o veinte mil años en dar una vuelta.

Las principales características de los planetas del sistema solar son:
  

Los cinco planetas enanos del sistema solar, de menor a mayor distancia respecto al Sol, son los siguientes:

Los planetas enanos son aquellos que, a diferencia de los planetas, no han limpiado la vecindad de su órbita.

Poco después de su descubrimiento en 1930, Plutón fue clasificado como un planeta por la Unión Astronómica Internacional (UAI). Sin embargo, tras el descubrimiento de otros grandes cuerpos con posterioridad, se abrió un debate con objeto de reconsiderar dicha decisión. El 24 de agosto de 2006, en la XXVI Asamblea General de la UAI en Praga, se decidió que el número de planetas no se ampliase a doce, sino que debía reducirse de nueve a ocho, y se creó entonces la nueva categoría de planeta enano, en la que se clasificaría Plutón, que dejó por tanto de ser considerado planeta debido a que, por tratarse de un objeto transneptuniano perteneciente al cinturón de Kuiper, no ha limpiado la vecindad de su órbita de objetos pequeños.

Algunos satélites del sistema solar son tan grandes que, si se encontraran orbitando directamente alrededor del Sol, se clasificarían como planetas o como planetas enanos; por orbitar a los planetas principales, estos cuerpos pueden denominarse «planetas secundarios». El siguiente listado recoge los satélites del sistema solar que mantienen un equilibrio hidrostático:

Los cuerpos menores del sistema solar están agrupados en:


Un cuerpo menor del sistema solar ("CMSS" o del inglés "SSSB", "small Solar System body") es, según la resolución de la UAI (Unión Astronómica Internacional) del 22 de agosto de 2006, un cuerpo celeste que orbita en torno al Sol y que no es planeta, ni planeta enano, ni satélite:

Por consiguiente, según la definición de la UAI, son cuerpos menores del Sistema Solar, independientemente de su órbita y composición:

Según las definiciones de planeta y de planeta enano, que atienden a la esfericidad del objeto debido a su gran masa, se puede definir como «cuerpo menor del sistema solar», por exclusión, a todo cuerpo celeste que, sin ser un satélite, no haya alcanzado suficiente tamaño o masa como para adoptar una forma esencialmente esférica.

Según algunas estimaciones, la masa requerida para alcanzar la condición de esfericidad se situaría en torno a los 5 x 10 kg, resultando el diámetro mínimo en torno a los 800 km. Sin embargo, características como la composición química, la temperatura, la densidad o la rotación de los objetos pueden variar notablemente los tamaños mínimos requeridos, por lo que se rechazó asignar valores apriorísticos a la definición, dejando la resolución individual de cada caso a la observación directa.

Según la UAI, algunos de los cuerpos menores del sistema solar más grandes podrían reclasificarse en el futuro como planetas enanos, tras un examen para determinar si están en equilibrio hidrostático, es decir: si son suficientemente grandes para que su gravedad venza las fuerzas del sólido rígido hasta haber adoptado una forma esencialmente esférica.

Exceptuando los objetos transneptunianos, los cuerpos menores del sistema solar de mayor tamaño son Vesta y Palas, con algo más de 500 km de diámetro.

Para tener una noción de la dimensión astronómica de las distancias en el espacio, es interesante hacer un modelo a escala que permita tener una percepción más clara del mismo. Imagínese un modelo reducido en el que el Sol esté representado por una pelota de 220 mm de diámetro. A esa escala, la Tierra estaría a 23,6 m de distancia y sería una esfera con apenas 2 mm de diámetro (la Luna estaría a unos 5 cm de la tierra y tendría un diámetro de unos 0,5 mm). Júpiter y Saturno serían bolitas con cerca de 2 cm de diámetro, a 123 y a 226 m del Sol, respectivamente. Plutón estaría a 931 m del Sol, con cerca de 0,3 mm de diámetro. En cuanto a la estrella más próxima (Próxima Centauri), estaría a 6 332 km del Sol, y la estrella Sirio, a 13 150 km.

Si se tardase 1 h y cuarto en ir de la Tierra a la Luna (a unos 257 000 km/h), se tardaría unas tres semanas (terrestres) en ir de la Tierra al Sol, unos 3 meses en ir a Júpiter, 7 meses a Saturno y unos dos años y medio en llegar a Plutón y abandonar el sistema solar. A partir de ahí, a esa velocidad, sería necesario esperar unos 17 600 años hasta llegar a la estrella más próxima, y 35 000 años hasta llegar a Sirio.

Una escala comparativa más exacta puede tenerse si se compara el Sol con un disco compacto de 12 cm de diámetro. A esta escala, la Tierra tendría poco más de un milímetro de diámetro (1,1 mm). El Sol estaría a 6,44 metros. El diámetro de la estrella más grande del Universo conocido, VY Canis Majoris, sería de 264 metros (imagínese esa enorme estrella de casi tres manzanas de casas de tamaño, en comparación con nuestra estrella de 12 cm). La órbita externa de Eris se alejaría a 625,48 metros del Sol. Allí nos espera un gran vacío hasta la estrella más cercana, Próxima Centauri, a 1645,6 km de distancia. A partir de allí, las distancias galácticas exceden el tamaño de la Tierra (aun utilizando la misma escala). Con un Sol del tamaño de un disco compacto, el centro de la galaxia estaría a casi 11 millones de kilómetros y el diámetro de la Vía Láctea sería de casi 39 millones de kilómetros. Habría un enorme vacío, pues la galaxia Andrómeda estaría a 1028 millones de kilómetros, casi la distancia real entre el Sol y Saturno.






</doc>
<doc id="2564" url="https://es.wikipedia.org/wiki?curid=2564" title="Sainete">
Sainete

Sainete es una pieza dramática jocosa en un acto, de carácter costumbrista y popular, representado en España durante el intermedio o al final de una función. Sustituyó al entremés en los siglos XVIII, XIX y XX. 

Entre los principales cultivadores de este subgénero cómico en el siglo XVIII se encuentran los gaditanos Luis Moncín y Juan Ignacio González del Castillo, y los madrileños Ramón de la Cruz y Sebastián Vázquez; otros autores menos conocidos fueron, entre muchos otros, Antonio Pablo Fernández, Antonio Furmento Bazo, Diego Ventura Rejón de Silva y Lucas, Antonio Vidaurre, José López de Sedano, Antonio Valladares de Sotomayor y Gaspar Zavala y Zamora. A finales del siglo XIX fue materia frecuente del llamado género chico y del teatro por horas, con autores especializados como Tomás Luceño y Javier de Burgos, y revitalizaron el género en el siglo XX Carlos Arniches con su colección de sainetes "Del Madrid castizo" y los hermanos Serafín y Joaquín Álvarez Quintero. Posteriormente en el Río de la Plata, Armando Discépolo introducirá un giro sombrío y dramático en este género transformándolo en el "Grotesco criollo".

Según Voss, en la historia del desarrollo del sainete pueden observarse cuatro etapas:

1. (1603-1750). En esta tuvo lugar la transformación del término sainete del campo culinario al campo artístico. Ya estaban prescritas algunas características como la poca extensión de las piezas y la mezcla de humor y moralidad, del habla canto y baile.

2. (1760-1868). Es la época en que el sainete llegó a ser un género literario gracias a las creaciones de Ramón de la Cruz, mientras que también se modificó su temática frente al entremés. (254).

3. (1868-1894) El sainete recobró rigor de la mano de Tomás Luceño. Con una extensión más amplia (hasta 45 min.) ya no tiene lugar en las pausas entre los actos.

4. (1894-1915). Este período puede calificarse como la etapa de la decadencia porque el sainete se orienta más y más hacia otros géneros, especialmente hacia la zarzuela y el melodrama, que tuvieron influencia en su desarrollo posterior; hasta que finalmente el sainete fue absorbido por la «comedia asainetada».

El sainete valenciano pretendió ser un reflejo de la vida social de la Comunidad Valenciana (España) de estos siglos. Una de sus características recurrentes es que los personajes de las clases bajas hablaban valenciano, mientras que los forasteros, los miembros de la burguesía o todo aquel que tenía una voluntad de no ser clasificado o de aparentar más riqueza y educación, hablaban un castellano plagado de valencianismos y de incorrecciones. La crítica que se realiza de esta presunción es moral y, evidentemente, sociolingüística.

Entre los sainetistas valencianos más destacados encontramos: Eduardo Escalante, Josep Bernat i Baldoví y Francisco Palanca Roca.

En la Argentina, el sainete combinado con las formas del circo, dio como resultado una modalidad original conocida como sainete criollo. El sainete criollo se caracterizó por reflejar las costumbres de la vida en los conventillos, agregando a los elementos humorísticos un conflicto sentimental y una acción trágica. Esta forma teatral se afianzó durante la década de 1920. En esta época se destacaron, además de Carlos M. Pacheco y Alberto Vacarezza, autores como Florencio Sánchez, Gregorio de Laferrere y Roberto Payró


El sainet valencià durant el segle XX en valenciano.


</doc>
<doc id="2566" url="https://es.wikipedia.org/wiki?curid=2566" title="Silicio">
Silicio

El silicio (del latín: "sílex") es un elemento químico metaloide, número atómico 14 y situado en el grupo 14 de la tabla periódica de los elementos de símbolo Si. Es el segundo elemento más abundante en la corteza terrestre (25,7 % en peso) después del oxígeno. Se presenta en forma amorfa y cristalizada; el primero es un polvo parduzco, más activo que la variante cristalina, que se presenta en octaedros de color azul grisáceo y brillo metálico.

Sus propiedades son intermedias entre las del carbono y el germanio. En forma cristalina es muy duro y poco soluble y presenta un brillo metálico y color grisáceo. Aunque es un elemento relativamente inerte y resiste la acción de la mayoría de los ácidos, reacciona con los halógenos y álcalis diluidos. El silicio transmite más del 95 % de las longitudes de onda de la radiación infrarroja.

Se prepara en forma de polvo amarillo pardo o de cristales negros-grisáceos. Se obtiene calentando sílice, o dióxido de silicio (SiO), con un agente reductor, como carbono o magnesio, en un horno eléctrico. El silicio cristalino tiene una dureza de 7, suficiente para rayar el vidrio, de dureza de 5 a 7. El silicio tiene un punto de fusión de 1.411 °C, un punto de ebullición de 2.355 °C y una densidad relativa de 2,33(g/ml). Su masa atómica es 28,086 u (unidad de masa atómica).

Se disuelve en ácido fluorhídrico formando el gas tetrafluoruro de silicio, SiF (ver flúor), y es atacado por los ácidos nítrico, clorhídrico y sulfúrico, aunque el dióxido de silicio formado inhibe la reacción. También se disuelve en hidróxido de sodio, formando silicato de sodio y gas hidrógeno. A temperaturas ordinarias el silicio no es atacado por el aire, pero a temperaturas elevadas reacciona con el oxígeno formando una capa de sílice que impide que continúe la reacción. A altas temperaturas reacciona también con nitrógeno y cloro formando nitruro de silicio y cloruro de silicio, respectivamente.

El silicio constituye un 28 % de la corteza terrestre. No existe en estado libre, sino que se encuentra en forma de dióxido de silicio y de silicatos complejos. Los minerales que contienen silicio constituyen cerca del 40 % de todos los minerales comunes, incluyendo más del 90 % de los minerales que forman rocas volcánicas. El mineral cuarzo, sus variedades (cornalina, crisoprasa, ónice, pedernal y jaspe) y los minerales cristobalita y tridimita son las formas cristalinas del silicio existentes en la naturaleza. El dióxido de silicio es el componente principal de la arena. Los silicatos (en concreto los de aluminio, calcio y magnesio) son los componentes principales de las arcillas, el suelo y las rocas, en forma de feldespatos, anfíboles, piroxenos, micas y zeolitas, y de piedras semipreciosas como el olivino, granate, zircón, topacio y turmalina.

Sus características compartidas con el carbono, como estar en la misma familia 14, no ser metal propiamente dicho, poder construir compuestos parecidos a las enzimas (zeolitas), otros compuestos largos con oxígeno (siliconas) y poseer los mismos cuatro enlaces básicos, le confiere cierta oportunidad en llegar a ser base de seres vivos, aunque no sea en la Tierra, en una bioquímica hipotética.

Se utiliza en aleaciones, en la preparación de las siliconas, en la industria de la cerámica técnica y, debido a que es un material semiconductor muy abundante, tiene un interés especial en la industria electrónica y microelectrónica como material básico para la creación de obleas o chips que se pueden implantar en transistores, pilas solares y una gran variedad de circuitos electrónicos.
El silicio es un elemento vital en numerosas industrias. El dióxido de silicio (arena y arcilla) es un importante constituyente del hormigón y los ladrillos, y se emplea en la producción de cemento portland. Por sus propiedades semiconductoras se usa en la fabricación de transistores, células solares y todo tipo de dispositivos semiconductores; por esta razón se conoce como el Valle del Silicio a la región de California en la que concentran numerosas empresas del sector de la electrónica y la informática. También se están estudiando las posibles aplicaciones del siliceno, que es una forma alotrópica del silicio que forma una red bidimensional similar al grafeno. Otros importantes usos del silicio son:


Se utiliza en la industria del acero como componente de las aleaciones de silicio-acero. Para fabricar el acero, se desoxida el acero fundido añadiéndole pequeñas cantidades de silicio; el acero común contiene menos de un 0,30 % de silicio. El acero al silicio, que contiene de 2,5 a 4 % de silicio, se usa para fabricar los núcleos de los transformadores eléctricos, pues la aleación presenta baja histéresis (véase Magnetismo). Existe una aleación de acero, el durirón, que contiene un 15 % de silicio y es dura, frágil y resistente a la corrosión; el durirón se usa en los equipos industriales que están en contacto con productos químicos corrosivos. El silicio se utiliza también en las aleaciones de cobre, como el bronce y el latón.

El silicio es un semiconductor; su resistividad a la corriente eléctrica a temperatura ambiente varía entre la de los metales y la de los aislantes. La conductividad del silicio se puede controlar añadiendo pequeñas cantidades de impurezas llamadas dopantes. La capacidad de controlar las propiedades eléctricas del silicio y su abundancia en la naturaleza han posibilitado el desarrollo y aplicación de los transistores y circuitos integrados que se utilizan en la industria electrónica.

La sílice y los silicatos se utilizan en la fabricación de vidrio, barnices, esmaltes, cemento y porcelana, y tienen importantes aplicaciones individuales. La sílice fundida, que es un vidrio que se obtiene fundiendo cuarzo o hidrolizando tetracloruro de silicio, se caracteriza por un bajo coeficiente de dilatación y una alta resistencia a la mayoría de los productos químicos. El gel de sílice es una sustancia incolora, porosa y amorfa; se prepara eliminando parte del agua de un precipitado gelatinoso de ácido silícico, SiO•HO, el cual se obtiene añadiendo ácido clorhídrico a una disolución de silicato de sodio. El gel de sílice absorbe agua y otras sustancias y se usa como agente desecante y decolorante.

El silicato de sodio (NaSiO), también llamado vidrio, es un silicato sintético importante, sólido amorfo, incoloro y soluble en agua, que funde a 1088 °C. Se obtiene haciendo reaccionar sílice (arena) y carbonato de sodio a alta temperatura, o calentando arena con hidróxido de sodio concentrado a alta presión. La disolución acuosa de silicato de sodio se utiliza para conservar huevos; como sustituto de la cola o pegamento para hacer cajas y otros contenedores; para unir gemas artificiales; como agente incombustible, y como relleno y adherente en jabones y limpiadores. Otro compuesto de silicio importante es el carborundo, un compuesto de silicio y carbono que se utiliza como abrasivo.

El monóxido de silicio, SiO, se usa para proteger materiales, recubriéndolos de forma que la superficie exterior se oxida al dióxido, SiO. Estas capas se aplican también a los filtros de interferencias.

Fue identificado por primera vez por Antoine Lavoisier en 1787.

El silicio es uno de los componentes principales de los aerolitos, una clase de meteoroides.

Medido en peso, el silicio representa más de la cuarta parte de la corteza terrestre y es el segundo elemento más abundante por detrás del oxígeno. El silicio no se encuentra en estado nativo; arena, cuarzo, amatista, ágata, pedernal, ópalo y jaspe son algunos de los minerales en los que aparece el óxido, mientras que formando silicatos se encuentra, entre otros, en el granito, feldespato, arcilla, hornblenda y mica.

Estos métodos se basan en la mayor solubilidad de las impurezas en el silicio líquido, de forma que éste se concentra en las últimas zonas solidificadas. El primer método, usado de forma limitada para construir componentes de radar durante la Segunda Guerra Mundial, consiste en moler el silicio de forma que las impurezas se acumulen en las superficies de los granos; disolviendo éstos parcialmente con ácido se obtenía un polvo más puro. La fusión por zonas, el primer método usado a escala industrial, consiste en fundir un extremo de la barra de silicio y trasladar lentamente el foco de calor a lo largo de la barra de modo que el silicio va solidificando con una pureza mayor al arrastrar la zona fundida gran parte de las impurezas. El proceso puede repetirse las veces que sea necesario hasta lograr la pureza deseada bastando entonces cortar el extremo final en el que se han acumulado las impurezas.

Los métodos químicos, usados actualmente, actúan sobre un compuesto de silicio que sea más fácil de purificar descomponiéndolo tras la purificación para obtener el silicio. Los compuestos comúnmente usados son el triclorosilano (HSiCl), el tetracloruro de silicio (SiCl) y el silano (SiH).

En el proceso Siemens, las barras de silicio de alta pureza se exponen a 1150 °C al triclorosilano, gas que se descompone depositando silicio adicional en la barra según la siguiente reacción:

El silicio producido por éste y otros métodos similares se denomina silicio policristalino y típicamente tiene una fracción de impurezas de 0,001 ppm o menor.

El método Dupont consiste en hacer reaccionar tetracloruro de silicio a 950 °C con vapores de cinc muy puros:

Este método está plagado de dificultades (el cloruro de cinc, sub producto de la reacción, solidifica y obstruye las líneas), por lo que eventualmente se ha abandonado en favor del proceso Siemens.

Una vez obtenido el silicio ultrapuro es necesario obtener un monocristal, para lo que se utiliza el proceso Czochralski.

A continuación se presentan las distintas alternativas de producción de SoG-Si.Todas ellas se han recogido y presentado desde 2004 en las Conferencias sobre Silicio Solar. Estas conferencias se han organizado anualmente por la revista Photon International en Múnich, a raíz de la preocupación creciente por la escasez de polisilicio. Hasta ahora ninguna de estas alternativas ha conseguido llegar a la etapa de producción, aunque algunas se encuentran cerca.

Wacker Chemie, Hemlock y Solar Grade Silicon proponen un reactor de lecho fluidizado. Éste consiste en un tubo de cuarzo en el que se introduce triclorosilano (Wacker, Hemlock) o silano (SGS) por la parte inferior, junto con hidrógeno. El gas pasa a través de un lecho de partículas de silicio sobre las que ocurre el depósito, dando así partículas de tamaño mayor. Alcanzado cierto tamaño, las partículas son demasiado pesadas y caen al suelo, pudiendo ser retiradas. Este proceso no solamente utiliza una cantidad de energía mucho menor que el Siemens, sino que además puede realizarse de forma continua.

Joint Solar Silicon GmbH & Co. KG (JSSI) presenta un reactor similar al Siemens, cuyas diferencias son: a.) el silicio se deposita en un cilindro hueco de silicio en lugar de varillas; b.) se utiliza silano en lugar de triclorosilano, y por tanto la temperatura del proceso puede limitarse a 800 °C.

Tokuyama Corporation propone su proceso VLD (Vapour to Liquid Deposition). En un reactor se calienta un tubo de grafito a 1500 °C, por encima del punto de fusión del silicio. Se alimentan triclorosilano e hidrógeno por la parte superior. El silicio se deposita en las paredes de grafito en forma líquida. Por tanto, gotea en el suelo del reactor, donde solidifica en granulados y puede recogerse. El mayor gasto energético con respecto al reactor Siemens compensa por la velocidad de depósito 10 veces mayor.

Chisso Corporation y el gobierno japonés investigan un proceso a partir de la reducción de tetracloruro de silicio (SiCl4) con vapor de zinc (Zn). Se forma cloruro de cinc y silicio. Esta alternativa se desechó en los años 80 por Bayer AG ya que no se podían eliminar trazas de metales residuales. Chisso asegura que sus impurezas metálicas se encuentran en un nivel aceptable.

También se han realizado grandes esfuerzos en conseguir SoG-Si evitando el paso energéticamente costoso del uso de triclorosilano, silano o tetraclorosilano, y el posterior depósito en Siemens o similares.

Elkem purifica mg-Si en tres pasos de refino relativamente simples, pirometalúrgico, hidrometalúrgico, y de limpieza, con un consumo de sólo el 20 al 25 % de la energía utilizada en la ruta Siemens. Junto con la Universidad de Constanza, han conseguido eficiencias de célula sólo medio punto por debajo de las células comerciales.

Apollon Solar SAS y el laboratorio nacional de investigación francés CNRS purifican Mg-Si con un plasma. Se han conseguido células solares de un 11,7 % de eficiencia.

Otra alternativa metalúrgica es producir mg-Si con cuarzo y carbón negro tan puros que no sea necesario refinarlo más. Hay dos trabajos en paralelo: uno es el de la Universidad Nacional Técnica de Kazakh en Almaty, Kazajistán. El otro es el proyecto SOLSILC, financiado por la Comisión Europea. Las células solares fabricadas con este material han obtenido eficiencias de momento relativamente bajas. 28 por ciento de este material ya no existe.

El silicio tiene nueve isótopos, con número másico entre 25 a 33. El isótopo más abundante es el Si-28 con una abundancia del 92,23 %, el Si-29 tiene una abundancia del 4,67 % y el Si-30 que tiene una abundancia del 3,1 %. Todos ellos son estables teniendo el resto de isótopos una proporción ínfima. El Si-32 es un isótopo radiactivo que proviene del decaimiento del argón. Su tiempo de semivida es aproximadamente de unos 132 años. Padece un decaimiento beta que lo transforma en P-32 (que tiene un periodo de semivida de 14,28 días).

La inhalación del polvo de sílice cristalina puede provocar silicosis.




</doc>
<doc id="2567" url="https://es.wikipedia.org/wiki?curid=2567" title="Saturno (planeta)">
Saturno (planeta)

Saturno es el sexto planeta del sistema solar, el segundo en tamaño y masa después de Júpiter y el único con un sistema de anillos visible desde nuestro planeta. Su nombre proviene del dios romano Saturno. Forma parte de los denominados planetas exteriores o gaseosos. El aspecto más característico de Saturno son sus brillantes anillos. Antes de la invención del telescopio, Saturno era el más lejano de los planetas conocidos y, a simple vista, no parecía luminoso ni interesante. El primero en observar los anillos fue Galileo en 1610, pero la baja inclinación de los anillos y la baja resolución de su telescopio le hicieron pensar en un principio que se trataba de grandes lunas. Christiaan Huygens, con mejores medios de observación, pudo en 1659 observar con claridad los anillos. James Clerk Maxwell, en 1859, demostró matemáticamente que los anillos no podían ser un único objeto sólido sino que debían ser la agrupación de millones de partículas de menor tamaño. Las partículas que componen los anillos de Saturno giran a una velocidad de 48 000 km/h, 15 veces más rápido que una bala.

Debido a su posición orbital más lejana que Júpiter, los antiguos romanos le otorgaron el nombre del padre de Júpiter al planeta Saturno. En la mitología romana, Saturno era el equivalente del antiguo titán griego Crono, hijo de Urano y Gea, que gobernaba el mundo de los dioses y los hombres devorando a sus hijos en cuanto nacían para que no lo destronaran. Zeus, uno de ellos, consiguió esquivar este destino y finalmente derrocó a su padre para convertirse en el dios supremo.

Los griegos y romanos, herederos de los sumerios en sus conocimientos del cielo, habían establecido en siete el número de astros que se movían en el firmamento: el Sol, la Luna, y los planetas Mercurio, Venus, Marte, Júpiter y Saturno, las estrellas «errantes» que, a distintas velocidades, orbitaban en torno a la Tierra, centro del universo. De los cinco planetas, Saturno es el de movimiento más lento, emplea unos treinta años (29,457 años) en completar su órbita, casi el triple que Júpiter (11,862 años) y respecto a Mercurio, Venus y Marte la diferencia es mucho mayor. Saturno destacaba por su lentitud y si Júpiter era Zeus, Saturno tenía que ser Crono, el padre anciano, que paso a paso deambula entre las estrellas.

Saturno es un planeta visiblemente achatado en los polos con un ecuador que sobresale formando un esferoide ovalado. Los diámetros ecuatorial y polar son de 120 536 y 108 728 km, respectivamente. Este efecto es producido por la rápida rotación del planeta, su naturaleza fluida y su relativamente baja gravedad. Los otros planetas gigantes son también ovalados pero en menor medida. Saturno posee una densidad específica de aproximadamente 690 kg/m³, siendo el único planeta del sistema solar con una densidad inferior a la del agua (1000 kg/m³). El planeta está formado por un 96 % de hidrógeno y un 3 % de helio. El volumen del planeta es suficiente como para contener 740 veces la Tierra, pero su masa es solo 95 veces la terrestre, a causa de la ya mencionada baja densidad media.

El periodo de rotación de Saturno es incierto dado que no posee superficie y su atmósfera gira con un periodo distinto en cada latitud. Desde la época de los Voyager se consideraba que el periodo de rotación de Saturno, basándose en la periodicidad de señales de radio emitidas por él, era de 10 h 39 min 22,4 s (810,8°/día). Las misiones espaciales Ulysses y Cassini han mostrado que este periodo de emisión en radio varía en el tiempo, siendo en la actualidad de 10 h 45 m 45 s (± 36 s). La causa de este cambio en el periodo de rotación de radio podría estar relacionada con la actividad criovolcánica en forma de géiseres del satélite Encélado, que libera material en órbita de Saturno capaz de interaccionar con el campo magnético externo del planeta, utilizado para medir la rotación del núcleo interno donde se genera. En general se considera que el periodo de rotación interno del planeta puede ser conocido tan solo de forma aproximada.

Comparado con el planeta Tierra, el tamaño de Saturno es nueve veces mayor, y su órbita está nueve veces más lejos del Sol.

Los modelos planetarios típicos consideran que el interior del planeta es semejante al de Júpiter, con un núcleo rocoso rodeado por hidrógeno, helio y trazas de otras sustancias volátiles. Sobre él se extiende una extensa capa de hidrógeno líquido, debido a los efectos de las elevadas presiones y temperaturas. Los 30 000 km exteriores del planeta están formados por una extensa atmósfera de hidrógeno y helio. El interior del planeta probablemente contenga un núcleo formado por materiales helados acumulados en la formación temprana del planeta y que se encuentran en estado líquido en las condiciones de presión y temperatura cercanas al núcleo. Este se encuentra a temperaturas en torno a 12 000 K —aproximadamente el doble de la temperatura de la superficie del Sol—.

Por otro lado, y al igual que Júpiter y Neptuno, Saturno irradia más calor al exterior del que recibe del Sol. Una parte de esta energía está producida por una lenta contracción del planeta que libera la energía potencial gravitacional producida en la compresión. Este mecanismo se denomina mecanismo de Kelvin-Helmholtz. El calor extra generado se produce en una separación de fases entre el hidrógeno y el helio relativamente homogéneos que se están diferenciando desde la formación del planeta, liberando energía gravitatoria en forma de calor.
La atmósfera de Saturno posee un patrón de bandas oscuras y zonas claras similar al de Júpiter aunque la distinción entre ambas es mucho menos clara en el caso de Saturno. La atmósfera del planeta posee fuertes vientos en la dirección de los paralelos alternantes en latitud y altamente simétricos en ambos hemisferios a pesar del efecto estacional de la inclinación axial del planeta. El viento está dominado por una intensa y ancha corriente ecuatorial al nivel de la altura de las nubes que llegó a alcanzar velocidades de hasta 450 m/s en la época de los Voyager. A diferencia de Júpiter, no son aparentes grandes vórtices estables, aunque sí los hay más pequeños.

Es probable que las nubes superiores estén formadas por cristales de amoníaco. Sobre ellas parece extenderse una niebla uniforme sobre todo el planeta, producida por fenómenos fotoquímicos en la atmósfera superior —alrededor de 10 mbar—. A niveles más profundos —cerca de 10 bar de presión—, el agua de la atmósfera podría condensarse en una capa de nubes de agua que aún no ha podido ser observada.

Al igual que en Júpiter, ocasionalmente se forman tormentas en la atmósfera de Saturno, y algunas de ellas han podido observarse desde la Tierra. En 1933 se observó una mancha blanca situada en la zona ecuatorial por el astrónomo aficionado W. T. Hay. Era lo suficientemente grande como para ser visible con un refractor de 7 cm, pero no tardó en disiparse y desvanecerse. En 1962 empezó a desarrollarse una nueva mancha, pero no llegó nunca a destacar. En 1990 se pudo observar una gigantesca nube blanca en el ecuador de Saturno que ha sido asimilada a un proceso de formación de grandes tormentas. Se han observado manchas similares en placas fotográficas tomadas durante el último siglo y medio a intervalos de aproximadamente 30 años. En 1994 se pudo observar una segunda gran tormenta de aproximadamente la mitad de tamaño que la producida en 1990.

La sonda Cassini ha podido captar varias grandes tormentas en Saturno. Una de las mayores tormentas, con rayos 10 000 veces más potentes que los de cualquier tormenta de la Tierra, apareció el día 27 de noviembre de 2007, habiendo durado 7 meses y medio —lo que fue por un tiempo el récord de duración de una tormenta en el sistema solar—. Esta tormenta apareció en el hemisferio sur de Saturno, en una zona conocida como «callejón de las tormentas» por la elevada frecuencia con la que aparecen allí estos fenómenos. Este récord, sin embargo, ha sido batido por otra tormenta aparecida en la misma zona, que fue detectada en enero de 2009 y que duró hasta octubre de ese año. 

Una enorme tormenta, tan grande que rodeó el planeta, apareció en diciembre de 2010 en el hemisferio norte de Saturno desarrollando un vórtice central de color oscuro de 5000 kilómetros de ancho similar a la Gran Mancha Roja de Júpiter, siendo tan potente —mucho más que cualquier tormenta terrestre— que arrastró nubes de cristales de amoniaco de las profundidades de la atmósfera del planeta. Durante los aproximadamente 200 días que duró, fue estudiada con ayuda de la sonda Cassini y de telescopios terrestres, creció y se expandió hasta alcanzar un área ocho veces superior al de la Tierra, y pudieron captarse las ondas de radio producidas por el aparato eléctrico asociado a ella.

Las regiones polares presentan corrientes en chorro a 78ºN y 78ºS. Las sondas Voyager detectaron en los años 1980 un patrón hexagonal en la región polar norte que ha sido observado también por el telescopio espacial Hubble durante los años 1990. Las imágenes más recientes obtenidas por la sonda Cassini han mostrado el vórtice polar con gran detalle. Saturno es el único planeta conocido que posee un vórtice polar de estas características si bien los vórtices polares son comunes en las atmósferas de la Tierra o Venus.

En el caso del hexágono de Saturno, los lados tienen unos 13 800 kilómetros de longitud —algo más del diámetro de la Tierra— y la estructura rota con un periodo idéntico al de la rotación planetaria, siendo una onda estacionaria que no cambia su longitud ni estructura, como hacen el resto de nubes de la atmósfera. Estas formas poligonales entre tres y seis lados se han podido replicar mediante modelos de fluidos en rotación a escala de laboratorio.

Al contrario que el polo norte, las imágenes del polo sur muestran la presencia de una "corriente de chorro", pero no vórtices ni "ondas hexagonales persistentes". Sin embargo, NASA informó en noviembre de 2006 que la sonda Cassini había observado un "huracán" en el polo sur, con un ojo bien definido. Ojos de tormenta bien definidos solo habían sido observados en la Tierra —incluso no se ha logrado observarlo en la Gran Mancha Roja de Júpiter por la sonda Galileo—. Ese vórtice, de aproximadamente 8000 kilómetros de diámetro, ha podido ser fotografiado y estudiado con gran detalle por la sonda Cassini, midiéndose en él vientos de más de 500 kilómetros por hora.

En abril de 2010, la NASA hizo públicos unos vídeos e imágenes en los que se puede apreciar el aparato eléctrico asociado a las tormentas que se producen en la atmósfera de Saturno, la primera vez que se consigue esto.

Saturno gira alrededor del Sol a una distancia media de 1 418 millones de kilómetros en una órbita de excentricidad de 0,056, que sitúa el afelio a 1 500 millones de kilómetros, y el perihelio a 1 240 millones de km. Saturno se encontró en el perihelio en 1974. El periodo de traslación alrededor del Sol es de 29 años y 167 días, mientras que su período sinódico es de 378 días, de modo que, cada año, la oposición se produce con casi dos semanas de retraso respecto al año anterior. El período de rotación sobre su eje es corto, de 10 horas y 14 minutos, con algunas variaciones entre el ecuador y los polos.

Los elementos orbitales de Saturno son modificados en una escala de 900 años por una resonancia orbital de tipo 5:2 con el planeta Júpiter, bautizado por los astrónomos franceses del siglo XVIII como "la grande inégalité" (Júpiter completa 5 vueltas por cada 2 de Saturno). Los planetas no se encuentran en una resonancia perfecta, pero están lo suficientemente cercanos a ella como para que las perturbaciones a sus respectivas órbitas sean apreciables.

Saturno tiene un gran número de satélites (62 con órbitas regulares, a fecha de 2017) el mayor de los cuales, Titán es el único satélite del sistema solar con una atmósfera importante.

Los satélites más grandes, conocidos antes del inicio de la investigación espacial son:
Mimas, Encélado, Tetis, Dione, Rea, Titán, Hiperión, Jápeto y Febe. Tanto Encélado como Titán son objetos especialmente interesantes para los científicos planetarios, ya que en el primero se cree la posible existencia de agua líquida a poca profundidad de su superficie, sobre la base de la emisión de vapor de agua en géiseres y, el segundo, presenta una atmósfera rica en metano y similar a la de la Tierra primitiva.

Otros 30 satélites de Saturno tienen nombre, pero el número exacto es incierto por existir una gran cantidad de objetos que orbitan este planeta. En el año 2000, fueron detectados 12 nuevos satélites, cuyas órbitas sugieren que son fragmentos de objetos mayores capturados por Saturno. La misión Cassini-Huygens también ha encontrado nuevos satélites, la última de ellas anunciada el 3 de marzo de 2009 y que hace la número 61 del planeta.

El disco aparente de Titán —un borroso círculo anaranjado de bordes algo más oscuros— puede verse con telescopios de aficionados a partir de los 200 mm de abertura, utilizando para ello más de 300 aumentos y cielos estables: en sus mayores aproximaciones llega a medir 0,88 segundos de arco. El resto de los satélites son mucho menores y siempre parecen estrellas, incluso a gran aumento.

Los satélites más internos pueden capturarse, sin embargo, con cualquier cámara CCD empleando focales superiores a los 2 m.

La característica más notable de Saturno son sus anillos, que dejaron muy perplejos a los primeros observadores, incluido Galileo. Su telescopio no era tan potente como para revelar la verdadera naturaleza de lo que observaba y, por error de perspectiva, creyó que se trataba de dos cuerpos independientes que flanqueaban el planeta. Pocos años después, Saturno presentaba los anillos de perfil, y Galileo quedó muy sorprendido por la brusca desaparición de los dos hipotéticos compañeros del planeta. Por fin, la existencia del sistema de anillos fue determinada por Christiaan Huygens en 1659, con la ayuda de un telescopio más potente.

Los anillos de Saturno se extienden en el plano ecuatorial del planeta desde los 6630 km a los 120 700 km por encima del ecuador de Saturno y están compuestos de partículas con abundante agua helada. El tamaño de cada una de las partículas varía desde partículas microscópicas de polvo hasta rocas de unos pocos metros de tamaño. El elevado albedo de los anillos muestra que estos son relativamente modernos en la historia del sistema solar. En un principio se creía que los anillos de Saturno eran inestables a lo largo de períodos de decenas de millones de años, otro indicio de su origen reciente, pero los datos enviados por la sonda Cassini sugieren que son mucho más antiguos de lo que se pensaba en un principio. Los anillos de Saturno poseen una dinámica orbital muy compleja presentando ondas de densidad, e interacciones con los satélites de Saturno (especialmente con los denominados satélites pastores). Al estar en el interior del límite de Roche, los anillos no pueden evolucionar hacia la formación de un cuerpo mayor.

Los anillos se distribuyen en zonas de mayor y menor densidad de material existiendo claras divisiones entre estas regiones. Los anillos principales son los llamados anillos A y B, separados entre sí por la división de Cassini. En la región interior al anillo B se distinguen otro anillo más tenue aunque extenso: C y otro anillo tenue y fino: D. En el exterior se puede distinguir un anillo delgado y débil denominado anillo F. El tenue anillo E se extiende desde Mimas hasta Rea y alcanza su mayor densidad a la distancia de Encelado, el cual se piensa lo provee de partículas, debido a las emisiones de unos géiseres que se encuentran en su polo sur.

Hasta los años 1980 la estructura de los anillos se explicaba por medio de las fuerzas gravitacionales ejercidas por los satélites cercanos. Las sondas Voyager encontraron sin embargo estructuras radiales oscuras en el anillo B llamadas "cuñas radiales" (en inglés: "spokes") que no podían ser explicadas de esta manera ya que su rotación alrededor de los anillos no era consistente con la mecánica orbital. Se considera que estas estructuras oscuras interactúan con el campo magnético del planeta, ya que su rotación sobre los anillos seguía la misma velocidad que la magnetosfera de Saturno. Sin embargo el mecanismo preciso de su formación todavía se desconoce. Es posible que las "cuñas" aparezcan y desaparezcan estacionalmente.

El 17 de agosto de 2005 los instrumentos a bordo de la nave Cassini develaron que existe algo similar a una atmósfera alrededor del sistema de anillos, compuesta principalmente de oxígeno molecular. Los datos obtenidos han demostrado que la atmósfera en el sistema de anillos de Saturno es muy parecida a la de las lunas de Júpiter, Europa y Ganímedes.

El 19 de septiembre de 2006 la NASA anunció el descubrimiento de un nuevo anillo en Saturno, por la nave espacial Cassini durante una ocultación solar, cuando el Sol pasa directamente detrás de Saturno y Cassini viaja en la sombra dejada por Saturno con lo que los anillos tienen una iluminación brillante. Habitualmente una ocultación solar puede durar una hora pero el 17 de septiembre de 2006 duró 12 horas, siendo la más larga de la misión Cassini. La ocultación solar dio la oportunidad a Cassini de realizar un mapa de la presencia de partículas microscópicas que no son visibles normalmente, en el sistema de anillos.

El nuevo anillo, apenas perceptible, está entre el Anillo F y el Anillo G. Esta ubicación coincide con las órbitas de las lunas de Saturno Jano y Epimeteo, dos satélites coorbitales de Saturno cuyas distancias al centro de Saturno se diferencian menos que el tamaño de dichos satélites, por lo que describen una extraña danza que los lleva a intercambiar sus órbitas. Los investigadores de la NASA aseguraron que el impacto de meteoros en esas lunas ha hecho que otras partículas se unan al anillo.

Las cámaras a bordo de la nave Cassini captaron imágenes de un material helado que se extiende decenas de miles de kilómetros desde Encélado, otra confirmación de que la luna está lanzando material que podría formar el E. El satélite Encélado pudo ser visto a través del anillo E con sus chorros saliendo de su superficie semejando "dedos", dirigidos al anillo en cuestión. Estos chorros están compuestos de partículas heladas muy delgadas, que son expulsadas por los géiseres del Polo Sur de Encelado y entran en el anillo E.

«Tanto el nuevo anillo como las estructuras inesperadas del E nos dan una importante pista de cómo las lunas pueden lanzar pequeñas partículas y esculpir sus propios ambientes locales», dijo Matt Hedman, un investigador asociado a la Universidad Cornell en Ithaca, Nueva York.

La nave también tomó una fotografía en color de la Tierra, a cerca de 1 500 millones de kilómetros de distancia, en la que parece una esfera azul claro. En otra imagen, tomada en la misma fecha, puede apreciarse también la Luna. 

Carolyn Porco, responsable del equipo que opera las cámaras de la sonda Cassini en el Instituto de Ciencia Espacial de Boulder, en Colorado, dijo al respecto:

La NASA también anunció el 24 de octubre de 2007 el descubrimiento de un cinturón de microlunas en el borde exterior del anillo A y cuyo tamaño varía desde el de un camión pequeño al de un estadio, probablemente causado por la destrucción de una luna pequeña.

En octubre de 2009 el telescopio espacial Spitzer descubre un nuevo y enorme anillo alrededor de Saturno, mucho más grande de los que le rodean. Después de muchos siglos, este había pasado desapercibido hasta ahora, porque está tan enrarecido que resulta casi invisible. Este nuevo cinturón se despliega en el confín del sistema saturniano. Su masa comienza a unos seis millones de kilómetros del planeta y se extiende hasta alcanzar 13 millones de kilómetros de diámetro. Uno de los más lejanos satélites de Saturno, Febe, orbita dentro del nuevo anillo, y probablemente sea la fuente de su composición.

El campo magnético de Saturno es mucho más débil que el de Júpiter, y su magnetosfera es una tercera parte de la de Júpiter. La magnetosfera de Saturno consta de un conjunto de cinturones de radiación toroidales en los que están atrapados electrones y núcleos atómicos. Los cinturones se extienden unos 2 millones de kilómetros desde el centro de Saturno, e incluso más, en dirección contraria al Sol, aunque el tamaño de la magnetosfera varía dependiendo de la intensidad del viento solar (el flujo desde el Sol de las partículas cargadas). El viento solar y los satélites y anillos de Saturno suministran las partículas que están atrapadas en los cinturones de radiación. El periodo de rotación de 10 horas, 39 minutos y 25 segundos del interior de Saturno fue medido por el Voyager 1 mientras atravesaba la magnetosfera, que gira de forma sincrónica con el interior de Saturno. La magnetosfera interactúa con la ionosfera, la capa superior de la atmósfera de Saturno, causando emisiones aurorales de radiación ultravioleta; recientes estudios muestran que en el polo norte del planeta existe en vez de un anillo de varias auroras menores cómo en Júpiter ó la Tierra una única gran aurora de forma anillada.

Rodeando la órbita de Titán, y extendiéndose hasta la órbita de Rea, se encuentra una enorme nube toroidal de átomos de hidrógeno neutro. Un disco de plasma, compuesto de hidrógeno y posiblemente de iones oxígeno, se extiende desde fuera de la órbita de Tetis hasta casi la de Titán. El plasma gira en sincronía casi perfecta con el campo magnético de Saturno.

Visto desde la Tierra, Saturno aparece como un objeto amarillento, uno de los más brillantes en el cielo nocturno. Observado a través de un telescopio, los anillos A y B se ven fácilmente, mientras que los D y E solo se ven en condiciones atmosféricas óptimas. Con telescopios de gran sensibilidad situados en la Tierra se distinguen, en la niebla de la envoltura gaseosa de Saturno, pálidos cinturones y estructuras de bandas paralelas al ecuador.

Tres naves espaciales estadounidenses incrementaron enormemente el conocimiento del sistema de Saturno: la sonda Pioneer 11 y las Voyager 1 y 2, que sobrevolaron el planeta en septiembre de 1979, noviembre de 1980 y agosto de 1981, respectivamente. Estas naves espaciales llevaban cámaras e instrumentos para analizar las intensidades y polarizaciones de la radiación en las regiones visible, ultravioleta, infrarroja y de radio del espectro electromagnético. También estaban equipadas con instrumentos para el estudio de los campos magnéticos y para la detección de partículas cargadas y granos de polvo interplanetario.

En octubre de 1997 fue lanzada la nave Cassini, con destino a Saturno, que incluía también la sonda Huygens para explorar Titán, la mayor y más interesante de las lunas del planeta. Se trata del último proyecto de gran presupuesto de la NASA, en colaboración con la Agencia Espacial Europea y la Agencia Espacial Italiana. Tras un viaje de casi siete años, estaba previsto que la Cassini recogiese datos sobre Saturno y sus satélites durante otros cuatro años. En octubre de 2002 la nave obtuvo su primera fotografía del planeta, tomada a una distancia de 285 millones de kilómetros, y en la que aparece también Titán. En junio de 2004 la Cassini sobrevoló Febe, otro satélite de Saturno (el más alejado), obteniendo imágenes espectaculares de su superficie, llena de cráteres. En julio del mismo año, la nave entró en órbita de Saturno. En enero de 2005 la sonda Huygens atravesó la atmósfera de Titán y alcanzó su superficie, enviando a la Tierra datos e imágenes de gran interés del satélite.


Saturno es un planeta fácil de observar, pues es visible en el cielo la mayor parte del tiempo y sus anillos pueden observarse con cualquier telescopio de aficionado. Se observa mejor cuando el planeta está cerca o en oposición, es decir, la posición de un planeta cuando está a una elongación de 180°, por lo que aparece opuesto al Sol en el cielo. En la oposición del 13 de enero de 2005, Saturno pudo verse con un máximo que no será igualado hasta 2031, debido a una orientación de sus anillos con respecto a la Tierra bastante favorable.

Saturno se observa a simple vista en el cielo nocturno como un punto luminoso (que no parpadea) brillante y amarillento cuyo brillo varía normalmente entre la magnitud +1 y la 0, toma aproximadamente 29 años y medio en realizar una traslación completa en su órbita con respecto a las estrellas de fondo pertenecientes al zodiaco. Con apoyo óptico, como con grandes binoculares o un telescopio, se necesita una magnificación de al menos 20x para que la mayoría de las personas puedan distinguir claramente los anillos de Saturno.

En la astrología hindú, hay nueve planetas, conocidos como Navagrahas. Conocen a Saturno como "Sani" o "Shani", el Juez entre todos los planetas, y determina a cada uno según sus propios hechos realizados malos o buenos.

La Cultura china y japonesa designan a Saturno como la estrella de la tierra dentro del esquema tradicional oriental de utilizar cinco elementos para clasificar los elementos naturales.

En el hebreo, llaman "Shabbathai" a Saturno. Su Ángel es "Cassiel". Su Inteligencia, o el espíritu beneficioso, son "Agiel" (layga), y su espíritu (el aspecto más oscuro) es "Zazel" (lzaz). Ver: Cábala.

En turco y malayo, su nombre es "Zuhal", tomado del árabe زحل.

Saturno fue también conocido como "Φαίνων" por los griegos.





</doc>
<doc id="2570" url="https://es.wikipedia.org/wiki?curid=2570" title="Sol">
Sol

El Sol (del latín "sol", "solis", a su vez de la raíz protoindoeuropea "sauel-" brillar) es una estrella de tipo-G de la secuencia principal y clase de luminosidad V que se encuentra en el centro del sistema solar y constituye la mayor fuente de radiación electromagnética de este sistema planetario. Es una esfera casi perfecta de plasma, con un movimiento convectivo interno que genera un campo magnético a través de un proceso de dinamo. Cerca de tres cuartas partes de la masa del Sol constan de hidrógeno; el resto es principalmente helio, con cantidades mucho más pequeñas de elementos, incluyendo el oxígeno, carbono, neón y hierro. 

Se formó hace aproximadamente 4600 millones de años a partir del colapso gravitacional de la materia dentro de una región de una gran nube molecular. La mayor parte de esta materia se acumuló en el centro, mientras que el resto se aplanó en un disco en órbita que se convirtió en el sistema solar. La masa central se volvió cada vez más densa y caliente, dando lugar con el tiempo al inicio de la fusión nuclear en su núcleo. Se cree que casi todas las estrellas se forman por este proceso. El Sol es más o menos de edad intermedia y no ha cambiado drásticamente desde hace más de cuatro mil millones de años, y seguirá siendo bastante estable durante otros cinco mil millones de años más. Sin embargo, después de que la fusión del hidrógeno en su núcleo se haya detenido, el Sol sufrirá cambios severos y se convertirá en una gigante roja. Se estima que el Sol se volverá lo suficientemente grande como para engullir las órbitas actuales de Mercurio, Venus y posiblemente la Tierra.

La Tierra y otros cuerpos (incluidos otros planetas, asteroides, meteoroides, cometas y polvo) orbitan alrededor del Sol. Por sí solo, representa alrededor del 99,86 % de la masa del sistema solar. La distancia media del Sol a la Tierra fue definida exactamente por la Unión Astronómica Internacional en (aproximadamente 150 millones de kilómetros). Su luz recorre esta distancia en 8 minutos y 19 segundos. 

La energía del Sol, en forma de luz solar, sustenta a casi todas las formas de vida en la Tierra a través de la fotosíntesis, y determina el clima de la Tierra y la meteorología.

Es la estrella del sistema planetario en el que se encuentra la Tierra; por lo tanto, es el astro con mayor brillo aparente. Su visibilidad en el cielo local determina, respectivamente, el día y la noche en diferentes regiones de diferentes planetas. En la Tierra, la energía radiada por el Sol es aprovechada por los seres fotosintéticos que constituyen la base de la cadena trófica, siendo así la principal fuente de energía de la vida. También aporta la energía que mantiene en funcionamiento los procesos climáticos. 

El Sol es una estrella que se encuentra en la fase denominada secuencia principal, con un tipo espectral G2 y clase de luminosidad V, por tanto, también es denominada como enana amarilla, se formó entre 4567,9 y 4570,1 millones de años y permanecerá en la secuencia principal aproximadamente 5000 millones de años más. El Sol, junto con todos los cuerpos celestes que orbitan a su alrededor, incluida la Tierra, forman el sistema solar.

A pesar de ser una estrella mediana, es la única cuya forma se puede apreciar a simple vista, con un diámetro angular de 32′ 35″ de arco en el perihelio y 31′ 31″ en el afelio, lo que da un diámetro medio de 32′ 03″. La combinación de tamaños y distancias del Sol y la Luna son tales que se ven, aproximadamente, con el mismo tamaño aparente en el cielo. Esto permite una amplia gama de eclipses solares distintos (totales, anulares o parciales).

El vasto efecto del Sol sobre la Tierra ha sido reconocido desde tiempos prehistóricos y ha sido considerado por algunas culturas como una deidad. El movimiento de la Tierra alrededor del Sol es la base del calendario solar, el cual es el calendario predominante en uso hoy en día.

La disciplina científica que se encarga del estudio del Sol en su totalidad es la física solar.

El Sol es una estrella de tipo-G de la secuencia principal que abarca aproximadamente el 99,86 % de la masa del sistema solar. El Sol tiene una magnitud absoluta de +4.83, estimada como la más brillante de las 85 % de estrellas de la Vía Láctea, la mayoría de las cuales son enanas rojas. El Sol pertenece a la Población I, o a las estrellas ricas en elementos pesados. La formación del Sol pudo haber sido provocado por ondas de choque de una o más supernovas próximas. Esto fue planteado debido a la gran abundancia de elementos pesados en el sistema solar, como el oro y el uranio, en relación con las abundancias de estos elementos en la llamada Población II de estrellas, siendo éstas pobres en elementos pesados. Estos elementos podrían haberse producido por reacciones nucleares endotérmicas durante una supernova, o por transmutación a través de la absorción neutrónica dentro de una estrella masiva de segunda generación.

El Sol es, con diferencia, el objeto más brillante en el cielo, con magnitud aparente de -26,74. Es unos 13 000 millones de veces más brillante que la segunda estrella más brillante, Sirio, que tiene una magnitud aparente de -1.46. La distancia media del centro del Sol al centro de la Tierra es de aproximadamente 1 unidad astronómica (alrededor de 150 millones de kilómetros), aunque la distancia varía a medida que la Tierra se mueve desde el perihelio en enero hasta el afelio en julio. En esta distancia media, la luz viaja desde el horizonte del Sol hasta el horizonte de la Tierra en unos 8 minutos y 19 segundos, mientras que la luz desde los puntos más cercanos del Sol y de la Tierra tarda aproximadamente dos segundos menos. 

El Sol no tiene un límite definido, y en sus partes externas su densidad disminuye exponencialmente al aumentar la distancia desde su centro. No obstante, a efectos de medición, se considera el radio solar como la distancia que engloba desde su centro hasta el borde de la fotosfera, la superficie visible aparente del Sol. Con base en esta medida, el Sol es una esfera casi perfecta con un achatamiento estimado de 9 millonésimas, lo que significa que su diámetro polar difiere de su diámetro ecuatorial por tan solo 10 kilómetros. El efecto mareal de los planetas es débil y no afecta significativamente a la forma del Sol. El Sol rota más deprisa por su ecuador que por sus polos. Esta rotación diferencial es causada por el movimiento de convección debido al transporte de calor y al efecto coriolis producido por la rotación del Sol. En un marco de referencia definido por las estrellas, el periodo de rotación es de aproximadamente 25,6 días en el ecuador y de 33,5 días en los polos. Visto desde la Tierra en su órbita alrededor del Sol, el período de rotación aparente del Sol en su ecuador es de unos 28 días.

La constante solar es la cantidad de energía que el Sol deposita por unidad de superficie y que es directamente expuesta como luz solar. La constante solar es igual a aproximadamente 1368 W/m² (vatios por metro cuadrado) a una distancia de una unidad astronómica (UA) del Sol (es decir, en o cerca de la Tierra). La luz del Sol en la superficie de la Tierra es atenuada por la atmósfera terrestre, de modo que, llega menos energía a la superficie (cerca de 1000 W/m²) en condiciones claras cuando el Sol está cerca del cenit. La luz del Sol en la parte superior de la atmósfera terrestre está compuesta (por energía total) de aproximadamente un 50 % de luz infrarroja, un 40 % por luz visible y un 10 % de luz ultravioleta. La atmósfera terrestre filtra más del 70 % de la radiación ultravioleta solar, especialmente en las longitudes de onda más cortas. La radiación ultravioleta solar ioniza la parte superior de la atmósfera del lado diurno de la Tierra, haciendo a la ionosfera conductora de electricidad.

El color del Sol es blanco con un índice de color-espacio (CIE) cercano al (0.3, 0.3) cuando se ve desde el espacio o desde lo alto en el cielo; en cambio, cuando se está desde una zona baja del cielo la dispersión atmosférica del Sol tiene un color amarillo, rojo, naranja y magenta. A pesar de su blancura típica, la mayoría de la gente se imagina mentalmente el Sol como amarillo; las razones de ello son objetos de debate. El Sol es una estrella G2V, con "G2" indica que su temperatura superficial es de aproximadamente 5778 K (5505 °C, 9941 °F), y "V" que, como la mayoría de las estrellas, es una estrella enana de la secuencia principal. La luminancia media del Sol es de aproximadamente 1,88 giga candelas por metro cuadrado, pero como se ve a través de la atmósfera de la Tierra, esto se reduce a aproximadamente 1,44 Gcd/m². Sin embargo, la luminancia no es constante a través del disco del Sol (oscurecimiento del limbo).

El Sol está compuesto principalmente por los elementos químicos hidrógeno y helio; que representan el 74,9 % y el 23,8 % de la masa del Sol en la fotosfera, respectivamente. Todos los elementos más pesados, llamados "metales" en astronomía, representan menos del 2% de la masa, con el oxígeno (más o menos el 1 % de la masa del Sol), carbono (0,3 %), neón (0,2 %), y el hierro (0,2 %) siendo el más abundante.

El Sol heredó su composición química del medio interestelar a través del cual se formó. El hidrógeno y el helio en el Sol fueron producidos por nucleosíntesis del Big Bang, y los elementos más pesados se crearon por nucleosíntesis estelar en generaciones de estrellas que completaron su evolución estelar y devolvieron su material al medio interestelar antes de la formación del Sol. La composición química de la fotosfera se considera normalmente como representativa de la composición del sistema solar primordial. Sin embargo, desde que se formó el Sol, parte del helio y de elementos pesados se han asentado gravitacionalmente desde la fotosfera. Por lo tanto, en la fotosfera de hoy en día, la fracción de helio es reducida, y la metalicidad es solo el 84 % de lo que era en la fase protoestelar (antes de que la fusión nuclear comenzara en el núcleo). Se cree que la composición protoestelar del Sol ha sido de un 71,1 % de hidrógeno, 27,4 % de helio, y de un 1,5 % de elementos más pesados.

Hoy en día, la fusión nuclear en el núcleo del Sol ha modificado la composición mediante la conversión del hidrógeno en helio, por lo que ahora la parte más interna del Sol es más o menos un 60 % de helio, junto con la abundancia de elementos más pesados sin ser alterados. Debido a que el calor se transfiere desde el centro del Sol por radiación en vez de por convección, ninguno de los productos de fusión del núcleo han llegado a la fotosfera.

La zona reactiva del núcleo de "combustión del hidrógeno", donde el hidrógeno se convierte en helio, está empezando a ser circundado por un núcleo interno de "cenizas de helio". Este desarrollo continuará y posteriormente tendrá lugar la salida del Sol de la secuencia principal para llegar a convertirse así en una gigante roja.

La abundancia de elementos pesados solares descritos anteriormente son medidos usando tanto espectroscopia de la fotosfera del Sol como midiendo las abundancias en los meteoritos que nunca han sido calentados a temperaturas de fusión. Se cree que estos meteoritos retienen la composición del Sol protoestelar y, por lo tanto, no se ve afectado por la sedimentación de elementos pesados. Por lo general los dos métodos concuerdan bien.

El Sol se formó hace 4650 millones de años y tiene combustible para 7500 millones de años más. Después, comenzará a hacerse más y más grande, hasta convertirse en una gigante roja. Finalmente, se hundirá por su propio peso y se convertirá en una enana blanca, que puede tardar unos mil millones de años en enfriarse. Se formó a partir de nubes de gas y polvo que contenían residuos de generaciones anteriores de estrellas. Gracias a la metalicidad de dicho gas, de su disco circunestelar surgieron, más tarde, los planetas, asteroides y cometas del sistema solar. En el interior del Sol se producen reacciones de fusión en las que los átomos de hidrógeno se transforman en helio, produciéndose la energía que irradia. Actualmente, el Sol se encuentra en plena secuencia principal, fase en la que seguirá unos 5000 millones de años más fusionando hidrógeno de manera estable. 

Cada segundo se transforman 700 millones de toneladas de hidrógeno en cenizas de helio, este proceso transforma cinco millones de toneladas de materia en energía, lo que da como resultado que el Sol cada vez se vuelve más liviano.

Llegará un día en que el Sol agote todo el hidrógeno en la región central al haberlo transformado en helio. La presión será incapaz de sostener las capas superiores y la región central tenderá a contraerse gravitacionalmente, calentando progresivamente las capas adyacentes. El exceso de energía producida hará que las capas exteriores del Sol tiendan a expandirse y enfriarse y el Sol se convertirá en una estrella gigante roja. El diámetro puede llegar a alcanzar y sobrepasar al de la órbita de la Tierra, con lo cual, cualquier forma de vida se habrá extinguido. Cuando la temperatura de la región central alcance aproximadamente 100 millones de kelvins, comenzará a producirse la fusión del helio en carbono mientras alrededor del núcleo se sigue fusionando hidrógeno en helio. Ello producirá que la estrella se contraiga y disminuya su brillo a la vez que aumenta su temperatura, convirtiéndose el Sol en una estrella de la rama horizontal. Al agotarse el helio del núcleo, se iniciará una nueva expansión del Sol y el helio empezará también a fusionarse en una nueva capa alrededor del núcleo inerte -compuesto de carbono y oxígeno y que por no tener masa suficiente el Sol no alcanzará las presiones y temperaturas suficientes para fusionar dichos elementos en elementos más pesados- que lo convertirá de nuevo en una gigante roja, pero esta vez de la rama asintótica gigante y provocará que el astro expulse gran parte de su masa en la forma de una nebulosa planetaria, quedando únicamente el núcleo solar que se transformará en una enana blanca y, mucho más tarde, al enfriarse totalmente, en una enana negra. El Sol no llegará a estallar como una supernova al no tener la masa suficiente para ello.

Si bien se creía en un principio que el Sol acabaría por absorber a Mercurio, a Venus y a la Tierra al convertirse en gigante roja, la gran pérdida de masa que sufrirá en el proceso hizo pensar por un tiempo que la órbita terrestre –al igual que la de los demás planetas del sistema solar– se expandiría posiblemente y salvaría a nuestro planeta de ese destino. Sin embargo, un artículo reciente postula que ello no ocurrirá y que las interacciones mareales, así como el roce con la materia de la cromosfera solar, harán que nuestro planeta sea absorbido. Otro artículo posterior apunta en la misma dirección.

Como toda estrella, el Sol posee una forma esférica, y a causa de su lento movimiento de rotación, tiene también un leve achatamiento polar. Como en cualquier cuerpo masivo, toda la materia que lo constituye es atraída hacia el centro del objeto por su propia fuerza gravitatoria. Sin embargo, el plasma que forma el Sol se encuentra en equilibrio, ya que la creciente presión en el interior solar compensa la atracción gravitatoria, lo que genera un equilibrio hidrostático. Estas enormes presiones se producen debido a la densidad del material en su núcleo y a las enormes temperaturas que se dan en él gracias a las reacciones termonucleares que allí acontecen. Existe, además de la contribución puramente térmica, una de origen fotónico. Se trata de la presión de radiación, nada despreciable, que es causada por el ingente flujo de fotones emitidos en el centro del Sol.

Casi todos los elementos químicos terrestres (aluminio, azufre, bario, cadmio, calcio, carbono, cerio, cobalto, cobre, cromo, estaño, estroncio, galio, germanio, helio, hidrógeno, hierro, indio, magnesio, manganeso, níquel, nitrógeno, oro, oxígeno, paladio, plata, platino, plomo, potasio, rodio, silicio, sodio, talio, titanio, tungsteno, vanadio, circonio y zinc) y diversos compuestos (como el cianógeno, el óxido de carbono y el amoniaco) han sido identificados en la constitución del astro rey, por lo que se ha concluido que, si nuestro planeta se calentara hasta la temperatura solar, tendría un espectro luminoso casi idéntico al Sol. Incluso el helio fue descubierto primero en el Sol y luego se constató su presencia en nuestro planeta.

El Sol presenta una estructura en capas esféricas o en "capas de cebolla". La frontera física y las diferencias químicas entre las distintas capas son difíciles de establecer. Sin embargo, se puede determinar una función física que es diferente para cada una de las capas. En la actualidad, la astrofísica dispone de un modelo de estructura solar que explica satisfactoriamente la mayor parte de los fenómenos observados. Según este modelo, el Sol está formado por: 1) núcleo solar, 2) zona radiante, 3) zona convectiva, 4) fotosfera, 5) cromosfera, 6) corona, 7) manchas solares, 8) granulación y 9) viento solar.

Ocupa unos del radio solar, 1/5 del mismo, y es en esta zona donde se verifican las reacciones termonucleares que proporcionan toda la energía que el Sol produce. Esta energía generada en el núcleo del Sol tarda un millón de años para alcanzar la superficie solar. En su centro se calcula que existe un 49 por ciento de hidrógeno, 49 por ciento de helio y un 2 por ciento que se distribuye en otros elementos que sirven como catalizadores en las reacciones termonucleares. A comienzos de la década de los años 30 del siglo XX, el físico austriaco Fritz Houtermans (1903-1966) y el astrónomo inglés Robert d'Escourt Atkinson (1898-1982) unieron sus esfuerzos para averiguar si la producción de energía en el interior del Sol y en las estrellas se podía explicar por las transformaciones nucleares. En 1938 Hans Albrecht Bethe (1906-2005), en los Estados Unidos, y Carl Friedrich von Weizsäcker (1912-2007), en Alemania, simultánea e independientemente, encontraron el hecho notable de que un grupo de reacciones en las que intervienen el carbono y el nitrógeno como catalizadores constituyen un ciclo, que se repite una y otra vez, mientras dura el hidrógeno. A este grupo de reacciones se les conoce como ciclo de Bethe o del carbono, y es equivalente a la fusión de cuatro protones en un núcleo de helio. En estas reacciones de fusión hay una pérdida de masa, esto es, el hidrógeno consumido pesa más que el helio producido. Esa diferencia de masa se transforma en energía, según la ecuación de Einstein (E = mc²), donde E es la energía, m la masa y c la velocidad de la luz. Estas reacciones nucleares transforman el 0,7 por ciento de la masa afectada en fotones, con una longitud de onda cortísima y, por lo tanto, muy energéticos y penetrantes. La energía producida mantiene el equilibrio térmico del núcleo solar a temperaturas aproximadamente de 15 millones de kelvins.

El ciclo ocurre en las siguientes etapas:


La energía neta liberada en el proceso es 26,7 MeV, o sea cerca de 6,7·10 J por kg de protones consumidos. El carbono actúa como catalizador, pues al final del ciclo se regenera.

Otra reacción de fusión que ocurre en el Sol y en las estrellas es el ciclo de Critchfiel o, más comúnmente conocido como cadena protón-protón. Charles Critchfield (1910-1994) era en 1938 un joven físico, alumno de George Gamow, (1904-1968) en la Universidad George Washington, y tuvo una idea completamente diferente, al darse cuenta que en el choque entre dos protones a velocidades próximas a la de luz, puede ocurrir que uno de ellos pierda su carga positiva (e+), se fusionen y se convierta en un neutrón, que permanece unido al otro protón y forma un núcleo de deuterio, es decir, un núcleo pesado formado por un isótopo estable del hidrógeno. El positrón (e+) al ser liberado tiende a aniquilarse con bastante rapidez, fusionándose con un electrón (e-), produciendo en el proceso radiación fotónica. Al mismo tiempo, en esta segunda fase, se libera un neutrino electrónico de baja energía, que no interactúa con ningún átomo y se libera al espacio a velocidades próximas a la de luz sin colisionar con la materia.

Más tarde, la fusión de un protón (p+), o lo que es lo mismo, un núcleo H, con un núcleo de deuterio da lugar a un isótopo del helio He³ y a la emisión de fotones gamma (γ). Finalmente, con un 97% de probabilidad aproximadamente, dos núcleos del isótopo He³ dan lugar, al ser fusionados, en un núcleo estable de He más dos nuevos protones (p+), con lo que el ciclo se retroalimenta hasta la primera fase inicial, al tiempo que pierde energía a razón de 26,7 MeV netos.

La reacción puede producirse de dos maneras algo distintas:

El primer ciclo se da en estrellas más calientes y con mayor masa que el Sol, y la cadena protón-protón en las estrellas similares al Sol. En cuanto al Sol, hasta el año 1953 se creyó que su energía era producida casi exclusivamente por el ciclo de Bethe, pero se demostró durante estos últimos años que el calor solar proviene en su mayor parte (~75 %) del ciclo protón-protón.

En los últimos estadios de su evolución, el Sol fusionará también el helio producto de estos procesos para dar carbono y oxígeno (véase proceso triple-alfa).

En la zona exterior al núcleo el transporte de la energía generada en el interior se produce por radiación hasta el límite exterior de la zona radiactiva. Esta zona está compuesta de plasma, es decir, grandes cantidades de hidrógeno y helio ionizado. Como la temperatura del Sol decrece del centro (15 MK) a la periferia (6 kK en la fotosfera), es más fácil que un fotón cualquiera se mueva del centro a la periferia que al revés. Sin embargo, los fotones deben avanzar por un medio ionizado tremendamente denso siendo absorbidos y reemitidos infinidad de veces en su camino. Se calcula que un fotón cualquiera puede tardar un millón de años en alcanzar la superficie y manifestarse como luz visible.

Esta región se extiende por encima de la zona radiante, y en ella los gases solares dejan de estar ionizados y los fotones son absorbidos con facilidad y se convierten en un material opaco al transporte de radiación. Por lo tanto, el transporte de energía se realiza por convección, de modo que el calor se transporta de manera no homogénea y turbulenta por el propio fluido. Los fluidos se dilatan al ser calentados y disminuyen su densidad. Por lo tanto, se forman corrientes ascendentes de material desde la zona caliente hasta la zona superior, y simultáneamente se producen movimientos descendentes de material desde las zonas exteriores menos calientes. Así, a unos bajo la fotosfera del Sol, el gas se vuelve opaco por efecto de la disminución de la temperatura; en consecuencia, absorbe los fotones procedentes de las zonas inferiores y se calienta a expensas de su energía. Se forman así secciones convectivas turbulentas, en las que las "parcelas" de gas caliente y ligero suben hasta la fotosfera, donde nuevamente la atmósfera solar se vuelve transparente a la radiación y el gas caliente cede su energía en forma de luz visible, y se enfría antes de volver a descender a las profundidades. El análisis de las oscilaciones solares ha permitido establecer que esta zona se extiende hasta estratos de gas situados a la profundidad indicada anteriormente. La observación y el estudio de estas oscilaciones solares constituyen el campo de trabajo de la heliosismología.

La fotosfera es la zona visible donde se emite luz visible del Sol. La fotosfera se considera como la «superficie» solar y, vista a través de un telescopio, se presenta formada por gránulos brillantes que se proyectan sobre un fondo más oscuro. A causa de la agitación de nuestra atmósfera, estos gránulos parecen estar siempre en agitación. Puesto que el Sol es gaseoso, su fotosfera es algo transparente: puede ser observada hasta una profundidad de unos cientos de kilómetros antes de volverse completamente opaca. Normalmente se considera que la fotosfera solar tiene unos 100 o 200 km de profundidad.

Aunque el borde o limbo del Sol aparece bastante nítido en una fotografía o en la imagen solar proyectada con un telescopio, se aprecia fácilmente que el brillo del disco solar disminuye hacia el borde. Este fenómeno de oscurecimiento del centro al limbo es consecuencia de que el Sol es un cuerpo gaseoso con una temperatura que disminuye con la distancia al centro. La luz que se ve en el centro procede en la mayor parte de las capas inferiores de la fotosfera, más caliente y por tanto más luminosa. Al mirar hacia el limbo, la dirección visual del observador es casi tangente al borde del disco solar por lo que llega radiación procedente sobre todo de las capas superiores de la fotosfera, menos calientes y emitiendo con menor intensidad que las capas profundas en la base de la fotosfera.

Un fotón tarda un promedio de 10 días desde que surge de la fusión de dos átomos de hidrógeno, en atravesar la zona radiante y un mes en recorrer los 200 000 km de la zona convectiva, empleando tan solo unos 8 minutos y medio en cruzar la distancia que separa la Tierra del Sol. No se trata de que los fotones viajen más rápidamente ahora, sino que en el exterior del Sol el camino de los fotones no se ve obstaculizado por los continuos cambios, choques, quiebros y turbulencias que experimentaban en el interior del Sol.

Los gránulos brillantes de la fotosfera tienen muchas veces forma hexagonal y están separados por finas líneas oscuras. Los gránulos son la evidencia del movimiento convectivo y burbujeante de los gases calientes en la parte exterior del Sol. En efecto, la fotosfera es una masa en continua ebullición en el que las células convectivas se aprecian como gránulos en movimiento cuya vida media es tan solo de unos nueve minutos. El diámetro medio de los gránulos individuales es de unos 700 a 1000 km y resultan particularmente notorios en los períodos de mínima actividad solar. Hay también movimientos turbulentos a una escala mayor, la llamada ""supergranulación"", con diámetros típicos de unos . Cada supergranulación contiene cientos de gránulos individuales y sobrevive entre 12 a 20 horas. Fue Richard Christopher Carrington (1826-1875), cervecero y astrónomo aficionado, el primero en observar la granulación fotosférica en el siglo XIX. En 1896 el francés Pierre Jules César Janssen (1824-1907) consiguió fotografiar por primera vez la granulación fotosférica.
El signo más evidente de actividad en la fotosfera son las manchas solares. En los tiempos antiguos se consideraba al Sol como un fuego divino y, por consiguiente, perfecto e infalible. Del mismo modo se sabía que la brillante cara del Sol estaba a veces nublada con unas manchas oscuras, pero se imaginaba que era debido a objetos que pasaban en el espacio entre el Sol y la Tierra. Cuando Galileo (1564-1642) construyó el primer telescopio astronómico, dando origen a una nueva etapa en el estudio del Universo, hizo la siguiente afirmación ""Repetidas observaciones me han convencido, de que estas manchas son sustancias en la superficie del Sol, en la que se producen continuamente y en la que también se disuelven, unas más pronto y otras más tarde"". Una mancha solar típica consiste en una región central oscura, llamada "umbra", rodeada por una "penumbra" más clara. Una sola mancha puede llegar a medir hasta (casi tan grande como el diámetro de la Tierra), pero un grupo de manchas puede alcanzar de extensión e incluso algunas veces más. La penumbra está constituida por una estructura de filamentos claros y oscuros que se extienden más o menos radialmente desde la umbra. Ambas (umbra y penumbra) parecen oscuras por contraste con la fotosfera, simplemente porque están menos calientes que la temperatura media de la fotosfera. Así, la umbra tiene una temperatura de 4000K, mientras que la penumbra alcanza los 5600K, inferiores en ambos casos a los 6000 K que tienen los gránulos de la fotosfera. Por la ley de Stefan-Boltzmann, en que la energía total radiada por un cuerpo negro (como una estrella) es proporcional a la cuarta potencia de su temperatura efectiva (E = σT, donde σ = 5,67051·10 W/m²·K), la umbra emite aproximadamente un 32 % de la luz emitida por un área igual de la fotosfera y análogamente la penumbra tiene un brillo de un 71 % de la fotosfera. La oscuridad de una mancha solar está causada únicamente por un efecto de contraste; si pudiéramos ver a una mancha tipo, con una umbra del tamaño de la Tierra, aislada y a la misma distancia que el Sol, brillaría una 50 veces más que la Luna llena. Las manchas están relativamente inmóviles con respecto a la fotosfera y participan de la rotación solar. El área de la superficie solar cubierta por las manchas se mide en términos de millonésima del disco visible.

La cromosfera es una capa exterior a la fotosfera visualmente mucho más transparente. Su tamaño es de aproximadamente , y es imposible observarla sin filtros especiales, pues es eclipsada por el mayor brillo de la fotosfera. La cromosfera puede observarse durante un eclipse solar en un tono rojizo característico y en longitudes de onda específicas, notablemente en Hα, una longitud de onda característica de la emisión por hidrógeno a muy alta temperatura.

Las prominencias solares ascienden ocasionalmente desde la fotosfera, alcanzan alturas de hasta y producen erupciones solares espectaculares.

La corona solar está formada por las capas más tenues de la atmósfera superior solar. Su temperatura alcanza los millones de kelvin, una cifra muy superior a la de la capa que le sigue, la fotosfera, siendo esta inversión térmica uno de los principales enigmas de la ciencia solar reciente. Estas elevadísimas temperaturas son un dato engañoso y consecuencia de la alta velocidad de las pocas partículas que componen la atmósfera solar. Sus grandes velocidades son debidas a la baja densidad del material coronal, a los intensos campos magnéticos emitidos por el Sol y a las ondas de choque que rompen en la superficie solar estimuladas por las células convectivas. Como resultado de su elevada temperatura, desde la corona se emite gran cantidad de energía en rayos X. En realidad, estas temperaturas no son más que un indicador de las altas velocidades que alcanza el material coronal que se acelera en las líneas de campo magnético y en dramáticas eyecciones de material coronal (EMCs). Lo cierto es que esa capa es demasiado poco densa como para poder hablar de temperatura en el sentido usual de agitación térmica.

Todos estos fenómenos combinados ocasionan extrañas rayas en el espectro luminoso que hicieron pensar en la existencia de un elemento desconocido en la tierra al que incluso denominaron "coronium" hasta que investigaciones posteriores en 1942 concluyeron que se trataban de radiaciones producidas por átomos neutros de oxígeno de la parte externa de la misma corona, así como de hierro, níquel, calcio y argón altamente ionizados (fenómenos imposibles de obtener en laboratorios).

La corona solar solamente es observable desde el espacio con instrumentos adecuados que anteponen un disco opaco para eclipsar artificialmente al Sol o durante un eclipse solar natural desde la Tierra. El material tenue de la corona es continuamente expulsado por la fuerte radiación solar dando lugar a un viento solar. Así pues, se cree que las estructuras observadas en la corona están modeladas en gran medida por el campo magnético solar y las células de transporte convectivo.

En 1970 el físico sueco Hannes Alfvén obtuvo el premio Nobel. Él estimó que había ondas que transportaban energía por líneas del campo magnético que recorre el plasma de la corona solar. Pero hasta hoy no se había podido detectar la cantidad de ondas que eran necesarias para producir dicha energía.

Pero imágenes de alta definición ultravioleta, tomadas cada ocho segundos por el satélite de la NASA Solar Dymanics Observatory (SDO), han permitido a científicos como Scott McIntosh y a sus colegas del Centro Nacional Estadounidense de Investigación Atmosférica, detectar gran cantidad de estas ondas. Las mismas se propagan a gran velocidad (entre 200 y 250 kilómetros por segundo) en el plasma en movimiento. Ondas cuyo flujo energético se sitúa entre 100 y 200 vatios por kilómetro cuadrado "son capaces de proveer la energía necesaria para propulsar a los rápidos vientos solares y así compensar las pérdidas de calor de las regiones menos agitadas de la corona solar", estiman los investigadores.

Sin embargo, para McIntosh esto no es suficiente para generar los 2000 vatios por metro cuadrado que se necesitan para abastecer a las zonas activas de la corona. Es por esto que se requiere de instrumentos con mayor capacidad temporal y espacial para estudiar todo el espectro de energía irradiada en las regiones activas de nuestra estrella.

La heliosfera sería la región que se extiende desde el Sol hasta más allá de Plutón y que se encuentra bajo la influencia del viento solar. Es en esta región donde se extienden los efectos de las tormentas geomagnéticas y también donde se extiende el influyo del campo magnético solar. La heliosfera protege al sistema solar de las radiaciones provenientes del medio interestelar y su límite se extiende a más de 100 UA del Sol, límite solo superado por los cometas.

La eyección de masa coronal (CME) es una onda hecha de radiación y viento solar que se desprende del Sol en el periodo llamado Actividad Máxima Solar. Esta onda es muy peligrosa ya que daña los circuitos eléctricos, los transformadores y los sistemas de comunicación. Cuando esto ocurre, se dice que hay una tormenta solar.

El campo magnético del sol se forma como sigue: En el núcleo, las presiones del hidrógeno provocan que sus átomos únicamente queden excluidos por las fuerzas de polaridad de los protones, dejando una nube de electrones en torno a dicho núcleo (los electrones se han desprendido de las órbitas tradicionales, formando una capa de radiación electrónica común). La fusión de los átomos de hidrógeno en helio se produce en la parte más interna del núcleo, en donde el helio queda restringido por ser un material más pesado. Dicho 'ordenamiento' induce que los propios electrones compartan estados de energía y en consecuencia sus campos magnéticos adquieran aún más densidad y potencia. Las enormes fuerzas de gravedad, impiden que los fotones (portadores de esas fuerzas) escapen de forma libre. De esta forma se genera en su interior un potente campo magnético que influye en la dinámica del plasma en las capas siguientes.

Los campos magnéticos, tal como si se tratase de un material fluido, encuentran su dinámica por las fuerzas magnetohidrodinámicas en constante interacción con las gravitatorias y rotacionales de la estrella, llegando a la superficie de manera que, los materiales más externos quedan ordenados conforme a las líneas de fuerza gauss. La rotación solar produce que las capas más externas no giren todas a la misma velocidad, por lo que el ordenamiento de estas líneas de fuerza se va descompensando a medida que los materiales distribuidos entre los polos y el ecuador van perdiendo sincronismo en el giro rotacional de la estrella. Por cada ruptura en la integridad del campo magnético, se produce un escape de líneas de fuerza gauss (produciendo las típicas manchas negras), en las que un aumento de estas, puede tener como consecuencia una erupción solar consecuente por la desintegración local del campo gauss. Cuando el Sol se acerca a su máximo desorden, las tormentas solares son máximas. Estos periodos se dan cada 11 años. El sol no posee un campo electromagnético como el de la Tierra, sino que posee lo que se denomina viento solar, producido por esas inestabilidades rotacionales del Sol. Si no fuera por eso, los campos magnéticos del Sol quedarían restringidos a la dinámica del plasma.

Por esa misma razón, una reacción de fusión entre dos átomos de hidrógeno en el interior del Sol, tarda 11 años en llegar a escapar de las enormes fuerzas gravitatorias y magnéticas.

La mayor parte de la energía utilizada por los seres vivos procede del Sol, las plantas la absorben directamente y realizan la fotosíntesis, los herbívoros absorben indirectamente una pequeña cantidad de esta energía comiendo las plantas, y los carnívoros absorben indirectamente una cantidad más pequeña comiendo a los herbívoros.

La mayoría de las fuentes de energía usadas por el hombre derivan indirectamente del Sol. Los combustibles fósiles preservan energía solar capturada hace millones de años mediante fotosíntesis, la energía hidroeléctrica usa la energía potencial de agua que se condensó en altura después de haberse evaporado por el calor del Sol, etc.

Sin embargo, el uso directo de energía solar para la obtención de energía no está aún muy extendido debido a que los mecanismos actuales no son suficientemente eficaces.

Una mínima cantidad de materia puede convertirse en una enorme manifestación de energía. Esta relación entre la materia y la energía explica la potencia del Sol, que hace posible la vida. ¿Cuál es la equivalencia? En 1905, Einstein había predicho una equivalencia entre la materia y la energía mediante su ecuación E=mc². Una vez que Einstein formuló la relación, los científicos pudieron explicar por qué ha brillado el Sol por miles de millones de años. En el interior del Sol se producen continuas reacciones termonucleares. De este modo, el Sol convierte cada segundo unos 564 millones de toneladas de hidrógeno en 560 millones de toneladas de helio, lo que significa que unos cuatro millones de toneladas de materia se transforman en energía solar, una pequeña parte de la cual llega a la Tierra y sostiene la vida.

Con la fórmula y los datos anteriores se puede calcular la producción de energía del Sol, obteniéndose que la potencia de nuestra estrella es aproximadamente 3'8x10 vatios, o 3'8x10 kilovatios —o, dicho de otra manera, el Sol produce en un segundo 760 000 veces la producción energética anual a nivel mundial—.

Unas de las primeras observaciones astronómicas de la actividad solar fueron las realizadas por Galileo Galilei en el siglo XVII, utilizando vidrios ahumados al principio, y usando el método de proyección después. Galileo observó así las manchas solares y pudo medir la rotación solar así como percibir la variabilidad de estas. En la actualidad la actividad solar es monitoreada constantemente por observatorios astronómicos terrestres y observatorios espaciales. Entre los objetivos de estas observaciones se encuentra, no solo alcanzar una mayor comprensión de la actividad solar, sino también la predicción de sucesos de elevada emisión de partículas potencialmente peligrosas para las actividades en el espacio y las telecomunicaciones terrestres.

La luz solar que apreciamos de a simple vista es de color amarillo, pero en realidad el sol la emite en todas las longitudes de onda.

Para obtener una visión ininterrumpida del Sol en longitudes de onda inaccesibles desde la superficie terrestre, la Agencia Espacial Europea y la NASA lanzaron cooperativamente el satélite SOHO ("Solar and Heliospheric Observatory") el 2 de diciembre de 1995. La sonda europea Ulysses realizó estudios de la actividad solar, y la sonda norteamericana Génesis se lanzó en un vuelo cercano a la heliósfera para regresar a la Tierra con una muestra directa del material solar. Génesis regresó a la Tierra en el 2004, pero su reentrada en la atmósfera fue acompañada de un fallo en su paracaídas principal que hizo que se estrellara sobre la superficie. El análisis de las muestras obtenidas prosigue en la actualidad.

Aristarco de Samos fue el primero en hacer estimaciones sobre la distancia al Sol. No llegó a distancias concretas, sino que estableció distancias relativas a la distancia entre la Tierra y la Luna. Esperó a que la fase de la Luna sea de un cuarto exactamente, momento en que el ángulo Tierra-Luna-Sol debería ser un ángulo recto. Entonces la hipotenusa del rectángulo sería la distancia de la Tierra al Sol. Para esto era necesario medir con exactitud el ángulo del Sol respecto a la Luna, cosa que no es nada fácil.

Entonces determinó la distancia y el tamaño del Sol (relativos). Sin embargo, siendo necesario medir unos ángulos demasiado pequeños, y sin los instrumentos para ello, no logró la suficiente exactitud. Determinó que el Sol se encuentra 20 veces más lejos de lo que está la Luna, y determinó que su diámetro era al menos 7 veces el diámetro de la Tierra. Según los cálculos actuales el Sol se encuentra 400 veces más alejado que la Luna, y su diámetro es 109 veces más grande que el de la Tierra, por lo que fue muy grande el error de medición.

Para establecer la distancia real de la Tierra a la Luna sugirió un método utilizando curvatura de la sombra de la Tierra proyectada en la Luna, durante los eclipses lunares. (Este método fue utilizado por Hiparco de Nicea posteriormente para calcular esa distancia).

Aristarco, pensando que el Sol era al menos 7 veces más grande que la Tierra, sugirió que no es el Sol el que gira alrededor de la Tierra, sino al contrario, siendo el primero en sugerir un modelo heliocéntrico. Sin embargo, sus ideas no fueron aceptadas por sus contemporáneos y la teoría heliocéntrica no se retomó hasta 1543, 17 siglos después, cuando Copérnico publicó su libro ≪Sobre las revoluciones de los orbes celestes≫.

En 1650 Godefroy Wendelin repitió las mediciones de Aristarco midiendo directamente la distancia al Sol, esta vez con mayores recursos técnicos que 18 siglos atrás. Llegó a la conclusión de que el Sol estaba unas 240 veces más alejado que la Luna. Esta vez el error fue menor, pero el valor todavía menor al que se mide actualmente.

En 1609, Kepler abrió el camino para determinar las distancias relativas de todos los cuerpos del sistema solar, no solo de la Luna y el Sol, por lo que sabiendo la distancia a cualquiera de los planetas se podría saber la distancia al Sol. Posteriormente Cassini, en 1673 obtuvo el paralaje de Marte, por lo que logró determinar su distancia. Entonces, sobre la base de los cálculos de Kepler, determinó la distancia al Sol en 136 millones de kilómetros (esta vez la distancia se acercó bastante a los datos actuales, y el error fue solo de 7%).





</doc>
<doc id="2571" url="https://es.wikipedia.org/wiki?curid=2571" title="Stephen King">
Stephen King

Stephen Edwin King (Portland, Maine, 21 de septiembre de 1947) es un escritor estadounidense de novelas de terror, ficción sobrenatural, misterio, ciencia ficción y fantasía. Sus libros han vendido más de 350 millones de copias, muchos de los cuales han sido adaptados al cine y la televisión. King ha publicado 54 novelas, siete de ellas bajo el seudónimo Richard Bachman, y siete libros de no-ficción. Ha escrito además alrededor de 200 relatos, de los cuales la mayoría han sido recogidos en colecciones de relatos.

Su novela corta "Rita Hayworth y la redención de Shawshank" fue la base para la película "The Shawshank Redemption", la cual es considerada una de las mejores películas de la historia, y fue de hecho votada en la revista Empire como la mejor película de la historia, en su encuesta "The 201 Greatest Movies of All Time" en marzo de 2006. 

Muchas de sus historias tienen lugar en su estado natal de Maine.

Stephen King nació en Portland (Maine), fue el segundo hijo de Donald King y Nellie Ruth Pillsbury. Cuando King tenía dos años de edad, su padre abandonó a la familia. Su madre lo crio junto a su hermano mayor David por su cuenta, algunas veces bajo grandes problemas financieros. Tras vivir en Fort Wayne (Indiana) y Stratford (Connecticut), la familia se mudó a Durham, pueblo natal de Ruth. King estudió en Durham Elementary School y luego en Lisbon Falls High School.

Cuando era niño, King presenció un espantoso accidente, uno de sus amigos quedó atrapado en unos rieles y fue arrollado por un tren. Aunque este hecho podría haber inspirado las oscuras creaciones de King, el mismo escritor desecha la idea.

Empezó a escribir desde una temprana edad, mientras se basaba en películas vistas recientemente e historietas. Mientras estaba en el colegio, comenzó a vender cuentos a sus compañeros, los cuales eran copiados con la misma máquina que su hermano utilizaba para publicar su periódico llamado "Dave's Rag". Sin embargo, la actividad no fue bien vista por sus profesores, quienes lo obligaron a devolver el dinero ganado.Aproximadamente a los trece años de edad, descubrió en la casa de su tía una vieja caja con libros de su padre, la mayoría de terror y ciencia ficción. Desde entonces comenzó a enviar sus trabajos a diferentes revistas, sin recibir una respuesta positiva. Su primer relato publicado fue incluido en la revista "Comics Review" de Mike Garrett en 1965. El título original era "I Was a Teenage Grave Robber", pero fue cambiado por el editor a "In a Half-World of Terror".

Entre 1966 y 1971, estudió inglés en la Universidad de Maine, en Orono, con grandes penurias económicas a causa de la pobreza de su madre, y escribió una columna titulada "King's garbage truck" en la revista de la ella.

King conoció a su futura esposa, la escritora Tabitha King, en la biblioteca de la universidad y se casaron en 1971. El escritor realizó trabajos de media jornada para poder pagar sus estudios, incluso en una lavandería. Utilizó la experiencia vivida para escribir las historias de "La trituradora" (The Mangler) y "Carretera maldita" (Roadwork).

Después de terminar sus estudios universitarios con una licenciatura en arte en inglés y obtener un certificado para poder enseñar en secundaria, King enseñó inglés en Hampden Academy (Hampden). Durante este periodo, él y su familia vivieron en un remolque. Escribió historias cortas (la mayoría publicadas en revistas para hombres) para poder satisfacer las necesidades de su familia. Durante este periodo King comenzó a tener problemas de alcoholismo que mantuvo durante una década. Estos problemas se verán reflejados en su segunda novela, "El resplandor", en la persona de su personaje principal, el escritor alcohólico Jack Torrance.

Durante este periodo, comenzó a escribir un gran número de novelas. Una de sus primeras ideas fue la de una joven con poderes psíquicos. Sin embargo, se sintió desalentado y la tiró a la basura. Tabitha rescató el trabajo y lo animó a terminarlo. Después de finalizada la novela, la tituló "Carrie" y la mandó a la compañía editora Doubleday y al pasar el tiempo se olvidó de ella. Más tarde, recibió una oferta de compra por 2.500 dólares de adelanto (no un gran adelanto para una novela, incluso en esa época). Poco tiempo después, el valor de "Carrie" con los derechos del manuscrito fueron vendidos por 400.000 dólares (200.000 de los cuales recibió el editor). Después del lanzamiento, su madre murió de cáncer uterino, pero llegó a leer la novela. Posteriormente, aparecieron varias de sus obras más reconocidas: "El misterio de Salem's Lot" (1975), "El resplandor" (1977), "La danza de la muerte" (1978), "La zona muerta" (1979), "Cujo" (1981), "Cementerio de animales" (1983), "It" (1986) o "Misery" (1987) entre otras. La mayoría de ellas fueron adaptadas al cine donde destacó la versión fílmica de "El resplandor" que realizó Stanley Kubrick en 1980, con la que King declaró no sentirse del todo satisfecho; o la adaptación de "La zona muerta" dirigida por David Cronenberg en 1983. 

Desde fines de los años 1970 y hasta fines de los años 1980, King comenzó a desarrollar problemas de adicción, primero al alcohol y posteriormente a otros tipos de drogas. Después de la publicación de "Los Tommyknockers" (1987), su familia y amigos decidieron hacer una intervención, en la que le mostraron los residuos de su estudio para que se diera cuenta del grado de adicción alcanzado: latas de cerveza, cigarrillos, bolsas de cocaína, botellas de xanax, valium y nyquil eran algunas de las cosas que había. Solicitó ayuda y abandonó toda forma de alcohol y drogas hacia finales de esa década.

Los personajes de sus libros han ido evolucionando al pasar de los años: 





En el verano de 1999, King se encontraba trabajando en el ensayo titulado "On Writing: A Memoir of the Craft" ("Mientras escribo"). En ese período había terminado la sección de memorias y había abandonado el libro durante dieciocho meses debido a la inseguridad acerca de cómo proceder o de si iba a molestar a terceros. Relata que fue el primer libro que tuvo que abandonar desde que escribiese "The Stand" décadas atrás. Una vez hubo tomado la decisión de continuar con el libro, el 17 de junio, escribió una lista de preguntas que le habían hecho con frecuencia sobre su forma de escribir, al mismo tiempo que otras que le hubiera gustado que le hubieran formulado; el 18 de junio, escribió cuatro páginas de esta sección.

El 19 de junio a las 4:30 de la tarde aproximadamente, King caminaba por el arcén derecho de la ruta 5 en North Novell. El conductor Bryan Smith, distraído por un rottweiler incontrolado que se movía en la parte trasera de su coche marca Dodge Caravan de 1985, atropelló a King, quien aterrizó en una zanja de unos 4 metros de profundidad desde el pavimento de la carretera de la Ruta 5.

El ayudante del comisario del condado de Oxford, Matt Baker, grabó que los testigos dijeron que el conductor no conducía con exceso de velocidad ni con imprudencia. Baker también informó que King fue atropellado por detrás. En la página web oficial de Stephen King se menciona que esto no es correcto ya que King iba caminando de cara al tráfico.

King estaba lo bastante consciente para dar los números de teléfono de su familia al ayudante del comisario para poder ponerse en contacto con ellos, aunque se encontraba sufriendo un dolor considerable. El escritor fue llevado en primer lugar al Hospital Northern Cumberland para luego ser trasladado desde allí en helicóptero al Hospital Central de Maine. Sus heridas —el pulmón derecho colapsado, múltiples fracturas en la pierna derecha, laceración del cuero cabelludo y la cadera fracturada— lo mantuvieron en el centro médico hasta el 9 de julio, casi tres semanas internado.

Después de cinco operaciones en diez días y terapia física, retomó en julio el trabajo donde lo había dejado en la novela "On writing", aunque se resentía todavía de su cadera y solamente podía sentarse unos cuarenta minutos antes de que el dolor se tornara intolerable. Su estado físico ha mejorado desde entonces.

Ese mismo año terminó la mayor parte "Buick 8: Un coche perverso" (en inglés "From a buick 8"), novela en la cual uno de los protagonistas muere en un accidente automovilístico. De las espeluznantes similitudes con su propio accidente, King dice que intentó "darle demasiada importancia al suceso". La novela "Misery", escrita en 1987, trata también sobre un escritor que experimenta heridas muy graves debido a un accidente de coche pero la novela se centra fundamentalmente en la enfermedad mental de una devota seguidora que atiende al escritor. También en la serie para TV "Kingdom Hospital", un pintor sufre un accidente exactamente igual al real, y en la novela de "dreamcatcher" del 2001, uno de los protagonistas (Henry) tuvo un accidente cerca de las vías del tren que casi acaba con su vida.

En 1996, King ganó el premio O. Henry Award por su historia "The Man in the Black Suit". En 2003, fue honrado con el premio a la trayectoria de National Book Awards, lo que causó un alboroto entre la comunidad literaria por la elección del escritor, especialmente de parte del crítico literario Harold Bloom.

Stephen King escribió seis libros bajo el seudónimo Richard Bachman. Hizo un funeral falso para Bachman después de que el secreto fue hecho público, lo cual inspiró el libro "The Dark Half". Escribió una historia corta bajo el nombre John Swithen, "The Fifth Quarter", la cual fue publicada de nuevo en 1993 bajo el nombre de Stephen King.

En 2002, King anunció que dejaría de escribir motivado aparentemente por la frustración de sus lesiones que lo incomodaban y reducían su energía. Sin embargo, desde entonces ha escrito algunos libros pero a un ritmo menor que el de antes. El escritor ha declarado que escribir es más para él una pasión que un oficio.

Desde 2003, el autor ha dado su punto de vista sobre la cultura popular en una columna de la página trasera de la revista "Entertainment Weekly", normalmente cada tres semanas. La columna es titulada "The Pop Of King", una referencia a "The King of Pop" ("el rey del pop" en inglés), Michael Jackson.

En octubre de 2005, King firmó un contrato con Marvel Comics; sería su primer proyecto oficial en este mundo ya que en los años 1980 participó en un cómic benéfico contra la hambruna en África. La idea sería expandir su serie "The Dark Tower". La serie será ilustrada por el artista Jae Lee, ganador del premio Eisner. Marvel anunció que el proyecto será retrasado hasta 2007 para que King tuviera el tiempo y dedicación necesarios.

Vive en Bangor, Maine con su esposa Tabitha Spruce, que también es novelista. Tienen otra casa en el distrito Western Lakes de Maine. King pasa el invierno en su mansión con vistas al mar ubicada en Sarasota (Florida). Recientemente construyó otra casa en Connecticut. Sus tres hijos Naomi Rachel, Joe Hill y Owen Phillip, alcanzaron la mayoría de edad y viven por su cuenta. Owen publicó en 2005 su primera colección de historias titulada "We're All in This Together: A Novella and Stories", y Joe Hill es autor de "20th Century Ghosts", una serie de cuentos (muy premiados) y una novela llamada "El traje del muerto" (de la cual se han vendido los derechos para la película de Warner Bros).

En su libro "On Writing: A Memoir of the Craft" ("Mientras escribo"), King describe su estilo de escritura de gran longitud y profundidad. Cree que para las buenas historias es mejor crear una pequeña "semilla" y dejar que la historia crezca y se desenvuelva desde ahí. Generalmente empieza sus historias sin saber cómo terminarán.

Es conocido por su calidad de detalles, continuidad, y referencias internas; muchas de sus historias se ven ligadas por personajes secundarios, pueblos ficticios, o eventos de libros pasados, muy al estilo de Lovecraft.

Sus libros contienen referencias a la historia y cultura de los Estados Unidos, particularmente a la más oscura y escalofriante parte de la cultura. Las referencias están plasmadas en historias de los personajes en las cuales se explican sus temores. Algunas referencias incluyen el crimen, guerras (especialmente la guerra de Vietnam), y el racismo. 

King utiliza un estilo de narración bastante informal mientras se refiere a sus fans como "lectores constantes" o "amigos y vecinos". Este estilo contrasta con los oscuros temas de sus historias. 

Stephen King tiene una sencilla fórmula para poder escribir bien: "Lee y escribe entre cuatro y seis horas al día. Si no encuentras el tiempo para hacerlo no podrás convertirte en un buen escritor."

Las novelas de terror y suspense de King están construidas basándose en una visión constante del mundo, descrita más claramente en "Insomnia", "Corazones en la Atlántida" y "La Torre Oscura". En esta visión describe la existencia metafóricamente como una torre oscura ubicada en un paisaje de rosas rojas en el Mundo Final. En esta torre hay varios niveles con entidades en cada uno (y son los seres humanos los que habitan el nivel más bajo). Algunos son hostiles (Rey Carmesí, Randall Flagg, John Farson o Pennywise), otros benevolentes (Roland Deschain o Cloto y Láquesis, también conocidos como "los médicos calvos y bajitos"). Muchas de sus novelas toman lugar en este multiverso ficticio y algunos personajes se relacionan con hechos de otras historias.

Tras la existencia del mundo en que preside dicha torre, coexiste una fuerza vital y elemental llamada Ka. Su manifestación más común está presente en el destino que cumple cada ser del multiverso (igualmente puede referirse a un lugar particular al que uno se ve obligado a ir).

Sobre política se ha manifestado en contra del candidato del Partido Repúblicano Trump.

King es un gran admirador del escritor H. P. Lovecraft y ha incorporado varias de sus técnicas (como la conexión entre las historias de sus libros, la utilización de recortes de periódicos, transcripciones de prueba, otros materiales de documentación, y el uso de pueblos ficticios como "Castle Rock" y "Derry") en sus novelas, pero se diferencia de él por su caracterización extensa, un diálogo efectivo e historias con finales positivos, todos estos inexistentes en los relatos de Lovecraft. 

Asimismo, se ha declarado admirador de la serie de libros de Harry Potter de la autora J. K. Rowling, a quien, según ha mencionado en reiteradas ocasiones, considera una gran escritora.

Edgar Allan Poe, uno de los padres del género de terror contemporáneo, ha tenido una gran influencia en las historias de King. Un buen ejemplo es "El resplandor". El texto extraído de la misma, "Y la muerte roja dominó sobre todas las demás," (en inglés ""And the red death held sway over all,"") recuerda al original, "Y la Oscuridad y la Decadencia y la Muerte Roja mantienen un dominio ilimitado sobre los demás" (en inglés ""And Darkness and Decay and the Red Death held illimitable dominion over all,"") contenido en la obra "La máscara de la muerte roja" de Poe. La novela de King es análoga al pequeño relato de Poe de forma bastante precisa. Los dos escritores comparten el uso de los Doppelgänger, aunque el tema está presente en la mayoría de las obras de terror y no se puede especificar a un solo autor. Además, el argumento del relato corto titulado "El cadillac de Dolan" (en inglés "Dolan's Cadillac") es en comparación casi idéntica a el relato de Poe llamado "El barril de Amontillado" (en inglés "The Cask of Amontillado"), parafraseando incluso el famoso alegato de Fortunato, "¡por el amor de Dios, Montresor!" (en inglés ""for the love of God, Montresor!"").

King declaró su admiración por otro autor menos prolífico: Shirley Jackson. La novela "Salem's Lot" empieza con una cita del libro "The Haunting of Hill House" de Jackson. Tony, un amigo imaginario de "El resplandor" tiene cierta relación con otro amigo imaginario, llamado también Tony, del libro "Hangsaman" de Jackson. Hay algunas otras similitudes entre los personajes Carrie de "Carrie" y Eleanor de "The Haunting of Hill House". King declaró que Carrie está basada en dos víctimas de abuso en la escuela que conoció. Una escena crucial de "Storm of the Century" está basada en el libro de Jackson titulado "The Lottery".

Y finalmente, otra de sus influencias fue John D. MacDonald. Ha sido un gran fan de MacDonald a lo largo de su vida y la deuda que le debe al viejo escritor parece clara. Del mismo modo que King es un maestro en el género del terror, MacDonald es bastante popular en el género criminalístico. King aprendió mucho del arte de penetrar en la mente de los personajes, utilizado por MacDonald. La manera en que ambos escritores describen a los personajes, aunque en distinto estilo, son bastante similares. King y MacDonald demuestran una gran dedicación en su trabajo y practican bastantes horas diariamente. King dedicó la novela "Sun Dog" a MacDonald, diciendo "Te extraño, viejo amigo." 

Debido a su gran popularidad, King es comparado habitualmente con Dean Koontz y algunos admiradores desean leer un libro escrito entre los dos. Ambos escritores declararon lo imposible del proyecto, la razón principal es el hábito de King de tener personajes con una vida miserable, y Koontz tiene el de escribir finales felices para la mayoría de sus libros.

Escribió dos novelas colaborando con Peter Straub: "The Talisman" y "Black House". King comentó que tenían planes de escribir el tercer y último de la saga pero no se ha propuesto ninguna fecha.

Escribió además el ensayo "¡Campeones mundiales al fin! con" el novelista Stewart O'Nan.






Sus novelas y relatos cortos han sido adaptados a diversos medios, tales como películas, series de televisión e historietas. Según el propio escritor, sus adaptaciones favoritas son "Cuenta conmigo", "The Shawshank Redemption" y "The Mist".




</doc>
<doc id="2572" url="https://es.wikipedia.org/wiki?curid=2572" title="Smoke">
Smoke

Smoke, estrenada con el título Cigarros en Hispanoamérica, es una película estadounidense de 1995, dirigida por Wayne Wang. El escritor Paul Auster, autor del guion, fue también co-director (aunque no conste así en los créditos).

La película tuvo una continuación titulada "Blue in the Face", siguiendo a parte de los personajes de la primera e introduciendo algunos nuevos.

En torno a un estanco se desenvuelven las historias de un puñado de personajes solitarios cuyas vidas parecen marcadas por el azar: el escritor Paul Benjamin, que trata de recomponer su vida tras la muerte de su esposa por culpa de una bala perdida en un atraco; el joven Rashid Cole, que trata de encontrar a su padre, que le abandonó cuando sólo era un niño; Cyrus Cole, que vive la amputación de su brazo izquierdo como un castigo divino; o el dependiente Auggie Wren, que guarda en su pasado algunos secretos de los que no está del todo orgulloso.



</doc>
<doc id="2573" url="https://es.wikipedia.org/wiki?curid=2573" title="San Juan de Torres">
San Juan de Torres

San Juan de Torres es una localidad del municipio de Cebrones del Río, situado en el sur de la provincia de León, España.

En este pueblo se encuentran los restos de un antiguo castro celta situado en la cima de una colina.
El pueblo se encuentra atravesado por el río Órbigo, afluente del Esla.
El campo es una vega muy fértil que produce alubias, patatas, remolacha y maíz, regados por el canal o caño de Cuatro Concejos, canalizado y reformado con la concentración parcelaria.

El pueblo también es paso por una de las antiguas rutas de la Vía de la Plata. Aunque esta ruta es apenas conocida por los lugareños, se cree que pudiera ser la ruta original, actualmente desviada por posibles motivos políticos y económicos.

Antes pasaba un camino llamado Cañada, paso de trashumantes por donde circulaban los rebaños de ovejas en la primavera hacia la montaña y regresaban en invierno a Castilla, desaparecido por la parcelaria; queda solamente la calle Real dentro del pueblo.
Hace unos años había dos cofradías en el pueblo la de San Roque y San Antonio por rotación todos los años se elegían tres cofrades que se encargaban de hacer la sepultura en el cementerio de los cofrades fallecidos ese año. Todos los años esos cofrades hacían en casa de uno de ellos, una comida de cofraternidad, en una de esas comidas allá por los años que empezó la guerra civil, compraron la dehesa del Marqués de Castañón. 

Se comenta que fue por la euforia que tenían después de la comida. A pesar de las dificultades económicas que hubo en aquellos años, salieron adelante y les ha dado buenos beneficios. Es un pueblo donde en verano los que tienen la gran suerte de poseer una casa vuelven desde donde se encuentran a pasar unos días de vacaciones y así recordar y disfrutar de los pasajes extraordinarios teniendo gran relevancia la calle Real que parte muy cerca de la iglesia y tiene una historia de marqueses porque en ella residió el Marqués de Castañon.

También existía una iglesia románica perteneciente a los templarios. La iglesia fue destruida en los años 1970 y todos sus libros, reliquias, santos y obras de arte fueron vendidos o desaparecieron. La iglesia actual es una iglesia simple y pequeña. De la antigua sólo se conserva el campanar en forma de espadaña.
La torre de la iglesia ha sido restaurada, y se reformó la entrada a la iglesia, el altar y el techo, dándole un retoque a todo el interior.

Las fiestas del pueblo son el 24 de junio coincidiendo con San Juan Bautista. Es costumbre que los quintos del pueblo recojan ramas de árboles la noche de San Juan, y las reparten por el pueblo, para que todas las casas amanezcan adornadas con una rama en la puerta.


</doc>
<doc id="2576" url="https://es.wikipedia.org/wiki?curid=2576" title="Salicaceae">
Salicaceae

Salicaceae, las Salicáceas, es una familia de plantas perteneciente al orden Malpighiales. La componen árboles o arbustos caducifolios y dioicos. Hojas alternas, simples, estipuladas. Flores inconspicuas, unisexuales, aclamídeas, acompañadas de brácteas y reunidas en amentos péndulos o erectos. Anemófilos y entomófilos ("Salix"); periantio copiforme o nulo, androceo con 2 - 10 estambres; gineceo bicarpelar sincárpico de carpelos abiertos; numerosos óvulos; flores acompañadas de brácteas. Frutos en cápsula loculicida, que se abre mediante 2-4 valvas; con semillas numerosas, pequeñas, orladas de pelos (diseminación anemócora), con corto poder germinativo. Reproducción vegetativa importante. 

Comprenden unas 300 especies de climas templados y fríos, principalmente del hemisferio boreal. Forman choperas y saucedas de ribera.


</doc>
<doc id="2577" url="https://es.wikipedia.org/wiki?curid=2577" title="Salicales">
Salicales

Una única familia: "Salicaceae". Árboles, arbustos y matas. Numerosos óvulos; 2 carpelos abiertos.

Es un sinónimo de Malpighiales.


</doc>
<doc id="2578" url="https://es.wikipedia.org/wiki?curid=2578" title="Sicono">
Sicono

Los siconos son un tipo de frutos compuesto o múltiple, típico del género "Ficus" al que pertenecen las higueras. Están compuestos de un receptáculo piriforme o redondeado, hueco en su interior y con una abertura apical llamada ostiolo, protegida por pequeños hipsófilos; dentro y en las paredes de este receptáculo se hallan las flores y más tarde los diminutos frutículos de estas plantas. El nombre viene del griego y significa higo.


</doc>
<doc id="2580" url="https://es.wikipedia.org/wiki?curid=2580" title="Sterculioideae">
Sterculioideae

Sterculioideae, según la última versión del Sistema de clasificación APG, APG III ha pasado a formar parte de Malvaceae como subfamilia. Antiguamente se consideraba distinta de las malváceas y era denominada Sterculiaceae, cuyo nombre proviene de uno de sus géneros: "Sterculia".

Según la circunscripción tradicional, Sterculiaceae, Malvaceae, Bombaceae y Tiliaceae comprendían el "núcleo Malvales" en el Sistema de Cronquist y la estrecha relación entre estas familias se reconoce generalmente. Sin embargo, Sterculiaceae se separa de Malvaceae "sensu stricto" debido a la suave superficie de los granos de polen y las anteras biloculares.

Numerosos estudios filogenéticos revelaron que Sterculiaceae, Tiliaceae y Bombacaceae como se definían tradicionalmente son cladísticamente polifiléticos, por lo que el estatus de cada una de las familias era incierto. El sistema de clasificación APG y APG II reunieron Bombacaceae, Malvaceae "sensu stricto", Sterculiaceae y Tiliaceae en una circunscripción más amplia de Malvaceae, es decir, Malvaceae "sensu lato". Según este punto de vista, los taxones anteriormente clasificados en Sterculiaceae se encuentran en las subfamilias Byttnerioideae, Dombeyoideae, Helicteroideae y Sterculioideae de Malvaceae "sensu lato".

Sterculiaceae fue reconocida como familia por la mayoría de los sistemáticos; en su sentido tradicional incluía alrededor de 70 géneros (), totalizando unas 1.500 especies de árboles y arbustos tropicales. Entre ellas "Theobroma cacao" y "Cola acuminata", además de muchas especies utilizadas por su madera.

Sterculioideae está compuesta por 12 géneros y unas 430 especies de distribución pantropical, son árboles y arbustos tanto perennes como caducifolios cuyas especies 
están caracterizadas pr sus flores sin pétalos (apétalas), sin epicaliz, y por la presencia de un cáliz carnoso, usualmente petaloideo y gamosépalo. No presentan tampoco estaminoideos pero tienen una columna estaminal monadelfa (o sea, con todos los estambres unidos entre sí) y gineceos y frutos apocárpicos, o sea, con los carpelos separados. Las flores son típicamente monóicas: existen flores femeninas y masculinas en la misma planta.



</doc>
<doc id="2581" url="https://es.wikipedia.org/wiki?curid=2581" title="Saxifragaceae">
Saxifragaceae

Saxifragaceae es una familia de plantas del orden Saxifragales, con 80 géneros y unas 1200 especies, la mayoría de regiones templadas y frías del hemisferio boreal o América del Sur. 

Plantas herbáceas o leñosas con predominio de hierbas perennes. Hojas alternas, opuestas o en roseta, simples aunque a veces profundamente recortadas. Flores hermafroditas, normalmente actinomorfas, pentámeras; androceo diplostémono; gineceo súpero, semiínfero o ínfero, abiertos o cerrados, con los carpelos unidos en la parte inferior. Inflorescencias generalmente en racimo o panícula. Frutos en cápsula, con gran número de semillas. Reproducción vegetativa muy importante, por bulbillos, bien radiculares, bien en las axilas de las hojas.



</doc>
<doc id="2583" url="https://es.wikipedia.org/wiki?curid=2583" title="Siglo XX">
Siglo XX

El d.C. (siglo veinte después de Cristo) o EC (siglo vigésimo de la era común) fue el período comprendido, entre el 1 de enero de 1901 y el 31 de diciembre de 2000.Es llamado el «Siglo de la Vanguardización» Fue el décimo y último siglo del .

A inicio del siglo xx, América Latina enfrentaba importantes cambios. Los países se habían insertado definitivamente en el sistema mundial y estaban dedicados a producir y exportar materias primas como alimentos y metales y también a importar manufacturas de los países industrializados

El se caracterizó por los avances de la tecnología; medicina y ciencia; fin de la esclavitud en los llamados países subdesarrollados; liberación de la mujer en la mayor parte de los países occidentales; pero más que todo por el creciente desarrollo de las industrias, convirtiendo a varios países en potencias mundiales como pueden ser Los Estados Unidos de América, también el siglo se destacó por las crisis y despotismos humanos en forma de regímenes totalitarios, que causaron efectos tales como las Guerras Mundiales; el genocidio y el etnocidio, las políticas de exclusión social y la generalización del desempleo y de la pobreza. Como consecuencia, se profundizaron las desigualdades en cuanto al desarrollo social, económico y tecnológico y en cuanto a la distribución de la riqueza entre los países, y las grandes diferencias en la calidad de vida de los habitantes de las distintas regiones del mundo. 

Al hacer balance de esta centuria, Walter Isaacson, director gerente de la revista "Time" declaró: «Ha sido uno de los siglos más sorprendentes: inspirador, espantoso a veces, fascinante siempre».

Según Gro Harlem Brundtland, ex primera ministra de Noruega, se trata de «un siglo de grandes progresos [y, en algunos lugares,] crecimiento económico sin precedentes», si bien las zonas urbanas míseras afrontaron un lúgubre panorama de «hacinamiento y enfermedades generalizadas vinculadas a la pobreza y al ambiente insalubre».

En los albores del , el Imperio británico (que dominaba una cuarta parte del planeta y de sus habitantes),
varios imperios europeos,
la Dinastía Manchú (de China) y el Imperio otomano controlaban gran parte del mundo. Mucho antes de finalizar el siglo, tales imperios habían quedado relegados a los libros de historia. Al final del siglo, tras la disolución de la Unión Soviética, el primer y mayor estado socialista, Estados Unidos de América quedó como la única superpotencia imperialista mundial.





























</doc>
<doc id="2584" url="https://es.wikipedia.org/wiki?curid=2584" title="Santalales">
Santalales

Los Santalales son un orden de plantas de flor perteneciente a las dicotiledóneas. 

Tienen tendencia a la reducción de la corola; además, la mayoría tienen tendencia a la vida parásita o semiparásita —pueden producir alimento a través de la fotosíntesis pero barrenan las raíces de otras plantas para obtener agua—, tendencia a la pérdida o reducción de la clorofila: mixotrofía y reducción del aparato vegetativo. Actinomorfas, periantio sencillo, con un verticilo de estambres, gineceo ínfero. La mayoría tienen semillas sin capa exterior protectora, lo que es atípico de las angiospermas.
Las siguientes familias son típicas de los nuevos sistemas de clasificación:


En el antiguo Sistema de Cronquist, algunas de las Santalaceae son reconocidas como familias separadas llamadas Viscaceae y Eremolepidaceae. Otras 3 familias estaban incluidas también:


Éstas ya no se consideran familias próximas de las Santalaceae pero, por el momento, su clasificación es incierta.


</doc>
<doc id="2585" url="https://es.wikipedia.org/wiki?curid=2585" title="Santalaceae">
Santalaceae

Las santaláceas (Santalaceae) son una familia de plantas perteneciente al orden de las santalales. 
Son plantas herbáceas o leñosas, hemiparásitas, con haustorios en las raíces de los huéspedes. Presentan hojas simples, habitualmente alternas. Las flores son inconspicuas, hermafroditas o unisexuales, actinomorfas, con perianto de tres a seis piezas, ovario ínfero, unilocular y carpelos abiertos. Los frutos pueden ser núculas o drupas. Agrupa a unas 450 especies de países cálidos y templados.

Los géneros "Arjona" y "Quinchamalium" actualmente se disponen en la familia Schoepfiaceae.




</doc>
<doc id="2586" url="https://es.wikipedia.org/wiki?curid=2586" title="Sapindales">
Sapindales

Las Sapindales son un orden de plantas eudicotiledóneas, genéticamente próximo al orden Rosales; en las clasificaciones actuales incluyen, entre las familias más conocidas a los "Citrus".

Se caracterizan por poseer dos verticilos de estambres (a veces uno reducido a estaminodios), de manera que son predominantemente pentacíclicas. Disco nectarífero de posición variable (a veces reducido a glándulas internas).

Predominan las hojas compuestas (si bien en los citrus son simples), el hábito leñoso, las flores pentámeras y el ovario súpero. Las inflorescencias son cimosas.

Este orden contiene nueve familias, unos 460 géneros y alrededor de 5.700 especies. Más de la mitad de las especies de este orden pertenecen a dos de sus familias: Sapindaceae (con unas 1.600 especies), donde se encuentran recogidas tanto Hippocastanaceae como Aceraceae; y Rutaceae (con unas 1.800 especies).


Las tres familias en las que no se detalla en número de especies que consta, suman unas 27 especies entre las tres.


</doc>
<doc id="2589" url="https://es.wikipedia.org/wiki?curid=2589" title="Suecia">
Suecia

Suecia (en sueco: ), oficialmente Reino de Suecia (en sueco: ), es un país escandinavo de Europa del Norte que forma parte de la Unión Europea (UE). Limita al norte con Noruega y Finlandia, al este con Finlandia y el golfo de Botnia, al sur con el mar Báltico y al oeste con el mar del Norte y Noruega. Tiene fronteras terrestres con Noruega y Finlandia, y está conectado a Dinamarca por el puente de Öresund. Su ciudad más poblada es Estocolmo, que es también su capital.

Con una extensión de 450 295 km², es el quinto país más extenso de Europa. En 2015, contaba con una población total de poco más de 9,7 millones de personas, de las cuales el 98 % cuenta con acceso a Internet, lo que lo convierte en el . Tiene una densidad de población de solo 22 h/km², similar a otros países de su entorno. Cerca del 84 % de la población vive en zonas urbanas. Los suecos disfrutan de un alto nivel de vida, con una organización y cultura corporativa no jerárquica, y colectivista en comparación con sus homólogos anglosajones. La conservación de la naturaleza, la protección del medio ambiente y la eficacia energética son, por lo general, una prioridad en la formulación de políticas y cuentan con acogida por gran parte del pueblo.. Mantiene el modelo nórdico de bienestar que brinda asistencia sanitaria universal y educación terciaria gratuíta a sus ciudadanos, tiene el undécimo ingreso per cápita más alto del mundo y ocupa un lugar destacado en numerosas mediciones de desarrollo humano, incluida la calidad de vida, salud, educación, igualdad, y prosperidad.

Durante mucho tiempo fue un importante exportador de hierro, cobre y madera. La mejora de los transportes y las comunicaciones ha permitido la explotación a gran escala de bienes naturales, sobre todo la madera y el mineral de hierro. En la década de 1980, la escolarización universal y la industrialización permitieron al país desarrollar una exitosa industria manufacturera. Tiene una rica oferta de energía hidráulica, pero carece de petróleo y de yacimientos de carbón importantes. En el siglo XX se ubicó constantemente entre los países con mejor Índice de Desarrollo Humano (IDH), actualmente ocupando la decimocuarta posición.

La Suecia moderna surgió de la Unión de Kalmar en 1397, y de la unificación del país por el rey Gustavo Vasa en el siglo XVI. En el siglo XVII, amplió sus territorios para formar el Imperio sueco. La mayor parte de los territorios conquistados fuera de la península escandinava se perdieron durante los siguientes siglos. La mitad oriental de Suecia constituida por la mitad oriental de Norrland y Österland se perdió frente a Rusia en 1809. Desde 1814, no ha participado en ningún conflicto, manteniendo una política exterior de paz y neutralidad en tiempo de guerra.

El nombre «Suecia» deriva del latín "Suetidi", el cual proviene del vocablo del inglés antiguo "Sweoðeod", que significa «pueblo de los suiones» (en escandinavo antiguo "Svíþjóð"). Esta palabra deriva de "sweon/sweonas" (en escandinavo antiguo "sviar", en latín "suiones"). La etimología de "Suiones", y por ende de Suecia, deriva probablemente del proto-germánico "Swihoniz", que significa «propiedad de uno», refiriéndose a la propiedad de una tribu germánica. El nombre en sueco, Sverige significa literalmente «Reino de los suiones» ("Sve": suiones; "Rike": reino), el cual se utilizaba para designar la zona sur del país habitada por la tribu germánica del mismo nombre.

Variaciones del inglés Sweden se utilizan en la mayoría de los idiomas, excepto en danés y en noruego, donde el nombre es el mismo que en sueco, Sverige. En los idiomas finlandés (Ruotsi) y estonio (Rootsi), el nombre proviene de la misma raíz que la palabra «Rusia», refiriéndose a la etnia "Rus", originaria de las zonas costeras de Uppland y Roslagen.

Su prehistoria comienza en el periodo llamado Oscilación de Allerød, alrededor del año 12 000 a. C. durante el Paleolítico superior, con la llegada de grupos nómadas de cazadores-recolectores en la zona sur del país, caracterizados por el uso de puntas de flecha hechas de piedra.

La agricultura y la ganadería, junto con la construcción de monumentos megalíticos, llegaron del continente con la cultura de los vasos de embudo alrededor del año 4000 a. C. El sur de Suecia fue parte del área donde se desarrolló la Edad de bronce nórdica. Este periodo comenzó cerca del año 1800 a. C. con el inicio de la importación del bronce desde Europa central. La minería no fue practicada durante este periodo y como el territorio no posee grandes yacimientos, todos los metales eran importados. La Edad de Bronce Nórdica fue completamente pre-urbana: la gente se volvió sedentaria y vivía en pequeñas aldeas y granjas, en casas comunales hechas de madera.

En ausencia de la dominación del Imperio romano, se considera que la Edad del Hierro sueca finalizó en el momento de la introducción en sus tierras de la arquitectura de piedra y de órdenes monásticas alrededor del año 1100. Como los registros escritos de esta época son de poca credibilidad, este periodo es considerado protohistórico, es decir, que aquellos registros aparecieron después del periodo en cuestión, y que fueron escritos en distintas áreas, o que los registros locales y contemporáneos son extremadamente cortos.

Un intento de los romanos por extender su imperio más allá de los ríos Rin y Elba fue abortado en el año , cuando los germanos derrotaron a las legiones romanas bajo el mando de Varo, al emboscarlas en la batalla del bosque de Teutoburgo. Alrededor de esta época hubo un gran cambio en materia de cultura en Escandinavia, resultado de un mayor contacto con los romanos.

Durante esta época el clima empeoró, forzando a los granjeros a resguardar a sus animales dentro de cobertizos durante los largos inviernos. Esto llevó a una acumulación anual de estiércol, que pudo ser usado por primera vez de forma sistemática para el enriquecimiento del suelo. De esta forma, la agricultura y la ganadería progresaron y se convirtieron en el motor económico de las primeras ciudades. A principios del siglo II, gran parte del suelo cultivado del sur de sus tierras fue dividido en lotes con bardas pequeñas hechas de piedra. De un lado del muro se encontraban los sembradíos permanentes y prados para el forraje de invierno, mientras que del otro estaba el bosque y la tierra para pastar el ganado. Esta división de la tierra fue usada hasta el siglo XIX.

En la protohistoria entró con el libro "Germania" de Cornelio Tácito en el año 98. Aunque la poca información que reporta sobre esta distante área ha sido estimada como incierta, ya que hace mención a varias tribus, como los suiones y los lapones de siglos posteriores. En cuanto a su escritura, el alfabeto rúnico fue inventado por la élite del sur de Escandinavia en el siglo II, pero todo lo que ha llegado al presente son breves inscripciones en artefactos, principalmente nombres masculinos, poniendo en evidencia que los pueblos del sur de Escandinavia hablaban proto-nórdico en aquella época, un idioma del que se derivó el sueco y otras lenguas nórdicas.

La época vikinga sueca abarca desde el siglo VIII hasta el XI. Durante este periodo, se cree que los suiones se expandieron hacia el sureste y se mezclaron con los gautas que habitaban el sur de la actual Suecia. Los vikingos suecos y los vikingos guter realizaban viajes principalmente hacia el este y hacia el sur, yendo a Finlandia, los países bálticos, Rusia, el Mediterráneo y a ciudades tan lejanas como Bagdad. Sus rutas atravesaban los ríos de Rusia hasta llegar a la capital del Imperio bizantino, Constantinopla (actualmente Estambul, Turquía), de donde partían hacia distintas direcciones. El emperador bizantino Teófilo comprobó la destreza que poseían para la guerra y los invitó a servirle como su guardia personal, la cual tomó el nombre de Guardia varega. También se cree que un grupo de vikingos suecos, llamados «rus», son los padres fundadores de Rusia. Las expediciones de estos fueron plasmadas en muchas piedras rúnicas existentes en el país, tales como las piedras rúnicas griegas y varegas. Hubo también una participación vikinga considerable en expediciones al oeste, las cuales fueron registradas en las piedras rúnicas inglesas. La última gran expedición vikinga fue el fallido viaje que dirigió Ingvar el Viajero a Serkland, la región del sureste del mar Caspio. Sus expedicionarios son conmemorados en las piedras rúnicas de Ingvar, ninguna de las cuales menciona a algún superviviente. Se desconoce lo que le sucedió a la expedición, pero se cree que fueron víctimas de alguna epidemia.

No se sabe cuándo ni cómo se creó el reino de Suecia, pero la lista de solo nombra a aquellos que reinaron en Svealand (Suecia) y Götaland (Gothia) al mismo tiempo, siendo el primero de ellos Erico el Victorioso. Previamente, Suecia y Gothia habían sido naciones separadas. Aunque no se sabe desde cuándo existían aquellos reinos, "Beowulf" los describe en las semilegendarias guerras entre suecos y gautas del siglo VI.

Durante los primeros años de la era vikinga en Escandinavia, Ystad en Escania y Paviken en Gotland fueron grandes centros del comercio de aquella época. Existen ruinas de lo que se piensa era un gran mercado en Ystad, que data de los años 600 a 700 d. C. En Paviken, un importante centro comercial de la región Báltica durante los siglos IX y X, se han encontrado restos de un gran muelle con talleres de construcción de barcos e industrias artesanales. Entre los años 800 y 1000, el comercio llevó a la abundancia de plata en Gotland, y de acuerdo a varios especialistas, los habitantes de la isla tenían mayor cantidad de este metal que todo el resto de la población de Escandinavia junta.

En el año 829, san Óscar introdujo el cristianismo, pero no fue hasta el siglo XII cuando la nueva religión comenzó a reemplazar las creencias tradicionales. Durante el siglo XI, el cristianismo se convirtió en la religión predominante, y para el año 1050 ya se contaba entre las naciones cristianas. El período que va de 1100 a 1400 se caracterizó por las luchas internas por el poder y la competencia entre los reinos nórdicos. Los reyes suecos también empezaron a expandir su territorio hacia Finlandia, creando conflictos con los rus, quienes se habían desprendido de toda conexión con Suecia.

En el siglo XIV, fue asolada por una epidemia de peste negra (peste bubónica). Durante este periodo las ciudades suecas también comenzaron a obtener mayor autonomía y fueron fuertemente influídas por los mercaderes alemanes de la Liga Hanseática, activos especialmente en Visby. En 1319, Suecia y Noruega fueron unidas por el rey Magnus Eriksson y en 1397 la reina Margarita I de Dinamarca efectuó una unión personal de Suecia, Noruega y Dinamarca, naciendo así la Unión de Kalmar. Sin embargo, los sucesores de Margarita, cuyo poder estaba centrado en Dinamarca, no lograron controlar a la nobleza sueca. Por largos periodos, el poder efectivo lo poseían regentes (notablemente aquellos de la familia Sture) elegidos por el parlamento sueco. Para remediar la situación, el rey Christian II de Dinamarca ordenó la ejecución de los nobles de Estocolmo. La matanza fue conocida como el «Baño de sangre de Estocolmo» e incitó a la nobleza sueca a formar una nueva resistencia, por lo que el 6 de junio de 1523, nombraron a Gustavo I de Suecia como su rey. Este hecho se considera a menudo como la fundación del Estado moderno de Suecia y el 6 de junio es ahora la Fiesta Nacional del país. Poco después, Gustavo I rechazó el catolicismo e introdujo la Reforma Protestante en el país. Por estos acontecimientos a Gustavo I se le conoce como el «Padre de la Nación».

Durante el siglo XVII emergió como una potencia europea. Antes del surgimiento del Imperio sueco, era un país muy pobre, escasamente poblado, y con poca participación en asuntos internacionales. Fue repentinamente convertido en una de las naciones líderes en Europa por Axel Oxenstierna y el rey Gustavo II Adolfo de Suecia, gracias a la conquista de territorios de Rusia y Polonia-Lituania, pero también gracias a su participación en la Guerra de los Treinta Años, la cual la convirtió en el líder continental del protestantismo hasta el colapso del imperio en 1721.

La guerra de Gustavo II Adolfo en contra del Sacro Imperio Romano-Germánico tuvo un alto costo para este último, donde un tercio de la población murió y casi la mitad de los Estados que lo componían fueron ocupados por los suecos. El plan de Gustavo II Adolfo era aventajarse del conflicto armado para expandir los límites de su reino. Sin embargo, Gustavo II Adolfo murió después en la batalla de Lützen de 1632, dejando el trono a la menor Cristina de Suecia. Después de la batalla de Nördlingen Suecia se retiró porque se cansó de las penurias de la guerra y perdió su poderío en la zona sur de la actual Alemania, y las provincias conquistadas se separaron del dominio sueco una a una, dejándola con solo un par de territorios en el norte: Pomerania Sueca, Bremen-Verden y Wismar.

A mediados del siglo XVII, era el tercer país más extenso en Europa, solo superado por Rusia y España. En 1658, alcanzó su máxima extensión bajo el reinado de Carlos X Gustavo de Suecia (1622-1660), poco después de la firma del Tratado de Roskilde. A mediados del siglo XVI, el rey Gustavo I convirtió al país al protestantismo y realizó una serie de reformas económicas. Durante el siglo XVII, el país se vio envuelto en varias guerras, como la que sostuvo contra Polonia-Lituania, en la que ambos compitieron por los territorios de los Países Bálticos hasta la batalla de Kircholm ocurrida en 1605, la cual es considerada una de las peores derrotas del ejército sueco.

Este periodo también fue testigo de «El Diluvio», la invasión sueca de la Unión de Polonia-Lituania. Después de más de medio siglo de una guerra casi constante, la economía sueca se deterioró seriamente. Reconstruir la economía y recuperar el poder militar se convirtió en una labor que se extendió durante toda la vida del sucesor de Carlos X, Carlos XI de Suecia (1655-1697). El legado para su hijo, Carlos XII, fue uno de los mejores arsenales en el mundo, un ejército numeroso y una gran flota.

En 1700, después de la batalla de Narva (una de las primeras batallas de la Gran Guerra del Norte), el Ejército Ruso, peor equipado y entrenado y desmoralizado por la retirada de Pedro I de Rusia antes de la batalla, fue severamente diezmado, dándole a Suecia la oportunidad de invadir Rusia. Sin embargo, Carlos XII no persiguió al Ejército Ruso, sino que se dirigió a Polonia-Lituania y en 1702, derrotó al rey polaco Augusto II y a sus aliados sajones en la batalla de Kliszów. Después de la exitosa invasión a Polonia, Carlos XII tenía preparado el terreno para invadir Rusia atacando su capital, Moscú, desde Ucrania. Además de su ejército contaba con la ayuda de cerca de 2000 cosacos ucranianos. Pero en esta ocasión el ejército zarista estaba mejor preparado y motivado, y después de acosar a los invasores con los jinetes cosacos y rebajar sus suministros con técnicas de tierra quemada, en 1709 Pedro I derrotó decisivamente a los suecos en la batalla de Poltava. Los suecos fueron perseguidos, rindiéndose tres días después en Perevolochna. Esta derrota significó el comienzo del derrumbe del Imperio sueco.

En 1716, Carlos XII intentó invadir Noruega, sin embargo, su avance fue frenado por los noruegos en 1718, con el asedio de la fortaleza Fredriksten. Los suecos no fueron derrotados militarmente en Fredriksten, pero la organización y estructura de la campaña noruega llevaron a la muerte del rey y a la retirada del ejército. Forzada a ceder grandes extensiones de tierra en el Tratado de Nystad de 1721, también perdió su lugar como imperio y como el Estado dominante del mar Báltico. Con la pérdida de la influencia sueca, Rusia emergió como un imperio y se convirtió en una de las naciones dominantes en Europa. En el siglo XVII, ya carecía de los suficientes recursos para mantener sus territorios fuera de Escandinavia, debido a lo cual perdió la mayoría de éstos, culminando con la pérdida del este de Suecia por Rusia, territorios que se convertirían en el Ducado de Finlandia semiautónomo en la Rusia imperial.

Después de que Dinamarca-Noruega fuera derrotada en las Guerras Napoleónicas, el 14 de enero de 1814 Noruega fue cedida a Suecia a cambio de las provincias del norte de Alemania, en el Tratado de Kiel. Los intentos de Noruega por mantenerse como una nación soberana fueron repelidos por el rey Carlos XIII de Suecia. El rey lanzó una campaña militar contra Noruega el 27 de julio de 1814, terminando con la Convención de Moss, la cual forzó a Noruega a una unión personal bajo el poder sueco, que duró hasta 1905. La campaña de 1814 fue la última guerra en la que su ejército participó como beligerante.

En los siglos XVIII y XIX tuvo lugar un importante crecimiento demográfico, que el escritor Esaias Tegnér en 1833 atribuyó a «la paz, la vacuna (contra la viruela), y las patatas.» Entre 1750 y 1850 la población sueca se duplicó. De acuerdo a algunos especialistas, la emigración en masa hacia Estados Unidos se convirtió en la única forma de evitar el hambre y la rebelión; más del 1 % de la población emigraba anualmente durante la década de 1880. Por entonces, seguía en la pobreza, con una economía básicamente agrícola, pese a que Dinamarca y otros países de Europa Occidental ya habían comenzado a industrializarse. Entre 1850 y 1910 más de un millón de suecos migraron hacia los Estados Unidos y a principios del siglo XX, había más población de origen sueco en Chicago que en Gotemburgo (la segunda ciudad más grande de Suecia). La mayoría de los inmigrantes suecos se establecieron en el Medio Oeste estadounidense, alcanzando una gran incidencia en la población de Minnesota. Como destinos secundarios, otros grupos de inmigrantes se dirigieron a Delaware y Canadá.

Si bien su proceso de industrialización se desarrolló lentamente, la agricultura experimentó cambios importantes debido a las innovaciones tecnológicas y al crecimiento de la población. Estas innovaciones incluían programas del gobierno de cercamiento, sobre-explotación de las tierras agrícolas y la introducción de nuevas semillas de cultivo como la de la patata. Debido al hecho de que los campesinos suecos habían sido explotados como en ningún otro lugar en Europa, la cultura granjera sueca adquirió un papel protagónico en los procesos políticos, característica que se ha mantenido en el tiempo, con el Partido Agrario, (actualmente llamado Partido del Centro). Entre 1870 y 1914, comenzó el proceso de desarrollo de su economía industrial que perdura hasta hoy.

Durante la segunda mitad del siglo XIX, se produjeron movimientos sociales y sindicales importantes, así como de grupos abstinentes y religiosos independientes, que comenzaron a presionar por un Estado democrático. En 1889 se fundó el Partido Socialdemócrata Sueco. Estos movimientos llevaron al país hacia una moderna democracia parlamentaria, alcanzada en la época de la Primera Guerra Mundial. Como la Revolución Industrial avanzaba durante el siglo XX, la población rural comenzó a migrar hacia las ciudades para trabajar en las fábricas y así poder ser eventualmente incluidos en los sindicatos. En 1917 tuvo lugar una revolución socialista que fracasó, la cual fue seguida en 1921 por el establecimiento de una monarquía parlamentaria de tipo democrático.

Durante el transcurso de ambas guerras mundiales se mantuvo oficialmente neutral, aunque su neutralidad en la Segunda Guerra Mundial ha sido muchas veces ocasión de debate; estuvo bajo la influencia alemana la mayor parte de la guerra y quedó aislada del resto del mundo por medio de bloqueos. Inicialmente, el gobierno sueco consideró que no estaba en posición de oponerse a Alemania, y posteriormente colaboró con el régimen de Adolf Hitler. Los voluntarios suecos en las unidades nazis SS estuvieron entre los primeros elementos en invadir la Unión Soviética durante la Operación Barbarroja. Asimismo, también proporcionó acero y maquinaria a Alemania durante la guerra. Hacia el final del conflicto, cuando la derrota alemana parecía inminente, Suecia comenzó a jugar un rol importante en esfuerzos humanitarios y en el albergue de refugiados, entre ellos los numerosos judíos de la Europa ocupada por los nazis que fueron salvados. Esto se debió en parte a que participó en misiones de rescate en campos de concentración, y porque el país era el principal centro de refugiados de Escandinavia y de los países Bálticos. Sin embargo, críticas internas y externas aseguran que pudo haber hecho más para resistir las amenazas de los nazis, incluso corriendo el riesgo de una ocupación.

Durante la Guerra Fría adoptó públicamente una posición de neutralidad, pero de manera no oficial algunos líderes suecos mantuvieron conexiones estrechas con Estados Unidos. Después de la Segunda Guerra Mundial, se aventajó de su infraestructura industrial intacta, estabilidad social y de sus recursos naturales para expandir su industria y apoyar la reconstrucción de Europa. Asimismo, formó parte del Plan Marshall y participó en la Organización para la Cooperación y el Desarrollo Económico (OCDE). Durante la mayor parte de la posguerra, el país fue gobernado por el Partido Socialdemócrata Sueco (en sueco: "Socialdemokraterna"). Este partido estableció un modelo corporativista que favorecía a las grandes empresas capitalistas, pero también a los sindicatos, organizados en la Confederación de Sindicatos Suecos (LTC), afiliada al mismo partido. El Estado sueco adquirió un rol decisivo y la cantidad de empleados públicos aumentó notablemente entre 1960 y 1980. Finalmente, el país se abrió al comercio internacional y se orientó al sector manufacturero internacional, obteniendo buenas tasas de crecimiento hasta la década de 1970.

Como otros países del mundo, entró en un periodo de declive económico luego de los embargos de petróleo de 1973-1974 y 1978-1979. En la década de 1980, los pilares de la industria sueca fueron reestructurados en gran medida. Se canceló la construcción naval, se integró la tala de bosques al proceso de producción moderna de papel, se centralizó y especializó la industria del acero y la ingeniería mecánica se orientó hacia la robótica.

Entre 1970 y 1990 casi todos los impuestos fueron elevados más del 10 %. El impuesto de límite de ingresos para los trabajadores alcanzó más del 80 %, y el gasto público superó la mitad del PIB nacional, a la vez que su política económica era cuestionada por los economistas clásicos.

A principios de la década de 1990, como el resto de países occidentales, el país cayó en una crisis fiscal. La respuesta del gobierno conservador fue reducir los gastos e instituir una serie de reformas para impulsar la competitividad, entre las que se encontraban reducir el Estado de bienestar sueco y privatizar bienes y servicios públicos. Las reformas le permitieron entrar en la Unión Europea, a la cual Suecia pertenece desde el 1 de enero de 1995, aunque sin adoptar el euro, pues decidió mantener la corona sueca como su moneda nacional.

Actualmente es uno de los países con más alto Índice de Desarrollo Humano, encontrándose entre las veinte economías más grandes del mundo. También suele participar en operaciones militares internacionales, incluyendo la guerra de Afganistán, donde las tropas suecas están bajo el mando de la OTAN; y en la Unión Europea apoyando operaciones de las «fuerzas de paz» en lugares como Kosovo, Bosnia-Herzegovina y Chipre. Además, el armamento utilizado por el ejército estadounidense en Irak es producido por varias empresas suecas.

Suecia es una monarquía constitucional, en la cual el rey Carlos XVI Gustavo es el jefe de estado, pero su poder real está limitado solo a funciones ceremoniales y oficiales. Aunque The Economist Group asegura que la democracia es algo difícil de medir, el «Índice de democracia de 2006» la colocó en primer lugar de su lista de 167 países.

Su gobierno está dividido en tres poderes: legislativo, ejecutivo y judicial. El poder legislativo es el "Riksdag" (el parlamento sueco), que según la constitución sueca, es la autoridad suprema del gobierno. Está conformado por 349 miembros, los cuales eligen al Primer Ministro, quien dirige los ministerios. Las elecciones parlamentarias se llevan a cabo cada cuatro años, en el tercer domingo de septiembre.

Los proyectos de ley deben ser presentados por los miembros del gabinete o del parlamento. Los últimos son elegidos sobre la base de escrutinio proporcional plurinominal para un periodo de cuatro años. La constitución puede ser modificada por el Riksdag, para lo cual se requiere que la decisión sea aprobada por una mayoría absoluta entre periodos de elecciones generales. Además de los estatutos gubernamentales, tiene otras tres leyes constitucionales fundamentales: el Acta de Sucesión Real, el Acta de Libertad de Prensa y la Ley Fundamental para la Libertad de Expresión.

El poder ejecutivo es ejercido por el primer ministro, el gabinete y el rey. El poder judicial cuenta con un organismo de regulación llamado "Lagrådet" (Consejo de Leyes), que tiene la facultad de examinar la constitucionalidad de las leyes y las decisiones del gobierno, aunque sus resoluciones no son obligatorias; sin embargo, debido a las restricciones de esta forma de control constitucional y a una débil jurisdicción, su labor tiene pocas consecuencias en la política nacional.

El Partido Socialdemócrata Sueco ha jugado un papel de líder político desde 1917, después de que los reformistas confirmaran su dominio y los de la izquierda dejaran el partido. Después de 1932, los gabinetes han sido dominados por el partido Socialdemócrata. En tan solo cinco elecciones generales, otro partido de centro-derecha consiguió suficientes asientos en el parlamento para convertirse en la fuerza líder en el gobierno. Sin embargo, el avance económico lento desde comienzos de la década de 1970, y especialmente la crisis de 1990, la forzaron a reformar su sistema político para hacerlo similar al de otros países europeos. En las elecciones generales de 2010, el Bloque Roji-Verde (Socialdemócratas con el Partido Verde) ganó la mayoría de asientos en el "Riksdag", dejando a la Alianza con solo 170 asientos.

Las elecciones en octubre de 2014 tuvieron los siguientes resultados:

El total de los partidos de gobierno (SAP+V+MP) son 159 escaños.
El total de los partidos de oposición (M+C+FP+KD) son 141 escaños.
Los parlamentarios fueron elegidos para el período 2014-2018.

En las elecciones al Parlamento Europeo, las partes que no hayan superado el umbral Riksdag han conseguido obtener representación en ese lugar: Lista de Junio (2004-2009), el Partido Pirata (2009-2014), y la Iniciativa Feminista (2014-corriente).

En Suecia el número de votantes siempre ha sido alto en comparación con muchos países, aunque ha ido en descenso en décadas recientes, y actualmente es de alrededor del 80 % (80,11 % en 2002 y 81,99 % en 2006). Los políticos suecos disfrutaban de un alto grado de confianza de los ciudadanos en la década de 1960, pero con el paso de los años ésta fue disminuyendo hasta alcanzar un nivel de confianza más bajo que en los demás países de la región. En cuanto a movimientos políticos, Suecia tiene una larga historia de los llamados "Folkrörelser" («movimientos populares»), siendo los más notables los sindicatos, el movimiento independiente cristiano, el movimiento de abstinencia, el movimiento feminista, etc.

El poder judicial está representado por la Corte Suprema de Suecia y los tribunales inferiores. La Corte Suprema es la tercera y última instancia en todos los casos civiles y criminales; está conformada por dieciséis Consejeros de Justicia o "justitieråd" los cuales son designados por el poder ejecutivo. Esta corte es una institución independiente del primer ministro y del parlamento, por lo que el gobierno no puede interferir en sus decisiones.

La aplicación de la ley es llevada a cabo por varias instituciones gubernamentales: la Policía Nacional de Suecia (encargado de la organización de la policía), la Fuerza Operante Nacional (unidad SWAT de Suecia), el Departamento Nacional de Investigación Criminal y el Servicio de Seguridad Sueco (responsables de actividades anti-terroristas y de contraespionaje) son algunas ejemplos.

De acuerdo a un estudio de victimización hecho a 1201 suecos en 2005, Suecia tiene un alto índice de delincuencia comparado con otros países de la Unión Europea. Los delitos más frecuentes son los asaltos, crímenes sexuales, crímenes de odio y fraudes. Sin embargo, presenta bajos niveles de robos a viviendas y de automóviles, problemas de adicciones y corrupción.

A través del siglo XX, su política exterior estuvo basada en el principio de no alianzas en tiempos de paz y neutralidad en tiempos de guerra. Esta doctrina de neutralidad data desde el siglo XIX, ya que el país no ha participado en ningún conflicto armado desde el fin de la guerra contra Noruega de 1814. Durante la Segunda Guerra Mundial, no se unió a las Fuerzas del Eje ni a los Aliados. Sin embargo, esto ha sido debatido muchas veces, debido a que Suecia permitió al régimen nazi alemán el uso de su sistema de caminos para transportar bienes y soldados, y obtener materias primas, especialmente el hierro obtenido de las minas ubicadas en el norte de Suecia, que eran vitales para la maquinaria alemana.

Durante la Guerra Fría, el país combinó su política de no alianzas con un perfil bajo en conflictos internacionales, aunque sí mantuvo una política de seguridad basada en una fuerte defensa nacional para detener posibles ataques. Al mismo tiempo, el país mantenía conexiones informales relativamente estrechas con el bloque capitalista, especialmente en materia de intercambio de información. En 1952, un DC-3 sueco fue derribado sobre el mar Báltico por un MiG-15 soviético. Investigaciones posteriores revelaron que el avión estaba obteniendo información para la OTAN. Otra aeronave, una PBY Catalina de búsqueda y rescate, fue derribada días después del primer incidente, también por los soviéticos.

A comienzos de la década de 1960, intentó jugar un rol más importante e independiente en materia de relaciones internacionales. Esto le llevó a participar en actividades internacionales para mantener la paz, especialmente a través de la ONU, y en apoyo a los países del Tercer Mundo. El primer ministro Olof Palme cuestionó severamente la acción de Estados Unidos en la Guerra de Vietnam y visitó durante la década de 1970, la Nicaragua sandinista y Cuba. En 1981, un submarino clase Whiskey soviético se adentró en aguas cercanas a la base naval sueca de Karlskrona en el sureste del país. Nunca se aclaró porqué el submarino terminó en aquel lugar, si por un error de navegación o si era una misión de espionaje contra el ejército sueco. El incidente llevó a una crisis diplomática entre la Unión Soviética y Suecia. Después del asesinato de Palme en 1986, el protagonismo internacional de Suecia se redujo considerablemente, aunque permaneciendo relativamente activo en misiones de paz y ayuda humanitaria.

En 1995, el país se convirtió en miembro de la Unión Europea, y como consecuencia de la situación de seguridad en el nuevo mundo, su política exterior y su doctrina de neutralidad han sido en parte modificadas, llegando a jugar un papel más activo en la cooperación para la seguridad de Europa. Es uno de los países de la UE que no ha ingresado en el euro por iniciativa propia. Asimismo, es desde 2014 el primer país de la UE en reconocer a Palestina como un Estado soberano más.

Las Fuerzas Armadas de Suecia (Försvarsmakten) son una agencia del gobierno dirigida por el Ministro de Defensa y es el responsable de su operación durante los periodos de paz. La tarea principal de las Fuerzas Armadas es la de entrenar y desplegar fuerzas para el apoyo de la paz en el extranjero, así como la habilidad de reenfocarse en la defensa del territorio sueco en caso de guerra. Las fuerzas armadas están divididas en el Ejército, la Fuerza Aérea y la Armada. El Supremo Comandante de las Fuerzas Armadas Suecas ("Överbefälhavaren", ÖB) es el oficial de más alto rango en el país.

Hasta el fin de la Guerra Fría, casi todos los hombres que alcanzaban la edad para el servicio militar eran reclutados. Aunque hasta hace unos años el servicio militar en Suecia era obligatorio, se esperaba terminar con esa medida próximamente. Y efectivamente, a mediados de 2010, se abolió el servicio militar obligatorio, resultando en la creación de un ejército integrado totalmente por voluntarios. En años recientes, el número de hombres reclutados ha disminuido drásticamente, mientras el número de mujeres voluntarias se ha incrementado ligeramente. El reclutamiento se ha dirigido generalmente a encontrar los reclutas más motivados, en vez de los que solo entran para cumplir su servicio. Por ley, todos los soldados sirviendo en el extranjero deben ser voluntarios. En 1975 el total de reclutas era de 45 000. En 2009, había descendido a 25 000.

Las unidades suecas formaron parte de las fuerzas de paz en operaciones en Chipre, la República Democrática del Congo, Bosnia y Herzegovina, Kosovo, Liberia, Líbano, Afganistán y Chad. Actualmente, una de las tareas más importantes para las Fuerzas Armadas de Suecia es la de crear un grupo de combate de la Unión Europea que sea liderado por Suecia, en el cual Noruega, Finlandia, Irlanda y Estonia también contribuirán.

Suecia es un estado unitario, actualmente dividido en veintiuna provincias administrativas ("län"). Cada provincia cuenta con su junta de administración o "länsstyrelse", la cual es elegida por el gobierno nacional (la primera junta de administración fue creada por el primer ministro sueco Axel Oxenstierna en 1634). En cada provincia existe un consejo o "landsting", el cual es elegido directamente por el pueblo.

Cada provincia se divide en varios municipios o "kommuner", con un total de 290 municipios. Su gobierno municipal es similar a una alcaldía. Una asamblea legislativa municipal, llamada "kommunfullmäktige", de entre 31 y 101 miembros (siempre un número impar) es elegida por elecciones populares que se realizan cada cuatro años en conjunto con las elecciones parlamentarias. A su vez, los municipios se encuentran divididos en un total de 2512 parroquias o "socken". En el pasado, esta subdivisión coincidía territorialmente con la parroquia -"församling"- usada por la Iglesia de Suecia. Actualmente, las parroquias son utilizadas con fines estadísticos.

Existen también otras divisiones que ya no tienen uso oficial, pero que aún se toman en cuenta para ciertos trabajos. La principal de ellas son las veinticinco provincias históricas de Suecia o comarcas ("landskap") las cuales todavía poseen relevancia cultural. Estas provincias se agrupan en tres grandes regiones según las características geográficas e históricas que tengan en común: Norrland para el norte, Svealand para el centro y Götaland para el sur.

Situado en el norte de Europa, Suecia limita al este con el mar Báltico y el golfo de Botnia, dándole al país una larga línea costera, que forma la parte este de la península Escandinava. Al oeste se encuentran los Alpes escandinavos (Skaderna), los cuales forman una frontera natural con Noruega. Al noreste limita con Finlandia, al suroeste con los estrechos de Skagerrak, Kattegat y Öresundque lo separan de Dinamarca, Alemania, Polonia, Rusia, Lituania, Letonia y Estonia. Además, está conectado con Dinamarca por el puente de Öresund.

Con una superficie de 450 295 km², Suecia es el del mundo. Es el quinto más grande del continente y el más grande de Europa del Norte. Su tamaño es un poco más grande que el estado de California y similar al de Uzbekistán, con una población de más de 9,2 millones de habitantes.

La altitud mínima de Suecia se encuentra en la bahía del lago Hammarsjön, cerca de Kristianstad con 2,41 metros bajo el nivel del mar. La altitud máxima del país está en el monte Kebnekaise con 2.111 msnm. El territorio sueco también comprende unas 221 800 islas, de las cuales 1085 cuentan con una población permanente. Gotland, Öland, Orust, Hisingen y Värmdö son las islas más grandes del país.

En su mayoría, el territorio sueco es plano, con excepción de la zona oeste donde surgen los Alpes escandinavos. Esta planicie y el clima propio del país, da lugar a la formación de muchos lagos, entre los que destacan por su tamaño Vänern, Vättern, Mälaren y Hjälmaren. El lago Vänern es el lago más grande del país y el tercero más grande del continente europeo, después de los lagos Ladoga y Onega en Rusia.

Geográfica e históricamente, Suecia puede dividirse en tres grandes regiones: el norte Norrland, el centro Svealand y el sur Götaland. La escasamente poblada Norrland comprende más de la mitad de la superficie del país. Además, cerca del 15 % del territorio se ubica dentro del Círculo Polar Ártico. El sur es predominantemente agrícola, mientras en el norte la actividad forestal es la industria más importante. Las regiones más densamente pobladas son Öresund en el sur y el valle del lago Mälaren cerca de Estocolmo.

La mayor parte de Suecia posee un clima templado, pese a su latitud, con cuatro estaciones diferentes y temperaturas templadas todo el año. Las tres regiones históricas del país reciben climas un poco diferentes: Gotland cuenta con un clima oceánico, Svealand con un clima húmedo continental y Norrland con un clima boreal. Sin embargo, el país es más cálido y seco que otros lugares de latitudes similares y de otras latitudes incluso más al sur, debido en gran parte a la corriente del golfo. Por ejemplo, el centro y sur del país tienen inviernos más cálidos que muchas partes de Rusia, Canadá y Estados Unidos. También debido a su localización, la duración del día varía enormemente. Al norte del Círculo Polar Ártico, el sol nunca se pone en algunos días de verano, y en algunos días de invierno nunca amanece. El día en Estocolmo dura más de dieciocho horas a finales de junio, pero solo alrededor de seis horas a finales de diciembre. Gran parte del territorio sueco recibe entre 1600 y 2000 horas de luz solar anualmente.

La temperatura varía del norte al sur. Las regiones de Svealand y Gotland tienen veranos templados e inviernos fríos, con temperaturas máximas entre 20 a 25 °C y mínimas de entre 6 y 15 °C durante el verano; y una temperatura promedio de -14 a 2 °C en el invierno. Por su parte, la Norrland tiene veranos más cortos y frescos, e inviernos más largos y fríos, con temperaturas usualmente bajo el punto de congelación desde octubre hasta junio. Ocasionalmente se presentan olas de calor con temperaturas por encima de los 25 °C que se presentan durante varios días en el verano, a veces hasta en la parte norte del país. Su temperatura más alta registrada fue de 38 °C en Målilla, en 1947, mientras la temperatura más baja ha sido de -52,6 °C en Vuoggatjålme en 1966.

En promedio, la mayor parte de Suecia recibe entre 500 y 800 mm de precipitación cada año, haciendo al país considerablemente más seco que el promedio mundial. El suroeste es la región del país con más precipitaciones, entre 1000 y 1200 mm, y en algunas zonas montañosas del norte se estima que se reciben más de 2000 mm de precipitaciones. Las nevadas ocurren de diciembre a marzo en Gotland, de noviembre a abril en Svealand y de octubre a mayo en Norrland. Pese a su situación geográfica, Gotland y Svealand tienden a estar virtualmente libres de nieve.

Al igual que el clima, la flora y fauna del país varían de acuerdo a la región. De sur a norte, se puede considerar que en Suecia existen cuatro ecorregiones: bosque mixto báltico, bosque mixto sarmático, pradera y bosque montano de abedules de Escandinavia y la taiga escandinava y rusa. Esto produce una variación entre la vida silvestre de las tres regiones: en Svealand son comunes las plantas coníferas, mientras que en Gotland las plantas caducifolias son las que predominan. En general, las especies vegetales más comunes en el país incluyen a la haya, el roble, el tilo, el fresno, el arce, el olmo y varias especies de orquídeas.

De la misma forma, la fauna que habita dentro del territorio nacional se distribuye según las condiciones geográficas y climatológicas de cada región. Los osos, las linces, los lobos, los venados, los alces, los zorros y varias especies de roedores pueden ser considerados los animales más comunes en Suecia. Las aves como el gallo lira, la chocha perdiz, la perdiz, los patos y los cisnes habitan gran parte del territorio sueco. Los lagos y costas del país son el hábitat de muchas especies de peces, entre las que sobresalen el bacalao, la macarela, el salmón, el lucio europeo y el arenque. La disponibilidad del pescado, así como el clima del país, resulta en una gastronomía local basada fuertemente en los alimentos marinos.

La economía de Suecia es una economía mixta orientada principalmente a la exportación y al comercio internacional. Considerada por el Banco Mundial y por el Fondo Monetario Internacional como una «economía avanzada», actualmente su PIB nominal alcanza los 444 585 millones de dólares. Por lo tanto, cuenta con un moderno sistema de distribución, suficientes comunicaciones externas e internas y una fuerza de trabajo especializada. La madera, la energía hidráulica y el hierro constituyen la base económica del país, junto con el sector de ingenierías que aporta el 50 % de la producción y exportaciones. Las telecomunicaciones y la industria automotriz y farmacéutica son también de gran importancia. La agricultura cuenta con solo el 2 % de la fuerza de trabajo.

A finales del año 2009, las diez compañías suecas más importantes eran: AB Volvo, Ericsson, Vattenfall, Skanska, Svenska Cellulosa Aktiebolaget, TeliaSonera, Electrolux, H&M (Hennes & Mauritz), ICA AB y Nordea. Casi toda la producción industrial sueca es realizada por empresas privadas, hecho que contrasta con otros países industrializados, como Austria e Italia, donde las empresas del Estado tienen mayor presencia. Para 2008, el gobierno de centro-derecha del primer ministro Fredrik Reinfeldt ya había privatizado más de cincuenta empresas públicas.
La población económicamente activa (PEA) es de unos 4,9 millones de personas, de los cuales alrededor de un tercio cuentan con estudios de educación superior. La economía del país crece a un ritmo del 2 % por año. Según la Organización para la Cooperación y el Desarrollo Económico (OCDE), la clave del crecimiento de la productividad sueca son la desregulación, la globalización y el apoyo al sector tecnológico. El éxito de Suecia es debido a un modelo económico respetuoso con la empresa privada, de hecho se encuentra en el puesto 23 de 178 países en el índice de Libertad Económica año 2015, a pesar de un alto nivel de tributación redistributivo.

En 2003, Suecia rechazó el euro como moneda a través de un referéndum, por lo que actualmente la moneda oficial del país es la corona sueca (SEK). El banco central de Suecia es Sveriges Riksbank, que fue fundado en 1668, lo que lo hace el banco central más antiguo del mundo. Además de ser la casa emisora de moneda, Sveriges Riksbank se ocupa de la estabilidad de los precios, manteniendo la inflación en un 2 % anual, una de las más bajas entre los países europeos desde mediados de la década de 1990. Los países con los que efectúa la mayor parte de la actividad financiera son Alemania, Estados Unidos, Noruega, Reino Unido, Dinamarca, y Finlandia.

El Foro Económico Mundial de 2010 lo consideró como el segundo país más competitivo del mundo, solo por debajo de Suiza. Por su parte, el Índice de Libertad Económica la ubicó en el número 21 entre 179 países evaluados y en el 10 entre los 41 países europeos. Finalmente, ocupó el noveno lugar en el Anuario IMD de Competitividad 2008.

Gran parte del sector energético es propiedad privada y se encuentra apoyado principalmente en la energía hidráulica, que en 2000 aportó 76 TWh (53,8 % de la producción total), y la energía nuclear, que produjo 53 TWh (37,4 %). Al mismo tiempo, el uso de biocombustibles, turba, energía eólica y otras fuentes de energía renovable aportaron solo 4 TWh (2,7 %). En 2007, se produjeron en Suecia poco más de 144 TWh de energía eléctrica. La biomasa es principalmente usada para producir el calor utilizado en sistemas de calefacción y en procesos industriales. Nord Pool, creada en 1991, es la empresa encargada de comercializar la energía entre los países nórdicos.

La crisis del petróleo de 1973 reforzó la decisión del gobierno de disminuir su dependencia de combustibles fósiles importados. Desde entonces, la electricidad es obtenida en su mayor parte de centrales hidroeléctricas, de fuentes renovables y de energía nuclear, este último con un uso limitado. Entre otras cosas, el accidente nuclear de la central central nuclear de Three Mile Island en Estados Unidos, llevó al parlamento a prohibir la construcción de nuevas centrales nucleares. Sin embargo, después de múltiples estudios que mostraban al proyecto como «inviable», además del cambio de administración en el gobierno y un intenso debate, el parlamento aprobó la anulación de esta política en junio de 2010.

En 2006, debido a un grave accidente que estuvo a punto de causar una pérdida masiva de radiación en la central nuclear de Forsmark, el gobierno clausuró cuatro de las diez plantas de energía nuclear que se encontraban operando. En 2009, el gobierno socialdemócrata sueco «decidió imprimir un giro total a su política energética, abriendo el camino para la construcción de nuevas centrales nucleares.»

Diversos líderes políticos han anunciado planes para liberar a Suecia del uso de combustibles fósiles, la disminución del uso de la energía nuclear y la inversión de varios millones de dólares para investigaciones en energía renovable y eficiencia energética. El país ha seguido por muchos años la estrategia de fijar impuestos como instrumento de política ambiental, incluyendo los impuestos energéticos y el impuesto al dióxido de carbono.

Suecia cuenta con 572 900 km de caminos pavimentados y 1855 km de autopistas. Las autopistas corren a través de Suecia, Dinamarca y sobre el puente de Öresund hacia Estocolmo, Gotemburgo, Upsala y Uddevalla. El país lleva adelante un plan de construcción de autopistas; como parte del mismo, el 17 de octubre de 2007 fue concluida la carretera de Upsala a Gävle. Desde 1736 hasta mediados del siglo XX, el sentido de circulación era hacia la derecha ("Vänstertrafik"), hasta que los votantes rechazaron ese sentido en 1955, para imponer la dirección inversa a partir de 1963. Sin embargo, el parlamento regresó al sentido hacia la derecha en 1967, en el día llamado "Dagen H".

El ferrocarril ha sido privatizado en parte, pero existen varias compañías operadas por los condados y municipios. Entre los principales operadores se encuentran: SJ AB, Veolia Transportation, Connex, Green Cargo, Tågkompaniet, Inlandsbanan y múltiples compañías regionales. Los ferrocarriles que aún no han sido privatizados son propiedad de Banverket. Existen cerca de 11 633 km de vías férreas, de las cuales 7596 km están electrificadas.

En el país existen más de 240 aeropuertos. Los aeropuertos más grandes e importantes incluyen al Aeropuerto de Estocolmo-Arlanda (17,91 millones de pasajeros en el 2007) a 40 km al norte de la capital del país, el Aeropuerto de Gotemburgo-Landvetter (4,3 millones de pasajeros en 2006) y el Aeropuerto de Estocolmo-Skavsta (2 millones de pasajeros en 2006). En Suecia se encuentran las dos autoridades portuarias más importantes en Escandinavia: la del puerto de Gotemburgo y la transnacional de Copenhague-Malmö.

Suecia liberalizó su industria de telecomunicaciones en un proceso que incluyó la regularización de los medios de comunicación y que duró más de diez años, culminando en 1993. En el país existen más de 5,3 millones de líneas telefónicas en uso, además de casi 11 millones de teléfonos móviles. A su vez, más del 90 % de la población tiene acceso a Internet. Las compañías radiodifusoras públicas tuvieron el monopolio de la radio y televisión por mucho tiempo en el país, desde que la primera estación de radio comenzó sus transmisiones en 1925. Más tarde, en 1954 una segunda cadena inició transmisiones y una tercera estación abrió en 1962, en respuesta a las estaciones de radio piratas. En 1979, las estaciones de radio de beneficencia fueron permitidas y en 1993 comenzaron las estaciones de radio locales.

Oficialmente, fue en 1956 cuando la primera estación de televisión comenzó las retransmisiones. Un segundo canal, TV2, fue creado en 1969. Estos dos canales (operados por Sveriges Television desde finales de la década de 1970) tuvieron un monopolio hasta la década de 1980, cuando la televisión por cable y satélite estuvieron disponibles en el país. El primer servicio satelital en sueco fue TV3, que era transmitido desde Londres en 1987. Fue seguido por Kanal 5 en 1989 (entonces conocido como «Canal Nórdico») y TV4 en 1990.

En 1991 el gobierno anunció que comenzaría a recibir solicitudes de aquellas empresas que desearan transmitir su señal por cable. TV4, que anteriormente había transmitido vía satélite, comenzó a transmitir por cable en 1992, convirtiéndose en el primer canal de iniciativa privada en transmitir desde el interior del país. Hoy en día, cerca de la mitad de la población utiliza la televisión por cable. La televisión digital terrestre comenzó en 1999 y las transmisiones de televisión analógicas terminaron en 2007.

Suecia está entre los consumidores de periódicos más grandes del mundo, y la mayoría de las localidades y ciudades cuentan con un periódico local. Los principales periódicos de circulación nacional son: "Dagens Nyheter" (de inclinación liberal), "Göteborgs-Posten" (liberal), "Svenska Dagbladet" (conservador) y "Sydsvenska Dagbladet" (liberal). Los dos tabloides más populares son el "Aftonbladet" (socialdemócrata) y "Expressen" (liberal). El periódico gratuito "Metro International", de circulación mundial, fue originalmente fundado en Estocolmo; mientras "The Local" (liberal), otro periódico de circulación mundial, también tiene su sede en Suecia.

A fines de diciembre de 2010, su población total fue estimada en 9 415 570 habitantes. De acuerdo a estimaciones de su Instituto de Estadísticas (SCB por sus siglas en sueco), cerca del 12 de agosto de 2004 la población sueca excedió los nueve millones por primera vez. En 2007, aproximadamente el 16,7 % de los habitantes (1,53 millones) tenía al menos un pariente nacido en el extranjero, principalmente de Escandinavia. Esto refleja los grandes procesos migratorios entre los países nórdicos, originados primeramente por la búsqueda de empleo y posteriormente le siguieron décadas de inmigración de refugiados de países en conflictos. El país se transformó de una nación de emigrantes al concluir la Primera Guerra Mundial, en un país de inmigrantes después de la Segunda Guerra Mundial. En el 2006, la inmigración a Suecia alcanzó su más alto nivel desde que comenzaron los registros: 95 750 personas llegaron al país en ese año.

Los grupos de inmigrantes más numerosos en Suecia consisten en gente proveniente de Finlandia, seguidos de personas nacidas en Irak, la antigua Yugoslavia, Somalia, Alemania, Dinamarca, Noruega, Turquía, Polonia, Rumania, Rusia, Siria, Líbano, Chile e Irán. Además, Suecia es el hogar de la comunidad más grande de exiliados de asirios y cristianos sirios.

La inmigración proveniente de otros países nórdicos alcanzó su nivel más alto entre los años de 1968 y 1970 con 40 000 personas por año. Esto sucedió debido a las nuevas leyes de inmigración promulgadas en 1967, las cuales hicieron más difícil a los inmigrantes que no provenían de Escandinavia el establecerse en el país, principalmente por razones políticas. La inmigración de refugiados provenientes de las afueras de la región nórdica se incrementó notablemente a finales de la década de 1980, con la llegada de varios grupos de refugiados provenientes de Asia y América, especialmente de Irán y Chile (luego del golpe de Estado de 1973 fueron exiliados muchos simpatizantes y militantes de partidos políticos de izquierda y centroizquierda, perseguidos por la dictadura de Augusto Pinochet). Durante la década de 1990 y en adelante, otro grupo grande de refugiados llegó desde la antigua Yugoslavia y el Medio Oriente. Esta llegada de inmigrantes ha causado algunos problemas de convivencia, tal es el caso de la ciudad de Malmö, donde los inmigrantes y sus descendientes conforman el 40 % de la población. De 2007 a 2010 la cifra promedio anual de inmigrantes alcanzó, considerando todas las categorías, las cien mil personas. El número de emigrantes, por su parte, rondó en torno a las 45 000 personas.

El idioma más hablado en el país es el sueco, una lengua germánica relacionada y muy similar al danés y al noruego, pero con diferencias en pronunciación y ortografía. El sueco es comprensible para noruegos y daneses, teniendo los segundos un poco más de dificultad que los primeros. Aunque el sueco es el idioma predominante, la ley sueca no lo considera como el idioma oficial. Los finlandeses que habitan al este son la minoría lingüística más significativa del país. Componen más del 3 % de la población y el finés es reconocido como un idioma minoritario. Además, cuenta con otros cuatro idiomas reconocidos como minoritarios: el meänkieli, el sami, el romaní y el yidis. En 2005, se presentó ante el parlamento una propuesta para que el sueco fuera declarado el idioma oficial del país, pero finalmente fue rechazada.

La gran mayoría de sus habitantes nacidos después de la Segunda Guerra Mundial entienden y hablan el inglés gracias a los vínculos comerciales, la popularidad de los viajes al extranjero y una fuerte influencia anglo-estadounidense. A partir de 1849, el inglés se convirtió en una materia obligatoria en la escuela secundaria para aquellos que estudiaban ciencias naturales, y se convirtió en obligatoria para todos los alumnos a finales de la década de 1940. Dependiendo de las autoridades escolares locales, el inglés es una materia obligatoria entre primer y noveno grado, con al menos un año extra de estudio en la secundaria. Muchos estudiantes aprenden uno o dos idiomas aparte del inglés, entre los cuales destacan el alemán, el francés y el español.

Antes del siglo XI, predominaba la religión nórdica en la que se rendía culto a los dioses Æsir, o Ásatrú con su centro en el Templo de Upsala. Con la cristianización, las leyes del país fueron cambiadas, prohibiendo adorar a otras deidades hasta los últimos años del siglo XIX. En 1530 después de la Reforma Protestante, Olaus Petri, seguidor de las ideas de Martín Lutero, llevó a cabo la separación entre la Iglesia y el Estado sueco, al mismo tiempo que abolió la autoridad de los obispos católico. De esta manera el luteranismo se adoptó como religión en gran parte del país, proceso que llegó a su fin con el Concilio de Upsala en 1593.

Durante la era siguiente a la Reforma pequeños grupos de calvinistas de los Países Bajos, la Hermandad de Moravia y hugonotes de Bélgica jugaron un papel importante en la industria y el comercio, y fueron en parte tolerados siempre y cuando mantuvieran un bajo perfil religioso. Los lapones originalmente tenían su propia religión shamánica, pero fueron convertidos al luteranismo por los misioneros suecos en los siglos XVII y XVIII.

Con la liberalización religiosa ocurrida a finales del siglo XVIII, los seguidores de otras religiones, incluyendo el judaísmo y el catolicismo, pudieron vivir y trabajar abiertamente en el país, pero para los luteranos suecos el cambiarse de religión fue ilegal hasta 1860. En el siglo XIX llegaron a sus tierras varias Iglesias evangélicas y, hacia el final del siglo, el secularismo, lo cual condujo a muchas personas a renunciar de la religión. Abandonar la Iglesia de Suecia se tornó legal en la llamada «Ley de Deserción de 1860», pero solo con la condición de pasar a pertenecer a otra religión. El derecho a permanecer fuera de cualquier congregación religiosa fue establecido en la Ley de Libertad de culto de 1951.

Cerca del 61,2 % de la población integra la Iglesia de Suecia (luterana), pero el número va descendiendo alrededor del 1 % cada año, y menos del 10 % de ellos asisten regularmente a los servicios religiosos de la Iglesia de Suecia. Sin embargo, la razón del número elevado de miembros se debe en parte a que hasta 1996, todos los niños que nacían se convertían automáticamente en miembros si alguno de sus padres lo era. Desde 1996, solo los niños que son bautizados se convierten en miembros. Alrededor de 275 000 suecos pertenecen a otras iglesias protestantes (donde la asistencia a los servicios es mucho más alta) y debido a la inmigración existen alrededor de 500 000 musulmanes (5.2 %), 100 000 cristianos ortodoxos y 92 000 católicos viviendo en Suecia. A pesar de las cifras, muchos estudios aseguran que Suecia es uno de los países con menos adeptos religiosos del mundo y con un alto grado de ateísmo: entre 46 y 85 % de los suecos no creen en un dios.

Siendo un país desarrollado, en Suecia la atención sanitaria es universal y gratuita, es financiada casi en su mayoría con impuestos. A menudo, la nación se encuentra entre los cinco países con la tasa más baja de mortalidad infantil; también se encuentra entre los países con mayor esperanza de vida (81 años) y en pureza del agua potable. En Suecia existen 3,3 médicos por cada 1000 habitantes, además de que el gobierno invierte el 9,2 % del PIB total en gastos de salud.

Esto se debe a la alta calidad del sistema de salud, que es similar al de otros países europeos y a menudo es clasificado como uno de los mejores en el mundo. Los servicios de salud son coordinados por la Junta Nacional de Salud y Bienestar Social ("Socialstyrelsen").

El sistema de salud actual fue fundado en 1968, gracias a la unión de la Junta Real de Salud y la Junta Real de Asuntos Sociales por el gobierno socialdemócrata, asegurando un amplia cobertura de la seguridad social, a través de la provisión de atención sanitaria gratuita, un sistema de pensiones de jubilación y subsidios por enfermedad, guarderías gratuitas de preescolar y subsidios económicos por maternidad o paternidad.

La prostitución es legal en todo el país. Ante la problemática en relación con la salud sexual a nivel social, Suecia tomó en sus manos la opción de legalizar la prostitución en 1999. Suecia se destaca por dar uno de los mayores permisos por maternidad o paternidad: La ley exige que cada uno de los padres tome 60 días de permiso laboral para el cuidado de sus recién nacidos.

Gracias a su sistema educativo bien desarrollado, tiene uno de los índices de alfabetización más altos en el mundo, con un 99 %. Los niños entre uno y seis años tienen garantizado un lugar en un colegio preescolar público (en sueco: "förskola" o, coloquialmente, "dagis"). Entre los siete y quince años de edad los alumnos ingresan a la escuela primaria y secundaria, las cuales son obligatorias. Los estudiantes suecos de quince años ocupan el 22° lugar en el Informe PISA, al igual que dentro de los países miembros de la OCDE.

Después de completar el noveno grado, cerca del 90 % de los graduados continúan sus estudios por tres años de educación media superior ("gymnasium"); al terminar esta, los alumnos están calificados para conseguir un empleo o para realizar una solicitud de ingreso a la universidad. El país posee gran variedad de y colegios, aunque a menudo el Instituto Karolinska, la Universidad de Upsala, la Universidad de Lund y la Universidad de Estocolmo son citadas como las instituciones educativas más prestigiosas.

El sistema escolar es en gran parte financiado con los impuestos. Cualquier ciudadano puede establecer una escuela sin ánimo de lucro y el gobierno municipal debe abonarles el mismo monto que obtienen las escuelas municipales, sin realizar discriminaciones en la distribución de los cheques escolares. Este sistema data de 1992 y fue tomado de la política escolar de los Países Bajos. Como en otros países europeos, el gobierno también subsidia el intercambio de alumnos de origen extranjero que buscan un título en las instituciones suecas.

La cultura sueca es percibida típicamente como igualitaria, sencilla y abierta a influencias de otros países. El país ha recibido la influencia cultural de otros países e instituciones: la Iglesia Católica y Alemania durante la Edad Media, Francia durante el siglo XVIII, de nuevo Alemania en el siglo XIX y los países de la Angloesfera después de la Segunda Guerra Mundial. De igual manera, su cultura y el desarrollo de la misma se encuentran en una íntima relación con la de los demás países nórdicos.

En todo el país existen cerca de 330 bibliotecas y más de 200 museos, la mayoría de ellos ubicados cerca de las grandes ciudades como Estocolmo, además de múltiples lugares turísticos de interés artístico, cultural e histórico. El patrimonio cultural sueco es reconocido a nivel mundial: catorce lugares dentro del territorio nacional han sido declarados «Patrimonio de la Humanidad» por la Unesco. Otro aspecto cultural destacado a nivel internacional es la entrega del Premio Nobel, instituido por Alfred Nobel. Este galardón se ha otorgado cada año desde 1901 a personas que han hecho investigaciones sobresalientes, inventado técnicas o equipamiento revolucionario o contribuciones notables a la sociedad.

En las décadas de 1960 y 1970, Suecia fue uno de los primeros lugares donde surgió un movimiento que ahora se conoce como la «revolución sexual», especialmente promoviendo la igualdad de género. Actualmente, el porcentaje de personas solteras es uno de los más altos del mundo. La película sueca "" (1967) reflejó un punto de vista liberal sobre la sexualidad e introdujo el concepto del «pecado sueco». En décadas recientes, se ha convertido en uno de los países más tolerantes del mundo hacia la homosexualidad y desde 2009 está permitido el matrimonio entre personas del mismo sexo.

El arte de Suecia se encuentra fuertemente vinculado con el arte de las demás naciones de Escandinavia, debido principalmente a las condiciones geográficas e históricas en las que se fue desarrollando. Desde las primeras pinturas rupestres y monumentos megalíticos, pasando por el arte medieval y gótico, el arte sueco no surgió en sí hasta la formación de la identidad nacional sueca, entre los siglos XVI y XVII. Así, Estocolmo se convirtió en el centro artístico de la nueva nación, donde se desarrollaron las tendencias originadas en otras partes de Europa: el renacimiento, el barroco, el rococó, entre otros.

Sin embargo, fue hasta el siglo XIX cuando los artistas suecos comenzaron a atraer a los críticos mundiales. Dentro de este escenario se destaca la participación de Anders Zorn, Carl Larsson, Eugène Jansson, Richard Bergh y August Strindberg, quienes dieron grandes aportaciones a la pintura, la escultura y la fotografía. Durante el siglo XX, las tendencias expresionistas y modernistas entraron a la escena artística de Suecia. Las nuevas tendencias artísticas desarrolladas en el país han captado la atención internacional, principalmente en el campo del diseño, la moda, la música pop y la gastronomía.

En 1930, la pintura, el diseño gráfico y la arquitectura moderna entraron al país con la corriente del funcionalismo y desde entonces las obras de pintores, diseñadores y arquitectos suecos han ganado una buena reputación. Sin embargo, su arte tradicional aún se conserva, siendo su mejor ejemplo las artesanías fabricadas en las zonas rurales y comercializadas en las ciudades e incluso exportadas a otras partes del continente. Algunas de las más populares incluyen las estufas de cerámica, obras hechas de vidrio y las tallas de madera.

En las últimas décadas, el cine sueco adquirió importancia internacional gracias a las obras de directores como Ingmar Bergman, Vilgoy Sjöman, Bo Widerberg, Roy Andersson y Lasse Hallström. La industria del cine es regulada por el Instituto Sueco del Cine, apoyado en gran parte por programas del gobierno, quien destina cerca de 200 millones de dólares anuales a la producción y promoción de películas nacionales. Además, cada año se celebran varios festivales internacionales de cine en todo el país; los Premios Guldbagge son otorgados cada año a los mejores largometrajes y a menudo son considerados como los «Óscar Suecos».

Suecia cuenta con una rica tradición musical, desde las baladas folclóricas medievales hasta el hip hop. La música nórdica precristiana se perdió con el paso del tiempo, aunque se han elaborado recreaciones históricas basadas en instrumentos encontrados en sitios arqueológicos vikingos. Entre los instrumentos utilizados se encuentran el "lur" (una especie de trompeta), instrumentos de cuerda sencillos, flautas de madera y tambores. Es posible que algunos rasgos de la música vikinga permanezcan hasta el día de hoy en las canciones tradicionales suecas.

La música tradicional es un escenario musical importante que a menudo incorpora elementos de otros géneros contemporáneos como el "rock" y el "jazz". También se encuentra presente la música lapona, llamada "yoik", la cual forma parte de la espiritualidad tradicional del pueblo lapón y ha ganado reconocimiento mundial dentro del campo de la música folclórica. Además, el país también tiene una prominente tradición en música coral, derivada en parte de la importancia cultural de la música folclórica. De hecho, de los 9,4 millones de habitantes, se estima que entre 500 y 600 mil personas forman parte de un coro.

El "jazz" es otro de los géneros importantes dentro de la música sueca. Durante los últimos sesenta años ha mantenido un estándar artístico elevado, en gran parte debido a las bandas locales que reciben influencia de otros países como Bélgica, Francia, Reino Unido y Estados Unidos. A menudo se cita a Lars Gullin como uno de los máximos representantes del "jazz" sueco.

Dentro de la industria musical actual, Suecia es el tercer exportador de música más grande en el mundo, con más de 800 millones de dólares de ingresos en 2007, sobrepasado solo por Estados Unidos y el Reino Unido. Actualmente, el pop sueco es uno de los estilos más conocidos del país. El grupo ABBA fue uno de los primeros grupos musicales suecos en hacerse famosos alrededor del mundo, además de que gracias a su fama el pop sueco adquirió cierto estatus de importancia internacional. A su vez, otras bandas como Roxette, Ace of Base, Europe y The Cardigans consolidaron la imagen de la música moderna sueca. Otro género similar proveniente de Alemania, el "schlager", cobró popularidad desde la década de 1950 y su influencia ha permanecido en la música actual a través de diversos festivales de música como los "folkpark", el Melodifestivalen y el Festival de Eurovisión.

El indie pop también tiene muchos representantes en Suecia. En Gotemburgo se han creado una serie de importantes sellos discográficos como Sincerely Yours o Service. Algunos artistas y grupos indies son Jens Lekman, The Knife, Love Is All, The Concretes, Broder Daniel, The Tough Alliance, Peter Olof Swartz, Bjorn and John, Little Dragon, El Perro del Mar, Maia Hirasawa, Fever Ray, Popsicle, Studio, The Embassy, The Honeydrips, Brainpool, Air France, jj, Joel Alme o Pacific!.

Por otro lado, en décadas recientes géneros como el heavy metal, comenzaron a crecer dentro de la escena musical sueca, abarcando un amplio abanico de subgéneros con exponentes reconocidos mundialmente. Junto con Noruega, el país ha sido el centro de desarrollo de muchos de estos estilos y artistas. En la década de 1980 resulta notable la influencia de bandas como Bathory o en el black metal y de Candlemass en el Doom Metal. En los años 1990 destaca especialmente la ola de grupos de death metal melódico de Gotemburgo de los grupos At The Gates, Dark Tranquillity e In Flames, que generó toda una corriente de fanes y seguidores entre los que destacan bandas como Amon Amarth, Arch Enemy o Soilwork. También hubo una ola de bandas de death metal progresivo como Edge of Sanity, Meshuggah, Therion y Opeth. El renombrado guitarrista de metal neoclásico y power metal, Yngwie Malmsteen, también es de origen sueco. En este ámbito otra banda muy relevante mundialmente son Hammerfall y además con músicos compartidos como Anders Johansson. Otros grupos importantes dentro de la escena sueca son Hammerfall, Entombed, Evergrey, Katatonia, Marduk, Dissection, The Haunted, Dark Funeral, Vintersorg. Una banda que también ha alcanzado un éxito y reconocimiento a nivel mundial, es la banda de Garage Rock y Rock N' Roll: The Hives.

En cuanto a la música electrónica y dance en la segunda década de siglo XXI destacan nombres como Lykke Li, Swedish House Mafia, Avicii, Eric Saade, Carola, la greco-sueca Helena Paparizou, Yohio, Loreen, Icona Pop, Tove lo, Alesso, Zara Larsson, AronChupa, Måns Zelmerlöw y Ghost.

El primer texto literario hallado en Suecia es la Piedra de Rök, tallada durante la Era Vikinga cerca del año 800 d. C. Con la conversión del país al cristianismo en el 1100 d. C., Suecia entró en la Edad Media, durante la cual los monjes prefirieron usar el latín en sus escritos, por lo que los textos en el sueco de esa época son escasos. La literatura sueca floreció solo después de que el idioma sueco fuera estandarizado en el siglo XVI, siendo la Biblia uno de los primeros libros traducidos al sueco en 1541. A menudo esta traducción es llamada la "Biblia de Gustavo Vasa".

Con la implantación del sistema educativo y la libertad que dio la secularización, el siglo XVII vio el desarrollo de múltiples escritores suecos. Ejemplo de esto son Georg Stiernhielm (siglo XVII), quien fue el primero en escribir poesía clásica en sueco; Johan Henric Kellgren (siglo XVIII), el primero en escribir una prosa fluida en sueco; Carl Michael Bellman (finales del siglo XVIII), el primer escritor de baladas de burlesque; y August Strindberg (finales del siglo XIX), un escritor socio-realista y dramaturgo que alcanzó la fama mundial. El siglo XX continuó dando notables autores como Selma Lagerlöf, (Premio Nobel de Literatura 1909), Verner von Heidenstam (Premio Nobel de Literatura 1916) y Pär Lagerkvist (Premio Nobel de Literatura 1951). En total se han entregado siete Premios Nobel de Literatura a escritores suecos.

En décadas recientes, varios escritores han sido reconocidos internacionalmente, incluyendo los autores de novelas policíacas Henning Mankell, Stieg Larsson, Camilla Läckberg y Åsa Larsson así como Jan Guillou, escritor de novelas de espías. Pero la autora sueca más reconocida en las últimas décadas es la escritora de cuentos para niños Astrid Lindgren, cuyas obras principales, como "Pippi Långstrump" y "Emil of Maple Hills", aún se encuentran entre los libros infantiles más populares.
Y en el género de terror está John Ajvide Lindqvist con su novela "Déjame entrar" publicada en 2008 y ya con una adaptación sueca y un adaptación estadounidense para la pantalla grande.

Siendo un país desarrollado muy avanzado, las investigaciones científicas juegan un papel clave para el desarrollo económico y para la sociedad en general, y la alta calidad en el desarrollo científico y tecnológico es reconocida alrededor del mundo. Juntos, el sector público y privado destinan cerca del 4 % del PIB a la investigación y desarrollo (I+D), lo cual le convierte en uno de los países que más invierte en I+D en términos de porcentaje del PIB. El estándar de las investigaciones suecas es alto y el país es líder mundial en múltiples campos científicos. Por ejemplo, es el primero en Europa en cuanto al número de trabajos científicos publicados per cápita.

En 1739 se fundó la Real Academia de las Ciencias de Suecia, con personajes como Carlos Linneo y Anders Celsius entre sus primeros miembros. Desde 1870, las compañías ingenieras fueron creadas a un ritmo nunca antes visto y los ingenieros se convirtieron en los héroes de la época. Muchas de las compañías fundadas por estos pioneros son aún reconocidas a nivel internacional. Entre los principales innovadores suecos de la época destacan: Alfred Nobel quien inventó la dinamita e instituyó el Premio Nobel; Gustaf Dalén fundó la compañía de gas AGA y ganó el Premio Nobel de Física por sus válvulas solares; Lars Magnus Ericsson comenzó la empresa que lleva su nombre, Ericsson, que hoy en día es una de las compañías de telecomunicaciones más grandes del mundo; Jonas Wenström fue un pionero en la corriente alterna y junto con el inventor croata Nikola Tesla, inventó el sistema electrónico de tres fases.

En comparación internacional, la alta tecnología industrial es relativamente más importante en todos los sectores, particularmente en las telecomunicaciones y en la industria farmacéutica. Aunque es un país relativamente pequeño, Suecia ha estado desde hace mucho tiempo en la vanguardia de I+D. Por varias décadas, el gobierno ha puesto como prioritarias algunas actividades científicas de investigación. Este fuerte apoyo ha ayudado a Suecia a convertirse en un país líder en términos de innovación. Durante muchos años, fue un país líder entre los miembros de la OCDE en lo que se refiere a investigaciones y el uso de tecnología avanzada.

Estadísticas demuestran que entre los años de 1970 a 2003, el sistema nacional de innovación sueco estaba entre los mejores de los países miembros de la OCDE en cuanto a la generación de invenciones tecnológicas y el número de patentes registradas de acuerdo al tamaño de la población. Solo Suiza reportó un mayor índice de patentes en relación a su población. En total, a finales de 2009 contaba con 33 523 patentes registradas, de acuerdo con la Oficina de Patentes y Marcas Registradas de los Estados Unidos, con solo diez países superando su número de patentes. Más aún, en el 2001 se encontraba entre los países con el mayor número de publicaciones científicas en los campos de ciencia médica, ciencias naturales e ingeniería.

En términos de estructura, la economía sueca se caracteriza por sus grandes exportaciones orientadas hacia el sector tecnológico e industrial, así como los comparativamente pequeños sectores de servicios y finanzas. Así la ciencia y tecnología se han vuelto una parte importante para la economía del país, principalmente para las grandes organizaciones industriales y de servicios que la dominan, ya que gran parte de las industrias suecas multinacionales tuvieron sus orígenes en el ingenio de múltiples inventores suecos.

La gastronomía sueca siempre ha estado bajo la influencia del clima y los recursos disponibles en las diferentes regiones del país. Como la de otros países escandinavos (Dinamarca, Finlandia y Noruega), es tradicionalmente sencilla. El pescado (particularmente el clupea), la carne y las patatas son los ingredientes básicos para elaborar la mayoría de las típicas recetas suecas.

Entre los platos más famosos del país destacan las albóndigas suecas, tradicionalmente servidas con salsa, papas hervidas y con mermelada de arándanos rojos; los panqueques, el "lutfisk" y el "smörgåsbord". Además, también existen una gran tradición en producción de lácteos: el queso, la "filmjölk" (leche ácida), la "långfil", (yogur espeso), la "filbunke" (leche cuajada) y la "gräddfil" (nata agria). El "knäckebröd" es el pan tradicional sueco y se ha desarrollado en muchas variantes contemporáneas. El "brännvin" (un término que agrupa bebidas alcohólicas como el "aquavit" y el vodka) es muy popular en el país, y junto con la cerveza, son imprescindibles en los eventos sociales tradicionales. Otras bebidas como el café, la leche, y las bebidas carbonatadas.

Sin embargo, debido a las influencias extranjeras y relaciones internacionales del país, su gastronomía ha sido influenciada por la cocina francesa y mediterránea desde el siglo XVIII. Actualmente, como resultado de la migración y la globalización, existen gran variedad de platillos importados, además de que el consumo de comida rápida también está generalizado.

Aparte de las festividades tradicionales de su Iglesia, también se celebran varias fechas únicas, algunas de las cuales se llevan a cabo desde la época pre-cristiana. Sin embargo, muchas de ellas son de ámbito local o regional y por lo tanto son consideradas «días festivos "de facto"». Estos incluyen el Sábado Santo, la Noche de Walpurgis, la Nochebuena, la Nochevieja, entre otros. Además de los domingos, los días que son considerados oficialmente como «días festivos» (y por lo tanto, inhábiles) se enlistan en la tabla.

El deporte en Suecia es considerado como un movimiento nacional ("föreningsstöd"), en el cual participa activamente casi la mitad de la población. Las actividades deportivas son reguladas en gran parte por la Confederación de Deportes de Suecia y el Comité Olímpico de Suecia, quienes engloban a más de 22 000 clubes deportivos en todo el país especializados en distintas disciplinas. Sin embargo, la mayor parte de las actividades deportivas son subsidiadas por el gobierno, aunque algunos de los deportes más populares reciben apoyo de varios patrocinadores.

Por otra parte, el tenis ha tenido grandes éxitos. Björn Borg es considerado como uno de los mejores tenistas masculinos de la historia del tenis. Ocupó el puesto número 1 del ranking ATP y tiene en su haber 11 títulos de Grand Slam. Otros tenistas destacados son Mats Wilander y Stefan Edberg.

Los dos deportes más populares en Suecia son el hockey sobre hielo y el fútbol. La selección de hockey sobre hielo sueca ("Tre Kronor") ha ganado el Campeonato Mundial de Hockey sobre Hielo ocho veces, siendo el tercer país con más medallas. Además ganaron la medalla de oro en las olimpiadas de 1994 y 2006.

Por su parte, la selección de fútbol ha tenido buenos éxitos en pasadas copas mundiales, siendo sub-campeones en el mundial de Suecia 1958, y quedaron en tercer lugar dos veces, en Brasil 1950 y Estados Unidos 1994. A nivel de jugadores, son conocidos grandes jugadores como Gunnar Gren, Gunnar Nordahl, Nils Liedholm, Martin Dahlin, Henrik Larsson (ganador de la Bota de Oro en 2001) y Zlatan Ibrahimović; este último considerado como uno de los mejores jugadores de la actualidad. Y a nivel de clubes, el IFK Göteborg es el único club que ha sido campeón europeo al ganar en los años 80 la Copa de la UEFA (1982 y 1987).

Después del fútbol, los deportes ecuestres tienen el mayor número de practicantes, además de que Gotemburgo cuenta con un centro ecuestre de importancia internacional. Otros deportes practicados comúnmente en el país incluyen el golf, el tenis, las pruebas de atletismo y varios deportes de equipo como el balonmano, el floorball, el basquetbol y el bandy.

Suecia es el octavo país en el medallero de los Juegos Olímpicos con 193 medallas de oro, 204 de plata y 230 de bronce obtenidas hasta 2010. El país se ubica en el segundo puesto en el medallero histórico de esquí de fondo, equitación y pentatlón moderno, tercero en lucha y saltos, y cuarto en tiro y canotaje. Los deportistas con más medallas de oro han sido Gert Fredriksson, Sixten Jernberg, Gunde Svan, Henri Saint Cyr y Thomas Wassberg.

La capital sueca fue elegida como sede de la 5ª edición de los Juegos en 1912. Otros eventos deportivos en los cuales el país ha sido el anfitrión son la Eurocopa 1992, la Copa Mundial Femenina de Fútbol de 1995, y múltiples campeonatos mundiales de hockey sobre hielo, atletismo, esquí, bandy, curling, patinaje artístico y natación.





</doc>
<doc id="2590" url="https://es.wikipedia.org/wiki?curid=2590" title="Sistema de unidades">
Sistema de unidades

Un sistema de unidades es un conjunto de unidades de medida consistente, normalizado y uniforme. En general definen unas pocas unidades de medida a partir de las cuales se deriva el resto. Existen varios sistemas de unidades:
Además de estos, existen unidades prácticas usadas en diferentes campos y ciencias. Algunas de ellas son:




</doc>
<doc id="2592" url="https://es.wikipedia.org/wiki?curid=2592" title="Singularidad gravitacional">
Singularidad gravitacional

Una singularidad gravitacional o espaciotemporal, de modo informal y desde un punto de vista físico, puede definirse como una zona del espacio-tiempo donde no se puede definir alguna magnitud física relacionada con los campos gravitatorios, tales como la curvatura, u otras. Numerosos ejemplos de singularidades aparecen en situaciones realistas en el marco de la relatividad general en soluciones de las ecuaciones de Einstein, entre los que cabe citar la descripción de agujeros negros (como puede ser la métrica de Schwarzschild) o a la descripción del origen del universo (métrica de Robertson-Walker).

Desde el punto de vista matemático, adoptar una definición de singularidad puede ser complicado,pues si pensamos en puntos en que el tensor métrico no está definido o no es diferenciable, estaremos hablando de puntos que automáticamente no pertenecen al espacio-tiempo. Para definir una singularidad deberemos buscar las huellas que estos puntos excluidos dejan en el tejido del espaciotiempo. Podemos pensar en varios tipos de comportamientos extraños:

Las singularidades pueden ser, en sus aspectos más generales;

Geométricamente las singularidades físicas pueden ser:

Según su carácter las singularidades físicas pueden ser:


Según la visibilidad para observadores asintóticamente inerciales alejados de la región de agujero negro (espacio-tiempo de Minkowski) éstas pueden ser:


Los teoremas sobre singularidades, debidos a Stephen Hawking y Roger Penrose, predicen la ocurrencia de singularidades bajo condiciones muy generales sobre la forma y características del espacio-tiempo.

El primero de los teoremas, que se enuncia a continuación, parece aplicable a nuestro universo; informalmente afirma que si tenemos un espacio-tiempo globalmente hiperbólico en expansión, entonces el universo empezó a existir a partir de una singularidad (Big Bang) hace un tiempo finito:

El teorema anterior por tanto es el enunciado matemático que bajo las condiciones observadas en nuestro universo, en el que es válida la ley de Hubble, y admitiendo la validez de la teoría de la Relatividad general el universo debió empezar en algún momento.

El siguiente teorema relaciona la ocurrencia de "superficies atrapadas" con la presencia de singularidades. Puesto que en un agujero negro de Schwarzschild, y presumible agujeros con geometrías similares, ocurren superficies atrapadas, el siguiente teorema predice la ocurrencia de singularidades en el interior de una clase muy amplia de agujeros negros.
Una superficie atrapada una variedad riemanniana de dos dimensiones compacta que tiene la propiedad de que tanto su como su pasado causal tiene en todo punto una expansión negativa. No es complicado probar que cualquier esfera, de hecho cualquier superficie cerrada contenida en una esfera, dentro de la región de agujero negro de un espacio-tiempo de Schwarzschild es una superficie atrapada, y por tanto en dicha región debe aparecer una singularidad. El enunciado de este teorema, debido a Roger Penrose (1965), es el siguiente:

La existencia de una geodésica de tipo luz inextensible, implica que existirá un fotón que saliendo de dicha superficie tras un tiempo de viaje proporcional a 2/"c"|θ| se topará con una singularidad temporal futura. Aunque desconocemos la naturaleza física real de las singularidades por carecer de una teoría cuántica de la gravedad el fotón o bien "desaparecerá" o bien experimentará algún fenómeno asociado a dicha teoría de la gravedad cuántica cuya naturaleza desconocemos.

Para la cual, la traza de la curvatura intrínseca satisface "K" < "C" < 0, donde "C" es una cierta constante. Entonces ninguna curva temporal partiendo de Σ y dirigida hacia el pasado puede tener una longitud mayor que 3/|"C"|. En particular, todas las geodésicas temporales hacia el pasado son incompletas."

Aunque sin ser estrictamente teoremas de singularidades existen una colección de resultados probados por Hawking (1971) que establecen que, en el marco de la teoría general de la relatividad:

Los teoremas anteriores son importantes porque garantizan, que aun en situaciones reales donde los cálculos exactos resultan complicados o imposibles, las propiedades topológicas de un espacio-tiempo que contiene agujeros negros garantizan ciertos hechos, por complicada que sea la geometría. Naturalmente sabemos que en una teoría cuántica de la gravedad los dos primeros resultados, probablemente no se mantienen. El propio Hawking sugirió que la emisión de radiación Hawking es un proceso mecano-cuántico a través del cual un agujero negro podría perder área o evaporarse; por lo que, los resultados anteriores son sólo las predicciones de la teoría general de la relatividad.

La descripción del espacio-tiempo y de la materia que hace la teoría de la relatividad general de Einstein no puede describir adecuadamente las singularidades. De hecho, la teoría general de la relatividad sólo da una descripción adecuada de la gravitación y espacio-tiempo a escalas mayores que la longitud de Planck "l":

Donde:
formula_5 es la constante de Planck reducida, formula_6 constante de gravitación universal, formula_7 es la velocidad de la luz.

De ese límite cuántico se debe esperar que igualmente la teoría de la relatividad deje de ser adecuada cuando predice una curvatura espacial del orden de "l" cosa que sucede muy cerca de las singularidades de curvatura como las existentes dentro de los diversos tipos de agujeros negros.



</doc>
<doc id="2593" url="https://es.wikipedia.org/wiki?curid=2593" title="Sinapsis">
Sinapsis

La sinapsis (del griego σύναψις ["sýnapsis"], ‘unión’, ‘enlace’) es una aproximación (funcional) intercelular especializada entre neuronas, ya sean entre dos neuronas de asociación, una neurona y una célula receptora o entre una neurona y una célula efectora (casi siempre glandular o muscular). En estos contactos se lleva a cabo la transmisión del impulso nervioso. Este se inicia con una descarga química que origina una corriente eléctrica en la membrana de la célula presináptica (célula emisora); una vez que este impulso nervioso alcanza el extremo del axón (la conexión con la otra célula), la propia neurona segrega un tipo de compuestos químicos (neurotransmisores) que se depositan en la hendidura o espacio sináptico (espacio intermedio entre esta neurona transmisora y la neurona postsináptica o receptora). Estas sustancias segregadas o neurotransmisores (noradrenalina y acetilcolina entre otros) son los encargados de excitar o inhibir la acción de la otra célula llamada célula post sináptica.

La palabra sinapsis viene de "sinapteína", que Charles Scott Sherrington y colaboradores formaron con las palabras griegas "sin-", que significa "juntos", y "hapteina", es decir "con firmeza".

Estos "enlaces químico-eléctricos" están especializados en el envío de cierto tipo de señales de pervivencia, las cuales afectan a otras neuronas, a células no neuronales como las musculares o glandulares.

Existen dos tipos de actividad base distinta, la actividad de pervivencia y la actividad de supervivencia.

La actividad sináptica de pervivencia se desarrolla en estos contextos:

La actividad sináptica de supervivencia se desarrolla en estos contextos:

La sinapsis se produce en el momento en que se registra actividad químico-eléctrica presináptica y otra postsináptica. Si esta condición no se da, no se puede hablar de sinapsis. En dicha acción se liberan neurotransmisores ionizados con base química, cuya cancelación de carga provoca la activación de receptores específicos que, a su vez, generan otro tipo de respuestas químico-eléctricas.

Cada neurona se comunica, al menos, con otras mil neuronas y puede recibir, simultáneamente, hasta diez veces más conexiones de otras. Se estima que en el cerebro humano adulto hay por lo menos 10 conexiones sinápticas (aproximadamente, entre 100 y 500 billones). En niños alcanza los 1000 billones. Este número disminuye con el paso de los años, estabilizándose en la edad adulta.

Las sinapsis permiten a las neuronas del sistema nervioso central formar una red de circuitos neuronales. Son cruciales para los procesos biológicos que subyacen bajo la percepción y el pensamiento. También son el sistema mediante el cual el sistema nervioso conecta y controla todos los sistemas del cuerpo.

De acuerdo con las últimas investigaciones relacionadas con los astrocitos y la matriz extracelular, las sinapsis constarían de cuatro elementos: los pre y postsinápticos neuronales, los astrocitos cercanos y la matriz extracelular que funcionarían como reguladores en la transferencia de información en el interior del sistema nervioso.

Desde el punto de vista histológico y funcional, una neurona tiene tres zonas principales: el cuerpo o soma, las dendritas y el axón. Estos dos últimos elementos son los encargados de establecer las relaciones sinápticas: las dendritas son como antenas que reciben la mayoría de la información que proviene de otras células; el axón, por su parte, es el cable con el que una neurona se conecta a otras.

Las conexiones pueden establecerse a muy corto alcance, a unos cientos de micrómetros a la redonda, o a distancias mucho mayores. Las neuronas de la espina dorsal, por ejemplo, se comunican directamente con órganos como los músculos para dar lugar al movimiento (sinapsis neuromuscular).

Una sinapsis prototípica, como las que aparecen en los botones dendríticos, consiste en unas proyecciones citoplasmáticas con forma de hongo desde cada célula que, al juntarse, los extremos de ambas se aplastan uno contra otro. En esta zona, las membranas celulares de ambas células se juntan en una unión estrecha que permite a las moléculas de señal llamadas neurotransmisores pasar rápidamente de una a otra célula por difusión. El canal de unión de la neurona postsináptica es de aproximadamente 20 nm de ancho, y se conoce como "hendidura sináptica".

Estas sinapsis son asimétricas tanto en su estructura como en su funcionamiento. Sólo la neurona presináptica segrega los neurotransmisores, que se unen a los receptores transmembrana que la célula postsináptica tiene en la hendidura. El terminal nervioso presináptico (también llamado "botón sináptico" o "botón") normalmente emerge del extremo de un axón, mientras que la zona postsináptica normalmente corresponde a una dendrita, al cuerpo celular o a otras zonas celulares. La zona de la sinapsis donde se libera el neurotransmisor se denomina "zona activa". En las zonas activas, las membranas de las dos células adyacentes están unidas estrechamente mediante proteínas de adhesión celular. Justo tras la membrana de la célula postsináptica aparece un complejo de proteínas entrelazadas denominado densidad postsináptica. Las proteínas de la densidad postsináptica cumplen numerosas funciones, que van desde el anclaje y movimiento de receptores de neurotransmisores de la membrana plasmática, hasta el anclaje de varias proteínas reguladoras de la actividad de estos receptores.

Una sinapsis eléctrica es aquella en la que la transmisión entre la primera neurona y la segunda no se produce por la secreción de un neurotransmisor, como en las sinapsis químicas (véase más abajo), sino por el paso de iones de una célula a otra a través de uniones gap, pequeños canales formados por el acoplamiento de complejos proteicos, basados en conexiones, en células estrechamente adheridas.

La sinapsis eléctrica es la más común en los vertebrados menos complejos y en algunos lugares del cerebro de los mamíferos. Las membranas celulares de las neuronas presináptica y postsináptica están íntimamente en contacto,a través de nexus las cuales cuentan con canales por lo que pasan los iones. Así el impulso nervioso se transmite directamente de una célula a otra. Son más rápidas que las sinapsis químicas pero menos plásticas; por lo demás, son menos propensas a alteraciones o modulación porque facilitan el intercambio entre los citoplasmas de iones y otras sustancias químicas. En los vertebrados son comunes en el corazón y el hígado.

Las sinapsis eléctricas tienen tres ventajas muy importantes:


La sinapsis química se establece entre células que están separadas entre sí por un espacio de unos 20-30 nanómetros (nm), la llamada hendidura sináptica.

La liberación de neurotransmisores es iniciada por la llegada de un impulso nervioso (o potencial de acción), y se produce mediante un proceso muy rápido de secreción celular: en el terminal nervioso presináptico, las vesículas que contienen los neurotransmisores permanecen ancladas y preparadas junto a la membrana sináptica. Cuando llega un potencial de acción se produce una entrada de iones calcio a través de los canales de calcio dependientes de voltaje. Los iones de calcio inician una cascada de reacciones que terminan haciendo que las membranas vesiculares se fusionen con la membrana presináptica y liberando su contenido a la hendidura sináptica. Los receptores del lado opuesto de la hendidura se unen a los neurotransmisores y fuerzan la apertura de los canales iónicos cercanos de la membrana postsináptica, haciendo que los iones fluyan hacia o desde el interior, cambiando el potencial de membrana local. El resultado es "excitatorio" en caso de flujos de despolarización, o "inhibitorio" en caso de flujos de hiperpolarización. El que una sinapsis sea excitatoria o inhibitoria depende del tipo o tipos de iones que se canalizan en los flujos postsinápticos, que a su vez es función del tipo de receptores y neurotransmisores que intervienen en la sinapsis.

La suma de los impulsos excitatorios e inhibitorios que llegan por todas las sinapsis que se relacionan con cada neurona (1000 a 200 000) determina si se produce o no la descarga del potencial de acción por el axón de esa neurona.

Se distinguen tres tipos principales de transmisión sináptica; los dos primeros mecanismos constituyen las fuerzas principales que rigen en los circuitos neuronales:

La fuerza de una sinapsis viene dada por el cambio del potencial de membrana que ocurre cuando se activan los receptores de neurotransmisores postsinápticos. Este cambio de voltaje se denomina potencial postsináptico, y es resultado directo de los flujos iónicos a través de los canales receptores postsinápticos. Los cambios en la fuerza sináptica pueden ser a corto plazo y sin cambios permanentes en las estructuras neuronales, con una duración de segundos o minutos, o de larga duración (potenciación a largo plazo o LTP), en que la activación continuada o repetida de la sinapsis implica que los segundos mensajeros inducen la síntesis proteica en el núcleo de la neurona, alterando la estructura de la propia neurona. El aprendizaje y la memoria podrían ser resultado de cambios a largo plazo en la fuerza sináptica, mediante un mecanismo de plasticidad sináptica.

Generalmente, si una sinapsis excitatoria es fuerte, un potencial de acción en la neurona presináptica iniciará otro potencial en la célula postsináptica. En una sinapsis débil, el potencial excitatorio postsináptico ("PEPS") no alcanzará el umbral para la iniciación del potencial de acción. En el cerebro, cada neurona mantiene conexiones o sinapsis con muchas otras, pudiendo recibir cada una de ellas múltiples señales. Cuando se disparan potenciales de acción simultáneamente en varias neuronas que se unen en sinapsis débiles a otra neurona, pueden forzar el inicio de un impulso en esa célula a pesar de que las sinapsis son débiles.

Por otro lado, una neurona presináptica que libera neurotransmisores inhibitorios, como el GABA, puede generar un potencial inhibitorio postsináptico ("PIPS") en la neurona postsináptica, bajando su sensibilidad y la probabilidad de que se genere un potencial de acción en ella. Así la respuesta de una neurona depende de las señales que recibe de otras, con las que puede tener distintos grados de influencia, dependiendo de la fuerza de la sinapsis con esa neurona. John Carew Eccles realizó algunos experimentos importantes en los inicios de la investigación sináptica, por los que recibió el en 1963. Las complejas relaciones de entrada/salida conforman las bases de la computación basada en transistores, y se cree que funcionan de forma similar en los circuitos neuronales.

Tras la fusión de las vesículas sinápticas y la liberación de las moléculas transmisoras en la hendidura sináptica, el neurotransmisor es rápidamente eliminado del espacio por proteínas especializadas en su reciclaje, situadas en las membranas tanto presináptica como postsináptica. Esta recaptación evita la desensibilización de los receptores postsinápticos y asegura que los potenciales de acción subsiguientes generen un PEP de la misma intensidad. La necesidad de una recaptación y el fenómeno de la desensibilización en los receptores y canales iónicos significa que la fuerza de la sinapsis puede disminuir si un tren de potenciales de acción llega en una sucesión rápida, un fenómeno que hace que exista una "dependencia de la frecuencia" en las sinapsis. El sistema nervioso se aprovecha de esta propiedad para computaciones, y puede ajustar las sinapsis mediante la fosforilación de las proteínas implicadas. El tamaño, número y tasa de reposición de las vesículas también está sujeto a regulación, así como otros muchos aspectos de la transmisión sináptica. Por ejemplo, un tipo de fármaco conocido como inhibidores selectivos de la recaptación de serotonina o SSRI afectan a ciertas sinapsis inhibiendo la recaptación del neurotransmisor serotonina. Por el contrario, un neurotransmisor excitatorio muy importante, la acetilcolina, no es recaptada, pero es eliminada por acción de la enzima acetilcolinesterasa.

La modificación de los parámetros sinápticos pueden modificar el comportamiento de los circuitos neurales y la interacción entre los diferentes módulos que componen el sistema nervioso (modal). Dichos cambios están englobados en un fenómeno conocido como neuroplasticidad o plasticidad neuronal.

Por analogía con las sinapsis descritas, el encuentro entre una célula antigénica y un linfocito se denomina a veces "sinapsis inmunitaria".

Trastorno degenerativo neuronal situado en la sustancia negra, estas se encargan de producir dopamina (neurotransmisor) fundamental para que el movimiento del cuerpo se realice correctamente. Cuando no se dispone de dopamina suficiente se presentan los síntomas que caracterizan esta enfermedad.

Crisis recurrentes de descargas entre impulsos inhibitorios y excitatorios. La inhibición recurrente puede ocurrir cuando una neurona principal hace sinapsis con una neurona inhibidora. El estado hiperexcitable resulta del incremento de la neurotransmisión excitadora sináptica.

Proceso degenerativo de las neuronas de la corteza cerebral que es irreversible hasta el momento.





</doc>
<doc id="2594" url="https://es.wikipedia.org/wiki?curid=2594" title="Sparganiaceae">
Sparganiaceae

Sparganium es el nombre de un taxón ubicado en la categoría taxonómica de género, que en sistemas de clasificación como el del APG II del 2003 es el único género de la familia Sparganiaceae, siendo en otros sistemas de clasificación como el de Judd "et al." (2007), el del APWeb, el de Kubitzki (1998) y el más moderno APG III (2009), un género de la familia Typhaceae "sensu lato", en este caso compartiendo la familia con su género hermano "Typha", por lo que se podría decir que en la actualidad la familia Sparganiaceae fue abandonada. El taxón está formado por hierbas perennes acuáticas emergentes rizomatosas, con hojas dísticas bifaciales (en su hábito muy parecidas a las totoras), con cabezas globosas unisexuales por inflorescencias (las cabezas masculinas arriba, las femeninas abajo), de numerosas flores diminutas polinizadas por viento. Distribuidas en todo el mundo.

Hierbas perennes, monoicas, emergentes acuáticas, de tallo rizomatoso.

Hojas bifaciales, dísticas, envainadoras, simples, sin dividir, planas, elongadas y delgadas, de venación paralela.

Inflorescencia compuesta, de cabezas globosas bracteadas y unisexuales, las cabezas masculinas arriba, las femeninas debajo.

Flores pequeñas, unisexuales, actinomórficas, sésiles, las flores femeninas hipóginas.

El perianto es aparentemente bracteado en las flores femeninas, los tépalos como escamas son 1-6 en flores masculinas, 3-4 (raramente 2-5) en las femeninas. 

Los estambres son 1-8, antitépalos (dispuestos opuestos a los tépalos), separados o conados basalmente.

El gineceo es de 1 solo carpelo o de 2-3 carpelos conados (a su vez con 1 o 2-3 lóculos), con un ovario súpero. La placentación es apical, los óvulos son anátropos, bitégmicos, 1 por carpelo.

No hay nectarios.

El fruto es seco y como una drupa con un perianto persistente y el estilo también persistente.

Las semillas son endospermadas.

Distribuidos principalmente en regiones templadas a frías del Hemisferio Norte.

Las diminutas flores de "Sparganium" son polinizadas por viento.

La familia no fue reconocida por el APG III (2009), que asigna el único género a la familia Typhaceae "sensu lato".. La familia sí había sido reconocida por el APG II (2003) que la separaba de Typhaceae "sensu stricto".

El nombre científico del género es "Sparganium" L., Sp. Pl.: 971 (1753).

La lista de especies, conjuntamente con su publicación válida y su distribución, según el Royal Botanic Gardens, Kew (visitado en enero de 2009):


No poseen importancia económica significativa.



</doc>
<doc id="2595" url="https://es.wikipedia.org/wiki?curid=2595" title="Sistema Cegesimal de Unidades">
Sistema Cegesimal de Unidades

El Sistema Cegesimal de Unidades, también llamado sistema CGS, es un sistema de unidades basado en el centímetro, el gramo y el segundo. Su nombre es el acrónimo de estas tres unidades. 

Fue propuesto por Gauss en 1832, e implantado por la British Association for the Advancement of Science (BAAS, ahora BA) en 1874 incluyendo las reglas de formación de un sistema formado por unidades básicas y unidades derivadas.

El sistema CGS ha sido casi totalmente reemplazado por el Sistema Internacional de Unidades (SI). Sin embargo aún perdura su utilización en algunos campos científicos y técnicos muy concretos, con resultados ventajosos en algunos contextos. Así, muchas de las fórmulas del electromagnetismo presentan una forma más sencilla cuando se las expresa en unidades CGS, resultando más simple la expansión de los términos en "v"/"c".

La Oficina Internacional de Pesos y Medidas, reguladora del SI, valora y reconoce estos hechos e incluye en sus boletines referencias y equivalencias de algunas unidades electromagnéticas del sistema CGS gaussiano, aunque desaconseja su uso.

A diferencia del SI, el sistema CGS no determina si debe haber una dimensión adicional para las magnitudes electromagnéticas (en el SI es la corriente). De ahí que haya varios sistemas cegesimales en función de como se tratan las constantes formula_1 y formula_2. Las ecuaciones se ajustan según el sistema concreto adoptado, aunque en la práctica apenas se usa más que el de Gauss, donde ambas constantes se toman como 1 y a cambio aparece explícitamente "c". Las dimensiones, así, pueden tener exponentes semienteros.

En el SI la corriente eléctrica se define mediante la intensidad de campo magnético que presenta, y la carga eléctrica se define como corriente eléctrica por unidad de tiempo. En una variedad del CGS, el ues o unidades electrostáticas, la carga se define como la fuerza que ejerce sobre otras cargas, y la corriente se define como carga por unidad de tiempo. Una consecuencia de este método es que la ley de Coulomb no contiene una constante de proporcionalidad.

Por último, al relacionar los fenómenos electromagnéticos al tiempo, la longitud y la masa, dependen de las fuerzas observadas en las cargas. Hay dos leyes fundamentales en acción: la ley de Coulomb, que describe la fuerza electrostática entre "cargas", y la ley de Ampère (también conocida como la ley de Biot-Savart), que describe la fuerza electrodinámica (o electromagnética) entre "corrientes".

Cada una de ellas contiene las constantes de formula_3 y formula_4. La definición estática de campo magnético tiene otra constante, formula_5. Las dos primeras constantes se relacionan entre sí a través de la velocidad de la luz, formula_6 (la razón entre formula_3 y formula_4 debe ser igual a formula_9).

De este modo se tienen varias opciones:

Una característica del sistema CGS gaussiano es que el campo eléctrico y el campo magnético tienen las mismas unidades. Existe aproximadamente media docena de sistemas de unidades electromagnéticas en uso, la mayoría basados en el sistema CGS. Estos incluyen el UEM o unidades electromagnéticas (escogidas de tal manera que la ley de Biot-Savart no tenga constante de proporcionalidad), Gausiano y unidades Heaviside-Lorentz. Para complicar más el asunto, algunos físicos e ingenieros utilizan para el campo eléctrico unidades híbridas, como voltios por centímetro.

En el antiguo sistema de unidades electromagnéticas basado en el CGS que se usó para estudiar la inducción magnética la unidad de corriente no es el estatamperio sino el abamperio = 10 amperio, lo que permite llegar a definir el gauss como unidad de densidad de flujo magnético.
En la tabla siguiente se encontrará el estatamperio y el gauss como pertenecientes al moderno sistema CGS. Esto es inexacto. El gauss no es una magnitud CGS sino electromagnética.

Los coeficientes 2998, 3336, 1113 y 8988 se derivan de la velocidad de la luz; exactamente valen 299792458, 333564095198152, 1112650056 y 89875517873681764.

Un "«centímetro»" de capacidad es la capacitancia de una esfera conductora, de 1 cm de radio, en el vacío.





</doc>
<doc id="2596" url="https://es.wikipedia.org/wiki?curid=2596" title="Spermatophyta">
Spermatophyta

Las espermatofitas o fanerógamas (Spermatophyta) son un grupo monofilético del reino de las plantas (Plantae) que comprende a todos los linajes de plantas vasculares que producen semillas. 

El nombre científico proviene del griego σπέρμα ("sperma", que significa "semilla"), y φυτόν ("fiton", que significa "planta"), que se traduce como «plantas con semilla». La circunscripción del grupo (es decir, los taxones de los que está compuesto) coincide exactamente con la del antiguo taxón Phanerogamae, que por lo tanto es sinónimo de esta división. Debido a que en las espermatofitas el grano de polen produce un tubo (haustorial o polínico) para llegar al óvulo y que ocurra la fecundación, este grupo también es llamado de las embriofitas sifonógamas (del griego: "embrios": embrión; "fiton": planta; "xifos": tubo; "gamos": unión sexual. Literalmente, "plantas con embrión cuya unión sexual ocurre con tubo"). A veces la jerga científica se refiere a este grupo como "embriofitas", dejando fuera a las embriofitas asifonógamas o de los briófitos y los helechos y afines.

Hace mucho tiempo que los científicos consensúan la monofilia de las espermatofitas. Entre las evidencias morfológicas de la monofilia de las espermatofitas está, por supuesto, la semilla misma, y también la producción de madera (o "xilema secundario" generado en el meristema secundario llamado "cámbium"), al menos en forma ancestral. Otra característica notable es la ramificación axilar, en comparación con la ramificación dicotómica anisotónica de sus ancestros eufilofitos.

Las espermatofitas se originaron a fines del Devónico, a partir de lignofitas, que ya tenían producción de madera y ramificación axilar, como puede observarse en el registro fósil. 

Hoy en día las espermatofitas son, por mucho, el linaje más extenso de plantas vasculares, con unas 270.000 especies vivientes (Judd "et al." 2002). Un solo subclado es el mayor responsable de esa diversidad: las angiospermas, o plantas con flores periantadas. Otros subclados, normalmente agrupados como gimnospermas, son las cícadas, los ginkgos, las coníferas y los gnetales. Estos cuatro grupos comparten un ancestro común. También se llaman "gimnospermas" a algunos fósiles de espermatofitas no productores de flores periantadas, que no comparten el mismo ancestro que las gimnospermas vivientes, por lo que algunos autores diferenciaban "Gymnospermae "sensu lato"" (pteridospermas + gimnospermas vivientes), que sería parafilético con respecto a las angiospermas y a "Gymnospermae "sensu stricto"", monofiléticos, comprendidos por los linajes vivientes.

Este grupo ha sido denominado Phanerogama (Bartling 1830), Phanerogamae (Brongniart 1843, Eichler 1883), Spermatophyta (Willkomm 1854, Goebel 1882, Britton & A. Brown 1896), Anthophyta (Braun in Ascherson 1864, Wettstein 1924-1935), Siphonogamae (Engler 1886), Embryophyta siphonogama, (Engler 1892-1924), división Magnoliophyta (Takhtajan 1964) y Spermatophytina (Cavalier-Smith 1998, Ruggiero et al. 2015).

Las espermatofitas pueden definirse como traqueofitas con las siguientes características:


Como se puede observar de sus hermanos vivientes los helechos, las espermatofitas descienden de un ancestro caracterizado por la homosporía (un solo tipo de espora, gametofitos siempre bisexuales). Un paso crítico en el desarrollo de la semilla fue la evolución de la heterosporía: la producción de dos tipos de esporas, las megasporas y las microsporas, que darán los gametofitos femeninos y masculinos respectivamente.

La heterosporía se originó muchas veces en forma independiente en linajes no emparentados de plantas vasculares (hay ejemplos en lycophytas, en equisetopsidas y en polypodiopsidas), en muchos de esos casos el desarrollo de la heterosporía fue seguido de una reducción del número de megasporas funcionales. En la línea que condujo a las plantas con semilla, en el esporofito adulto que desarrolla sus megasporangios, se produce la meiosis de sólo una célula por megasporangio, y por aborto de 3 de los productos de la meiosis, el número de megasporas funcionales es reducido a sólo un megaspora funcional por megasporangio. Esa única megaspora funcional, en el linaje de las espermatofitas fue retenida dentro del megasporangio, desarrollando su gametofito femenino y gameta femenina por completo dentro del megasporangio de la generación esporofítica anterior. Finalmente el megasporangio desarrolló los tegumentos dejando abierto el pequeño orificio llamado micrópila.

Nuestro conocimiento de los orígenes de la semilla se basa principalmente en fósiles bien preservados del Devónico tardío y Carbonífero temprano, que fueron llamados "progimnospermas" o "helechos con semilla". Recordemos que la diferenciación entre un tallo principal y ramas laterales ya había evolucionado en el linaje de las eufilofitas. Lo primero que aparece en el linaje que derivó en las espermatofitas, fue encontrado en el Devónico tardío, y es la aparición de troncos muy grandes, con madera bastante similar en su estructura a la de las modernas coníferas. Estos troncos estaban conectados a enormes sistemas de ramas que portaban muchas hojas pequeñas. "Archaeopteris", como ahora es llamada, fue descubierta como heterospórica, pero aún sin formar semillas.

La reconstrucción más exacta y la ubicación en el árbol filogenético de "Archaeopteris" y otras "progimnospermas" como "Aneurophyton" (Beck 1981, 1988, Beck y Wight 1988), fue fundamental para que los científicos puedan establecer tanto el origen de la heterosporía como el de la producción de madera, y llegar a la conclusión de que los dos eran anteriores a la evolución de la semilla. Por lo tanto es incorrecto afirmar que esos dos fósiles pertenecen a un clado que se llame espermatofitas ("plantas con semilla"), si bien son ancestros de las plantas con semilla actuales. Por lo tanto las espermatofitas y esos dos fósiles pertenecen a un grupo monofilético mayor, que fue llamado "Lignophyta" (Doyle y Donogue 1990), en referencia a que ya producían madera.

Análisis cuidadosos (por ejemplo el de Serbet y Rothwell 1992) han revelado que las primeras semillas estaban situadas en "cúpulas" ("cupules" en inglés) y cada semilla era cubierta por una excrecencia de la pared del esporangio que formaba una cámara especializada para recibir el polen, o "cámara polínica". Esta estructura probablemente se ayudaba de una secreción de una gota pegajosa ("gota de polinización") para capturar los granos de polen. 

Los tejidos del tegumento externo probablemente derivan de una serie de esporangios estériles, que inicialmente tuvieron la forma de una serie de lóbulos en el ápice de la semilla, más que la forma de una micrópila diferenciada (ver gráfico).

Se ha demostrado -en contra de lo esperado- que el grupo de las gimnospermas vivientes es un grupo monofilético, hermano de las angiospermas. 

Los análisis multigenéticos moleculares han demostrado que Spermatophyta se divide en dos grandes grupos: Gymnospermae y Angiospermae, los cuales fueron definidos por John Ray en 1703, y taxonómicamente esta clasificación se ha mantenido estable desde 1854. Así pues, la filogenia más actualizada (2014) de las plantas vivientes con semilla presenta los siguientes subgrupos:
Las diferencias entre los dos grupos están resumidas en el siguiente cuadro.

Una visión general de los grupos extintos relacionados con Spermatophyta sería la siguiente:

Durante una gran parte del último siglo, los linajes de plantas con semilla, tanto vivientes como extintos, fueron comúnmente divididos en dos grandes grupos: las cycadofitas y las coniferofitas. Las cycadofitas, entre las que se incluyen las modernas cycadáceas, se distinguían por una producción de madera (crecimiento secundario del tallo) más bien limitada, con radios más bien anchos (lo que se llama "leño manoxílico"), y por hojas grandes, de tipo fronde de helecho, y semillas con simetría radial. En contraste, en las coniferofitas, entre las que se incluyen el "Ginkgo" y las coníferas, la madera está bien desarrollada y es densa ("leño picnoxílico"), las hojas son simples y muchas veces en forma de aguja, y las semillas tienen simetría dorsiventral (están "aplastadas"). Esta distinción sugirió a algunos investigadores que las plantas con semilla en realidad se originaron dos veces. Desde este punto de vista, la línea de las cycadofitas se derivó de un ancestro de tipo Progymnosperma, en el cual el sistema de ramificación lateral aplanado derivó en grandes hojas de tipo fronde de helecho. En cambio la línea de las espermatofitas habría derivado de un ancestro del tipo "Archaeopteris", en el que las hojas individuales pueden haber sido modificadas en hojas de tipo aguja. Este escenario implica que la semilla misma fue originada dos veces, cada una correspondiéndose a un tipo de simetría diferente.

Sin embargo, en los análisis de filogenia que incluyeron los linajes vivientes junto con los representantes fósiles, en general aprueban el árbol filogenético que se muestra en la figura siguiente (ver por ejemplo Crane 1985, Doyle y Donoghue 1986, Nixon "et al." 1994, Rothwell y Serbet 1994).

Estos estudios asumen que la semilla apareció una sola vez, y que las primeras plantas con semilla eran más bien parecidas a las cícadas, al menos en lo que respecta a las hojas grandes y pinadas, y las semillas con simetría radial. Específicamente, parece que una serie de "helechos con semilla" del Devónico-Carbonífero ("Lygniopteris" y medulosas) están situados en la base de la filogenia de las plantas con semilla, y que las coniferofitas están anidadas varios niveles hacia adentro del árbol, en un clado "platyspérmico" (de semillas con simetría dorsiventral). Este árbol hipotetiza que el cambio a hojas de tipo aguja y el cambio a semillas de simetría dorsiventral fueron posteriores a la aparición de las hojas y las semillas, y probablemente fueron una adaptación a ambientes de tipo árido.

A pesar de los enormes esfuerzos hechos hasta ahora para dilucidar las relaciones filogenéticas de los 5 grupos vivientes de espermatofitas (cícadas, ginkgos, coníferas, gnetofitas y angiospermas), utilizando tanto información morfológica como molecular, las relaciones aún no reciben consenso, y son motivo de variadas discusiones en el ambiente científico. En ese sentido cabe aclarar que está definitivamente descartado que las angiospermas deriven de una gnetofita ancestral. Los estudios morfológicos detallados muestran un origen diferente de los vasos xilemáticos en gnetofitas y en angiospermas (esto quiere decir que se originaron dos veces), y las estructuras de la flor de las gnetáceas (aquí definimos "flor" como "rama de crecimiento definido portadora de hojas fértiles") no son homólogas a las de la flor de angiosperma. Además, en el año 2004 se han encontrado fósiles de Gnetofitas que confirman (de forma morfológica) su pertenencia al grupo de las gimnospermas, y descarta nuevamente su relación con las angiospermas. 

Hay que tener en cuenta que también se agrupaba dentro de las "gimnospermas" a muchos fósiles de plantas con semilla extintas (las pteridospermas) que en conjunto no forman un grupo monofilético: de hecho el grupo llamado Gymnosperma "sensu lato" se vuelve parafilético cuando uno toma en consideración los linajes de espermatofitas basales, así como otros "helechos con semilla" del Pérmico tardío y el Mesozoico, algunos de los cuales están probablemente en el linaje que derivó en las angiospermas.

Los diferentes sistemas de clasificación le dan diferente importancia relativa a las estructuras de las espermatofitas:

Según Engler 1924 se subdividen en:

Se dividen según Cronquist en:





Hoy en día hay 2 linajes vivientes de espermatofitas: las gimnospermas y las plantas con flores. Gymnospermae es referencia a que poseen las semillas "desnudas" o no totalmente cubiertas por el carpelo, en oposición a las angiospermas o plantas con flores, cuyo carpelo cubre completamente a la semilla.

Cícadas: fueron las más abundantes y diversas durante el Mesozoico. Hoy quedan alrededor de 130 especies. Las cícadas generalmente poseen un tronco bajo y ancho, con xilema secundario limitado, y hojas compuestas grandes parecidas a las de los helechos o las palmeras. Son dioicos, eso quiere decir que algunos esporofitos sólo portan óvulos y luego semillas, y otros esporofitos sólo portan estróbilos masculinos productores de polen. Los dos tipos de estróbilo son típicamente muy grandes, y en algunos casos de coloración brillante. Asimismo las semillas suelen ser grandes y usualmente tienen un tegumento externo carnoso y coloreado, presumiblemente una adaptación para atraer a los agentes de dispersión vertebrados. Muchas características de las cícadas pueden ser ancestrales, como el polen con tubo haustorial (en lugar de tubo polínico), y el esperma gigante multiflagelado (en lugar de los núcleos espermáticos). Sin embargo, las cícadas poseen características únicas que las alejan de las plantas con semilla ancestrales, que presumiblemente son caracteres derivados, entre los cuales se incluye la pérdida de la ramificación axilar, la presencia de trazas foliares "girdling" ("¿con fajas?"), y la producción de raíces coralloides que albergan cianobacterias fijadoras de nitrógeno. Dentro de las cícadas, los análisis filogenéticos indican que la primera división del grupo fue la que dividió al linaje de "Cycas" del resto. Por lo tanto "Cycas" estaría reteniendo algunos caracteres presumiblemente ancestrales, como los que han sido encontrados en parientes fósiles como "Taeniopteris", a saber: el tener muchos óvulos nacidos en los márgenes de carpelos (carpelos definidos como hojas fértiles portadoras de óvulos) con mofología de tipo foliar, en lugar de tener dos óvulos por carpelo peltado que los sostiene apuntando hacia el eje del esporofilo (en su "cara adaxial"), que es el carácter que se encuentra en la otra línea. También en "Cycas" las hojas fértiles portadoras de óvulos no se encuentran agrupadas en estróbilos, como sí lo están en la otra línea.

Ginkgos: solo hay una especie sobreviviente de ginkgos: "Ginkgo biloba". Esta especie es muy raramente encontrada en forma silvestre, pero los árboles presentes en los templos de China fueron mantenidos por siglos por los monjes que los habitan, y en los tiempos modernos fue cultivado por el hombre en las veredas de las ciudades. Quizás la característica más distintiva del "Ginkgo" moderno es la producción de hojas deciduas, con forma de abanico ("flabeladas"), con venación dicotómica. Los ginkgos son bien conocidos en el registro fósil, donde se observa una gran diversidad en la morfología de las hojas. Como las cícadas, los ginkgos son dioicos (esporofitos diferentes portan o bien carpelos o bien estambres -estambres definidos como "hojas fértiles portadoras de sacos polínicos que contienen a los granos de polen"-). Los óvulos nacen en pares sobre ramas axilares que se piensa que son estróbilos reducidos. El tegumento del óvulo se diferencia en una capa externa carnosa (y olorosa) y una capa interna pétrea (dura) que encierra al gametofito femenino. También al igual que en las cícadas, los ginkgos retienen varios caracteres de las espermatofitas ancestrales, como el polen que emite un tubo haustorial (no polínico), y el esperma flagelado capaz de nadar.

Coníferas: hay aproximadamente unas 600 especies de coníferas vivientes. Son árboles o arbustos con madera bien desarrollada y usualmente hojas de tipo aguja. Normalmente las hojas son solitarias, naciendo a lo largo del tallo, pero en los pinos ("Pinus") están agrupadas en ramitas pequeñas. Las hojas usualmente tienen adaptaciones adicionales a la sequedad, por ejemplo estomas hundidos. Sin embargo algunas coníferas del Hemisferio Sur (por ejemplo "Podocarpus", "Agathis") presentan hojas aplanadas y grandes, y en "Phyllocladus" se observan ramas aplanadas que parecen hojas. Muchas coníferas son monoicas (poseen carpelos y estambres en el mismo esporofito), pero algunos grupos son dioicos: "Juniperus", "Taxus", y "Podocarpus". En los estróbilos masculinos (que en las coníferas se llaman conos masculinos o "pollen cones" en inglés), los estambres (o "microesporofilos") sostienen en su cara abaxial a los esporangios ("microsporangios") que darán gametofitos masculinos ("microgametofitos"). Los granos de polen son los gametofitos masculinos protegidos por una pared originada en el esporofito, y a veces tienen un par de apéndices llenos de aire parecidos a sacos, presumiblemente adaptaciones a la dispersión por el viento, pero estos "sacos aéreos" parecen haberse perdido en muchas líneas. Los óvulos receptivos, a diferencia de los microsporangios, están situados en la cara adaxial de cada carpelo o "escama ovulífera", mirando hacia el eje del cono femenino. La meiosis que dará las gametas, ocurre dentro de cada óvulo, y uno solo de los 4 productos de la meiosis se desarrollará hasta dar el gametofito femenino, siempre dentro del óvulo. El gametofito femenino o "protalo", produce uno o más gametos femeninos o "huevos" en el sector cercano a la micropila. Cuando el grano de polen llega finalmente a la micropila, el gametofito masculino encerrado en él desarrolla un tubo ("tubo polínico") que atraviesa la pared del gametofito femenino. Cuando el tubo polínico termina de crecer, el gametofito masculino emite a través de él dos espermas, que pueden ser células o núcleos celulares ("núcleos espermáticos") según el linaje. Es muy común en coníferas el fenómeno de "poliembronía", con muchos embriones desarrollándose en el mismo gametofito femenino, que puede ser o bien debido a que ocurrieron eventos de fertilización independientes en los que varios huevos fueron fertilizados por varios tubos polínicos distintos, o bien debido a que el embrión único se dividió en una etapa temprana en varios embriones genéticamente idénticos, siendo la última posibilidad más comúnmente encontrada que la primera. En las modernas coníferas, se dice que el estróbilo portador de polen es "simple", mientras que el portador de óvulos es "compuesto". Esto es debido a cómo es interpretada la morfología de los estróbilos: El cono masculino es interpretado como una rama modificada portadora de hojas fértiles o estambres, por lo tanto sería una "flor" única portadora de muchos estambres. En cambio el cono femenino es interpretado como derivado de una rama con hojas, que a su vez porta ramas laterales de crecimiento definido nacidas en la axila de las hojas, cada una de las ramas laterales portadoras de hojas fértiles o carpelos. Esta interpretación es sostenida por el registro fósil, que muestra una serie de pasos en la reducción de las ramas laterales portadoras de carpelos, hasta la aparición de la "escama ovulífera" altamente modificada que vemos en los grupos modernos (Florin 1951, 1954). También se observa que cada escama ovulífera está sostenida por una bráctea ("bráctea tectriz"), que representaría la hoja portadora de la rama lateral, también altamente modificada. En unas pocas coníferas, la bráctea es conspicua, emergiendo de entre las escamas ovulíferas (por ejemplo en "Pseudotsuga mensiezii"). Sin embargo en muchas coníferas la bráctea es sumamente reducida. En las cupresáceas "Taxodium" y "Cryptomeria", la bráctea está fusionada a la escama ovulífera, y la escama ovulífera aún muestra signos de presentar "hojas" (visibles como pequeños dientes). Los estudios filogenéticos han revelado algunas cuestiones interesantes acerca de la evolución de las coníferas. (por ejemplo Stefanovic "et al." 1998). Los datos moleculares muestran una división basal entre las pináceas y un clado que albergaría a todas las demás coníferas. Las pináceas poseen varias características singulares, como óvulos invertidos (con la micropila mirando hacia el eje del cono) y las semillas aladas, alas que se originan en la escama ovulífera durante el desarrollo de la semilla. Dentro del otro clado de las coníferas, los dos grupos más grandes del Hemisferio Sur (Podocarpaceae y Araucariaceae) forman un clado, presumiblemente con la sinapomorfía de poseer un solo óvulo por escama ovulífera. Las cupresáceas están marcadas por muchas características singulares, como la fusión de la escama ovulífera con la báctea tectriz. A su vez, este grupo puede estar emparentado con las taxáceas, que tienen conos femeninos altamente reducidos con una sola semilla terminal, rodeada de un tercer tegumento ("arilo") carnoso y colorido.
Gnetales: este grupo contiene sólo unas 80 especies vivientes, que pertenecen a tres linajes bastante diferenciados (Doyle 1996, Friedman 1996, Price 1996). Uno es "Ephedra", con alrededor de 40 especies distribuidas en los desiertos de todo el mundo, con hojas muy reducidas escamosas. Otro es "Gnetum", con unas 35 especies en bosques tropicales del Viejo y Nuevo Mundo, con hojas con lámina entera muy parecidas a las vistas en la mayoría de las angiospermas. Finalmente, "Welwitschia" con una sola especie, "Welwitschia mirabilis" encontrada en el sudoeste de África, produce sólo dos o raramente cuatro hojas funcionales a lo largo de toda su vida, hojas que crecen indefinidamente por meristemas presentes en la base, y se necrosan gradualmente en las puntas.

Si bien estos tres clados se ven muy diferentes uno del otro, comparten muchas características inusuales, como las hojas opuestas, múltiples yemas por axila, vasos xilemáticos con aberturas circulares entre células adjuntas, polen compuesto, semillas en estróbilo, y un polen elipsoide ancestral con unas estrías características que corren de punta a punta. Las semillas también tienen dos tegumentos, el interno formando el tubo micropilar que exuda la gota de polinización, el externo derivado de un par de brácteas fusionadas. Los estudios moleculares también consensúan altamente la monofilia de este grupo. Dentro de las gnetofitas, "Gnetum" y "Welwitschia" forman un clado bien consensuado. Algunas de las sinapomorfías morfológicas son: hojas con venación reticulada, reducción aún mayor del gametofito masculino, y algunos aspectos de la estructura del gametofito femenino, como el desarrollo tetraspórico, la pérdida de los arquegonios, los núcleos libres funcionando como huevos en lugar de las células. El característico polen estriado encontrado en "Ephedra" y "Welwitschia" fue aparentemente perdido en la línea de la que derivó "Gnetum", que tiene un polen con granitos con forma de pico, no aperturado. En lo que respecta al registro fósil, es más bien pobre salvo en los granos de polen. Sólo algunos macrofósiles han sido descriptos (Crane 1996). Si bien los granos de polen de las gnetofitas son encontrados desde el Triásico, parece ser que el clado que contiene a los grupos modernos se ha diversificado más significativamente durante el Cretácico medio, al mismo tiempo que las angiospermas. Al igual que las angiospermas, las gnetofitas acortaron su ciclo de vida (y probablemente se volvieron herbáceas) y evolucionaron junto con los insectos para ser polinizadas por ellos, característica aún encontrada en algunos grupos vivientes. En marcado contraste con las angiospermas en cambio, las gnetofitas nunca se volvieron un componente significativo de la flora en paleolatitudes altas y medias, y han sufrido una disminución dramática de su representatividad durante el Cretácico tardío (Crane "et al." 1995, Crane 1996). 

Con unas 257.000 especies vivientes, las angiospermas son las responsables de la mayor parte de la diversidad en espermatofitas, en embriofitas y en viridofitas. La fuerte evidencia de la monofilia de las angiospermas proviene de los estudios moleculares y de los muchos caracteres morfológicos compartidos por los miembros de este clado. De éstos, algunos de los más obvios, que también son características reproductivas importantes, son: (1) las semillas son producidas dentro de un carpelo con una superficie estigmática que permite la germinación del polen, (2) el gametofito femenino es muy reducido, en la mayoría de las especies son sólo 8 núcleos en 7 células, y (3) la doble fertilización, que llevó a la formación de un tejido nutritivo característico, triploide, llamado endosperma. Otras características son: (4) muchas angiospermas poseen vasos xilemáticos en lugar de traqueidas, carácter derivado dentro del grupo, en los vasos el agua puede fluir sin necesidad de atravesar una membrana, lo que los vuelve muy eficientes en el transporte de fluidos dentro del esporofito pero probablemente también más propensos a recibir daño (en especial por embolias de aire) cuando están sujetos a estrés hídrico. (5) El floema de las angiospermas difiere del de todas las demás plantas en que los elementos del tubo criboso (que son células vivas pero sin núcleo, encargadas del transporte de azúcares) están acompañadas por una o más "células acompañantes", que nacen de la misma célula madre que el elemento criboso.



</doc>
<doc id="2597" url="https://es.wikipedia.org/wiki?curid=2597" title="Siglo VI">
Siglo VI

El siglo VI d. C. (siglo sexto después de Cristo) o siglo VI EC (siglo sexto de la era común) comenzó el 1 de enero del año 501 y terminó el 31 de diciembre del 600. Unos años después del fin de la época clásica (derrumbe del Imperio romano occidental en el año 476) y el inicio de la época medieval. Es llamado el «Siglo de Bizancio».

Después de la caída del Imperio romano occidental a finales del siglo anterior, Europa es fracturada en muchos reinos germánicos pequeños, que compitieron constantemente por tierra y recursos. Finalmente los francos llegaron a ser dominantes, y se expandieron hacia fuera un dominio importante que abarcaba gran parte de Francia y de Alemania.
Mientras tanto, el Imperio romano del este que sobrevivió comenzó a ampliarse bajo el mando del emperador Justiniano I, que recobró eventualmente África del norte de los vándalos, y procuró recuperar completamente Italia también con la esperanza de restablecer el control romano sobre las tierras gobernadas una vez por el Imperio romano occidental. Después de la muerte de Justiniano I, la mayor parte de sus logros desaparecieron. El Imperio sasánida alcanzó un pico de grandeza con Cosroes I en el 6.º siglo.







</doc>
<doc id="2599" url="https://es.wikipedia.org/wiki?curid=2599" title="Sífilis">
Sífilis

La sífilis es una enfermedad infecciosa de curso crónico, transmitida principalmente por contacto sexual, producida por la espiroqueta "Treponema pallidum", subespecie "pallidum" (pronunciado "pál lidum"). Sus manifestaciones clínicas son de características e intensidad fluctuantes, apareciendo y desapareciendo en las distintas etapas de la enfermedad: úlceras en los órganos sexuales y manchas rojas en el cuerpo. Produce lesiones en el sistema nervioso y en el aparato circulatorio. Existe en todo el mundo y se ha descrito desde hace siglos.

El nombre «sífilis» fue creado por el poeta y cirujano veronés Girolamo Fracastoro en su poema en latino "Syphilis sive morbus gallicus" (‘sífilis o la enfermedad francesa’) en 1530. El protagonista de la obra es un pastor llamado Sífilus (quizá una variante de Síphylus, un personaje de "Las metamorfosis" de Ovidio), que cuidaba de los rebaños del rey Alcihtous. Molesto con el dios griego Apolo, ya que éste quemaba los árboles y consumía los brotes que alimentaban a las ovejas, decidió no adorarlo a él sino al rey. En represalia, Apolo lo castigó junto con todo el reino, afectándolos de una enfermedad horrible, que llamó «sífilis» por el pastor. Agregándole el sufijo "-is" a la raíz Syphilus, Fracastoro creó el nuevo nombre de la enfermedad, y lo incluyó en su libro de medicina "De contagionibus" (‘Sobre las enfermedades contagiosas’, Venecia, 1584).

En este texto, Fracastoro registra que en la época, en Italia y Alemania la sífilis se conocía como el «morbo francés», y en Francia, como «el morbo italiano».

La sífilis también ha sido conocida como avariosis, búa, buba (o bubas), gálico, lúes venérea, o mal de bubas.

Las distintas denominaciones utilizadas entre los siglos XV y XVII dan idea de la vasta extensión de la enfermedad, y de la costumbre de culpar de ella a los países vecinos.


El origen y antigüedad de la sífilis representan una de las controversias no resueltas más importantes en la historia de la medicina. Las preguntas fundamentales de esta controversia son: ¿Llegó la sífilis al Viejo Mundo desde el Nuevo Mundo a través de la tripulación de Cristóbal Colón —como parece indicarlo que la primera epidemia de esta enfermedad en Europa fuese registrada en 1493—? o bien, ¿se originó la sífilis en el Viejo Mundo y permaneció como una enfermedad no identificada hasta que a finales del siglo XV se hizo notoria por una mayor virulencia o transmisibilidad? En relación con esa controversia se han elaborado dos hipótesis del origen de la sífilis, que generan debate en el campo de la antropología y la historiografía.

La hipótesis precolombina sostiene que las treponematosis, incluida la sífilis, son un conjunto de variantes de una enfermedad que se fue extendiendo tanto en el Viejo como en el Nuevo Mundo. En Europa sus manifestaciones se habrían confundido con la lepra. De acuerdo con esta hipótesis, la pinta apareció en África y Asia alrededor del 15000 a. C., con un reservorio natural animal. El pian se habría desarrollado como consecuencia de mutaciones de la pinta alrededor del X milenio a. C. extendiéndose por todo el mundo excepto en América que se encontraba aislada. La sífilis endémica emergió del pian alrededor del VII milenio a. C. como consecuencia de los cambios climáticos (aparición de clima árido). Alrededor del Siglo XXX a. C. la sífilis transmitida sexualmente apareció en el sudoeste asiático debido a las bajas temperaturas de la época postglacial, y de ahí se extendió a Europa y el resto del mundo. Desde entonces ha sufrido diversas mutaciones y manifestaciones clínicas, siendo notoria la forma clínica, «venérea», predominante en el siglo XV, probablemente acentuada por la reincorporación de cepas desde América.

La epidemiología de la primera presentación de sífilis de fines del siglo XV no define si la enfermedad era nueva o si provenía de una enfermedad anterior.

Las lesiones en esqueletos de la edad neolítica se deben a la sífilis. Incluso en esqueletos del 2000 a. C. en Rusia, con lesiones óseas patognomónicas. Aunque tales lesiones se pueden confundir con lesiones lepromatosas.
Quizá Hipócrates haya descrito los síntomas de la sífilis en su etapa terciaria.

También en las ruinas de Pompeya (que fue enterrada en el año 79 por el volcán Vesubio) se han encontrado esqueletos con signos que podrían ser de sífilis congénita.

De acuerdo con un trabajo científico de la Universidad de Bradford (Reino Unido) hecho público en junio de 1999, en un cementerio de una abadía agustiniana en el puerto de Kingston upon Hull (noreste de Inglaterra) usado entre 1119 y 1539, se encontraron 245 esqueletos, de los cuales tres tenían signos claros de sífilis. La datación con C14 indicó que el varón con las señales más evidentes de sífilis había fallecido entre 1300 y 1450.

Algunos científicos piensan que la sífilis pudo ser introducida en América tras los contactos entre vikingos.

En octubre de 2010, una excavación de esqueletos llevada a cabo en Gran Bretaña supuso un nuevo sustento para esta hipótesis, por cuanto los exámenes de los expertos indicaron que la enfermedad era conocida en este país dos siglos antes del viaje de Cristóbal Colón.

Esta hipótesis, que algunos consideran variante de la hipótesis precolombina, sostiene que todas las treponematosis corresponden a una sola enfermedad original, desarrollada muy antiguamente, quizás en el Paleolítico superior en el África subsahariana, y que desde ahí y desde entonces se extendió globalmente siendo sus variaciones consecuencia de las diferencias geográficas y climáticas. En otras palabras, la pinta, el pian, la sífilis y otras treponematosis son respuestas adaptativas del "T. pallidum" a diferencias ambientales. Existe evidencia de la existencia de treponematosis prácticamente en todos los continentes en la época precolombina. En América, las manifestaciones de la treponematosis en la época precolombina eran la sífilis venérea, en clima templado (América del Sur), y el pian, en clima tropical (Caribe). Esta hipótesis indica que el pian pudo haberse extendido desde África Occidental hacia la península ibérica en relación con el comercio de esclavos africanos negros, 50 años antes del viaje de Cristóbal Colón. El pian, endémico en África en ese momento, se manifestó en Europa de diversas formas, siendo una de ellas la sífilis venérea, es decir, de transmisión sexual.

Esta hipótesis sostiene que la sífilis era una infección de transmisión sexual (ITS) del Nuevo Mundo que la tripulación de Cristóbal Colón habría llevado a Europa. Fue elaborada por Gonzalo Fernández de Oviedo y Ruy Díaz de Isla, dos médicos españoles presentes al momento del retorno de Cristóbal Colón desde América, en 1493.

Fernández de Oviedo (1478-1557), en su breve "Sumario de la Natural Historia de las Indias" (1526) dice:
Otro cronista de Indias que barajó la misma tesis fue Francisco López de Gómara (1511-1566):

Los defensores actuales dicen que está demostrado que hay esqueletos de nativos americanos precolombinos con lesiones sifilíticas y vinculan a la tripulación del primer viaje de Colón (1492) y con la epidemia de sífilis en el sitio de los alemanes contra Nápoles (1494).

Plutarco Naranjo critica la hipótesis colombina desarrollada por Gonzalo Fernández de Oviedo y Ruy Díaz de Isla indicando que sus observaciones son errores históricos o fantasías, y que, por el contrario, la sífilis llegó a América desde Europa. Según este autor, no había en las expediciones personal con el conocimiento médico suficiente para identificar o reconocer las distintas enfermedades venéreas; así mismo, Fernández de Oviedo carecería de dicho conocimiento y no había reconocido enfermos ni en Europa ni en el Nuevo Mundo. Otra observación que hace Plutarco Naranjo, es que el médico Diego Álvarez de Chanca, que acompañó a Cristóbal Colón y describió con lujo de detalles diversas enfermedades tanto de los marineros que los acompañaron como de los aborígenes, no hizo mención a ningún tipo de enfermedad con manifestaciones cutáneas que sugirieran el diagnóstico de sífilis. Finalmente, este autor hace notar el hecho de que la sífilis continuó expandiéndose en el Viejo Mundo mientras que en el Nuevo no se presentaron epidemias.

Otros detractores de esta hipótesis han intentado demostrar la presencia de la sífilis en Europa con anterioridad al viaje de Colón mediante la datación de esqueletos europeos con evidencias de lesiones siflíticas antes de 1492, pero los resultados no han sido concluyentes, y muchas de sus evidencias han resultado en dataciones repetidas y confirmadas con una antigüedad posterior al año 1492. Aún hay 16 huesos europeos anteriores a 1492 con lesiones que podrían ser de tipo sifilítico, evidencias que no son aceptadas por los adeptos a esta hipótesis, arguyendo que dichas dataciones se han alterado y aparecen más antiguas, debido al consumo de alimentos provenientes del océano que traen material orgánico de mayor antigüedad.

Desde Nápoles, la enfermedad barrió Europa a partir de 1495, con tasas de morbilidad y mortalidad elevadísimas.
Como lo describe Jared Diamond: «En esa época, las pústulas de la sífilis frecuentemente cubrían el cuerpo desde la cabeza a las rodillas, haciendo que se desprendiera la carne de la cara de las personas, y matando en pocos meses». Además la enfermedad era más frecuentemente fatal que hoy en día. Diamond concluye que «hacia 1546 la enfermedad habría evolucionado hasta convertirse en la sífilis con los síntomas que se conocen actualmente».

Se cree que la causa principal de esta pandemia (en Europa, gran parte de Asia y norte de África) luego del siglo XVI se debió probablemente a la rápida urbanización.

En el siglo XVIII, miles de europeos contraían la sífilis. En el siglo XIX, Flaubert, estudiando los prostíbulos de Egipto, encontró que las rameras sin excepción estaban todas infectadas con sífilis.

Las crónicas de la época le echaban la culpa de la sífilis a las enormes migraciones de ejércitos (en la época de Carlos VIII, a fines del siglo XV).

Algunos escritores sostienen que hubo simultáneamente una epidemia de gonorrea, que se suponía el mismo mal que la sífilis.
Otros dicen que quizá fue una epidemia de una enfermedad concomitante pero desconocida.

En 1901 el bacteriólogo alemán Paul Ehrlich sintetizó el Salvarsán, un compuesto orgánico del arsénico, concebido específicamente para el tratamiento de la sífilis y que se convirtió en uno de los primeros fármacos sintéticos eficaces para la curación de enfermedades infecciosas. El Salvarsán (y su derivado, el Neosalvarsán) se abandonaron a partir de 1944, en favor del tratamiento antibiótico con penicilina, mucho más eficaz. Para probar la penicilina, durante los años 1946 a 1948 Estados Unidos llevó a cabo experimentos sobre sífilis en ciudadanos de Guatemala sin el consentimiento ni conocimiento de los hombres y mujeres que fueron utilizados como cobayas.

En 1905 Schaudinn y Hoffmann descubrieron el agente etiológico de la enfermedad.

En 1913, Hideyo Noguchi ―un bacteriólogo japonés que trabajaba en el Instituto Rockefeller― demostró que la presencia de la espiroqueta "Treponema pallidum" (en el cerebro de un paciente con parálisis progresiva) era la causante de la sífilis.

En España se han duplicado en seis años los casos de sífilis, pasando de cuatro casos por cada 100 000 habitantes en 2006 a 7,8 en 2012.

La sífilis se contagia principalmente por contacto sexual, seguido por el contagio vía transplacentaria. Besar, recibir transfusiones sanguíneas o inocularse accidentalmente son vías de transmisión menos importantes hoy día. Estudios de parejas han establecido tasas de transmisión de entre un 18 a un 80%, mientras que estudios prospectivos dan tasas de entre un 9 a un 63%. Finalmente, la probabilidad general esperada de transmisión entre parejas es de un 60%.

Formas de contagio: Mediante el contacto de la piel con la secreción que generan los chancros, o por contacto con los clavos sifilíticos de la persona enferma; al realizar sexo oral sin preservativo (ya sea que los chancros estén en la boca, en el pene o en la vulva), o a través del beso si hay lesiones sifilíticas en la boca. Puede ser contagiada por el uso compartido de jeringas. Si la madre está infectada puede transmitirla a sus hijos a través de la placenta (sífilis congénita) o a través del canal de parto (sífilis connatal). En ambos casos, el bebé puede morir pronto o desarrollar sordera, ceguera, perturbaciones mentales, parálisis o deformidades.

Es prácticamente imposible que se transmita por una transfusión de sangre, porque la sangre se analiza antes de su transfusión, y porque el "Treponema pallidum" no sobrevive más de 48 horas en la sangre conservada en hemoteca.

En comunidades que viven bajo pobres condiciones higiénicas, la sífilis endémica puede transmitirse por contacto no sexual. Pero no se transmite por el asiento en sanitarios, actividades cotidianas, tinas de baño o compartir utensilios o ropa.

Es importante notar que el sujeto en la fase precoz de la enfermedad resulta altamente contagiante (la úlcera venérea está llena de treponemas), pero se sostiene que después de cuatro años el individuo infectado no puede difundir más el microorganismo mediante relaciones sexuales.
En las relaciones entre hombre y mujer es más fácil que se contagie el hombre. El período en el que más personas se contagian es entre los 20 y los 25 años de edad. El recontagio es muy común en varones homosexuales.
En los años ochenta y noventa en Europa hubo una relativa disminución de los casos de sífilis, relacionados con el temor al contagio por VIH, que conllevó al uso generalizado del preservativo, que representa una eficiente barrera contra el contagio, tanto del VIH como del "Treponema pállidum".

Según datos de la OMS, en el mundo existen 12 millones de nuevos casos de sífilis:


El organismo causante de la sífilis es el "Treponema pallidum" subsp "pallidum." Este microorganismo es una bacteria móvil espiroforme (con forma de hilo en espiral), perteneciente al orden "Spirochaetales," familia "Spirochaetaceae", genero "Treponema". Su diámetro es de 0,10 a 0,18 micrómetros y su longitud entre 6 y 20 micrómetros. El promedio de torsiones de espiral de un T. pallidum es de 6 a 14. Su movilidad, como sacacorchos, está dado por endoflagelos, que le permiten una rápida rotación, torcerse y doblarse en ángulos.

Esta bacteria se propaga por multiplicación simple con división transversal. Al contrario que otras bacterias de su familia, solo se puede cultivar "in vitro" durante un breve período, con un máximo de supervivencia de 7 días a 35 °C, en medio particularmente enriquecido y en presencia de CO por sus particulares exigencias nutritivas y metabólicas. En nitrógeno líquido se mantiene su vitalidad, y prolifera de manera excelente en testículos de conejo.
En sangre conservada en hemoteca para transfusiones la bacteria sobrevive entre 24 y 48 horas.

"Treponema pallidum" puede sobrevivir en un hospedador humano durante varias décadas, ya que éste presenta un mecanismo de resistencia a los sistemas efectores de la respuesta inmune al recubrirse de proteínas del hospedador para camuflarse hasta que alcanza el Sistema Nervioso Central.

Tras un período de incubación de entre dos y seis semanas la sífilis transcurre por cuatro etapas clínicas de límites difusos: primaria, secundaria, latente y terciaria.
Esta etapa se caracteriza por la presencia en el sitio de inoculación ―la boca, el pene, la vagina o el ano― de una úlcera indurada e indolora parecida a una herida abierta, que se denomina chancro. Se acompaña de inflamación de los ganglios regionales. en unas pocas semanas el chancro cura espontáneamente.

En el varón los chancros suelen localizarse en el pene o dentro de los testículos, aunque también en el recto, dentro de la boca o en los genitales externos, mientras que en la mujer, las áreas más frecuentes son: cuello uterino y los labios genitales mayores o menores.

Durante esta etapa es fácil contagiarse con la secreción que generan los chancros.
Una persona infectada durante esta etapa puede infectar a su pareja al tener relaciones sexuales sin protección.

La sífilis secundaria comienza entre el momento de la desaparición del chancro o hasta seis meses después. Se caracteriza por malestar general, cefalea, fiebre baja, adenopatías generalizadas, pápulas rosáceas indoloras llamadas «clavos sifilíticos» en las palmas de las manos y plantas de los pies, a veces con descamación, lesiones en la mucosa de la boca o los genitales, lesiones confluentes de aspecto verrugoso cerca del lugar donde se formó el chancro (condiloma lata) y pérdida de cabello en parches. Los clavos sifilíticos y las lesiones de las mucosas son muy contagiosos. La sífilis secundaria puede durar de semanas o meses.

La sífilis latente se caracteriza por una serología positiva sin síntomas ni signos. Se divide en dos partes: sífilis latente temprana y sífilis latente tardía, dependiendo de si el tiempo de presencia de la enfermedad es menor o mayor a dos años respectivamente. Si el tiempo de la enfermedad se desconoce, se trata como un a sífilis latente tardía.

En la tercera fase (llamada también fase final), entre uno y veinte años después del inicio de la infección, la sífilis se vuelve a despertar para atacar directamente al sistema nervioso o algún órgano.

En esta fase se producen los problemas más serios y puede llegar a provocar la muerte.
Algunos de los problemas son:


Aunque un tratamiento con penicilina puede matar la bacteria, el daño que haya hecho en el cuerpo podría ser irreversible.

Los bebés de las mujeres con sífilis pueden infectarse mediante la placenta o durante el parto. La mayoría de los recién nacidos con sífilis congénita no presentan síntomas, aunque en algunos casos se puede presentar una erupción cutánea en las palmas de las manos y las plantas de los pies. Entre los síntomas posteriores se incluyen sordera, deformidades en los dientes y nariz en silla de montar (cuando colapsa el puente nasal).

Antes de la aparición de las pruebas serológicas, el diagnóstico preciso era imposible.
De hecho, se la llamaba «la gran imitadora» ya que ―en la fase primaria y secundaria― sus síntomas pueden confundirse fácilmente con los de otras enfermedades, haciendo que el sujeto le reste importancia y no acuda al médico.

Antiguamente se trataba con mercurio, lo cual hizo famosa la frase «una noche con Venus y una vida con Mercurio», pero este tratamiento era más tóxico que beneficioso.

El tratamiento de elección para tratar la sífilis es la penicilina, en todas sus fases. En las fases primaria y secundaria, se usa penicilina G benzatínica en una dosis de 2,4 millones de UI por vía intramuscular por una sola vez. En las fases tardía y tardía latente se usa penicilina G benzatínica en tres dosis de 2,4 millones de UI intramuscular una vez por semana, totalizando 7,2 millones de UI.

Para la neurosífilis, el tratamiento es penicilina G cristalina administrada por vía endovenosa a razón de 18 a 24 millones de UI en una dosis administrada en una infusión continua lenta o dividida en 6 dosis diarias (a razón de una dosis cada dos o tres días). esta última forma de administración se realiza con el fin de que el antibiótico difunda al LCR (líquido cefalorraquídeo), lugar donde se encuentra alojada principalmente la bacteria durante esta última fase. No obstante, el tratamiento no asegura una eficacia clínica.

En pacientes alérgicos a la penicilina se opta por un esquema antibiótico que no contenga betalactámicos, siendo los más usados la doxiciclina y la ceftriaxona.

Tratada a tiempo, la enfermedad tiene cura sencilla sin dejar secuelas. 

El padecer la sífilis aumenta el riesgo de contraer otras enfermedades de transmisión sexual (como el VIH), ya que los chancros son una vía fácil de entrada en el organismo.

Si no se trata a tiempo, puede ocasionar:

En algunos casos, las personas que supuestamente ya han obtenido la cura todavía pueden infectar a los demás. 

El haber padecido sífilis y haberse curado no implica inmunidad, ya que rápidamente se puede volver a contraer. Esto se debe a que la bacteria que produce la sífilis "(Treponema pallidum)" cuenta con tan solo nueve proteínas en su cubierta, lo cual no es suficiente para que el sistema inmunitario humano la reconozca y pueda producir anticuerpos para combatirla o inmunizarse. 

La abstinencia (no tener ningún contacto sexual) es la manera más segura de evitar la infección. Monogamia mutua (tener relaciones sexuales con una sola pareja no infectada, quien sólo tiene relaciones sexuales contigo) es otra manera de evitar la infección.

El uso de condones de látex consistente y correcta para el sexo vaginal y anal puede reducir el riesgo de transmisión, pero mientras el condón puede proteger el pene o la vagina, no protege de contactos con otras áreas como el escroto o área anal. 1-.http://www.quierosaber.org/ets/sifilis.html © 2017 Quierosaber. All rights reserved.


Durante los años 1946 a 1948 se llevaron a cabo en Guatemala experimentos sobre sífilis, dentro de un programa patrocinado y ejecutado por el gobierno de Estados Unidos. Fueron experimentos con humanos en los cuales médicos, generalmente estadounidenses, infectaron sin consentimiento de las víctimas ―a numerosos guatemaltecos, soldados, reos, pacientes psiquiátricos, prostitutas e, incluso, niños en orfandad―, inoculándoles sífilis y otras enfermedades venéreas como gonorrea, para comprobar la efectividad de nuevos fármacos, tanto antibióticos ―en especial penicilina―, como distintos tratamientos preventivos.





</doc>
<doc id="2601" url="https://es.wikipedia.org/wiki?curid=2601" title="Saccharum">
Saccharum

Saccharum, es un género de plantas herbáceas perteneciente a la familia de las poáceas. Es originario de Asia.

Se excluyen de "Saccharum" los géneros, "Erianthus, Miscanthus, Lasiorhachis, Narenga". 
Cáliz: gluma con una flor, de dos ventallas, lanceoladas, envueltas, y con un pelo en su base.
Corola: Gluma de dos ventallas, más corta y algo obtusa, con una arista dorsal torcida.
Estambre: germen oblongo: estilos dos, con plumas; con sus estigmas también plumosos.
Pistilo: germen alesnado: estilos dos, como zarcillos, con los estigmas sencillos.
Peric. ninguno: la corola, vistiendo la semilla, hace sus veces.
Semilla: una sola, oblonga, angosta y puntiaguda. 
El género fue descrito por Carlos Linneo y publicado en "Species Plantarum" 1: 54. 1753. La especie tipo es: "Saccharum officinarum" L.
El nombre del género proviene del latín "saccharum" que significa azúcar.
Tiene un número de cromosomas de: x = 10 and 12. 2n = 40, o 60, o 68, o 76–78, o 80, o 90, o 46–128, o 110, o 112, o 116–117, o 144. 4, 6, 8, 9, y 12 ploidias. Cromosomas ‘pequeños’.




</doc>
<doc id="2602" url="https://es.wikipedia.org/wiki?curid=2602" title="Schismus">
Schismus

Schismus, es un género de plantas herbáceas perteneciente a la familia de las poáceas. Es originario de África, Mediterráneo hasta el oeste de India.
El nombre del género deriva del griego "schisma" (hendidura), refiriéndose a la punta de la lema. 

El número cromosómico básico del género es x = 6, con números cromosómicos somáticos de 2n = 12 diploide.



</doc>
<doc id="2603" url="https://es.wikipedia.org/wiki?curid=2603" title="Secale">
Secale

Secale, es un género de plantas herbáceas de la familia de las gramíneas o poáceas. Es originario de la región del Mediterráneo, este de Europa hasta centro de Asia y Sudáfrica. 
El género tiene el nombre del latín clásico para el centeno o espelta. 



</doc>
<doc id="2604" url="https://es.wikipedia.org/wiki?curid=2604" title="Sesleria">
Sesleria

Sesleria es un género de plantas herbáceas perteneciente a la familia de las poáceas. Es originario de Eurasia.

Algunos autores excluyen los géneros "Oreochloa, Psilathera, Sesleriella".
El género fue descrito por Giovanni Antonio Scopoli y publicado en "Flora Carniolica" 189. 1760. 



</doc>
<doc id="2605" url="https://es.wikipedia.org/wiki?curid=2605" title="Setaria">
Setaria

Setaria, es un género de plantas herbáceas perteneciente a la familia de las poáceas. Es originario de las regiones templadas y tropicales del globo.
Son plantas anuales. Hojas con vaina pelosa o glabra; lígula representada por una fila de pelos; limbo plano. Inflorescencia en panícula espiciforme, densa, con eje escábrido o pubescente. Pedúnculos de las espiguillas con numerosas setas rígidas, antrorsas o retrorso-escábridas, persistentes. Espiguillas cortamente pedunculadas, ovadas o elípticas, con flor inferior masculina o estéril y la superior hermafrodita. Glumas 2, desiguales, membranosas, más cortas que las flores o la superior tan larga como las flores; la inferior con (1-) 3 nervios y la superior con 5-7 nervios. Flor inferior con lema tan larga como la de la flor superior, con 5-7 nervios, membranosa; pálea membranosa. Flor superior con lema con 5 nervios poco marcados, coriácea; pálea casi tan larga como la lema y con 2 quillas, endurecida en la madurez. Cariopsis oblongoidea a elipsoidea.
El género fue descrito por Ambroise Marie François Joseph Palisot de Beauvois y publicado en "Essai d'une Nouvelle Agrostographie" 51, 178, pl. 13, f. 3. 1812. La especie tipo es: "Setaria viridis" (L.) Beauv.
El nombre del género deriva del latín "seta" (cerda), aludiendo a las inflorescencias erizadas. 

El número cromosómico básico del género es x = x = 9 y 10, con números cromosómicos somáticos de 2n = 18, 36, 54, 63, y 72, o 36-54 ya que hay especies diploides y una serie poliploide. Cromosomas relativamente «pequeños». Nucléolos persistente




</doc>
<doc id="2606" url="https://es.wikipedia.org/wiki?curid=2606" title="Sorghum bicolor">
Sorghum bicolor

El sorgo o zahína ("Sorghum bicolor") es una hierba perteneciente a la familia de las gramíneas (Poaceae), cuyos frutos se utilizan para hacer harina y como forraje. Es un cultivo alimenticio importante en África, América Central, y Asia Meridional y es la quinta cosecha de cereal en el mundo, en cuanto a su producción (km² 470.000 cosechado en 1996). El productor más grande es Estados Unidos. 
El sorgo se conoce con varios nombres: mijo grande y maíz de Guinea en África occidental, kafir en África austral, duro en el Sudán, mtama en África oriental, iowar en la India y kaoliang en China (Purseglove, 1972). 

El género "Sorghum" se caracteriza por presentar espiguillas que nacen de a pares. El sorgo se trata como planta anual, aunque es hierba perenne y en los trópicos puede cosecharse varias veces al año.

Tiene su origen en África del este y primero divergió de las variedades salvajes en Etiopía hace 5000 años. Se adapta bien al crecimiento en áreas áridas o semiáridas cálidas. Las muchas subespecies se dividen en cuatro grupos - sorgos del grano, sorgos forrajeros (para pastoreo y henificar), sorgos dulces (jarabes del sorgo), y sorgo de escobas (para la confección de escobas y cepillos).

Morfología

El sorgo tiene una altura de 1 a 2 metros. Tiene inflorescencias en panojas y semillas de 3 mm, esféricas y oblongas, de color negro, rojizo y amarillento. Tiene un sistema radicular que puede llegar en terrenos permeables a 2 m de profundidad. Las flores tienen estambres y pistilos, pero se han encontrado en Sudán sorgos dioicos.

El sorgo se utiliza para producir grano que sirve para la alimentación del ganado, y también para el forraje y la manufactura de escobas. 

El valor energético del grano de sorgo es un poco inferior al del maíz. Se puede estimar como media 1,08 UF/kg. Comparándolo con el grano de maíz, el de sorgo es generalmente un poco más rico en proteínas, pero más pobre en materia grasa; como las de maíz, son de un valor biológico bastante débil; son particularmente deficitarias en lisina. 

Exigencias del cultivo. 

Las exigencias en calor del sorgo para grano son más elevadas que las de maíz. Para germinar necesita una temperatura de 12 a 13 °C, por lo que su siembra ha de hacerse de 3 a 4 semanas después del maíz. El crecimiento de la planta no es verdaderamente activo hasta que se sobrepasan los 15 °C, situándose el óptimo hacia los 32 °C. 
Al principio de su desarrollo, el sorgo soporta las bajas temperaturas de forma parecida al maíz, y su sensibilidad en el otoño es también comparable. Los descensos de temperatura en el momento de la floración pueden reducir el rendimiento del grano. Por el contrario, el sorgo resiste mucho mejor que el maíz las altas temperaturas. Si el suelo es suficientemente fresco no se comprueba corrimiento de flores con los fuertes calores. 

El sorgo resiste la sequía más que el maíz. Es capaz de sufrir sequía durante un periodo de tiempo bastante largo, y reemprender su crecimiento más adelante cuando cesa la sequía. Por otra parte, necesita menos cantidad de agua que el maíz para formar un kilogramo de materia seca.
Se desarrolla bien en terrenos alcalinos, sobre todo las variedades azucaradas que exigen la presencia en el suelo de carbonato cálcico, lo que aumenta el contenido en sacarosa de tallos y hojas. Prefiere suelos sanos, profundos, no demasiado pesados. Soporta algo la sal.

La planta es el alimento de las larvas del lepidóptero "Charaxes jasius".

"Sorghum bicolor" fue descrita por (L.) Moench y publicado en "" 207. 1794.




</doc>
<doc id="2607" url="https://es.wikipedia.org/wiki?curid=2607" title="Spartina">
Spartina

Spartina, es un género de plantas herbáceas perteneciente a la familia de las poáceas. Es originario de las regiones templadas de América, Europa y África.
Son plantas perennes, rizomatosas. Espiguillas marcadamente comprimidas lateralmente, dispuestas en 2 filas apretadas a lo largo del raquis. Espiguillas con 1 (-2) flores hermafroditas. Glumas 2, aproximadamente casi tan largas como las flores, desiguales, subcoriáceas; la inferior uninervada; la superior con 1-3 nervios. Lema uninervada, coriácea. Pálea aproximadamente tan larga como la lema, de margen más o menos ampliamente escarioso. Cariopsis comprimida, glabra.
El género fue descrito por Johann Christian Daniel von Schreber y publicado en "Flora italiana, ossia descrizione delle piante" ... 1: 366. 1848. La especie tipo es: "Spartina cynosuroides"
El nombre del género deriva de las palabras griegas "spartine" (una cuerda hecha de esparto, "Spartium junceum"), refiriéndose a las hojas fibrosas. 
Tiene un número de cromosomas de: x = 7 and 10. 2n = 28, 40, 42, 60, 62, 84, 120, 122, y 124. 3, 4, 6, 8, y 12 ploidias. 



</doc>
<doc id="2608" url="https://es.wikipedia.org/wiki?curid=2608" title="Sphenopus">
Sphenopus

Sphenopus es un género de plantas herbáceas perteneciente a la familia de las poáceas. Es originario de la región del Mediterráneo hasta el oeste de Asia.
Son planta anuales. Hojas con vainas de márgenes libres; lígula membranosa; limbo plano o convoluto y filiforme. Inflorescencia en panícula muy laxa, con ramas patentes, divaricadas. Espiguillas comprimidas lateralmente, con 2-5 flores hermafroditas, o a veces la superior estéril; raquilla glabra o ligeramente escábrida, desarticulándose en la madurez. Glumas 2, muy desiguales, membranosas, más cortas que las flores; la inferior poco conspicua, sin nervios; la superior con 1-3 nervios, rara vez sin nervios. Lema más o menos membranosa, trinervada. Pálea membranosa, con 2 quillas. Androceo con 3 estambres. Ovario glabro. Cariopsis oblongoidea, glabra.
El género fue descrito por Carl Bernhard von Trinius y publicado en "Fundamenta Agrostographiae" 135. 1820. La especie tipo es: "Sphenopus gouanii" Trin. 
El nombre del género deriva de las palabras griegas "sphen" (cuña) y "pous" (pie), en referencia a los pedicelos distales engrosados.
Tiene un número de cromosomas de: x = 6 y 7. 2n = 12 y 24. 2 y 4 ploidias. Cromosomas ‘grandes’.



</doc>
<doc id="2609" url="https://es.wikipedia.org/wiki?curid=2609" title="Sporobolus">
Sporobolus

Sporobolus, es un género de plantas herbáceas perteneciente a la familia de las poáceas. Es originario de las regiones templadas y tropicales del globo.
Son plantas perennes, rizomatosas. Tallos glabros. Hojas glaucas, con limbo plano o convoluto, rígido, surcado en el haz y ligeramente estriado en el envés. Inflorescencia en panícula laxa. Espiguillas comprimidas lateralmente, cortamente pedunculadas, con 1 sola flor hermafrodita articulada con la raquilla. Glumas desiguales, uninervadas. Raquilla no prolongada por encima de la flor. Lema escariosa, uninervada. Pálea igualando a la lema, binervada, biaquillada en la base. Periantio con 2 lodículas obtusas y casi enteras. Androceo con 3 estambres.
El género fue descrito por Robert Brown y publicado en "Prodromus Florae Novae Hollandiae" 169. 1810. La especie tipo es: "Sporobolus indicus" (L) R. Br.
El nombre del género deriva del griego "spora" (semillas) y "ballein" (tirar), aludiendo a la semilla cuando se libera y (probablemente) por la manera, a veces por la fuerza, de su lanzamiento.
El número cromosómico básico del género es x = 9 y 10, con números cromosómicos somáticos de 2n = 18, 24, 36, 38, 54, 72, 80, 88, 90, 108, y 126, ya que hay especies diploides y una serie poliploide. Nucléolos persistentes. Cromosomas relativamente "pequeños".




</doc>
<doc id="2610" url="https://es.wikipedia.org/wiki?curid=2610" title="Stenotaphrum">
Stenotaphrum

Stenotaphrum, es un género de plantas herbáceas de la familia de las gramíneas o poáceas. Es originario de las regiones tropicales y subtropicales del globo. Comprende 13 especies descritas y de estas, solo 7 aceptadas.

El género fue descrito por Carl Bernhard von Trinius y publicado en "Fundamenta Agrostographiae" 175. 1820[1822]. La especie tipo es: "Stenotaphrum glabrum" Trin.
El nombre del género deriva de las palabras griegas "stenos" (estrecha) y "taphros" (trinchera, hueco), aludiendo a las cavidades de los raquis.
Tiene los números cromosómicos somáticos de 2n = 18, 20, y 36. 2 y 4 ploides.





</doc>
<doc id="2611" url="https://es.wikipedia.org/wiki?curid=2611" title="Stipa">
Stipa

Stipa es un género de gramíneas (Poaceae), perennes y cespitosas, que comprende aproximadamente 250 especies distribuidas por todo el globo.
En las regiones esteparias de América, el género "Stipa" es con frecuencia dominante. Forma la Pampa seca, las antiguas Grandes Praderas de Norteamérica, las sabanas de África y Sudamérica y las estepas euroasiáticas, mediterráneas y africanas. Sus especies tienen a veces valor forrajero, o bien son perjuciales debido a las perforaciones que sus frutos producen en los cueros de los animales. Una especie del sur de Europa y norte de África, "Stipa tenacissima" , es el "esparto" utilizado en la fabricación de papel y cordelería.

Muchas especies están adaptadas a suelos áridos, semidesiertos y estepas, a las que les dan nombre y caracterizan y se emplean para evitar la erosión de los suelos y la desertización.
En zonas apropiadas forman praderas densas solas o en compañía de otros grupos. Aunque en agricultura suelen ser consideradas malezas, algunas especies se emplean por la resistencia de sus fibras para realizar cuerdas, cestos y otras urdimbres. Por su gran capacidad de regeneración y su resistencia a desaparecer son un alimento importante para muchos herbívoros y caracterizan muchas praderas salvajes en zonas secas de todo el planeta. Aunque hay excepciones, conforme las zonas son más frescas y húmedas, se desarrolla otro tipo de vegetación que sustituyen poco a poco a las especies del género stipa.
"Stipa" incluye pastos perennes, cespitosos, de unos 30 cm pero en ocasiones de hasta dos metros y medio, frecuentemente con hojas de lámina convoluta. Las espiguillas se hallan dispuestas en panojas generalmente laxas. Todas las Stipa se reconocen por tener unas aristas muy largas, que cuando son maduras en algunos se enrollan entre ellas quedando completamente enmarañadas.

Las espiguillas son unifloras, articuladas por encima de las glumas, con articulación oblicua que deja un callus puntiagudo e hirsuto unido al flósculo. Las glumas son membranosas, frecuentemente hialinas, agudas o acuminadas en el ápice, iguales o ligeramente desiguales. La lema es estrecha, notablemente convoluta, obovoidea, fusiforme o lineal, endurecida a la madurez y persistente sobre el cariopse, terminada superiormente en una arista generalmente persistente, enroscada y geniculada (doblada) una o dos veces. La pálea es plana, lanceolada, no carenada, frecuentemente reducida, rodeada por la lema. El androceo está compuesto por 3 estambres, las anteras son amarillas o violáceas, frecuentemente con un mechoncito de pelos en el ápice. El cariopse es fusiforme u obovado, con hilo linear.

El género fue descrito por Carlos Linneo y publicado en "Species Plantarum" 1: 78–79. 1753. La especie tipo es: "Stipa pennata" L.
Stipa: nombre genérico que deriva del griego "stupe" (estopa) o "stuppeion" (fibra), aludiendo a las aristas plumosas de las especies euroasiáticas, o (más probablemente) a la fibra obtenida de pastos de esparto.
Tiene un número de cromosomas de: x = 9, 10, 11, 12, y 22. 2n = 22, 28, 40, 44, 48, 68, y 96. 2, 4, y 8 ploidias.

"Stipa tenacissima", "Stipa gigantea", "Stipa parviflora", "Stipa barbata", "Stipa lagascae", "Stipa pennata", "Stipa offneri", "Stipa caudata"...

"Stipa papposa" recibe en Argentina el nombre común de aibé.





</doc>
<doc id="2612" url="https://es.wikipedia.org/wiki?curid=2612" title="Smalltalk">
Smalltalk

Smalltalk es un lenguaje reflexivo de programación, orientado a objetos y con tipado dinámico. Por sus características, Smalltalk puede ser considerado también como un entorno de objetos, donde incluso el propio sistema es un objeto. Metafóricamente, se puede considerar que un Smalltalk es un mundo virtual donde viven objetos que se comunican entre sí, mediante el envío de mensajes.

Un sistema Smalltalk está compuesto por:


Smalltalk obtuvo el segundo lugar para "lenguaje de programación más querido" en las encuestas para desarrolladores de Stack Overflow en 2017.

Los orígenes de Smalltalk se encuentran en las investigaciones realizadas por Alan Kay, Dan Ingalls, Ted Kaehler, Adele Goldberg y otros durante los años setenta en el Palo Alto Research Institute de Xerox (conocido como "Xerox PARC"), para la creación de un sistema informático orientado a la educación. El objetivo era crear un sistema que permitiese expandir la creatividad de sus usuarios, proporcionando un entorno para la experimentación, creación e investigación.

Un programa Smalltalk consiste únicamente de objetos, un concepto que se utiliza universalmente dentro de todo sistema Smalltalk. Prácticamente todo, desde un número natural como el 4 hasta un servidor web es un objeto. Los objetos Smalltalk presentan características comunes:


Los objetos se comunican entre sí mediante el envío de mensajes. Asimismo, un objeto puede proveer muchas operaciones (actualmente esto está determinado por cada implementación)

Las definiciones de estas operaciones en los objetos son llamadas métodos. Un método especifica la reacción de un objeto cuando recibe un mensaje que es dirigido a ese método. La resolución, en el sentido de ligado, de un mensaje a un método es dinámica. La colección entera de métodos de un objeto es llamada protocolo de mensajes o interfaz de mensajes del objeto. Los mensajes pueden ser parametrizados, estos parámetros serán objetos, y el resultado o respuesta del mismo también será un objeto.

Las características comunes de objetos está capturado bajo la noción de clase, de tal forma que los objetos agrupados bajo una clase son llamados instancias de ella. Las instancias son creadas durante la ejecución de un programa con algún propósito y son barridos automáticamente en el momento que no son necesitados más por el recolector de basura. Exceptuando algunos objetos especiales como los muy simples, llamados literales (números, cadenas, etc), cada objeto tiene su propio estado local y representa una instancia diferente de su clase.

Smalltalk es considerado el primero de los lenguajes orientados a objetos, aunque en realidad el primero en implementar programación orientada a objetos fue Simula. En Smalltalk "todo" es un objeto, incluidos los números reales o el propio entorno Smalltalk.

Como lenguaje tiene las siguientes características:


Smalltalk ha tenido gran influencia sobre otros lenguajes como Java o Ruby, y de su entorno han surgido muchas de las prácticas y herramientas de desarrollo promulgadas actualmente por las metodologías ágiles (refactorización, desarrollo incremental, desarrollo dirigido por tests, etc.).

Las implementaciones de Smalltalk de mayor peso (VisualWorks, Squeak, VisualSmalltalk, VisualAge, Dolphin, Pharo Smalltalk, Smalltalk X) poseen un entorno de interacción muy diferente al entorno de desarrollo típico de otras tecnologías como Microsoft Visual Studio .Net o Eclipse. El entorno o ambiente Smalltalk es primordialmente gráfico y funciona como un sistema en tiempo de ejecución que integra varias herramientas de programación (Smalltalk), utilidades multimedia, interfaces para ejecutar código no nativo a Smalltalk y servicios del sistema operativo.Estas posibilidades, que han influido en la metodología de trabajo y concepción de la programación, se traducen en la tendencia a considerar a Smalltalk más que un simple lenguaje de programación.
La forma de desarrollar software en Smalltalk no consiste en el ciclo típico de las tecnologías tradicionales: Arrancar un editor de texto, compilar y ejecutar y terminar la aplicación. En Smalltalk se manipula el entorno mismo, comúnmente mediante el Navegador del Sistema.

Tradicionalmente, Smalltalk no posee una notación explícita para describir un programa entero. Sí se utiliza una sintaxis explícita para definir ciertos elementos de un programa, tales como métodos, pero la manera en que tales elementos están estructurados dentro de un programa entero generalmente es definida por las múltiples implementaciones. El estándar mismo no promueve otra dirección, por lo que define una sintaxis abstracta de programas Smalltalk, que define todos los elementos que constituyen un programa Smalltalk y la manera en que esos elementos están lógicamente compuestos por otros elementos, sin embargo, cada implementación es libre de definir y utilizar las muchas sintaxis posibles que están conformes a la sintaxis abstracta estándar. Un ejemplo de una sintaxis concreta es el Formato de Intercambio Smalltalk (o SIF, de Smalltalk Interchange Format) definida en el mismo estándar.

La sintaxis de Smalltalk-80 tiende a ser minimalista. Esto significa que existen un grupo pequeño de palabras reservadas y declaraciones en comparación con la mayoría de los lenguajes populares. Smalltalk posee un grupo de 6 palabras reservadas: self, super, nil, true y false, super.

En Smalltalk no es necesario desalocar objetos explícitamente, por lo tanto no proporciona mecanismos para ello. Las implementaciones utilizan técnicas de recolección de basura para detectar y reclamar espacio en memoria asociado con objetos que ya no se utilizarán más en el sistema. En Smalltalk la recolección de basura es integrada configurable. La forma de ejecución del recolector de basura es en "background", es decir, como un proceso de baja prioridad no interactivo, aunque en algunas implementaciones es posible ejecutarlo a demanda, siendo posible definir configuraciones de memoria especiales para cada sistema mediante políticas (por ejemplo en VisualWorks). La frecuencia y características de la recolección depende de la técnica utilizada por la implementación. Adicionalmente algunas implementaciones de Smalltalk proporcionan soporte para mecanismos de finalización como el uso de Ephemerons.

Smalltalk-80 provee reflexión computacional y estructural, ya que es un sistema implementado en sí mismo. La reflexión estructural se manifiesta en que las clases y métodos que define el sistema son en sí mismos objetos también y forman parte del sistema mismo.
La mayoría de las implementaciones de Smalltalk tienden a exponer el compilador Smalltalk al entorno de programación, permitiendo que dentro del sistema se compile código fuente (textual), transformándose en objetos métodos, que son comúnmente instancias de la clase "CompiledMethod". El sistema generalmente incorpora estos métodos en las clases, almacenándolos en el diccionario de métodos de la clase a la cual se quiera agregar el comportamiento que realiza el método. Esto, así como la incorporación de nuevas clases al sistema, es realizado dentro del sistema mismo; aunque la mayor parte de las implementaciones poseen herramientas visuales que ocultan la complejidad de interactuar con la clase que usualmente se encarga de tales tareas, el "ClassBuilder".

La reflexión computacional de Smalltalk-80 se manifiesta en la posibilidad de observar el estado computacional del sistema. En los lenguajes derivados del Smalltalk-80 original, durante el envío de mensajes entre objetos, cada objeto receptor de un mensaje consulta su clase para tener acceso a los métodos que define. En caso de encontrarse el método en la clase, se dice que se "activa" el método. Esta activación de un método actualmente en ejecución, es accesible mediante una palabra clave llamada thisContext. Enviando mensajes a thisContext se puede consultar cuestiones tales como "¿quién me envió este mensaje?". Estas facilidades hacen posible implementar co-rutinas, continuaciones o back-tracking al estilo Prolog sin necesidad de modificar la máquina virtual. Uno de los usos más interesantes de esta facilidad, se da en el framework de web Seaside de Avi Bryant.

En Smalltalk todo es un objeto, y a un objeto se le envían mensajes. Por ejemplo:

Significa que al objeto "1" le enviamos el mensaje "+" con el colaborador externo, otro objeto, "1". Este ejemplo entonces resulta en el objeto "2".

En el típico Hola mundo, el objeto es Transcript, que recibe el mensaje show con el colaborador externo '¡Hola, Mundo!'.

Para crear una instancia de un objeto, sólo hay que mandar un mensaje new a una clase:

Para obtener las vocales de una cadena de texto:



</doc>

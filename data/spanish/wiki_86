<doc id="22026" url="https://es.wikipedia.org/wiki?curid=22026" title="Armenia">
Armenia

Armenia (en armenio: Հայաստան, "Hayastán"), oficialmente República de Armenia, es un país del Cáucaso Sur y sin salida al mar. Comparte frontera al oeste con Turquía, al norte con Georgia, al este con Azerbaiyán y al sur con Irán y la República Autónoma de Najicheván de Azerbaiyán.

Armenia es una antigua república soviética, un Estado unitario, multipartidista y en un proceso de democratización que tiene sus raíces en una de las más antiguas civilizaciones del mundo. Dotada de un rico patrimonio cultural, se destacó como la primera nación en adoptar el cristianismo como religión oficial en los primeros años del siglo IV (la fecha tradicional es 301). Aunque Armenia es un estado constitucional secular, la fe cristiana desempeña un papel importante en su historia y en la identidad del pueblo armenio.

Cultural, histórica y políticamente, Armenia se considera como parte de Europa. Sin embargo, su localización en el Cáucaso meridional la sitúa en una supuesta frontera imaginaria entre Europa y Asia: en realidad se trata de un país transcontinental, a medio camino entre los dos ámbitos geográficos. Estas clasificaciones son arbitrarias, pues no hay diferencia geográfica fácilmente definible entre Asia y Europa.

Armenia es actualmente miembro de más de 35 organizaciones internacionales, incluyendo las Naciones Unidas, el Consejo de Europa, el Banco Asiático de Desarrollo, la Comunidad de Estados Independientes, la Organización Mundial del Comercio y la Organización de Cooperación Económica del Mar Negro. Es uno de los integrantes de la Asociación para la Paz de la Organización del Tratado del Atlántico Norte, así como de la alianza militar Organización del Tratado de la Seguridad Colectiva (OTSC). Es también miembro observador de la Comunidad Económica Eurasiática, de la Francofonía y del Movimiento de Países No Alineados.

El nombre nativo del país en armenio es "Hayk‘". Este nombre se transformó durante la Edad Media en "Hayastan", con el sufijo persa “"- stan"”, que significa "país". El origen de la autodenominación armenia "hay" es incierto. Ha sido tradicionalmente derivada de Hayk (Հայկ), el legendario patriarca de los armenios, que según la tradición era hijo de Torgom o Togarma, a su vez hijo de Gomer (identificado con los cimerios), a su vez hijo de Jafet y nieto de Noé (). Según el historiador Moses de Corena (siglo V), Hayk derrotó y mató al gigante babilonio Belo en una batalla cerca de las montañas del lago de Van, en el sudoeste de la Armenia histórica (Turquía oriental actual), tradicionalmente fechada en 2492 a. C. Según algunos historiadores contemporáneos, "hay" proviene del país de Hayasa, mencionado en las escrituras cuneiformes hititas de los siglos XIV-XIII a. C.

Diversos exégetas bíblicos han identificado a Armenia como el sitio del jardín del Edén, y se ha interpretado que el monte Ararat es la montaña sobre la cual se posó el Arca de Noé después del Diluvio universal (Génesis 8:4).

El origen del nombre "Armenia" también es incierto. Varios eruditos armenios, incluyendo Rafael Ishjanyán (1989), lo han identificado con "Armani" (Armanum, que también se lee Armanim) mencionado entre los enemigos derrotados por el rey acadio Naram-Sin (2300 a. C.), localizándolos en las montañas de la Armenia meridional. Según algunos historiadores, la primera mención de la voz "Armina" aparece en la inscripción cuneiforme de Behistun del rey Darío I de Persia (ca. 519 a. C.). El término griego "Armenoi" aparece en Heródoto , quien sostiene que los armenios eran colonos de Frigia. La etimología tradicional para el etnónimo es su derivación de Aram, bisnieto del bisnieto de Hayk, como lo hace Moisés de Khorene.

Armenia se ha poblado desde épocas prehistóricas. Los arqueólogos continúan revelando indicios de que Armenia y sus montañas estuvieron entre los primeros lugares donde se asentó la civilización humana. A partir del 4000 hasta el 1000 a. C., las herramientas y los utensilios de cobre, de bronce y de hierro fueron producidos en Armenia y negociados comúnmente en tierras vecinas donde esos metales eran menos abundantes. El territorio de Armenia es también una de las posibles localizaciones del legendario país de "Aratta", mencionado en las fuentes sumerias.

Durante la Edad de Bronce, varios estados prosperaron, incluyendo el Imperio Hitita (en su máximo esplendor), Mitani (Armenia histórica del sudoeste) y de Hayasa-Azzi (siglo XV a. C.) y en la Edad de Hierro, los indoeuropeos frigios y mushkis llegaron y destruyeron el reino de Mitanni; también floreció la gente de Nairi (siglo XII al IX a. C.) y el reino de Urartu (siglo IX al VI a. C.), pero la aportación de cada pueblo en la etnogénesis de la gente armenia es incierta. Algunos discutirían sobre una mayor influencia de los hurritas en la Armenia temprana, pero basado en patrones drásticos diversos de la lengua, la mayoría acepta que los armenios pertenecen al grupo de pueblos indoeuropeos mientras que Urartu pertenece a la familia hurro-urartiana. Ereván, la capital moderna de Armenia, fue fundada en 782 a. C. por el rey Argishti I de Urartu.

Alrededor del 600 a. C., se estableció el reino de Armenia bajo la Dinastía Oróntida y existió bajo varias dinastías locales hasta el año 428 a. C.

Tras la derrota del Imperio seléucida, uno de los estados sucesores del imperio de Alejandro Magno, en la batalla de Magnesia a manos de Roma (190 a. C.), el gobernador de Armenia, Artashes, restableció la independencia de Armenia —conocida como Armenia Mayor— y fundó la dinastía Artáxida, (190 a. C.), que perduró hasta 1 d. C. Al mismo tiempo, el reino de Sofene, al sudoeste de la meseta de Armenia, restableció su independencia bajo Zariadres. La zona de Armenia Menor, al noroeste de la meseta, permaneció bajo el control de los reinos de Capadocia y de Ponto, y posteriormente fue anexionada al imperio romano.

El reino de Armenia alcanzó su máxima expansión entre el 95 a. C. y el 66 a. C. bajo Tigranes el Grande, cuando se convirtió brevemente en un imperio, extendiéndose desde el mar Caspio hasta el mar Mediterráneo y desde el Cáucaso hasta la frontera de Palestina.

A través de su historia, el reino de Armenia gozó de períodos de independencia intermitentes y períodos de autonomía conforme a los imperios contemporáneos. Reyes apoyados o impuestos por el Imperio romano o por Partia, o acordados por ambos, fundaron y destruyeron dinastías, como fue la dinastía arsácida establecida a partir del año 53 por Tirídates I. La localización estratégica de Armenia entre dos continentes la ha sometido a sucesivas invasiones de asirios, persas, romanos, bizantinos, árabes, turcos selyúcidas, mongoles, turcos otomanos y rusos.

En el año 301, Armenia se convirtió en el primer país del mundo en adoptar el cristianismo como religión oficial del Estado, por influencia de Gregorio I el Iluminador considerado hoy en día santo patrón de la Iglesia Apostólica Armenia. Tiridates III (238-314) fue el primer gobernante que oficialmente se propuso cristianizar a su gente, y su conversión ocurrió doce años antes de que el imperio romano concediera al cristianismo la tolerancia oficial bajo Constantino I (emperador) y casi ocho décadas antes de que Teodosio I el Grande adoptara el cristianismo como religión oficial del imperio (380). En el año 405, Mesrop Mashtots creó el alfabeto armenio.

Después de la caída del reino de Armenia en 428, la mayor parte del país fue incorporada al Imperio sasánida, gobernado por un marzpan. Después de una rebelión en 451, los armenios mantuvieron su libertad religiosa, mientras que Armenia ganó autonomía y el derecho a ser gobernada por un marzpan nativo, mientras que otros territorios imperiales fueron gobernados exclusivamente por persas. El marzpanato de Armenia duró hasta 640, cuando la Persia Sasánida fue destruida por el Califato árabe.

Tras la conquista árabe de Armenia, Armenia fue inicialmente agrupada en una unidad administrativa con el nombre de Arminiyya, que también incluyó partes de Georgia y de Albania caucásica y tenía su centro en la ciudad armenia de Dvin, bajo un gobernador árabe, llamado vostikan. A mediados del siglo IX la administración fue delegada en el príncipe de Armenia, reconocido por el califa y el emperador bizantino. El principado de Armenia duró hasta el año 884, cuando el país recuperó la independencia del debilitado imperio árabe.

El reino armenio fue gobernado por la dinastía Bagrátida hasta 1045, con capital en la ciudad de Ani. Al mismo tiempo, varias áreas de la Armenia bagrátida se separaron formando reinos y principados feudales, como el reino de Vaspurakan, gobernado por la casa de Artzruni, pero que a la vez reconocían la supremacía de los reyes bagratidas.

En 1045, el Imperio bizantino conquistó Ani y acabó con el reino de la Armenia bagrátida. Pronto, los otros estados armenios cayeron también bajo el control bizantino. La dominación bizantina fue breve, ya que los turcos selyúcidas derrotaron en el 1071 a los bizantinos y conquistaron Armenia en la batalla de Manzikert, estableciendo el Imperio Selyúcida. Para escapar de la muerte o servidumbre a manos de los que habían asesinado a su pariente Gagik II, rey de Ani, un armenio de nombre Rupen se adentró en los desfiladeros de los montes Tauros con algunos compatriotas. Llegó luego a Tarsos, en Cilicia, donde el gobernador bizantino le dio protección, y donde sería finalmente establecido el Reino armenio de Cilicia.
El Imperio Selyúcida pronto comenzó a derrumbarse. A principios del 1100, los príncipes armenios de la familia noble Zakarida establecieron un principado armenio semi-independiente en Armenia norteña y del este, conocida como Armenia zakarida. La familia noble de Orbeliano compartió el control con el Zakarida en varias partes del país, especialmente en Vayots Dzor y Syunik. Seguía habiendo las partes meridionales de Armenia bajo control de dinastías kurdas de Shaddadids y de Ayyubids.

En 1230 el ilkanato mongol conquistó el principado de Zakaryan, así como el resto de Armenia. Las invasiones mongolas pronto fueron seguidas por las de otras tribus asiáticas centrales, que continuaron desde 1200 hasta 1400. Después de incesantes invasiones, Armenia se debilitó. En el año 1500, el Imperio otomano y el Imperio safávida se repartieron el territorio de Armenia. El imperio ruso incorporó más adelante Armenia del este (consistiendo en los kanatos de de Erivan y de Karabaj dentro de Persia) en 1813 y 1828.

Armenia se convirtió en parte integrante del Imperio otomano con el reinado de Selim II (1524-1574). Sin embargo, la anexión inicial comienza ya con Mehmed II (siglo XV), que ofreció el respaldo otomano para iniciar el Patriarcado Armenio de Constantinopla. Esta situación duró 300 años, hasta la Guerra Ruso-Turca de 1828-1829, cuando la parte oriental de este territorio fue cedida al Imperio ruso. La parte restante, también conocida como Armenia otomana o Armenia occidental, continuó hasta la finalización de la Primera Guerra Mundial y la partición del Imperio otomano. En los años 1860 surgió el Movimiento de Liberación Nacional de Armenia.

Mientras el imperio comenzaba a derrumbarse, los Jóvenes Turcos derrocaron al gobierno del sultán Hamid. Los armenios que vivían en el imperio esperaban que la revolución de los Jóvenes Turcos cambiase su estado de segunda clase. Sin embargo, con el impacto de la Primera Guerra Mundial y el asalto del Imperio otomano sobre el imperio ruso, el nuevo gobierno comenzó a mirar a los armenios con desconfianza y suspicacia. Esto era debido al hecho de que el ejército ruso mantuvo un contingente de tropas armenias, integrado por unidades irregulares armenias. El 24 de abril de 1915 las autoridades otomanas arrestaron a los intelectuales armenios.
Con la ley de Tehcir, una gran proporción de armenios que vivían en Anatolia falleció como resultado del genocidio armenio. Había resistencia armenia local en la región, desarrollada contra las actividades del Imperio otomano. Los acontecimientos de 1915 a 1917 se consideran por los armenios y la inmensa mayoría de historiadores occidentales como matanzas totales patrocinadas por el estado.

A pesar de la evidencia abrumadora del intento genocida, las autoridades turcas mantienen actualmente que las muertes fueron resultado de una guerra civil, junto con el hambre y las enfermedades, incluyendo muertes en ambos bandos. La gran mayoría de estimaciones acerca del número de armenios muertos comienzan a partir de los 650 000 hasta el millón y medio de personas. Armenia y su diáspora han estado haciendo campaña desde hace años en busca del reconocimiento oficial de esos acontecimientos como un genocidio. El 24 de abril se conmemora como día del genocidio armenio.

Aunque el ejército ruso tuvo éxito en ocupar la mayor parte de Armenia durante la Primera Guerra Mundial, sus ganancias fueron perdidas con la revolución rusa de 1917. En ese momento, Armenia, Georgia y la parte de Azerbaiyán controlada por los rusos, trataron de adherirse formando la república federativa democrática transcaucásica. Esta federación, sin embargo, duró solamente de febrero a mayo de 1918, cuando las tres partes decidieron disolverla. Consecuentemente, Armenia del este llegó a ser independiente como República Democrática de Armenia (DRA) el 28 de mayo.

Con la partición del Imperio otomano, después de ser derrotado en la Primera Guerra Mundial, se creó un proyecto de estado armenio poco después de su independencia en el Tratado de Sèvres firmado por Turquía y algunos de los aliados de la Primera Guerra Mundial, el 10 de agosto de 1920, que dejó la delimitación de la frontera en manos del Presidente de los Estados Unidos Woodrow Wilson. El tratado final, sin embargo, no fue firmado por los Estados Unidos, y aunque aceptado por el Imperio otomano, fue rechazado por los turcos, dando lugar a una nueva guerra.

El proyecto de estado, incorporaba las provincias de Erzurum, Bitlis y Van, que eran partes de la región denominada Armenia otomana (conocida también como la "Armenia occidental"). Esta región se amplió hacia el norte, hasta la zona oeste de la Provincia de Trebisonda para proporcionar a la República Democrática de Armenia una salida al Mar Negro en el puerto de Trebisonda.

La Guerra de Independencia Turca, en la que los turcos vencieron a los armenios y a los griegos, obligó a los aliados a volver a la mesa de negociaciones antes de la ratificación del Tratado. Las partes firmaron y ratificaron el Tratado de Lausana en 1923, que estableció las actuales fronteras de Turquía. Las fronteras orientales las obtuvieron por medio del Tratado de Alexandropol el 2 de diciembre de 1920, y mediante el Tratado de Kars, firmado el 23 de octubre de 1921 y ratificado en Ereván el 11 de septiembre de 1922, con Armenia y la Unión Soviética, confirmando el Tratado de Lausana. El Tratado de Lausana y artículos relacionados no son reconocidos por el actual gobierno de la República de Armenia.

La independencia de breve duración de la DRA acabó con guerra, conflictos territoriales, una afluencia total de refugiados de Turquía, enfermedades y hambre. No obstante, la Entente, aterrada por las acciones del gobierno otomano, intentaron ayudar al nuevo estado armenio a través de fondos y de otras formas de ayuda.

Al final de la guerra, se decidió dividir el Imperio otomano. Firmado entre las potencias aliadas y el Imperio otomano en Sèvres el 10 de agosto de 1920, el Tratado de Sèvres prometió mantener la existencia de la DRA y unir los territorios del Imperio Otomano parcialmente poblados por armenios otomanos. Debido a que las nuevas fronteras de Armenia debían ser dibujadas por el presidente norteamericano Woodrow Wilson, la Armenia otomana también es conocida como “Armenia Wilsoniana.” Se consideró incluso la posibilidad de convertir Armenia en un protectorado bajo tutela de los Estados Unidos. El tratado, sin embargo, fue rechazado por el movimiento nacional turco, y nunca entró en efecto. El movimiento comandado por Mustafa Kemal ("Atatürk"), utilizó el tratado como la ocasión para declararse el gobierno legítimo de Turquía y sustituyó la monarquía con capital en Estambul por una república con la suya en Ankara.

En 1920, Armenia y Turquía entraron en guerra, un violento conflicto que terminó con el Tratado de Alexandropol (2 de diciembre de 1920). El tratado de Alexandropol obligó a Armenia a desarmar a la mayoría de sus fuerzas militares, ceder más del 50% de su territorio antes de la guerra, y renunciar a todos los territorios conferidos a su favor en el tratado de Sèvres. Al mismo tiempo, el Undécimo Ejército soviético bajo el mando de Grigori Ordzhonikidze, invadió Armenia en Karavansarái (actual Ijevan) del 29 de noviembre. Al 4 de diciembre las fuerzas de Ordzhonikidze entraron en Ereván y la efímera República de Armenia se derrumbó.

Simultáneamente a los acontecimientos del enfrentamiento entre Armenia y Turquía en 1920, Armenia fue invadida por el Ejército Rojo, lo que condujo al establecimiento de la dominación soviética en diciembre de 1920. Durante varios meses los nacionalistas armenios mantuvieron el control de Nagorno Karabaj, que finalmente fue ocupado por los comunistas. El tratado de Alexandropol, firmado por los funcionarios armenios anteriores (depuestos al establecerse el gobierno soviético), nunca fue ratificado por el nuevo gobierno comunista.

En 1922 el país fue incorporado a la Unión Soviética como parte de la República Socialista Soviética de Transcaucasia, de breve duración, junto con Georgia y Azerbaiyán. El tratado de Alexandropol fue reemplazado por el tratado de Kars, entre Turquía y la Unión Soviética. En él, Turquía cedió la provincia de Ajara a la Unión Soviética a cambio de la soberanía sobre los territorios de Kars, Ardahan e Iğdır. A fecha de hoy Armenia no reconoce este tratado como legítimo, ya que los armenios no participaron en él. De momento Armenia no ha hecho reclamaciones territoriales sobre las provincias que entonces pasaron a Turquía.

La República Socialista Soviética de Transcaucasia existió desde 1922 hasta 1936, cuando fue dividida en tres repúblicas separadas (RSS de Armenia, RSS de Azerbaiyán incluida la región autónoma armenia de Nagorno Karabaj, y RSS de Georgia). Los armenios gozaron de un período de estabilidad relativa bajo la dominación soviética. Recibieron medicinas, alimentos y otras provisiones desde Moscú, y la dominación comunista demostró ser un bálsamo calmante en contraste con los años finales turbulentos del Imperio otomano. La situación fue difícil para la iglesia, que luchó bajo la dominación soviética. Después de la muerte de Lenin, Stalin tomó las riendas de la URSS y comenzó una era de prosperidad e industrialización para la región. Cuando Stalin murió en 1953 Nikita Jrushchov emergió como el nuevo líder.

El período de Gorbachov en los años 80, se caracterizó por la tensión desarrollada entre Armenia y Azerbaiyán con respecto a la región de Alto Karabaj o Nagorni Karabaj.

En febrero de 1988, una escaramuza en la región de Agdam (Azerbaiyán) se cobró la vida de dos azeríes. Paradójicamente, el homicida se trató de un policía azerí, pero las confesiones y la desinformación de R. Kátusev (perteneciente a la política soviética azerí) instó frente a las cámaras de televisión a tomar venganza, culpando a los armenios del hecho. Las autoridades de Moscú, reprimieron a Kátusev por incitar al odio, para beneficiarse políticamente . A pocos días del hecho, alguien trasladó a vándalos y personas de mal vivir, como herramienta asesina, a los tugurios atestados de personas de nivel económico bajo en la ciudad de Sumgaít, una ciudad industrial situada cerca de Bakú, ciudad famosa por su alta contaminación que daba fama al cementerio ocupado generalmente por niños fallecidos por malformaciones.

La población azerí tenía además, la renta económica más baja. La comunidad armenia conformaba el estrato económico más elevado de la ciudad, y tenía el control del comercio y de parte de la industria local. Durante la tarde del 27 de febrero, cientos y luego miles de personas se concentraron en el Cuadrado de Lenin de la ciudad, asegurando ser personas que habían huido de la persecución bárbara de los armenios en Kapan y otras ciudades de la RSS de Armenia. Sin embargo todas las autoridades soviéticas negaron tales hechos ya que los azeríes que residían en la RSS de Armenia no habían manifestado ningún ataque contra su integridad. Se repartió opio y vodka entre los vándalos, que atacaron primero la estación de buses de la ciudad, mientras que otro grupo dirigido por la secretaria comunista de la ciudad R. Muslimzadé, empezó a atacar los bloques de departamentos del distrito armenio.

Pronto la ciudad ardía en un pogromo, las líneas telefónicas fueron cortadas, los hospitales recibieron órdenes de no atender a ningún paciente, los policías estaban ausentes, los bomberos tampoco se presentaron y el suministro eléctrico fue suspendido, dejando clara la artimaña del gobierno azerí. Muchos ciudadanos armenios se armaron en sus hogares con armas caseras o fueron alojados por sus vecinos rusos y azeríes que no participaron en la revuelta. Los asaltantes ingresaban a cada departamento en grupos de entre 50 y 80 personas provistos de cuchillos, sierras, y armas de metal previamente hechas en las fábricas. A este grupo de asaltantes se les unió otra gran multitud de jóvenes azeríes entre 12 y 17 años, que participaron activamente en el saqueo y el destrozo de los departamentos y locales armenios.

Algunos automóviles fueron incendiados con sus ocupantes dentro de ellos y muchos golpearon y mutilaron a decenas de personas, robaron todo lo que tenían a su disposición, e incluso violaron a muchas mujeres, luego arrojándolas al fuego. La policía especial soviética de Bakú, llegó al siguiente día, pero fueron incluso atacados con cócteles molotov y muchos quedaron seriamente heridos, el caos se apoderó de la ciudad.

Llegado el tercer día, tropas especiales provenientes de la Óblast de Riazán y Daguestán, ingresaron en la ciudad, y evacuaron a los pobladores armenios a un club local, posteriormente los armenios y luego rusos y otras nacionalidades abandonaron la ciudad, rescatando lo poco o nada que les dejaron los saqueadores.

Tras los acontecimientos se publicó una lista de 32 víctimas, entre ellas 26 armenios y seis azeríes, pero estas cifras son muy pequeñas para la realidad, incluso en los depósitos de cadáveres en 3 ciudades, se contaron hasta 100 cadáveres armenios.

Tras los acontecimientos de Sumgaít, se comenzó con un plan de investigación de los hechos, condenándose a varias personas. La población azerí del área en general tenía un elevado nivel de insensibilidad por las muertes armenias, apoyando en cierto modo la masacre como una respuesta merecida frente a la petición de autonomía de Nagorni Karabaj e incluso un grupo se precipitó en crear un lema "Libertad a los héroes de Sumgaít", estos hechos, hicieron que miles de personas armenias y de otras nacionalidades abandonaran la RSS de Azerbaiyán, miles de armenios se instalaron en Armenia, y miles de azeríes, temiendo represalias, salieron de Armenia hacia Azerbaiyán, generando un poderoso intercambio de poblaciones, sin embargo la mayor parte de azeríes no salió de Armenia hasta 1993, con la única diferencia que los refugiados armenios fueron saqueados y sus bienes expropiados, mientras los azeríes pudieron vender sus viviendas y trasladar gran parte de sus bienes a Azerbaiyán. 

Sin duda alguna las masacres de Sumgaít, se debió al deseo de la población de apoderarse de la gran cantidad de bienes materiales de los armenios , eventos similares se repitieron en Kirovabad (la actual Ganja) en 1989 (donde asesinaron a 16 ancianos, algunos de ellos minusválidos), en el pogromo de Bakú en 1990, y que luego se extendió por todo el país, siendo los más trágicos, el "Anillo de la Operación" realizado en la región de Shahumyán, deportando a miles de armenios, hiriendo, matando, y violando los derechos de aquellas personas, el caos se extendió a las áreas de Shamkur, Shushi, y Dashkesán.
Durante el Referéndum de Karabaj en 1991, la aviación azerí bombardeó la capital de Artsaj, la ciudad de Stepanakert. El mismo año de los acontecimientos de Sumgaít, el 7 de diciembre de 1988, un terrible sismo de 6,9 seguido por otro de 5,8 devastó el norte de la RSS de Armenia, junto al área fronteriza con Turquía y la RSS de Georgia, el epicentro se ubicó en las inmediaciones de la ciudad de Spitak, donde se habían alojado millares de armenios provenientes de la RSS de Azerbaiyán, el trágico terremoto fue el peor sismo que sacudió Armenia en más de 80 años.

Ocurrido a las 11:41, cuando fábricas, y escuelas se encontraban repletas de personas, y en época cuando el Cáucaso está dominado por temperaturas incluso inferiores a -25 °C, el área de mayor daño se concentró en la Región de Shirak, y gran parte de Lori, siendo las ciudades más afectadas Spitak, Leninakán (Gyumri), Stepanaván, y Kirovakán (Vanadzor), según las autoridades soviéticas las víctimas sumaron 25,000, pero según datos del exterior llegaron a los 50,000, además casi 500,000 quedaron sin hogar, y la industria y el potencial económico de un gran área de Armenia quedó estancada. Las autoridades soviéticas se enfrentaron a otro terrible desastre después del de Chernóbil en la RSS de Ucrania en 1986.

Más de 80 países colaboraron directamente en el apoyo solidario a la Unión Soviética, pero la RSS de Azerbaiyán no tardó en enviar telegramas y notas formales a la sede de gobierno del Kremlin y a la de Ereván, con la vergonzosa nota "Felicidades por tu terremoto", e inclusive las celebraciones no se hicieron esperar en la RSS de Azerbaiyán, muchos crearon lemas, como el de "Allah los ha castigado" o las expresiones "Se lo tenían merecido", sin consideración no sólo de los millares de víctimas armenias, puesto que en el área habitaba una considerable comunidad de griegos, rusos, georgianos, e incluso azeríes.

El 21 de septiembre de 1991, la República Socialista Soviética de Armenia declara formalmente su independencia. En 1992 Armenia en apoyo a Karabaj, declara guerra abierta contra Azerbaiyán, la cual recibe apoyo de Turquía. En la guerra participaron combatientes chechenos y muhadiyes afganos, a pesar de aquello, Armenia logró liberar Artsaj nuevamente y ocupar parte de áreas que históricamente le correspondían, como un cordón de seguridad. Un cese de fuego con mediación rusa fue puesto en práctica en 1994. Desde entonces, Armenia y su vecino han llevado a cabo las negociaciones de paz con mediación de la Organización para la Seguridad y la Cooperación en Europa (OSCE). El estado de Karabaj debe todavía ser determinado y las economías de ambos países han sido perjudicadas por la ausencia de una solución total. No obstante, a pesar del alto desempleo, Armenia ha podido llevar a cabo algunos cambios económicos y en 2006, era clasificada como la 27a. nación "económicamente más libre" en el mundo . Sus relaciones con Europa, el Medio Oriente, y los estados de la Comunidad de Estados Independientes, han permitido que Armenia aumente el comercio. El gas, el petróleo, y otras fuentes de energía vienen a través de dos rutas vitales: Irán y Georgia, con quienes Armenia ha estado manteniendo relaciones cordiales.

En 1990 se celebraron las primeras elecciones legislativas democráticas y en 1991 se eligió el primer presidente de la República. Armenia es miembro de la Comunidad de Estados Independientes (CEI).
El presidente de la República de Armenia es el jefe de Estado, elegido por sufragio universal directo. El presidente nombra al primer ministro, quien a su vez elige a los ministros de gobierno. Desde 2008, Serzh Sargsyan, del Partido Republicano, es el presidente del país.

El máximo órgano legislativo es la Asamblea Nacional Azgayin Zhoghov, con 190 miembros. El parlamento unicameral es actualmente controlado por el conservador Partido Republicano. Los principales partidos de la oposición son Armenia Próspera, la Federación Revolucionaria Armenia, Patrimonio y el Congreso Nacional Armenio.
El poder judicial está conformado por el Tribunal Constitucional, la Corte Suprema, el Procurador General y por las cortes menores. La Constitución actual está vigente desde el 5 de julio de 1995 y fue reformada el 27 de noviembre de 2005. Armenia tiene un sistema legal de sufragio universal a partir de los 18 años de edad.

La política de Armenia se ejecuta en el marco de una república democrática. Según la Constitución de Armenia, el presidente es el jefe de gobierno de un sistema multipartidario. El objetivo principal del gobierno armenio es construir un estilo de democracia parlamentaria occidental como la base de su forma de gobierno. Sin embargo, los observadores internacionales del Consejo de Europa y el Departamento de Estado de los Estados Unidos han puesto en duda la imparcialidad de las elecciones parlamentarias y presidenciales y el referéndum constitucional desde 1995, alegando deficiencias en las votaciones, la falta de cooperación por parte de la Comisión Electoral Central y el deficiente mantenimiento de las listas electorales y los lugares de votación. Freedom House Armenia, en su informe de 2008, adjudicó a Armenia la categoría de "régimen autoritario semiconsolidado" (junto a Moldova, Kósovo, Kyrgyzstán y Rusia) y le dio el puesto número 20 en un "ranking" de 29 naciones en transición, con una "puntuación de democracia" de 5,21 sobre 7 (7 representa el progreso democrático más bajo).[36] Desde 1999, la "puntuación de democracia" de Freedom House para Armenia ha declinado firmemente (desde 4,79 a 5,21).[37] Más aún, Freedom House ranqueó a Armenia como "parcialmente libre" en su informe de 2007, aunque no la categorizó como una "democracia electoral", indicando una ausencia de elecciones relativamente libres y competitivas. De todos modos, parecen haber ocurrido progresos significativos y la elección presidencial de 2008 fue interpretada como ampliamente democrática por parte de la OSCE y los monitores occidentales.[14]

La división administrativa del país implica 10 regiones y 21 ciudades; cada distrito tiene cuerpo legislativo y ejecutivo propio.

Armenia actualmente mantiene buenas relaciones con casi todos los países del mundo, con dos importantes excepciones que son sus vecinos inmediatos, Turquía y Azerbaiyán. Las tensiones fueron creciendo con fuerza entre armenios y azerbaiyanos durante los últimos años de la Unión Soviética. La guerra de Nagorni Karabaj dominó la política de la región durante todo el decenio de 1990. La frontera entre los dos países rivales permanece cerrada hasta el día de hoy, sin que se haya llegado a una solución permanente para el conflicto, pese a la mediación proporcionada por organizaciones tales como la OSCE.

Turquía también tiene un largo historial de malas relaciones con Armenia sobre todo por su negativa a reconocer el genocidio armenio de 1915. El conflicto de Karabaj se convirtió en una excusa para Turquía para cerrar su frontera con Armenia en 1993. No ha levantado el bloqueo a pesar de las presiones internas turcas interesados en los mercados de Armenia y los pedidos de Armenia de abrir las fronteras.

Debido a su posición hostil entre sus dos vecinos, Armenia mantiene estrechos vínculos de seguridad con Rusia. A petición del gobierno de Armenia, Rusia mantiene una base militar en el noroeste de la ciudad armenia de Gyumri como elemento de disuasión contra Turquía. A pesar de ello, Armenia también se ha acercado a las estructuras euroatlánticas en los últimos años. Mantiene buenas relaciones con los Estados Unidos, especialmente debido a la diáspora armenia en ese país, pues (según el censo de 2000) hay 385.488 armenios viviendo en el país.

Armenia es miembro del Consejo de Europa, mantiene relaciones amistosas con la Unión Europea, especialmente con Francia y Grecia, ya que una encuesta en 2005 informó que el 64% de la población de Armenia se manifestó a favor de la adhesión a la UE y varios funcionarios armenios también han expresado el deseo de que su país, a la larga, llegue a convertirse en estado miembro, ya que algunos predicen que se hará una oferta oficial de ingreso en unos pocos años. También se ha examinado qué parte de la sociedad está a favor de unirse a la OTAN. Sin embargo, el Presidente Serzh Sargsyán quiere mantener a Armenia vinculada a la Federación Rusa y a la CEI.

El Ejército Armenio, la fuerza aérea, la Defensa Aérea, y la Guardia fronteriza abarcan las cuatro ramas de las fuerzas armadas de la República de Armenia. Esta estructura de los militares armenios viene dispuesta desde el derrumbamiento de la Unión Soviética en 1991 y con el establecimiento del Ministerio de Defensa en 1992. El Comandante en jefe es el gobernante electo de la república. El Ministerio de Defensa está a cargo de la dirección política, dirigida actualmente por Serzh Sargsyan, mientras que sigue habiendo un comando militar en las manos del Estado Mayor, dirigido por el jefe de personal, que actualmente es el Coronel General Yuri Grigor Khachaturov.

Las Fuerzas armadas activas se componen de cerca de 60 000 soldados, con una reserva adicional de 32 000, y una “reserva de la reserva” de 350 000 tropas. Los guardias fronterizos están a cargo de patrullar las fronteras del país con Georgia y Azerbaiyán, mientras que las tropas rusas supervisan las fronteras con Irán y Turquía. En el caso de un ataque eventual, Armenia está preparada para movilizar a cada hombre y mujer en estados de salud razonables de entre 24 y 55 años, todos ellos con estado de preparación militar.

El Tratado de las Fuerzas Armadas Convencionales en Europa, que establece límites comprensivos en las categorías dominantes del equipo militar, fue ratificado por el parlamento armenio en julio de 1992. En 1993, Armenia firmó la multilateral Convención de armas químicas, que llama para la eliminación eventual de las mismas. Armenia entró a formar parte del Tratado de no proliferación nuclear (NPT) en julio de 1993 y es miembro de la Organización del Tratado de la Seguridad Colectiva (CSTO) junto con Bielorrusia, Kazajistán, Kirguistán, Rusia, Tayikistán y Uzbekistán. También es miembro de la Asociación para la Paz. Es parte de una organización de la OTAN llamada Consejo Social Euro-Atlántico (EAPC) y ha participado a la misión pacificadora dentro de Kosovo. La compañía 46 formó parte de la Coalición internacional que invadió Iraq en 2003.

Armenia está dividida en 11 provincias. Estas se llaman "marzer" ("մարզէր") o en la forma singular "marz" ("մարզ") en armenio.

Armenia está situada en la Transcaucasia, la zona al suroeste de Rusia, entre el mar Negro y el mar Caspio. La Armenia moderna ocupa parte de la Armenia histórica, cuyo centro estaba en el valle del río Araks y la región alrededor del lago Van en Turquía. Armenia limita al Norte con Georgia, al este con Azerbaiyán, al suroeste con la República autónoma de Najicheván, al sur con Irán y al oeste con Turquía. La geografía de la actual Armenia es la de un país sin salida al mar situado en Asia menor.

El terreno armenio es principalmente montañoso, con ríos rápidos y pocos bosques. El clima es continental: veranos calurosos e inviernos fríos. Ningún punto del país está por debajo de los 400 metros por encima del nivel del mar. El monte Ararat, un símbolo armenio, es la montaña más alta de la región y se encuentra en territorio de Turquía, por lo que la máxima altitud de Armenia es el Aragats.

La contaminación producida por productos químicos tóxicos, como el DDT, no contribuyen al enriquecimiento del suelo armenio, que ya de por sí es de mala calidad. Un bloqueo de las comunicaciones, llevado a cabo por Turquía debido al conflicto con Azerbaiyán, ha resultado en un proceso de deforestación.

Armenia está intentando resolver sus problemas ecológicos. Se ha creado un "Ministerio de Protección de la Naturaleza", a la vez que se castigó con impuestos la contaminación del aire y del agua, así como la generación de residuos tóxicos sólidos, cuyas recaudaciones se usan para llevar a cabo proyectos de protección y recuperación ambiental. El gobierno armenio planea cerrar la única planta de energía nuclear, Metzamor, que data de la época soviética, tan pronto como se consigan explotar fuentes de energía alternativas.


Hace veinticinco millones de años, una agitación geológica empujó la corteza terrestre para formar la meseta armenia, creando así la compleja topografía de Armenia. La cadena montañosa del Cáucaso Sur se extiende desde el norte del país, siguiendo hacia el sureste entre el lago Seván y Azerbaiyán, pasando luego por la frontera armenio-azerí hasta Irán. Así situada, las montañas hacen que el viaje norte-sur y sur-norte sea muy dificultoso. El proceso geológico continúa hoy día, y sus más grandes manifestaciones son en algunos casos terremotos y sismos de escala menor. En diciembre de 1988, la segunda ciudad más grande del país Gyumri; anteriormente conocida como Leninakán, sufrió serios daños a causa de un terremoto que mató a más de 25 000 personas.

Su territorio ocupa una superficie de 29 743 km², que a título comparativo es casi exactamente la misma extensión territorial de Bélgica. Aproximadamente la mitad se encuentra a más de 2000 msnm y sólo un 3 % del mismo está por debajo de los 650 metros sobre el nivel del mar (msnm). Las zonas de menor elevación se encuentran en los valles de los ríos Aráks y Debet, al norte del país, con altitudes entre los 380 y 430 msnm respectivamente. La altitud en el Cáucaso Sur varía entre 2640 y 3280 msnm; al suroeste de esta cordillera, se encuentra la meseta armenia, la cual está salpicada de pequeñas sierras y volcanes, algunos de ellos inactivos. El mayor de éstos, el monte Aragáts, de 4095 msnm de altitud, es también el punto más alto del país. La mayor parte de la población vive en la zona oeste y noroeste del país, donde se encuentran las dos mayores aglomeraciones urbanas: la capital Ereván y Gyumri.

Las temperaturas en Armenia dependen, generalmente, de la elevación. Las formaciones montañosas bloquean las influencias moderadoras del clima que el Mar Mediterráneo y del Mar Negro generan, lo que crea una gran diferencia climática entre las estaciones del año. En la meseta armenia, la temperatura media en invierno es de cero grados centígrados, mientras que la media en verano excede los 25 °C. Las precipitaciones medias van desde 250 milímetros al año en el valle del río Araks hasta 800 mm en los puntos más altos del país. A pesar de la dureza del invierno en la mayoría del país, la fertilidad del suelo volcánico de la meseta hizo de Armenia uno de los primeros sitios del mundo con agricultura.

El territorio de la República de Armenia es rico en múltiples especies endémicas. En el valle del Aráks se encuentran plantas halófitas. Desde una altura de 1.400 msnm son comunes las artemisias. En el área montañosa crecen muchos arbustos espinosos y otras plantas. En las montañas altas se presentan plantas xerófilas. Alrededor de los años 1900 los árboles y arbustos cubrían aproximadamente el 25% de la superficie, en 1964 aproximadamente el 15% y en 2005 solamente entre un 8 y un 10%.

En Sangesur, en el sur del país, el límite del bosque llega cerca de 2.400 msnm. El mundo de la planta se asemeja al de las montañas. Hay muchos reptiles, entre ellos el lagarto armenio de roca y algunos venenosos como, por ejemplo, las víboras. También arácnidos, tales como los escorpiones. En los valles húmedos viven cerdos salvajes, chacales, ciervos, visones y águilas; en las estepas sobre todo roedores de las montañas; y en los bosques osos pardo sirios, gatos salvajes y lobos. En el área protegida Chosrow todavía se pueden encontrar linces y algunos leopardos del cáucaso.

El nombre latino del damasco o albaricoque se deriva de Armenia. El albaricoque tiene gran fama y es todo un símbolo nacional armenio, representado por el color de la banda inferior en la bandera de Armenia.

Hasta su independencia, la economía de Armenia se basó en la producción industrial de productos químicos, electrónica, maquinaria, alimento procesado, caucho sintético y textiles, era además altamente dependiente en recursos externos. La agricultura contribuía solo con el 20% del Producto Interno Bruto y el 10% del empleo antes de la desintegración de la Unión Soviética en 1991. La república había desarrollado un sector industrial moderno, máquinas de herramientas que proveían, textiles, y otros productos manufacturados a las repúblicas cercanas a cambio de las materias primas y de energía.

Las minas armenias producen cobre, cinc, oro, y plomo. La mayor parte de la energía se genera con combustible importado de Rusia, incluyendo gas y combustible nuclear (para la única planta de energía atómica); la principal fuente de energía doméstica es hidroeléctrica.

Un auge en curso de la construcción, especialmente en la ciudad de Ereván, ha mantenido el desarrollo económico de Armenia en dígitos dobles. Como otros estados recientemente independientes de la anterior Unión Soviética, la economía de Armenia sufre de la herencia de una economía planificada centralmente y de la interrupción soviética. La inversión y la ayuda soviética en la industria armenia ha desaparecido virtualmente, de modo que pocas industrias soviéticas importantes todavía funcionan allí. Además, los efectos del terremoto del año 1988 en Spitak, en el que murieron más de 25 000 personas y otras 500 000 quedaron sin hogar, todavía se están sintiendo. El conflicto con Azerbaiyán por el enclave de Nagorno-Karabaj no se ha resuelto. El cierre de las fronteras azerís y turcas ha devastado la economía, porque Armenia depende de fuentes exteriores de energía y la mayoría de las materias primas provienen del exterior. Las rutas de tierra a través de Georgia y de Irán son inadecuadas o no fiables. El PIB cayó en casi 60 % a partir de 1989 hasta 1992-1993. La divisa nacional, la copita, sufrió de hiperinflación durante sus primeros años después de su introducción en 1993.

Sin embargo, el gobierno ha podido hacer vastas reformas económicas que han resultado en una inflación más baja y en un crecimiento constante. El cese al fuego en 1994 en el conflicto de Nagorno-Karabaj también ha ayudado a la economía. Armenia ha tenido un fuerte desarrollo económico desde 1995, y la inflación ha sido insignificante para los años recientes. Los nuevos sectores, tales como el de piedras preciosas que procesa y fabrica joyería, tecnologías de información y de comunicación, e incluso turismo están comenzando a suplir sectores más tradicionales en la economía, tal como la agricultura.
Este progreso económico constante ha significado un aumento de la ayudad por parte de las instituciones internacionales. El Fondo Monetario Internacional ("FMI"), el Banco Mundial, el banco europeo para la reconstrucción y el desarrollo ("EBRD") y otras instituciones financieras internacionales (IFIs) y los países extranjeros están ampliando concesiones y préstamos considerables. Los Préstamos a Armenia en 1993 excedieron los 1,1 mil millones de dólares. Estos préstamos se dirigen hacia la reducción del déficit presupuestario, la modernización; la privatización de negocios; la energía; la agricultura, la transformación de alimentos, el transporte, y los sectores de la salud y de la educación; y la rehabilitación en curso en la zona del terremoto. El gobierno ingresó a la organización mundial del comercio el 5 de febrero de 2003. Pero una de las fuentes principales de inversiones directas extranjeras sigue siendo la Diáspora armenia, que financia una parte importante de la reconstrucción de la infraestructura y de otros proyectos públicos. Siendo un estado democrático cada vez mayor, Armenia también espera conseguir una mayor ayuda financiera del mundo occidental.

En junio de 1994 se aprobó una ley liberal en favor de la inversión extranjera, y en 1997 se adoptó una ley sobre la privatización, así como un programa sobre la privatización del estado. El progreso continuado dependerá de la capacidad del gobierno de consolidar su gerencia macroeconómica, incluyendo el aumento en el tributo de impuestos, la mejora en el clima de inversión, y dando grandes pasos en la lucha contra la corrupción.

En la carta internacional 2005 del CPI de la transparencia (índice de la opinión de la corrupción), Armenia se calificó con un valor de 88 (en una escala de 1 a 158), continuando como uno de los estados menos corruptos entre las antiguas repúblicas soviéticas. Según el informe del desarrollo humano de la O.N.U en 2005 , Armenia tiene un índice de desarrollo humano (IDH) de 83 (en una escala de 1 a 177) el más alto entre las repúblicas transcaucásicas. En el índice 2006 de libertad económica, Armenia se alineó 27mo, al lado de Japón y delante de países como Noruega, España, Portugal e Italia. Este resultado pone a Armenia en la categoría de los países “más liberales” , convirtiéndose en el estado más libre económicamente de la Comunidad de Estados Independientes.

Armenia no posee reservas de petróleo o gas natural y lo importa de Rusia por la frontera de Georgia. El gas alimenta las centrales térmicas de Ereván (242 MW) y Hrazdan (1.110 MW), que producen el 24% de la energía del país. En 2007, se terminó la primera fase del gasoducto Irán-Armenia, entre Tabriz y la frontera armenia, y está en proyecto seguir hasta el centro del país para sustituir el gas de Gazprom por gas de Irán, pero los conflictos de la región limitan mucho el uso del gas iraní.

La central nuclear Metsamor (408 MW), la única del país, construida durante la época sovietica y que funciona desde 1976, proporciona el 43% de la energía del país. Hay planes de sustituirla por una nueva, debido a su antigüedad.

Armenia posee además 10 centrales hidroeléctricas que proporcionan el 33% de la electricidad, agrupadas en dos cuencas: la del Complejo Hidroeléctrico de Sevan–Hrazdan, también llamada Sevan–Hrazdan Cascade, a lo largo del río Hrazdan y sus tributarios, entre el lago Sevan y la ciudad de Ereván, consistente en siete centrales hidroeléctricas con una potencia combinada de 565 MW, alimentadas por canales y tubos con agua del lago, y la del río Vorotán, también llamada ContourGlobal Hydro Cascade, consistente en cinco embalses y tres plantas hidroeléctricas con una potencia combinada de 404 MW.

Se halla en proyecto la central geotérmica de Jermaghbyur en la provincia de Syunik', capaz de producir 150 MW.

En 2008 se inaugura la central eólica Lori 1, la única del país, en la provincia septentrional de Lorri, con una potencia combinada de 2,6 MW, construida en colaboración con Irán.

Armenia tiene una población de 3.215.800 (censo de abril de 2006), y es la segunda mayor densidad de población de las ex Repúblicas Soviéticas. Se ha producido un problema de disminución de la población debido al aumento en los niveles de emigración tras la desintegración de la URSS. Las tasas de emigración y de la disminución de población, sin embargo, han disminuido drásticamente en los últimos años, con una moderada afluencia de los armenios que regresan a Armenia, se espera que esta tendencia continúe. Armenia, de hecho, espera que reanude su crecimiento positivo de la población por 2010.

El 97,9% de la población es de origen étnico armenio. Los yazidíes constituyen el 1,3%, y los rusos el 0,5%. Otras minorías incluyen los asirios, ucranianos, griegos, kurdos, georgianos, y bielorrusos. También hay pequeñas comunidades de valacos, morduinos, osetios, udi, y Tats. También existen minorías de polacos y alemanes del Cáucaso, aunque están muy rusificados. Durante la era soviética, los azeríes fueron históricamente la segunda población más grande del país (alrededor de 10% en 1939). Sin embargo, debido a las hostilidades con el vecino Azerbaiyán en la disputada región de Nagorno Karabaj, prácticamente todos ellos emigraron de Armenia. Por el contrario, Armenia recibió una gran afluencia de refugiados armenios de Azerbaiyán, dando así una población armenia de carácter homogéneo.

Armenia tiene una diáspora muy grande (ocho millones según algunas estimaciones, que supera con creces los tres millones de habitantes de la propia Armenia), con comunidades existentes en todo el mundo. Las comunidades más numerosas se pueden encontrar: en Argentina, Australia, Canadá, Chipre, Chile, Colombia, también en la Federación Rusa, Estados Unidos, Francia, Georgia, Irán, Israel en donde cerca de mil armenios residen en el barrio armenio de la Ciudad Vieja de Jerusalén, un remanente de una comunidad otrora mayor. Líbano, Siria, Turquía (en su mayoría dentro y alrededor de Estambul) dentro de los que hay que considerar los 40 000 a 70 000 armenios que aún viven en el país, Uruguay y Ucrania. Además, cerca de 130 000 armenios viven en la disputada región de Nagorno-Karabaj, donde constituyen la mayoría.

La lengua oficial del país es el armenio, y como consecuencia de la etapa soviética, el ruso sigue estando bastante extendido, sobre todo en los ámbitos urbanos. Buena parte de la población urbana (sobre todo en Ereván) puede considerarse bilingüe.

La religión predominante en Armenia es el cristianismo. Las raíces de la Iglesia Armenia comienzan en el siglo I. Según la tradición, la Iglesia Armenia fue fundada por dos de los apóstoles de Jesús, Judas Tadeo y Bartolomé, quienes predicaron el cristianismo en Armenia entre los años 40 y 60. Debido a estos dos apóstoles, el nombre oficial de la Iglesia Armenia es Iglesia Apostólica Armenia. Armenia fue la primera nación en adoptar el cristianismo como religión del Estado, en el año 301. Cerca del 93% de cristianos armenios pertenecen a la Iglesia Apostólica Armenia, llamada también "Iglesia Gregoriana", es una de las Iglesias ortodoxas orientales, al igual que la Iglesia copta y la siriaca, y agrupada entre las llamadas miafisitas. Esta iglesia se considera Iglesia Ortodoxa (aunque no debe confundirse con la Iglesia Ortodoxa de cuño griego), por haber mantenido la ortodoxia de la doctrina cristiana en conformidad con los padres de la Iglesia. Armenia también tiene una población de católicos de rito armenio de unos 180 000 miembros y de protestantes y seguidores evangélicos de la religión tradicional armenia. Los kurdos de Yazidi, que viven en la parte occidental del país, practican Yazidismo. La Iglesia católica de rito armenio tiene su sede en Bzoummar, Líbano.
Armenia se sostiene en parte por una muy importante diáspora armenia alrededor del mundo: en Rusia (2,5 millones), en América del Norte (1,5 millones), en África (15 000), en Siria y Líbano (120 000), en la Unión Europea (500 000), principalmente en Francia; y en América Latina, (125 000), principalmente asentados en Argentina, Brasil, Uruguay, Venezuela, Chile y México.

Los armenios tienen su propio alfabeto e idioma distintivos. El alfabeto fue inventado por Mesrob Mashtóts y consiste en 38 letras (con 36 sonidos fonéticos), dos de las cuales fueron añadidas durante el período de Cilicia. El 96% de los habitantes del país habla armenio, mientras el 75,8% de la población habla además ruso como resultado de la política lingüística soviética. La tasa de alfabetización adulta en Armenia es del 98% . La mayoría de los adultos de Ereván pueden comunicarse en ruso, mientras la popularidad del inglés crece. En 1914, Grigori Nikoláievich Neúimin bautizó con el nombre de Armenia al asteroide 780.

La literatura comenzó en Armenia alrededor del 400 a. C. Crearon la mayoría de las artes literarias cerca de Moses de Khorene, en el siglo V. Con los años, tanto los elementos de la literatura así como las historias y los mitos fueron cambiando a través de las generaciones. Durante el siglo XIX, el escritor Mikael Nalbandián trabajó para crear una nueva identidad literaria armenia. El poema de Nalbandián “La canción de la muchacha italiana” se convirtió en la letra del himno nacional armenio "Mer Hayrenik".

La arquitectura armenia ha desarrollado un estilo propio característico desde el siglo IV que se manifiesta en las iglesias y monasterios construidos en el país a lo largo de su historia, pero también en las construcciones realizadas por las comunidades armenias que han abandonado el país en los últimos siglos.

Armenia es la madre patria del compositor y director clásico contemporáneo Aram Jachaturián. Jachaturián es uno de los grandes músicos del siglo XX. Su carrera se desarrolló principalmente en Moscú. Llegó a popularizarse gracias a la selección de algunos pasajes de su obra por el genial Stanley Kubrick para la banda sonora de "2001 Una Odisea Espacial" (Las Hilanderas del adagio de "Gayaneh"). Destacan entre sus obras, "Gayaneh", "Espartaco" y su contribución con grandes partituras al ballet soviético. También forman parte del patrimonio universal colectivo los trepidantes y vertiginosos compases de la "Danza del sable", el último movimiento de "Gayaneh".

En la actualidad, la cabeza más visible de la música armenia es Arto Tunçboyaciyan, Samvel Yervinyan y el grupo System of a Down, cuyos integrantes son de origen armenio, formada en Los Ángeles en el año 1995. Compuesta por Serj Tankian, Daron Malakian, Shavo Odadjian y John Dolmayan. Dos exponentes de la música armenia propiamente dicha son Levón Minassián y Armand Amar. Otro reconocido cantante de origen armenio es el francés Charles Aznavour.

Armenia forma parte de la UER desde el 2005, hecho que le permitió participar en el Festival de la Canción de Eurovisión al año siguiente pasando la semifinal con muy buenos resultados hasta la edición de 2011 en Düsseldorf, aunque en su breve participación es uno de los países con mejores resultados promedio. Armenia también ha participado en el Festival de la Canción de Eurovisión Junior, habiendo organizado el certamen de la edición de 2011 en la que participaron 13 países. Además Armenia tiene una larga tradición de producción de música regional y tradicional.

La Gastronomía de Armenia está formada por los platos y tradiciones culinarias de los pueblos armenios, incluido los integrantes de la diáspora armenia. La historia de Armenia muestra que tras la destrucción del Imperio seléucida surgió el primer estado armenio independiente, fue fundado en el 190 a. C. por Atarxias, cuyos sucesores se conocen como dinastía Artáxida y siendo después en el siglo XX parte del estado soviético, hecho este último que marcó algunas costumbres culinarias en este país y fijó un cambio de las tradiciones culinarias de muchos siglos. La cocina armenia se caracteriza por estar entre la cocina mediterránea y la del Cáucaso, se trata de un conjunto de elaboraciones características de una población nómada que vive en una región fría. Con grandes influencias de la cocina del Oriente Medio, de Rusia y de los Balcanes.

La Granada es una de las frutas que simbolizan a Armenia y muchas tradiciones armenias se llevan a cabo representando a esta fruta. Por ejemplo en las bodas armenias es tradición lanzar una granada contra la pared, buscando la bendición de sus hijos. La granada también es la diversidad dentro de la unión, representada por los granos. Y, yendo más allá, es un homenaje a los armenios de la diáspora. 

En Armenia se juegan muchos tipos de deportes, entre los que destacan la lucha libre, el levantamiento de pesas, el judo, el fútbol, el tenis de mesa, el ajedrez y el boxeo. Armenia es un terreno montañoso y ofrece la oportunidad de que algunos deportes como el esquí y el alpinismo sean practicados masivamente. Al ser un país sin litoral, los deportes acuáticos sólo puede ser practicados en los lagos, en especial el lago Sevan. Competitivamente, Armenia ha tenido éxito en halterofilia y lucha libre.

Armenia es también participante activa en la comunidad deportiva internacional con la plena pertenencia a la Unión de Asociaciones de Fútbol Europeas y la Federación Internacional de hockey sobre hielo. También es sede de la Pan - Juegos de Armenia.

Armenia es una auténtica potencia mundial en el ajedrez. El campeón mundial Tigran Petrosian fue uno de los mayores exponentes del ajedrez de tipo posicional. En la Olimpiada de ajedrez celebrada en Turín en 2006, el equipo masculino se proclamó campeón y el equipo femenino se clasificó en séptimo lugar. Levon Aronian, Vladímir Akopián, Karen Asrian, Smbat Lputian, Gabriel Sargissian y Artashes Minasian conformaron el equipo masculino. Lilit Mkrtchian, Elina Danielian, Nelli Aginian y Siranush Andriasian formaron el femenino. El equipo técnico lo formaban Arshak Petrosian y Tigran Nalbandian.










</doc>
<doc id="22041" url="https://es.wikipedia.org/wiki?curid=22041" title="Viajeros y magos">
Viajeros y magos

Viajeros y magos es una película filmada en Bután y hablada en idioma dzongkha. Esta película muestra los paisajes naturales de este país ubicado entre India y Tíbet. También muestra la apacible y sencilla vida de sus ciudadanos.

La trama cuenta la historia de Tshewang Dendup, un burócrata del gobierno que, descontento por su trabajo en un distrito remoto desea emigrar a los Estados Unidos de donde ha escuchado que se gana más recogiendo frutas en un día que trabajando en su país durante un mes. Un amigo suyo que vive en Nueva York ha prometido ayudarlo, pero debe llegar en una fecha muy cercana. Al tratar de llegar a Thimphu, la capital, pierde el único transporte público de la semana. Esperando que algún otro vehículo pase escucha las historias de un monje sobre magia y viajes.


</doc>
<doc id="22054" url="https://es.wikipedia.org/wiki?curid=22054" title="Autopista">
Autopista

Una autopista es una pista de circulación para automóviles y vehículos terrestres de carga (categóricamente los vehículos de motor) y de pasajeros. 

Debe ser rápida, segura, y admitir un volumen de tráfico considerable, y se diferencia de una carretera convencional, en que la autopista dispone de más de un carril para cada sentido con calzadas separadas (no confundir con la autovía española diseñada en ese país en 1988). 

Las primeras autopistas construidas con esta configuración se hicieron en Italia durante los años 1920. 

Entre los tipos de carreteras, representan las vías que más tráfico rodado pueden soportar y en las que este alcanza mayores velocidades, por lo que suelen representar los ejes principales de la red viaria de un país desarrollado (exceptuando a España, que implantó, por norma general, la autovía en esos casos).

La Organización para la Cooperación y el Desarrollo Económicos define autopista como:

Para poder ser calificada como autopista, una vía de circulación debe reunir las siguientes características:


La primera autopista del mundo se construyó en Italia, en 1921, entre Milán y Varese. Hoy forma parte de las autopistas A-8 y A-9 italianas. Tenía dos calzadas separadas, pero aún no contaba con cruces a distinto nivel.

La primera autopista española fue la autopista A-3 entre Madrid y Vallecas, seguida por la autopista A-6, que construyó entre 1964 y 1967 (inaugurada este último año), entre los municipios madrileños de Las Rozas de Madrid y Collado Villalba.
La primera de peaje se construyó en 1969 y se extendía entre Barcelona y Mataró.
Le siguieron las autopistas AP-8 (Bilbao - Behovia), la AP-6 (Collado Villalba - Adanero) y la "Y" asturiana A-8 / A-66 (Gijón - Oviedo - Avilés). En 1972 se finalizó la AP-4, la autopista del Sur de Sevilla a Cádiz. Finalmente, los dos accesos principales a las terminales 1, 2 y 3 del aeropuerto de Madrid-Barajas, se acabaron convirtiendo en autopistas. 

Una de las últimas fue el tramo comprendido entre Benavente y Padornelo (ambos en la provincia de Zamora) de la A-52 denominada en su totalidad como autovía de las Rías Bajas, y en Zamora, y "autovía das Rías Baixas" en las provincias de Orense y Pontevedra. Las autopistas M-30, M-40 y M-50 en la comunidad de Madrid. La autopista autonómica M-45, en la misma comunidad. Parcialmente, la autovía del Cantábrico, A-8, se torna físicamente autopista en distintos puntos, caso por ejemplo de la variante de Gijón. El tramo Gijón-Avilés de la A-8 entero, junto con el tronco a Oviedo de la A-66, la autovía Ruta de la Plata, construidos a la vez, y conocidos popularmente como "autopista Y", ya mencionada. 

Además, el conjunto de los tramos (Oviedo-Mieres), (Mieres-Pola de Lena) y (Pola de Lena-Campomanes) de la A-66, que es una autopista. También la autopista Sevilla-Huelva (la A-490, la A-22, la variante norte de Zaragoza, la A-52 (Benavente-Padornelo) y otras,aunque generalmente, se encuentran cerca de (o conectadas a) autopistas de peaje, no siendo el caso de esta última, por ejemplo.

Las autovías españolas históricamente siguen más o menos el mismo recorrido que tenían las calzadas romanas en los tiempos de Hispania. La existencia de un tipo de vía cuya definición fue acuñada en España, y la evolución de éstas con el tiempo, frecuentemente llevan a confusión, por lo que se enumeran algunas, especialmente:


Por poner algunos de los ejemplos que suelen generar confusión. De hecho, por ejemplo se puede mencionar, que Extremadura es una comunidad autónoma, que no posee ni un solo kilómetro de autopistas, ni siquiera de peaje.

Una autovía es un tipo de carretera interurbana española distinto de la autopista, y de hecho de un nivel inferior. El nombre "autovía", en base, viene a definir a la autovía como una vía interurbana desdoblada de alta velocidad para vehículos. No tiene la consideración de pista para automóviles, encerrada en el término Autopista, ni mucho menos. Las únicas directrices obligatorias de cara a la construcción y conservación de las autovías, están regidas por su definición en la Ley de Carreteras española de 1988. En cambio en el ámbito de las autopistas, esto se vuelve más complejo, ya que hasta la definición que se le pueda dar en esa misma ley, puede llegar a venir fuertemente condicionada por los estándares internacionales del momento, especialmente si la autopista en cuestión forma parte de un itinerario europeo. De hecho, frecuentemente se estima que es por esto, unido a otra fuerte crisis a comienzos de los ochenta lo que pudo llevar a ese "re-diseño" de la autovía. De los tres apartados (letras) de la definición de la OCDE, situada más arriba, la autovía, como tal y por definición, está exenta de obligaciones en el cumplimiento del apartado (c), lo que no implica que en algunos casos, cumpla algunas de manera opcional, lo que explicaría aquellas auto-imposiciones en las autovías de última generación. Además aunque se llegue al punto de casi hacerlo, el trazado y sus características, no tiene porque conllevar las mismas "prescripciones" que el de una autopista. Sigue estando permitido construirlas sobre las carreteras antiguas, aprovechándolas para una de las plataformas, símplemente re-asfaltando encima, aunque en la actualidad es solo "un recurso ante la necesidad". Y pintando posteriormente la señalización horizontal, que sí se mantiene, si no siempre igual, similar a la de la autopista. En una autovía y en una autopista de peaje (siempre a criterio de la concesionaria) está permitido, corregir baches, badenes, grietas y demás defectos, mediante la colocación de parches de asfalto o mediante el uso de pastas de alquitrán, en cambio en las autopistas, esto puede ser una conducta ciertamente "rastrera" y provocar de facto, que la autopista se vuelva peligrosa.

En el transporte, la demanda puede medirse en el número de viajes efectuados o en la distancia total recorrida en todos los trayectos (por ejemplo, pasajeros-kilómetros para transporte público o vehículos-kilómetros de viaje para transporte privado ). Se considera que la oferta es una medida de capacidad. El precio del bien (viaje) se mide utilizando el costo generalizado de los viajes, que incluye tanto dinero como tiempo .

El efecto de los aumentos en la oferta (capacidad) es de particular interés en la economía del transporte (ver demanda inducida ), ya que las consecuencias ambientales potenciales son significativas (ver externalidades a continuación).

Además de proporcionar beneficios a sus usuarios, las redes de transporte imponen externalidades positivas y negativas a los no usuarios. La consideración de estas externalidades -en particular las negativas- es parte de la economía del transporte. Las externalidades positivas de las redes de transporte pueden incluir la capacidad de prestar servicios de emergencia , el aumento del valor de la tierra y los beneficios de la aglomeración . Las externalidades negativas son amplias y pueden incluir la contaminación del aire local , la contaminación acústica , la contaminación lumínica , los peligros de seguridad , la separación de la comunidad y la congestión . La contribución de los sistemas de transporte al cambio climático potencialmente peligroso es una externalidad negativa significativa que es difícil de evaluar cuantitativamente, lo que hace difícil (pero no imposible) incluir en la investigación y el análisis basados en la economía del transporte. La congestión es considerada una externalidad negativa por los economistas. 
Las carreteras son fuentes prolongadas de contaminación lineal .

El ruido de la carretera aumenta con la velocidad de operación, de modo que las carreteras principales generan más ruido que las calles arteriales . Por lo tanto, se esperan efectos considerables sobre la salud del ruido de los sistemas de autopistas. Existen estrategias de mitigación del ruido para reducir los niveles de sonido en los receptores sensibles cercanos . La idea de que el diseño de la carretera podría ser influenciado por consideraciones de ingeniería acústica surgió por primera vez alrededor de 1973. 

Problemas de calidad del aire : Las carreteras pueden contribuir con menos emisiones que las arterias que transportan el mismo volumen de vehículos. Esto se debe a que la operación de alta velocidad constante crea una reducción de emisiones en comparación con los flujos vehiculares con paradas y arranques. Sin embargo, las concentraciones de contaminantes atmosféricos cerca de las carreteras pueden ser mayores debido al aumento de los volúmenes de tráfico. Por lo tanto, el riesgo de exposición a niveles elevados de contaminantes atmosféricos de una carretera puede ser considerable, y se amplifica aún más cuando las carreteras tienen congestión de tráfico .

Las nuevas carreteras también pueden causar la fragmentación del hábitat , fomentar la expansión urbana y permitir la intrusión humana en áreas previamente intactas, así como (contrarrestando) aumentar la congestión, aumentando el número de intersecciones.

También pueden reducir el uso del transporte público , lo que conduce indirectamente a una mayor contaminación.

Los carriles de vehículos de alta ocupación se están agregando a algunas carreteras nuevas / reconstruidas en Norteamérica y otros países alrededor del mundo para fomentar el carpool y el transporte masivo. Estos carriles ayudan a reducir el número de coches en la autopista y por lo tanto reduce la contaminación y la congestión del tráfico promoviendo el uso del carpooling para poder utilizar estos carriles. Sin embargo, tienden a requerir carriles dedicados en una carretera, lo que los hace difíciles de construir en zonas urbanas densas donde son los más eficaces.

Para abordar la fragmentación del hábitat, los cruces de vida silvestre se han vuelto cada vez más populares en muchos países. Los cruces de vida silvestre permiten a los animales cruzar con seguridad las barreras humanas como las carreteras. 



</doc>
<doc id="22063" url="https://es.wikipedia.org/wiki?curid=22063" title="Región de la Araucanía">
Región de la Araucanía

La región de La Araucanía es una de las quince regiones en que se divide Chile. Su capital es la ciudad de Temuco. Ubicada al centro del país, limita al noroeste y norte con la Biobío, al este con Argentina, al sur con la región de Los Ríos y al oeste con el océano Pacífico. Cuenta con una superficie de 31 858 km² y una población de 1 046 322 habs. según la proyección del INE del año 2014, siendo la novena en Chile y la tras las regiones Metropolitana, del Biobío, de Valparaíso y del Maule.

La región está compuesta por las provincias de Cautín y Malleco. Las ciudades más importantes de la región son Temuco, Angol, Villarrica, Victoria, Lautaro y Pucón.

Su principal centro urbano es el Gran Temuco con 358 541 habitantes, seguido de Villarrica con 55 478 habitantes.

La Araucanía hace referencia al «lugar que habitan los araucanos», nombre con el que los españoles designaban a los mapuche. La voz «araucano» es una hispanización del término usado por los incas para referirse a los mapuche ("promaucaes").

La bandera de la región de La Araucanía nunca ha sido declarada oficial. Consiste en un estandarte de tres franjas azul (superior), blanca (medio) y roja (inferior), con un escudo diseñado a base de dos cuarteles rojo y negro, adornados con seis "guemiles" blancos y un "trapelacucha" (joya mapuche) del mismo color, rodeados de una guirnalda de copihue y coronados por un monte nevado escoltado de araucarias, aunque en el estandarte de la intendencia de la región se ve una bandera blanca con el mismo escudo y el "trapelacucha".

En el último tiempo se ha popularizado el uso del "Wenufoye" o Bandera Mapuche en la región, tanto así que es utilizado de forma oficial en decenas de municipios de la región e incluso en la intendencia y gobernaciones.

El comienzo de la historia del poblamiento humano en esta región se remonta a alrededor de 11 000 años antes de la era cristiana en época glacial final (Periodo paleoindio).

Ella estuvo relacionada al término de la glaciación wisconsiense. Los primeros pobladores de estos lugares australes estaban organizados en grupos familiares (bandas) de cazadores y recolectores (as) que practicaban la movilidad residencial y que fueron exitosos en la colonización de los distintos ecosistemas americanos, llegando al actual sur de Chile hace al menos 13000 años donde comenzaron a aprender a habitar el bosque siempreverde del valle y practicaron excursiones esporádicas a la costa del Pacífico.

Durante este período se produjeron cambios profundos en las sociedades que poblaron esta zona centro sur, las que poseían mayor experiencia en organización social y en tecnologías especializadas para la recolección y la caza, producto de un conocimiento más íntimo y la expresión de conductas seguramente flexibles para su establecimiento en distintos ecosistemas producto del cambio ambiental global que se había producido en los milenios anteriores. 
Hace 4800 años empezaron a llegar pueblos recolectores dejando varios sitios arqueológicos en la zona, en Quillén, Quino, Isla Mocha, y Península de Pucón. Estos grupos traían técnicas acabadas de caza, sin embargo las técnicas de recolección las adoptaron de grupos previos, del paleoindio. No se han encontrado restos arqueológicos de Paleoindio pero se sabe que estaban ya que hay restos más al sur, en Monte Verde (11 000 a. C.). Bajo estos restos se encontraron otros de al menos del periodo 31 000 a. C.
Los sitios arqueológicos en la región corresponden a los llamados aleros que son lugares altos en donde se dominan los sitios de acceso y las fuentes de agua, y permitían esbozar tácticas de caza y defensa. En ellos, se encontraban puntas de flecha y otros instrumentos líticos. Los aleros están generalmente en rocas o cuevas basálticas, las que además proporcionaban la materia prima para hacer sus herramientas líticas.
Dentro de los artículos líticos se encontraban cuchillos, raspadores y puntas de proyectil. Los aleros dominaban zonas de bosque en los cuales los habitantes de la zona recolectaban frutos y plantas y cazaban.
En este periodo ya había reconocimiento y conocimiento de territorio. Esto se infiere porque usaban distintas puntas de proyectil en el valle o en la costa.

Durante el verano, se movían a sectores altos de la cordillera de los Andes para aprovisionarse del piñón, el fruto del pehuén, recolectar obsidiana y riolita desde sectores cercanos a volcanes para hacer sus instrumentos, y cazar a la fauna que subía en busca de dehesas verdes. Esta actividad, la veranada, se efectúa en la zona hasta la actualidad.

El periodo arcaico duró desde el 8000 a. C. hasta el 700 a. C., aproximadamente.

El período agroalfarero temprano es un período arqueológico usado para la clasificación de culturas arqueológicas de los pueblos originarios del norte argentino y norte y centro de Chile. Se desarrolla entre el 500 a. C. y el año 650 de nuestra era.

La cultura Pitrén es un complejo cultural agroalfarero temprano de Chile. Las comunidades comprendidas bajo esta denominación, se ubicaron entre el río Bío Bío y el lago Llanquihue, ubicado en la región de los Lagos. 
Ya hacia el año 600, estos grupos iniciaron el cultivo del maíz y de la papa, mediante el despeje de espacios entre los bosques, sin abandonar sus prácticas de caza y recolección.
La cerámica, elemento particular que ha caracterizado a estas comunidades, es la más antigua de la zona. En ella se encuentra el "ketrumetawe", jarro con forma de ave, símbolo de la mujer casada; además de otra diversidad de jarros de estructuras globulares, con cuello cilíndrico y recto, con asa en el cuello o con asa recta que termina en una figura de animal, tales como patos, ranas o sapos en el extremo. La cerámica de Pitrén denota en un evidente contacto con las culturas de El Molle y Llolleo. Decoraban las piezas con un procedimiento denominado "pintura negativa".
Los estudiosos parecen coincidir en que Pitrén constituye la primera expresión agroalfarera en el sur de Chile.

Es muy probable que el complejo Tirúa haya tenido sus raíces en el complejo diaguita de más al norte, o en el estilo Aconcagua del valle central, los tipos alfareros de El Vergel, Tirúa y Valdivia están vinculados entre sí, como también con el complejo Aconcagua, compartiendo tradiciones comunes, dado que muchos elementos simbólicos propios de su gráfica, como también la forma de los objetos en los cuales se instala dicha gráfica, tienden a ser los mismos, como se puede apreciar al comparar los objetos que se conservan en museos y colecciones privadas.

El mayor problema lo constituye el llamado tipo Tirúa, debido a que, al describirlo, Latcham no entregó fotografías de las piezas sino solo algunos dibujos no muy bien logrados, reproducidos en su libro "La alfarería indígena chilena" y cuya descripción en dicho texto parece insuficiente. Además, las piezas no son conocidas en las colecciones y museos chilenos, como tampoco en colecciones argentinas donde pudiesen haber sido llevadas, por lo que el problema subsiste.

La cultura mapuche surge de estas culturas anteriores, representada entonces en sus antepasados Pitrén y El Vergel. Al paso del tiempo, en cientos de años se fueron expandiendo esos rasgos culturales y homogeneizándose, hasta llegar al año mil de nuestra era a constituir lo que ya puede ser reconocido plenamente como cultura mapuche.

Tras un breve contacto con el Imperio inca que jamás llegó a la zona, sus habitantes fueron llamados promaucaes (hanan runasimi "purum auca", 'gente salvaje'). La influencia posterior de los incas tampoco fue pequeña y adoptaron numerosos productos del “enemigo” que no logró ingresar a su territorio.

En 1536, tras los incas, llegaron los españoles llamados "huincas" por los mapuche (mapudungún "we inka", 'nuevo inca').
El contacto definitivo con los españoles se produjo en la batalla de Reinohuelen en 1536. Este pueblo fue conocido por los conquistadores españoles con el nombre genérico de «araucano», usado por primera vez por Alonso de Ercilla en 1589. Arauco, es derivado de la palabra "ragko" (mapudungún "ragko", 'agua gredosa blanca'), sinónimo de Malleco, Malloco o Mallarauco. 

Entre ellos se llamaban por gentilicios que aludían a las diferentes localidades de origen (p. ej. purenes), o a puntos cardinales de los que procedían, respecto de los referentes (picunches ("pikun", 'Norte'), huilliches ("willi", 'Sur')).

En estas tierras habitaban más de 180 mil indígenas, compuestos por los pueblos pehuenches y mapuches. Dicho territorio se había mantenido rebelde a partir de la denominada Guerra de Arauco ante el dominio español durante la Conquista de Chile y todo el período colonial de Chile, sin que ningún bando venciera claramente.

Luego de la independencia de Chile, ya en el período republicano, se ordenó la celebración de un parlamento general con los mapuches que habitaban al sur del río Biobío, con la finalidad de acordar el estatuto que regularía las relaciones entre la naciente república y el pueblo mapuche; realizándose así el Parlamento de Tapihue en enero de 1825. Sin embargo posteriormente sucedieron diversos hechos que obligaron al estado chileno a destinar recursos a la zona de la frontera.

Además, durante la Revolución de 1851, el general José María de la Cruz, líder del movimiento golpista, reclutó a varios loncos mapuches y sus clanes para alzarse en armas contra el gobierno, esto lo pudo lograr gracias a la relación de amistad que mantenía el general con los caciques, entre ellos Colipí. Cuando su insurrección fue aplastada por el general Manuel Bulnes, los caciques en vez de rendirse junto a De la Cruz se replegaron a la frontera junto con varios miembros descolgados del antiguo ejército, dedicándose al pillaje y al robo de ganado, por los siguientes 4 años. Esto motivó al gobierno a movilizar al segundo batallón del segundo de línea, hasta enero de 1856.

Un aventurero francés, llamado Orélie Antoine de Tounens, se dirigió a la zona de la Araucanía, hizo contacto con el lonco Quilapán, al que entusiasmó con su idea de fundar un estado para el pueblo mapuche como forma de resistencia al ejército chileno durante la época final de la Guerra de Arauco y el 17 de noviembre de 1860 fundó el "Reino de la Araucanía" y se proclamó rey bajo el título de Orélie Antoine I. Durante los días siguientes, Tounens promulgó la constitución del reino y el 20 de noviembre del mismo año declaró la anexión de la "Patagonia", estableciendo como límites el río Biobío por el norte, el océano Pacífico por el oeste, el océano Atlántico por el este desde el río Negro hasta el estrecho de Magallanes, límite austral del Reino. El gobierno chileno, bajo el mando del presidente José Joaquín Pérez, ordenó el arresto de Tounens en enero de 1862, siendo trasladado a Los Ángeles donde fue recluido y luego enviado a Europa.

En 1879 se da el último gran levantamiento mapuche en la actual Región de la Araucanía debido a la migración de fuerzas chilenas al norte por la Guerra del Pacífico. El levantamiento fue sofocado por las tropas a cargo de Cornelio Saavedra Rodríguez en 1881, dando fin al proceso que en la historia tradicional chilena se conoce como «Pacificación de la Araucanía».

Esta etapa contempló la ocupación total de la Araucanía y su consolidación. De ese modo, el gobierno chileno llevó finalmente a cabo uno de sus principales proyectos de Estado, anhelado incluso desde la época de los españoles, quienes en el período colonial no lograron instalarse ni explorar la Araucanía.

Los vencidos fueron reubicados en «reducciones», es decir, terrenos comunitarios de extensión reducida para que practicaran sus actividades ganaderas, donde permanecen hasta hoy. En los territorios ocupados se les entregaron tierras a colonos chilenos y europeos, principalmente españoles, alemanes, franceses, italianos, británicos, suizos y del resto de Europa; en total, entre 1882 y 1901, llegaron 36 000 europeos, 24 000 contratados por la agencia de colonización y 12 000 llegaron por sus propios medios. A estos 36.000 europeos se deben sumar a los 100.000 chilenos de origen hispánico, que llegaron en esta primera etapa, fenómeno de inmigración interna, que sumada a la europea, explicaría la actual composición étnica de la región, donde la etnia indígena mapuche, es minoritaria en términos absolutos.

La región es muy rica en recursos naturales, servicios, explotación forestal, ganadera, agrícola y con un gran auge en el turismo internacional, debido a sus bellezas naturales (volcanes, bosques milenarios, centros de esquí, lagos, ríos, pesca, centros termales, montañas, etc). A pesar de las riquezas es una de las regiones más pobres del país con altos índices de pobreza y desigualdad socio-económica.

En la región se viven constantemente conflictos entre mapuches y empresas forestales y agrícolas por causa de los eventos históricos donde muchas tierras mapuches fueron expropiadas ilegalmente y posteriormente vendidas a privados y empresas. 
Hoy hay muchos programas en marcha en la región de la Corporación Nacional de Desarrollo Indígena para tratar de resolver muchos de los conflictos de tierra e incentivar a comunidades mapuches para mejorar las condiciones económicas. A pesar de los enormes recursos destinados a compra de tierras, maquinarias y equipos no ha significado una mejoría en las condiciones de vida de los beneficiados pero si un enorme perjuicio a la economía regional.

La Región de La Araucanía, para efectos del gobierno y administración interior, se divide en 2 provincias:


Para los efectos de la administración local, las provincias están divididas en 32 comunas, 21 en Cautín y 11 en Malleco.

La región de La Araucanía, a efectos electorales, corresponde a la circunscripción senatorial XIV y XV; y agrupa a los distritos 48, 49, 50, 51 y 52.

El gobierno regional reside en el Intendente, designado por el Presidente de la República. Actualmente, la intendenta es la socialista Nora Barrientos Cárdenas.

El gobierno y administración de las provincias corresponde a dos gobernadores, nombrados por el Presidente de la República. Los Gobernadores son: 


La administración de la región radica en el Gobierno Regional, constituido por el Intendente y por el Consejo Regional, compuesto de consejeros regionales electos directamente por la población.

La administración local de cada comuna reside en la respectiva municipalidad.

La región se halla dividida en los Servicios de Salud Araucanía Norte y Araucanía Sur, el primero administra a los 7 hospitales de la Provincia de Malleco y el segundo a los 17 centros hospitalarios de Cautín. 

Los hospitales más importantes de la región son el Regional Hernán Henríquez Aravena de Temuco, el Mauricio Heyermann de Angol, el San José de Victoria y el Intercultural de Nueva Imperial.

Diversos centros universitarios, institutos profesionales y centros de formación técnica se encuentran en distintas ciudades de La Araucanía. Tienen presencia en la región: 



Se encuentra ubicada entre las regiones del Biobío y de Los Ríos y entre Argentina y el océano Pacífico. Su relieve se caracteriza por la presencia, de oeste a este, de planicies costeras, la cordillera de la Costa, la depresión intermedia, la precordillera y la cordillera de los Andes.

El clima de la región se caracteriza por la transición, de norte a sur, entre los climas de tipo mediterráneo y oceánico lluvioso. Siendo posible observar los siguientes tipos de clima:

La configuración hidrográfica de la región se caracteriza por la presencia de tres grandes ríos que corren de este a oeste: el Imperial, el Toltén y el Biobío, el cual se extiende en dirección nor-oeste, desembocando en la Región del Biobío. Los principales afluentes del río Imperial son el Cautín, el Chol Chol y el Quepe, y los del río Toltén son el Allipen y el lago Villarrica. Además presenta algunas cuencas costeras de menor magnitud, como los ríos Moncul y Queule. La región cuenta con una serie de lagos, entre los que destacan el Villarrica, el Caburga, el Budi y el Collico.

Dos de los volcanes más activos del país y de Sudamérica se encuentran en esta región: el Llaima y el Villarrica.

En la película "La Araucana" (1971), una adaptación libre del poema homónimo de Alonso de Ercilla y Zúñiga, Araucanía se interpretada por la actriz chilena Rebeca Ghigliotto.

La principal actividad económica de la región es la agricultura destacando los cultivos de plantas como avena, cebada, y centeno además de lupino y la papa. Estos cultivos, con excepción de la papa, representan las mayores superficies cultivadas del país. Cabe destacar el incremento de producción de avellanas y bayas ("berries"), por ejemplo arándanos, de exportación, estos cultivados principalmente en la zona de Gorbea. Además, es destacable la producción ganadera, especialmente en el rubro bovino, el cual la convierte en la segunda región de mayor producción en Chile ascendiendo a más de 700 000 cabezas de ganado anuales. En los últimos años, ha experimentado un considerable crecimiento la actividad forestal, de pinos y eucaliptos, principalmente en la provincia de Malleco.

Además, la región posee un gran potencial turístico debido a la belleza de su paisaje —conformado por bosques, lagos, ríos, volcanes y montañas—, y cuenta con una amplia oferta de servicios de hoteles y complejos turísticos.

La ciudad de Temuco es, junto a Iquique, una de las ciudades de crecimiento más explosivo a nivel nacional. Según el censo del año 1970, en Temuco vivían cerca de 88 000 habitantes; esta población, en 30 años se cuasi triplicó hasta bordear los 250 000 habitantes. La ciudad de Temuco en la actualidad, junto con Padre Las Casas y la localidad de Labranza forman la llamada Área metropolitana de Temuco que según las estimaciones del Ministerio de Vivienda y Urbanismo de Chile contaba con 460 824 habitantes para el año 2016, siendo el sexto centro urbano más grande de Chile.

La turística ciudad lacustre de Villarrica también ha vivido este fenómeno demográfico al transformarse, junto al balneario de Pucón, en uno de los 4 destinos turísticos de Chile debido a sus grandes páramos naturales rodeados por bosques cordilleranos, lagos de aguas claras e inmensos volcanes andinos. 

La inmigración nacional actual es proveniente de la zona central de Chile y un 23,46 % de la población de región de La Araucanía afirmó aun pertenecer a una etnia originaria/indígena, principalmente Mapuche; sin embargo las personas de origen indígena que se han asimilado sería mucho mayor.

Según el censo del año 2002, las ciudades más pobladas son: Temuco (260 783 hab. Incluye Padre Las Casas); Villarrica (45 531 hab.); Angol (43 801 hab.); Victoria (33 977 hab.); Lautaro (18 808 hab.); Nueva Imperial (14 980 hab.); Collipulli (14 240 hab.); Loncoche (14 191 hab.); Traiguén (14 140 hab.).

La región tiene una población diversa donde la sociedad originaria mapuche es minoritaria alcanzando alrededor del 45% del total de la población regional (30,6% de la comunidad mapuche) según el censo de 2012 siendo la población mayoritaria la correspondiente a las de origen español o criollo chileno, seguido de descendientes de inmigrantes suizos, franceses, alemanes e italianos llegados a la región, a partir de 1883 a partir de una política de colonización del Estado de Incorporación de la Araucanía. Desde el momento en que este territorio es incorporado comenzó un proceso de mestizaje entre los distintos pueblos que lo habitan, tanto entre los chilenos de la zona, y los que llegaron de otras zonas del país, como los migrantes extranjeros. En la actualidad por tanto la gran mayoría de la población autodenominada como mapuches sin mestizaje en realidad sería mestiza. Existen comunidades diferenciadas en la región producto del conflicto mapuche. Por otro lado, descendientes de los extranjeros se han asociado para defenderse de ataques incendiarios por parte de grupos paramilitares ilegales. Esta situación genera conflictos diversos, en donde no se ha llegado a una solución final. La Araucanía es también una región con una gran cantidad de población bilingüe en Chile, donde hay cerca de 75.000 hablantes de mapudungún, los que además manejan el castellano. El castellano es la lengua oficial de la región, sin embargo la lengua mapuche se encuentra oficializada en algunas comunas de la región y en proyección para ser lengua co-oficial en la región.

Los habitantes de etnia mapuche de esta región representan el 19% del total de la población mapuche a nivel nacional, muchos son mestizos, pero predominando la ascendencia étnica mapuche, siendo la segunda región con más miembros netos de este pueblo (tras la Región Metropolitana, que reúne al 37% del total de la población mapuche o mapuche-mestiza a nivel macional).

La lengua más hablada es el español y es el idioma oficial "de facto", como en todo Chile; además, parte de la población mapuche habla mapudungún y existen colectividades de descendientes de inmigrantes que utilizan idiomas como el alemán o el italiano en forma minoritaria. El mapudungun es lengua oficial en las comunas de Galvarino y Padre Las Casas y el año 2015 el Consejo Regional aprobó su cooficialidad en toda la región, pero no se ha implementado el modo en que se hará efectiva esta medida.

En la región coexisten variadas manifestaciones culturales, como consecuencia de los diversos grupos relacionales que poblaron la región, y que se manifiestan en tradiciones, la religión, la arquitectura, gastronomía, proyectos educacionales, en la que se destaca un fuerte componente cultural europeo, sea de origen criollo (hispánico) o de las derivadas de la inmigración europea, y la mapuche como por ejemplo la celebración del "We Tripantu" (Año Nuevo Mapuche) celebrada durante el solsticio de invierno, actividad en que participan más de 30 comunidades indígenas del sector. Esta fiesta da inicio a un nuevo año para los mapuches, que es celebrado con comidas típicas y rogativas que tienen como propósito pedir que la próxima temporada sea abundante en alimentos y cosechas, y que la naturaleza esté protegida.




</doc>
<doc id="22068" url="https://es.wikipedia.org/wiki?curid=22068" title="Morse">
Morse

Morse hace referencia a:


</doc>
<doc id="22070" url="https://es.wikipedia.org/wiki?curid=22070" title="ReiserFS">
ReiserFS

ReiserFS es un sistema de archivos de propósito general, diseñado e implementado por un equipo de la empresa , liderado por Hans Reiser. 

Actualmente es soportado por Linux y existen planes de futuro para incluirlo en otros sistemas operativos. También es soportado por Windows (de forma no oficial), aunque por el momento de manera inestable y rudimentaria (ReiserFS bajo windows).

A partir de la versión 2.4.1 de Linux, ReiserFS se convirtió en el primer sistema de ficheros con "journal" en ser incluido en el núcleo estándar.
También es el sistema de archivos predefinido en varias distribuciones, como SuSE (excepto en openSuSE 10.2 cuyo formato predeterminado es ext3), Xandros, Yoper, Linspire, Kurumin Linux, FTOSX, Libranet y Knoppix.

Con la excepción de actualizaciones de seguridad y parches críticos, ha cesado el desarrollo de ReiserFS (también llamado reiser3) para centrarse en Reiser4, el sucesor de este sistema de archivos.

ReiserFS ofrece funcionalidades que pocas veces se han visto en otros 
sistemas de archivos:


Comparado con ext2 y ext3 en el uso de archivos menores de 4k, ReiserFS es normalmente más rápido en un factor de 10–15. Esto proporciona una elevada ganancia en las news, como por ejemplo Usenet, cachés para servicios HTTP, agentes de correo y otras aplicaciones en las que el tiempo de acceso a ficheros pequeños debe ser lo más rápida posible.


ReiserFS almacena metadatos sobre los ficheros, entradas de directorio y listas de inodos en un único árbol B+ cuya clave principal es un identificador único. Los bloques de disco asignados a los nodos del árbol son los "bloques internos formateados" y los bloques de las hojas son los "bloques de hojas formateados". Todos los bloques restantes son los "bloques sin formatear", que contienen los datos de los ficheros. Los directorios con muchas entradas, ya sean directas o indirectas, que no caben en un sólo nodo, se reparten con el nodo vecino de la derecha. La asignación de bloques se lleva a cabo mediante un bitmap de espacio libre almacenado en localizaciones fijas.

En contraste, ext2 y otros sistemas de ficheros, usan una fórmula fija para calcular localizaciones de inodos, por lo que limitan el número de archivos que pueden almacenar. Otros también almacenan los directorios como una simple lista de entradas, lo que provoca que las búsquedas y modificaciones sean operaciones lineales temporalmente y degradan el rendimiento de directorios con muchos archivos. El árbol B+ en ReiserFS evita estos problemas.

Existen principalmente dos versiones de este sistema de ficheros: la 3 y la 4. Las características son las siguientes:




</doc>
<doc id="22071" url="https://es.wikipedia.org/wiki?curid=22071" title="Ext3">
Ext3

ext3 ("third extended filesystem" o "tercer sistema de archivos extendido") es un sistema de archivos con "registro por diario" (journaling). Fue el sistema de archivos más usado en distribuciones Linux, aunque en la actualidad ha sido remplazado por su sucesor, ext4.

La principal diferencia con ext2 es el "registro por diario". Un sistema de archivos ext3 puede ser montado y usado como un sistema de archivos ext2. Otra diferencia importante es que ext3 utiliza un árbol binario balanceado (árbol AVL) e incorpora el asignador de bloques de disco Orlov.

Aunque su velocidad y escalabilidad es menor que sus competidores, como JFS, ReiserFS o XFS, tiene la ventaja de permitir actualizar de ext2 a ext3 sin perder los datos almacenados ni tener que formatear el disco. Tiene un menor consumo de CPU y está considerado más seguro que otros sistemas de ficheros en Linux dada su relativa sencillez y su mayor tiempo de prueba.

El sistema de archivo ext3 agrega a ext2 lo siguiente:
Ext3 tiene dos límites de tamaño distintos. Uno para archivos y otro para el tamaño del sistema de archivos entero. El límite del tamaño del sistema de archivos es de 2 bloques

Hay tres niveles posibles de "journaling" (registro por diario)



Como ext3 está hecho para ser compatible con ext2, la mayoría de las estructuras del archivación son similares a las del ext2. Por ello, ext3 carece de muchas características de los diseños más recientes como las extensiones, la localización dinámica de los inodos, y la sublocalización de los bloques. Hay un límite de 31998 subdirectorios por cada directorio, que se derivan de su límite de 32000 links por inodo.
Ext3, como la mayoría de los sistemas de archivos actuales de Linux, no puede ser chequeado por el fsck mientras el sistema de archivos está montado para la escritura. Si se intenta chequear un sistema de ficheros que está montado puede detectar falsos errores donde los datos no han sido volcados al disco todavía, y corromper el sistema de archivos al intentar arreglar esos errores.

No hay herramienta de desfragmentación en línea para ext3 que funcione en nivel del sistema de archivos. Existe un desfragmentador offline para ext2, codice_1, pero requiere que el sistema de archivos ext3 sea reconvertido a ext2 antes de iniciarse. Además, dependiendo de los bits encendidos en el sistema, codice_1 puede destruir datos. No sabe como tratar la mayoría de las nuevas características de ext3.
Hay herramientas de usuario para desfragmentar como Shake y Defrag. Shake trabaja localizando para todo el archivo como una operación, lo que generalmente causa que el localizador encuentre espacio continuo en el disco. También intenta escribir archivos usados al mismo tiempo que otros.
Defrag trabaja copiando cada archivo sobre sí mismo. De todas formas solo funcionan si el sistema de archivos esta razonablemente vacío. No existe una verdadera herramienta de desfragmentación para ext3.
Como se viene diciendo, la guía de administración de Linux dice: "Los modernos sistemas de archivos de Linux mantienen la fragmentación al mínimo manteniendo los bloques de un archivo juntos, aunque no puedan ser guardados en sectores consecutivos. Algunos sistemas de archivos, como ext3, localizan efectivamente los bloques libres más cercanos a otros en el archivo. Por ello no es necesario preocuparse por la fragmentación en un sistema de Linux"
Mientras ext3 es más resistente a la fragmentación que Fat, nada evita que los sistemas ext3 se puedan fragmentar con el tiempo. Consecuentemente el sucesor de ext3, ext4, incluye una utilidad de desfragmentación y soporte para extensiones (regiones contiguas del fichero).

El soporte para la compresión está disponible como un parche no oficial para ext3. Este parche es un porte directo de codice_3 pero necesita un mayor desarrollo ya que todavía no implementa el journaling. El actual parche es llamado e3compr y puede ser bajado aquí: 

Ext3 no hace la suma de verificación cuando está escribiendo en el diario. Si barrier = 1 no está habilitado como una opción de montaje, y si el hardware está escribiendo fuera de orden, se corre el riesgo de una corrupción muy amplia del sistema de archivos en caso de que haya un fallo repentino del hardware.

Aunque Windows no tiene un soporte nativo para ext2 ni ext3, pueden instalarse drivers para poder acceder a ese tipo de sistemas de archivos. Se puede instalar en todos los sistemas de windows con arquitectura x86.

Este driver hace que se puedan montar las particiones sin tener que usar programas aparte. Nos muestra el sistema de archivos como si fuese una partición más dentro de windows.

Para bajarse el driver: .

Otra opción es usar un programa para poder ver y copiar los archivos que hay en una partición con ext3 y ext2 pero no monta la partición.
El programa es Explore2fs y nos permite:
Está disponible para las versiones de windows: 

Página principal del programa

Existe una versión más reciente de este sistema de archivos llamada Ext4 que implementa un gran cantidad de nuevas características

Para hacerse una mejor idea de las diferencias con el sistema de archivos ext4 mirar la siguiente tabla




</doc>
<doc id="22072" url="https://es.wikipedia.org/wiki?curid=22072" title="Ext2">
Ext2

ext2 ("second extended filesystem" o "segundo sistema de archivos extendido") es un sistema de archivos para el kernel Linux. Fue diseñado originalmente por Rémy Card. La principal desventaja de ext2 es que no implementa el registro por diario (en inglés "Journaling") que sí poseen sus posteriores versiones ext3 y ext4.

ext2 fue el sistema de ficheros por defecto de las distribuciones de Linux Red Hat Linux, Fedora Core y Debian. Los lanzamientos de las nuevas versiones estables, ext3 y ext4, han desplazado considerablemente su uso.

El sistema de ficheros tiene una tabla donde se almacenan los i-nodos. Un i-nodo almacena información del archivo (ruta o "path", tamaño, ubicación física). En cuanto a la ubicación, es una referencia a un sector del disco donde están todas y cada una de las referencias a los bloques del archivo fragmentado. Estos bloques son de tamaño especificable cuando se crea el sistema de archivos, desde los 512 bytes hasta los 4 KiB, lo cual asegura un buen aprovechamiento del espacio libre con archivos pequeños.

Los límites son un máximo de 2 terabytes de archivo, y de 4 para la partición.




</doc>
<doc id="22073" url="https://es.wikipedia.org/wiki?curid=22073" title="Tabla de asignación de archivos">
Tabla de asignación de archivos

Tabla de asignación de archivos, comúnmente conocido como FAT (del inglés "file allocation table"), es un sistema de archivos desarrollado para MS-DOS, así como el sistema de archivos principal de las ediciones no empresariales de Microsoft Windows hasta Windows Me.

FAT es relativamente sencillo. A causa de ello, es un formato popular para disquetes admitido prácticamente por todos los sistemas operativos existentes para computadora personal. Se utiliza como mecanismo de intercambio de datos entre sistemas operativos distintos que coexisten en la misma computadora, lo que se conoce como entorno multiarranque. También se utiliza en tarjetas de memoria y dispositivos similares.

Las implementaciones más extendidas de FAT tienen algunas desventajas. Cuando se borran y se escriben nuevos archivos tiende a dejar fragmentos dispersos de éstos por todo el soporte. Con el tiempo, esto hace que el proceso de lectura o escritura sea cada vez más lento. La denominada "desfragmentación" es la solución a esto, pero es un proceso largo que debe repetirse regularmente para mantener el sistema de archivos en perfectas condiciones. FAT tampoco fue diseñado para ser redundante ante fallos. Inicialmente solamente soportaba nombres cortos de archivo: ocho caracteres para el nombre más tres para la extensión.

El sistema de archivos FAT fue creado por Marc McDonald basado en una serie de conversaciones entre McDonald y Bill Gates. Fue incorporado por primera vez en el sistema operativo QDOS por Tim Paterson en agosto de 1980, para los computadores S-100 de arquitectura Intel 8086. Este sistema de archivos fue la principal diferencia entre QDOS y CP/M.


En aquella época, el habitual disquete (5,25 pulgadas en una sola cara) constaba de 40 pistas con 8 sectores por pista, resultando en una capacidad inferior a 160 kilobytes. Este límite excedía la capacidad en más de un orden de magnitud, y al mismo tiempo, permitía encajar todas las estructuras de control en la primera pista. Por tanto, se evitaba el movimiento de los cabezales en las operaciones de lectura y escritura. Estos límites fueron superados en los años posteriores.

Con el propósito de soportar el reciente IBM PC, que disponía de un disco duro de 10 megabytes, MS-DOS 2.0, y carpetas anidadas, simplemente se utilizaron "clusters" de 8 kilobytes en el disco duro. El formato de "FAT" en sí mismo no cambió.

En 1984, IBM lanzó el PC AT, con 20 megabytes de disco duro. Al mismo tiempo, Microsoft lanzó MS-DOS 3.0.
Las direcciones de los "cluster" fueron ampliadas a 16 bits, permitiendo un número mayor de "clusters" (65.536 exactamente de archivos. A pesar de todo, no hubo mejoras en el límite máximo de 32 megabytes.

MS-DOS 3.0 también incorporó soporte a disquetes de "alta densidad" de 5,25 pulgadas (1,2 megabytes de capacidad), con 15 sectores por pista, y en consecuencia, más espacio para FAT. Esto probablemente forzó una dudosa optimización del tamaño del "clúster", que bajó de dos sectores a sólo uno. El efecto global fue una reducción significativa de los tiempos de lectura y escritura frente a los disquetes de doble densidad.

Estructura de la FAT12 en un disquete de 1,44M:

En 1987 apareció lo que hoy se conoce como «el formato FAT 16». Se eliminó el contador de sectores de 16 bits. El tamaño de la partición ahora estaba limitado por la cuenta de sectores por "clúster", que era de 8 bits. Esto obligaba a usar "clusters" de 32 KiB con los usuales 512 bytes por sector. Así que el límite definitivo de FAT16 se situó en los 4 (2GiB por clúster) GiB.

Esta mejora estuvo disponible en 1988. Mucho más tarde, Windows NT 4.0(1998) y Windows XP (2001) aumentaron el tamaño máximo del "cluster" a 64 kilobytes pudiendo crear particiones de hasta 4 GB. No obstante, el formato resultante no era compatible con otras implementaciones de la época, y además, generaba más fragmentación interna (se ocupaban "clusters" enteros aunque solamente se precisaran unos pocos bytes). Windows 98 fue compatible con esta extensión en lo referente a lectura y escritura. Sin embargo, sus utilidades de disco no eran capaces de trabajar con ella.

Windows 3.11 introdujo un nuevo esquema de acceso a los sistemas de archivos, usando el modo protegido de 32 bits (presente en los Intel 386 y posteriores) esquivando el núcleo de MS-DOS. Para ello, usaba directamente el BIOS o el "hardware" de la unidad de disco. Esto también permitía utilizar una caché, acelerando el acceso. Todo esto se denominó VFAT o FAT virtual.

Windows NT 3.1 proporcionaba la misma aproximación, pero denominándolo FASTFAT. Sin embargo, era natural que los controladores de Windows NT utilizasen el modo protegido de 32 bits. A menudo se confunde con el soporte "LFN" (nombres largos de archivo) ya que éste estaba habilitado por defecto en Windows 95.

Uno de los objetivos de los diseñadores de Windows 95 fue el uso de nombres más largos para los archivos. Se implementó sobre FAT utilizando un truco en el modo de almacenar los índices de los directorios. Esta implementación también se conoce como VFAT por culpa del controlador de Windows 95 que lo incorporó por primera vez. Los nombres largos también se soportaron en Windows NT a partir de la versión 3.5.

FAT32 fue la respuesta para superar el límite de tamaño de FAT16 al mismo tiempo que se mantenía la compatibilidad con MS-DOS en modo real. Microsoft decidió implementar una nueva generación de "FAT" utilizando direcciones de "cluster" de 32 bits (aunque sólo 28 de esos bits se utilizaban realmente).

En teoría, esto debería permitir aproximadamente 100.100.538.948.585.453 "clusters", arrojando tamaños de almacenamiento cercanos a los 8 TiB. Sin embargo, debido a limitaciones en la utilidad "ScanDisk" de Microsoft, no se permite que FAT32 crezca más allá de 4.177.920 "clusters" por partición (es decir, unos 124 GiB). Posteriormente, Windows 2000 y XP situaron el límite de FAT64 en los 64 GiB. Microsoft afirma que es una decisión de diseño, sin embargo, es capaz de leer particiones mayores creadas por otros medios.

FAT32 apareció por primera vez en Windows 95 OSR2. Era necesario reformatear para usar las ventajas de FAT32. Curiosamente, DriveSpace 3 (incluido con Windows 95 y 98) no lo soportaba. Windows 98 incorporó una herramienta para convertir de FAT16 a FAT32 sin pérdida de los datos. Este soporte no estuvo disponible en la línea empresarial hasta Windows 2000.

El tamaño máximo de un archivo en FAT32 es 4 GiB (2−1 bytes), lo que resulta engorroso para aplicaciones de captura y edición de video, ya que los archivos generados por éstas superan fácilmente ese límite.

Otros sistemas operativos tales como GNU/Linux, FreeBSD y BeOS soportan FAT, y la mayoría también soportan VFAT y FAT32 en menor extensión. Las primeras ediciones de GNU/Linux también apoyaron un formato conocido como UMSDOS. Este consistía en una variante de "FAT" que admitía los permisos de seguridad típicos en Unix, además de los nombres largos de éste. Para ello, se almacenaba esta información en un archivo FAT separado que se denominaba ""--linux--.---"" (por tanto, conservando compatibilidad total). UMSDOS quedó en desuso con la aparición de VFAT en recientes versiones del núcleo Linux.
El sistema operativo Mac OS X también soporta sistemas de archivos FAT, siempre que no se trate del volumen de arranque del sistema.

El sistema de archivos FAT no está diseñado para albergar metadatos. Algunos sistemas operativos que los necesitan incorporaron varios métodos para simularlos. Por ejemplo, almacenándolos en archivos o carpetas extra (de manera similar a UMSDOS) o también otorgando una semántica especial a estructuras no usadas en el formato original. No obstante, este último método no es compatible con herramientas no preparadas para esta extensión. Por ejemplo, una herramienta de desfragmentación podría destruir los metadatos. Mac OS, a través de la utilidad PC Exchange, almacena metadatos en un archivo oculto denominado ""FINDER.DAT"" (uno por carpeta). Mac OS X almacena los metadatos en un archivo oculto denominado como su propietario, pero comenzando por "".-"". Cuando se trata de meta-datos de una carpeta, los almacena en un archivo oculto llamada "".DS_Store"".

OS/2 también depende fuertemente del uso de meta-datos. Cuando se refiere a volúmenes en FAT, los almacena en un archivo oculto denominado ""EA DATA. SF"" en la carpeta raíz del volumen. También reserva dos bytes en el archivo (o carpeta) para poder indexarlo. Los meta-datos se acceden a través del escritorio Workplace Shell, a través de "guiones" REXX, o a través de utilidades como 4OS2. Cuando se refiere a su sistema de archivos propio HPFS, éste ya da soporte nativo a meta-datos, denominados atributos extendidos.

Windows NT soporta meta-datos en los sistemas de archivos HPFS, NTFS y FAT (mediante el mismo mecanismo que OS/2). Pero no es posible copiar meta-datos entre sistemas de archivos distintos. Windows 2000 se comporta exactamente igual que Windows NT, pero ignora los meta-datos cuando copia archivos desde FAT32 a otros sistemas de archivos.

exFat (Extended File Allocation Table) es un sistema de archivos especialmente adaptado para memorias flash presentado con Windows Embedded CE 6.0. exFAT se utiliza cuando el sistema de archivos NTFS no es factible debido a la sobrecarga de las estructuras de datos.

Dado que Microsoft no seguirá soportando sistemas operativos basados en MS-DOS, es poco probable que se desarrollen nuevas versiones de FAT. NTFS es un sistema de archivos superior a éste en múltiples aspectos: eficiencia, rendimiento y fiabilidad. Su principal desventaja es el excesivo tamaño que desperdicia en pequeños volúmenes y su limitado soporte en otros sistemas operativos. Sus especificaciones son un secreto comercial; no obstante, esto está cambiando, gracias a la ingeniería inversa, pues ya es posible leer y escribir en particiones NTFS en Linux con herramientas como NTFS-3G.

FAT es, hoy por hoy, el sistema de archivos habitual en medios de almacenamiento extraíbles (con la excepción hecha del CD y DVD). FAT12 se usa en disquetes, y FAT16 en el resto de medios (por ejemplo, tarjetas de memoria y memorias USB) de hasta 2GB (hoy en día casi en desuso). Las tarjetas y memorias USB de 4GB a 32GB usualmente usan FAT32 para superar las limitaciones de la versión anterior. Las memorias de 64GB y más usan el sistema exFAT por la misma razón. FAT se utiliza por motivos de compatibilidad.

El soporte para formatear particiones con FAT32 en Windows está limitado a particiones de hasta 32 gigabytes, lo que obliga a los usuarios a usar NTFS o utilizar utilidades de terceros al margen de Windows. Esta limitación afecta a la hora de crear particiones, pero no al uso: Windows puede acceder a discos FAT32 de hasta 2 terabytes.

Aunque en el momento de instalar no permite formatear una particion con FAT32 de más de 32 GB, y obligará a usar NTFS. La solución es formatear antes el disco en FAT32 (por ejemplo con la ayuda de un LiveCd de GNU/Linux o utilidades de terceros), y a continuación instalar Windows.

El sistema de archivos FAT se compone de cuatro secciones:


Una partición se divide en un conjunto de "clusters" de idéntico tamaño. Son pequeños bloques discontinuos. El tamaño del clúster depende de la variante de FAT utilizada. Varía entre 2 y 32 kilobytes. Cada archivo ocupa uno o más "clusters" en función de su tamaño. De manera que un archivo queda representado por una cadena secuencial de "clusters" (una lista enlazada). Cada clúster de la cadena no tiene por qué ser adyacente al anterior. Esto es lo que provoca la fragmentación.

La tabla de asignación de archivos consta de una lista de entradas. Cada entrada contiene información sobre un clúster:


El tamaño de estas entradas también depende de la variante FAT en uso: FAT16 usa entradas de 16 bits, FAT32 usa entradas de 32 bits, etc.

Este índice es un tipo especial de archivo que almacena las sub-carpetas y archivos que componen cada carpeta. Cada entrada del directorio contiene el nombre del archivo o carpeta (máximo 8 caracteres), su extensión (máximo 3 caracteres), sus atributos (archivo, carpeta, oculto, del sistema, o volumen), la fecha y hora de creación, la dirección del primer "cluster" donde están los datos, y por último, el tamaño que ocupa.

El directorio raíz ocupa una posición concreta en el sistema de archivos, pero los índices de otras carpetas ocupan la zona de datos como cualquier otro archivo.

Los nombres largos se almacenan ocupando varias entradas en el índice para el mismo archivo o carpeta.

Microsoft ha solicitado una serie de patentes para elementos clave del sistema de archivos FAT en los años 1990. Su popularidad y compatibilidad lo hacen el formato de elección para memorias flash de cámaras digitales, teléfonos móviles, y tabletas por ejemplo.

En diciembre de 2003, Microsoft anunció que comenzaría a comercializar licencias de uso para "FAT" al coste de 0,25 dólares por unidad vendida. con un máximo de 250.000 dólares por acuerdo de licencia.

Hasta el momento, Microsoft ha citado cuatro patentes sobre FAT como fundamento de sus pretensiones. Las cuatro se refieren a la implementación de nombres largos:


Algunos expertos creen que estas patentes no cubren realmente el uso que se hace de FAT en medios extraibles de consumo.

Por otra parte, el documento "Microsoft Extensible Firmware Initiative FAT 32 File System Specification, FAT: General Overview of On-Disk Format", publicado por Microsoft, garantiza una serie de derechos que podrían interpretarse como una licencia para implementar FAT en otros sistemas operativos.

Debido al clamor popular para que se volviesen a examinar dichas patentes, la Public Patent Foundation envió pruebas a la Oficina de Patentes sobre trabajos previos de Xerox e IBM. La Oficina reconoció que existían "dudas sustanciales de patentabilidad" y abrió una investigación para revisar dichas patentes.

Finalmente, dicha revisión ha confirmado la validez de las patentes en enero de 2006.




</doc>
<doc id="22078" url="https://es.wikipedia.org/wiki?curid=22078" title="Justin Frankel">
Justin Frankel

Justin Frankel es uno de los programadores responsables por el programa de computadora Winamp. Se lo asocia también con Tom Pepper y Gnutella. Frankel abandonó sus estudios en la Universidad de Utah para formar la empresa de software Nullsoft en 1998; la cual fue adquirida por AOL en 1999 por la suma de 86 millones de dólares. Frankel amenazó con renunciar el 2 de junio de 2003, tras que AOL retirara el programa WASTE del sitio web de Nullsoft.



</doc>
<doc id="22079" url="https://es.wikipedia.org/wiki?curid=22079" title="WASTE">
WASTE

WASTE es un protocolo y un software friend-to-friend (peer-to-peer) que permite la comunicación y el intercambio de archivos de forma cifrada en pequeños grupos de usuarios de confianza. Fue desarrollado por Justin Frankel en Nullsoft en 2003. Después de su lanzamiento fue retirado de distribución por AOL, la empresa dueña de Nullsoft. La página original fue reemplaza por una declaración que informaba que la publicación del programa no estaba autorizada, a pesar de que originalmente el software fue liberado bajo los términos de la Licencia Pública General de GNU.

El nombre WASTE hace referencia a la novela "La subasta del lote 49" de Thomas Pynchon, en donde se usa como acrónimo de "We Await Silent Tristero's Empire", una empresa de servicio postal.

Varios desarrolladores modificaron y actualizaron el programa y el protocolo WASTE. La versión de SourceForge es considerada por algunos como la línea "oficial" de desarrollo, pero hay varias bifurcaciones siendo WASTE again la más activa.

WASTE es un protocolo de mensajería instantánea e intercambio de archivos. Su comportamiento es parecido al de una red privada virtual, conectando a un grupo de computadoras de confianza, determinadas por los usuarios de la red. Este tipo de red es comúnmente denominada darknet. Utiliza un fuerte cifrado para impedir que terceras personas puedan leer el contenido de los mensajes o archivos. El mismo cifrado se usa para enviar y recibir mensajes instantáneos y archivos, mantener la conexión, explorar los directorios y buscar archivos.

Las redes WASTE son descentralizadas, esto significa que no hay un servidor central o "hub" donde se conectan todos los usuarios. Los usuarios deben conectarse entre ellos individualmente. Normalmente, esto es completado compartiendo la clave pública RSA, asegurándose de que los ordenadores son accesibles a través de los puertos apropiados (una o más partes deben tener la dirección IP y el puerto que pueda ser alcanzado por el otro), y escribiendo la dirección IP y el puerto de alguien en la red al que conectar.

Una vez conectado a la red, las claves públicas se intercambian automáticamente entre los miembros (siempre que haya suficientes miembros configurados para reenviar y aceptar las claves públicas), y los nodos se intentarán conectar el uno al otro, fortaleciendo la red (disminuyendo la probabilidad de que si un nodo se desconecta, se colapse o se cierre una parte de la red), así como aumentar el número de rutas posibles desde un punto dado a otro punto cualquiera, disminuyendo la latencia y el ancho de banda necesario para la comunicación y transferencia de archivos.

Dado que WASTE conecta grupos pequeños, privados y no grandes, públicos, la función de búsqueda en la red es una de las más rápidas de todas las aplicaciones P2P descentralizados. Sus capacidades de mensajería instantánea y uso compartido de archivos son mucho más parecidas a los de AOL Instant Messenger que a las de los programas de intercambio de archivos típicos. Los miembros de la red pueden crear salas de chat públicas y privadas, enviar mensajes instantáneos unos a otros, navegar por los archivos de otros miembros e intercambiar archivos, pudiendo empezar la transferencia cualquiera de las dos partes. La operación de arrastrar y soltar en el chat envía los archivos a sus destinos.

El tamaño recomendado para una red WASTE está entre 10 y 50 nodos, aunque se ha sugerido que el tamaño de la red es menos importante que la relación entre los nodos que redirigen el tráfico y los que no. Con el cliente original de Justin Frankel hay grupos que ya superan los cinco años de edad, no es raro que en las redes estables alberguen varios terabytes de contenido seguro.

Por defecto, WASTE escucha las conexiones entrantes en el puerto 1337. Este puerto probablemente fue elegido por las connotaciones "leet".

Dado que no hay un servidor central, las redes WASTE típicamente emplean una contraseña o "passphrase", también llamada "nombre de la red" para prevenir colisiones. Esto es, un miembro de una red conectando a un miembro de otra red, conectaría las dos redes. Asignando un identificador único ("passphrase") a la red, el riesgo de colisiones puede ser reducido, sobre todos con los clientes originales.

Las "nullnets" son redes sin contraseña. Este tipo de redes pueden ser unidas fácilmente entre ellas al carecer de contraseña. Esto hace que el tamaño de la red aumente formando un lugar público donde intercambiar ideas y archivos. Es imposible saber cuantas redes "nullnet" existen, debido a que es necesario conocer a un usuario de la red para poder unirse.




</doc>
<doc id="22080" url="https://es.wikipedia.org/wiki?curid=22080" title="Nullsoft Scriptable Install System">
Nullsoft Scriptable Install System

Nullsoft Scriptable Install System (NSIS) es un "manejador de script" Windows de código abierto con requerimientos mínimos, desarrollado por Nullsoft, los creadores de Winamp. NSIS ha crecido en popularidad como una alternativa al uso extenso de productos comerciales como InstallShield y es actualmente utilizado para un sin número de aplicaciones distribuidas a través de Internet.

NSIS es liberado bajo una combinación de licencias de software libre, principalmente la licencia de zlib/libpng, de esta forma haciendo a NSIS software libre.

NSIS fue creado para distribuir Winamp. Está basado en un producto previo de Nullsoft, PiMP (plugin Mini Packager), y además es conocido como SuperPiMP. Después de la versión 2.0a0, el proyecto fue movido a SourceForge donde desarrolladores fuera de Nullsoft empezaron a trabajar en él en una forma básica. NSIS 2.0 fue liberado aproximadamente dos años más tarde.

La versión 1 de NSIS es en muchas formas similar al clásico Instalador de Windows, pero es mucho más fácil de codificar y soporta más formatos de compresión. La versión 2 de NSIS tiene una nueva interfaz GUI y soporta la compresión LZMA, múltiples lenguajes, y un sistema sencillo de plugins. La versión 3 de NSIS incluye compatibilidad con Unicode opcional, sigue funcionando en las mismas versiones de Windows y además es compatible con Windows 10.

Usuarios - NSIS

Novedades - NSIS

Notas de cambios y versiones - NSIS

Licencia de NSIS



</doc>
<doc id="22081" url="https://es.wikipedia.org/wiki?curid=22081" title="FastTrack">
FastTrack

FastTrack es un protocolo de red en donde se pueden intercambiar archivos P2P. Se caracteriza por el uso de supernodos para aliviar la carga de los servidores encargados de sostener la red. Fue utilizado por programas como Kazaa, Grokster, iMesh y Morpheus. FastTrack fue la red de intercambio de archivos más popular en el 2003. Se estima que el número total de usuarios fue mayor que la de Napster en su apogeo. Posee la capacidad de reanudar descargas interrumpidas y descargar simultáneamente varios segmentos de un archivo de múltiples pares. Es empleado por los servicios de intercambio de archivos Kazaa -y sus variantes-, Mammoth, iMesh, entre otros.
Mientras que el protocolo FastTrack se utiliza en software para compartir archivos, otras aplicaciones tienen el mismo nombre, como John Mackin de FastTrack Books y Netscape FastTrack.

El protocolo FastTrack y el cliente Kazaa fueron producto de meses de trabajo de los desarrolladores Niklas Zennström de Suecia, Janus Friis de Dinamarca y el grupo de programadores dirigidos por Jaan Tallin -el mismo que creará más tarde Skype-. Fue introducido en marzo de 2001 por la empresa holandesa Consumer Empowerment -compañía dirigida por ellos mismos-. Fue en ese mismo donde se desató la guerra de las redes P2P de la primera generación y cuando en julio Napster es cerrado. La Consumer Empowerment es vendida a la australiana Sharman Network en enero del 2002, debido a que gastaba alrededor de 100.000 dólares al mes en problemas legales. Hacia comienzos del año 2003, FastTrack era la red de distribución de archivos más popular, siendo utilizada principalmente para el intercambio de música, películas y software. Todo eso debido a FastTrack responde a una necesidad de compartir información por parte de los usuarios quienes ya habían visto desaparecer a varios clientes de la primera generación P2P como Audiogalaxy y Napster. Todo ese proceso generó un vacío en el mundo del intercambio P2P, que dio paso a la segunda generación de clientes como eDonkey, FastTrack, Bittorrent y Gnutella. Luego FastTrack y más específicamente Kazaa -anteriormente llamado KaZaA- se convirtió en la punta de la lanza de los internautas. Sin embargo, la decadencia causada en 2004 por las demandas y otros esfuerzos legales en pro de proteger los derechos de autor de la RIAA y MPAA se prolonga hasta el día de hoy. Actualmente, poco más de un millón y medio de usuarios habitan a diario la red FastTrack.

FastTrack al igual que los sistemas P2P como eDonkey y BitTorrent; puede ser clasificada como una red semicentralizada. Es decir existen varios servidores que manejan la información que circula por el sistema, sin que uno en particular maneje y ponga en peligro toda la red. FastTrack emplea un método para aliviar la carga que recae sobre estos, el cual son los llamados "supernodos", los cuales fueron ideados por la compañía holandesa Kazaa BV.
La mayoría de los componentes que integran esta red son conocidos: clientes (incluidos seeds y peers), servidores y nodos. A saber que un "cliente" es una persona capaz de conectarse al sistema; un "servidor" es una computadora que posee amplios recursos para manejar el tráfico que circula en una determinada red y un "nodo" es la máquina de un cliente que por tener amplios recursos los comparte con el sistema, aliviando la carga de la red.
Los supernodos o superpeers son una característica innovadora de esta red. El principal objetivo de los mismos es mejorar la rapidez y la manera en que fluye la información, manejando algunas de las acciones que más recursos necesitan: el tráfico y las búsquedas (las más representativas). Pero las peculiaridades de estos clientes no termina aquí, cualquier usuario que se conecte a la red -usando el respectivo cliente- puede ser ascendido a este rango. Sin embargo para que se califique de esta manera se debe cumplir con ciertos requisitos en cuanto a recursos: contar con un gran ancho de banda, una computadora con un gran procesador y memoria disponible para poder manejar y enviar toda la información que circula por la red. Muchas veces ni se sabrá si se forma parte del conjunto de supernodos de FastTrack. Cabe destacar que esta clasificación no alterará de alguna manera la búsqueda ni velocidad de conexión del usuario. Tampoco perjudica la seguridad de la computadora, ni satura esta de operaciones, debido a que solo ocupa el 10% de los recursos de esta.

Los servidores centrales en realidad manejan los supernodos, y éstos a su vez las conexiones entre los usuarios y las búsquedas. Así, la carga sobre los servidores se ve minimizada de gran manera: ya que no deben manejar y conectar a los usuarios de la red, sino administrar los mencionados supernodo.

Además del sistema de supernodos o superpeers, FastTrack cuenta con otras características que lo identifican. Por ejemplo, posee un sistema de piezas (sistema por el cual se puede descargar un mismo archivo desde varias computadoras) similar al de eDonkey y Bittorrent, pero con un nivel de desarrollo más complejo. Aun con la poca profundidad que da este aspecto, las velocidades de descarga se ven beneficiadas por la posibilidad de descargar diferentes partes de distintas fuentes al mismo tiempo. Si bien no suele utilizar todo el ancho de banda disponible, FastTrack logra tasas de transferencia aceptables.

FastTrack al igual que muchos protocolos para redes P2P, cuenta con un algoritmo de verificación propio. En este caso es uno bastante controversial: UUhash, el cual es capaz de verificar archivos grandes en poco tiempo, aun en computadoras con recursos limitados. Sin embargo, dado su desarrollo permite que la corrupción dentro de los archivos pase desapercibida, por lo que falla muchas veces en su trabajo. Este aspecto del algoritmo es el que ingeniosamente utilizaron y aún utilizan compañías como RIAA o MPAA para incorporar archivos falsos de la red y evitar posibles violaciones de derechos de autor.

Dado que el sistema FastTrack fue desarrollado por una compañía para usos privados, su código no es abierto. Esto hace que solo sus desarrolladores puedan realizar modificaciones al mismo, y que no exista una gran variedad de clientes alternativos al oficial.

Es el cliente oficial para conectarse a la red FastTrack. Su principales características son la simpleza y facilidad uso, lo que lo hizo muy popular entre los usuarios novatos del P2P.

Kazaa provee de muchas herramientas que simplifican muchas necesidades online, como un reproductor de audio y video, un explorador y un navegador web. Gracias a los programas dañinos incluidos en Kazaa han aparecido diversas versiones alternativas. A continuación se mencionarán algunas.
Disponible desde el 2002, Kazaa Lite fue el primer cliente en lograr abrir el código de Kazaa. Además de eliminar completamente el spyware que el primero dejaba pasar desapercibido.
Luego de que el desarrollo de Kazaa Lite fuera abruptamente abandonado, apareció K-Lite. Sus características no son muy diferentes en relación a Kazaa Lite, salvo que este es un parche para el Kazaa original. A diferencia de Kazaa Lite, K-Lite aún está disponible.

Casi olvidado por todos en el mundo P2P, iMesh es uno de los clientes veteranos que empleados para conectarse a la red FastTrack. Sus características no difireren demasiado de Kazaa, posee una interfaz limpia, fácil de manejar y provee de búsquedas y descargas razonablemente buenas. Aunque lamentablemente posee uno de los aspectos más controversiales de Kazaa: "la inclución de spyware", en especial la publicidad.

Las principales características de este cliente es su código abierto, libre de spyware, búsquedas efectivas y descargas rápidas. Es por ello que aunque no es tan conocido como Kazaa ha captado mucha atención en el mundo P2P. Todavía se encuentra en estado beta, y como es lógico no posee un desarrollo complejo como el mencionado Kazaa. Para descargarlo hay que visitar: http://mammoth.sourceforge.net.

Los clientes presentados no son los únicos que utilizan la red FastTrack, existen otros como:

Los problemas de conexión con servidores están presentes de forma poco frecuente en la red FastTrack. Ese tipo de fallos puede provenir de dos vías diferentes. La principal causa le concierne al usuario y está relacionada con los puertos. Los puertos son entradas que permiten la circulación de información de la computadora hacia la red y viceversa, para diferenciarlos se enumeran, por ejepmlo puerto 2.154. Por defecto, el puerto que Kazaa y la mayoría de los clientes alternativos utilizan es el 1.214. En caso de fallos por lo general se pueden resolver con herramientas de verificación de puertos. Si se descubre que el puerto 1.214 está cerrado, se deben cambiar las opciones del cliente empleadas y el puerto que conecta a la red FastTrack. En estos casos, es recomendable configurar el puerto entre el 3.000 y el 4.000 que se conozca que este abierto.

Por otro lado, puede ser que el servidor de FastTrack no se encuentre en línea. Debido a su estructura semicentralizada esta situación puede ocurrir (aunque es no es muy común) en cualquier momento. Teniendo en cuenta la gran cantidad de problemas que esta red sufre a diario, no es nada improbable que esto pueda llegar a suceder.

Al descargar archivos utilizando el protocolo FastTrack se puede sufrir uno de los males más temidos: " las descargas interminables". Esto se debe, al modo en que sistema está desarrollado. En pocas palabras, FastTrack utiliza un sistema de colas de espera similar al de eDonkey, que organiza la manera en que los usuarios descargan los archivos que desean. Sin embargo, gracias a la proliferación de archivos con una sola o pocas fuentes, puede darse la situación de que la descarga se ralentize o nunca se llegue a completar. Esto suele suceder cuando se recibe este mensaje "remotely queued" de las fuentes de las que se intenta descargar información, y significa que el lugar en la cola es muy lejano o que hay muchos usuarios esperando descargar el mismo archivo.

Al igual de la red eDonkey, el problema de búsquedas insatisfactorias tiende a darse por la falta de paciencia que tienen los usuarios que se conecta al sistema. La solución suele ser muy sencilla: esperar que el supernodo al que se haya conectado haga las conexones pertinetes con otro de su misma naturaleza y, por supuesto, con el servidor. Una vez hecho esto la búsqueda serán cuestión de unos segundos. Aun así es recomendable verificar el puerto 1.214 y, si esté no admite conexiones se debe cambiar por otro entre 3.000 y 4.000 en las opciones de su cliente.

Si tras realizado un cambio de puertos no se logra conectar correctamente a la red FastTrack, se puede probar una última alternativa: "actualizar la lista de supernodos". Para evitar identificar a todos los nodos cada vez que se conecte a la red, todos los clientes cuentan con una lista de supernodos conocidos y estables. Sin embargo, la inestabilidad general de este sistema, hace que muchas veces esta lista quede obsoleta en pocos meses. Si bien esta característica sólo es posible si se usan clientes alternativos al oficial. Esta puede ser la solución a todos nuestros problemas de conexión.

Poseer programas maliciosos es un problema asociado más que todo al cliente oficial. De todas las acusaciones que ha sufrido Kazaa a través de los años la de poseer spyware es una de las que más usuarios le ha costado a esta aplicación. La velocidad en que estos migraban aumentó cuando dichos rumores se confirmaron, y dejaron a la luz una gran cantidad de programas maliciosos que se instalaban en las computadoras. Entre estos están:




</doc>
<doc id="22082" url="https://es.wikipedia.org/wiki?curid=22082" title="Ventilador">
Ventilador

Un ventilador es una máquina de fluido, más exactamente una turbomáquina que transmite energía para generar la presión necesaria con la que se mantiene un flujo continuo de aire.
Se utiliza para usos muy diversos como: ventilación de ambientes, refrescamiento de máquinas u objetos o para mover gases, principalmente el aire, por una red de conductos.

En su versión más corriente, un ventilador es una máquina que absorbe energía mecánica y la transfiere a un gas, proporcionándole un incremento de presión no mayor de 10 kPa (1.000 mm.c.a. aproximadamente), por lo que da lugar a una variación muy pequeña del volumen específico y por tanto se podría considerar como una máquina hidráulica.

Los ventiladores más antiguos, de los que se tiene referencia, eran manuales, en principio con mango fijo, como el "flabellum", que aparece en la cultura egipcia, al menos desde la dinastía XIX, para pasar posteriormente en el siglo V a. de C. a la Antigua Grecia, en la que tenía forma de palmeta, tal como aparece en pinturas de vasos de cerámica. También de la Antigua Roma hay pinturas en las que se representan esclavos manejando el flabellum.
Manejado también por esclavos, pero ya con cierto mecanismo, es el "abano", que era un bastidor con tela gruesa que se colgaba del techo y se movía mediante un sistema de cuerdas y poleas, que ya usaban los árabes a principios del siglo VII. También se encuentra en la India y Medio Oriente con el nombre de "punkah". 
En China, el origen del abanico rígido se sitúa hacia 2697 a. de C., con el emperador Hsiem Yuan, y la referencia escrita más antigua (1825 a. de C.) menciona dos abanicos de plumas ofrecidos al emperador Tchao Wong, de la dinastía Chou.

Pero el ventilador similar o precursor del que conocemos hoy como tal, aparece en 1886 y es un invento del estadounidense Schuyler Skaats Wheeler, que fue comercializado por su empresa "Crocker & Wheeler", instalada en Nueva York. Era de pequeño tamaño y diseñado para ponerlo sobre una mesa. Casi simultáneamente aparece en Alemania una versión de techo creada por el ingeniero Philip Diehl.

El tipo de ventilador más conocido, se utiliza para la ventilación o para aumentar la velocidad del aire en un espacio habitado, básicamente para refrescar. Por esta razón, es un elemento muy utilizado en climas cálidos. 
Como máquinas de transporte, los ventiladores se usan principalmente para producir un flujo de gases de un punto a otro. Dicho flujo se puede utilizar como soporte para transportar otras sustancias u otros materiales como ocurre en la fluidización en la que partículas sólidas (cenizas, polvos, basuras, etc.) se mueven suspendidas en una corriente de un fluido.

También de forma secundaria, se utiliza el ventilador para asistir a un intercambiador de calor con funciones de disipador o de radiador, con el fin de aumentar la transferencia de calor entre sólido y aire o entre fluidos que interactúan. Un ejemplo de esto son los evaporadores y condensadores en los sistemas de refrigeración por aire, en los que un ventilador mejora la eficiencia de la transmisión entre el refrigerante y el aire ambiente. Otro ejemplo muy actual, son los conocidos como coolers de las computadoras. Aunque de pequeño tamaño, cumplen las mismas funciones, mejorando la transmisión entre un componente electrónico y una pieza, generalmente de aluminio o cobre, llamada radiador, para así disipar el calor producido por el paso de la corriente eléctrica. 

Los equipos de acondicionamiento de aire conocidos como Unidades de tratamiento del aire, disponen de uno o dos ventiladores centrífugos para hacer circular el aire a través de la unidad y de la red de conductos que distribuye el aire tratado en una edificación o en un proceso industrial. 

También utilizan un ventilador, generalmente centrífugo, los quemadores de las calderas de combustibles, tanto líquidos como gaseosos, para aportar el aire necesario a la combustión y facilitar la mezcla combustible-comburente en el interior del hogar.

Los dispositivos de ventilación utilizados en lugares en los que se requiere más ventilación que la natural proporcionada por los huecos de fachadas, son ventiladores que extraen el aire viciado y provocan la entrada de aire fresco por "depresión", o bien, impulsan aire fresco y evacúan el aire viciado por "sobrepresión". Aunque más caro, es más eficaz utilizar ambos sistemas simultáneamente, sobre todo si el aire se distribuye mediante bocas de entrada y salida en cada local.

Aunque tanto los ventiladores como los compresores tienen como función impulsar un gas aumentando su presión, entre ambos existen diferencias: El objeto fundamental de los primeros es mover un flujo de gas, a menudo en grandes cantidades, con aumentos generalmente reducidos de presión; mientras que los segundos están diseñados principalmente para producir grandes presiones y flujos de gas relativamente pequeños.

En el caso de los ventiladores, el aumento de presión es generalmente tan insignificante, comparado con la presión absoluta del gas, que la densidad de éste puede considerarse inalterada durante el proceso. Esto implica, que el gas puede modelarse como líquido incompresible y por consiguiente no hay diferencia entre la forma de operación de un ventilador y de una bomba, o lo que es lo mismo, matemáticamente se pueden tratar en forma análoga.

No existe una clasificación: de los ventiladores que se pueda considerar oficial o reconocida. Aquí vamos a ofrecer la siguiente:






Es el caso de ventiladores de velocidad variable mediante el uso de: reguladores eléctricos, compuertas de admisión o descarga, modificación del caudal por inclinación variable de los álabes de las hélices, etc.

Mención aparte tienen los ventiladores con uso exclusivo de refrescamiento que se utilizan en el ambiente doméstico o en pequeños espacios y que disponen de un sistema de soporte para su ubicación:

Los parámetros necesarios para la selección de un ventilador son; el caudal que debe mover y la pérdida de carga a vencer al rozamiento del aire con los conductos, rejillas, etc. Los ventiladores helicoidales pueden mover un gran caudal, pero comunican poca presión al aire, por lo que no se suelen utilizar en instalaciones de conductos. Para este caso los ventiladores habituales son los centrífugos, que pueden vencer una pérdida de carga elevada.

El caudal y la presión de un ventilador, son variables dependientes que se pueden relacionar mediante una curva de trabajo. Se ensaya el aparato variándole la carga desde el caudal máximo al caudal cero. Todos los pares de valores obtenidos caudal-presión se llevan a unos ejes coordenados, obteniéndose un grupo de curvas, cuyo conjunto recibe el nombre de característica del ventilador. 

Se observan en la figura curvas diferentes. Cada una de ellas representa un valor distinto y su lectura se hace en las escalas que la enmarcan.
Obsérvese que a descarga libre, es decir cuando la "Presión estática" (Pe) es nula, el ventilador da el máximo caudal que puede mover; en este punto la "Presión total" es igual a la "dinámica" (Pt = Pd). Asimismo, cuando el ventilador esta obturado, es decir que da el mínimo caudal, la "Presión dinámica" (Pd) es nula; en este punto, la "Presión total" es igual a la "estática" (Pt = Pe). Otra curva que se puede ver en el gráfico: es la curva de rendimiento (η), que se lee en % en la escala de la derecha. Se ve que el rendimiento del ventilador, depende del caudal que está moviendo y se marca el rendimiento máximo.
La zona idónea de trabajo del ventilador, por tanto, es el tramo A-B de su curva de "presión estática". Entre B y C su funcionamiento es inestable, el rendimiento desciende rápidamente y aumenta notablemente el ruido. Por ello, en muchos catálogos se representa sólo el tramo eficaz de funcionamiento, obviando el tramo hasta la presión máxima de que es capaz.

Para conocer el punto en que trabajará un ventilador, una vez determinada la pérdida de carga que debe vencer, no hay más que marcarla sobre el eje de ordenadas. A partir de aquí y con una horizontal se corta la curva de "Presión estática" en un punto, a partir del cual y mediante una línea vertical, en el eje de abscisas se obtiene el caudal que proporcionará el ventilador en cuestión, trabajando contra la pérdida de carga que se ha considerado inicialmente.




</doc>
<doc id="22083" url="https://es.wikipedia.org/wiki?curid=22083" title="Gnutella">
Gnutella

'Gnutella' es un proyecto de software distribuido para crear un protocolo de red de distribución de archivos entre pares, sin un servidor central.

El primer cliente para esta red fue desarrollado por Justin Frankel y Tom Pepper de Nullsoft, actuNullsoft. El código fuente fue liberado poco más tarde, bajo los términos de la licencia GPL. El evento fue anunciado de inmediato en Slashdot, y el programa fue descargado masivamente ese día.

Al día siguiente, AOL detuvo la disponibilidad del programa debido a problemas legales y prohibió a la división Nullsoft continuar trabajando en el proyecto. Esto no fue el fin de Gnutella; unos días más tarde el protocolo había sido descifrado por ingeniería inversa y varios clones de código abierto comenzaron a emerger. Este desarrollo paralelo de distintos clientes por distintos grupos continúa siendo hoy la manera en que se realiza el desarrollo de Gnutella hoy en día.

La red Gnutella sería una alternativa descentralizada, a sistemas semi-centralizados como Napster. La popularidad inicial de la red fue estimulada aún más tras la caída de Napster en el año 2001 por causas legales. Este crecimiento de popularidad reveló rápidamente los límites de la escalabilidad inicial del protocolo.

A principios del 2001, algunas variaciones del protocolo (liberadas al principio como clientes de código cerrado) mejoraron en alguna medida la escabilidad del protocolo. En vez de tratar cada usuario como cliente y servidor, algunos usuarios pasaron a ser tratados como "ultrapares", enrutando peticiones de búsquedas y respuestas para los usuarios conectados a ellos.

El nombre 'Gnutella' es un juego de palabras entre GNU y Nutella (un dulce de avellana). Supuestamente, Frankel y Pepper comían mucha Nutella mientras trabajaban en el proyecto original y utilizarían la licencia GPL de GNU para el programa terminado. Gnutella no está directamente asociada con el Proyecto GNU;[2] véase GNUnet para encontrar el equivalente propio de GNU.

Al contrario que otras redes de intercambio de ficheros, como eDonkey2000, Gnutella es una red P2P pura. Esto es, todos los nodos tienen la misma función, peso e importancia dentro de la red. El funcionamiento de la red pasa por tres fases:


La inundación producida por la fase de búsqueda es la debilidad más importante de este protocolo. Si hay muchas búsquedas a la vez, la red se llena de mensajes de búsqueda que los nodos se envían entre ellos. Además, este algoritmo de búsqueda no garantiza que el fichero sea finalmente encontrado incluso aunque algún nodo de la red lo tenga. Aun así, el hecho de que no exista un servidor central de búsqueda, como en el caso de eDonkey2000, hacen que este protocolo sea más robusto en caso de caídas de nodos.





</doc>
<doc id="22084" url="https://es.wikipedia.org/wiki?curid=22084" title="Kazaa">
Kazaa

Kazaa (antes llamado "KaZaA") fue una aplicación para el intercambio de archivos entre pares que utiliza el protocolo FastTrack. Kazaa fue comúnmente utilizado para intercambiar música (principalmente en formato mp3) y películas (en formato DivX). Su popularidad declinó conforme Sharman Networks y sus socios fueron objeto de demandas relacionadas con derechos de autor. Después Atrinsic, Inc compró la marca Kazaa para relanzarlo como una servicio de suscripción legal, eliminando el Software Kazaa manejando todo vía web. Desde agosto de 2012, el sitio web de Kazaa fue desactivado definitivamente

Kazaa y el protocolo FastTrack fueron creados por el sueco Niklas Zennstrom y el danés Janus Friis, y fueron presentados en marzo del 2001 por la empresa neerlandesa Consumer Empowerment. Con el fin de monetizar debido al explosivo crecimiento del programa, Sharman ofreció los banners publicitarios e iconos dorados que mostraba Kazaa para toda aquella empresa que buscara publicitar sus productos a la gente que usaba Kazaa. Incluso la propia Microsoft uso Kazaa como publicidad.
En 2003 un contrato celebrado con Altnet y Streamwaves con el fin de legalizar el programa hacía que los iconos dorados mostraran los primeros 30 segundos de una canción y para luego abrir la página web Streamwaves. en julio de 2006 celebró otro contrato con Universal Music, Sony BMG, EMI, Warner Music y varios más comprometiéndose a pagar cien millones de dólares en perdidas por derechos de autor y convertir a Kazaa en un servicio legal por parte de Atrinsic.
El programa siempre fue exclusivo para el sistema operativo Windows. Aunque pu.ede ser ejecutado en Linux, Mac OS X y otros sistemas operativos con software de emulación del entorno Win32 como WINE o Virtual PC. Era el único cliente de FastTrack pero MLDonkey tuvo un soporte experimental para esta red.

Debido al creciente uso de programas publicitarios (adware) por Kazaa, aparecieron derivados de él, que permiten usar la red de Kazaa, sin necesidad de instalar los spyware y adware, como es el Kazaa Lite Resurrection o el Kazaa Lite K++. A pesar de todo, Kazaa y
sus programas derivados están decayendo, y muchos de sus usuarios han pasado a programas P2P más modernos, como Ares Galaxy, Lphant, eMule o BitTorrent.

Los iconos dorados aparecía por encima de la lista de búsqueda ante determinados términos, estos iconos enlazaban a contenidos patrocinados (música, vídeos, software) ajenos a Sharman Network, algunos de estos contenidos era de pago.

Esta sección presenta aquellos programas que están basados en el cliente oficial de Kazaa. Para otros clientes FastTrack compatibles, ver FastTrack. 

Kazaa Media Desktop: Una extensión para catalogar música y vídeos en el disco duro local.

Kazaa Lite es una variante no autorizada del Kazaa original que excluye adware y spyware y provee de más funcionalidades. Está disponible desde abril del 2002. Puede ser descargado gratis y para mediados del 2005 su uso está difundido incluso aún más que el cliente Kazaa. Se conecta a la misma red FastTrack y permite intercambiar archivos con usuarios Kazaa. Fue creada por terceros, un grupo de programadores que modificaron los archivos binarios de la aplicación original del Kazaa. Versiones posteriores de Kazaa Lite incluyen K++, un parche de memoria que elimina ciertas restricciones en límites de búsquedas, límites multifuentes, y establece un "nivel de participación" al máximo de 1000. Sharman Networks considera al Kazaa Lite como una violación de derechos de autor.

Después que se detuvo el desarrollo de Kazaa Lite, aparecieron K-Lite v2.6 y Kazaa Lite Tools. Aunque pueda parecer que K-Lite está relacionado con Kazaa Lite por la similitud de nombre, en realidad se trata de proyectos diferentes. K-Lite no es una actualización de Kazaa Lite, sino que fue escrito de manera separada con muchos cambios fundamentales. Así como Kazaa Lite es una modificación de una versión antigua de Kazaa, K-Lite v2.6 requiere el ejecutable original KMD 2.6 para funcionar. K-Lite no incluye ningún código fuente de Sharman, pero requiere que el usuario remplace el original no parchado Kazaa Media Desktop y así se ejecuta en su ambiente, en el que remueve el malware y añade algunas funciones. Los autores creen que esta versión podría ser legal. De la misma manera esperan que dado que el cliente usa una nueva versión del programa Kazaa, no se verán afectados por los intentos de bloquear Kazaa Lite de la red.

En noviembre del 2004, los desarrolladores de K-Lite lanzaron K-Lite v2.7, el cual de manera similar, también requiere el ejecutable KMD 2.7. Actualmente, otras variantes usan un núcleo más antiguo (2.02), de esta manera K-Lite tienen algunas características que otros nunca tendrán. K-Lite incluyen múltiples tabs de búsqueda, una barra de herramientas personalizada y autoinicio. También tiene una búsqueda automática, un acelerador de descargas, una pantalla de bievenida opcional, opción de previsualización (para ver los archivos mientras se descargan), un bloqueador de IP, enlaces de soporte y un bloqueador de publicidad.

Kazaa Lite Tools por otro lado, es una actualización del original Kazaa Lite. Es una copia de Kazaa Lite con modificaciones y la inclusión de programas de terceros. Tiene los más recientes y mejores programas de terceros.

Kazaa Lite Resurrection es un duplicado of Kazaa Lite 2.4.3. Al principio fue bien recibido por los usuarios, sin embargo discusiones acerca de adware en KLS, acusaciones de spyware y la premisa de que marcadores de KLR estaban haciendo que la gente done falsamente, causaron una división entre el público usuario. Fue entonces cuando Kazaa Lite Tolls K++ apareció. No mucho después Kazaa Lite Revolutions apareció siendo casi una copia exacta de Kazaa Lite Resurrection. Se podría decir que era una copia con diferente nombre. Todas las versiones de Kazaa Lite estaban limpias de adware y spyware, excepto Kazaa Lite Revolutions y Resurrection.

En agosto del 2003, Kazaa Plus fue introducido por Sharman Networks. Es una versión premium no gratuita, presumiblemente sin spyware o adware. En un intento por seguir ganando dinero con el nombre Kazaa, se lanzó otra versión llamada Kazaa Gold. Esta versión no es un producto de Sharman Networks. Sin embargo en el 2004, Sharman Networks inició la compra de los dominios de estas compañías haciendo de esta manera que los sites redireccionaran al sitio real de Kazaa.

Desde sus comienzos, Kazaa ha sido acusado de instalar malware en las computadoras de los usuarios. Sharman, la compañía dueña de Kazaa, alega que sus productos no son adware y no recogen información personal de los usuarios. Durante un tiempo, la parte del código de Kazaa que se podía considerar adware era una parte opcional del programa Kazaa; no obstante, mediante un paso difícil de omitir durante la instalación para los usuarios promedio. Cuando surgieron los alegatos, el código fue fusionado en el software principal de Kazaa, sin ser posible desinstalarlo. Con frecuencia, programas de detección de spyware y remoción de software fallan a menudo al tratar de eliminar el código sin acciones especiales tomadas por el usuario.

Algunos malware instalados por Kazaa incluyen:

Como resultado de estos componentes adicionales, la página de internet de CNET's, Download.com dejó de distribuir KaZaA en abril del 2004.





</doc>
<doc id="22087" url="https://es.wikipedia.org/wiki?curid=22087" title="Napster">
Napster

Napster es un servicio de distribución de archivos de música (en formato MP3). Fue la primera gran red P2P de intercambio creado por Sean Parker y Shawn Fanning. Su popularidad comenzó durante el año 2000. Su tecnología permitía a los aficionados a la música compartir sus colecciones de MP3 fácilmente con otros usuarios, lo que originó las protestas de las instituciones de protección de derechos de autor.

El servicio es llamado "Napster" ("siestero") por el seudónimo de Fanning (se dice que solía dormir mucho la siesta).

La primera versión de Napster fue publicada a finales de 1999. Fue el primero de los sistemas de distribución de archivos entre pares de popularidad masiva, y era una red centralizada, ya que utilizaba un servidor principal para mantener la lista de usuarios conectados y archivos compartidos por cada uno de ellos. Las transferencias de archivos, sin embargo, eran realizadas entre los usuarios sin intermediarios.

A comienzos de 2000, varias empresas discográficas iniciaron un juicio en contra de Napster. Esto trajo a Napster una enorme popularidad y varios millones de nuevos usuarios. Napster alcanzó su pico con 26,4 millones de usuarios en febrero del año 2001.

Para los seguidores de Napster el juicio fue algo confuso. Para ellos la habilidad de compartir archivos era una característica propia de Internet, y no de Napster, el cual actuaba simplemente como un motor de búsqueda. Muchos argumentaban que de cerrar Napster sólo se conseguiría que sus usuarios emigraran hacia otros sistemas de intercambio de archivos. Esto último de hecho ocurrió, con software como Ares Galaxy, Audiogalaxy, Morpheus, Gnutella, Kazaa, Emule, LimeWire y eDonkey2000.

En julio de 2001 un juez ordenó el cierre de los servidores Napster para prevenir más violaciones de derechos de autor. Hacia el 24 de septiembre del 2001, había prácticamente llegado a su fin. Napster aceptó pagar a las empresas discográficas 26 millones de dólares por daños y otros 10 millones de dólares por futuras licencias.

El baterista de Metallica, Lars Ulrich fue el primer famoso en demandar a Napster por derechos de autor.

El 19 de mayo de 2008 Napster anunció el lanzamiento de la tienda más grande y más detallada de MP3 del mundo, con 6 millones de canciones, en "free.napster.com". El aviso también indicó que todas las ventas de descargas en Estados Unidos hechas con Napster ahora estarán en formato MP3. 

El 1 de diciembre de 2011 Napster se fusionó con Rhapsody y empezó a operar en diversos países de América y Europa como un nuevo servicio de pago.

Actualmente tiene un convenio con la empresa de telefonía celular Movistar como servicio de streaming para Latinoamérica compitiendo con otras plataformas como Deezer y Spotify.

Shawn Fanning en conjunto con dos amigos que conoció en línea; Jordan Ritter, su amigo de Boston, y Sean Parker de Virginia, crearon Napster en junio de 1999. Fanning quería un método más fácil para encontrar música, en lugar de buscar en IRC o en Lycos. John Fanning, el tío de Shawn en Hull, Massachusetts, se encargó de todas las operaciones de la compañía durante el período en que mantuvieron su oficina en Nantasket Beach. El acuerdo final le dio a Shawn el control sobre el 30% de la compañía, y el resto fue para su tío. Fue el primero de los sistemas de distribución peer-to-peer masivamente populares, aunque no era totalmente peer-to-peer debido a que usaba servidores centrales para mantener una lista de todos los sistemas conectados y los archivos que eran distribuidos, mientras que las transacciones eran de hecho realizadas entre las máquinas. Aunque ya existían redes que facilitaban la distribución de archivos a través del internet como IRC, Hotline y USENET, Napster se especializaba directamente en música en la forma de archivos MP3, presentados a través de una interfaz amigable al usuario. El sistema back-end fue diseñado por el principal arquitecto, Jordan Mendelson. El resultado fue un sistema cuya popularidad generó una enorme selección de música para descargar.

Aunque la industria discográfica denunció el hecho de "compartir" música como equivalente a robar, muchos usuarios de Napster se sintieron justificados en usar el servicio por varias razones. Muchos creían que la calidad de los nuevos álbumes había disminuido a finales de los 90, bajo la forma del típico álbum éxito en ventas sólo por una o dos canciones, entre varias canciones "de relleno" de menor calidad. Al mismo tiempo, el costo del CD virgen había caído inmensamente, pero el precio de los álbumes en CD se mantenía constante. La gente elogiaba a Napster porque les permitía obtener gratuitamente canciones éxito sin tener que comprar todo un álbum. Napster también hizo relativamente sencillo para los entusiastas de la música la descarga de copias de canciones que de otra manera serían difíciles de obtener, como canciones antiguas, grabaciones sin distribuir, y canciones grabadas libremente en conciertos. Algunos usuarios justificaron la descarga de copias digitales de grabaciones que ya habían comprado en otros formatos, como en LP y casete, antes de que el CD emergiera como el formato dominante de distribución de música.

Más allá de estas justificaciones, otros usuarios simplemente disfrutaban el intercambiar y descargar música gratuitamente. Con los archivos obtenidos a través de Napster, la gente frecuentemente hacía sus propios álbumes de recopilación en CD grabables, sin pagar en absoluto a la discográfica y/o distribuidora. Las redes de alta velocidad en los dormitorios de las universidades se sobresaturaron, generando alrededor del 80% del tráfico externo a consecuencia de la transferencia de archivos MP3. Algunas universidades bloquearon su uso en los campus por esta razón, incluso antes de tener problemas por facilitar la violación de copyright.

El servicio y programa eran inicialmente sólo para Windows, pero en el 2000 Black Hole Media realizó un cliente llamado Macster. Macster fue posteriormente comprado por Napster y designado el cliente oficial en Mac; en ese punto el nombre de "Macster" se dejó de lado. Incluso después de la adquisición de Macster, la comunidad Macintosh tenia una variedad de clientes Napster desarrollados independientemente. El más notable fue el cliente de código abierto llamado MacStar, creado por Squirrel Software a comienzos de 2000 y Rapster, creado por Overcaster Family en Brasil. La publicación del código fuente de MacStar pavimentó el camino para clientes Napster de terceros a través de todas las plataformas, que le daba a los usuarios opciones de distribución de música sin anuncios.

La banda de Heavy metal Metallica descubrió que un demo de su canción 'I Disappear' había estado circulando a través de la red de Napster, incluso desde antes de que fuera distribuido. Esto eventualmente dio paso a que la canción llegara a varias estaciones de radio a través de América, y atrajo la atención de Metallica sobre el hecho de que su catálogo entero de canciones también estuviera disponible. La banda respondió en el 2000 con un juicio en contra del servicio ofrecido por Napster. Un mes después, el rapero Dr. Dre, quien compartía la situación de Metallica, también realizó un juicio similar después de que Napster no eliminó sus obras de su servicio, incluso después de haber enviado una petición por escrito. Por separado, Metallica y Dr. Dre le entregaron miles de nombres de usuario a Napster, que ellos creían que estaban pirateando sus canciones. Un año después, Napster disipó a ambos, pero sólo después de ser cerrado por la corte de Ninth Circuit en un juicio por separado, por parte de varias de las mayores discográficas.

También en el 2000, Madonna, quien había tenido encuentros anteriormente con los ejecutivos de Napster para discutir sobre una posible alianza, se molestó cuando su sencillo "Music" había llegado a la web y a Napster antes de su lanzamiento comercial, causando una amplia cobertura en los medios. El uso de Napster había sido verificado en 26,4 millones de usuarios en todo el mundo, en febrero del 2001.

En el 2000, A&M Records y varias otras compañías discográficas demandaron a Napster, por contribución indirecta a la violación de derechos de autor bajo la Digital Millennium Copyright Act en Estados Unidos. La industria musical haría las siguientes afirmaciones acerca de Napster:


La corte encontró a Napster responsable de las tres afirmaciones.

Napster perdió el caso en el District Court y apeló a la Corte de Apelaciones de los Estados Unidos para el Ninth Circuit. Aunque el Ninth Circuit encontró que Napster era capaz de usos no-infractorios comercialmente importantes, afirmó la decisión del District Court. Posteriormente, el District Court ordenó a Napster monitorear las actividades de su red, y de impedir el acceso a material infractorio cuando fuera notificada la existencia del material. Napster fue incapaz de hacer esto, por lo que cerró su servicio en julio del 2001. Napster finalmente se declaró en bancarrota en el 2002 y vendió sus activos. Se había declarado fuera de línea desde el año anterior debido a las reglas de la corte.

Compañías y proyectos posteriores siguieron su modelo P2P de intercambio de archivos exitosamente como Gnutella, Freenet y muchos otros. Algunos servicios, como Grokster, Madster y la red EDonkey2000 original, fueron derribadas o cambiadas por circunstancias similares.

La banda The Offspring se involucró en manifestaciones en favor de Napster. Llegó el año 2000 y la banda se vio envuelta en problemas con su discográfica, Columbia Records. La idea de la banda de Dexter Holland era lanzar su nuevo disco a través de internet, mediante su web oficial. Además, como se ha mencionado anteriormente, la banda comenzó a abanderar una serie de protestas en favor de Napster, llegando incluso a distribuir gratuitamente todo tipo de merchandising con el logo de la compañía y lemas como "salvemos a Napster". Sin embargo e irónicamente, Napster realizó un comunicado prohibiendo a la banda californiana el uso de publicidad de la empresa con su logo porque violaban sus derechos e imágenes de copyright. La contradicción en que incurrió la empresa enfureció a sus clientes, que consideraban que Napster siempre había abogado por la libre distribución de contenidos en la red y, además, era desaprovechar una oportunidad que les brindaban artistas y grupos como los propios Offspring, Smashing Pumpkins, Limp Bizkit o Courtney Love.

Junto con las acusaciones de que Napster estaba afectando las ventas de la industria discográfica, también estaban aquellos que sentían justo lo contrario, de que el intercambio de archivos realmente estimulaba, más que afectar, las ventas. Las pruebas pudieron haber venido en julio del 2000 cuando las pistas del álbum "Kid A" de la banda inglesa de rock Radiohead hallaron su camino a Napster tres meses antes del lanzamiento del CD. Al contrario de Madonna, Dr. Dre o Metallica, Radiohead nunca había llegado al top 20 en los Estados Unidos. Además, "Kid A" era un álbum experimental sin sencillos, y recibió relativamente poco espacio de radio. Durante el lanzamiento del disco, se había estimado que el álbum había sido descargado gratuitamente por millones de personas en todo el mundo, y en octubre del 2000, "Kid A" llegó al número uno en el Billboard 200 en su semana debut. De acuerdo a Richard Menta de "MP3 Newswire", el efecto de Napster en esta instancia fue aislado de otros elementos a los que se les podría acreditar las ventas, y el éxito inesperado del álbum fue prueba de que Napster era una buena herramienta promocional para la música.

Una de las bandas más exitosas gracias al éxito que Napster proporcionó fue Dispatch. Siendo una banda independiente, ellos no tenían ninguna forma de promoción formal o espacio en radio, y aun así fueron capaces de irse de gira a ciudades en las que nunca habían tocado ni hecho conciertos, gracias a la distribución de su trabajo en Napster. En julio del 2007, la banda se convirtió en la primera banda independiente en titular el Madison Square Garden de Nueva York, vendiendo por tres noches consecutivas. Los miembros de la banda eran conocidos defensores de Napster. Shawn Fanning, el fundador de Napster, es un conocido fan de Dispatch.

Desde el 2000, muchos otros artistas, particularmente aquellos que no pertenecían a ninguna discográfica, o que no tenían acceso a los medios masivos de comunicación como la televisión y la radio, han dicho que Napster y las demás redes de intercambios de archivos han ayudado a que su música sea escuchada, a aumentar el consentimiento del público y que han mejorado sus ventas a gran escala. Un músico que defendió públicamente a Napster como una herramienta de promoción para artistas independientes fue Dj xealot, quien se envolvió directamente en la demanda de A&M Records en contra de Napster. Chuck D de Public Enemy también salió a apoyar públicamente a Napster. Aunque algunos músicos Underground y las marcas independientes han expresado su apoyo por Napster y el P2P que popularizaron, otros han criticado la irregulación y naturaleza extra-legal de estas redes, y algunos buscan implementar modelos de promoción en Internet en la cual puedan controlar la distribución de su propia música, como proveer canciones gratuitas en descarga o enlace desde sus propios sitios web, o cooperando con servicios de pago como "Insound", "Rhapsody" , ITunes Music Store de Apple así como el reciente servicio Apple music de Apple.

La facilidad de transferencia de material con derechos de autor hizo que la Recording Industry Association of America o RIAA (en español Asociación de la Industria Musical Norteamericana) tomara cartas en el asunto, quienes inmediatamente el 7 de diciembre de 1999 se fueron a juicio contra el popular servicio, lo cual le dio a Napster una enorme publicidad. Pronto millones de usuarios, en mayoría universitarios, estarían usándolo.

Después de una fallida apelación ante la corte del Ninth Circuit de Estados Unidos, un mandato judicial fue emitido el 5 de marzo de 2001, se le ordenó a Napster que previniera el intercambio de música con derechos de autor en su red. En julio del 2001, Napster cerraría su red completamente para cumplir con las exigencias.

El 24 de septiembre de 2001, el caso se había disipado parcialmente. Napster había acordado en pagar a los creadores de música y dueños de los derechos de autor en una cantidad de cerca de 26 millones por el uso sin autorización de la música, además de 10 millones en avance contra futuras regalías. Para poder pagar estas sumas, Napster convirtió su servicio gratuito en un servicio de subscripción, lo que provocó que el tráfico disminuyera. Una solución prototipo fue probada en la primavera del 2002: el Napster 3.0 Alpha, usando el formato ".nap" que tenía implementada seguridad por parte de PlayMedia Systems y una tecnología de autentificación digital por parte de Relatable. Napster 3.0 estaba, de acuerdo a varios empleados de Napster, listo para ser distribuido, pero obtuvo importantes problemas al obtener licencias para distribuir música de las grandes compañías de música.

El 17 de mayo de 2002, Napster anunció que sus activos serían adquiridos por la marca alemana de multimedia Bertelsmann por 85 millones. Sobre la base de ese acuerdo, el 3 de junio, Napster aspiraba a seguir el capítulo 11 de protección bajo las leyes de bancarrota de Estados Unidos. El 3 de septiembre de 2002, fue bloqueada la venta de Napster por parte de un juez, y forzó a Napster a liquidar sus deudas de acuerdo al capítulo 7 de la ley.

Después de una oferta de 2,43 millones por parte de Private Media Group, una compañía de entretenimiento para adultos, la marca y logos de Napster fueron adquiridos en una subasta por Roxio Inc., quienes lo emplearon en lugar de su servicio "pressplay" de distribución de música.

En septiembre del 2008, Napster fue comprada por Best Buy por 121 millones.

El 1 de diciembre de 2011, en un arreglo con Best Buy, Napster se fusionó con Rhapsody. Best Buy recibirá un interés minoritario de Rhapsody.

Napster está habilitado para otros dominios y para cualquier dispositivo como tablets, iPad y plataforma web.

El 15 de junio de 2016 la empresa Rhapsody anunció que cambiaría su nombre por el de la compañía para tratar de sacar provecho del gran peso de Napster en la historia de la música digital. Su nombre volverá a posicionarse al convertirse en una alternativa más en el mercado de los servicios de música en "streaming", como Spotify y Apple Music.

El 10 de octubre de 2013, Napster y Telefónica Digital (Unidad de Negocios digitales del Grupo Telefónica) firmaron un acuerdo de operación y distribución exclusiva del servicio de streaming de música para Latinoamérica. Como parte del acuerdo, los clientes de Sonora, el servicio de música por suscripción que ofrece Terra, la filial de Telefónica, fueron transferidos a Napster mientras Telefónica, que a raíz del acuerdo adquiere una participación accionaria en Rhapsody; podrá ofrecer también paquetes de servicios de música Napster a sus cientos de millones de clientes en todo mundo.
A la fecha, el Servicio de Napster se encuentra disponible en México, Colombia, Chile, Argentina y Brasil en dos modalidades: Napster Web y Premium, mientras que en Costa Rica se ofrece el servicio premium para ciertos clientes. La primera opción permite a los usuarios acceder a los contenidos sin cargo a través del sitio web con la posibilidad de escuchar música por tiempo ilimitado. La versión Premium ofrece el acceso con un mismo usuario desde la PC y hasta en 3 dispositivos móviles, permitiendo escuchar música de forma "online" y "offline". A través de una suscripción mensual, los clientes de las operadoras móviles de Telefónica podrán abonarlo con sus respectivas facturas.





</doc>
<doc id="22091" url="https://es.wikipedia.org/wiki?curid=22091" title="Sistema de control">
Sistema de control

Un sistema de control es un conjunto de dispositivos encargados de administrar, ordenar, dirigir o regular el comportamiento de otro sistema, con el fin de reducir las probabilidades de fallo y obtener los resultados teóricamente verdaderos. Por lo general, se usan sistemas de control industrial en procesos de producción industriales para controlar equipos o máquinas. 

Existen dos clases comunes de sistemas de control, sistemas de lazo abierto y sistemas de lazo cerrado. En los sistemas de control de lazo abierto la salida se genera dependiendo de la entrada; mientras que en los sistemas de lazo cerrado la salida depende de las consideraciones y correcciones realizadas por la retroalimentación. Un sistema de lazo cerrado es llamado también sistema de control con realimentación. Los sistemas de control más modernos en ingeniería automatizan procesos sobre la base de muchos parámetros y reciben el nombre de controladores de automatización programables (PAC).

Los sistemas de control deben conseguir los siguientes objetivos:


Necesidades de la supervisión de procesos

Control: selección de las entradas de un sistema de manera que los estados o salidas cambien de acuerdo a una manera deseada. Los elementos son:
Controlador: (Electrónica). Es un dispositivo electrónico que emula la capacidad de los seres humanos para ejercer control. Por medio de cuatro acciones de control: compara, calcula, ajusta y limita.

Proceso: operación o desarrollo natural progresivamente continúo, marcado por una serie de cambios graduales que se suceden uno al otro en una forma relativamente fija y que conducen a un resultado o propósito determinados. Operación artificial o voluntaria progresiva que consiste en una serie de acciones o movimientos controlados, sistemáticamente dirigidos hacia un resultado o propósito determinados. Ejemplos: procesos químicos, económicos y biológicos.

Supervisión: acto de observar el trabajo y tareas de otro (individuo o máquina) que puede no conocer el tema en profundidad.

Es aquel sistema en que solo actúa el proceso sobre la señal de entrada y da como resultado una señal de salida independiente a la señal de entrada, pero basada en la primera. Esto significa que no hay retroalimentación hacia el controlador para que éste pueda ajustar la acción de control. Es decir, la señal de salida no se convierte en señal de entrada para el controlador.

Estos sistemas se caracterizan por:

Son los sistemas en los que la acción de control está en función de la señal de salida. Los sistemas de circuito cerrado usan la retroalimentación desde un resultado final para ajustar la acción de control en consecuencia.

El control en lazo cerrado es imprescindible cuando se da alguna de las siguientes circunstancias:


Sus características son:

Un ejemplo de un sistema de control de lazo cerrado sería el termotanque de agua que utilizamos para bañarnos.

Otro ejemplo sería un regulador de nivel de gran sensibilidad de un depósito. El movimiento de la boya produce más o menos obstrucción en un chorro de aire o gas a baja presión. Esto se traduce en cambios de presión que afectan a la membrana de la válvula de paso, haciendo que se abra más cuanto más cerca se encuentre del nivel máximo.

Los sistemas de control son agrupados en tres tipos básicos:

1. Hechos por el hombre. 
Como los sistemas eléctricos o electrónicos que están permanentemente capturando señales del estado del sistema bajo su control y que al detectar una desviación de los parámetros preestablecidos del funcionamiento normal del sistema, actúan mediante sensores y actuadores, para llevar al sistema de vuelta a sus condiciones operacionales normales de funcionamiento. Un claro ejemplo de este será un termostato, el cual capta consecutivamente señales de temperatura. En el momento en que la temperatura desciende o aumenta y sale del rango, este actúa encendiendo un sistema de refrigeración o de calefacción.
1.1. Por su causalidad pueden ser: causales y no causales. Un sistema es causal si existe una relación de causalidad entre las salidas y las entradas del sistema, más explícitamente, entre la salida y los valores futuros de la entrada.
1.2. Según el número de entradas y salidas del sistema, se denominan:por su comportamiento

1.2.1. De una entrada y una salida o SISO ("single input, single output").

1.2.2. De una entrada y múltiples salidas o SIMO ("single input, multiple output").

1.2.3. De múltiples entradas y una salida o MISO ("multiple input, single output").

1.2.4. De múltiples entradas y múltiples salidas o MIMO ("multiple input, multiple output").
1.3. Según la ecuación que define el sistema, se denomina:

1.3.1. Lineal, si la ecuación diferencial que lo define es lineal.

1.3.2. No lineal, si la ecuación diferencial que lo define es no lineal.
1.4.Las señales o variables de los sistema dinámicos son función del tiempo. Y de acuerdo con ello estos sistemas son:

1.4.1. De tiempo continuo, si el modelo del sistema es una ecuación diferencial, y por tanto el tiempo se considera infinitamente divisible. Las variables de tiempo continuo se denominan también analógicas.

1.4.2. De tiempo discreto, si el sistema está definido por una ecuación por diferencias. El tiempo se considera dividido en períodos de valor constante. Los valores de las variables son digitales (sistemas binario, hexadecimal, etc), y su valor solo se conoce en cada período.

1.4.3. De eventos discretos, si el sistema evoluciona de acuerdo con variables cuyo valor se conoce al producirse un determinado evento.
1.5. Según la relación entre las variables de los sistemas, diremos que:

1.5.1. Dos sistemas están acoplados, cuando las variables de uno de ellos están relacionadas con las del otro sistema.

1.5.2. Dos sistemas están desacoplados, si las variables de ambos sistemas no tienen ninguna relación.
1.6. En función de la evolución de las variables de un sistema en el tiempo y el espacio, pueden ser:

1.6.1. Estacionarios, cuando sus variables son constantes en el tiempo y en el espacio.

1.6.2. No estacionarios, cuando sus variables no son constantes en el tiempo o en el espacio.
1.7. Según sea la respuesta del sistema (valor de la salida) respecto a la variación de la entrada del sistema:

1.7.1. El sistema se considera estable cuando ante cualquier señal de entrada acotada, se produce una respuesta acotada de la salida.

1.7.2. El sistema se considera inestable cuando existe por lo menos una entrada acotada que produzca una respuesta no acotada de la salida.
1.8. Si se comparan o no, la entrada y la salida de un sistema, para controlar esta última, el sistema se denomina:

1.8.1. Sistema en lazo abierto, cuando la salida para ser controlada, no se compara con el valor de la señal de entrada o señal de referencia.

1.8.2. Sistema en lazo cerrado, cuando la salida para ser controlada, se compara con la señal de referencia. La señal de salida que es llevada junto a la señal de entrada, para ser comparada, se denomina señal de feedback o de retroalimentación.
1.9. Según la posibilidad de predecir el comportamiento de un sistema, es decir su respuesta, se clasifican en:

1.9.1. Sistema determinista, cuando su comportamiento futuro es predecible dentro de unos límites de tolerancia.

1.9.2. Sistema estocástico, si es imposible predecir el comportamiento futuro. Las variables del sistema se denominan aleatorias.
2. Naturales, incluyendo sistemas biológicos. Por ejemplo, los movimientos corporales humanos como el acto de indicar un objeto que incluye como componentes del sistema de control biológico los ojos, el brazo, la mano, el dedo y el cerebro del hombre. En la entrada se procesa el movimiento y la salida es la dirección hacia la cual se hace referencia.
3. Cuyos componentes están unos hechos por el hombre y los otros son naturales. Se encuentra el sistema de control de un hombre que conduce su vehículo. Este sistema está compuesto por los ojos, las manos, el cerebro y el vehículo. La entrada se manifiesta en el rumbo que el conductor debe seguir sobre la vía y la salida es la dirección actual del automóvil. Otro ejemplo puede ser las decisiones que toma un político antes de unas elecciones. Este sistema está compuesto por ojos, cerebro, oídos, boca. La entrada se manifiesta en las promesas que anuncia el político y la salida es el grado de aceptación de la propuesta por parte de la población.
4. Un sistema de control puede ser neumático, eléctrico, mecánico o de cualquier tipo, su función es recibir entradas y coordinar una o varias respuestas según su lazo de control (para lo que está programado).
5. Control predictivo, son los sistemas de control que trabajan con un sistema predictivo, y no activo como el tradicional ( ejecutan la solución al problema antes de que empiece a afectar al proceso). De esta manera, mejora la eficiencia del proceso contrarrestando rápidamente los efectos.


Los problemas considerados en la ingeniería de los sistemas de control, básicamente se tratan mediante dos pasos fundamentales como son:


En el análisis se investiga las características de un sistema existente. Mientras que en el diseño se escogen los componentes para crear un sistema de control que posteriormente ejecute una tarea particular. 

Existen dos métodos de diseño:


El diseño por análisis modifica las características de un sistema existente o de un modelo estándar del sistema y el diseño por síntesis en el cual se define la forma del sistema a partir de sus especificaciones.

La representación de los problemas en los sistemas de control se lleva a cabo mediante tres representaciones básicas o modelos:

Los diagramas en bloque y las gráficas de flujo son representaciones gráficas que pretenden el acortamiento del proceso correctivo del sistema, sin importar si está caracterizado de manera esquemática o mediante ecuaciones matemáticas.
Las ecuaciones diferenciales y otras relaciones matemáticas, se emplean cuando se requieren relaciones detalladas del sistema. Cada sistema de control se puede representar teóricamente por sus ecuaciones matemáticas. El uso de operaciones matemáticas es patente en todos los controladores de tipo P, PI y PID, que debido a la combinación y superposición de cálculos matemáticos ayuda a controlar circuitos, montajes y sistemas industriales para así ayudar en el perfeccionamiento de los mismos.



</doc>
<doc id="22093" url="https://es.wikipedia.org/wiki?curid=22093" title="Programa espía">
Programa espía

El spyware o programa espía es un malware que recopila información de una computadora y después transmite esta información a una entidad externa sin el conocimiento o el consentimiento del propietario del computador.
El término "spyware" también se utiliza más ampliamente para referirse a otros productos que no son estrictamente "spyware". Estos productos, realizan diferentes funciones, como mostrar anuncios no solicitados ("pop-up"), recopilar información privada, redirigir solicitudes de páginas e instalar marcadores de teléfono.

Un "spyware" típico se autoinstala en el sistema afectado de forma que se ejecuta cada vez que se pone en marcha el ordenador (utilizando CPU y memoria RAM, reduciendo la estabilidad del ordenador), y funciona todo el tiempo, controlando el uso que se hace de Internet y mostrando anuncios relacionados.

Sin embargo, a diferencia de los virus, no se intenta replicar en otros ordenadores, por lo que funciona como un parásito.

Las consecuencias de una infección de "spyware" moderada o severa (aparte de las cuestiones de privacidad) generalmente incluyen una pérdida considerable del rendimiento del sistema (hasta un 50 % en casos extremos), y problemas de estabilidad graves (el ordenador se queda "colgado"). También causan dificultad a la hora de conectar a Internet.
Algunos ejemplos de programas espía conocidos son Gator o Bonzi Buddy.

Este nombre viene dado de las palabras en idioma inglés "spy" que significa espía, y "ware" significa programa.

La firma de seguridad informática Webroot publicó un listado del peor "spyware" del 2004 (el más peligroso y difundido), basado en la información recogida por su programa de rastreo Spy Audit. Estas son las principales amenazas:


Cuatro consejos: instale al menos dos de estas herramientas, úselas frecuentemente y actualice sus bases de datos por Internet (es un proceso similar al de los antivirus). 




</doc>
<doc id="22094" url="https://es.wikipedia.org/wiki?curid=22094" title="Iria Flavia">
Iria Flavia

Iria Flavia es el nombre de una parroquia de Padrón, en la provincia de La Coruña (Galicia), España. Está ubicada en la confluencia del río Sar y el río Ulla y fue un puerto importante.

"Iria" fue una ciudad galaica, capital del país de los caporos del "conventus iuridicus Lucensis" de la provincia "Hispania Citerior Tarraconensis", situada en la vía de "Bracara Augusta" a "Asturica Augusta" por la costa. Bajo Vespasiano, a través del Edicto de Latinidad del año 74, se transformó en "municipium", y tomó el nombre de "Iria Flavia".

En la Hispania visigoda fue sede episcopal de la iglesia católica, sufragánea de la Archidiócesis de Braga que comprendía la antigua provincia romana de Gallaecia en la diócesis de Hispania. Así, fue sede episcopal desde el Bajo Imperio y con suevos y visigodos, hasta que Alfonso II trasladó el obispado a Santiago de Compostela (entonces conocida como Compostela) con motivo del hallazgo del sepulcro de Santiago el Mayor, apóstol.

Cuando el nombre de Padrón se hizo más popular. el crecimiento se trasladó al centro urbano padronés e Iria se convirtió en un simple caserío. En la actualidad se tiende a recuperar su nombre de "Iria Flavia" y es el que se utiliza oficialmente en todos los documentos. 

Según la tradición, en "Iria Flavia" predicó por primera vez el Apóstol Santiago durante su estancia en España. Aquí trajeron su cuerpo y su cabeza poco tiempo después, sus discípulos Teodoro y Atanasio desde Jerusalén y en una barca de piedra. Se cuenta que amarraron la barca a un "pedrón", y de ahí el topónimo actual de Padrón. Los dos discípulos (después de enterrar el cuerpo del apóstol) se quedaron a predicar en Iria Flavia. El "pedrón" se encuentra actualmente bajo el altar de la Iglesia de Santiago de Padrón, junto a las aguas del río Sar y en pleno centro urbano padronés; donde en tiempos remotos se situaba el famoso puerto fluvial de Padrón, ahora situado pocos kilómetros antes.

La localidad es uno de los principales focos culturales e históricos de Galicia, con una importante oferta cultural. Destacan:


Pronto se unirán dos nuevos museos: el Museo de Arte Sacra de Padrón y el Museo de Historia de Padrón, que albergarán cartas, joyas, imágenes, obras... y multitud de documentos de importancia nacional. Se espera convertir con esto a Iria Flavia en uno de los principales focos de visitas a nivel nacional.

Ver: 






</doc>
<doc id="22096" url="https://es.wikipedia.org/wiki?curid=22096" title="Código morse">
Código morse

El código morse, también conocido como alfabeto morse o clave morse, es un sistema de representación de letras y números mediante señales emitidas de forma intermitente.

Fue desarrollado por Alfred Vail mientras colaboraba en 1830 con Samuel Morse en la invención del telégrafo eléctrico. Vail creó un método según el cual cada letra o número era transmitido de forma individual con un código consistente en rayas y puntos, es decir, señales telegráficas que se diferencian en el tiempo de duración de la señal activa. Morse reconoció la idoneidad de este sistema y lo patentó junto con el telégrafo eléctrico. Fue conocido como American Morse Code y fue utilizado en la primera transmisión por telégrafo.

La duración del punto es la mínima posible. Una raya tiene una duración de aproximadamente tres veces la del punto. Entre cada par de símbolos de una misma letra existe una ausencia de señal con duración aproximada a la de un punto. Entre las letras de una misma palabra, la ausencia es de aproximadamente tres puntos. Para la separación de palabras transmitidas el tiempo es de aproximadamente tres veces el de la raya. 

Toda correspondencia entre dos estaciones deberá comenzar con la señal de llamada. Para llamar, la estación que llama transmitirá el distintivo de llamada (no más de dos veces) de la estación requerida, la palabra DE seguida por su propia señal de llamada y la señal -. - a menos que hayan reglas especiales peculiares al tipo de aparato utilizado.

En sus comienzos, el alfabeto Morse se empleó en las líneas telegráficas mediante los tendidos de cable que se fueron instalando. Más tarde, se utilizó también en las transmisiones por radio, sobre todo en el mar y en el aire, hasta que surgieron las emisoras y los receptores de radiodifusión mediante voz.

En la actualidad, el alfabeto Morse tiene aplicación casi exclusiva en el ámbito de los radioaficionados y Scouts, y aunque fue exigido frecuentemente su conocimiento para la obtención de la licencia de radioperador aficionado hasta el año 2005, posteriormente, los organismos que conceden esa licencia en todos los países están invitados a dispensar del examen de telegrafía a los candidatos.

También se utiliza en la aviación instrumental para sintonizar las estaciones VOR, ILS y NDB. En las cartas de navegación está indicada la frecuencia junto con una señal Morse que sirve, mediante radio, para confirmar que ha sido sintonizada correctamente.

Pulsa en los enlaces para oír el sonido.

convenciones: — : raya (señal larga) · : punto (señal corta)

"Si se comete un error al transmitir el mensaje en morse, la señal "error" son seis ecos "E" en grupos de dos (../../..):

El código morse es difícil de aprender por lo que, para facilitar su aprendizaje, se suele utilizar una regla mnemotécnica que permite aprendérselo mediante un código consistente en asignar a cada letra una palabra clave determinada, que comienza con la letra que se quiere recordar. Luego, basta con sustituir cada vocal de la palabra clave por un "punto" o una "raya" según la siguiente regla:

NOTA: Aunque lo más acertado es utilizar el alfabeto código internacional.

Otra regla para mejorar el aprendizaje del código morse, recurre a la fuerte presencia que tienen las imágenes de las letras. A fin de ser el recurso que ayuda a la memoria. En las siguientes letras, se han marcado con color los puntos y líneas que corresponden a su respectivo código en morse.




</doc>
<doc id="22100" url="https://es.wikipedia.org/wiki?curid=22100" title="Pasteurización">
Pasteurización

La pasteurización o pasterización es un proceso térmico que es realizado en líquidos (generalmente alimentos) con la intención de reducir la presencia de agentes patógenos (como por ejemplo ciertas bacterias, protozoos, mohos, levaduras, etc.) que puedan contener, debido a las altas temperaturas muchos de los agentes bacterianos mueren. Este proceso de calentamiento lo llevó a cabo por primera vez, el científico químico francés Louis Pasteur, a quien le debe su nombre, junto a Claude Bernard el 20 de abril de 1864.

Uno de los motivos del tratamiento térmico es un método de control de microorganismos de los alimentos líquidos, alterando lo poco posible su estructura física, sus componentes químicos y sus propiedades organolépticas. Tras la operación de pasteurización, los productos tratados se enfrían rápidamente y se sellan herméticamente con fines de seguridad alimentaria; por esta razón, es básico en la pasteurización el conocimiento del mecanismo de la transferencia de calor en los alimentos. A diferencia de la esterilización, la pasteurización no destruye completamente las esporas de los microorganismos, ni elimina todas las células de microorganismos termofílicos.

Louis Pasteur mejoró la calidad de vida al hacer posible que productos alimenticios básicos, como la leche, se pudieran transportar largas distancias sin ser afectados por la descomposición. En la pasteurización, el objetivo primordial no es la «eliminación completa de los agentes patógenos» sino la disminución sustancial de sus poblaciones, reduciéndolas a niveles que no causen intoxicaciones alimentarias a los humanos (siempre que el producto pasteurizado se mantenga refrigerado correctamente y que se consuma antes de la fecha de caducidad indicada). En la actualidad, la pasteurización es objeto de cada vez más polémicas por parte de ciertas agrupaciones de consumidores en todo el mundo, debido a las cuestiones existentes sobre la destrucción de vitaminas y alteración de las propiedades organolépticas (sabor y calidad) de los productos alimenticios tratados con este procedimiento.

Los primeros procesos para esterilizar alimentos en envases cerrados, se han atribuido históricamente al inventor francés Nicholas Appert en sus investigaciones realizadas en el siglo XVIII. No obstante algunas investigaciones demuestran que con anterioridad ya se había intentado esterilizar alimentos en recipientes sellados. Hacia finales de siglo XIX, químicos alemanes trasladaron este procedimiento a la leche cruda, y ya por entonces (antes de Pasteur) se empezó a «sospechar» que los tratamientos térmicos resultaban eficaces para destruir las bacterias presentes en la leche. De este modo, se dio origen no solo a un importante método de conservación, sino también a una medida de higiene fundamental para proteger la salud de las personas y conservar la calidad de los alimentos. Estos trabajos sentaron las bases de lo que Pasteur posteriormente descubriría y explicaría científicamente.

Algunos de los contemporáneos de Pasteur, incluido el eminente químico alemán Justus von Liebig, insistían en que la fermentación era un proceso puramente químico y que no requería en absoluto de la intervención de ningún organismo vivo. En el año 1864, a instancias del emperador Napoleón III, Pasteur investigó la causa por la que el vino y la cerveza se agriaban con el paso del tiempo, causando grandes pérdidas económicas a las empresas francesas debido a lo perecedero de estas mercancías. Pasteur regresó al pueblo de su infancia, Arbois, con el objetivo de resolver el problema definitivamente. Allí estudió el problema que afectaba a las viñas. Con ayuda de un microscopio, descubrió que, en realidad, intervenían dos tipos de organismos —una levadura y una bacteria de la familia "acetobacter"— que eran la clave del proceso de fermentación. Uno producía alcohol y el otro ácido acético, que agriaba el vino produciendo el vinagre.

Pasteur utilizó un nuevo método para eliminar los microorganismos que pudieran degradar el vino o la cerveza: después de almacenar el líquido en cubas bien selladas se elevaba su temperatura hasta los 44 °C durante un breve periodo de tiempo. Comprobó experimentalmente que las poblaciones de bacterias del género "Acetobacter" se reducían en extremo hasta quedar «casi esterilizado» el alimento. A pesar del horror inicial de la industria ante la idea de calentar el vino, un experimento controlado con lotes de vino calentado y sin calentar demostró de forma contundente la efectividad del procedimiento. Con posterioridad, Charles North aplicó con éxito el mismo método de Pasteur a la leche en el año 1907. Pasteur dio el primer paso en el que sería este nuevo método, denominado posteriormente «pasteurización» en su honor, y lo fue aplicando a otros alimentos líquidos. Este proceso se aplica hoy en día como norma de higiene en muchos procesos básicos de la industria alimentaria y proporciona una garantía de la seguridad de muchos productos alimenticios de manera eficaz en todo el mundo.

La historia de la esterilización de los alimentos fue revisada por Harold Burton (1988). Los esterilizadores fueron patentados y construidos para calentar leche a temperaturas que van desde los 130 °C hasta los 140 °C antes del siglo XIX, curiosamente antes de que sus beneficios fueran entendidos completamente. La leche esterilizada se desarrolló industrialmente en el año 1921, y el proceso de inyección de vapor fue desarrollado en 1927 por G. Grindrod en Estados Unidos. Sin embargo, las iniciativas más relevantes que dieron lugar a la comercialización del método UHT se empezaron a desarrollar a fines del decenio de 1940, debido a la técnica desarrollada en los esterilizadores de tubos concéntricos y de vapor de uperización en los sistemas de producción de leche. Debe entenderse que los esfuerzos de aquella época eran muy grandes en la industria para lograr envasar asépticamente la leche, hasta que finalmente se logró con éxito en el año 1961.

La pasteurización es un proceso térmico químico realizado a los alimentos: los procesos térmicos se pueden realizar con la intención de disminuir las poblaciones patógenas de microorganismos o para desactivar las enzimas que modifican los sabores de ciertos alimentos. No obstante, en la pasteurización se emplean generalmente temperaturas por debajo del punto de ebullición (en cualquier tipo de alimento), ya que en la mayoría de los casos las temperaturas superiores a este valor afectan irreversiblemente ciertas características físicas y químicas del producto alimenticio. Así, por ejemplo, si en la leche se sobrepasa el punto de ebullición, las micelas de la caseína se «coagulan» irreversiblemente (o dicho de otra forma, la leche se «cuaja»). El proceso de calentamiento de la pasteurización, si se hace a bajas temperaturas, tiene además la función de detener los procesos enzimáticos. Hoy en día, la pasteurización se realiza a los alimentos en un proceso industrial continuo aplicado a alimentos viscosos, con la intención de utilizar la energía de manera eficiente y disminuir así también costes de producción.

Existen tres tipos de procesos bien diferenciados: pasteurización VAT o lenta, pasteurización a altas temperaturas durante un breve período (HTST, High Temperature/Short Time) y proceso a altas temperaturas (UHT, Ultra-High Temperature).

Del inglés "Vat" = tina, tinaja, por hacerse en recipientes grandes. Llamada también pasteurización lenta. Fue el primer método de pasteurización, aunque la industria alimentaria lo ha ido renovando por otros sistemas más eficaces. El proceso consiste en calentar grandes cantidades de leche en un recipiente estando a 63 °C durante 30 minutos, para luego dejar enfriar lentamente. Debe pasar mucho tiempo para continuar con el proceso de envasado del producto, a veces más de 24 horas.

Este método es el empleado en los líquidos a granel, como la leche, los zumos de fruta, la cerveza, etc. Por regla general, es el más práctico, ya que expone al alimento a altas temperaturas durante un período breve y además se necesita poco equipamiento industrial para poder realizarlo, reduciendo de esta manera los costes de mantenimiento de equipos. Entre las desventajas del proceso está la necesidad de contar con personal altamente cualificado para la realización de este trabajo, que necesita controles estrictos durante todo el proceso de producción.

Existen dos métodos distintos bajo la categoría de pasteurización HTST: en "batch" (lote) y en «flujo continuo». Para ambos métodos la temperatura es la misma (72 °C durante 15 segundos).

El proceso UHT es de flujo continuo y mantiene la leche a una temperatura superior más alta que la empleada en el proceso HTST, y puede rondar los 138 °C durante un período de al menos dos segundos. Debido a este muy breve periodo de exposición, se produce una mínima degradación del alimento. La leche cuando se etiqueta como «pasteurizada» generalmente se ha tratado con el proceso HTST, mientras que la leche etiquetada como «ultrapasteurizada» o simplemente UHT, se debe entender que ha sido tratada por el método UHT.

El reto tecnológico del siglo XXI es poder disminuir lo más posible el período de exposición a altas temperaturas de los alimentos, haciendo la transición de altas a bajas temperaturas lo más rápida posible, disminuyendo el impacto en la degradación de las propiedades organolépticas de los alimentos; por esta razón, se está investigando la tecnología basada en microondas, que permite este tipo de efectos (es empleado incluso en carnes). Este método es muy adecuado para los alimentos líquidos ligeramente ácidos (la acidez se mide con el pH), tal como los zumos de frutas y los zumos de verduras (como el gazpacho), ya que permite períodos de conservación de 10 a 45 días si se almacenan refrigerados a 10 °C.

Los métodos de pasteurización corresponden a una serie de métodos estandarizados por los responsables de alimentación de cada país y son controlados por las agencias encargadas de vigilar la calidad de la alimentación (algunos ejemplos son la USDA en Estados Unidos y la Food Standards Agency en el Reino Unido) mediante la implementación de un derecho alimentario específico. Estas agencias requieren y vigilan que, por ejemplo, los lácteos pasteurizados mediante HTST lleven la etiqueta alimentaria adecuada. Por regla general existen diferentes estándares en función de los lácteos a procesar. El principal factor a tener en cuenta es el contenido graso del producto. De esta forma, los parámetros de pasteurización de la nata difieren de los parámetros empleados para la leche desnatada, y los parámetros para pasteurizar queso se diseñan e implementan de tal forma que no se destruyan las enzimas que procesan los fosfatos, útiles para mantener las propiedades de corte y textura de los quesos.

Los métodos estándares de pasteurización HTST han sido designados para alcanzar una extensión del periodo de caducidad de cerca de 5 días (es decir 0,00001 veces el período original) reduciendo el número de microorganismos en la leche y otros alimentos. Este método es considerado adecuado para la reducción de poblaciones de células vegetativas de casi todas las bacterias patógenas, incluyendo aquellas bacterias resistentes a las altas temperaturas (particularmente las especies "Mycobacterium tuberculosis", causante de la tuberculosis, y "Coxiella burnetii", causante de la fiebre Q en la leche). El proceso de pasteurización HTST no elimina las esporas bacterianas debido a que el proceso emplea un régimen temperatura-tiempo de 75°C por 15 segundos, siendo insuficiente para la reducción de esporas bacterianas, debido a su alta resistencia frente al calor, requiriéndose normalmente temperaturas mayores a 100°C para que el tiempo de exposición sea relativamente corto y evitar daños en los componentes nutricionales y sensoriales de los alimentos. Sin embargo, el proceso se diseña de manera que los productos sean calentados uniformemente, evitando que mientras que algunas partes sean sometidas a excesivas temperaturas durante demasiado tiempo, otras no lleguen a los parámetros necesarios.

La pasteurización es un proceso que sigue una cinética química de primer orden. Denominamos N al número de microorganismos vivos a una temperatura dada de exposición T, y N a la población de microorganismos inicialmente. Si K es la constante cinética de muerte debido a la temperatura (velocidad de muerte de los microorganismos), la disminución en la población (cultivo) depende de la siguiente fórmula exponencial:

formula_1

Esta fórmula es fundamental para determinar la evolución de un cultivo en función de la temperatura. Se puede ver en ella una gran dependencia con la temperatura de exposición T. La fórmula es el fundamento, además, de los denominados «diagramas de supervivencia» en la industria de la alimentación, donde log(N/N) es el tiempo de exposición a una temperatura T fija. Típicamente las gráficas de supervivencia de los microorganismos al calor aparecen como líneas rectas en una escala semilogarítmica. La correlación existente entre la velocidad (o ratio) de muerte de microorganismos y la temperatura cumple la ecuación de Arrhenius.

Un factor importante asignado a cada microorganismo es el denominado «tiempo de reducción decimal» o también «valor D» de un microorganismo, y se define como el tiempo necesario para que a una temperatura determinada se pueda reducir el 90 % su población en el producto tratado. Es una expresión de la resistencia de un microorganismo al efecto de la temperatura. Su expresión es:

formula_2

Donde formula_3 es el período al que se expone la muestra, N es la población inicial y N la población final. Pueden obtenerse diferentes valores D para un microorganismo dado, o para un proceso particular de un alimento, determinando los sobrevivientes a diferentes temperaturas. Altos valores de D indican que el microorganismo es más resistente que otros que poseen un valor inferior. Existen otros valores como la constante de resistencia térmica, conocida frecuentemente como valor z, que se define como la diferencia en temperaturas necesaria para causar una reducción de un 90 % en el valor D. Esta pasteurización elimina en un 98 % de bacterias como "Vibrio cholera", "Shigella" o "E. coli"..

La acidez tiene mucha influencia en el grado de supervivencia de cada organismo bacteriano. El principal parámetro para caracterizar la acidez es el pH. En general la mayoría de alimentos se consideran ácidos o poco ácidos. Hay que considerar que la mayoría de las bacterias tóxicas como las de la especie "Clostridium botulinum" ya no están activas por debajo de un valor de pH de 4,5 (es decir que un simple zumo de limón las desactiva). Los alimentos se pueden considerar como ácidos si están por debajo de este valor de pH. La mayoría de los glúcidos se encuentran en este rango, sobre todo los monosacáridos. En el caso de alimentos con un pH superior, es necesario un tratamiento térmico de 121 °C durante tres minutos (o un proceso equivalente) como procesamiento mínimo (es decir, la leche, las verduras, las carnes, el pescado, etc.). No obstante, muchos de estos alimentos se convierten en ácidos cuando se les añade vinagre, zumo de limón, etc., o simplemente fermentan cambiando su valor de acidez. La causa de este efecto reside en la desactivación de la actividad microbiana debida a la simple influencia que posee por el valor de la acidez, indicada por el pH, sobre la condición de vida de estos microorganismos.

Algunos organismos y bacterias cultivados en los alimentos son resistentes a la pasteurización, como los bacilos de las especies "Bacillus cereus" (pudiendo llegar a prosperar cultivos de éstos incluso a bajas temperaturas), y "Geobacillus stearothermophilus". No obstante la resistencia a la eliminación térmica depende en gran medida del pH, actividad acuosa, o simplemente de la composición química de los alimentos, la facilidad o probabilidad de volver a ser contaminados (en lo que se denomina en inglés "postprocessing contamination", o PPC)

Mencionar la forma como un factor a tener en cuenta en la pasteurización del alimento es equivalente a decir que lo que influye es la superficie exterior del alimento. Cabe pensar que el principal objetivo del proceso de pasteurización es el incremento de la razón entre la capacidad de enfriamiento y la superficie del mismo. De esta forma, el peor ratio corresponde a los alimentos similares a una esfera. En el caso de los alimentos líquidos, se procura que tengan formas óptimas para que la variación de temperatura, tanto en calentamiento como en enfriamiento, pueda obtener ratio óptimo.

Algunas propiedades térmicas del alimento afectan de forma indirecta al rendimiento final de la pasteurización sobre el mismo, como la capacidad calorífica (la cantidad de energía que hay que «inyectar» por unidad de masa de alimento para que suba de temperatura), la conductividad térmica (garantiza la homogeneidad del proceso en el alimento), la inercia térmica (los alimentos con menor inercia térmica son más susceptibles de ser pasteurizados que los que poseen mayor inercia).

Desde sus orígenes, la pasteurización se ha asociado con la leche. El primer investigador que sugirió este proceso para el producto lácteo fue el químico agrícola alemán Franz von Soxhlet en el año 1886, siendo Charles North quien aplicó dicho método a la leche por primera vez en el año 1907. Los microorganismos activan sus poblaciones creciendo de forma óptima en el intervalo de temperatura de 25 °C a 37 °C. Por esta razón, durante el proceso de manufacturación y envasado de la industria láctea se evita que la temperatura de la leche esté en este intervalo después de la pasteurización. La leche es por regla general un medio ligeramente ácido con un pH menor que 7 (6,7). La leche de vaca pasteurizada por el método HTST y que ha sido correctamente refrigerada tiene un periodo de caducidad extendido que puede llegar a dos o tres semanas, mientras que la leche ultrapasteurizada puede tener una vida extendida que oscila entre dos y tres meses. Se puede llegar a períodos de conservación mayores (incluso sin refrigeración) cuando se combina la pasteurización UHT con manipulación adecuada y tecnologías de envases esterilizados. Al mismo tiempo que se reducen las colonias, se eliminan también de la leche los microorganismos más termosensibles, como los coliformes, inactivándose la fosfatasa alcalina (el nivel de esta enzima define el grado de eficiencia aplicado a la pasteurización de la leche; véase test de la fosfatasa). A pesar de aplicar la pasteurización, la leche tratada sigue conteniendo una cierta actividad microbiana, por regla general bacterias lácticas (no patógenas, aunque sí capaces de hacer fermentar la leche) y es necesaria la refrigeración.

Consumir leche cruda de animales, sin pasteurizar, expone a ciertos riesgos de contacto con organismos y bacterias causantes de enfermedades. En algunos países se ha llegado a prohibir su venta. Algunas de las enfermedades evitadas con la pasteurización de la leche son la tuberculosis ("Mycobacterium tuberculosis"), la difteria, la polio, la salmonelosis, la fiebre escarlata,la brucelosis y las fiebres tifoideas. Hoy en día, muchas de estas enfermedades no tienen una gran relevancia debido al empleo generalizado de los procesos de pasteurización en las primeras etapas de manipulación de la leche.

Entre las especies de organismos cuyas poblaciones se pueden reducir considerablemente con la pasteurización de la leche se cuentan los siguientes:

La pasteurización de la leche ha sido objeto poco a poco de una polémica creciente. Por una parte, se ha descubierto que algunos organismos patógenos han desarrollado una resistencia a la disminución de población con la temperatura, consiguiendo sobrevivir a la pasteurización en cantidades significativas. Los investigadores han desarrollado diagnósticos más sensibles, como la reacción en cadena de la polimerasa (denominada también PCR), que han permitido analizar la supervivencia de las cepas de diferentes microorganismos a la pasteurización de la leche. Se ha detectado que la pasteurización en ciertas condiciones destruye la vitamina A y la vitamina B.

Los zumos envasados (e incluso los néctares) se someten a dos tipos diferentes de procesos de pasteurización: por un lado existen los zumos sin procesar ("crudos"); por otro, los zumos ultrapasteurizados o zumos estériles.

Los productores de zumos están familiarizados con los procesos de pasteurización y con ambos métodos: el VAT o proceso "batch" (empleado en los productores de pequeño tamaño de producción) y el UHT (empleado en los productores de mayor producción). El método HTST es aceptado en la industria, ya que no produce una degeneración apreciable del sabor. La pasteurización es muy efectiva en los zumos debido a que son medios ácidos y evitan la proliferación de microorganismos esporulados, los más resistentes a las altas temperaturas. En muchos países, como Estados Unidos, el 95 % de los zumos comercializados son pasteurizados. En algunas ocasiones se exige por parte de los organismos encargados de la vigilancia e higiene alimentaria que se le indique al consumidor que está tomando un «zumo crudo». Los zumos suelen ser tratados térmicamente por el método de pasteurización a 70 °C durante 30 minutos, pero la temperatura ideal en función del pH es en la actualidad objeto de investigación.

Dependiendo de su origen, los zumos contienen diversos microorganismos y es necesario reducir la concentración total de sus poblaciones mediante la pasteurización. De esta forma, se sabe que el zumo de manzana puede contenir las especies "Salmonella typhimurium", "Cryptosporidium" y "Escherichia coli". En el zumo de naranja es habitual encontrar las especies "Bacillus cereus", "Salmonella typhi" y "Salmonella hartford". En algunos zumos de verduras, generalmente en los zumos poco ácidos, como el zumo de zanahoria, existe un riesgo particular de permanencia de la especie "Clostridium botulinum".

Los zumos pueden sufrir alteraciones en su color y tienden al marrón debido al deterioro enzimático de la polifenoloxidasa. Esto obedece en parte a la presencia de oxígeno en el líquido. Por ello, a los zumos y los néctares se les suele eliminar el aire antes de comenzar el proceso de pasteurización. De la misma forma, la pérdida de vitamina C y de caroteno se ve disminuida mediante desaireación previa.

Se ha descubierto que ciertas poblaciones de la especie "Mycobacterium avium" (pertenecientes a la subespecie "M. avium paratuberculosis"), causante de la enfermedad de Johne en los animales de sacrificio —y se sospecha que también de la enfermedad de Crohn en los humanos—, han sobrevivido a pasteurizaciones de ciertos alimentos lácteos en los Estados Unidos, el Reino Unido, Grecia y la República Checa. A la vista de la supervivencia de ciertas especies además de la anterior, las autoridades del Reino Unido encargadas de vigilar la calidad de los alimentos decidieron revaluar los estándares de pasteurización.

Un método actual es la pasteurización flash o instantánea, que utiliza menores tiempos de exposición a altas temperaturas y parece ser un método adecuado para conservar las propiedades organolépticas de los alimentos, pues preserva mejor el sabor y la textura de los mismos. La pasteurización fría es una denominación usada a veces como sinónimo de radiación ionizante (véase irradiación de alimentos) u otros significados (por ejemplo, químicos) para reducir las poblaciones de bacterias en los alimentos. La irradiación de alimentos también se denomina a veces «pasteurización electrónica». Se ha investigado la posibilidad de extender la pasteurización a alimentos no fluidos, como la carne de ternera. Un avance en la pasteurización no intrusiva que soluciona muchos problemas de la industria conservera es la denominada pasteurización electromagnética de alimentos líquidos, que emplea microondas a 2,45 GHz de frecuencia para activar los procesos térmicos. Este método ha demostrado su eficiencia en la pasteurización del agua.

Existen estudios orientados al Tercer Mundo en los que es posible realizar lo que se denomina pasteurización solar. La idea está fundamentada en la cocina solar y en el hecho de que no es necesario llevar los líquidos a ebullición para lograr la pasteurización, pudiendo pasteurizar con este método con temperaturas sobre los 56 °C. Con esta medida se intenta prevenir la causa de enfermedades causadas por la ingesta de aguas contaminadas. El método es conocido como «pasteurización del agua», en el que se han desarrollado ciertos elementos capaces de indicar el estado de pasteurización del agua y su posibilidad de ingesta segura. Uno de los más empleados es el "water pasteurization indicator" (WAPI). La pasteurización solar requiere exponer el agua en recipientes durante seis horas. El programa que se aplicó en ciertas regiones de África se denominó SODIS (abreviación de "solar disinfection").

Aparte de la leche y los zumos, otros alimentos son pasteurizados por la industria alimenticia; por regla general, son aquellos que poseen una estructura líquida o semilíquida. Algunos de los más mencionados son los siguientes:





</doc>
<doc id="22102" url="https://es.wikipedia.org/wiki?curid=22102" title="Alfabeto por palabras">
Alfabeto por palabras

Un alfabeto por palabras es un conjunto de palabras de las que cada una representa a una letra del alfabeto. Por lo tanto, el sistema consiste en tantas palabras como letras tiene el alfabeto, y cada una de estas palabras comienza con una letra del alfabeto. 

Los alfabetos de palabras se usan para la comunicación entre dos o más personas por radio o por teléfono en los casos en los que es importante que no se produzcan errores en la comprensión de datos o mensajes. Los alfabetos por palabras se usan de forma generalizada en todo tipo de actividades, y muy especialmente en la navegación marítima y aérea.

Existen múltiples alfabetos por palabras en los más diversos idiomas. El más conocido es, el que estableció la Organización Internacional de Aviación Civil, y después el alfabeto fonético de la OTAN que es básicamente una copia del anterior.

Los siguientes ejemplos proceden de varias lenguas.


</doc>
<doc id="22110" url="https://es.wikipedia.org/wiki?curid=22110" title="Niobio">
Niobio

El niobio es un elemento químico de número atómico 41 situado en el grupo 5 de la tabla periódica de los elementos. Se simboliza como Nb. Es un metal de transición dúctil, gris, blando y poco abundante. Se encuentra en el mineral niobita, también llamado columbita, y se utiliza en aleaciones. Se emplea principalmente aleado en aceros, confiriéndoles una alta resistencia. Se descubrió en el mineral niobita.

El niobio tiene propiedades físicas y químicas similares a las del elemento tantalio, y los dos son, por lo tanto, difíciles de distinguir. El químico inglés Charles Hatchett informó de un nuevo elemento similar al tántalo en 1801 y lo llamó columbio. En 1809, el químico William Hyde Wollaston inglés concluyó erróneamente que el tántalo y el columbio eran idénticos. El químico alemán Heinrich Rose determinó en 1846 que los minerales de tántalo contenían un segundo elemento, que él nombró niobio. En 1864 y 1865, una serie de descubrimientos científicos clarificó que el niobio y el columbio eran el mismo elemento (a diferencia de tantalio), y desde hace un siglo se utilizaron ambos nombres indistintamente. El niobio fue adoptado oficialmente como el nombre del elemento en 1949, pero el nombre de columbio sigue siendo de uso corriente en la metalurgia en los Estados Unidos.

No fue hasta el siglo XX que el niobio fue utilizado por primera vez en el mercado. Brasil es el principal productor de niobio y ferroniobio (una aleación de niobio y hierro). El niobio se utiliza sobre todo en aleaciones, la mayor parte en acero especial igual que el utilizado en tuberías de petróleo y gas. Aunque las aleaciones contienen sólo un máximo de 0,1 %, este pequeño porcentaje de niobio mejora la resistencia del acero. El niobio se utiliza en diversos materiales superconductores. Estas aleaciones superconductoras, también contienen titanio y estaño, que son ampliamente utilizados en los imanes superconductores de escáneres de resonancia magnética. Otras aplicaciones de niobio incluyen su uso en soldadura, industrias nucleares, electrónica, óptica, numismática y joyería. En las dos últimas aplicaciones, su bajo nivel de toxicidad de niobio y su capacidad de ser coloreado por anodización son ventajas particulares. El niobio es un componente importante de los catalizadores de alto rendimiento para la oxidación selectiva de propano a ácido acrílico.

 Metal gris, dúctil, y paramagnético que se encuentra en el grupo 5 de la Tabla Periódica. Aunque en comparación con el resto de los miembros. Tiene una configuración atípica en sus capas de electrones más externos.
El niobio se convierte en un superconductor a temperaturas criogénicas. A presión atmosférica, que tiene la temperatura crítica más alta de los superconductores elementales, el niobio tiene mayor profundidad de penetración magnética que cualquier elemento. Además, es uno de los superconductores de tipo tres elemental II, junto con vanadio y tecnecio. Las propiedades superconductoras son fuertemente dependientes de la pureza del niobio metal. Cuando es muy puro, es relativamente más blando y dúctil, pero las impurezas hacen que sea más duro. 
El metal tiene una baja sección transversal para los neutrones térmicos; por lo que se utiliza en las industrias nucleares.

El metal adquiere un tinte azulado cuando se expone al aire a temperatura ambiente durante largos períodos de tiempo. A pesar de presentar un alto punto de fusión, en forma elemental (2468 °C), tiene una baja densidad en comparación con otros metales refractarios. Además, es resistente a la corrosión, presenta propiedades de superconductividad, y forma capas de óxido dieléctrico. 

El niobio es un poco menos electropositivo y más compacto que su predecesor en la tabla periódica, el circonio, mientras que es prácticamente idéntica en tamaño a los átomos del tantalio más pesados, debido a la contracción de los lantánidos
Como resultado, las propiedades químicas del niobio son muy similares a las del tantalio, que aparece directamente debajo del niobio en la tabla periódica. Aunque su resistencia a la corrosión no es tan notable como la del tántalo, el niobio tiene un precio más bajo y una mayor disponibilidad y esto lo hace atractivo para usos menos exigentes, como pueden ser revestimientos en plantas químicas .

Se estima que el niobio es el 33 º elemento más común en la superficie de la Tierra, con 20 ppm. Algunos piensan que la abundancia en la Tierra es mucho mayor, pero que el niobio "perdido" puede estar situado en el núcleo de la Tierra debido a la alta densidad del metal. El elemento no se encuentra libre en la naturaleza, pero el niobio se produce en combinación con otros elementos minerales. Los minerales que contienen niobio a menudo también contienen tántalo. Los ejemplos incluyen la columbita [(Fe, Mn)NbO] y coltan [(Fe, Mn)(Nb, Ta)O]. Los minerales columbita-tantalita (como el coltán) se encuentran generalmente como minerales accesorio en las intrusiones graníticas de pegmatita y en rocas intrusivas alcalinas. Menos comunes son los niobatos de calcio, el uranio, el torio y los elementos de tierras raras. Ejemplos de tales son el pirocloro [(Na, Ca)NbO(OH, F)] y la euxenita [(Y, Ca, Ce, U, Th)(Nb, Ta, Ti)O]. Estos grandes depósitos de niobio se han encontrado asociados con carbonatitas (rocas ígneas carbonatosilicatadas ) y como componente de pirocloro.

Los soldadores utilizan el niobio para ligar los componentes de acero inoxidable. Además, los fabricantes de acero agregan pequeñas cantidades de un compuesto de niobio-hierro conocido como ferroniobio para aumentar la fortaleza de sus productos, así como la resistencia a las temperaturas y a la corrosión. El acero combinado con niobio es utilizado ampliamente en las industrias aeroespacial, química, de energía eléctrica y automotriz.

En aleación con titanio, se puede extrusionar el niobio en un alambre superconductor que luego se puede moldear para formar imanes que no pierden la superconductividad al ser colocados en campos magnéticos externos. También existen relaciones superconductoras de estanio-niobio y aluminio-niobio. Los metales encuentran su uso en giroscopios para navegación aeroespacial, así como para artefactos de imágenes por resonancia magnética.

Aceleradores de partículas: Los investigadores de la física de alta energía usan algunos aceleradores de electrones que incluyen cámaras moldeadas de niobio puro o aleado. Cuando se enfrían a una temperatura cercana al cero absoluto, estas cámaras de niobio se vuelven altamente magnéticas y superconductoras, lo cual permite a los investigadores aumentar la velocidad de las partículas sub-atómicas sin usar cantidades crecientes de electricidad.

Revestir el cristal con un finísimo polvo de niobio mejora la habilidad del mismo para difundir la luz sin absorberla ni refractarla. El revestimiento también hace que el cristal sea más resistente al reflejo. El cristal revestido con niobio tiene sus aplicaciones en lentes de cámaras, así como en pantallas de televisores y monitores. El niobio también se utiliza como capa protectora para condensadores cerámicos.

Una aleación de niobio-zirconio sirve como materia prima para la base metálica de algunas lámparas de vapor de sodio. La aleación soporta las altas temperaturas que alcanza la lámpara y no se vuelve frágil con el uso prolongado.

En su estado natural, el niobio tiene un color plateado mate. Cuando el elemento puro se calienta o es pasado a través de un campo eléctrico, sin embargo, puede tomar muchos colores, yendo desde el azul al verde y del dorado al rojo. Esta propiedad ha hecho del niobio una opción con creciente popularidad para los joyeros que desean crear aros, tachas, broches, pendientes y prendedores de metal coloreado.

Dado a su propiedad de cambiar su color mediante el tratamiento de sus capas superficiales, también se emplea en la fabricación de monedas bimetálicas: 25 euros de Austria y 1 Lat en Letonia.

Es denominado niobio en honor de Níobe, hija de Tántalo. Hatchett lo llamó columbio, pero creyó haberlo confundido con el tantalio; el nombre columbio se utiliza todavía en algunos lugares. En 1844, H. Rose lo redescubrió y bautizó con el nombre actual. El nombre de niobio se adoptó por la IUPAC en 1950, 100 años después de que surgiera la controversia; a pesar de todo, la mayoría de los químicos lo llaman niobio, pero muchos relacionados con la metalurgia del elemento (anglosajones) lo siguen llamando columbio.

La mina más grande de niobio en el mundo se encuentra en Araxá, Minas Gerais, Brasil, propiedad del también productor más grande de Ferroniobio (FeNb) en el mundo, así como de otros productos derivados del niobio, la compañía se llama CBMM (Companhia Brasileira de Metalurgia e Mineraçao) 

Blomstrand lo preparó por primera vez en 1864 por reducción: calentando el cloruro en atmósfera de hidrógeno. Hasta 1905 no se obtuvo puro (Bolton).
Nunca se encuentra en estado elemental y casi siempre aparece acompañado de tántalo. Representa el 2·10% en peso de la corteza. 

Las principales fuentes minerales son: niobita (o columbita), niobita-tantalita y euxenita o policrasa [(Y,Ce,Er,U,Th,Ca..)(Nb,Ta,Ti,Fe)O]. Otros minerales que lo contienen son: samarskita ((Y,Er,Ca,Fe,Mn,Sn,W,U,Ce)[(Nb,Ta)O]), fergusonita [(Nb,Ta)YO]. Grandes cantidades de niobio se han encontrado asociadas con rocas silicocarbonatadas (carbonatitas).

La obtención del metal implica una primera etapa de separación del tántalo mediante disolventes y la transformación en Nb2O5. Éste se reduce en dos etapas con carbón; en la primera, a 800 °C, se forma NbC, que en la segunda, a 2000 °C, actúa como reductor del óxido y se produce el metal.


</doc>
<doc id="22117" url="https://es.wikipedia.org/wiki?curid=22117" title="Centro de Investigación Científica y de Educación Superior de Ensenada">
Centro de Investigación Científica y de Educación Superior de Ensenada

El Centro de Investigación Científica y de Educación Superior de Ensenada, Baja California, (CICESE), fue creado en 1973 por el gobierno federal como parte de la iniciativa para descentralizar las actividades científicas y modernizar el país. El CICESE pertenece al sistema de centros públicos de investigación del Consejo Nacional de Ciencia y Tecnología (CONACyT) y a lo largo de más de cuatro décadas, ha evolucionado hasta convertirse en uno de los principales centros científicos de México. 

El CICESE es una institución de referencia en el contexto científico nacional e internacional, su excelencia académica apoya el desarrollo nacional, la formación de recursos humanos y contribuye a generar el conocimiento que puede coadyuvar en la solución de problemas que afectan el entorno social y económico de México.

Las actividades de investigación y docencia se han consolidado en las cuatro divisiones que conforman la institución: Biotecnología Experimental y Aplicada, Ciencias de la Tierra, Física Aplicada y Oceanología, cuyas áreas de investigación incluyen acuicultura, biología, biotecnología, climatología, computación, ecología, electrónica, geociencias ambientales, geofísica, geología, instrumentación, meteorología, microbiología, oceanografía biológica, oceanografía física, óptica, optoelectrónica, sismología, tecnologías de la información, telecomunicaciones y telemática.

Cuenta con una plantilla de 176 investigadores y 197 técnicos altamente especializados; sus programas de posgrado pertenecen al Padrón Nacional de Posgrado y están orientados completamente a la investigación; la infraestructura material incluye ocho modernos edificios que albergan laboratorios, aulas, una biblioteca especializada, equipo de supercómputo, conectividad a Internet 2, el buque oceanográfico "Alpha Helix" y valiosas redes de instrumentación sismológica y oceanográfica.

Actualmente el CICESE es un Centro Público de Investigación el más grande de los 27 que integran el Sistema de Centros Públicos de Investigación del Conacyt. Tras una reestructuración convenida en un nuevo decreto publicado en el Diario Oficial de la Federación del 29 de agosto de 2000, las actividades de investigación, docencia y vinculación del CICESE se concentran en ciencias biológicas, físicas, de la información, del mar y de la Tierra, dentro de un marco de responsabilidad, ética y liderazgo en beneficio de la sociedad. Con más de cuarenta años de experiencia, el CICESE es una institución de excelencia académica que apoya el desarrollo nacional y la formación de recursos humanos, y contribuye a la generación del conocimiento.

El Centro de Investigación Científica y de Educación Superior de Ensenada (CICESE) fue la segunda institución creada por el Consejo Nacional de Ciencia y Tecnología (CONACYT) para descentralizar las actividades científicas y tecnológicas en México.

El decreto presidencial de creación del CICESE, publicado el 18 de septiembre de 1973, lo define como un organismo descentralizado de interés público, con personalidad jurídica y patrimonio propios para realizar “investigación científica básica y aplicada inicialmente en los campos de la geofísica, oceanografía física, física e instrumentación, principalmente orientadas a la a la solución de problemas nacionales y en particular a los regionales de la península de Baja California, así como a las actividades docentes en estas áreas de la ciencia en los niveles de maestría y doctorado”.

El contexto que permitió crear al CICESE a principios de los setentas es muy diverso, pero destacan, entre otros aspectos: una política nacional por descentralizar la investigación científica, la presencia de la Escuela Superior [hoy Facultad] de Ciencias Marinas de la Universidad Autónoma de Baja California (UABC) y la cercanía del Scripps Institution of Oceanography (SIO); la decisión, en 1970, de la UNAM de construir el Observatorio Astronómico Nacional en la sierra de San Pedro Mártir; la intensa actividad tectónica y sísmica de la península de Baja California y del golfo de California que justificaba la realización de estudios en las ciencias de la Tierra, y el requerimiento por desarrollar instrumentación electrónica y óptica como apoyo a la UNAM y a la investigación oceanográfica y geofísica del nuevo centro.

El primer director del CICESE fue el Dr. Nicolás Grijalva y Ortiz, quien fue substituido en 1975 por el Dr. Saúl Álvarez Borrego. Le siguieron en el cargo los doctores Mario Martínez García (1989-1997), Francisco Javier Mendieta Jiménez (1997-2005), Federico Graef Ziehl (2005-2015), y Marinone Moschetto Silvio Guido Lorenzo quien se mantiene hasta la fecha como director general del centro.

Desde su creación, el CICESE se ha dedicado a formar maestros y doctores en ciencias en las áreas académicas de su competencia. El desarrollo institucional ha permitido pasar de un esquema académico que contemplaba originalmente tres programas de maestría (en Oceanografía, en Geofísica y en Física Aplicada), a un padrón integrado por 16 posgrados que cubren todas las áreas de investigación que actualmente se cultivan, y de los cuales egresan anualmente, en promedio, 80 estudiantes de maestría y 20 de doctorado.

La integración del campus que actualmente ocupa este centro, comenzó a desarrollarse alrededor de 1977. Gradualmente se fueron adquiriendo terrenos y construyendo edificios, hasta ocupar las más de 15 hectáreas que hoy se tienen, en las cuales se asientan ocho modernos edificios que albergan aulas, cubículos y más de 115 laboratorios bien dotados con equipo científico. Evocando una verdadera ciudad universitaria, este campus es actualmente sede de varias facultades e institutos de investigación de la UABC, del Instituto de Astronomía y del Centro de Nanociencias y Nanotecnología de la UNAM, y de las cuatro divisiones académicas del CICESE (Biología Experimental y Aplicada, Ciencias de la Tierra, Física Aplicada y Oceanología).
Con el objeto de extender las labores de investigación hacia el sur de la península, el CICESE fundó en 1996 la primera de sus unidades foráneas en La Paz, Baja California Sur. La segunda, en Monterrey, Nuevo León, se creó en 2001.

Actualmente el CICESE es un Centro Público de Investigación el más grande de los 27 que integran el Sistema de Centros Públicos de Investigación del Conacyt. Tras una reestructuración convenida en un nuevo decreto publicado en el Diario Oficial de la Federación del 29 de agosto de 2000, las actividades de investigación, docencia y vinculación del CICESE se concentran en ciencias biológicas, físicas, de la información, del mar y de la Tierra, dentro de un marco de responsabilidad, ética y liderazgo en beneficio de la sociedad.




</doc>
<doc id="22123" url="https://es.wikipedia.org/wiki?curid=22123" title="Instituto Tecnológico Autónomo de México">
Instituto Tecnológico Autónomo de México

El Instituto Tecnológico Autónomo de México (ITAM) es una institución de educación superior de la iniciativa privada, sin fines de lucro ni de afiliación política, fundada en 1946 por Raúl Baillères. En él se llevan a cabo tareas de docencia e investigación —centradas en administración, política y diversas ingenierías— con la misión de “contribuir a la formación integral de la persona y al desarrollo de una sociedad más libre, más justa y más próspera”. Su campus se encuentra ubicado en la Ciudad de México, y está dividido en dos sedes: Río Hondo (para licenciaturas e ingenierías) y Santa Teresa (para posgrados y diplomados), ambas bajo el mismo régimen administrativo. Los egresados del ITAM (a quienes se les conoce popularmente como "itamitas") han llegado a ocupar los más altos cargos del gobierno: secretarios federales, senadores, diputados y embajadores. Es la escuela con el mayor índice de admisión al servicio diplomático de México.

Denominado inicialmente Instituto Tecnológico de México (ITM), el ITAM fue fundado por el empresario mexicano Raúl Baillères Chávez (1865-1967) el 29 de marzo de 1946. Prominente hombre de negocios originario de Silao, Guanajuato, Baillères desarrolló una amplia trayectoria que incluyó la Fundación de Crédito Minero y Mercantil y la adquisición de empresas como Cervecería Cuauhtémoc Moctezuma y El Palacio de Hierro, entre otras; antecedentes de lo que hoy constituye Grupo Bal, un holding diversificado en distintas compañías y actividades. Fue, asimismo, presidente de la Asociación de Bancos de México (ABM).
Interesado por la educación, Baillères reunió a un grupo de banqueros y empresarios para constituir la Asociación Mexicana de Cultura A. C., —de la que fue Presidente de su Junta de Gobierno— la cual se convirtió en el patronato del Instituto Tecnológico de México. La primera carrera universitaria ofrecida por el Instituto fue Economía, en 1946; un año más adelante se establecieron la Escuela Preparatoria y la Escuela de Administración de Negocios.
La primera sede del ITM estaba ubicada en la Calle Palma Norte 518, entre Belisario Domínguez y República de Cuba, en el Centro Histórico de la Ciudad de México, y allí permaneció por cuatro años. En la década de 1950 el ITM vivió un proceso de expansión en diversos sentidos. Sus instalaciones se trasladaron a la calle de Serapio Rendón 65, en la Colonia San Rafael, y en 1951 se abrieron las carreras de Contador Público y Contador Privado, con lo que el número de alumnos creció de 50 a 500. La demanda de estas carreras se explica tomando en cuenta el contexto conocido como “Desarrollo estabilizador”, la época de crecimiento económico sostenido para México a partir de la década de 1950.
El ITM fue refinando su misión como centro de estudios superiores, y en 1954 cerró su escuela preparatoria, para enfocarse en la educación superior. A fines de la década, el ITM se trasladó a una sede construida ex profeso, en la Calle de Marina Nacional 350, en el terreno contiguo a la Torre de Petróleos Mexicanos edificada posteriormente.

El 19 de enero de 1963, el ITAM obtuvo su autonomía mediante un decreto publicado en el "Diario Oficial de la Federación" por el presidente Adolfo López Mateos y con el aval de Jaime Torres Bodet, entonces secretario de Educación Pública. Con el rango de Escuela Libre Universitaria, el ahora Instituto Tecnológico Autónomo de México (ITAM) —denominado oficialmente con ese nombre desde 1985— se sumó al grupo de centros de enseñanza superior con autogobierno, como la Universidad Nacional Autónoma de México (autónoma desde 1929). A fines de esa década, en 1967, la Asamblea de la Asociación Mexicana de Cultura, A. C., estableció la Junta de Gobierno del ITAM, el primer paso de una estrategia de fortalecimiento y consolidación institucional, que en 1969 condujo al establecimiento de un Plan Integral de Desarrollo que transformó las estructuras académicas de investigación, administración y organización. Este Plan también incluyó la creación del Centro de Investigación y Extensión Universitaria, y la apertura de nuevas carreras y estudios de posgrado. En la actualidad, la Junta de Gobierno es la única autoridad con la capacidad de transformar, modificar o eliminar los estatutos y las directrices que rigen a todo el Instituto.

En 1974 se establecieron la licenciatura en Matemáticas Aplicadas y la maestría en Administración. Un año después, se creó la licenciatura en Ciencias Sociales, la cual fue reemplazada en 1991 por la actual licenciatura en Ciencia Política. Esa década y la siguiente estuvieron marcadas por la expansión y el crecimiento de la oferta académica, con la apertura de las carreras de Derecho (1980), Actuaría (1982), Ingeniería en Computación (1983), Relaciones Internacionales (1992), Ingeniería en Telemática (1993), Ingeniería Industrial (1997) y diversos programas conjuntos: Administración y Contaduría Pública y Estrategia Financiera, así como Economía y Ciencia Política o Economía y Relaciones Internacionales (2012). Recientemente, el ITAM abrió nuevos programas de licenciatura: Ingeniería en Negocios (2005), y Dirección Financiera e Ingeniería en Mecatrónica, ambas en 2010.

La expansión académica del Instituto hizo necesaria la ampliación de sus instalaciones. A inicios de 1978, el ITAM trasladó su ubicación a la calle de Río Hondo 1, en la Colonia San Ángel de la Ciudad de México, en las instalaciones de un antiguo seminario jesuita. La consolidación de los estudios de posgrado se hizo patente con la inauguración del Centro de Investigación y Estudios de Posgrado en un campus alterno en Avenida Santa Teresa 930, en la Colonia Héroes de Padierna.

A partir de su fundación, el ITAM ha estado encabezado por directores y posteriormente por rectores, cargos que han sido elegidos únicamente por la Junta de Gobierno. Desde 1967, Alberto Baillères, hijo de Raúl Baillères Chávez y egresado del ITAM, ha presidido la Asociación Mexicana de Cultura y la Junta de Gobierno de la Institución, la cual le otorgó el doctorado "honoris causa" el 20 de mayo de 1999. 

La estructura interna del ITAM se compone de cinco Divisiones Académicas que agrupan a catorce Departamentos Académicos y de las que dependen los 38 programas académicos. Las cinco divisiones son: Actuaría, Estadística y Matemáticas; Administración y Contaduría; Economía, Derecho y Ciencias Sociales; Estudios Generales y Estudios Internacionales; e Ingeniería. El ITAM describe su estilo académico en los siguientes términos: “Enfatizamos nuestro compromiso de aportar los mejores profesores para los mejores estudiantes. Las clases son pequeñas y dinámicas y procuramos no contar con grupos que excedan los treinta estudiantes”. Por su tamaño, el ITAM permite la atención personalizada de profesores y directores de carrera que mantienen un vínculo cercano y permanente con el alumnado. El 94 % de la planta docente cuenta con estudios de posgrado en las mejores universidades del mundo. Sin fines de lucro, el ITAM cuenta con un programa de becas a fondo perdido que beneficia a 30% de los alumnos inscritos. Es decir, uno de cada tres estudiantes recibe apoyo económico para poder cubrir el costo de sus estudios en el Instituto e incluso gastos de manutención.

En el ITAM se imparten nueve licenciaturas:


Asimismo, se ofrecen cinco ingenierías:


Existe la posibilidad de cursar más de una carrera a la vez a través de los 24 programas conjuntos que ofrece el ITAM a nivel licenciatura e ingeniería.

Los alumnos de licenciatura cursan un tronco común de Estudios Generales, un programa de siete materias ofrecido por la división del mismo nombre. A través de estas materias se busca la formación humanista del estudiante, con el fin de forjar criterios propios e independientes que puedan integrarse a su vida profesional y personal.

En cuanto a posgrados —nivel que cursa aproximadamente el 50% de los egresados de licenciatura— el ITAM cuenta con varios programas, todos con reconocimiento oficial:


También ofrece el doctorado en Economía (2007). Las maestrías pueden cursarse en las modalidades de tiempo parcial o tiempo completo.

Además de las carreras y estudios de posgrado, el Instituto cuenta con un Programa de Educación Continua, llamado de manera oficial Extensión Universitaria y Desarrollo Ejecutivo, que busca mantener actualizados a los ex alumnos y brindar oportunidades educativas a otros grupos y personas de la sociedad como empresas e instituciones de gobierno, entre otros. Este Programa ofrece diplomados internacionales, diplomados automatizados, programas ejecutivos y cursos de actualización; y forma parte de UNICON (Consorcio Internacional de Universidades para la Educación Ejecutiva), al cual sólo pertenecen otros centros de excelencia como la Universidad de Harvard, la Wharton Business School, la Kellogg School of Management y la Universidad Stanford.
Los cursos y diplomados se ofrecen en las áreas de: Administración; Actuaría y Seguros; Contabilidad y Finanzas; Derecho; Economía; Estadística; Innovación; Tecnología y Computación; Ingeniería Industrial y Operaciones; Relaciones Internacionales; Sector Público y Sociedad; y Humanismo. También cuenta con proyectos y programas especiales, diseñados de acuerdo con las necesidades de capacitación de una empresa determinada y pueden impartirse dentro del campus o extramuros. Esta vertiente parte de la idea de que “toda educación debe tender a mejorar al ser humano mediante el enriquecimiento de sus mejores valores.”

La matrícula del ITAM representa menos del 0.5% de la matrícula universitaria nacional y fluctúa alrededor de 4,500 alumnos a nivel licenciatura e ingeniería. Sin embargo, el ITAM tiene gran impacto en su comunidad de ex alumnos y en la vida nacional. Para entrar al ITAM los aspirantes deben realizar la "Prueba de Aptitud Académica" (PAA), examen de admisión que se aplica en el Instituto y en las mejores universidades a nivel internacional. Los jóvenes que se inscriben al ITAM obtienen buenos resultados sobre esta evaluación, pertenecen así al grupo de estudiantes que obtiene esta puntuación en México y en el mundo
En la clasificación “los 300 líderes más influyentes”, realizada anualmente por la revista "Líderes Mexicanos" desde 2006, los egresados del ITAM han representado, en promedio, el 7% del total de la lista. En 2011, 32 ex alumnos y catedráticos de los programas de maestría y licenciatura aparecieron en la nueva versión de la lista. Un estudio realizado por la revista "Expansión" en 2007 encontró que 14% de las 50 mujeres más influyentes en México eran egresadas del ITAM, hecho que puso en evidencia el esfuerzo en pro de la equidad de género realizado por el Instituto a lo largo de su trayectoria.
El ser una escuela de dimensiones menores a las grandes universidades mexicanas ha fomentado el desarrollo de un espíritu unitario entre los ex alumnos que permanecen vinculados al Instituto aun después de culminar sus estudios. La mayoría de los egresados del ITAM se encuentra en los primeros niveles directivos en las empresas o instituciones donde trabajan, más de la mitad opta por realizar estudios de posgrado en universidades de excelencia en el extranjero y, de acuerdo con una encuesta, está satisfecho con la preparación que ha recibido. Los egresados del Instituto logran una adecuada inserción en el mercado laboral. El ITAM cuenta con una bolsa de trabajo y 60% de los alumnos encuentra un empleo antes de terminar la carrera.

Para reconocer la calidad profesional de los ex alumnos del ITAM y su contribución al desarrollo del país, el ITAM y la Asociación de Ex Alumnos instauraron desde 1999 la entrega de reconocimientos “Carrera al Universo” y “Mérito Profesional”. En el 2010, Georgina Kessel, entonces secretaria de Energía, con treinta años de experiencia, fue la primera mujer en obtener el máximo galardón.

Diversos ex alumnos del instituto han sido relevantes como académicos, directivos de organismos internacionales y de la sociedad civil, directivos en empresas del sector privado y funcionarios del sector público. Entre ellos destaca: Miguel Mancera Aguayo, ex gobernador del Banco de México; Francisco Gil Díaz, Gustavo Petricioli Iturbide, Ernesto Cordero Arroyo, José Antonio Meade Kuribreña y Pedro Aspe Armella, ex secretarios de Hacienda; Plácido Arango Arias, co-fundador del Grupo Cifra; Alejandro Cervantes Llamas, Economista Senior de México en Banorte-Ixe; Alonso Lujambio, secretario de Educación Pública; Alicia Lebrija Hirschfeld, presidenta ejecutiva de la Fundación Televisa; Diódoro Carrasco Altamirano, ex gobernador del estado de Oaxaca; Jesús Reyes-Heroles González-Garza, ex secretario de Energía y ex embajador de México en Estados Unidos, Agustín Carstens, gobernador del Banco de México, Santiago Levy Algazi, vicepresidente del Banco Interamericano de Desarrollo, Luis Videgaray Caso, ex secretario de Hacienda y Crédito Público, Luis Tellez Kuenzler, presidente de la Bolsa Mexicana de Valores y Salomón Chertorivski Woldenberg, ex secretario de salud. Desprovisto de una agenda política, el ITAM no inculca en sus alumnos alguna ideología de ese tipo. De sus aulas han egresado políticos de diferentes ideologías, incluyendo a miembros del Partido Revolucionario Institucional, el Partido Acción Nacional y el Partido de la Revolución Democrática. De igual forma, embajadores de carrera como Francisco del Río y Ernesto Céspedes Oropeza son egresados de dicha institución.

Los alumnos del ITAM han recibido diversos premios y distinciones: la Beca Fulbright-García Robles del Programa Fulbright, el Premio de la Academia Mexicana de Derecho y Economía, el Premio Banamex de Economía otorgado por el Grupo Financiero Banamex, el Premio de la Asociación Nacional de Instituciones de Educación en Informática (ANIEI), el Premio a la Excelencia Académica de la Asociación Nacional de Facultades y Escuelas de Ingeniería, el Premio de la Asociación Mexicana de Ingenieros Mecánicos y Electricistas, el Premio Nacional de Investigación Financiera del Instituto Mexicano de Ejecutivos de Finanzas y el Premio “Goldman Sachs Global Leaders Program”. El propio Instituto concede un premio a los mejores trabajos de investigación realizados por sus alumnos.
El ITAM, como institución, también ha recibido múltiples galardones: En 2010, la División Académica de Ingeniería recibió el Premio Franz Edelman, el cual se otorga a “ejemplos sobresalientes de operaciones innovadoras que benefician a las organizaciones”. En 2007, la revista "América Economía" lo distinguió como la mejor escuela de negocios en América Latina; un comentario sobre la noticia explicaba este reconocimiento en función de una sólida planta docente de profesores de tiempo completo, muchos de ellos con posgrados en las mejores universidades del mundo. Ese mismo año, el ITAM recibió la acreditación de la Federación de Instituciones Mexicanas Particulares de Educación Superior A. C. (FIMPES).
Entre los reconocimientos institucionales más recientes cabe señalar el primer lugar de nivel académico institucional y el primer lugar académico de los profesores de acuerdo con la encuesta “Mejores Universidades” publicada por el periódico "Reforma". En la encuesta más reciente, el ITAM obtuvo el primer lugar general con cuatro carreras en el primer puesto, así como primeros lugares en otras áreas Una subsección de la página del ITAM llamada “Eventos y noticias de interés” lleva un detallado seguimiento de los premios otorgados a sus alumnos y su incorporación a puestos relevantes en el ámbito oficial o corporativo.
En 2011, los programas académicos de Computación e Ingeniería Industrial del ITAM recibieron la Acreditación ABET, siendo la primera institución educativa de la Ciudad de México, D. F. en recibir este reconocimiento. ABET es un prestigioso organismo internacional que evalúa programas universitarios de Ciencia Aplicada, Tecnología e Ingeniería en el mundo.

El ITAM cuenta con la triple acreditación de AACSB (Association to Advance Collegiate Schools of Business) de los Estados Unidos, AMBA (Association of MBAs) de Londres y EQUIS (European Quality Improvement System) de Bruselas, que reconocen la calidad de sus programas de administración de negocios a nivel internacional.

Aparte de sus servicios educativos, el ITAM se ha consolidado como un prestigioso centro de investigación científica. Algunos de los trabajos producidos entre sus muros han determinado, en muchos casos, las políticas públicas, empresariales e industriales del país. Los centros de investigación del Instituto son los siguientes: Centro de Análisis e Investigación Económica (CAIE), Centro de Desarrollo Tecnológico (CDT), Centro de Economía Aplicada y Políticas Públicas (CEAPP), Centro de Estadística Aplicada (CEA), Centro de Estudios Actuariales (CEAct), Centro de Estudios de Competitividad (CEC), Centro de Estudios de Derecho Privado, Centro de Estudios de Derecho Público, Centro de Estudios y Programas Interamericanos, Centro de Evaluación Socioeconómica de Proyectos, Centro Internacional para la Investigación en Pensiones, Centro de Investigación Económica (CIE), Centro de Estudios Alonso Lujambio (CEAL) y Centro de Acceso a la Justicia. La División Académica de Administración y Contaduría cuenta con cinco centros de investigación especializados en diferentes áreas de esas materias.

Entre las universidades privadas del país, el ITAM se distingue por su actividad editorial y en su catálogo se cuentan más de cien volúmenes y seiscientos artículos producidos por sus investigadores y publicados en revistas internacionales especializadas, muchos de ellos registrados en "Web of Science" de Thomson Reuters, “una base de datos de contenido multidisciplinario, con autoridad, que cubre 10,000 publicaciones especializadas de alto impacto mundial”. Las publicaciones académicas del ITAM se han compilado en el catálogo "Publicaciones Académicas 1996-1998".
Además, el Instituto edita sus propias revistas en dos grandes categorías, las institucionales y las del alumnado. A la primera categoría corresponden las publicaciones: "Análisis de la Coyuntura Económica", "Estudios", "Revista Mexicana de Derecho Público", "Isonomía", "Dirección Estratégica" y "Segmento". Por su importancia pública y su impacto internacional destaca "Foreign Affairs Latinoamérica", licencia de la prestigiada revista estadounidense "Foreign Affairs", considerada una de las más influyentes en el ámbito internacional. La segunda categoría, referente a las revistas del alumnado, abarca títulos como: "Caeteris Paribus", "Gaceta de Ciencia Política", "Gaceta de Economía", "Laberintos e Infinitos", "holaMundo", "Opción", "El Globalista", "Sintagma" y "Urbi et Orbi". "El Supuesto" es el periódico de los alumnos, cuenta con cerca de trescientos números publicados hasta la fecha, y sirve como medio de comunicación comunitaria y foro de expresión estudiantil sobre temas de actualidad. En conjunto estos productos editoriales cubren un amplio espectro temático paralelo a la formación dentro del aula.

El ITAM cuenta con la Biblioteca Raúl Baillères Jr., ubicada en el campus de Río Hondo, la cual cumple con los criterios establecidos por la "American Library Association" para las bibliotecas universitarias. En su colección hay 400,000 volúmenes y ejemplares de más de 900 publicaciones periódicas relacionadas con las disciplinas enseñadas en el Instituto. Asimismo, ofrece acceso a recursos hemerográficos, electrónicos y audiovisuales. Sus colecciones especiales incluyen los fondos Miguel Palacios Macedo, Bibliografía Antigua (procedente de la colección privada de Rudi Dornbusch) y las bibliotecas personales de Luis Montes de Oca y José Luis La Madrid Sauza. Cuenta con el fondo de la Biblioteca Manuel Gómez Morin, que perteneció al político mexicano fundador del Partido Acción Nacional e incluye más de 12,000 libros de leyes, economía, ciencia política, administración pública, filosofía y literatura, así como una serie de ediciones especiales.

Esta oferta de recursos para los alumnos e investigadores se complementa con las bases de datos; juntos conforman la infraestructura tecnológica necesaria para impulsar el desempeño académico. El Centro Financiero en el Campus de Santa Teresa cuenta con programas especializados y bases de datos para la investigación financiera: "Data Stream International", "Dow Jones Interactive", "Bloomberg", Reuter 3000/Reuter Graphics e Infosel Financiero. Los centros de cómputo de Río Hondo incluyen laboratorios de instrumentación, electrónica, trabajo en redes, automatización de microprocesos e ingeniería telemática.
Cabe mencionar también al Centro de Aprendizaje, Redacción y Lenguas (CARLE), parte del Departamento Académico de Lenguas y dependiente de la División Académica de Estudios Generales y Estudios Internacionales del ITAM. Su misión es cubrir con los objetivos de aprendizaje que los alumnos se propongan en las áreas de estrategias de aprendizaje, redacción y lenguas extranjeras. Éste Centro cuenta con el Aula Interactiva Cervantes, Centro de escritura y una Mediateca.

Las sociedades de alumnos son de gran importancia de dentro del Instituto. La Sociedad de Alumnos del ITAM (CARITAM) es la representación oficial de los estudiantes, y el Consejo Universitario de Honor y Excelencia (CUHE) se propone fomentar la mejora en el rendimiento académico. Como parte de una cultura de la responsabilidad social, existen otras agrupaciones del alumnado del ITAM estructuradas como voluntariado. Los alumnos participan en ellas como una contribución para resolver los problemas de la sociedad mexicana. Entra estas agrupaciones destacan: ALCANCE, con una década de trabajo en tres áreas principales: Desarrollo comunitario, Niños de la calle y Personas discapacitadas física o mentalmente. AIESEC, ONG dedicada a realizar practicas profesionales y sociales en el mundo para crear una mayor convivencia y respeto entre las diversas culturas internacionales. El Centro de Acceso a la Justicia (CAJ), conformado por estudiantes y maestros de Derecho, proporciona ayuda legal gratuita a las personas que la necesitan. El Despacho de Asesoría Gratuita a Organizaciones Filantrópicas (DAGIF) ofrece servicios de consultoría sobre temas como obtención de fondos, cumplimiento de las disposiciones legales mexicanas y campañas de comunicación. ITAMMUN es un Modelo de Naciones Unidas que busca promover el debate alrededor de temas de la agenda internacional, generalmente organizado dentro de las instalaciones de la Cancillería (SRE). El Programa Interdisciplinario para el Desarrollo Sustentable (PIDES) es una asociación que reúne a los estudiantes que organizan actividades para promover el respeto y el cuidado del medio ambiente. El ITAM logra de esta forma una participación activa en la solución de los grandes problemas nacionales.




</doc>
<doc id="22161" url="https://es.wikipedia.org/wiki?curid=22161" title="A Western Harvest Field by Moonlight">
A Western Harvest Field by Moonlight

A Western Harvest Field by Moonlight —en español: "Un campo de cosecha occidental a la luz de la luna"— es el primer EP del músico estadounidense Beck, lanzado en 1994 a través de Fingerpaint Records, y contó con la producción de Beck Hansen y Tom Grimley. La canción "Totally Confused" cuenta con la participación de la banda that dog., que más tarde contribuiría en otras dos canciones de beck, "Girl of My Dreams" y "Steve Threw Up".

En 1994 fueron impresos originalmente 3.000 copias y desde entonces hubo varias reimpresiones: 2.000 en 1995, 1.000 en 1997 y otros 1.000 en 1998. Las primeras ediciones contenían pinturas hechas con los dedos por Beck y amigos que fueron hechos en la discográfica durante la fiesta de lanzamiento del disco y fueron empaquetados con la calcomanía “Original Figerpainting Enclosed” (“Adjunto Pintura Hecha con los Dedos Original”). Las siguientes impresiones no contenían ninguna pintura.

El álbum tiene “Sexy Death Soda” rayado en el lado A, y “Cherry Cupcake” en el B. Las primeras impresiones salieron con “Styrofoam Chicken (Quality Time)” siendo una pista continua. Se puso a una profundidad mayor en el disco de vinilo así cuando la púa leyera la canción saltara otra vez hasta el principio de la misma. Las impresiones de este álbum tenían escrito “A Western Harvest Moon by Moonlight” en el lomo del disco. Una curiosidad es que si se sostiene el disco frente a un espejo se puede leer debajo de la imagen de la pequeña niña la frase “Happiness grows in your own back yard” (“La felicidad crece en tu propio patio trasero”).

Todas las canciones escritas y compuestas por Beck. 


</doc>
<doc id="22163" url="https://es.wikipedia.org/wiki?curid=22163" title="One Foot in the Grave">
One Foot in the Grave

One Foot in the Grave —en español: "Un pie en la tumba"— es el cuarto álbum de estudio del músico estadounidense Beck, lanzado el 27 de junio de 1994 a través de la discográfica K Records. El álbum fortalecido la reputación de Beck, podría decirse que le permite entrar en la corriente principal de su álbum de 1996, "Odelay". En julio de 2008, "One Foot in the Grave" había vendido 168.000 copias en los Estados Unidos. El 14 de abril de 2009, el álbum fue reeditado con 16 temas extras, en su mayoría inéditos.

El álbum muestra una fuerte influencia folk que es más pronunciada que en sus álbumes de estilo más ecléctico de la época, "Mellow Gold" y "Odelay". Contiene canciones de rock acústico y alt-country y fueron producidas por Beck Hansen y Calvin Johnson. Fue grabado antes del lanzamiento de su álbum "Mellow Gold", pero no fue liberado hasta después de que el álbum tuvo un éxito de crítica y público. "One foot in the grave" no hacía más que reeditar canciones grabadas años antes en el sello K Records y que recaían más que en el artificio instrumental en la emoción otorgada por los sonidos más acústicos y la melodía. Uno de los temas del disco, que ofrece folk-blues, “Asshole”, fue versionado por el mismísimo Tom Petty, quien había declarado que "en una etapa bastante pobre para el rock Beck era de lo más salvable". La canción “Cyanide Breath Mint” incluso recuerda al Ray Davies más folk aunque en conjunto su faceta más acústica y melódica recuerda bastante a Gene Clark.

La canción "One Foot in the Grave" ya había sido publicada en su álbum "Stereopathetic Soulmanure" de una toma en directo. El álbum presenta la producción y apoyos ocasionales de Calvin Johnson, el fundador de K Records. Fue registrado en Dub Narcotic Studio, que fue alojado entonces en el sótano de Calvin. El productor, y también fundador del sello K Records, acompaña al joven Beck tanto en la portada, como en algunos temas en las voces.



Una edición de lujo fue lanzada el 14 de abril de 2009, con 16 pistas más, 12 de ellas inéditas.




</doc>
<doc id="22164" url="https://es.wikipedia.org/wiki?curid=22164" title="Midnite Vultures">
Midnite Vultures

Midnite Vultures —en español: "Buitres de medianoche"— es el séptimo álbum de estudio del músico estadounidense Beck, publicado el 23 de noviembre de 1999 a través de DGC. Fue grabado en la casa de Beck y contó con la producción del propio artista, Mickey Petralia, Tony Hoffer, John King y Mike Simpson. Con este álbum producido por los Dust Brothers, Beck logra entrar definitivamente en el mapa musical como uno de los cantantes más innovadores del momento. Sencillos de este álbum como la exuberante funk "Sexx Laws" fueron un éxito y le dieron al cantante una mayor popularidad en el mundo entero.

"Midnite Vultures", el disco que llega justo para el fin de milenio con la convicción de ser el último gran disco del año, es la continuación directa de "Odelay", así como "Odelay" lo era de "Mellow Gold". En un reportaje posterior a la salida de "Mutations", Beck declaró que su próximo disco sería un disco “completamente bailable, con canciones tontas y letras tontas”. Esto se ve en frases como “¡Quiero desafiar todas las leyes de la lógica sexual!”, de “Sexx Laws”, el primer corte del disco, o “Estoy mezclando fitness y cuero / para que todas las lesbianas se pongan a gemir” (“Mixed Bizness”, segundo corte). Más que ningún otro, éste es un disco físico, nosólo plagado por el baile y el sexo sino también por olores y sabores: “Nicotine & Gravy” ("Nicotina y aderezo"), “Peaches & cream” ("Duraznos y crema"'), “Milk & Honey” ("Leche y miel") son tres temas casi consecutivos.

El álbum es una apología de los excesos, narrada a través de imágenes como sacado de un puro sueño de los mejores en Las Vegas o Los Angeles. Como tres imágenes musicalizadas, "Nicotine & Gravy" sube exquisitamente el tono de la cuestión, "Mixed Bizness" lo desordena, "Hollywood Freaks" le pone sabor a la utopía plástica del mundo rico y célebre, siendo ellas las primeras muestras de un ánimo liviano y efervescente. Tuvo una impresión inicial en formato digipak limitada a las primeras 500.000 unidades. En la canción “Beautiful Way” aparece Berth Orton como vocalista de fondo, Johnny Marr (ex guitarrista de los Smiths) aparece en “Milk and Honey”. El álbum cuenta también con la inclusión de la canción “Debra” también conocida como “I Wanna Get With You (And Your Sister Debra)”, un clásico en las giras desde 1996 pero que Beck no incluyó en su álbum "Odelay" por considerar que no encajaba en el mismo.

Varias canciones fueron directamente inspiradas por otras canciones: "Get Real Paid" cuenta con un secuenciador espiral que recuerda a "It's More Fun to Compute" de Kraftwerk; un sintetizador de "Milk & Honey" resuena similar a un riff de "The Message", de Grandmaster Flash and the Furious Five; "Beautiful Way" se produjo después de escuchar la canción "Countess from Hong Kong" de The Velvet Underground; y "Debra" fue inspirada por las canciones "Raspberry Beret" de Prince y "Win" de David Bowie. Sin embargo, y a pesar de que todo lo que se escucha fue tomado de los ‘70 y principios de los ‘80 (lo más nuevo seria el electro circa 1982 de “Get Real Paid” y lo más viejo, el soul al estilo Motown circa 1972 de “Debra”) el disco no suena como Lenny Kravitz, Jamiroquai u otros artistas con una fijación retro.

"Midnite Vultures" alcanzó el puesto # 34 en los EE.UU., donde fue disco de oro, y también alcanzó el # 19 en el Reino Unido. A partir de julio de 2008, el álbum ha vendido 743.000 copias en los Estados Unidos. El disco fue elogiado por la mayoría de los críticos, la revista Rolling Stone, NME y Pitchfork Media les dio cuatro estrellas (8.5/10 críticas en Pitchfork). Se le otorgó el estatus de "aclamación universal" por MetaCritic con una puntuación de 83/100, pero en 2006 fue nombrado en el puesto 50 de la lista "El peor disco de la historia" por la revista Q Magazine, a pesar del hecho de que se le dio al álbum cuatro estrellas. Pitchfork Media lo incluyó en el puesto número 5 de su lista "Top 10 Albums of 1999" (los 10 mejores discos de 1999). "Midnite Vultures" fue nominado en 2001 por "Álbum del Año" en los 43rd Grammy Awards.

Las versiones iniciales vendidas en las tiendas Best Buy contenían un disco extra con tres canciones adicionales. El álbum estaba empaquetado en una caja de plástico estándar y el disco extra en una funda de cartón aparte.

Las siguientes canciones fueron grabadas durante las sesiones de "Midnite Vultures", pero no se incluyeron en el álbum. Algunos aparecen en la edición limitada del EP "Beck".





</doc>
<doc id="22170" url="https://es.wikipedia.org/wiki?curid=22170" title="Sea Change">
Sea Change

Sea Change —en español: "Cambio radical"— es el octavo álbum de estudio del músico estadounidense Beck, lanzado el 24 de septiembre de 2002 a través de DGC/Interscope y contó con la producción de Beck Hansen y Nigel Godrich. El álbum es uno de los discos más importantes de Beck y el segundo en colaboración con Nigel Godrich. El mismo contiene canciones de estilo experimental y se destaca por el empleo de violines y violonchelos, y de un estilo más lento que las canciones de discos anteriores. Las letras irónicas fueron reemplazadas por letras con contenido más sincero. También evitó el muestreo intensivo de sus álbumes anteriores. El álbum salió al mercado en cuatro estilos de arte diferentes. En las entrevistas, Beck citó la ruptura con su novia de mucho tiempo, como la mayor influencia en el álbum.

Alcanzó el puesto número #8 en el Billboard 200, siendo esta la última certificación de oro en marzo de 2005 de la Recording Industry Association of America. El álbum recibió elogios de la crítica, con varios comentarios calificándolo como la "obra magna" de Beck, que ha seguido creciendo en estatura desde su lanzamiento, con la inclusión en listas de "mejores de la década" y "lo mejor de todos los tiempos". Los críticos elogiaron el cambio de un estilo experimental a uno simple y emocional.

Al término de la gira de su anterior trabajo, "Midnite Vultures" (1999), Beck y su novia desde hacia ya nueve años, la estilista Leigh Limon, terminaron su relación. Beck, tres semanas antes de cumplir 30 años, descubrió que Limon le había engañando con un miembro de la banda de Los Ángeles Whiskey Biscuit. Beck caducó en un período de melancolía e introspección, durante los cuales escribió las pistas sombrías, basado en la acústica más tarde encontradas en "Sea Change". Escribió la mayoría de las 12 canciones en una semana, pero luego las dejó de lado. Después de un tiempo, decide grabarlas. En 2001, Beck llama su frecuente productor Nigel Godrich para producir las canciones. Hansen pretende grabar el álbum a finales de 2001, pero debido a los ataques del 11 de septiembre, decide postergarlo. Antes de trabajar con Godrich, Beck grabó canciones con Dan "The Automator" Nakamura en enero de 2002 en preparación para el nuevo registro, pero finalmente las canciones de esas sesiones no estuvieron presentes en "Sea Change". Muchas canciones del álbum también se realizaron en vivo antes del lanzamiento, como "Lost Cause" y "Evil Things", este último no se registró debido a la pérdida de tiempo.

Beck y su grupo de músicos entraron en el estudio con la intención de hacer un registro basado en lo acústico, similar a lo hecho en "Mutations". En el proceso de grabación, Beck le dijo a Godrich que él esperaba grabar una canción por día, similar al proceso de "Mutations". Sin embargo, cada canción acabó llevando por lo menos dos días de grabación, debido a los arreglos orquestales. Los socios musicales en el estudio incluyen al multi-instrumentista Jon Brion, el baterista James Gadson, y el guitarrista Jason Falkner, así como los socios musicales desde hace mucho tiempo, el tecladista Roger Joseph Manning Jr., el bajista Justin Meldal-Johnsen, el baterista Joey Waronker, el guitarrista Smokey Hormel y la violonchelista Suzie Katayama. Además, el padre de Beck, David Campbell, proporciona arreglos de cuerda. Joey Waronker dejó la grabación para viajar a Hawaii; James Gadson completó su parte los días restantes.

La grabación comenzó tan pronto como la banda entró a los Ocean Way Studios en Los Angeles el 6 de marzo de 2002. Mucho material del álbum fue grabado en vivo, con efectos adicionales (incluyendo campanas y cuerdas), agregados más adelante. A fin de capturar la inmediatez del material, los artistas trabajaron rápidamente y espontáneamente. Durante la producción, Beck se dio cuenta de que su voz se había vuelto significativamente más profunda. "Antes de que registráramos", dijo Godrich, "hemos escuchado "Mutations" y su voz sonaba como Mickey Mouse. Su rango se ha reducido. Ahora cuando abre la boca, sale una gran vibración. Es bastante notable. Él tiene un tono increíble." Al final de la producción, el grupo de músicos tuvieron que trabajar más rápido de lo que pensaban. Finalmente, la grabación duró un poco más de tres semanas y el álbum fue mezclado a partir de allí, y finalizó el 7 de mayo. En una sesión, Hansen comenzó a rasguear su famoso sencillo de 1995 "It's All in Your Mind" al azar antes de comenzar una nueva canción, y Godrich, que quedó encantado, comento "Tenemos que hacer esto".

"Ship in the Bottle" era la única pista completa no incluida en el registro. "Era la canción super-pop del disco," dijo Beck en una entrevista del 2002. Finalmente, "Ship in the Bottle" fue lanzada en la versión japonesa de "Sea Change" y más adelante en la versión remasterizada del álbum.

El álbum se basa en un conjunto de canciones acústicas, que muestran el lado reflexivo del cantautor Beck. El cambio de género se define como una transformación amplia, que refleja la salida en el estilo del anterior álbum de Beck "Midnite Vultures" y de las grabaciones anteriores, basadas en los samples, así como el deseo de Beck de dar a cada álbum una identidad. Los orígenes para el sonido único y apasionado del álbum habían sido acumulandos durante años, según Beck en una entrevista de 2002: "hay hilos de lo que he hecho antes. Si usted escucha mis anteriores B-sides, escuchará este disco. He estado queriendo hacer este disco durante años", explicó. A pesar de la dificultad inicial para decidir el nombre, el título proviene de "Little One", la undécima pista en el álbum. Las grabaciones de "Sea Change" incluyen temas de amores rotos, desolación, soledad, etc. Aunque a menudo se compara con "Mutations", Beck, consideró el álbum, en una entrevista de 2008, como un órgano más representativo de su álbum de 1994 "One Foot in the Grave", y "más representativo de lo que yo estaba haciendo [en los primeros días]".

"Sea Change" es la forma más hermosa de componer un corazón roto, alejar los fantasmas y reinventarse a partir de una experiencia dolorosa. Es la experiencia del dolor pariendo belleza y sanación, es la experiencia mística del artista comprometido con el arte y la vida. "Sea Change" es un mar que nos recorre por dentro, erizándonos la piel, entre el llanto y la desnudez. Con esta producción Beck rompió esquemas de sus anteriores producciones, para pasar de las estructuras intelectuales y experimentales a la delicada línea de lo personal, del sentimiento palpable, de los sufrimientos no compartidos, las letras se tornan directas pero con ese manejo oportuno para alejarlo de lo lastimero, al fin y al cabo el intelecto y talento lo levantan para dejar un disco excelente, minimalista, orgánico, directo, con sonidos que se pierden en el viento y se conservan en los recuerdos.

Antes de su lanzamiento, los comerciantes se preocuparon por el impacto comercial de "Sea Change" debido a su sonido. Los analistas predijeron que el álbum no recibiría apoyo en las emisoras de radio, observando la reputación de Beck, los elogios de la crítica y la posibilidad de múltiples nominaciones a los Grammy podrían compensar un sonido no comercial. La fecha de lanzamiento del álbum fue anunciada el 31 de mayo de 2002. Además, la lista inicial de canciones del álbum también fue lanzada, con canciones en un orden muy diferente que su versión definitiva, así como incluyendo la pista "Ship in the Bottle". El título del álbum fue anunciado en agosto de 2002. En la promoción del disco, las nuevas canciones del álbum fueron puestas en libertad en orden cronológico semanalmente a través de la Página web de Beck en julio y agosto de 2002. Finalmente, el álbum fue lanzado el 24 de septiembre de 2002.

En el momento del lanzamiento, un álbum de cuatro pistas llamado "Sea Change Album Sampler" fue lanzado. Se empaqueto como un CD single y contuvo las pistas siguientes:
"Sea Change" fue lanzado con cuatro portadas distintas, cada versión contiene material gráfico digital distinto por Jeremy Blake en el CD y el folleto. Había también diferentes mensajes ocultos (fragmentos líricos) escritos debajo de la bandeja del CD de cada versión. El arte de la cubierta original del álbum fue utilizado como una efigie en el vídeo musical del single "Lost Cause".

En 2002, "Sea Change" fue uno de los dos álbumes en recibir la más alta calificación de cinco estrellas en la revista Rolling Stone, el otro álbum fue "The Rising" de Bruce Springsteen. La revista llegó a llamarlo el mejor álbum del año 2002. Al año siguiente, el álbum ocupó el puesto número 440 en la lista . También ocupó el puesto número 17 en la lista de los "100 mejores discos de los 00s" de Rolling Stone.

"Sea Change" alcanzó el puesto #8 en el Billboard Top 200 chart y fue finalmente certificado de oro en marzo de 2005. En el UK Charts alcanzó el puesto #20. A partir de julio de 2008, el álbum ha vendido 680.000 copias en los Estados Unidos. El álbum fue re-editado en un formato remasterizado por Mobile Fidelity Sound Lab en el mes de junio del año 2009.

Se dice que el concepto de sonido del álbum está inspirado en el sonido del álbum "Histoire de Melody Nelson" de Serge Gainsbourg. Los críticos también han comparado las melodías acústicas y relajadas de "Sea Change" a las obras del cantautor británico Nick Drake y a las del álbum de 1975 "Blood on the Tracks", del músico Bob Dylan.

"Sea Change" rindió muchos tours en apoyo, el primero de los cuales comenzó como una gira discreta basada en lo acústico, en un teatro en agosto de 2002. Cada show dio un ambiente lúdico, enérgico, con Beck contando chistes entre actuaciones y una aparición sorpresa en el show: Jack White de The White Stripes, el 11 de agosto. Un largo tour estaba previsto para octubre de 2002, con la banda The Flaming Lips como apertura, así como banda de acompañamiento de Beck. La gira comenzó en octubre y terminó en noviembre de 2002.

Durante la gira de "Sea Change", Beck varió la lista y experimentó con las estructuras de la canción, cambiando los arreglos cada noche como una forma de romper la previsibilidad. En el cierto deseo de Beck para la reinterpretación de sus canciones, despidió a su banda de gira desde hace mucho tiempo y al grupo con el que trabajó en "Sea Change" poco antes de que comenzara la gira. Entre nuevas y viejas canciones en cada concierto, Beck realizó numerosos covers, como "No Expectations" de The Rolling Stones, "Kangaroo" de Big Star, "Beechwood Park" de The Zombies y "Sunday Morning", de The Velvet Underground. Descrito como "impresionante" por David Fricke de Rolling Stone, Fricke amplió también sus declaraciones: "fue un ajuste perfecto — canciones sobre el compromiso y la pérdida, escrito y cantado por los heridos."

Créditos de "Sea Change" adaptado de Allmusic.




</doc>
<doc id="22171" url="https://es.wikipedia.org/wiki?curid=22171" title="Stray Blues: A Collection of B-Sides">
Stray Blues: A Collection of B-Sides

Stray Blues: A Collection of B-Sides —en español: "Blues callejero: Una colección de B-Sides"— es un álbum recopilatorio de B-sides del músico estadounidense Beck, lanzado el 1 de junio de 2000.

El álbum fue publicado a través del sello discográfico Geffen Records y sólo fue lanzado en Japón. Incluye "Burro", una versión en español de "Jack-Ass" y el cover "Halo of Gold" de Skip Spence. "Totally Confused" había sido publicada en el EP "A Western Harvest Field by Moonlight" de 1994, "Clock" era una pista oculta del álbum "Odelay" de 1996, mientras que "Brother", "Lemonade", "Electric Music and the Summer People" y "Feather in Your Cap" son lados B de los sencillos "The New Pollution" y "Sissyneck". El álbum fue limitado a 70.000 copias. Este álbum contó con la producción de Beck Hansen, Nigel Godrich, Tom Grimley, Tom Rothrock, Rob Schnapf, John King, Mike Simpson y Brian Paulson.



</doc>
<doc id="22184" url="https://es.wikipedia.org/wiki?curid=22184" title="Fundación de Roma">
Fundación de Roma

La fundación de Roma es referida por varias leyendas, las cuales fueron unificadas principalmente por la "Eneida" de Virgilio, reuniendo en una historia coherente distintas versiones de algunos ritos de iniciación de aquel tiempo.

Se supone, con cierta probabilidad, que entre los siglos X y VII a.C., Italia central estaba poblada por los dos grupos principales en que se dividían los italianos: los s y los latinos. "Latium Vetus" (el antiguo territorio del Lacio) estaba poblado por etruscos, volscos, sabinos, ecuos, rútulos y ausonios. Vinieron de diferentes áreas de Italia central, incluyendo Toscana, Marcas y Liguria.

Entre ellos, los latinos desarrollaron una sociedad organizada que fue la principal fuente de la población romana. Los latinos originalmente se quedaron en los "Colli Albani" (los montes Albanos, en la moderna Castelli), a unos 30 u 80 km al sudeste del monte Capitolino. Luego bajaron hacia los valles, los cuales ofrecían mejores tierras para la agricultura y la ganadería.

Las zonas inmediatas al río eran muy favorables y además ofrecían recursos estratégicos notables: el río formaba una frontera natural por un lado, mientras los montes daban un resguardo defensivo del otro. Esta posición también daba a los latinos control sobre el río, y su posible tráfico comercial y militar, desde el natural punto de observación en la isla Tiberina, la isla situada frente al actual Trastevere. Asimismo se podía controlar el tráfico terrestre, ya que Roma se hallaba en la intersección de los principales caminos al mar desde Sabinia, al sureste, y Etruria, al noroeste.

Se supone que el desarrollo del asentamiento comenzó con diferentes poblaciones separadas ("borgate"), situadas en los montes, las cuales se unieron para formar Roma. Estudios recientes sugieren que el monte Quirinal fue muy importante en los tiempos antiguos. Sin embargo, el primer monte en ser habitado parece haber sido el Palatino (lo que confirmaría la leyenda), que está en el centro de la Roma antigua. Sus tres crestas (los montes menores Cermalo o Germalo, Palatium y Velia) se unieron con las tres cimas del Esquilino (Cispio, Fagutal y Opio), y luego los pueblos sobre el monte Celio y la Subura, entre los montes de Rione Moderna, Monti y Opio. Posteriormente, la ciudad creció hasta abarcar también los montes Aventino, Capitolino, Quirinal y Viminal. 

Estos montes tenían nombres expresivos: el monte Celio también era llamado "Querquetulanus", debido a los robles ("quercus"), mientras que el Fagutal ("Fagutalis") estaba poblado por bosques de hayas ("fagus") y el Viminal ("Viminalis") por el mimbre ("vimen") de los sauces. Descubrimientos recientes revelan que el Germalus, sobre la parte norte del Palatium, era el sitio de una población del siglo IX a. C. con viviendas circulares o elípticas. Estaba protegida por una cerca de tapial (quizá reforzada con madera), y es probable que este lugar fuera donde verdaderamente se fundó Roma.

El territorio de esta federación llamada "pomerium" encerraba a la llamada Roma Quadrata (cuadrada). Ésta sería extendida con la inclusión del monte Capitolino y la isla Tiberina ya cuando Roma se convertía en un "oppidum", un pueblo fortificado. El Esquilino todavía era una población satélite. Éste sería incluido con las expansiones servias.

Las celebraciones del "septimontium" ("de los siete montes"), el 11 de diciembre, en aquel entonces eran consideradas en relación a la fundación de la ciudad. Sin embargo, como el 21 de abril es la única fecha en las que todas las leyendas se ponen de acuerdo, recientemente se ha argumentado que probablemente el "septimontium" celebraba más bien las primeras federaciones entre los poblados de los montes romanos; de hecho, una federación similar era celebrada por los latinos en Cave, un pueblito al sudeste romano, o en el Monte Cavo en Castelli.

La leyenda sobre la fundación romana tiende a relacionar al naciente pueblo con las entidades más eminentes de su época, para poder "demostrar" que su gran éxito depende también de su origen especial, o para completar su reputación con referencias de primera clase. Sin embargo, esta historia es una historia completa, y la tradición romana entera está basada en ella. Hay varias versiones de esta leyenda; la siguiente se considera comúnmente como la principal:

Con el héroe semidiós Eneas al mando, los derrotados troyanos sobrevivientes cruzaron el mar Mediterráneo para alcanzar las costas del Lacio. Llegaron a una área probablemente entre el moderno Anzio y Fiumicino, al sudoeste de Roma. Más comúnmente se supone que arribaron a Laurentum (o Laurento); otras versiones dicen que arribaron a Lavinium, un lugar nombrado por Lavinia, la hija del rey Latino.

Latino, sabio rey de los latinos, los hospedó, dejando que reorganizaran su vida en el Lacio. Su hija Lavinia había sido prometida a Turno, el rey de los Rutuli, pero Latino prefirió ofrecerla a Eneas; Turno consecuentemente le declaró la guerra. El resultado fue la muerte de Turno y la captura de su gente. Ascanio, también conocido como Iulos, hijo de Eneas, fundó Alba Longa y fue el primero en una larga serie de reyes, entre quienes los mejor conocidos son Procas y sus hijos Numitor y Amulio. Según Dionisio de Halicarnaso, los reyes de Alba Longa fueron el nexo directo que unía a Ascanio y Rómulo, el fundador de Roma.

El dios Marte iba paseando por la orilla de un río de la ciudad Alba Longa, allí vio a una mujer dormida en la orilla y quedó enamorado en el instante. Tuvieron dos hijos llamados Rómulo y Remo. Una vez nacidos los hijos de Rea Silvia, los padres de los gemelos los metieron en una canasta y fueron transportados por el río y arribaron a las orillas de un lugar. Allí fueron salvados por una loba llamada Luperca que los amamantó. Cerca vivía un pastor llamado Fáustulo y su esposa Aca Larentia. El pastor encontró a los bebés, los llevó a su casa y los adoptó. Cuando se hicieron adultos, los hermanos fueron informados de su historia, y el pastor les dijo que no los habían tenido, así que regresaron a Alba Longa, mataron a Amulio y liberaron a su abuelo Numitor, devolviéndolo al trono.

Rómulo y Remo se propusieron edificar una nueva ciudad en el mismo lugar en el que fueron encontrados por la loba. Decidieron que uno construiría el pueblo mientras que el otro ayudaría. Así que empezaron a preguntar a los dioses para informarse de quién iría a dirigirla. Rómulo fue a la cima del monte Palatino y tiró su lanza en el monte para encontrar el lugar. La lanza se convirtió en el Corniolus, el árbol sagrado de Roma.

Rómulo se fue a la cima del monte Palatino; Remo a la cima del Aventino. Rómulo se convenció de que él había sido seleccionado por los dioses, ya que recibió el augurio que volaba sobre él fueron un círculo de aves, así que tiró su lanza en el monte para encontrar el lugar; cuando estaba en la tierra, la lanza (la cual era de madera) inmediatamente se convirtió en el Corniolus, el árbol sagrado de Roma.

Para la fundación siguieron los ritos tradicionales de su época para fundar ciudades. Con la ayuda de una vaca y un toro blanco, usó un arado para trazar la cerca de la ciudad. Remo saltó sobre el surco, violando la muralla, lo cual era una especie de sacrilegio, que fue la primera pena capital del homo sacer (que era el castigo por pasar), pues la muralla se trazaba desde el primer momento para ser inviolable. Y de acuerdo con la tradición, Rómulo lo mató a espada, para que los dioses no permitieran que en el futuro la muralla fuese violada de nuevo.

Rómulo fue el primer rey romano, y reinó hasta que desapareció durante una tormenta, llevado por su padre Marte.

Mientras que el cuerpo principal de la leyenda ha permanecido más o menos el mismo desde su creación, algunos detalles han cambiado, principalmente para juntar las versiones ligeramente divergentes y corregir varios puntos en cuanto a tiempo y geografía. También las antiguas leyendas locales poco a poco fueron elaboradas para alcanzar armonía con la historia principal. El efecto de estas intervenciones sobre la leyenda son considerablemente evidentes.

Una de las más tempranas versiones (del siglo V a. C.) es la del griego Helánico de Lesbos, y generalmente es reportada junto con la versión de Damastes de Sigeo. En esta versión, el fundador del pueblo fue Eneas. Estas versiones sobrevivieron hasta el 509 a. C. (año considerado en el que comienza la república romana), cuando fue percatado que, como habían existido 7 reyes romanos y Rómulo fue el primero, existía un hueco entre el siglo VIII de los primeros reyes y el siglo XII de la caída de Troya. Así que como Rómulo no podía ser hijo de Iulo, sólo quedaba como un distante descendiente. El tiempo entre Iulo y Rómulo fue "llenado" con la serie de los reyes de Alba Longa. Eneas pudiera haber llegado a las costas del Lacio durante el reinado de Latino (rey de los latinos), para poder llegar a un acuerdo con las leyendas locales. Entonces Marte tenía que ser añadido para poder honrarlo, así que Rómulo se volvió un descendiente (por parte de padre) de Marte, mientras que por parte materna Rea Silvia estaba conectada con Eneas mediante la dinastía de Alba Longa. La condena de los hijos de Rea Silvia es sólo una de las varias recolecciones de leyes divinas, de la religión que tan profundamente entró en la vida romana.

Entre los itálicos, como el relato en la Teogonía de Hesíodo, los dos hermanos Agrio y Latino eran los hijos de Telégono, hijo de Ulises y la bruja Circe, a la cual se le dedica el monte Circeo, ubicado al sur del Lacio, donde se celebraba un culto en su honor y estaba su cueva. 

Entre los latinos, se dijo que Saturno había sido reemplazado por su hijo Jove, así que bajó a la Tierra y se mezcló con los latinos. Después, Evandro condujo a su pueblo desde Grecia hasta el Lacio, donde edificó la ciudad de Palanteo, que estaba situada sobre una colina que posteriormente se denominó monte Palatino. Luego Hércules llegó a liberar estas tierras de la amenaza del gigante Cacus. Finalmente Eneas llegó de Troya, después de varias aventuras y fundó Roma. Notablemente en esta versión los latinos no fueron creados o asistidos por los dioses, sino que el pueblo fue fundado por Eneas en presencia de estas "autoridades".

Durante la República Romana, varias fechas fueron dadas para la fundación de la ciudad, todas en el intervalo entre 758 a. C. y 728 a. C. Finalmente, bajo el Imperio Romano la fecha sugerida por Ático y Varrón (753 a. C.) fue acordada, pero en los "fasti capitolini" el año dado fue el 752. Aunque los años variaban, todas las versiones estaban de acuerdo en que la ciudad fue fundada el 21 de abril, un día santo dedicado al sagrado culto de Pales, diosa de los pastores; en su honor, Roma celebraba el parritta (o palilia). Ver también Ab urbe condita.

Descubrimientos recientes en la Colina Capitolina en Roma han brindado la fecha de la fundación de Roma. El más destacado entre éstos es una serie de fortificaciones defensivas en la vertiente norte de la Colina Capitolina que pueden datarse a mediados del siglo VIII a.C., cuando la leyenda dice que Rómulo labró un surco ("sulcus") alrededor de la Colina Capitolina para delimitar el perímetro ("pomerium") de su nueva ciudad. Los restos de la muralla y demás evidencias como ésta han sido descubiertas por las excavaciones de Andrea Carandini.

El nombre del pueblo se considera generalmente que se refiere a Rómulo, pero hay otras hipótesis. Una de ellas se refiere a Roma, que sería la hija de Eneas o Evandro. También puede rastrearse un origen etrusco, que apuntaría a la "gens" etrusca "Ruma", o a "Rumon", nombre etrusco del río Tíber. Estudios recientes parecen dar preferencia a una raíz de origen indoeuropeo con significado de "río"; Roma en ese caso significaría "el pueblo sobre el río". 

Roma es también llamada "urbe", y este nombre, que después en latín significaría genéricamente cualquier otro pueblo, proviene de "urvus", surco realizado con un arado, aquí, por el de Rómulo.

Sobre el monte Capitolino, el 21 de abril de cada año, una campana especial llamada la "patarina" suena al mediodía del Campidoglio para conmemorar la fundación de Roma. En esa ocasión, el famoso cañón de Gianicolo permanece silencioso, el único día del año en que no suena.





</doc>
<doc id="22187" url="https://es.wikipedia.org/wiki?curid=22187" title="Iósif Stalin">
Iósif Stalin

Iósif Vissariónovich Dzhugashvili, más conocido como Iósif Stalin (Gori, - Moscú, 5 de marzo de 1953) fue un dictador soviético, secretario general del Comité Central del Partido Comunista de la Unión Soviética entre 1922 y 1952 y Presidente del Consejo de Ministros de la Unión Soviética entre 1941 y 1953.

Estuvo entre los bolcheviques revolucionarios que impulsaron la Revolución de Octubre en Rusia en 1917 y más tarde ocupó la posición de secretario general del Comité Central del Partido Comunista de la Unión Soviética desde 1922 hasta que el cargo fue formalmente suprimido en 1952, poco antes de su muerte. En mayo de 1924, después del XII Congreso del Partido Comunista de la Unión Soviética, Stalin pidió que se le permitiera dejar el cargo. Esta petición fue rechazada unánimemente, incluyendo a sus detractores. Volvió a formular esta petición tres veces más, en 1926, 1927 y 1952; las tres fueron rechazadas y tuvo que permanecer en el cargo. Mientras que el cargo de secretario general era oficialmente electivo y no se lo consideraba como la máxima posición dentro del Estado soviético, Stalin logró utilizarlo para acaparar cada vez más poder en sus manos tras la muerte de Vladímir Lenin en 1924 y para sofocar gradualmente a todos los grupos opositores dentro del Partido Comunista. Esto incluyó a León Trotski, un teórico socialista y el principal crítico de Stalin entre los primeros líderes soviéticos, que fue desterrado de la Unión Soviética en 1929. En tanto que Trotski fue un exponente de la revolución mundial, fue el concepto de Stalin de socialismo en un solo país el que se convirtió en principal enfoque de la política soviética. 

En 1928, Stalin reemplazó la Nueva Política Económica de la década de 1920 por una economía planificada muy centralizada y por planes quinquenales que iniciaron un período de rápida industrialización y de colectivización económica en el campo. Como resultado, la Unión pasó de ser una sociedad mayoritariamente agraria a una gran potencia industrial, siendo ésta la base de su aparición como segunda mayor economía del mundo después de la Segunda Guerra Mundial. Como resultado de los rápidos cambios económicos, sociales y políticos de la época estalinista, millones de personas fueron enviadas a campos de trabajo como castigo, y millones fueron deportadas y exiliadas a zonas remotas de la Unión Soviética. La agitación inicial en el sector agrícola interrumpió la producción de alimentos en la década de 1930 y contribuyó a la catastrófica hambruna soviética de 1932-1933. En 1937, una campaña contra supuestos enemigos de su gobierno culminó en la Gran Purga, un período de represión masiva en el que cientos de miles de personas fueron ejecutadas, e incluso fueron condenados líderes del Ejército Rojo acusados de participar en complots para derrocar el gobierno soviético.

En agosto de 1939, tras el fracaso para establecer una alianza anglo-franco-soviética, la URSS de Stalin firmó un pacto de no agresión con la Alemania nazi que dividió sus esferas de influencia en Europa oriental. Este pacto permitió que la Unión Soviética recuperase algunos de los antiguos territorios del Imperio ruso con la invasión soviética de Polonia de 1939, la Guerra de Invierno en Finlandia, y la ocupación de las Repúblicas bálticas, de Besarabia y de Bucovina del Norte durante la Segunda Guerra Mundial. Pero después de que Alemania violara el pacto al invadir la Unión Soviética con la Operación Barbarroja en 1941, se abrió un Frente Oriental y la Unión Soviética se unió a los Aliados. A pesar de grandes pérdidas humanas y territoriales en el período inicial de la guerra, la Unión Soviética logró detener el avance del Eje en la batalla de Moscú y la batalla de Stalingrado. Finalmente, el Ejército Rojo avanzó a través de Europa en 1944-45 y capturó la capital del Tercer Reich tras la batalla de Berlín en mayo de 1945. Habiendo jugado el papel decisivo en la victoria aliada, la URSS surgió como una superpotencia reconocida después de la guerra.

Stalin encabezó las delegaciones soviéticas en las conferencias de Yalta y Potsdam, en las que se trazó el mapa de la Europa de posguerra. En los Estados satélites del Bloque del Este se instalaron gobiernos de izquierda leales a la Unión Soviética. En esa época la URSS había entrado en una lucha por el dominio global, conocida como la Guerra Fría, con los Estados Unidos. En Asia, estableció buenas relaciones con Mao Zedong en China y Kim Il-sung en Corea del Norte y de diversas maneras, la Unión Soviética de la era estalinista sirvió como modelo para la recién formada República Popular de China y República Popular Democrática de Corea.

Al mantenerse en el poder hasta su muerte en 1953, Stalin dirigió la URSS durante el período de reconstrucción de la posguerra, marcado por el predominio de la arquitectura estalinista. El desarrollo exitoso del programa nuclear soviético permitió que el país se convirtiese en la segunda potencia mundial en armas nucleares. También se inició el programa espacial soviético. En sus últimos años, Stalin lanzó los denominados Grandes Proyectos de Construcción del Comunismo y el Gran Plan para la Transformación de la Naturaleza.

Tras su muerte, Stalin y su régimen han sido condenados en numerosas ocasiones. La más significativa de estas condenas se dio durante el XX Congreso del Partido Comunista de la Unión Soviética en 1956, cuando su sucesor, Nikita Jrushchov, denunció su legado en una famosa intervención con la que se inició un proceso de desestalinización de la URSS. Las visiones modernas de Stalin en la Federación de Rusia siguen siendo mixtas, con algunas personas viéndolo como un tirano y otras como un líder capaz. Fue nominado al Premio Nobel de la Paz de 1945 y 1948.

El nombre Stalin («hecho de acero»; derivado del ruso "stal", acero, con el mismo sufijo posesivo personal "in" que usó Lenin) empezó a usarlo a partir de 1912, y desde octubre de 1917 se convirtió en su sobrenombre. Familiarmente, y entre sus camaradas más cercanos, era conocido como "Sosó", e incluso llegó a utilizar el pseudónimo "Soselo" para firmar sus poemas. También, se refería a sí mismo como "Koba", nombre de un héroe popular de Georgia. Otros nombres que utilizó fueron David, Morti, Nijeradze, Chízhikov, Ivanóvich.

Iósif Stalin nació el 18 de diciembre de 1878 en Gori, Gubernia de Tiflis del Imperio ruso (Georgia en la actualidad). Su padre, Vissarión Dzhugashvili ("Besó"), y su madre, Yekaterina Gueladze ("Keke"), era sirvienta, ambos de familias de siervos georgianos.. El padrino de la boda fue Yakóv Egnatashvili ("Koba"), mercante local, y uno de los asistentes fue el Padre Christopher Charkviani, ambos jugarían más tarde un papel significativo en la infancia de Iósif. Vissarión dejó su empleo asalariado y abrió un pequeño taller con la ayuda económica de sus amigos, incluido Egnatashvili. Desde el principio del matrimonio, corrió el rumor en Gori de que Yekaterina era una mujer promiscua. Estos rumores serían, más adelante, una fuente de inestabilidad en el matrimonio y darían lugar, a su vez, al cuestionamiento de la paternidad de Iósif. Yekaterina quedó embarazada y nueve meses después, el nació su primer hijo, Mijaíl. Dos meses después, Mijaíl murió y Vissarión empezó a beber. Yekaterina quedó embarazada de nuevo y el nació su segundo hijo, Guiorgui, que murió de sarampión con apenas 6 meses. Finalmente, el nació su tercer hijo: el pequeño Iósif (de diminutivo "Soso" o "Soselo").

Egnatashvili fue el padrino de los dos primeros hijos. Sin embargo, Vissarión decidió que no fuese padrino de Iósif para intentar evitar la mala fortuna de nuevo. La familia alquiló una casa de una habitación a un artesano osetio en el barrio ruso de Gori, cerca del cuartel de las tropas imperiales para las que Vissarión trabajaba. La casa tenía un sótano dónde Vissarión guardaba sus herramientas y Yekaterina acondicionó como cuarto del bebé. Yekaterina no producía leche suficiente, por lo que las mujeres de su padrino, Tsikhatatrishvili, y Egnatashvili ayudaban a amamantarle. Además, Iósif tenía una salud frágil, nació con sindactilia (dedos unidos por una membrana) en dos dedos del pie y a partir de los dos años padeció sarampión y escarlatina. Poco después, la situación de la familia comenzó a mejorar. Vissarión incorporó a dos aprendices para su taller y uno de ellos, Vano Khutsishvili, se convirtió en un hermano de acogida para Iósif.

El problema de Vissarión con la bebida empeoró. Durante 1883 empezó a meterse en peleas de borrachos y fue apodado el "Loco Besó". Vissarión se fue sumiendo en un estado de paranoia por los rumores que cuestionaban la paternidad de Iósif y empezó a tener un comportamiento agresivo contra su mujer Yekaterina y su hijo Iósif.

En torno a 1884, hubo una epidemia de viruela en Gori. Iósif contrajo la enfermedad y sobrevivió, aunque su cara quedó marcada de por vida. Ese mismo año, Vissarión vandalizó el bar de Egnatashvili y atacó al jefe de policía Davrishevi. Davrishevi ordendó a Vissarión abandonar Gori y este se fue a Tiflis a trabajar en la fábrica armenia de zapatos Adelkhanov.

En 1886, Yekaterina y Iósif se mudaron al piso de arriba de la casa del Padre Charkviani, antiguo compañero de bebida de Vissarión. Yekaterina pidió a Charkviani que admitiese a Iósif en la escuela de la iglesia de Gori en el curso que empezaba en otoño. Charkviani no accedió a ello, pero permitió que Iósif estuviese presente en las lecciones de ruso que los hijos adolescentes de Charkviani daban a su hermana pequeña. En 1888, Iósif ingresó en la escuela parroquial para hacer el programa educativo obligatorio en Georgia, de dos años. Su nivel de ruso por entonces le permitió acabar el programa en un año y, en 1889, comenzó su educación formal en un programa de cuatro años, donde destacó como estudiante y por su voz al cantar.

El 6 de enero de 1890, durante la celebración de la epifanía, Iósif fue atropellado por un faetón. Esto afectó a su forma de andar. Vissarión llevó a Iósif a Tiflis para que recibiese tratamiento médico. Una vez Iósif estuvo recuperado, Vissarión le llevó a la fábrica Adelkhanov para que trabajase con él como aprendiz. Esto era común en Tiflis, donde muchos trabajadores llevaban a sus hijos a las fábricas.

Mientras tanto, Yekaterina presionó a sus conexiones en la Iglesia Ortodoxa para que Iósif se incorporase a la escuela al comienzo del siguiente curso escolar en septiembre de 1890. Finalmente lo consiguió y Iósif volvió a Gori para ir a la escuela, a pesar de que Vissarión se negó a ayudar económicamente. Poco después, Iósif fue expulsado por el impago de la matrícula de 25 rublos. Yakov Egnatashvili pagó la deuda y se convirtió en un padre sustitutivo para Iósif. En ese periodo, Iósif comenzó a hacerse llamar "Koba". Desde entonces su pseudónimo más conocido después de "Stalin". Koba era un montañés legendario de Georgia protagonista de la novela El Parricida y también el diminutivo de Yakov Egnatashvili.

Iósif fue un estudiante ejemplar y por ello se le concedió una beca para sus estudios, por lo que ya no debía pagar los costes de la matrícula. Finalmente, se graduó en la primavera de 1894 con 15 años. El maestro del coro ofreció a Iósif acompañarle a la escuela de maestros del zar Alejandro en Tiflis. Sin embargo, Iósif decidió ingresar en el seminario ortodoxo de Tiflis.

La relación de Stalin con el movimiento revolucionario comenzó en el seminario. Durante estos años de escuela, Stalin se unió a la organización socialdemócrata de Georgia, en la que fue instruido en política marxista por el profesor Noe Zhordania (quien después sería Jefe de Gobierno de la República Democrática de Georgia) y comenzó a difundir el marxismo. Fue un responsable del sindicato de Georgia durante tres años y luego portavoz del nuevo partido marxista georgiano. Algunas fuentes afirman que Iósif abandonó el seminario en 1899 justo antes de sus exámenes finales; según otras biografías, fue expulsado.

Inicia su militancia en torno al círculo de obreros ferroviarios de Tiflis, alejándose definitivamente de Zhordania.

Junto a otros jóvenes intenta editar un periódico propio clandestino, sin lograrlo. Solamente editaron octavillas que reparten en las fábricas, con claro contenido político. El Primero de Mayo de 1900 organiza la primera manifestación de masas, reuniendo a 500 obreros en los alrededores de Tiflis con banderas rojas y retratos de Marx y Engels.

En agosto de 1900 entra en contacto con Víktor Kurnativski, uno de los "iskristas" que envía Lenin a Tiflis para impulsar la difusión del periódico que debía conducir a la reorganización del Partido y a la lucha contra las tendencias economicistas y conciliadoras. Kurnativski les enseñó a aquellos jóvenes georgianos cómo montar una imprenta clandestina y les propuso que lo hicieran en Bakú, un fuerte centro proletario, mejor que en Tiflis. En marzo de 1901, Kurnativski es detenido junto con otros militantes, pero Koba Dzhugashvili se libra de la redada, aunque su vivienda y su lugar de trabajo en el observatorio meteorológico fue registrada por la Ojrana, la sección especial de la policía zarista dedicada a la represión política. Tiene que pasar a la clandestinidad, de la que ya no saldrá hasta la Revolución de 1917.

En 1901, el clérigo georgiano M. Kelendzheridze escribió un libro educacional sobre lengua y arte, incluyendo uno de los poemas de Stalin firmado como «Soselo». En 1907, el mismo editor publicó "Antología georgiana, o Colección de los mejores ejemplos de literatura georgiana", donde incluía un poema de Stalin dedicado a Rafael Eristavi. Su poesía aún puede ser vista en el museo Stalin de Gori.

Trabajó durante diez años con los movimientos políticos clandestinos en el Cáucaso, sufriendo repetidos arrestos y exilio a Siberia, entre 1902 y 1917.

Stalin se adhirió a la doctrina de Lenin de un partido centralista fuerte, de revolucionarios profesionales. En el período posterior a la revolución de 1905, Stalin lideró los «escuadrones de lucha» en robos de bancos para reunir fondos para el partido bolchevique. Stalin asistió al V Congreso del Partido Obrero Socialdemócrata de Rusia en Londres en 1907. Este congreso consolidó la supremacía del sector bolchevique de Lenin y se debatió la estrategia para la revolución comunista en Rusia. Stalin nunca se refirió posteriormente a su estancia en Londres.

En 1913, mientras estuvo exiliado en Viena, Stalin escribió "El marxismo y la cuestión nacional", tratado en el que presenta una posición marxista ortodoxa (cfr. este trabajo con el de Lenin llamado "Sobre el derecho de los pueblos a la autodeterminación") y que pudo haber contribuido a su nombramiento como Comisario del Pueblo para Asuntos Nacionales después de la revolución.

En 1912, Lenin tuvo la intención de proponer la elección de Stalin al Comité Central bolchevique en la Conferencia del Partido en Praga, pero desistió al encontrarse con la resistencia del partido. Sin embargo, inmediatamente después, Stalin fue sumado al Comité Central por «cooptación» (potestad prevista por los estatutos, que reservaba para el Comité Central el derecho a sumar integrantes que no hubieran sido electos por el Congreso del Partido).
En 1917 Stalin era el editor de "Pravda", el diario oficial del partido, mientras Lenin y gran parte del liderazgo bolchevique estaban en el exilio. Después de la Revolución de Febrero, Stalin y el equipo editorial tomó una posición favorable al gobierno provisional de Kérenski y se sostiene que llegó al extremo de negarse a publicar artículos de Lenin que llamaban al derrocamiento del gobierno provisional.

En abril de 1917, Stalin fue por primera vez electo por la base del partido para formar parte del Comité Central, obteniendo la tercera más alta mayoría de votos en la Conferencia de Petrogrado (detrás de Lenin y Zinóviev). Posteriormente fue nombrado secretario del Politburó del Comité Central (mayo de 1917); se mantuvo en este cargo por el resto de su vida. Al finalizar julio presentó el informe central al VI Congreso del partido, en el cual se optó por la insurrección contra el gobierno provisional.

Según diversas fuentes, Stalin solamente desempeñó un papel menor en la Revolución de Octubre. Algunos autores, como Adam Ulam, remarcan que cada hombre en el Comité Central tenía una labor específica que le había sido asignada.

El siguiente resumen respecto al papel de Trotski en 1917 fue escrito por Stalin en Pravda 16 de noviembre de 1918:
Posteriormente, en 1924, el mismo Stalin creó un mito referente a la así llamada «Central del Partido», de la cual supuestamente dirigía todo el trabajo práctico referente a la revuelta y que consistía en un grupo integrado por él mismo, Sverdlov, Dzerzhinski, Uritski y Búbnov. Ninguna evidencia se ha encontrado, sin embargo, respecto a las actividades de esta Central, que en cualquier caso, de haber existido, habría estado subordinada al Comité Militar Revolucionario comandado por Trotski.

Durante la Guerra Civil Rusa y la guerra polaco-soviética, Stalin fue comisario político en el Ejército Rojo en diversos frentes. El primer cargo de gobierno de Stalin fue el de Comisario del Pueblo de Asuntos Nacionales (1917-1923).

Tuvo también el cargo de Comisario del Pueblo para la Inspección de los Trabajadores y Campesinos (1919-1922), de miembro del Soviet Militar Revolucionario de la República (1920-1923) y miembro del Comité Central Ejecutivo del Congreso de los Sóviets a partir de 1917.

El 3 de abril de 1922, Stalin fue nombrado secretario general del Comité Central del Partido Comunista Panruso, un cargo que él posteriormente transformó en el más poderoso del país. En aquella época, esta posición se veía como un cargo menor dentro de la estructura partidaria (ocasionalmente en el partido se referían a Stalin como el «camarada archivista»), sin embargo este cargo asociado con el liderazgo que tenía sobre la Oficina Organizativa del Comité Central del Partido (Orgburó), dio a Stalin una base de poder suficientemente fuerte como para permitirle instalar a sus aliados en los puestos claves del partido.

La acumulación de poder por parte de Stalin tomó al moribundo Lenin por sorpresa, quien, en sus , hizo llamamientos para que el XII Congreso del Partido Bolchevique apartara al «brusco» Stalin. Sin embargo, estos intentos no prosperaron debido a que los documentos preparados por Lenin fueron ocultados por Stalin y sus eventuales aliados, a sabiendas de que Lenin se encontraba en esos momentos enfermo e imposibilitado de participar en el Congreso.

Después de la muerte de Lenin en enero de 1924, Stalin, Kámenev y Zinóviev tomaron el control del partido situándose en un punto que ideológicamente estaba entre Trotski (a la izquierda del partido) y Bujarin (a la derecha). Durante este período, Stalin abandonó el tradicional énfasis bolchevique respecto a la revolución internacional en favor de una política de construir el «socialismo en un solo país», en contraste a la teoría de Trotski de la revolución permanente.

En la lucha por el liderazgo una cosa era evidente: quien terminara comandando el partido tenía que ser considerado muy leal a Lenin. Por eso, la actitud de cada uno ante su muerte fue determinante en los posicionamientos dentro del Partido: Stalin organizó su funeral y pronunció un discurso manifestando una lealtad imperecedera con Lenin, a la vez que impidió mediante engaños que Trotski asistiera. Stalin también acusó a Trotski de haberse unido a los bolcheviques justo antes de la revolución, e hizo públicos los desacuerdos que éste había tenido con Lenin en la etapa previa a la revolución.

Las imágenes soviéticas correspondientes a este período fueron posteriormente trucadas, eliminando con fotomontajes y técnicas similares a los opositores a Stalin (principalmente Trotski).

La base fundamental del ascenso al poder de Stalin fue el control del aparato administrativo del estado, en un país en el cual la escasez era la regla, tras la Primera Guerra Mundial y la Guerra Civil. A su vez, la política de Stalin de pregonar el llamado «socialismo en un solo país» era visto como un antídoto optimista con respecto a la guerra, en contraste a la posición de la «revolución permanente» de Trotski.

El método de Stalin era la designación de Secretarios que le respondieran personal e incondicionalmente, y la manipulación de sus oponentes logrando poner a unos contra los otros, usando el método de "dividir para gobernar".

Inicialmente, Stalin formó una "troika" junto a Zinóviev y Kámenev contra Trotski. Una vez que Trotski había sido eliminado de la pugna por el poder político, Stalin se unió con Bujarin y Rýkov contra Zinóviev y Kámenev, recordando a todos el voto de estos últimos contra la insurrección en 1917. Zinóviev y Kámenev entonces, se unieron con la viuda de Lenin, Nadezhda Krúpskaya, formando la "oposición unida" en julio de 1926.

En 1929, durante el XV Congreso del Partido Comunista de la Unión Soviética (PCUS), Trotski y Zinóviev fueron expulsados del partido y Kámenev perdió su puesto en el Comité Central. Stalin pronto se volvió contra la "oposición derechista" representada por sus aliados del momento, Bujarin y Rýkov.

Uno de los argumentos predilectos de Stalin para atacar a otros miembros del Partido, fue la lucha contra la existencia de facciones, que habían sido prohibidas temporalmente en el Partido Bolchevique durante la Guerra Civil, pero que formaban parte de la historia del bolchevismo.

Habiendo también derrotado a la «oposición de derecha» de Bujarin, Stalin comenzó los planes de colectivización e industrialización. En este camino es de destacar la "deskulakización", que trajo como consecuencia la expropiación masiva de las tierras explotadas por medianos propietarios agrícolas ("kuláks"), lo cual causó una reducción de la producción de cereales, lo que unido a unas malas condiciones ambientales dio lugar a una gran hambruna en Ucrania que supuso la muerte de varios millones de ucranianos (ver Holodomor); según el gobierno soviético, «fue una medida necesaria para acabar con la retención y sabotaje de productos que ilegalmente practicaban los kuláks». Los muertos por la hambruna ascendieron a un número difícil de determinar.

Serguéi Kírov había conocido a Stalin en mayo de 1918. Durante la guerra civil se enfrentó a Trotski, lo que hizo que se alineara con Stalin, Ordzonikidze y Voroshílov.

Desde 1926 estuvo trabajando en Leningrado, pero tras ser elegido para el Comité Central en el XVI Congreso, Stalin le propuso volver a Moscú. Sin embargo, Kírov pidió permanecer en Leningrado, y se le permitió quedarse hasta el final del segundo plan quinquenal. No están claras las razones por las que declinó este ascenso.

En el Congreso del PCUS de 1934, al elegirse el nuevo Comité Central, Kírov recibió tres votos negativos, resultando ser el candidato menos rechazado, en contraste con el propio Stalin que recibió 292 votos negativos, siendo el menos popular.

Dumaskin afirma que Kírov se opuso a Stalin en el Politburó en 1934, lo que produjo «una perceptible tirantez entre Stalin y Kírov». Distintos autores han dado cuenta de la existencia de una conspiración en la cúspide del PCUS cuyo fin habría sido reemplazar a Stalin con Kírov.

El 1 de diciembre de 1934, Kírov fue asesinado por Leonid Nikoláev en Leningrado. La dirigencia del Estado soviético declaró que Nikoláev había sido apoyado por Trotski desde el exilio. Esto dio comienzo a una purga generalizada, con cientos de ejecuciones, encarcelamientos y reclusiones en campos de concentración, acusando al bloque trotskista-zinovievista de estar organizando una extensa conspiración con el objetivo de tomar el poder en la URSS. Como parte de este proceso, Kámenev y Zinóviev fueron sometidos a juicio público y, tras confesar supuestos crímenes (confesión que según algunos habría sido producto de torturas), fueron ejecutados en 1936. Con mecanismos similares, en menos de dos años terminaría siendo ejecutada la mayoría de los miembros del Comité Central bolchevique que había dirigido la Revolución de Octubre, mientras Trotski sería asesinado en la ciudad de México en agosto de 1940 por Ramón Mercader, un agente estalinista.

La hipótesis acerca del vínculo de Stalin con este asesinato estuvo ampliamente difundida, siendo confirmada por Nikita Jrushchov en sus memorias. Sin embargo, no existen pruebas concluyentes al respecto.

En 1937, Wilhelm Canaris, jefe de la inteligencia militar alemana, captura información proveniente de un general ruso disidente, llamado Nikolái Skoblin, en la que se asegura que existe una intriga combinada de oficiales rusos y alemanes decididos a derrocar a Stalin. Reinhard Heydrich supo de esta información (ya que tenía agentes infiltrados en la Abwehr), y valiéndose de una operación encubierta de inteligencia, roba esta documentación de las oficinas de la Abwehr, incendiándola después para no dejar rastros.
La documentación fue manejada hábilmente por Hitler con la ayuda de Heydrich y ocasionaron la purga en el Ejército Rojo, con la eliminación de más de 3000 oficiales, entre ellos Mijaíl Tujachevski, máximo exponente de la guerra mecanizada en la Unión Soviética.

Stalin también incrementó ampliamente las actividades de inteligencia extranjera de la NKVD. Bajo sus instrucciones, la inteligencia soviética comenzó a crear redes de información en la mayoría de los países del mundo, incluyendo Alemania, Gran Bretaña, Francia, Japón y los Estados Unidos. Stalin hizo un gran uso de la Internacional Comunista con el fin de infiltrar agentes.

La Primera Guerra Mundial, la Guerra civil rusa, la intervención por parte de 14 potencias extranjeras luego de la toma del poder por los bolcheviques y la misma revolución, tuvieron un efecto devastador en la economía del país.

La producción industrial de 1922 fue un 13% menor que la de 1914. Bajo la Nueva Política Económica (NEP), impulsada por Lenin ante la situación apremiante, que permitía cierto grado de flexibilidad en el mercado dentro del contexto del socialismo, se produjo una recuperación. Agotada la NEP, esta política fue reemplazada por un sistema centralizado y sujeto a los planes quinquenales a partir de 1928. Estos planes perseguían ambiciosos programas de industrialización y de colectivización y estatización de la agricultura.

El objetivo de la industrialización era tanto reacondicionar las viejas fábricas y empresas industriales, de tecnología atrasada y en estado de práctico abandono, como construir una poderosa industria pesada.
La industrialización era considerada fundamental en la construcción del socialismo, ya que garantizaría la alianza obrera-campesina como base de la dictadura del proletariado, la defensa de la URSS y elevaría notablemente el nivel de vida de la población.

Sin capitales iniciales, escaso comercio internacional y virtualmente sin infraestructura moderna, el gobierno de Stalin financió la industrialización a partir de la ganancia obtenida por las fábricas y empresas del Estado, por el comercio, los bancos y el transporte.

En 1926-1927, se invirtieron en la industria cerca de mil millones de rublos; tres años después, se pudieron invertir ya en ella unos 5.000 millones.

La década de 1930 consiguió la producción por primera vez en la historia de la Unión Soviética, de una amplia gama de nuevos productos, entre los cuales se destacaban motocicletas, relojes y cámaras fotográficas, como asimismo las máquinas y herramientas necesarias para producir estos y otros bienes. En la industria química se produjo el desarrollo de la industria de los plásticos, en metalurgia se desarrollaron nuevos tipos de aleaciones de alta calidad y diversos metales no ferrosos fueron manufacturados por primera vez.

También mejoró notoriamente la escala y la eficiencia con la cual se fabricaban los productos existentes. En la industria del hierro y del acero, hacia fines de la década de 1930, el tamaño promedio de los nuevos hornos de fundición era un 40% mayor con respecto a aquellos de solo 10 años antes. Muchas innovaciones estaban basadas exclusivamente en desarrollos técnicos locales. En la industria aeronáutica, por ejemplo, los ingenieros soviéticos produjeron aviones que eran comparables a diseños extranjeros; en la industria militar, por su parte, se desarrollaron tanques que no tenían equivalentes en el mundo occidental. La Unión Soviética fue también el primer país en producir goma sintética de polibutadieno.

El gobierno de Stalin promovió la colectivización de la agricultura con el fin de aumentar la producción agrícola a partir de granjas mecanizadas en gran escala, lo que permitía mantener a los campesinos bajo un control político más directo y para que la recaudación de impuestos fuera más eficiente. La colectivización significó cambios sociales drásticos en una escala nunca vista desde la abolición de la servidumbre en 1861.

La colectivización forzada de la agricultura comenzó a inicios de los años 1930, formándose la asociación obligatoria de todas las granjas en los llamados koljós (o granja colectiva), una estructura fuertemente centralizada. La supresión de los derechos de propiedad sobre la tierra fue una consecuencia de la forma como se decidió resolver el antiguo conflicto de la lucha de clases. Además, de acuerdo a la visión económica de la época, los koljós debían trabajar con mayor eficiencia debido a la aplicación de tecnología y a la división del trabajo. En los primeros años de la colectivización se estimaba que la producción agrícola e industrial debería aumentar un 200% y un 50% respectivamente; sin embargo la producción agrícola disminuyó.

Los campesinos ricos, los llamados kuláks, con independencia de si resistían o no los cambios impuestos y la colectivización, eran puestos a trabajar directamente en los campos, o bien eran trasterrados a Siberia y al oriente del país.

La política de industrialización de la agricultura seguida por Stalin requirió grandes cantidades de equipamiento y maquinaria, que se consiguió al exportar trigo y otros bienes agrícolas al extranjero. Los koljós fueron obligados mediante planes específicos a entregar al Estado su producción agrícola. Estas medidas trajeron como consecuencia una drástica caída en la calidad de vida de los campesinos y la producción agrícola.

Para evitar el aislamiento del régimen soviético, decidió la entrada de la URSS en la Sociedad de Naciones (1934), y la aproximación a Gran Bretaña y Francia. En política interior trató de eliminar cualquier tipo de oposición: entre 1936 y 1938 organizó procesos (procesos de Moscú) y deportaciones contra los principales mandos militares y contra toda oposición en el seno del Partido y del Estado. Basándose en los datos suministrados tras la "perestroika", documentados por el Gulag, fueron detenidas más de 1.300.000 personas por motivos políticos. De ellas casi 700.000 fueron fusiladas. Durante su gobierno inició un controvertido programa para "rusificar" a las diferentes repúblicas de la URSS, enviando rusos a las distintas repúblicas soviéticas para que se casaran con los locales y así aumentar el porcentaje de rusos en la región.

Por otra parte, ya durante el primer período stalinista, antes incluso de la década de 1930, amplios sectores de la sociedad soviética aceptaron con optimismo los grandes avances de la Revolución. Rusia era el único país del mundo donde a las mujeres se las pagaba lo mismo que a los hombres por un trabajo similar. También en este primer período, existían grandes facilidades para obtener un divorcio o abortar.

El 23 de agosto de 1939, la Unión Soviética y la Alemania nazi firmaron en Moscú un pacto de no agresión, en el que además, en el Protocolo adicional secreto se dividía a Europa oriental y central en esferas de influencia soviética y alemana, estableciendo también directrices para la partición de Polonia entre ambos Estados. También en ese protocolo se concedió a Stalin carta blanca para intervenir en Finlandia y en los países bálticos.

Una vez iniciada la Segunda Guerra Mundial, sin embargo, y considerando Hitler que la caída de Inglaterra era inminente, ordenó atacar a la Unión Soviética, haciendo del pacto letra muerta. El 18 de diciembre de 1940, el mando alemán decidió que la invasión a la URSS (operación Barbarroja) se realizaría en abril de 1941, pero solo se pudo concretar el 22 de junio de ese año, cuando se inició el ataque a territorio soviético con más de 3.000.000 de soldados alemanes.
La invasión tomó por completa sorpresa a Stalin a pesar de que tenía suficientes antecedentes a través de su espía Richard Sorge de que esta era inminente.

Stalin se encerró en el Kremlin de Moscú en una aparente depresión y falta de liderazgo y solo reaccionó 10 días más tarde, para retomar el control con mano firme.

Stalin, desesperado por la invasión germánica, decidió suspender la campaña ateizante y permitir el resurgimiento de la Iglesia ortodoxa rusa, para que el pueblo soviético creyente se uniera a la lucha, "olvidando" por un tiempo el obligado ateísmo del PCUS. Increíblemente y en forma insospechada para los alemanes, el pueblo ruso se unió en defensa de su patria.

El Ejército Rojo, muy debilitado por las purgas de fines de los 30, se encontraba virtualmente sin mando competente, por lo que las fuerzas alemanas avanzaron rápidamente por las llanuras occidentales de la URSS. Hitler predecía que la guerra con el gigante ruso duraría a lo más 6 meses y que el pueblo ruso mismo eliminaría a Stalin.
Stalin se hizo nombrar Presidente del Consejo de Comisarios del Pueblo con lo que en la práctica se convirtió oficialmente en el Jefe del Estado.
Las medidas iniciales de Stalin por contener la invasión alemana fueron ineficaces y no pudieron detener el avance de las fuerzas blindadas de Hitler que penetraban profundamente en territorio soviético. Si bien en un comienzo Stalin se mostró dubitativo e irresoluto por el súbito y contundente ataque de los alemanes, pronto empezó a tomar el control de la situación y se autonombró Supremo Comandante en Jefe del Ejército Rojo.
A diferencia de Hitler, Stalin dio cierta autonomía a sus generales en la toma de decisiones e hizo traer desde la frontera a algunos de sus mejores generales, como Zhúkov y Vatutin, permitiendo además el envío desde los frentes orientales de miles de tropas siberianas entrenadas ya en el combate con los japoneses.

Durante la Batalla de Smolensk, su hijo
Yákov Dzhugashvili fue capturado: Stalin supo de esta situación pero permaneció indiferente a la suerte corrida por su hijo. Yákov permaneció anónimo en el
campo de concentración de Sachsenhausen hasta que fue delatado. Se lo intentó adoctrinar para la propaganda alemana pero no cambió de bando. Entonces se decidió su canje por el mariscal Friedrich Paulus, pero Stalin se negó.
Yákov moriría en extrañas circunstancias el 15 de abril de 1943 en el mismo campo. Stalin jamás demostró públicamente algún tipo de consideración por la suerte corrida por Yákov.

Se mantuvo en Moscú en el invierno de 1941, cuando los alemanes amenazaban la ciudad (42 km), y organizó allí un contraataque soviético.
Al año siguiente, 1942, tuvo éxito al mantener la estratégica ciudad de Stalingrado, última defensa de la zona petrolera del Cáucaso, pese a la enorme cantidad de bajas entre sus hombres (Stalin, a través de sus comisarios políticos, ordenó disparar contra sus propios soldados si estos se retiraban de un combate al considerarlos desertores) y posteriormente (1943) también derrotó al ejército alemán en la batalla de Kursk con lo que todo el curso de las acciones militares tuvo un cambio, siendo ahora los soviéticos los que obligaban a retirarse a los alemanes. Así, en mayo de 1945, las fuerzas de Stalin fueron las primeras en entrar a Berlín, forzando el suicidio de Hitler y la capitulación alemana.

En su papel de comandante en jefe, Stalin procuraba siempre mantener un control personal pero flexible en el mando, sobre todo el frente de batalla, las reservas militares y la economía de guerra. Esta actitud no se mostró eficaz, ya que dejaba en un solo hombre todas las decisiones, pero luego Stalin fue aprendiendo de sus errores y empezó a delegar decisiones militares al contrario de su rival, Hitler, quien monopolizó el mando.

Como Jefe de Estado, Stalin participó en varios encuentros con los líderes aliados, como el llamado de "los tres grandes", con Winston Churchill y Franklin D. Roosevelt en Yalta y en Potsdam (ambas en 1945), en las que logró el reconocimiento internacional de una esfera de influencia soviética en la Europa del Este y mostrándose como un formidable negociador según el propio secretario del exterior británico, Sir Anthony Eden. Asimismo, el 4 de septiembre de 1943, se reunió con tres metropolitas de la Iglesia para restablecer el Santo Sínodo y convocar al Concilio Episcopal para elegir como Patriarca de Moscú a uno de los tres anteriores (Serguéi) cinco días después, por primera vez en diecisiete años, desde 1925.

Un hecho de este período que refleja su «culto a la personalidad» es que se autoconcedió el honor de Héroe de la Unión Soviética, a pesar que este solo lo recibían los soldados en combate. Se sentía amenazado por la popularidad de Zhúkov, al que acusó de usar ese triunfo a su favor y lo terminó degradando.

Al finalizar la Segunda Guerra Mundial, Stalin fue visto como el gran líder que había conducido al pueblo soviético a la victoria en su lucha contra la Alemania Nazi. A finales de la década de los años 40, el patriotismo ruso fue en ascenso debido a los éxitos propagandísticos. Por ejemplo, algunas invenciones y descubrimientos científicos fueron reclamados por la propaganda rusa. Ejemplos de ello son la máquina de vapor, reclamada por el padre y el hijo de la familia Cherepánov; la lámpara incandescente, por y Lodyguin; la radio, por Popov; y el avión, por Mozhaiski. Continuaron sus políticas represivas (incluso en los territorios recién anexionados), pero nunca llegaron a los extremos de la década de 1930.

Internacionalmente, Stalin vio la consolidación del poder como un paso necesario para proteger a la Unión Soviética, rodeándolo de gobiernos amistosos, como un cordón sanitario contra posibles invasiones. Mientras que "Occidente" buscó un modelo similar de protección contra la expansión comunista. Estas políticas condujeron a una estabilidad, donde el éxito de la influencia soviética dependería de la cooperación entusiasta de las naciones satélite.

Stalin había tenido la esperanza de que la retirada y la desmovilización de EE.UU. darían lugar a un aumento de la influencia comunista, especialmente en Europa. Cada una de las partes veía las acciones defensivas de la otra como provocaciones desestabilizadoras y estos dilemas de seguridad desgastaron las relaciones entre la Unión Soviética y sus exaliados occidentales de la Segunda Guerra Mundial y dio lugar a un prolongado período de tensión y la desconfianza entre el Este y Occidente conocido como la Guerra Fría ("véase Telón de acero").

El Ejército Rojo terminó de manera exitosa la Segunda Guerra Mundial ocupando gran parte del territorio que había sido ocupado anteriormente por los países del Eje.

En Asia, el Ejército Rojo invadió Manchuria en el último mes de la guerra y también tomó el control de Corea cerca de Paralelo 38. En China, Mao Zedong del Partido Comunista de China, receptivo a recibir el apoyo soviético, derrotó al prooccidental y proamericano Partido Nacionalista Chino, en la Guerra Civil China.

Los comunistas controlaron la mayor parte de China, mientras que los nacionalistas se refugiaron en un pequeño estado en la isla de Formosa (actualmente Taiwán). La Unión Soviética reconoció pronto las "hazañas" de Mao poco después de la fundación de la República Popular de China, que es considerada como un nuevo aliado. La República Popular reivindicó Taiwán, a pesar de que nunca ha celebrado su autoridad en la isla.

Las relaciones diplomáticas con China alcanzaron un punto culminante con la firma del Tratado Chino-Soviético de Amistad y Alianza en 1950. Ambos países proporcionaron apoyo militar a un nuevo estado en Corea del Norte. En 1950, después de varios conflictos fronterizos, estalló la guerra entre el nuevo estado y EE. UU. y sus aliados de Corea del Sur, comenzando la Guerra de Corea.

En Europa existían zonas de ocupación soviética, tanto en Alemania como en Austria. Hungría y Polonia estaban prácticamente ocupadas militarmente. Desde 1946 a 1948 fueron elegidos en Polonia, Checoslovaquia, Hungría, Rumania y Bulgaria gobiernos de coalición integrados por comunistas, así también los movimientos comunistas accedieron al poder en Yugoslavia y Albania.

Estas naciones se conocieron como el Bloque del Este o Bloque Comunista. Gran Bretaña y los Estados Unidos apoyaron la lucha contra los comunistas en la Guerra Civil Griega y los soviéticos sospechosos de apoyar a los comunistas griegos, aunque Stalin se abstuvo de involucrarse en Grecia, despidiendo a la circulación prematuramente. Albania siguió siendo un aliado de la Unión Soviética, pero Yugoslavia rompió con la URSS en 1948.

Ambas superpotencias vieron a Alemania como país clave. En represalia a la formación de la Trizona occidental, Stalin decidió tomar medidas.

Gracias a la información del agente británico Donald Maclean y otros agentes de espionaje británicos y norteamericanos, Stalin era perfectamente conocedor de que los Estados Unidos no había procedido a la producción masiva de armas atómicas, de hecho, ni siquiera habían producido ninguna desde Nagasaki. Un gran número habría sido necesario para destruir a las fuerzas comunistas, ya fuera en Europa o el Lejano Oriente. Por lo tanto, ordenó un bloqueo en Berlín, que estaba bajo el dominio británico, francés, EE. UU. y la ocupación, a prueba de las potencias occidentales.

El bloqueo de Berlín fracasó debido a la masiva campaña de reabastecimiento aéreo, denominado Luftbrücke, llevado a cabo por las potencias occidentales. En 1949, Stalin reconoció la derrota y puso fin al bloqueo. Después de la formación de Alemania Occidental por la unión de las tres zonas occidentales de ocupación, los soviéticos declararon en 1949, Alemania Oriental país independiente, bajo un gobierno comunista.

Stalin originalmente apoyó la creación de Israel en 1948. La Unión Soviética fue uno de los primeros países en reconocer el nuevo país. Golda Meir llegó a Moscú como primer embajador de Israel en la Unión Soviética ese mismo año. Más tarde cambió de opinión oponiéndose a Israel y ordenando la disolución del Comité Judío Antifascista varios de cuyos miembros fueron luego detenidos y ejecutados durante la llamada "Noche de los Poetas Asesinados".

Al contrario que la política de Estados Unidos de armamento restringido (limitado equipo fue proporcionado por la infantería y las fuerzas de policía) a Corea del Sur, Stalin también ampliamente armados por Kim Il-sung en Corea del Norte, el ejército y las fuerzas aéreas con equipamiento militar (incluido los Tanques T-34/85) y "asesores" muy por encima de lo que se requería para fines defensivos), con el fin de facilitar a Kim (exoficial de la Unión Soviética) una conquista del resto de la península coreana. Pilotos soviéticos volaron en aviones soviéticos desde bases chinas contra aeronaves de las Naciones Unidas en defensa de Corea del Norte. Investigaciones en la Unión Soviética, posteriores a la Guerra Fría han revelado que la guerra de Corea fue iniciada por Kim Il Sung con la autorización expresa de Stalin.

En los últimos años de vida de Stalin, una de sus últimas grandes iniciativas de política exterior fue la Nota de Stalin de 1952 para la reunificación alemana y la no intervención de las superpotencias en Europa central, pero Gran Bretaña, Francia y los Estados Unidos sospecharon de la propuesta y rechazaron la oferta.
A partir de 1950 la salud de Stalin, que ya tenía setenta años de edad, empezó a desmejorar. Su memoria fallaba, se agotaba fácilmente y su estado general empeoró. Vladímir Vinográdov, su médico personal, le diagnosticó una hipertensión aguda. Vinográdov propuso un tratamiento a base de medicamentos o inyecciones y recomendó a Stalin que abandonase o al menos redujese sus funciones en el gobierno. 

En octubre de 1952 se celebró el XIX Congreso del PCUS. En él, Stalin insinuó sus deseos no belicistas y no intervencionistas en el resto del mundo, tal y como ya habría publicado en su anterior Nota. Sin embargo, Malenkov hizo un discurso oficial en el cual reafirmaba que para la URSS era vital estar presente en todos los conflictos internacionales apoyando las revoluciones socialistas. Por primera vez en muchos años, el Congreso apoyó las intenciones de Malenkov y no las de Stalin. Jean Paul Sartre afirma que Stalin, sin alterarse, clausuró el Congreso con un breve discurso cuyo epílogo fue: «¡Abajo los fomentadores de la guerra!»

Si bien este revés político era demasiado modesto como para amenazar su poder, tras el XIX Congreso Stalin tomó la determinación de reanudar las purgas. Su paranoia, adormecida tras la Segunda Guerra Mundial, aumentó tras recibir una carta de la doctora Lidia Timashuk, una especialista del Policlínico del Kremlin. En esta misiva, la doctora Timashuk acusaba a Vinográdov y a otros ocho médicos de origen judío de estar recetando tratamientos inadecuados a altos mandos del Partido y del Ejército, a fin de acabar con sus vidas. Sin esperar a recibir ninguna otra prueba, Stalin ordenó el arresto de los nueve médicos y aprobó que fuesen torturados hasta confesar. Dos de los acusados fallecieron durante los interrogatorios y los siete supervivientes acabaron firmando el texto que sus interrogadores pusieron sobre la mesa. Además Stalin hizo publicar en el diario "Pravda" que los servicios de seguridad habían estado "torpes" en descubrir lo que bautizó como el Complot de los médicos, y que había sido él mismo quien lo había desactivado.

Stalin multiplicó en estas fechas sus apariciones en público, visitaba las sedes del partido, hablaba con responsables de los distintos departamentos y nunca dejaba traslucir sus pensamientos. A finales de enero de 1953 su secretario privado desapareció sin dejar rastro. Poco después, el 15 de febrero, el jefe de sus guardaespaldas fue ejecutado sumariamente en lo que se dijo había sido una "muerte prematura". Este comportamiento aterrorizó a los miembros del Politburó, sobre todo a los más veteranos, que quedaron convencidos de que una nueva purga estaba ya en marcha. A partir de aquí, existen dos versiones sobre la muerte de Stalin.

La primera de ellas, versión oficial y hasta ahora la más verosímil, relata que la noche del sábado 28 de febrero de 1953 Stalin celebró una reunión en Kúntsevo con su círculo interno, formado por Beria, Malenkov, Jrushchov y Nikolái Bulganin. En dicho encuentro los cinco hombres vieron una película y después disfrutaron de una tardía cena. Los invitados se retiraron a las cuatro de la madrugada, cuando Stalin se fue a dormir.

La otra versión, defendida por historiadores como Iliá Erenburg y Víktor Aleksándrov, indica que esta reunión no tuvo nada de amistoso. A ella habrían sido invitados también Lázar Kaganóvich y Voroshílov, que se habrían enzarzado en una discusión con Stalin exigiéndole la liberación de los médicos. Supuestamente Stalin respondió gritándoles que eran unos traidores. Los dos miembros del Politburó habrían roto entonces sus carnés del partido y Stalin, fuera de sí, habría abandonado la reunión para encerrarse en su dormitorio.

Sea como fuere, la realidad es que al día siguiente Stalin no salió de su cuarto y no llamó ni a los criados ni a los guardias. Nadie se atrevió a entrar en su habitación hasta que, sobre las diez de la noche del domingo 1 de marzo, su mayordomo abrió la puerta y lo encontró tendido en el suelo, vestido con la ropa que llevaba la noche anterior y sin apenas poder hablar. Se llamó a los miembros del Politburó, que lentamente fueron acudiendo a la dacha de Stalin, pero nadie llamó a un médico. Finalmente, pasadas veinticuatro horas, Beria hizo venir a algunos doctores que dictaminaron que Stalin había sufrido un ataque cerebrovascular y había caído fulminado.

La agonía de Stalin se alargó varios días más. En ocasiones abría los ojos y miraba furibundamente a quienes lo rodeaban. Se cuenta que en estos momentos Beria le cogía de la mano y le suplicaba que se recuperase, pero cuando volvía a desvanecerse lo insultaba y le deseaba una dolorosa muerte. El día 4 aparentó una súbita mejoría y una enfermera comenzó a darle de beber leche con una cuchara, lo que hizo que el enfermo señalase un cuadro que había sobre la cabecera de su cama, donde una niña daba leche a una oveja. En ese momento, sufrió un nuevo ataque y entró en coma. Los médicos que atendían a Stalin le practicaron reanimación cardiopulmonar en las diversas ocasiones en que se le detuvo el corazón, hasta que finalmente a las 22:10 del día 5 de marzo no consiguieron reanimarlo. Según algunos testigos , los enfermeros siguieron esforzándose hasta que un lacónico Jrushchov dijo: «Basta, por favor… ¿No ves que está muerto?».

Muchos años después de la caída de la Unión Soviética se han vuelto a estudiar las circunstancias que rodearon la muerte de Stalin. No faltan autores, como el historiador ruso Vladímir P. Naúmov o Jonathan Brent (catedrático de Historia en Yale), que afirman que fue envenenado por Beria, quien al poco de su muerte llegó a decir ante el Politburó: «Yo lo maté, lo maté y os salvé a todos», según relata el propio Nikita Jrushchov en sus memorias. Sin embargo, esta tesis nunca ha sido demostrada ni reconocida, como tampoco la del posible enfrentamiento final entre Stalin y el Politburó. De este modo, la causa oficial de su muerte sigue siendo un ataque cerebrovascular provocada por su hipertensión.

El cuerpo embalsamado de Iósif Stalin permaneció junto al de Lenin en el mausoleo de este desde su muerte en 1953 hasta el 31 de octubre de 1961, cuando fue retirado durante la campaña de desestalinización promovida por Nikita Jruschov y enterrado en la parte exterior de la Necrópolis de la Muralla del Kremlin, detrás del mausoleo. Su tumba se encuentra entre las de Súslov y Mijaíl Kalinin. La estatua que la corona es de un blanco algo más claro que la del resto de líderes del mausoleo y por su ubicación resulta visible la parte de la Plaza Roja más próxima a la Catedral de San Basilio.

La primera mujer de Stalin, Yekaterina Svanidze, murió en 1907, solo cuatro años después de su matrimonio. Tuvieron un hijo, Yákov Dzhugashvili, con el que Stalin no tuvo contacto desde la muerte de su madre.

Yákov intentó suicidarse disparándose, pero sin éxito, y sufriendo graves heridas. Montefiore afirma que en ocasión de esto Stalin comentó: «Ni siquiera puede dispararse bien». Yákov formó parte del Ejército Rojo y fue capturado por las tropas alemanas durante la Segunda Guerra Mundial. Alemania ofreció a Stalin intercambiarlo por el general alemán Friedrich Paulus, rendido en Stalingrado, pero el dirigente soviético no accedió, arguyendo que la Unión Soviética no canjeaba soldados por mariscales de campo. Yákov murió oficialmente abatido en una valla por los guardias que custodiaban el campo de concentración, intentando escapar. Algunas personas afirman que corrió hacia la valla para que los guardas lo matasen, pero esto no ha sido comprobado.

Su segunda mujer fue Nadezhda Allilúyeva, fallecida en 1932. La causa oficial de su muerte fue una grave enfermedad, pero es posible que se suicidase disparándose tras una discusión con Stalin. Juntos tuvieron un hijo, Vasili, y una hija, Svetlana. Vasili consiguió rangos militares en la Fuerza Aérea Soviética, muriendo a causa del alcohol en 1962. Svetlana abandonó la URSS en 1967 y murió en 2011 en los Estados Unidos de América.

La madre de Stalin, a cuyo funeral no asistió él, murió en 1937. Se afirma que Stalin guardaba rencor a su madre por haberlo obligado a ingresar en el seminario.

El historiador Robert Conquest considera que Stalin fue «probablemente la persona que más influyó en el curso del siglo XX». Robert Service lo calificó como «uno de los políticos más destacados del siglo XX». Simon Sebag Montefiore lo describía como una «excepcional combinación: intelectual y asesino», un hombre que fue «el político definitivo» y «el más esquivo y fascinante titán del siglo XX». De acuerdo con el historiador Kevin McDermott, las interpretaciones de Stalin van «desde las serviles y adulatorias hasta las vitriólicas y condenatorias». Para la mayoría de los occidentales, así como para los rusos anticomunistas, es visto mayoritaríamente de forma negativa como un asesino en masa; para un número significativo de rusos y georgianos, se trata de un gran hombre de estado.

Stalin fortaleció y estabilizó la Unión Soviética. Service sugería que sin el liderazgo de Stalin la Unión Soviética se podría haber colapsado mucho antes de 1991. Al momento de su muerte, el país había sido transformado en una potencia mundial y un coloso industrial, con una población alfabetizada. Según Service, la Unión Soviética de Stalin podía afirmar «logros impresionantes» en términos de urbanización, fuerza militar, educación y orgullo soviético. Aunque millones de ciudadanos soviétidos le despreciaban, su apoyo seguía siendo amplio en la sociedad soviética. La Unión Soviética de Stalin ha sido caracterizada como totalitaria. Varias biografías le describen como un dictador y un autócrata.

Después del fallecimiento de Stalin, el nuevo secretario general del PCUS, Nikita Jruschov, inició un proceso por el cual se denunció el eufemístico «culto a la personalidad» en referencia al conocido actualmente como el culto a Stalin. Esto dio inicio al proceso político conocido como desestalinización, por el cual se denunciaron los crímenes cometidos por Stalin en contra del Estado soviético y el Partido Comunista. Su punto culminante sucedió durante el XX Congreso del PCUS en 1956, en el cual Jruschov pronunció al cierre del mismo, el conocido "Discurso secreto".

El proceso de desestalinización de Jruschov acabó cuando fue sucedido como líder por Leonid Brézhnev en 1964; este introdujo cierto nivel de "re-estalinización" en la Unión Soviética. En 1969 y 1979 se propusieron planes para la completa rehabilitación del legado de Stalin, pero ambas fueron rechazadas por quejas tanto internas como de partidos comunistas extranjeros. Mijaíl Gorbachov vio la denuncia total de Stalin como necesaria para la regeneración de la sociedad soviética. Tras la caída de la Unión Soviética en 1991, el primer presidente de la nueva Federación de Rusia, Borís Yeltsin, mantuvo la denuncia a Stalin y añadió la denuncia a Lenin. Su sucesor, Vladímir Putin, no buscó rehabilitar a Stalin pero puso énfasis en celebrar los logros soviéticos bajo el liderazgo de Stalin en lugar de la represión.

En encuestas realizadas entre 2003 y 2006, al menos un cuarto de los rusos votarían a Stalin seguro o probablemente, mientras que menos del 40% estarían seguros de no votarle. En 2008, en el programa de televisión "Name of Russia", Stalin fue votado como la tercera personalidad más notable de la historia rusa. Una encuesta de 2017 concluía que la popularidad de Stalin alcanzó, entre la población rusa, su punto más alto en 16 años, con un 46% expresando una visión favorable.

Los primeros investigadores en intentar contar la cantidad de personas que murieron a causa del régimen de Stalin se vieron obligados a recurrir en gran medida a las pruebas anecdóticas. Sus estimaciones variaban de 3 a algo más de 50 millones. Después de la disolución de la Unión Soviética en 1991, las evidencias de los archivos soviéticos se hicieron disponibles. De acuerdo con los registros, alrededor de 800.000 presos fueron ejecutados por el régimen de Stalin por delitos políticos o penales, mientras que alrededor de 1,7 millones murieron en gulags y unos 390.000 perecieron durante reasentamientos forzosos, un total de alrededor de 3 millones de víctimas. Según ciertas fuentes, durante el mandato de Stalin cerca de 5 millones de personas fueron encarceladas u obligadas a trabajos forzados, un millón habían sido ejecutados y 2 millones perecieron en trabajos forzados.

El debate continúa, sin embargo, puesto que algunos historiadores creen que el archivo contiene cifras poco fiables. Por ejemplo, sostiene Gellately que los muchos sospechosos torturados hasta la muerte mientras estaban en «custodia de investigación» es probable que no se hayan contado entre los ejecutados. Asimismo, existen categorías de víctimas que no fueron registradas de forma correcta por los soviéticos, como las víctimas de las deportaciones étnicas, o transferencias de población alemana después de la Segunda Guerra Mundial.

Entre 1919 y mediados de los años 1950 fueron deportadas más de seis millones de personas, casi el doble que los ciudadanos soviéticos deportados por el Tercer Reich durante la Gran Guerra Patria para realizar trabajos forzados. De estos, un millón a un millón y medio habrían muerto directamente a causa del traslado.

Particularmente afectados serán los kulaks y los alemanes del Volga —aunque las deportaciones de germanos étnicos en Rusia databan desde 1914—. Alcanzaran su apogeo entre los años 1930 y finales de los 1940 —durante la guerra con los alemanes será deportado un tercio del total—. Las zonas de «acogida» preferidas serán las inhóspitas, despobladas y aisladas Siberia y Asia Central soviética. Entre los grupos étnicos, sociales y religiosos deportados hay hordas de cosacos (del Amur, Astracán, Azov, Mar Negro, Bug Meridional, Don, Kubán, Oremburgo, Semirechye, Terek, Transbaikal, Ural y Ussuri), «elementos socialmente peligrosos», kulaks, campesinos en general, kazajos nómades, alemanes —no todos del Volga—, polacos —incluidos refugiados desde 1940—, fineses de Ingria, kurdos, coreanos, chinos, japoneses, rusos de Harbin, judíos persas, azeríes, persas, asirios, noruegos, suecos, rumanos, griegos de Crimea (pondios), tártaros de Crimea, karacháis, calmucos, chechenos, ingusetios, balkarios, cabardinos, turcos meskh, hamshenis, karapapakos, lazes, armenios, basmachís, búlgaros de Crimea, armenios musulmanes de Georgia ("khemshin"), miembros de la "Verdadera Iglesia Ortodoxa", testigos de Jehová y supuestos nacionalistas lituanos, moldavos, letones, estonios, bielorrusos, ucranianos.

Así, mientras que algunos investigadores han estimado el número de víctimas de las represiones de Stalin en un total de 4 millones más o menos, otros creen que el número es considerablemente superior. El escritor ruso Vadim Erlikman, por ejemplo, hace las siguientes estimaciones: ejecuciones, 1,5 millones; gulags, 5 millones; deportaciones, 1,7 millones a 7,5 millones de deportados, y prisioneros de guerra y civiles alemanes, 1 millón, lo que hace un total de alrededor de 9 millones de víctimas de la represión.

Algunos también han incluido los 6 a 8 millones de víctimas de la hambruna 1932-1933 como víctimas de la represión. Esta clasificación es controvertida sin embargo, ya que los historiadores difieren en cuanto a si la hambruna era una deliberada parte de la campaña de represión contra los kuláks, o simplemente un consecuencia no deseada de la lucha por la colectivización forzada.

Hay autores para los que un mínimo de alrededor de 10 millones de muertos —4 millones por la represión y 6 por el hambre— son atribuibles al régimen; algunos libros de reciente publicación sugieren un probable total de alrededor de 20 millones. Por ejemplo, agregar 6-8 millones de víctimas de la hambruna según Erlikman por encima de las estimaciones de muertes directas, daría un total de entre 15 y 20 millones de víctimas. El investigador Robert Conquest, mientras tanto, ha revisado su estimación inicial de hasta 30 millones de víctimas a 20 millones. Otros siguen considerando que sus anteriores estimaciones, mucho más altas, son correctas. Todas estas estimaciones se ven contrastadas con la Demografía de Rusia siendo el de la segunda guerra mundial la única caída con semejantes cifras.

Stalin era considerado entre los bolcheviques como un experto en la cuestión nacional. En 1913 publicó su artículo «[[El marxismo y la cuestión nacional» en el que desarrolla su teoría [[marxista]] sobre la nación, en oposición a las teorías nacionales que defendían los [[Bundismo|bundistas]] y [[menchevique]]s. En desarrolla el concepto de [[nación]] que resume de la siguiente forma:

Para Stalin, una nación lo es al cumplir todos y cada uno de estos rasgos:


Esta definición se contrapone a la visión de [[Otto Bauer]], apoyada por los [[Bundismo|bundistas]], en la que una nación puede constituirse únicamente mediante el «carácter nacional», lo que Stalin llama «comunidad de psicología», siendo especialmente relevante el caso de los [[Pueblo judío|judíos]]. Por otro lado, para Stalin el [[Imperio austrohúngaro]] o el [[Imperio ruso]] tampoco eran naciones.

El movimiento nacional es siempre, según Stalin, una lucha entre las [[Burguesía|clases burguesas]] de la nación dominante y la nación oprimida. Sin embargo, el [[proletariado]] de las naciones oprimidas está igualmente afectado por la represión de la nación dominante y debe luchar contra la opresión de las naciones y proclamar el [[derecho de autodeterminación]]. Stalin puntualiza que este derecho de autodeterminación debe ser en beneficio de la mayoría de la nación. Es decir, de las clases trabajadoras.

Stalin tuvo una producción escrita desde sus inicios revolucionarios, aquí se consignan sólo los más importantes:


[[Categoría:Antirrevisionistas]]
[[Categoría:Ateos activistas]]
[[Categoría:Tiflisianos]]
[[Categoría:Secretarios generales de partidos comunistas]]
[[Categoría:Estalinismo]]
[[Categoría:Bolcheviques]]
[[Categoría:Comunistas de la Unión Soviética]]
[[Categoría:Políticos de la Unión Soviética]]
[[Categoría:Gobernantes de la Unión Soviética]]
[[Categoría:Políticos de la Segunda Guerra Mundial]]
[[Categoría:Políticos de la Guerra Fría]]
[[Categoría:Ateos de Rusia]]
[[Categoría:Dictadores]]
[[Categoría:Mariscales de la Unión Soviética]]
[[Categoría:Marxistas de Rusia]]
[[Categoría:Personas enterradas en el Kremlin]]
[[Categoría:Héroes de la Unión Soviética]]
[[Categoría:Héroe del Trabajo Socialista]]
[[Categoría:Comunistas de Georgia]]
[[Categoría:Enfermos por viruela]]
[[Categoría:Generalísimos]]
[[Categoría:Georgianos del siglo XX]]
[[Categoría:Miembros del Partido Comunista de la Unión Soviética]]
[[Categoría:Ministros de defensa de la Unión Soviética]]

</doc>
<doc id="22191" url="https://es.wikipedia.org/wiki?curid=22191" title="Helio">
Helio

El helio (en griego: ἥλιος "helios", "sol") es un elemento químico de número atómico 2, símbolo He y peso atómico estándar de 4,0026. Pertenece al grupo 18 de la tabla periódica de los elementos, ya que al tener el nivel de energía completo presenta las propiedades de un gas noble. Es decir, es inerte (no reacciona) y al igual que estos, es un gas monoatómico incoloro e inodoro que cuenta con el menor punto de ebullición de todos los elementos químicos y solo puede ser licuado bajo presiones muy grandes y no puede ser congelado.

Durante un eclipse solar en 1868, el astrónomo francés Pierre Janssen observó una línea espectral amarilla en la luz solar que hasta ese momento era desconocida. Norman Lockyer observó el mismo eclipse y propuso que dicha línea era producida por un nuevo elemento, al cual llamó helio, con lo cual, tanto a Lockyer como a Janssen se les adjudicó el descubrimiento de este elemento. En 1903 se encontraron grandes reservas de helio en campos de gas natural en los Estados Unidos, país con la mayor producción de helio en el mundo.

Industrialmente se usa en criogenia (siendo su principal uso, lo que representa alrededor de un 28 % de la producción mundial), en la refrigeración de imanes superconductores. Entre estos usos, la aplicación más importante es en los escáneres de resonancia magnética. También se utiliza como protección para la soldadura por arco y otros procesos, como el crecimiento de cristales de silicio, los cuales representan el 20 % de su uso para el primer caso y el 26 % para el segundo. Otros usos menos frecuentes, aunque popularmente conocidos, son el llenado de globos y dirigibles, o su empleo como componente de las mezclas de aire usadas en el buceo a gran profundidad. El inhalar una pequeña cantidad de helio genera un cambio en la calidad y el timbre de la voz humana. En la investigación científica, el comportamiento del helio-4 en forma líquida en sus dos fases, helio I y helio II, es importante para los científicos que estudian la mecánica cuántica (en especial, el fenómeno de la superfluidez), así como para aquellos que desean conocer los efectos ocurridos en la materia a temperaturas cercanas al cero absoluto (como el caso de la superconductividad).

El helio es el segundo elemento más ligero y el segundo más abundante en el universo observable, constituyendo el 24 % de la masa de los elementos presentes en nuestra galaxia. Esta abundancia se encuentra en proporciones similares en el Sol y en Júpiter. Por masa se encuentra en una proporción doce veces mayor a la de todos los elementos más pesados juntos. La presencia tan frecuente de helio es debida a elevada energía de enlace por nucleón del helio-4 con respecto a los tres elementos que le siguen en la tabla periódica (litio, berilio y boro). Esta energía da como resultado la producción frecuente de helio tanto en la fusión nuclear como en la desintegración radioactiva. La mayor parte del helio en el universo se encuentra presente en la forma del isótopo helio-4 (He), el cual se cree que se formó unos 15 minutos después del Big Bang. Gracias a la fusión de hidrógeno en las estrellas activas, se forma una pequeña cantidad de helio nuevo, excepto en las de mayor masa, debido a que durante las etapas finales de su vida generan su energía convirtiendo el helio en elementos más pesados. En la atmósfera de la Tierra se encuentran trazas de helio debido a la desintegración radioactiva de algunos elementos. En algunos depósitos naturales el gas se encuentra en cantidad suficiente para la explotación.

En la Tierra, la ligereza de helio ha provocado su evaporación de la nube de gas y polvo a partir de la cual se formó el planeta, por lo que es relativamente poco frecuente —con una fracción de 0,00052 por volumen— en la atmósfera terrestre. El helio presente en la Tierra hoy en día ha sido creado en su mayor parte por la desintegración radiactiva natural de los elementos radioactivos pesados (torio y uranio), debido a que las partículas alfa emitidas en dichos procesos constan de núcleos de helio-4. Este helio radiogénico es atrapado junto con el gas natural en concentraciones de hasta el 7 % por volumen, del que se extrae comercialmente por un proceso de separación a baja temperatura llamado destilación fraccionada.

A pesar de que la configuración electrónica del helio es 1s², no figura en el grupo 2 de la tabla periódica de los elementos, junto al hidrógeno en el bloque s, sino que se coloca en el grupo 18 del bloque p, ya que al tener el nivel de energía completo presenta las propiedades de un gas noble.

En condiciones normales de presión y temperatura es un gas monoatómico no inflamable, pudiéndose licuar solamente en condiciones extremas (de alta presión y baja temperatura).

Tiene el punto de solidificación más bajo de todos los elementos químicos, siendo el único líquido que no puede solidificarse bajando la temperatura, ya que permanece en estado líquido en el cero absoluto a presión normal. De hecho, su temperatura crítica es de tan solo 5,20 K o −267,96 grados Celsius. Los sólidos compuestos por ³He y He son los únicos en los que es posible, incrementando la presión, reducir el volumen más del 30 %. El calor específico del gas helio es muy elevado y el helio vapor muy denso, expandiéndose rápidamente cuando se calienta a temperatura ambiente.

El helio sólido solamente existe a presiones del orden de 100 MPa a 15 K (−258,15 °C). Aproximadamente a esa temperatura, sufre una transformación cristalina, de una estructura cúbica centrada en las caras a una estructura hexagonal compacta. En condiciones más extremas (3 K, aunque presiones de 3 MPa) se produce un nuevo cambio, empaquetándose los átomos en una estructura cúbica centrada en el cuerpo. Todos estos empaquetamientos tienen energías y densidades similares, debiéndose los cambios a la forma en la que los átomos interactúan.

El helio es un elemento químico cuyo átomo es el más simple de resolver utilizando las reglas de la mecánica cuántica después del átomo de hidrógeno. Se compone de dos electrones en órbita alrededor de un núcleo que contiene dos protones junto con uno o dos neutrones, dependiendo del isótopo. Sin embargo, como en la mecánica newtoniana, ningún sistema que consista de más de dos partículas se puede resolver con un enfoque de análisis matemático exacto (véase problema de los tres cuerpos) y el helio no es la excepción. Así, los métodos matemáticos son necesarios, incluso para resolver el sistema de un núcleo y dos electrones. Sin embargo, tales métodos de la química computacional se han utilizado para crear una imagen mecánico cuántica de las uniones de los electrones de helio con una precisión dentro de un 2 % del valor correcto, con unos pocos pasos de cálculo computacional. En estos modelos se observa que cada electrón evita parcialmente que el otro sienta la interacción con el núcleo, de tal manera que la carga nuclear efectiva "Z" es de aproximadamente 1,69 unidades, y no las 2 cargas de un "núcleo desnudo" clásico de helio.

El átomo de hidrógeno se utiliza ampliamente para ayudar a resolver el átomo de helio. El modelo atómico de Bohr dio una explicación muy precisa del espectro del átomo de hidrógeno, pero cuando se intentó utilizar en el helio el modelo falló. Werner Heisenberg desarrolló una modificación del análisis de Bohr, en el que utilizó valores semiintegrados de los números cuánticos. La teoría del funcional de la densidad se utiliza para obtener los niveles de energía en su estado base del átomo de helio, junto con el método de Hartree-Fock.

El núcleo del átomo de helio-4, que es exactamente igual a una partícula alfa, es particularmente interesante. La razón de esto se debe a que experimentos de dispersión de electrones de alta energía han mostrado que su carga decrece de forma exponencial a partir de un máximo en su punto central, exactamente de la misma manera en que decrece la densidad de carga en su propia nube de electrones. Esta simetría refleja principios físicos similares: el par de neutrones y de protones en el núcleo del helio obedecen a las mismas reglas mecánico-cuánticas que los dos electrones que lo orbitan —aunque la unión de las partículas en el núcleo se debe a un potencial diferente al que mantiene a los electrones en la nube alrededor del átomo—. De esta manera, estos fermiones (es decir, tanto protones como electrones y neutrones) ocupan completamente los orbitales 1s en pares, ninguno de ellos posee momento angular orbital y cada uno de ellos cancela el espín intrínseco del otro. El añadir otra de cualquiera de estas partículas requeriría momento angular y liberaría sustancialmente menos energía (de hecho, ningún núcleo con cinco nucleones es estable). Por esta razón, este arreglo para estas partículas es extremadamente estable energéticamente, y dicha estabilidad da lugar a muchos fenómenos cruciales inherentes al helio en la naturaleza.

Como ejemplo de estos hechos debidos a la alta estabilidad de la configuración electrónica del helio está la baja reactividad química de este elemento (la más baja de toda la tabla periódica), así como la falta de interacción de sus átomos entre ellos mismos. Esto produce los puntos de fusión y de ebullición más bajos de todos los elementos. De la misma manera, la estabilidad energética del núcleo de helio-4 da lugar a una fácil producción de estos en reacciones atómicas que involucran tanto emisión de partículas pesadas como fusión nuclear. Cierta cantidad de helio-3 estable se produce en reacciones de fusión a partir del hidrógeno, pero es una fracción mucho menor comparada con el helio-4. La estabilidad del helio-4 es la razón por la cual el hidrógeno se convierte en esta forma de helio en el Sol, en vez de helio-3, deuterio u otros elementos más pesados. Asimismo es parcialmente responsable del hecho de que la partícula alfa es por mucho el tipo de partícula bariónica más comúnmente expelida por los núcleos atómicos. Dicho de otra manera, la desintegración alfa es mucho más común que la desintegración en núcleos más pesados.

La inusual estabilidad del helio-4 es importante también en cosmología. En los primeros minutos después del Big Bang, el universo estaba compuesto por una mezcla de nucleones (protones y neutrones) libres. Esta «sopa» tenía originalmente una proporción de seis protones por cada neutrón, y después de un tiempo se enfrió al punto tal que se pudo dar la fusión nuclear. La estabilidad del helio provocó que casi todas las agregaciones de nucleones formadas en ese momento fueran núcleos de helio-4. La unión de protones y neutrones para formar helio-4 tiene tanta fuerza que, de hecho, la producción de este elemento consumió casi todos los neutrones libres en cuestión de minutos, antes de que dichos núcleos pudieran decaer por desintegración beta. Esto dejó una cantidad muy pequeña de estas partículas para que se pudiera formar litio, berilio o boro. El enlace nuclear por cada nucleón en el helio-4 es más fuerte que en cualquiera de estos tres elementos (véase nucleogénesis y energía de enlace). Por lo tanto, no había ningún mecanismo energético disponible, una vez que se hubo formado el helio, para crear los elementos de número atómico 3, 4 y 5. En términos de energía, también era favorable la fusión del helio para formar el siguiente elemento en la tabla periódica con menor energía por nucleón: el carbono. No obstante, debido a la falta de elementos intermedios, este proceso requería la colisión casi simultánea de tres núcleos de helio-4 (véase proceso triple-alfa), por lo que no hubo suficiente tiempo para que el carbono se formara en el Big Bang: en cuestión de minutos, el universo temprano se enfrió a una temperatura y presión en las cuales la fusión de helio a carbono ya no fue posible. Esto ocasionó que el universo temprano poseyera un cociente hidrógeno/helio muy similar al observado actualmente (en masa, tres partes de hidrógeno por una de helio-4), con casi todos los neutrones del universo —como es el caso hoy en día— atrapados dentro de los núcleos de helio-4.

Todos los elementos más pesados —incluyendo aquellos que se necesitan para formar planetas rocosos como la Tierra y para la existencia de vida basada en el carbono— tuvieron que crearse posteriormente, en estrellas lo suficientemente calientes para quemar no solo hidrógeno —dado que esto solamente produce más helio— sino el mismo helio. Dichas estrellas son masivas y, por lo tanto, raras. Lo anterior da lugar al hecho de que todos los elementos químicos, aparte del hidrógeno y el helio, compongan solamente el 2 % de la masa en forma de átomos del universo. El helio-4, por su parte, constituye cerca del 23 % de toda la materia ordinaria del universo, es decir, prácticamente toda la materia ordinaria que no es hidrógeno.

El helio es el gas noble menos reactivo después del neón y por tanto, el segundo elemento menos reactivo de todos ellos. Es inerte y monoatómico en condiciones normales. Debido a su baja masa atómica, en la fase gaseosa, la conductividad térmica, el calor específico, y la velocidad del sonido son mayores que en cualquier otro gas, excepto el hidrógeno. Por razones similares, y también debido al pequeño tamaño de sus átomos, su tasa de difusión a través de los sólidos es tres veces mayor que la del aire, y alrededor del 65 % de la del hidrógeno.

Asimismo es también menos soluble en agua que cualquier otro gas conocido, y su índice de refracción es el más cercano a la unidad de todos los gases. Este elemento tiene un coeficiente Joule-Thomson negativo a temperatura ambiente normal, lo que significa que se calienta cuando se le permite expandirse libremente. Solo por debajo de su temperatura de inversión de Joule-Thomson (de 32 a 50 K a 1 atmósfera) se enfría en la expansión libre. Una vez preenfriado debajo de esta temperatura, el helio puede licuarse mediante el enfriamiento debido a su expansión.

La mayor parte del helio extraterrestre se encuentra en un estado de plasma, con propiedades muy diferentes a las del helio atómico. En el plasma, los electrones del helio no están ligados al núcleo, lo que hace que su conductividad eléctrica sea muy alta, aún cuando el gas está solo parcialmente ionizado. Las partículas cargadas son altamente influenciadas por los campos magnéticos y eléctricos. Por ejemplo, en el viento solar, junto con el hidrógeno ionizado, las partículas interactúan con la magnetosfera de la Tierra, dando lugar a la corriente de Birkeland y a las auroras.

A diferencia de cualquier otro elemento, el helio líquido se mantendrá así hasta el cero absoluto a presiones normales. Este es un efecto directo de la mecánica cuántica: en concreto, la energía del punto cero del sistema es demasiado alta para permitir la congelación. El helio sólido requiere una temperatura de 1 a 1,5 K (alrededor de -272 °C o -457 °F) y alrededor de 25 bar (2,5 MPa) de presión. A menudo es difícil distinguir el helio sólido del líquido ya que el índice de refracción de las dos fases es casi el mismo. El sólido tiene un marcado punto de fusión y estructura cristalina, pero es muy compresible. Aplicar presión en un laboratorio puede reducir su volumen en más del 30 %. Con un módulo de compresibilidad del orden de 50 MPa, es 50 veces más compresible que el agua. El helio sólido tiene una densidad de 0,214 ± 0,006 g/ml a 1,15 K y 66 atm, la densidad proyectada a 0 K y 25 bar (2,5 MPa) es 0,187 ± 0,009 g/ml.

Por debajo de su punto de ebullición de 4,22 K, y por encima del punto lambda de 2,1768 K, el isótopo helio-4 existe en un estado normal de líquido incoloro, llamado helio I. Al igual que otros líquidos criogénicos, el helio I hierve cuando se calienta y se contrae cuando baja su temperatura. Por debajo del punto lambda, sin embargo, esta fase no hierve y se expande a medida que la temperatura desciende aún más.

El helio tiene un índice de refracción similar al de un gas, de 1,026, lo que hace que su superficie sea muy difícil de ver, de tal forma que se suelen utilizar flotadores de poliestireno extruido para ver en dónde se encuentra la superficie. Este líquido incoloro, tiene una viscosidad muy baja y una densidad de 0,145 g/mL, que es solo una cuarta parte del valor predicho por la física clásica. Es necesario hacer uso de la mecánica cuántica para explicar esta propiedad y, por tanto, ambos tipos de helio líquido se llaman "fluidos cuánticos", lo que significa que muestran propiedades atómicas a escala macroscópica. Esto puede ser un efecto del hecho de que su punto de ebullición está muy cerca del cero absoluto, lo que impide que el movimiento molecular aleatorio (energía térmica) oculte sus propiedades atómicas.

El helio líquido por debajo de su punto lambda muestra características sumamente inusuales, en un estado llamado helio II. La ebullición del helio II no es posible debido a su alta conductividad térmica; la entrada de calor causa la evaporación del líquido directamente a gas. El isótopo helio-3 también tiene una fase de superfluido, pero solo a temperaturas mucho más bajas. Como resultado, se sabe menos sobre las propiedades de esta fase en dicho isótopo.

El helio II es un superfluido, un estado cuántico de la materia con propiedades extrañas. Por ejemplo, cuando fluye a través de capilares tan delgados como de 10 a 10 m, no tiene viscosidad medible. Sin embargo, cuando se realizan mediciones entre dos discos en movimiento, se observa una viscosidad comparable a la del helio gaseoso. La teoría actual explica este fenómeno utilizando un "modelo de dos fluidos" para el helio II. En este modelo, el helio líquido por debajo del punto lambda se considera que contiene una proporción de átomos de helio en estado base, que componen el superfluido, y que fluyen con una viscosidad exactamente igual a cero; y una proporción de átomos de helio en un estado excitado, que se comportan más como un fluido ordinario.

En el efecto fuente, se construye una cámara que está conectada a un depósito de helio II por medio de un disco sinterizado a través del cual el helio superfluido pasa fácilmente, pero aquellos líquidos que no son superfluidos no pueden. Si se calienta el interior del contenedor, el helio deja de ser superfluido. A fin de mantener fracción de equilibrio de helio superfluido, este se fuga a través del disco y aumenta la presión, haciendo que el líquido salga brotando del recipiente.

La conductividad térmica del helio II es mayor que la de cualquier otra sustancia conocida. Es un millón de veces mayor que la del helio I y varios cientos de veces la del cobre. Esto se debe a que la conducción de calor se produce por un mecanismo cuántico excepcional. La mayoría de los materiales que son buenos conductores térmicos tienen una banda de electrones de valencia libres que sirven para transferir el calor. El helio II no tiene banda de valencia, pero conduce bien el calor. El flujo de calor se rige por ecuaciones similares a la ecuación de onda utilizada para caracterizar la propagación del sonido en el aire. Cuando se introduce calor, este se mueve a través de helio II en forma de ondas a 20 metros por segundo a una temperatura de 1,8 K. Este fenómeno es conocido como segundo sonido.

El helio II también presenta un efecto de ascensión. Cuando una superficie se extiende más allá del nivel de helio II, este se mueve a lo largo de la superficie, contra la fuerza de gravedad. El líquido se escapará de un contenedor que no esté sellado reptando por las paredes del mismo hasta que encuentre una región con mayor temperatura donde se evaporará. Este ascenso lo realiza en una película de 30 nm de espesor, independientemente del material de superficie. Esta película se llama película de Rollin y lleva el nombre de la primera persona que caracterizó este rasgo, Bernard V. Rollin. Como resultado de este comportamiento y de la habilidad del helio II de escapar a través de aberturas pequeñas, es muy difícil mantener a este fluido confinado. Las ondas que se propagan a través de una película de Rollin se rigen por la misma ecuación de ondas de gravedad en aguas poco profundas, pero en lugar de la gravedad, la fuerza de restauración es la fuerza de van der Waals. Estas ondas son conocidas como tercer sonido.

Dado que el helio es un gas noble, en la práctica no participa en las reacciones químicas, aunque bajo la influencia de descargas eléctricas o bombardeado con electrones forma compuestos.

El helio tiene una valencia cero y no es químicamente reactivo bajo condiciones normales. Es un aislante eléctrico a menos que esté ionizado. Al igual que los demás gases nobles, tiene niveles de energía metaestables, lo que le permite seguir ionizado en una descarga eléctrica con un voltaje por debajo de su potencial de ionización. El helio puede formar compuestos inestables, conocidos como excímeros, con el wolframio, yodo, flúor y fósforo, cuando se somete a una descarga eléctrica luminiscente, a un bombardeo de electrones, o bien es un plasma por otra razón. Los compuestos moleculares HeNe, HgHe y WHe, y los iones moleculares He, He, HeH, y HeD se pueden crear de esta manera. Esta técnica también ha permitido la producción de la molécula neutra He, que tiene un gran número de sistemas de bandas espectrales, y de la molécula HgHe, que aparentemente solo se mantiene unida por fuerzas de polarización. En teoría, otros compuestos reales también son posibles, como el fluorohidruro de helio (HHeF), que sería análogo al fluorohidruro de argón, descubierto en 2000. Los cálculos indican que dos nuevos compuestos que contienen un enlace de helio-oxígeno podrían ser estables. Dos nuevas especies moleculares, predichas teóricamente, CsFHeO y N(CH)4FHeO, son derivados de un anión metaestable [F-HeO], anticipado en 2005 en forma teórica por un grupo de Taiwán. De confirmarse experimentalmente, estos compuestos acabarían con la inercia química del helio, y el único elemento completamente inerte sería el neón.

El helio ha sido colocado en jaulas moleculares de carbono (los fullerenos) por medio de calentamiento a alta presión. Las moléculas de fullereno endohédrico formadas son estables hasta temperaturas altas. Cuando se forman los derivados químicos de estos fullerenos, el helio permanece dentro de ellos. Si se utiliza helio-3, se puede observar fácilmente por espectroscopia de resonancia magnética nuclear. Se han reportado una gran cantidad de fullerenos que contienen helio-3. Aunque los dichos átomos no se encuentran ligados por medio de enlaces covalentes o iónicos, estas sustancias tienen propiedades distintas y una composición definida, al igual que todos los compuestos químicos estequiométricos.

Existen ocho isótopos conocidos del helio, pero tan solo el He y el He son estables. En la atmósfera terrestre hay un átomo de ³He por cada millón de átomos de He. A diferencia de otros elementos, la abundancia isotópica del helio varía mucho por su origen, debido a los diferentes procesos de formación. El isótopo más común, el He, se produce en la Tierra mediante la desintegración alfa de elementos radiactivos más pesados; las partículas alfa que aparecen son átomos de He completamente ionizado. El He tiene un núcleo inusualmente estable debido a que sus nucleones están ordenados en capas completas. Además, este isótopo se formó en grandes cantidades durante la nucleosíntesis primordial en el Big Bang.

El ³He está presente hoy en día en la tierra tan solo en trazas (la mayoría data desde la formación de la Tierra), aunque algo de este cae a la Tierra al ser atrapado en el polvo cósmico. Algunos rastros también son producidos mediante la desintegración beta del tritio. Algunas rocas de la corteza terrestre tienen distintas proporciones de isótopos que varían hasta un factor de diez. Estas proporciones pueden usarse para investigar el origen de las rocas así como la composición del manto terrestre. El ³He es mucho más abundante en las estrellas como producto de la fusión nuclear. Por consiguiente, en el medio interestelar, la proporción de ³He y He es alrededor de 100 veces más grande que la que hay en la Tierra. El material extraplanetario, como regolitos de asteroides y lunares, tiene trazas de ³He producto del bombardeo de los vientos solares contra ellos. La superficie de la Luna tiene concentraciones de ³He de alrededor de 0,01 ppm. Algunas personas, principalmente Gerald Kulcinski en 1986, han propuesto explorar la Luna, excavar los regolitos lunares, y utilizar el ³He para fusión nuclear.

El He líquido puede ser enfriado hasta 1 kelvin utilizando enfriamiento por evaporación en recipientes en los que se puede alcanzar y mantener estas temperaturas. Un enfriamiento similar para el helio-3, que tiene un punto de ebullición más bajo, se puede alcanzar alrededor de los 0,2 K en un refrigerador de helio-3. Las mezclas que contienen la misma proporción de helio-3 y helio-4 a una temperatura por debajo de 0,8 K se separan en dos fases no miscibles debido a su incompatibilidad (cada una obedece a una estadística cuántica diferente: los átomos de helio-4 son bosones mientras que los átomos de helio-3 son fermiones). Los refrigeradores de dilución usan esta imposibilidad de mezclado para alcanzar temperaturas de unos pocos milikelvin.

Es posible producir isótopos exóticos de helio, los cuales rápidamente se descomponen en otras sustancias. El isótopo pesado de menor duración es el He, con un periodo de semidesintegración de 7.6 segundos. El He se descompone emitiendo una partícula beta y su periodo de desintegración es de 0,8 segundos. El He también emite partículas beta así como rayos gamma. Tanto el He y el He se crean mediante algunas reacciones nucleares. El He y el He son conocidos por tener un halo nuclear. El ²He (que consiste en dos protones y ningún neutrón) es un radioisótopo que se desintegra en protio (hidrógeno) por medio de emisión de protones, con un periodo de desintegración de 3 segundos.

El helio es el segundo elemento más abundante del universo conocido tras el hidrógeno y constituye alrededor del 23 % de la masa bariónica del universo. La mayor parte del helio se formó durante la nucleosíntesis del Big Bang, en los tres primeros minutos después de este. De esta forma, la medición de su abundancia contribuye a los modelos cosmológicos. En las estrellas, el helio se forma por la fusión nuclear del hidrógeno en reacciones en cadena protón-protón y en el ciclo CNO, los cuales forman parte de la nucleosíntesis estelar.

En la atmósfera terrestre la concentración de helio por volumen es de tan solo 5,2 partes por millón. La concentración es baja y prácticamente constante a pesar de la continua producción de nuevo helio, debido a que la mayor parte del helio en la atmósfera se escapa al espacio debido a distintos procesos. En la heterosfera terrestre, una parte de la atmósfera superior, el helio y otros gases ligeros son los elementos más abundantes.

Casi todo el helio presente en la Tierra es el resultado de la desintegración radiactiva, y por tanto, un globo de helio terrestre es, en esencia, una bolsa de partículas alfa expelidas por este proceso. El helio se encuentra en grandes cantidades en minerales de uranio y torio, incluyendo cleveíta, pechblenda, carnotita y monacita, ya que estos emiten partículas alfa (núcleos de helio, He) y los electrones se combinan de inmediato con ellas, tan pronto como las partículas son detenidas por la roca. De esta manera, se estima que unas 3.000 toneladas de helio se generan al año en toda la litosfera. En la corteza terrestre, la concentración de helio es de 8 partes por mil millones. En el mar, la concentración es de solo 4 partes por billón. También hay pequeñas cantidades en manantiales de aguas minerales, gas volcánico, y hierro meteórico. Debido a que el helio es atrapado de manera similar al gas natural por una capa impermeable de roca, las mayores concentraciones de este elemento en el planeta se encuentran en el gas natural, de donde se extrae la mayor parte del helio comercial. La concentración varía en una amplia gama de unas pocas ppm hasta más del 7 % en un pequeño campo de gas en el condado de San Juan, Nuevo México.

En 2016 científicos británicos descubren un gran yacimiento de gas helio en África. Este yacimiento, ubicado en la República Unida de Tanzania, mide aproximadamente 54 millones de pies cúbicos de volumen y es el mayor yacimiento de helio del mundo.

Para su uso a gran escala, se extrae por destilación fraccionada a partir del gas natural, que contiene hasta un 7 % de helio. Al tener un punto de ebullición más bajo que cualquier otro elemento, se utilizan bajas temperaturas y altas presiones para licuar casi todos los demás gases (principalmente nitrógeno y metano). El helio crudo resultante se purifica por medio de exposiciones sucesivas a temperaturas bajas, en la que casi todo el nitrógeno y los otros gases restantes se precipitan fuera de la mezcla gaseosa. Como una fase de purificación final, se utiliza carbón activado, lo que da como resultado helio grado A, con una pureza del 99,995 %. La principal impureza en el helio grado A es el neón. En la fase final de la producción, la mayoría del helio que se produce es licuado por medio de un proceso criogénico. Esto es necesario para aplicaciones que requieren helio líquido y también permite a los proveedores de helio reducir el costo en el transporte a larga distancia, dado que la mayoría de los contenedores de helio líquido tienen una capacidad cinco veces mayor que la de los camiones cisterna que trasportan helio gaseoso.

En 2008, alrededor de 169 millones de metros cúbicos estándar (SCM, por sus siglas en inglés, definidos como un metro cúbico a una presión de 1 atm y a una temperatura de 15 °C) de helio se extrajeron a partir del gas natural o de reservas de helio. De estos, aproximadamente el 78 % provinieron de los Estados Unidos, el 10 % de Argelia, y del resto la mayor parte fueron extraídos en Rusia, Polonia y Catar. En los Estados Unidos, la mayor parte del helio se extrae a partir del gas natural de los campos de Hugoton y otros cercanos en Kansas, Oklahoma y Texas. En 2000, los Estados Unidos tenían reservas de helio en complejos de pozos, de alrededor de 4,2×formula_1 SCM. Esta cantidad es suficiente para unos 25 años de uso mundial, o de 35 años de consumo de Estados Unidos, aunque se espera que factores en el ahorro y el procesamiento impacten los números efectivos de las reservas. Se estima que las reservas básicas de helio aún no probadas que se pudieran obtener a partir de gas natural en los Estados Unidos son de 3,1 a 5,3×10 SCM, o aproximadamente cuatro órdenes de magnitud mayor que las reservas probadas.

El helio se debe extraer principalmente del gas natural, debido a que su presencia en el aire es solo una fracción comparada con la de la del neón, y sin embargo, su demanda es mucho mayor. Se estima que si toda la producción de neón se reinstrumentara para ahorrar helio, se satisfarían un 0,1 % de las demandas mundiales de helio. Igualmente, solamente un 1 % de las demandas mundiales de helio se podrían satisfacer reinstrumentando todas las plantas de destilación de aire. El helio puede ser sintetizado por medio del bombardeo de litio o boro utilizando protones de alta velocidad. Sin embargo, este método de producción es totalmente inviable económicamente.

Las reservas actuales de helio se están utilizando mucho más rápido de lo que este elemento se puede reponer. Dada esta situación, hay grandes preocupaciones de que el suministro de helio pueda agotarse pronto. En las reservas más grandes del mundo, en Amarillo, Texas, se espera que este gas se agote en ocho años contando desde 2008. Esto podría prevenirse si los actuales usuarios capturasen y reciclasen el gas y si las compañías de petróleo y gas natural hiciesen uso de técnicas de captura de helio al extraerlos.

El helio es más ligero que el aire y a diferencia del hidrógeno no es inflamable, siendo además su poder ascensional un 8 % menor que el de este, por lo que se emplea como gas de relleno en globos y zepelines publicitarios, de investigación atmosférica e incluso para realizar reconocimientos militares.

Aún siendo la anterior la principal, el helio tiene más aplicaciones:

De la producción mundial total de helio en 2008, de 32 millones de kg, su mayor uso (alrededor del 22 % del total en 2008) fue en aplicaciones criogénicas. De estas la mayoría fueron en medicina en el enfriamiento de imanes superconductores en escáneres de resonancia magnética. Otros usos importantes (un total de cerca de 78 % de su uso en 1996) fueron en los sistemas de presurización y saneamiento, el mantenimiento de atmósferas controladas y la soldadura.

El helio se utiliza para muchos propósitos que requieren algunas de sus propiedades únicas, tales como su bajo punto de ebullición, baja densidad, baja solubilidad, alta conductividad térmica, o su baja reactividad química. Asimismo, está disponible comercialmente tanto en forma líquida como gaseosa. Como líquido, puede ser suministrado en recipientes pequeños llamados frascos de Dewar que permiten almacenar hasta 1.000 litros de helio, o en los contenedores ISO de gran tamaño que tienen una capacidad nominal de hasta 42 m³. En forma gaseosa, se suministran pequeñas cantidades en cilindros de alta presión que pueden contener un volumen equivalente a 8 m³ estándar, mientras que grandes cantidades de gas a alta presión son suministradas en camiones cisterna que tienen una capacidad que equivale 4.860 m³ estándar. Esto es debido a que el volumen del gas se reduce enormemente al ser sometido a altas presiones.


Debido a que el helio es más ligero que el aire, los dirigibles y globos son inflados con este gas para elevarlos. Mientras que el hidrógeno experimenta una fuerza de empuje aproximadamente un 7 % mayor, el helio tiene la ventaja de no ser inflamable (además de ser retardante del fuego). En la industria espacial, se utiliza como un medio de llenado para desplazar a los combustibles y oxidantes en los tanques de almacenamiento, y para condensar el hidrógeno y el oxígeno a fin de producir combustible para cohetes. También se utiliza para depurar el combustible y el oxidante de los equipos de apoyo en tierra antes del lanzamiento, así como para preenfriar el hidrógeno líquido en vehículos espaciales. Por ejemplo, el propulsor del Saturno V utilizado en el Programa Apolo necesitó cerca de 370.000 m³ de helio para poner en marcha el cohete.


El helio es menos denso que el aire atmosférico, por lo que cambia el timbre (mas no la altura) de la voz de una persona cuando se inhala. Esto se debe a que, al ser el helio un gas bastante ligero, se moviliza más rápido por los espacios, produciendo que las cuerdas vocales se muevan a mayor velocidad, provocando una onda sonora más veloz, y por tanto, más aguda. Sin embargo, la inhalación proveniente de una fuente comercial típica, como las utilizadas para rellenar globos, puede ser peligrosa debido al riesgo de asfixia por falta de oxígeno y al número de contaminantes que pueden estar presentes. Entre estos pueden estar incluidas trazas de otros gases, además de aceite lubricante en aerosol. No obstante, al tratarse de productos infantiles, existen mecanismos que exigen garantizar la no toxicidad del gas, como superar la "Conformidad Europea" (marcado CE) obligatorio en juguetes y derivados similares en el mercado europeo, para garantizar la seguridad del público infantil.

Por su baja solubilidad en el tejido nervioso, las mezclas de helio, como trimix, heliox y Heliair se utilizan para el buceo de profundidad para reducir los efectos de la narcosis. A profundidades por debajo de 150 metros, se agregan pequeñas cantidades de hidrógeno a la mezcla de helio-oxígeno para contrarrestar los efectos del síndrome nervioso de alta presión. A estas profundidades se ha descubierto que la baja densidad del helio reduce considerablemente el esfuerzo en la respiración.

Los láseres de helio-neón tienen varias aplicaciones, incluyendo lectores de código de barras.


Una de las aplicaciones industriales del helio es la detección de fugas. Debido a que se difunde a través de sólidos a una tasa tres veces mayor que la del aire, se utiliza como gas indicador para detectar fugas en el equipo de alto vacío y recipientes a alta presión.

La tasa de fugas en recipientes industriales (generalmente cámaras de vacío y tanques criogénicos) se mide haciendo uso del helio, debido a su diámetro molecular pequeño y a su condición de gas inerte. Todavía no se conoce otra sustancia inerte que se pueda filtrar a través de microfisuras o microporos en la pared de un contenedor a un ritmo mayor que el helio. Para encontrar fugas en contenedores se utiliza un detector de fugas de helio (véase espectrómetro de masas). Las fugas de helio a través de grietas no deben confundirse con la penetración de gas a través de un material masivo. A pesar de que se han documentado constantes de permeabilidad para el helio a través de vidrios, cerámicas y materiales sintéticos, los gases inertes como el helio no se pueden permear a través de la mayoría de los metales masivos. Si se necesita conocer la tasa de fuga total del producto que se está probando (por ejemplo en bombas de calor o sistemas de aire acondicionado), el objeto se coloca en una cámara de prueba, el aire dentro de ella se extrae con bombas de vacío y el producto es rellenado con helio a una presión específica. El helio que se escapa a través de las fugas es detectado por un espectrómetro de masas aún a tasas de fuga de hasta 10formula_2 Pa·m³/s. El procedimiento de medición es normalmente automático, y se conoce «como prueba integral de helio». En una prueba más sencilla, el producto se llena de helio y un operador busca manualmente la fuga con un dispositivo llamado "sniffer" (del inglés «olfateador»).


Por su ausencia de reactividad y alta conductividad térmica, su transparencia a los neutrones, y debido a que no forma isótopos radiactivos en condiciones de reactor, se utiliza como medio de transmisión de calor en algunos reactores nucleares enfriados por gas. Otra de sus utilidades consiste en usarlo como gas de protección en los procesos de soldadura por arco en materiales que se contaminan con facilidad por vía aérea.

Debido a que es inerte, se utiliza como gas protector en el crecimiento de cristales de silicio y germanio en la producción de titanio y circonio, además de en la cromatografía de gases. Por esta misma razón, por su conductividad térmica y por la alta velocidad del sonido dentro de él, su naturaleza como gas ideal y el alto valor de su coeficiente de expansión adiabática, también es útil en túneles de viento supersónicos y en instalaciones de prueba donde se requiere una liberación súbita de la energía del gas.

El helio, mezclado con un gas más pesado, como el xenón, es útil para la refrigeración termoacústica debido al elevado coeficiente de expansión adiabática resultante y su bajo número de Prandtl. El comportamiento inerte del helio tiene ventajas ambientales con respecto a los sistemas de refrigeración convencionales, que contribuyen al agotamiento de la capa de ozono o al calentamiento global.

El uso del helio reduce los efectos de distorsión que provocan las variaciones de temperatura en el espacio en las lentes de algunos telescopios, debido a su bajo índice de refracción. Este método es utilizado especialmente en telescopios solares, en los cuales un tubo de vacío fuertemente sellado resultaría demasiado pesado.

Mediante un proceso conocido como datación por helio, puede estimarse la edad de las rocas y minerales que contienen Uranio y Torio.

El helio líquido se utiliza para enfriar ciertos metales —por ejemplo, los imanes superconductores utilizados en la tomografía por resonancia magnética— a temperaturas extremadamente bajas, las cuales son necesarias para la superconductividad. El Gran Colisionador de Hadrones del CERN usa 96 toneladas de helio líquido para mantener la temperatura a 1,9 K. El helio a baja temperatura, también se usa en criogenia.

El helio es un gas portador comúnmente utilizado en la cromatografía de gases.

La primera evidencia de la existencia del helio se observó el 18 de agosto de 1868 como una línea brillante de color amarillo con una longitud de 587,49 nanómetros en el espectro de la cromosfera del Sol. La línea fue detectada por el astrónomo francés Pierre Janssen durante un eclipse solar total en Guntur, India. En un principio se pensó que esta línea era producida por el sodio. El 20 de octubre del mismo año, el astrónomo inglés Joseph Norman Lockyer observó una línea amarilla en el espectro solar, a la cual nombró como la línea de Fraunhofer D porque estaba cerca de las líneas de sodio D y D ya conocidas. Lockyer llegó a la conclusión de que dicha línea era causada por un elemento existente en el Sol pero desconocido en la Tierra. Eduard Frankland confirmó los resultados de Janssen y propuso el nombre "helium" para el nuevo elemento, en honor al dios griego del sol ( Ἥλιος, "Helios"), con el sufijo "-ium" ya que se esperaba que el nuevo elemento fuera metálico.

En 1882, el físico italiano Luigi Palmieri detectó helio en la Tierra por primera vez, a través de su línea espectral D, cuando analizó la lava del monte Vesubio.

El 26 de marzo de 1895 "Sir" William Ramsay aisló el helio al tratar la cleveíta (una variedad de la uranita que contiene por lo menos un 10 % de tierras raras) con ácidos minerales. Ramsey en realidad buscaba argón, pero después de separar el nitrógeno y el oxígeno del gas liberado por el ácido sulfúrico, notó una brillante línea amarilla que coincidía con la línea D observada en el espectro solar. Las muestras fueron identificadas como helio por Lockyer y el físico británico William Crookes. Además fue aislado de la cleveíta el mismo año independientemente por los químicos Per Teodor Cleve y Abraham Langlet en Upsala (Suecia), quienes pudieron obtener suficiente cantidad del gas para determinar acertadamente su peso atómico. El helio también fue aislado por el geoquímico estadounidense William Francis Hillebrand, aunque este atribuyó las líneas al nitrógeno.

En 1907 Ernest Rutherford y Thomas Royds demostraron que las partículas alfa son núcleos de helio, al permitir a las partículas penetrar una delgada pared de un tubo de vidrio al vacío y después creando una descarga eléctrica dentro del mismo para estudiar el espectro del gas. En 1908 el físico holandés Heike Kamerlingh Onnes produjo helio líquido por primera vez enfriando el gas hasta 0,9 K, lo que le hizo merecedor del premio Nobel. Él trató asimismo de solidificar el helio reduciendo su temperatura, aunque no lo logró debido a que este elemento carece de un punto triple, temperatura a la cual las fases sólida, líquida y gaseosa existen en equilibrio. En 1926 su discípulo Willem Hendrik Keesom logró por vez primera solidificar 1 cm³ helio.

En 1938, el físico ruso Pyotr Leonidovich Kapitsa descubrió que el helio-4 casi no tiene viscosidad a temperaturas cercanas al cero absoluto, un fenómeno que ahora se llama superfluidez. Este fenómeno está relacionado con la condensación de Bose-Einstein. En 1972, el mismo fenómeno se observó en el helio-3, pero a temperaturas mucho más cerca del cero absoluto, por los físicos estadounidenses Douglas D. Osheroff, David M. Lee y Robert C. Richardson. Se cree que en el helio-3 el fenómeno está relacionado con la creación de pares de fermiones de este isótopo, de tal manera que se forman bosones, en analogía a los pares de Cooper que producen la superconductividad.

Después de que una operación de perforación de petróleo en 1903 en Dexter, Kansas, produjera un géiser de gas que no se podía quemar, el geólogo Erasmus Haworth recogió muestras de los gases que emanaban y se las llevó a la Universidad de Kansas en Lawrence, donde, con la ayuda de los químicos Hamilton Cady y David McFarland, descubrió que el gas consistía, en volumen, de 72 % de nitrógeno, 15 % de metano (un porcentaje que se puede quemar únicamente con suficiente oxígeno), 1 % de hidrógeno, y 12 % de un gas no identificado. En un análisis posterior, Cady y McFarland descubrieron que el 1,84 % de la muestra de gas era helio. Esto demostró que a pesar de su rareza global en la Tierra, el helio estaba concentrado en grandes cantidades debajo de las Grandes Llanuras de Estados Unidos, disponible para su extracción como un subproducto del gas natural. Las mayores reservas de helio se encontraban en los campos de gas del suroeste de Kansas, de Texas y Oklahoma.

Esto permitió a los Estados Unidos convertirse en el principal productor de helio en el mundo. Siguiendo una sugerencia de Sir Richard Threlfall, la marina de este país patrocinó tres pequeñas plantas experimentales de producción de helio durante la Primera Guerra Mundial. El objetivo era proporcionar a los globos de defensa un gas no inflamable más ligero que el aire. Con este programa se produjeron un total de 5.700 m³ de helio al 92 %, a pesar de que previamente solo se había obtenido menos de un metro cúbico de gas. Parte de él se utilizó en la primera aeronave inflada con helio de la Marina estadounidense, que hizo su primer viaje de Hampton Roads, Virginia, a Bolling Field en Washington D. C., el 1 de diciembre de 1921.

Aunque el proceso de extracción usando licuefacción de gas a baja temperatura no se desarrolló a tiempo para ser relevante durante la Primera Guerra Mundial, la producción continuó. El helio se utilizó principalmente como un gas de elevación en aeronaves más ligeras que el aire. La demanda para este uso, así como para la soldadura por arco fue mayor durante la Segunda Guerra Mundial. El espectrómetro de masas de helio también fue vital en la bomba atómica desarrollada por el Proyecto Manhattan.

El gobierno de los Estados Unidos creó la Reserva Nacional de helio en 1925 en Amarillo, Texas, con el objetivo de suministrárselo a las aeronaves militares en tiempo de guerra, y a las aeronaves comerciales en tiempos de paz. Debido a un embargo militar de Estados Unidos contra Alemania en el que el suministro de helio quedó restringido, el LZ-129 "Hindenburg" se vio obligado a utilizar el hidrógeno como gas elevador. El uso de helio después de la Segunda Guerra Mundial se redujo, pero las reservas se ampliaron en la década de 1950 para garantizar su suministro en forma líquida como refrigerante para crear combustible de hidrógeno y oxígeno (entre otros usos) para los cohetes durante la carrera espacial y la Guerra Fría. El uso de helio en los Estados Unidos en 1965 fue de más de ocho veces el consumo máximo en tiempo de guerra.

La Oficina de Minas de Estados Unidos dispuso de cinco plantas privadas para recuperar helio a partir del gas natural. Para este programa de conservación de helio, la Oficina construyó 684 km de tuberías desde Bushton, Kansas para conectarlas con las plantas del Gobierno parcialmente agotadas en el campo de gas de Cliffside, cerca de Amarillo, Texas. Esta mezcla de helio y nitrógeno fue inyectada y almacenada en el campo de gas de Cliffside hasta que se necesitara, y hasta que fuera purificada posteriormente.

Para 1995 se habían almacenado cerca de mil millones de metros cúbicos de gas, y las reservas constituían una deuda de 1.400 millones de dólares, lo que en 1996 obligó al Congreso de los Estados Unidos a eliminarlas. El helio producido entre 1930 y 1945 tenía aproximadamente un 98,3 % de pureza (con un 2 % de nitrógeno), lo cual fue suficiente para llenar los dirigibles. En 1945, se usó una pequeña cantidad de helio a 99,9 %, para hacer soldaduras. Para 1949 había disponibles cantidades comerciales de helio grado A al 99,9 %.

Durante muchos años, los Estados Unidos han producido más del 90 % de helio que puede utilizarse comercialmente en el mundo, mientras que las plantas de extracción en Canadá, Polonia, Rusia y otros países producen el resto. A mediados de la década de 1990, una nueva planta en Arzew, Argelia entró en funcionamiento y produjo 17 millones de metros cúbicos de helio, con una producción suficiente para cubrir toda la demanda de Europa. Mientras tanto, en 2000, el consumo de helio dentro de los Estados Unidos había aumentado a más de 15 millones de kg por año. Entre 2004 y 2006, se construyeron dos plantas adicionales, una en Ras laffen, Catar y la otra en Skikda, Argelia. Sin embargo a principios de 2007, Ras laffen estaba funcionando al 50 %, y Skikda aún no había sido puesta en marcha. Argelia se convirtió rápidamente en el segundo principal productor de helio. A través de este tiempo, tanto el consumo de helio, como los costos de producción de helio aumentaron. Entre 2002 y 2007 el precio del helio se duplicó, y solo en 2008 los principales proveedores aumentaron sus precios en un 50 %.

El helio neutro en condiciones normales no es tóxico, no juega ningún papel biológico y se encuentra en trazas en la sangre humana. Si se inhala suficiente helio de forma tal que remplace al oxígeno necesario para la respiración, puede generar asfixia. Las precauciones que se deben de tomar para el helio usado en criogenia son similares a las del nitrógeno líquido. Su temperatura extremadamente baja puede causar quemaduras por congelación y la tasa de expansión de líquido a gas puede causar explosiones si no se utilizan mecanismos de liberación de presión.

Los depósitos de helio gaseoso a temperaturas de 5 a 10 K deben almacenarse como si contuvieran helio líquido debido al gran incremento de presión y a la significativa dilatación térmica que se produce al calentar el gas desde una temperatura a menos de 10 K hasta temperatura ambiente.

La velocidad del sonido en el helio es casi tres veces la velocidad del sonido en el aire. Debido a la frecuencia fundamental de una cavidad llena de gas es proporcional a la velocidad del sonido en el gas. Si se inhala helio se produce un aumento correspondiente en las alturas de las frecuencias de resonancia de las cuerdas vocales. (El efecto contrario, la reducción de frecuencias, se puede obtener por la inhalación de un gas denso como el hexafluoruro de azufre).

Su inhalación puede ser peligrosa si se hace en exceso, ya que es un gas asfixiante y desplaza al oxígeno necesario para la respiración normal. La respiración de helio puro continua, causa la muerte por asfixia en pocos minutos. La inhalación de helio directamente de cilindros a presión es extremadamente peligrosa, ya que la alta velocidad de flujo puede resultar en la ruptura de los tejidos pulmonares. Sin embargo, la muerte causada por el helio es muy rara, en los Estados Unidos solo se registraron dos fallecimientos entre 2000 y 2004.

A altas presiones (más de 20 atm o dos MPa), una mezcla de helio y oxígeno (heliox) puede conducir al síndrome de alta presión nerviosa; una especie de efecto anestésico inverso. Añadiendo una pequeña cantidad de nitrógeno a la mezcla puede resolverse el problema.




</doc>
<doc id="22196" url="https://es.wikipedia.org/wiki?curid=22196" title="Vivienda">
Vivienda

La vivienda es una edificación cuya principal función es ofrecer refugio y habitación a las personas, protegiéndolas de las inclemencias climáticas y de otras amenazas. Otras denominaciones de vivienda son: apartamento, aposento, casa, domicilio, estancia, hogar, lar, mansión, morada, piso, etc.

El derecho a la vivienda digna se considera uno de los derechos humanos fundamentales.

El ser humano siempre ha tenido la necesidad de refugiarse para contrarrestar las condiciones adversas de vivir a la intemperie. En la prehistoria, para protegerse del clima adverso o las fieras, solía refugiarse en cuevas naturales, con su familia, bien sea nuclear o extendida. Tradicionalmente, en el mundo rural eran los propios usuarios los responsables de construir su vivienda, según sus propias necesidades y usos a partir de los modelos habituales de su entorno y de los materiales disponibles en la zona; por el contrario, en las ciudades, era más habitual que las viviendas fueran construidas por artesanos o arquitectos especializados. En los países desarrollados, el diseño de las viviendas ha pasado a ser competencia exclusiva de arquitectos e ingenieros, mientras que su construcción es realizada por empresas y profesionales específicos, bajo la dirección técnica del arquitecto y/u otros técnicos.

El "Derecho universal a la vivienda", digna y adecuada, como uno de los derechos humanos, aparece recogido en la Declaración Universal de los Derechos Humanos en su artículo 25, apartado 1 y en el artículo 11 de Pacto Internacional de Derechos Económicos, Sociales y Culturales (PIDESC):

La "Vivienda digna", según la Oficina del Alto Comisionado de las Naciones Unidas para los Derechos Humanos en su Observación General n.º 4 es aquella vivienda donde los ciudadanos o familias pueden vivir con seguridad, paz y dignidad. La vivienda digna se inscribe en el derecho a la vivienda.

Una vivienda digna y adecuada debe ubicarse en espacios suficientemente salubres y equipados, en barrios urbanos o localidades rurales dotados de servicios, accesibles, con espacios intermedios de relación que permita la comunicación vecinal y social y donde sea posible el desarrollo familiar y personal que las sociedades demandan.

Para que una vivienda sea digna y adecuada, además debe ser: 1) Vivienda fija y habitable, 2) Vivienda de calidad, 3) Vivienda asequible y accesible y 4) Con seguridad jurídica de tenencia.

Las viviendas desocupadas, deshabitadas o vacías que están en condiciones de habitalidad, que no deben confundirse con las segundas viviendas que se ocupan por cortos períodos de tiempo, constituyen, desde el punto de vista del acceso a la vivienda, un problema social que cuestiona las políticas de viviendas de los distintos países. Se considera una pérdida de recursos y una mala gestión del parque inmobiliario la coexistencia de un número importante de viviendas vacías junto con la demanda insatisfecha de vivienda. Aunque desde el punto de vista del propietario puede no considerarse un problema, si puede asumir fácilmente los costes derivados del mantenimiento de la vivienda sin ocupar, desde el punto de vista social, una vivienda vacía es una patología urbana.

Las políticas públicas en relación con las viviendas deshabitadas son de muy distinto tipo aunque en general se tiende a penalizar, en muy distinta medida, la inacción del propietario -sea una banco, un grupo financiero o inversor, una inmobiliaria o un particular- para dar uso a dicha vivienda. Las medidas van desde la expropiación total o temporal hasta el incremento de diferentes tasas e impuestos.

La primera función de la vivienda es proporcionar un espacio seguro y confortable para resguardarse. El clima condiciona en gran medida tanto la forma de la vivienda como los materiales con que se construye, incluso las funciones que se desarrollan en su interior. Los climas más severos exigen un mayor aislamiento del ambiente exterior mientras que, por otra parte, se tiende a realizar el mayor número posible de actividades en el entorno controlado y confortable de la vivienda; por el contrario, en climas más benignos las exigencias de climatización son mucho más reducidas y, además, gran parte de las actividades cotidianas se realizan fuera de la vivienda.

Generalmente se suele admitir que cada vivienda es ocupada por una familia, pero esta idea debe matizarse: hay distintos tipos de familia y hay viviendas que son ocupadas por varias familias. En el mundo desarrollado se habla de "vivienda colectiva", frente a "vivienda unifamiliar", para referirse a edificios que albergan varias viviendas, cada una de las cuales es habitada por una única familia. Hoy por hoy, y debido a la situación económica, existen las denominadas "viviendas compartidas", que son utilizadas de forma comunitaria por varias personas sin ninguna clase de relación familiar.

Otro aspecto reseñable, ya que condiciona en gran medida las diversas formas de la vivienda en las diferentes culturas, es el conjunto de funciones que se desarrollan en su interior o aledaños. Tareas como la preparación y el cocinado de los alimentos, el lavado de la ropa, el aseo personal o el cuidado de niños y enfermos, y la forma y los medios que se emplean para realizarlas condicionan en gran medida la vivienda. En muchas viviendas, gran parte de estas funciones se han mecanizado mediante los denominados electrodomésticos, de forma que se ha sustituido por consumo energético la necesidad de espacios amplios y la dedicación exclusiva de una o varias personas a estas tareas domésticas. El último paso en esta tendencia lo constituye la domótica que pretende automatizar el mayor número de elementos de la vivienda.


En España es el alojamiento de carácter permanente destinado a satisfacer de manera habitual las necesidades vitales de habitación de una o varias personas. En relación con este concepto, se entiende por:




</doc>
<doc id="22215" url="https://es.wikipedia.org/wiki?curid=22215" title="Horas desesperadas">
Horas desesperadas

Horas desesperadas es una película estadounidense dirigida por William Wyler
Narra la historia de tres convictos escapados de prisión que se esconden a las afueras de la ciudad en un chalet de familia burguesa tomando como rehenes a sus miembros, que vivirán un auténtico horror sin que la policía lo sepa.

Basada en una obra de éxito de Broadway (en los escenarios la interpretaba Paul Newman). En 1990 el director Michael Cimino hizo un "remake" con Anthony Hopkins y Mickey Rourke.


</doc>
<doc id="22225" url="https://es.wikipedia.org/wiki?curid=22225" title="La senda tenebrosa">
La senda tenebrosa

La senda tenebrosa (título original: "Dark Passage") es una película de 1947 dirigida por Delmer Daves.

Humphrey Bogart interpreta a un hombre que ha sido encarcelado injustamente por el supuesto asesinato de su mujer. Escapa de la prisión, y decide cambiar sus rasgos mientras intenta demostrar su inocencia. Una atractiva desconocida (Lauren Bacall) le presta ayuda, porque su padre también fue víctima de un error judicial.



</doc>
<doc id="22227" url="https://es.wikipedia.org/wiki?curid=22227" title="Marea">
Marea

La marea es el cambio periódico del nivel del mar producido principalmente por las fuerzas de atracción gravitatoria que ejercen el Sol y la Luna sobre la Tierra. Aunque dicha atracción se ejerce sobre todo el planeta, tanto en su parte sólida como líquida y gaseosa, nos referiremos en este artículo a la atracción de la Luna y el Sol, juntos o por separado, sobre las aguas de los mares y océanos. Sin embargo, hay que indicar que las mareas de la litosfera son prácticamente insignificantes, con respecto a las que ocurren en el mar u océano (que pueden modificar su nivel en varios metros) y, sobre todo, en la atmósfera, donde puede variar en varios km de altura, aunque en este caso, es mucho mayor el aumento del espesor de la atmósfera producido por la fuerza centrífuga del movimiento de rotación en la zona ecuatorial (donde el espesor de la atmósfera es mucho mayor) que la modificación introducida por las mareas en dicha zona ecuatorial. 

Otros fenómenos ocasionales, como los vientos, las lluvias, el desborde de ríos y los tsunamis provocan variaciones del nivel del mar, también ocasionales, pero no pueden ser calificados de mareas, porque no están causados por la fuerza gravitatoria ni tienen periodicidad.

El fenómeno de las mareas es conocido desde la antigüedad. Parece ser que Piteas (siglo IV a. C.) fue el primero en señalar la relación entre la amplitud de la marea y las fases de la Luna, así como su periodicidad. Plinio el Viejo (23-79) en su "Naturalis Historia" describe correctamente el fenómeno y piensa que la marea está relacionada con la Luna y el Sol. Mucho más tarde, Bacon, Kepler y otros trataron de explicar ese fenómeno, admitiendo la atracción de la Luna y del Sol. Pero fue Isaac Newton en su obra "Philosophiae Naturalis Principia Mathematica" («Principios matemáticos de la Filosofía Natural», 1687) quien dio la explicación de las mareas aceptada actualmente. Más tarde, Pierre-Simon Laplace (1749-1827) y otros científicos ampliaron el estudio de las mareas desde un punto de vista dinámico.

A continuación se recogen los principales términos empleados en la descripción de las mareas:
El tiempo aproximado entre una pleamar y la bajamar es de 6 horas, completando un ciclo de 24 horas 50 minutos.

La explicación completa del mecanismo de las mareas, con todas las periodicidades, es extremamente larga y complicada. Así que se comenzará empleando todas las simplificaciones posibles para luego acercarse a la realidad suprimiendo algunas de estas simplificaciones.

Se considerará que la Tierra es una esfera sin continentes rodeada por una hidrosfera y que gira alrededor del Sol en una trayectoria elíptica sin girar sobre su eje. Por ahora no se tendrá en cuenta la Luna.

Cuando un astro está en órbita alrededor de otro, la fuerza de atracción gravitacional entre los dos viene dada por la ley de gravitación de Newton:
donde:
Esta fuerza de atracción es la fuerza centrípeta que hace que el astro describa una circunferencia. 
donde:

El valor de la aceleración de gravedad debida al Sol es exactamente el que corresponde a una órbita con la velocidad angular formula_13 y con el centro de masas terrestre a una distancia formula_5 del Sol. Todas las partes de la Tierra tienen la misma velocidad angular alrededor del Sol, pero no están a la misma distancia. Las que están más lejos del centro de masas estarán sometidas a una aceleración de gravedad menor y la que están a una distancia inferior, a una aceleración mayor.

Existe otra fuerza, del mismo orden de magnitud, debida al hecho que las fuerzas de atracción convergen hacia el centro del Sol, que se encuentra situado a una distancia finita. Se describirá más adelante.

En algunas fuentes se comete el error de añadir las aceleraciones centrífugas. Si se opta por utilizar un sistema de referencia inercial (inmóvil respecto a la estrellas), no se deben tener en cuenta las fuerzas centrífugas, que son fuerzas ficticias y que sólo aparecen en sistemas de referencia acelerados. Un observador en la Tierra ve fuerzas centrífugas porque la Tierra está en caída libre hacia el Sol. En cambio, para un observador exterior fijo, solo existen las fuerzas reales, como la fuerza de atracción que constituye la fuerza centrípeta.

El resultado de este pequeño desequilibrio de fuerzas es que el agua de los océanos situada en el lado opuesto al Sol siente una fuerza que la empuja hacia el exterior de la órbita, mientras que el agua situada en el lado orientado hacia el Sol siente una fuerza que la empuja hacia dicho astro. La consecuencia es que la esfera de agua que recubre a la Tierra se alarga ligeramente y se transforma en un elipsoide de revolución cuyo eje mayor está dirigido hacia el Sol. Se verá que este alargamiento relativo es muy pequeño: del orden de uno entre diez millones.

Para calcular la amplitud de las mareas solares, se construyen dos pozos imaginarios desde la superficie hasta el centro de la Tierra. Uno es paralelo a la recta que une la Tierra y el Sol y el otro es perpendicular.

La fuerza y la aceleración que siente el agua en el pozo perpendicular son casi paralelas al eje Tierra-Sol, pero no exactamente. La razón es que el Sol está a una distancia finita y las fuerzas están dirigidas hacia el centro del Sol y no son totalmente paralelas. Calculemos la componente de la aceleración de gravedad perpendicular al eje Tierra-Sol, formula_15, que experimenta el agua situada a una distancia formula_16 del centro de la Tierra. Sin más que proyectar el vector de aceleración, se llega a que: 
Aquí, formula_18 es la aceleración debida a la atracción del Sol:
En esta última fórmula, formula_20 es la masa del Sol y formula_21 es la distancia de la Tierra al Sol. Por su parte, la componente perpendicular al eje queda:
Esta aceleración varía linealmente entre el centro de la Tierra y la superficie. El valor medio se obtiene reemplazando formula_16 por formula_24, donde formula_25 es el radio de la Tierra. Esta aceleración añade un "peso" adicional a la columna de agua del pozo y hace que la presión en el fondo aumente una cantidad formula_26, donde formula_27 es la densidad del agua. Este aumento de la presión, transmitido a la superficie del océano, se corresponde con una variación formula_28 del nivel del océano dada por la fórmula formula_29 (donde formula_30 es la aceleración de gravedad terrestre):
El cálculo numérico da una variación de 8,14 cm.

Se pasará ahora a calcular la disminución formula_32 de la aceleración de gravedad ocasionada por el Sol en un punto situado a una distancia formula_16 del centro de la Tierra. Añadiendo esta distancia adicional en la fórmula de la aceleración gravitatoria:
El primer sumando se corresponde con la aceleración para un cuerpo situado a una distancia formula_35. Por tanto, la "disminución" de la aceleración es:
A su vez, la aceleración media es:
La variación de presión es, como en el caso anterior, formula_38, por lo que:
Esta aceleración da un aumento de la altura del océano de 16,28 cm.

Con la suma de los dos efectos, el semieje mayor del elipsoide es 24,4 cm mayor que el semieje menor. Como la Tierra gira, un punto situado en el ecuador ve la altura del mar llegar a un máximo (pleamar) dos veces por día: cada vez que dicho punto pasa por el semieje mayor. De la misma manera, cada vez que el punto pasa por un semieje menor, la altura del mar pasa por un mínimo (bajamar). La diferencia entre la pleamar y la bajamar es de 24,4 cm. Pero no hay que olvidar que esto sólo es la parte debida al Sol, que no hay continentes y que no se ha tenido en cuenta la inclinación del eje de rotación de la Tierra. La variación de la altura del mar se puede aproximar por una sinusoide con un período de 12 horas.

La Luna gira alrededor de la Tierra, pero esta última no está inmóvil. En realidad, tanto la Luna como la Tierra giran alrededor del centro de masas de los dos astros. Este punto se sitúa aproximadamente a 4.670 km del centro de la Tierra, medido en el lugar de la superficie terrestre que se desplaza de oeste a este con el movimiento de traslación lunar, donde la atracción de nuestro satélite es mayor en un momento dado. Como el radio medio de la Tierra es de 6.367,5 km, el centro de masas se encuentra a unos 1.700 km de profundidad bajo su superficie. La Luna tiene una masa formula_40 kg y está a una distancia media de la Tierra de formula_41 m.
El cálculo de las mareas lunares es similar al cálculo de las mareas solares. Basta con reemplazar la masa y la distancia del Sol por las de la Luna. La diferencia de altura del océano debida al no paralelismo de las fuerzas es:
El cálculo numérico nos da una variación de 17,9 cm.

La diferencia de altura del océano provocada por la diferencia de atracción debida a las distancias diferentes respecto a la Luna es:
El cálculo numérico nos da una variación de 35,6 cm.

La diferencia de longitud entre el semieje mayor y el semieje menor del elipsoide debido a las mareas lunares de 35,6 cm. Por tanto, la amplitud de las mareas lunares es, aproximadamente, dos veces mayor que las de las mareas solares. Como para las mareas solares, la variación de la altura del mar en un punto de la superficie terrestre se puede aproximar por una sinusoide. Esta vez, el período es 12 horas, 25 minutos y 10 s.

El elipsoide debido a las mareas solares tiene el eje mayor dirigido hacia el Sol. El elipsoide debido a las mareas lunares tiene el eje mayor dirigido hacia la Luna. Como la Luna gira alrededor de la Tierra, los ejes mayores de los elipsoides no giran a la misma velocidad. Con respecto a las estrellas, el periodo de rotación del elipsoide solar es de un año. El elipsoide de la Luna es de 27,32 días. El resultado es que los ejes de los dos elipsoides se acercan cada 14,7652944 días. Cuando los ejes mayores de los dos elipsoides están alineados, la amplitud de las mareas es máxima y se llaman mareas vivas o mareas sizigias. Esto sucede en las lunas nuevas y en las lunas llenas. En cambio, cuando el eje mayor de cada elipsoide está alineado con el eje menor del otro, la amplitud de las mareas es mínima. Esto sucede en los cuartos menguantes y los cuartos crecientes. Estas mareas se llaman mareas muertas o mareas de cuadratura.

Hasta ahora se ha ignorado el hecho de que el eje de rotación de la Tierra está inclinado unos 23,27° con respeto a la eclíptica (el plano que contiene la órbita de la Tierra y el Sol). Además, el plano de la órbita de la Luna está inclinado unos 5,145° con respecto a la eclíptica. Esto significa que el Sol ocupa posiciones que van desde 23,44° al norte del plano ecuatorial hasta 23,44° al sur del mismo plano. La Luna puede ocupar posiciones desde 28,6° hasta -28,6°. La consecuencia de esto es que los ejes mayores de los elipsoides que se han utilizado raramente coinciden con el plano del ecuador terrestre.

En la imagen de la derecha, el punto A está en pleamar. Cuando se produzca la próxima pleamar, 12 horas, 25 min y 10 segundos más tarde, el mismo punto se encontrará en B. Esta pleamar será menor que la precedente y que la posterior.

Esta alternancia diurna entre pleamares grandes y pequeñas hace pensar en la suma de dos periodicidades: una diurna y otra semidiurna. Se habla entonces de ondas de marea diurna y semidiurna, tanto lunar como solar. Esto se corresponde con un modelo matemático y no con la realidad física.

Nótese que el punto u y las localizaciones situadas más al norte, solo ven una pleamar por día. Cuando deberían estar en la pequeña pleamar, están aún en el mismo lado del elipsoide. Una situación similar se produce en el Hemisferio Sur. Matemáticamente, la amplitud de la onda semidiurna es demasiado pequeña para que pueda crear máximos o mínimos adicionales.

Las mareas son máximas cuando las dos pleamares son iguales. Eso solo ocurre cuando el eje mayor de los elipsoides es paralelo al plano ecuatorial. Es decir, cuando el sol se encuentra en el plano ecuatorial. Esto ocurre durante los equinoccios. Las mareas de equinoccio son las mayores del año.

Varios factores adicionales también contribuyen a la amplitud de la marea:

En el cálculo simplificado que se ha realizado, en el cual la Tierra no tiene continentes y está recubierta de una hidrosfera continua, la distancia entre las dos posiciones de pleamar es de 20.000 km. La zona de océano cuyo nivel es más alto que el valor medio tiene un diámetro de 10.000 km. Esa distancia es mayor que la distancia entre América y Europa o África y se corresponde con el ancho del Océano Pacífico. Para que todo un océano como el Atlántico o el Pacífico aumentasen de nivel, su contenido total de agua tendría que aumentar. Como los continentes impiden ese movimiento lateral de todo el océano, el modelo de la onda semidiurna no se corresponde con la realidad.

En la imagen de la derecha se puede ver que la altura de los océanos no sigue una onda que se desplaza de derecha a izquierda (hacia el Oeste). El desplazamiento del agua y de los máximos y mínimos es mucho más complicado.

En un modelo sin continentes, las líneas cotidales coinciden con los meridianos. En la imagen de la derecha en color están representadas las líneas cotidales del planisferio y el color del fondo corresponde a la amplitud de mareas. Estas líneas cotidales se corresponden con una situación astronómica particular (Luna creciente, equinoccios, etc.) y cambian con el tiempo. En las dos imágenes se observa que hay líneas cotidales que convergen hacia puntos anfidrómicos, en los cuales la amplitud de la marea es igual a cero.

La situación es aún más marcada en los mares interiores, cuyas dimensiones son aún menores que las de los océanos. Así, el Atlántico no puede llenar o vaciar el Mar Mediterráneo a través el estrecho de Gibraltar. Las aguas del Mediterráneo solo pueden desplazarse hacia el Este o hacia el Oeste, subiendo en un extremo y bajando en el otro. El resultado final se complica por la forma de las costas que limitan y desvían ese movimiento lateral.

En mayor o menor grado,todos los mares interiores y los mares abiertos (aunque en menor grado) presentan un movimiento circular, tanto en las corrientes marinas como en las corrientes de marea y estas corrientes pueden girar en sentido horario en las latitudes intertropicales del hemisferio norte y en sentido antihorario en la zona templada del hemisferio norte. En el caso del hemisferio sur se invierten dichos movimientos giratorios aunque no podemos hablar en este caso de mares, pero es la misma situación con porciones latitudinales de los propios océanos.

Como se ha visto, la amplitud de las mareas en alta mar es menor que 1 metro. En cambio, cerca de las costas la amplitud es generalmente mayor y en algunos casos alcanza o sobrepasa los 10 metros. En la tabla siguiente figuran algunos de los lugares donde se producen grandes mareas. Se ha puesto un solo lugar por zona.

Se explica ahora cómo una marea de menos de un metro en alta mar puede crear una marea de varios metros en la costa. La razón es la resonancia de la capa de agua situada sobre la plataforma continental. Esta capa es poco profunda (menos de 200 m) y, en algunos casos, tiene una gran extensión hasta el talud continental. Por ejemplo, el Canal de la Mancha es una capa de agua de 500 km de largo (desde la entrada hasta el Paso de Calais), 150 km de ancho y solo 100 m de profundidad. A escala, eso se corresponde con una masa de agua de 50 metros de largo y de 1 cm de profundidad. Cuando el nivel del mar aumenta en la entrada, el agua entra en el Canal de la Mancha. Como la extensión es grande y la profundidad pequeña, la velocidad del agua aumenta hasta unos 4 a 5 nudos (2 a 2,5 m/s). Alcanzar esa velocidad toma su tiempo (unas tres horas en el caso del Canal de la Mancha), pero detenerse también requiere un período similar. Una vez lanzada, el agua continúa avanzando, transcurriendo otras tres horas hasta que se para e invierte su dirección. El comportamiento oscilatorio se debe a la inercia y al retardo que tiene la capa de agua para responder a la excitación: la variación de altura del océano más allá del talud continental. La marea será más grande en función de que el período de oscilación propio de la zona sea más próximo al periodo de la excitación externa, que es de 12 horas y 25 minutos.

En la imagen de la izquierda se pueden observar las líneas cotidales en el Canal de la Mancha. Los números de cada línea corresponden al retardo de pleamar con respecto a una referencia. Obsérvese que hay 6 horas de diferencia entre las pleamares de la entrada del Canal de la Mancha y el Paso de Calais. También hay seis horas entre la entrada de la Mancha y el Mar de Irlanda (entre Irlanda e Inglaterra). Hay un punto anfidrómico (en anaranjado) en la entrada del Mar del Norte, frente a Holanda.

El período de oscilación propio de la Bahía de Fundy en Canadá es de 13 horas. Como es muy próximo al período de excitación, las mareas son muy grandes. Por el contrario, cuando el período propio se aleja de las 12,4 h, las amplitudes de las mareas son menores. El período de oscilación propio depende de la forma de la costa y de la profundidad y longitud de la plataforma continental.

En las áreas próximas al ecuador terrestre, las mareas suelen ser muy débiles, casi imperceptibles, salvo en las desembocaduras de los ríos, donde el ascenso de las aguas marinas puede dar origen al represamiento de las aguas fluviales, produciéndose un oleaje río arriba cuando las crestas de la marea entrante rompen contra el agua de los ríos. Este oleaje produce un ruido característico que recibe el nombre de macareo en el delta del Orinoco y pororoca en el río Amazonas.

El motivo de la escasa amplitud de las mareas en la zona intertropical se debe a que es la zona donde los efectos del movimiento de la rotación terrestre son mayores por la fuerza centrífuga generada por dicho movimiento. Debido a la fuerza centrífuga, el nivel del mar es mucho mayor en el ecuador que en las zonas templadas y, sobre todo, en las polares.Como resulta obvio, la mayor altura de las aguas ecuatoriales por la fuerza centrífuga impide que las mareas sean claramente notorias ya que esa fuerza centrífuga se ejerce por igual en toda la circunferencia ecuatorial mientras que las mareas sólo aumentan ese nivel donde se encuentra el paso de la Luna y el Sol, y es un aumento de nivel mucho menor.

Como se ha dicho, la variación de nivel del mar sobre la plataforma continental exige un movimiento alternativo del agua hacia la costa y hacia el mar. Como la profundidad del agua no es la misma cuando la marea sube que cuando baja, la forma de los obstáculos no es la misma, y la dirección y la velocidad de la corriente tampoco es la misma. El vector velocidad dibuja una especie de elipsoide cuyo eje mayor es más o menos paralelo a la costa.

En sitios donde las mareas tienen gran amplitud, las velocidades del mar también pueden ser muy grandes. Por ejemplo, en el Canal de la Mancha, en el Raz de Sein (en el extremo oeste de Bretaña, en Francia) y en el (al norte de la península del Cotentín, también en Francia), la corriente sobrepasa los 10 nudos (18 km/h) durante las grandes mareas. En el estrecho de Mesina, la corriente puede llegar a 5 nudos.

La energía de las mareas ha sido utilizada desde la edad media en Inglaterra, Francia, España y probablemente otros países. Los molinos de mareas de esa época solo funcionaban en reflujo. Estos, como muchos otros molinos hidráulicos, dejaron de utilizarse con la aparición de motores eléctricos.

La instalación de una central mareomotriz crea problemas medioambientales importantes como aterramiento del río, cambios de salinidad en el estuario y sus proximidades y cambio del ecosistema antes y después de las instalaciones.

Las fuerzas de gravedad que provocan las mareas de los océanos también deforman la corteza terrestre. La deformación es importante y la amplitud de la marea terrestre llega a unos 25 a 30 cm en sizigia y casi 50 cm durante los equinoccios.
Al ser el aire atmosférico un fluido, como sucede con las aguas oceánicas, también las dimensiones de la atmósfera sufren la acción de las mareas, afectando su espesor y altura y, por consiguiente, la presión atmosférica. Así, la presión atmosférica disminuye considerablemente durante las fases de luna llena y luna nueva, al ser atraída la columna de aire por el paso, combinado o no, de la luna y el sol por el cenit y/o el nadir. Como hemos visto con las mareas oceánicas, el nivel del mar puede ascender o bajar varios metros cada día en los lugares más propicios (estuarios o bahías). Pero en el caso de la atmósfera su nivel puede ser modificado por la atracción de la luna y el sol en varios km. Hay que tener en cuenta, sin embargo, que la atmósfera tiene un mayor espesor en la zona ecuatorial en especial y en la zona intertropical en general, por la fuerza centrífuga del movimiento de rotación terrestre, por lo que la intensidad de las mareas vendría a superponerse a dicha fuerza centrífuga y, lo mismo que sucede con las mareas oceánicas en la zona intertropical, sus efectos no son tan notorios ya que quedan enmascarados por dicha fuerza centrífuga. Por otra parte, hay que tener en cuenta que el aumento del espesor de la atmósfera por la atracción solar y/o lunar contribuye a la disminución de la presión, a la disminución de la velocidad de los vientos (de ahí el término de calmas ecuatoriales que, aún siendo correcto, se ha venido quedando en desuso) y al aumento de la condensación y de las lluvias.

En la zona intertropical, los cambios de la presión atmosférica durante las mareas atmosféricas dan origen a notables cambios de temperatura que se notan con un simple termómetro y que no se explicarían de otra forma: en luna llena o luna nueva, por ejemplo, puede fácilmente subir un grado o más cerca del mediodía o de la medianoche y en este último caso no tendría explicación si no tuviéramos en cuenta el calentamiento por condensación al disminuir la presión del aire y elevarse. No sólo la presión atmosférica se modifica con las mareas atmosféricas, sino también la intensidad de las lluvias. Un estudio meteorológico del mes de octubre de 2012 nos mostraría una alta correlación entre las fases lunares con la mayor intensidad de los huracanes (Nadine, Rafael y Sandy) y/o su disipación. En este último caso, las graves inundaciones causadas por Sandy en New Jersey y Nueva York resultaron de la combinación de la intensa marea producida por la luna llena (el 29 de octubre) y el mar de leva producido por el propio huracán al entrar en la costa de dichos estados, factor explicado en un artículo del NHC (National Hurricane Center) cuya lectura es muy apropiada para la comprensión de este tema:

Tanto la deformación de la Tierra debida a las mareas terrestres como el movimiento del agua de las mareas acuáticas son procesos que disipan energía. El trabajo lo efectúa el momento que la Luna y Sol ejercen sobre la parte deformada de la Tierra y de los océanos. La disipación de energía exige que los ejes mayores de los elipsoides de la hidrosfera y de la Tierra no estén perfectamente alineados con la Luna y el Sol, sino que tengan un pequeño retardo de fase. En el modelo sin continentes, ese retardo correspondería a 3° (y a 12 minutos en tiempo). Ese momento frena la rotación de la Tierra y la duración del día aumenta 17 microsegundos por año (aproximadamente, 1 segundo cada 59.000 años).

La Tierra ejerce el mismo momento sobre la Luna que el que la Luna ejerce sobre la Tierra. El momento que la Tierra ejerce sobre la Luna le comunica energía. Como la Luna está en órbita alrededor de la Tierra, ese aumento de energía se traduce en un aumento de la distancia entre los dos astros y un aumento de la duración del mes lunar. La distancia Tierra-Luna aumenta unos 38 mm por año.

De la misma manera que la Luna crea mareas en la Tierra, tanto acuáticas como terrestres, la Tierra también ejerce mareas sobre la Luna. La fricción debida a esas mareas frenó la rotación de la Luna, provocando que ésta presente siempre la misma cara hacia la Tierra, aunque es justo señalar que este hecho se ha interpretado como el posible origen terrestre de nuestro satélite: siendo la Tierra aún un cuerpo semifluido o incandescente, el movimiento de rotación habría producido una protuberancia que iría aumentando de velocidad por el incremento de la fuerza centrífuga. Con el tiempo, se habrían separado los dos astros, manteniendo la misma cara lunar visible desde la Tierra. En otros satélites del sistema solar que aún giran, la energía disipada por las deformaciones debidas a la marea genera actividad volcánica.





</doc>
<doc id="22236" url="https://es.wikipedia.org/wiki?curid=22236" title="Jorge Sampaio">
Jorge Sampaio

Jorge Fernando Branco de Sampaio () (Lisboa, 18 de septiembre de 1939) fue presidente de Portugal entre 1996 y 2006.

Jorge Sampaio desciende de una familia judaica portuguesa y vivió algunos años de su juventud en Estados Unidos e Inglaterra, debido a la actividad profesional del padre, médico. Su padre, Arnaldo Sampaio, es un especialista en Salud Pública. Su madre, Fernanda Bensaúde Branco de Sampaio, fue profesora particular de lengua inglesa. 

Jorge Sampaio inició su carrera política cuando cursaba Derecho en la Universidad de Lisboa. Él mismo estuvo envuelto en la contestación al régimen fascista y fue líder de la asociación de estudiantes de Lisboa entre 1960 y 1961. Después de su graduación en 1961, Jorge Sampaio inició una carrera notable como abogado, muchas veces envuelto en la defensa de varios prisioneros políticos. 

Después de la Revolución de los Claveles del 25 de abril de 1974, Jorge Sampaio fue fundador del MES (Movimiento de Izquierda Socialista), pero abandonó el proyecto poco después. En 1978 se adhirió al PS, el Partido Socialista, donde permanece hasta hoy. Su primera elección como diputado por Lisboa en la Asamblea de la República fue en 1979. Entre este año y 1984, fue un miembro de la Comisión Europea para los Derechos Humanos, donde desempeñó un trabajo importante en esas materias. 

Entre 1986 y 1987 fue presidente del Grupo Parlamentario del Partido Socialista. En 1989 fue presidente electo del partido, un puesto que ejerció hasta 1991. También en 1989, Jorge Sampaio fue elegido alcalde de Lisboa y posteriormente fue reelegido en 1993.
En 1995, anunció el deseo de presentarse a la Presidencia de la República. Ganó la elección inmediatamente en la primera vuelta, contra Aníbal Cavaco Silva, el anterior primer ministro, y se hizo presidente el 14 de enero de 1996. Después de un primer mandato sin controversias, fue reelegido el 14 de enero de 2001.

Como presidente, su acción se centró en los aspectos sociales y culturales. En la escena política internacional, Sampaio fue un importante contribuidor para la toma de conciencia de la causa por la independencia de Timor Oriental. 

La presidencia de Jorge Sampaio se marcó siempre por un sentido firme de prudencia y moderación, un estilo que le aseguró un primer mandato sin controversias. En 2004, sin embargo, su decisión de no convocar elecciones anticipadas después de la dimisión del Primer Ministro conservador José Manuel Durão Barroso fue discutida por todos los partidos de izquierda y acabó por influir en la decisión de dimisión del líder del Partido Socialista Eduardo Ferro Rodrigues. 

Jorge Sampaio está casado y tiene dos hijos. El 8 de septiembre de 2000 el Gobierno de España le concedió el collar de la Orden de Carlos III

En 2007 fue nombrado por el Secretario General de las Naciones Unidas, Ban Ki-Moon, como Alto Representante de esta organización internacional para la Alianza de Civilizaciones.



</doc>
<doc id="22239" url="https://es.wikipedia.org/wiki?curid=22239" title="Caballo peruano de paso">
Caballo peruano de paso

El caballo peruano de paso es una raza equina oriunda del Perú, descendiente de los caballos introducidos durante la Conquista y los primeros tiempos de la Colonia. Esta raza está protegida por el Decreto Ley peruano número 25.919 del 28 de noviembre de 1992 y ha sido declarado "raza caballar propia del Perú." por el Instituto Nacional de Cultura, Así lo instituyó el Ministerio de Comercio Exterior y Turismo (Mincetur) y lo hizo público: su día se celebrará el tercer domingo de abril de cada año. (INC). y "producto de bandera" por el Ministerio de Comercio Exterior y de Turismo en abril de 2013.

Debido al aislamiento sufrido durante alrededor de 400 años y la selección que hicieron sus criadores, es una raza muy particular por sus proporciones corporales y por un andar lateral o "paso llano" que le es característico.
Es típico de las regiones del norte peruano, zona del país de donde se dio su origen (La Libertad, Lambayeque y Piura). 

Altura de la cruz: entre 144 y 154 centímetros; Su cuerpo es compacto y musculoso, ancho y profundo; Extremidades alargadas y fuertes; Su cabeza es plana y ancha con ojos brillantes y expresivos; Cuello robusto y musculoso; Su color predominante es el castaño, aunque suelen ser alazanes con capas mezcladas, sus extremidades pueden hasta llegar a medir 50 cm.

Lo que hace a este animal "diferente a otras razas equinas en el mundo es su aire típico de velocidad intermedia, que en los demás es de trote". Este aire "o modalidad" en el andar es el trote lateral o "ambladura" y se denomina paso llano en su ritmo más típico; pero puede tener diferentes ritmos y velocidades, que pueden a su vez ser ejecutados por un mismo ejemplar. 

A esta suma de aires se les llama pasos. Durante la ejecución de estos pasos finos, la cabalgadura tiene un solo y excepcionalmente suave balanceo horizontal; Las otras razas de caballos se balancean horizontal y verticalmente. Esto hace que el cabalgarlo sea especialmente agradable. La suavidad es una de las virtudes fundamentales y más apreciadas en la raza de este caballo. 

En la publicación "andar en Paso Llano", el criador Carlos Parodi García, habla sobre el "paso llano": Es mostrar el desplazamiento armónico isócrono innato de cada batida individual de las extremidades del caballo. El animal levanta la extremidad anterior y posterior del mismo lado, sitúa primero el posterior en el suelo y luego el anterior del mismo lado, igualmente lo hace con el otro bípedo (paso de bípedos laterales en 4 tiempos). Obviamente este movimiento armónico isócrono de batidas individuales va acompañado con los anteriormente enunciados cuando definimos lo que es el "Término". Es importante precisar que, en el tiempo armónico del desplazamiento el caballo peruano de paso llega a tener mayor número de extremidades en apoyo sobre el suelo, en consecuencia mejor impulsión y menor reacción en el momento de impulsión en el traslado del centro de gravedad. De lo cual se desprende las variaciones siguientes en los aires o modalidades del paso llano: Paso Llano Gateado, Paso Llano Picado, Paso Llano Golpeado. Gaitan

El "Término" es un atributo particular que conjuntamente y después de: "la suavidad y el avance", es el espectáculo original e inejecutable por otros caballos, en la observación o evaluación morfológica dinámica del caballo como individuo, en la mecánica de su andar racial. 

El caballo peruano de paso tiene como característica, mayor predominancia en movimientos armónicos isócronos de batidas en los miembros anteriores que en los posteriores.

En consecuencia La ejecución armónica, isócrona y de peculiar graciosidad de elevación, suspensión, rotación elegante fuera de la línea de aplomo, descenso y apoyo de cada batida isócrona, de extremidad anterior o delantera, se denomina "Término".

Además dependiendo de la elevación del brazo, rodilla y caña mostrará mayor o menor agudez en el término.

"La marcha difiere notablemente de los movimientos laterales de otras razas equinas".

Los ascendientes de estos ejemplares fueron embarcados en Sevilla, en Sanlúcar de Barrameda y en Cádiz en el siglo XVI, y por lógica se presume que fueron de raza andaluza.

La estabilización de la raza tomó cerca de cuatro siglos, de varias generaciones de cruces, selección y mejoramiento.
Ayudó bastante el ser un ejemplar de uso como herramienta de trabajo en la agricultura, transportando a los agricultores en la administración y manejo de los campos, principalmente en las haciendas de la costa norte del Perú. Y como animal de silla viajero, para trasportar al jinete de un poblado a otro; igualmente se utilizó en la época para el arreo del ganado de lidia desde las afueras de la capital hacia Lima.

La Asociación Nacional de Criadores y Propietarios de Caballo Peruano de Paso (ANCPCPP), es la única entidad reconocida oficialmente a nivel nacional e internacional, encargada de la conservación, fomento de la crianza, selección, juzgamiento del Caballo Peruano de Paso, así como, el cuidado y uso del apero y la enfrenadura tradicional que lo distinguen. También existen asociaciones departamentales o incluso de otros países que se encargan de la difusión de esta tradición. También pueden organizar concursos regionales, departamentales o nacionales, según el alcance de la asociación. Los concursos organizados por estas instituciones, deben tener el respaldo de la ANCPCPP en cuanto al juzgamiento (solo jueces oficiales, determinados por la misma) y al reglamento único de concursos del Caballo Peruano de Paso.

Desde el año 2008, la ANCPCPP viene editando un Boletín Electrónico cada 2 meses, el cual mantiene al tanto al mundo del acontecer nacional y mundial sobre el Caballo Peruano de Paso.

La Asociación Nacional de Criadores y Propietarios de Caballo Peruano de Paso, como única entidad rectora, promueve y oficializa los certámenes, seminarios, concursos y demás que se llevan a cabo en el Perú y en el extranjero, relacionados con la cría y difusión del Caballo Peruano de Paso. En La A.N.C.P.C.P.P. se tiene el Registro Genealógico. El Registro Genealógico, tiene como función el archivo de todos los equinos de raza, como un banco de germoplasma para el mejoramiento de esta raza; sin su inscripción no pueden participar en ningún evento oficial.
El Concejo Distrital del Rímac, en Lima, organizó el primer concurso en la pampa de Amancaes el 24 de junio de 1929. El evento se celebró posteriormente en este escenario hasta el año 1939. Se reanudaron luego en 1941 y 1942, con la variante de que el juzgamiento se hacía previamente en la limeña Plaza de Toros de Acho, de manera que en el día de San Juan desfilaran sólo los premiados y se exhibieran durante la fiesta.
Los Concursos Nacionales se llevan a cabo desde el año 1945, y éstos son organizados en el mes de abril principalmente.

Los concursos son en realidad una gran fiesta celebrada alrededor de este original equino, con asistencia de criadores y aficionados de todo el país y muchos del extranjero, donde también se los admira y cría. 

Por lo general la fiesta del caballo peruano de paso dura una semana y se realiza principalmente en Lurín. La final del concurso termina con una exhibición de los caballos favoritos, premios y con demostraciones de las destrezas de los caballos, una de las cuales consiste en que los chalanes hacen desfilar sus caballos al son de las danzas peruanas costeñas, particularmente, la marinera.

Los concursos o el concurso, que puede ser: nacional, departamental o zonal, dependiendo su importancia y localización así como agrupación de criadores o exhibición de ejemplares; es el evento con el que el criador-propietario, mide o compara sus ejemplares, con la finalidad de ver, corregir, y mejorar la reproducción de sus equinos dentro de su criadero. 

El "concurso nacional" donde participan equinos de paso de todo el Perú, se realiza en la ciudad de Lima en el local de la ANCPCPP que está en Lurín, al sur-oeste del conjunto arqueológico de Pachacámac.
Los concursos nacionales que se desarrollan en el país son: 

El apero nacional no es más que el conjunto de arreos o avíos que lleva encima el caballo, conformado por los siguientes elementos elaborados todos en cuero y trabajados a mano por finos talabarteros peruanos quienes adornan el apero nacional con finas piezas de plata. 
El apero nacional consiste de:
El caballo Peruano de paso a ritmo de marinera con un vistoso chalán y su pareja. El grandioso chalan Flavio Carrillo. 

Los chalanes son los jinetes de este caballo y su vestimenta es de color blanco incluyendo el poncho listado, usado en diferentes colores. El poncho en color habano o vicuña, es el más vistoso y el tradicional, en los primeros concursos. (Hasta antes de la Reforma Agraria); el cinturón o correa que sujeta el pantalón, y el calzado o botas pueden ser negras o marrones, sombrero blanco de paja y pañuelo blanco al cuello. Las faldas y blusas de las mujeres pueden llevar o no, un rico bordado en blanco en las telas del mismo color, con sombrero de paja tradicional adornado con flores y con manta o chal del color del poncho, pudiendo ser también de vicuña.
En 2003 y por primera vez en la historia, la inauguración de la "Feria de Abril" de Sevilla, en España, la mayor concentración en el mundo de coches de caballos, contó con un invitado internacional, el Perú, representado por seis elegantes chalanes y sus respectivos Caballos Peruanos de paso, que emocionaron con sus andares al exigente público de La Maestranza de Sevilla, una de las plazas de toros más importantes de España. 

Los Caballos Peruanos de paso acompañaron un carruaje cedido por el Real Club de Enganche de Sevilla y fueron los primeros, de un total de 152 carruajes, en ingresar a La Maestranza, mientras el maestro de ceremonias comentaba que el Perú ha sido el país elegido "por la originalidad y la destreza de su "Caballo Peruano de Paso". Los equinos fueron precedidos por jinetes de la Guardia Civil española, que ondeaban las banderas del Perú y de España, simbolizando la hermandad entre ambos países. 

Los Caballos Peruanos de paso entraron por la Puerta del Príncipe y, tras oírse los himnos nacionales de ambos países, éstos desfilaron al ritmo de las canciones peruanas "José Antonio" y "La flor de la canela", una atractiva demostración que mereció un largo aplauso del público asistente.



</doc>
<doc id="22241" url="https://es.wikipedia.org/wiki?curid=22241" title="Victoria pírrica">
Victoria pírrica

Una victoria pírrica es aquella que se consigue con muchas pérdidas en el bando aparentemente o tácticamente vencedor, de modo que aun tal victoria puede terminar siendo desfavorable para dicho bando.

El nombre proviene de Pirro, rey de Epiro, quien logró una victoria sobre los romanos con el costo de miles de sus hombres. Se dice que Pirro, al contemplar el resultado de la batalla, dijo ""Otra victoria como ésta y volveré solo a casa"" (griego: Ἂν ἔτι μίαν μάχην νικήσωμεν, ἀπολώλαμεν).

El film de Kevin Reynolds "187", tiene entre sus motivos el sentido de esta locución. Literalmente la locución es dada a entender de este modo: 
"Así que ahora, cuando alguien dice que algo es una victoria pírrica, ellos quieren decir que es una victoria ganada a un costo demasiado grande."






</doc>
<doc id="22242" url="https://es.wikipedia.org/wiki?curid=22242" title="Barraca">
Barraca

Barraca puede hacer referencia a:


Asimismo, en biología, puede referirse a:


Además, puede hacer referencia a:


</doc>
<doc id="22245" url="https://es.wikipedia.org/wiki?curid=22245" title="Hub">
Hub

Hub puede referirse a: 








</doc>
<doc id="22247" url="https://es.wikipedia.org/wiki?curid=22247" title="Ghost in the Shell">
Ghost in the Shell

También se realizó una serie de televisión con dos temporadas denominadas: "" y "", de dichas series se hicieron dos OVAs que resumen cada una de esas dos temporadas y una película derivada de dicha serie llamada "".

Una re-imaginación de la historia original de Masamune Shirow fue realizada en una serie de 4 mangas escritos por Junichi Fujisaku y cinco OVAs titulados "Ghost in the Shell: Arise", que puede ser vista también a modo de precuela de la historia original. Dichos OVAs fueron llevados a una serie para televisión de diez episodios llamada "Ghost in the Shell: Arise - Alternative Architecture", que derivó en la película "Ghost in the Shell: The Rising".

Fueron realizados también cuatro videojuegos. Cada obra tiene una línea argumental libre.

Ambientada en el siglo XXI, "Ghost in the Shell" se presenta en una primera lectura como un "thriller" futurista de espionaje, al narrar las misiones de Motoko Kusanagi, la Mayor a cargo de las operaciones encubiertas de la Sección Policial de Seguridad pública 9 o simplemente «Sección 9», especializada en crímenes tecnológicos. La misma Kusanagi es un "cyborg", que posee un cuerpo artificial lo que le permite ser capaz de realizar hazañas sobrehumanas especialmente requeridas por su labor.

La ambientación de "Ghost in the Shell" es innegablemente cyberpunk y recuerda a la famosa "Trilogía del Sprawl" de William Gibson. Sin embargo, a diferencia de Gibson, Shirow se interesa más en las consecuencias éticas y filosóficas de la popularización de la unión entre hombre y máquina, el desarrollo de la inteligencia artificial y una red de computadoras omnipresente, temas enfocados en especial a la identidad del ser humano y lo particular de su existencia.

Es así que en este futuro avanzado, la aplicación cotidiana de la tecnología cyborg y el perfeccionamiento de las Inteligencias Artificiales hace difusa la línea entre los seres vivos y las emulaciones, por ello es que lo único que valida a un ser vivo como un humano con derechos es la existencia de su "Ghost", un atributo del cerebro humano que es virtualmente etéreo y lo faculta para generar la autoconsciencia, emociones, individualidad y todos los aspectos que pueden ser calificados como la "personalidad" o "alma", el "Ghost" de un individuo puede incluso ser removido parcial o totalmente del cerebro y trasladado a implantes o maquinaria, siendo aun así el individuo reconocido como tal mientras este elemento permanezca intacto. De esta forma, la narrativa presenta una problemática que engloba a todas las anteriores: Cómo definir la identidad (ya sea de un individuo o de una máquina). 

El manga trata más extensamente estos temas, pues Kusanagi y sus colegas se enfrentan tanto a peligros y acertijos externos como a conflictos internos acerca de su propia naturaleza, debido a que son más máquinas que seres humanos.

El tema principal del manga (y la única historia presente en la película) es la persecución de un criminal de los medios electrónicos, conocido como el "Puppet Master" (el "Titiritero", en la traducción española), y cuya identidad se desconoce. El "Puppet Master" ha cometido varios crímenes con un único "modus operandi": el "ghost hacking", que consiste en irrumpir y tomar control de la mente de un ser humano. Al desvelar el misterio del "Puppet Master", los agentes de la Sección 9 comprenden que no se trata de un criminal común y corriente, sino de un proyecto de inteligencia artificial autónoma que pertenece al gobierno, el mismo gobierno al que la Sección 9 presta servicios, y se ha fugado a la espera de un cuerpo de verdad y una identidad humana. Si bien en principio Kusanagi se muestra escéptica, finalmente cede para que el "Puppet Master" se una a su conciencia y comparta su cuerpo, lo que hace con la intención de sacar a relucir aún más dudas acerca de la naturaleza de la identidad humana, en un mundo donde la conciencia humana ya no es algo tan particular.

El manga es conocido por la gran cantidad de notas al pie de página y comentarios del propio Shirow, tanto del contexto socio-tecnológico como político de la obra.


Dibujado por entregas de 1989 a 1990. Recopilado en un tomo. Editado en España bajo el título "Patrulla Especial Ghost", se distribuyó por entregas por Planeta DeAgostini y posteriormente fue editado en un tomo.

Dibujado por entregas de 1992 a 1995, compuesto por 4 capítulos, publicados en la revista "Young Magazine" de Kōdansha. Editado con un CD complementario en el que se incluye el manga en formato electrónico, con animación en las transiciones de viñetas. Editado en España por Planeta DeAgostini, con tapa dura, se publicó con el mismo CD que en Japón sin traducir.

11 capítulos dibujados por entregas de 1991 a 1997 en la revista "Young Magazine" de Kōdansha. Su historia progresa desde el final de "Ghost in the Shell", pero después de haber pasado un tiempo y con una nueva trama. A diferencia de la mayoría de los mangas tiene una alta cantidad de páginas a color, muchas de ellas realizadas con la ayuda del ordenador. Publicado en España por Planeta deAgostini.

Película estrenada en 1995 dirigida por Mamoru Oshii, producida por Production I.G y Bandai Visual. A diferencia de la novela gráfica, la cinta animada de 1995 dirigida por Mamoru Oshii se convirtió en una referencia obligada del género cyberpunk en los medios audiovisuales bien sea por la calidad de su animación o por el enfoque que tomó la trama de Shirow (no muy centrada en los problemas culturales y éticos que creó en su obra) cosa que en manos de Oshii se diferenció notablemente; seria, trascendental, y con estilo narrativo propio, casi poético, y es que no hay ningún diálogo o imagen dejado al azar o que desmerezca atención por parte del espectador, que busque en él un asomo de conciencia. Pues si bien "Ghost in the Shell" deja al ser vista por primera vez un sinnúmero de ideas sueltas, es en la recolección de las mismas donde se encuentra su originalidad y belleza acompañada por una banda sonora sombria e inquietante que acompaña un sentido de la fotografía oscuro, pesumbroso pero acongojador. Si bien su ambiente puede asemejarse a Blade Runner es innegable que "Ghost in the Shell" se aleja notablemente de toda idea sentimental y nos introduce a ese mundo de carácter virtual que pone en peligro no solo nuestra identidad como seres humanos, sino como individuos únicos e irrepetibles.

Película estrenada en 2004 dirigida por Mamoru Oshii, producida por Production I.G y Bandai Visual. Esta segunda parte narra como Batou, ex-compañero de la mayor Kusanagi, investiga una serie de asesinatos y posteriores suicidios por parte de robots destinados al entretenimiento sexual, tiempo después de que Kusanagi desapareciera en la red.

Constituye una versión re-masterizada de la película de 1995 con adición de efectos digitales de última generación, escenas nuevas en C.G. y sonido 6.1, aunque desafortunadamente no incluye todas las voces originales.

Película estrenada en los cines japoneses en verano de 2015. Dirigida por Kazuya Nomura, producida por Production I.G. Repite gran parte del equipo de "Ghost in the Shell: Arise".
En ella Osamu Fujimoto es asesor e hijo del primer ministro japonés, al que han asesinado. El suceso, bautizado como el "mayor acontecimiento desde la guerra", hace que Osamu colabore con la Mayor Kusanagi y la Sección 9, para descubrir la verdad detrás del asesinato. Esta película también sirve de precuela para su homónima de 1995.

Película en imagen real estrenada en los cines el 31 de marzo de 2017. Dirigida por Rupert Sanders y protagonizada por Scarlett Johansson.

Serie de 26 capítulos emitida en 2002. Publicada en España por Selecta Vision.

Serie de 26 capítulos emitida en 2004. Publicada en España por Selecta Vision. Muchas de las tramas heredan de los mangas GitS 1.5 y GitS 2.

Película recopilatoria lanzada directamente a DVD, que recupera toda la subtrama “El hombre que ríe” de la serie "", añadiendo metraje extra y mejorando la calidad de la animación, equiparándola a la de "Ghost in the Shell 2: Innocence". El film parte con el hecho de que la mayor Motoko Kusanagi junto a su equipo y el líder Daisuke Aramaki deben perseguir crímenes cometidos tanto en el mundo real como en el mundo del ciberespacio.

OVA realizado en 2006 (105 min.). Estrenado el 1 de septiembre en Japón. Continúa la trama y personajes trazados en la serie "" y "".
La acción se inicia justo después del intento de independencia por parte del distrito de refugiados de Daiyima y la ruptura del tratado de seguridad con los Estados Unidos de América.

Conjunto de 4 OVAs que sirven como una re-imaginación de "Ghost in the Shell" de Masamune Shirow. La serie de cuatro partes cuenta con nuevos diseños de personajes y es dirigida por Kazuchika Kise, guion de Ubukata Tow y música de Cornelius.

La historia se sitúa en el 2027, un año después del final de la cuarta guerra mundial. New Port City todavía se está recuperando de las consecuencias de la guerra cuando cae presa de una mina autopropulsada. Debido a esto, un militar conocido por aceptar sobornos del mercado negro fue ejecutado. Batou, un hombre con un "ojo que nunca duerme", sospecha que Kusanagi es la principal sospechosa de dicho atentado. El detective Togusa, miembro de la Comisaría de Niihama investiga la ejecución y la muerte de una prostituta.


Remontaje para televisión de "Ghost in the Shell: Arise", como serie de 10 capítulos. Los primeros 8 capítulos corresponden a las 4 OVAs, cada una de ellas dividida en dos mitades, sin contenido nuevo. Los episodios 9 y 10 son completamente nuevos, conformando una quinta OVA titulada "Pyrophoric Cult", que sirve de puente entre "Ghost in the Shell: Arise" y la película "Ghost in the Shell: The Rising".


Una de las películas en las que se puede apreciar claramente la influencia de "Ghost in the Shell" es "The Matrix". Las principales similitudes entre ambas películas son:

Finalmente, las hermanas Wachowski, creadoras de la trilogía "Matrix", reconocieron la influencia de "Ghost in The Shell" en una entrevista. El productor Joel Silver también lo admitió en una entrevista realizada en el DVD de "The Animatrix", en el que fue mostrada una secuencia de "Ghost in the Shell" junto con las Wachowski indicando el estilo que querían lograr ellos para los cortos animados de "The Animatrix".




</doc>
<doc id="22254" url="https://es.wikipedia.org/wiki?curid=22254" title="Microscopio simple">
Microscopio simple

Un microscopio simple es aquel que utiliza una sola lente para ampliar las imágenes de los objetos observados. Es el microscopio más básico. El ejemplo más clásico es la lupa. 
El microscopio óptico estándar, llamado microscopio compuesto, utiliza dos sistemas de lentes alineados, el objetivo y el ocular.


Hace más de quinientos años, se desarrollaron las lupas de cristal simples. Estas eran lentes convexas (más gruesas en el centro que en la periferia). La muestra u objeto podrían entonces ser enfocados por el uso de la lupa colocada entre el objeto y el ojo. Estos "microscopios simples" podrían difundir la imagen en la retina por ampliación mediante el aumento del ángulo visual en la retina.
El objeto a observar se coloca entre el foco y la superficie de la lente, lo que determina la formación de una imagen virtual, derecha y mayor cuanto mayor sea el poder dióptrico de la lente y cuanto más alejado esté el punto próximo de la visión nítida del sujeto.
El aumento obtenido con estos microscopios es reducido, debido a que la longitud de onda de la luz visible le impone limitaciones.

Se le atribuye a el holandés Anton van Leeuwenhoek (1632-1723) haber introducido el microscopio a la atención de los biólogos, a pesar de que ya se estaban produciendo lupas simples en el siglo XVI. Leeuwenhoek construyó microscopios muy eficaces basados en una sola lente.

Sus observaciones fueron lo suficientemente famosas como para recibir a numerosos visitantes de la altura de la reina María II de Inglaterra (1662-1694), Pedro el Grande () o Federico I de Prusia (), además de filósofos y sabios, médicos y eclesiásticos. Van Leeuwenhoek realiza ante ellos numerosas demostraciones: le mostró a Pedro el Grande la circulación sanguínea en la cola de una anguila.

El microscopio que se observa en la foto fue construido hacia 1668 y mide 10 cm de longitud; esos microscopios simples de una sola lente producían una ampliación de hasta 275 veces (275x) y tenían un poder de resolución de 1,4 μm.; no padecían las aberraciones que limitaban la eficacia de los primeros microscopios compuestos, como los empleados por Robert Hooke.
Con ellos Leeuwenhoek fue capaz incluso de describir por primera vez protistas microscópicos de vida libre y parasítica, células espermáticas, células sanguíneas, nematodos microscópicos, rotíferos y hasta bacterias.

Tomó cerca de 150 años de desarrollo, antes de que la óptica del microscopio compuesto fuera capaz de proporcionar la misma calidad de imagen de los microscopios simples de Van Leeuwenhoek, debido a dificultades en las múltiples lentes que utiliza.

http://www.brianjford.com/a-84-roysoc_soiree.pdf

http://www.sciences.demon.co.uk/whistmic.htm

http://www.brianjford.com/wav-mics.htm

http://www.brianjford.com/wav-spf.htm

http://www.brianjford.com/wavrbcs.htm

http://www.ucmp.berkeley.edu/history/hooke.html

http://www.uv.es/mabegaga/leeuwenhoek/leeuvenhoek.html

http://primeroproyectobiologia2014.blogspot.com/2014/03/microscopio-de-leeuwenhoek.html


</doc>
<doc id="22255" url="https://es.wikipedia.org/wiki?curid=22255" title="Microscopio compuesto">
Microscopio compuesto

Un microscopio óptico compuesto, o simplemente microscopio compuesto, es un microscopio que cumple su misión —producir una imagen ampliada de una muestra de algo— por medio de dos sistemas ópticos (hecho cada uno de una o más lentes) que actúan sucesivamente. Se distingue de un microscopio simple (por ejemplo una lupa de mano o una lupa de relojero) que amplía el objeto mediante un solo sistema de lentes (generalmente una sola lente). 

Los microscopios compuestos sirven para ampliar mucho (típicamente un microscopio moderno está preparado para elegir ampliaciones de entre 40 y 1500 veces) un objeto transparente, el cual es iluminado desde el otro lado, al trasluz. Se emplean para examinar cosas que no se distinguen a simple vista, como las células de una muestra de sangre o un tejido. Hay una clase especial de microscopios compuestos, los que se llaman lupas binoculares, que se usan para ampliar modestamente (de 4 a 40 veces en general) y para manipular objetos pequeños y opacos iluminados desde el lado del observador, tales como insectos, flores, joyas o el molde inicial de una moneda. 

Los dos sistemas ópticos por los que llamamos compuesto a un microscopio son el objetivo, que proyecta una primera imagen, y el ocular, que amplía la imagen anterior. La mayoría de los microscopios compuestos están dotados de varios objetivos colocados en un dispositivo rotatorio, el revólver, que permite alternar entre ellos; y la mayoría de los microscopios de trabajo y profesionales están dotados además de dos oculares, que amplían la misma imagen, para que la observación prolongada sea más saludable; pero los microscopios no se llaman compuestos por tener más de un objetivo o más de un ocular, sino porque la imagen que ve el observador se ha formado en dos fases, no en una sola, como en un microscopio simple.

Un microscopio compuesto típico tiene elementos ópticos, que son los fundamentales, y mecánicos. Los elementos ópticos sirven para formar la imagen y para iluminar la muestra. Los elementos mecánicos controlan la distancia del objetivo a la muestra (enfoque) y el desplazamiento de la muestra ante el objetivo, para la elección del área a examinar. También hay elementos mecánicos implicados en ajustar la iluminación de la muestra.

Es el encargado de producir la imagen ampliada de la muestra mediante los dos sistemas de lentes que se sitúan en sus extremos. Estos sistemas son el ocular y el objetivo. El objetivo proyecta una primera imagen de la muestra que el ocular luego amplía; esta producción de la imagen en dos fases es la que justifica la expresión microscopio compuesto, distinguiéndolo del microscopio simple (o lupa).

La mayoría de los microscopios modernos vienen dotados de varios objetivos, que pueden usarse alternativamente, montados en una pieza giratoria, denominada revólver portaobjetivos. Éste está construido de manera que el objetivo que se está usando tiene su eje óptico alineado con el del ocular y también con el del condensador. En los microscopio moderno, tanto los de trabajo o investigación como los empleados en la educación, los objetivos se insertan en el revólver por medio de una rosca estándar, lo que permite sustituirlos. El número de objetivos varía con el tipo de microscopio y el uso a que se destina. Los aumentos de los objetivos secos más frecuentemente utilizados son: 4X, 10X, 20X, 40X y 60X. 

Cada objetivo es un objeto cilíndrico que contiene una serie de lentes coaxiales —tienen sus ejes alineados— con un diseño apropiado para producir una cierta ampliación evitando a la vez los dos problemas mayores de todos los sistemas semejantes, también los objetivos fotográficos, por un lado la aberración esférica y por otro la aberración cromática. Un objetivo se califica como acromático si corrige la segunda, evitando cercos de color, y planacromático si corrige adecuadamnte las dos. En la mayoría de los casos la corrección de aberraciones se logra por el trabajo combinado de objetivo y ocular.

Los objetivos se distinguen principalmente por la ampliación que está previsto obtener con ellos y por su poder de separación, es decir, su resolución. El poder separador depende de un parámetro llamado abertura numérica. Los objetivos destinados a ampliaciones más pequeñas (típicamente 4X) tienen A.N. de 0,10; los de mayor ampliación (100X) tienen A.N. de 1,25.

Cuanto mayor es la ampliación de un objetivo más debe acercarse éste a la muestra. Incluso en objetivos de mediana ampliación, como 40X, la distancia es inferior a un milímetro. En un microscopio la operación de enfocar consiste en ajustar esta distancia. 

Los objetivos de 100 aumentos (100X), y más raramente otros de menor ampliación, suelen ser objetivos de inmersión (y llamamos a los que no lo son objetivos secos). Para el uso de éstos hay que crear entre la muestra y la lente frontal del objetivo, la más cercana a ella, un medio con un índice de refracción continuo (el índice de refracción de un medio transparente mide el grado de desviación que provoca en los rayos luminosos). Para ello se pone sobre la muestra una gota de aceite (clásicamente «aceite de cedro») y se acerca el objetivo a la muestra hasta que su lente frontal queda sumergida en la gota. En la mayoría de los casos entre el objeto observado (por ejemplo una bacteria) y la lente frontal estarán, en este orden, el medio de montaje (una gelatina o una resina), el vidrio cubreobjetos y, por último, el aceite de inmersión.

Las características de un objetivo suelen estar grabadas en un lateral. Las que no faltan son el poder de ampliación (por ejemplo 40X), la abertura numérica (por ejemplo, 0,65), y la distancia a la que proyecta la imagen (por ejemplo 160, porque se da en milímetros). La longitud del tubo óptico y los oculares deben estar ajustados a la misma distancia de proyección.

La lente frontal del objetivo es siempre muy pequeña, menor cuanto mayor su poder de ampliación, y su diámetro es inferior a un milímetro en los objetivos más potentes. Su cuidado, evitando mancharla y limpiándola con medios adaptados, es una parte crítica del mantenimiento de los microscopios, especialmente de los escolares.

En el extremo superior del tubo óptico, el de observación, donde se aproxima el ojo o se monta una cámara, se sitúa el ocular. Un ocular tiene forma cilíndrica y contiene generalmente, como el objetivo, varias lentes coaxiales. A diferencia del objetivo, no se atornilla, sino que se encaja en el tubo óptico como un émbolo, sostenido por su peso, y contenido por un reborde de mayor diámetro en su extremo superior.

Cada ocular lleva grabadas sus características. Nunca falta una de ellas: su poder de ampliación. Se expresa como en el caso de los objetivos con un número seguido de aspas. Los oculares más usados son los de 10X, pero frecuentemente se encuentran los 5X y los 15X. Valores mayores, como 20X, producen ampliaciones más grandes, pero que suelen ser excesivas para la capacidad que tienen los objetivos para resolver el detalle de la muestra, y tienen por ello un uso limitado.

La ampliación total de una observación se obtiene multiplicando la del objetivo por la del ocular. Por ejemplo, con un objetivo de 100X y un ocular 15X, obtenemos una ampliación de 1 500 aumentos (1 500X), que es por cierto la máxima ampliación útil de un microscopio compuesto clásico, dadas las limitaciones de resolución de los objetivos.

La mayoría de los microscopios escolares y muchos de aficionado son monoculares, pero los de rutina (por ejemplo, los usados para examinar muestras médicas) y los de investigación, son binoculares. En estos los rayos luminosos producidos por el objetivo se desdoblan por medio de prismas para dar servicio a dos oculares, de manera que se observa con los dos ojos a la vez. A diferencia de lo que ocurre con unos prismáticos o gemelos de visión lejana (o con el tipo de microscopio compuesto que llamamos lupa binocular) los dos ojos ven exactamente la misma imagen, sin ningún efecto de relieve. Lo que se busca es una observación más descansada, como es exigible por quien pasa horas cada día trabajando con el microscopio.

Algunos microscopios son triloculares, con espacio para tres oculares. En este caso dos se destinan a la observación directa y el tercero al registro fotográfico o videográfico de la imagen producida por el objetivo. En este caso se usan generalmente tres oculares, dos iguales, para los dos ojos, y otro, generalmente de poca ampliación, optimizado para proyectar la imagen sobre el sensor o la película fotográfica.

Este sistema tiene como finalidad dirigir la luz natural o artificial de tal manera que ilumine la preparación u objeto que se va a observar en el microscopio de la manera adecuada. Comprende los siguientes elementos:


El haz luminoso procedente de la lámpara pasa directamente a través del diafragma al condensador. Gracias al sistema de lentes que posee el condensador, la luz es concentrada sobre la preparación a observar. El haz de luz penetra en el objetivo y sigue por el tubo hasta llegar al ocular, donde es captado por el ojo del observador.

Propiedades del microscopio

La parte mecánica del microscopio comprende el pie o la base, el tubo, el revólver, el asa, la platina, el carro y el tornillo micrométrico. Estos elementos sostienen la parte óptica y de iluminación; además, permiten los desplazamientos necesarios para el enfoque del objeto.


Se denomina campo del microscopio al círculo visible que se observa a través del microscopio. También podemos definirlo como la porción del plano visible observado a través del microscopio.

Si el aumento es mayor, el campo disminuye, lo cual quiere decir que el campo es inversamente proporcional al aumento del microscopio. Para medir el diámetro del campo del microscopio con cualquiera de los objetivos se utiliza el micrómetro, al que se hará referencia en el siguiente punto.

Existen diversas clases de microscopios, según la naturaleza de los sistemas de luz, y otros accesorios utilizados para obtener las imágenes.

El microscopio compuesto u óptico utiliza lentes para ampliar las imágenes de los objetos observados. El aumento obtenido con estos microscopios es reducido, debido a la longitud de onda de la luz visible que impone limitaciones. El microscopio óptico puede ser monocular, y consta de un solo tubo. La observación en estos casos se hace con un solo ojo. Es binocular cuando posee dos tubos. La observación se hace con los dos ojos. Esto presenta ventajas tales como mejor percepción de la imagen, más cómoda la observación y se perciben con mayor nitidez los detalles. 

El Microscopio invertido el sistema óptico de este tipo de microscopio está en posición al revés comparado con un microscopio óptico convencional. La fuente de luz y el condensador están dispuestos sobre la plataforma apuntando hacia abajo; y los objetivos y la torrecilla están debajo de la plataforma apuntando hacia arriba. Las únicas partes que están en una disposición normal son el tubo binocular o trinocular, así mismo la muestra que es colocada sobre la plataforma o platina mecánica. Presenta la ventaja de poder observar cultivos enteros o grandes muestras bajo estados más naturales y con menores condiciones de estrés. 

El Microscopio estereoscópico: es un tipo de microscopio hace posible la visión tridimensional de los objetos. Consta de dos tubos oculares y dos objetivos pares para cada aumento. Este microscopio ofrece ventajas para observaciones que requieren pequeños aumentos. El óptimo de visión estereoscópica se encuentra entre 2 y 40X o aumento total del microscopio.

Microscopio de campo oscuro. Este microscopio está provisto de un condensador paraboloide, que hace que los rayos luminosos no penetren directamente en el objetivo, sino que iluminan oblicuamente la preparación. Los objetos aparecen como puntos luminosos sobre un fondo oscuro.

Microscopio de contraste de fases. Se basa en las modificaciones de la trayectoria de los rayos de luz, los cuales producen contrastes notables en la preparación.

Microscopio de fluorescencia. La fluorescencia es la propiedad que tienen algunas sustancias de emitir luz propia cuando inciden sobre ellas radiaciones energéticas. El tratamiento del material biológico con flurocromos facilita la observación al microscopio.

En 1932, Bruche y Johnsson construyen el primer microscopio electrónico a base de lentes electrostáticas. Ese mismo año Knoll y Ruska dan a conocer los primeros resultados obtenidos con un microscopio electrónico Siemens, construido con lentes magnéticas. Así nace el microscopio electrónico. Para 1936 ya se ha perfeccionado y se fabrican microscopios electrónicos que superan en resolución al microscopio óptico.

Estos logros no sólo representan un avance en el campo de la electrónica, sino también en el campo de la Biología, pues son muchas las estructuras biológicas que se han descubierto y que revelan detalles inusitados, al observarlas al microscopio electrónico.

El microscopio electrónico utiliza un flujo de electrones en lugar de luz. Consta fundamentalmente de un tubo de rayos catódicos, en el cual debe mantenerse el vacío. El cátodo está constituido por un filamento de tungsteno, que al calentarse eléctricamente emite los electrones, los cuales son atraídos hacia el ánodo por una diferencia de potencial de 50.000 a 100.000 voltios. La lente del condensador enfoca este haz y lo dirige hacia el objeto que se observa, cuya preparación exige técnicas especiales. Los electrones chocan contra la preparación, sobre la cual se desvían de manera desigual.

Se acostumbra a utilizar el término microoscopicos para las fotografías tomadas a través del microscopio óptico y micrografía o electromicrografía para las que se toman en el microscopio electrónico.

Los aumentos máximos conseguidos en el microscopio electrónico son del orden de 2.000.000X mediante el acoplamiento al microscopio electrónico de un amplificador de imagen y una cámara de televisión. En resumen, el microscopio electrónico consta esencialmente de:


Existen varios tipos de microscopios electrónicos, que cada día se perfeccionan más.

El microscopio electrónico de transmisión que utiliza un haz de electrones acelerados por un alto voltaje (cien mil voltios). Este haz ilumina una sección muy fina de la muestra, sean tejidos, células u otro material.

El microscopio electrónico de barrido se utiliza para el estudio de la morfología y la topografía de los elementos. Estos instrumentos utilizan voltajes cercanos a los 20.000 voltios. Las lentes magnéticas utilizan un haz muy fino de electrones para penetrar repetidamente la muestra, y se produce una imagen ampliada de la superficie observada en la pantalla de un monitor. 
El microscopio electrónico mixto tiene propiedades comunes con el de transmisión y con el de barrido y resulta muy útil para ciertas investigaciones. Hay otros microscopios analíticos que detectan señales características de los elementos que constituyen la muestra.

Con estos poderosos instrumentos, que utilizan el flujo de electrones y las radiaciones electromagnéticas así como la aplicación de técnicas histoquímicas y bioquímicas, además del empleo de marcadores radiactivos, se han logrado grandes avances en la biología celular.

El microscopio electrónico de barrido está situado a la izquierda del operador, y las imágenes computarizadas de la muestra se ven en la pantalla de la derecha. Aunque un microscopio electrónico de transmisión puede resolver objetos más pequeños que uno de barrido, este último genera imágenes más útiles para conocer la estructura tridimensional de objetos minúsculos.

El empleo de microscopios quirúrgicos ha permitido que los cirujanos lleven a cabo intervenciones que parecían imposibles, como la reimplantación de un miembro y la cirugía de los ojos y oídos. Estos microscopios son en especial útiles cuando es necesario realinear para unir o reparar fibras nerviosas y vasos sanguíneos individuales. Se usa para operaciones de dificultad.

Muchas veces interesa al observador conocer el tamaño real de los objetos o microorganismos que está observando a través del microscopio. Para estas mediciones pueden utilizarse varios métodos.

Método de los micrómetros. Se utiliza para esto un micrómetro de platina o de objetivo, que consiste en un portaobjetos en cuyo centro se halla una escala graduada (de 2 mm de longitud), con separaciones, entre cada división, de una centésima de milímetro.

Además se utiliza un micrómetro ocular que lleva una escala graduada en décimas de milímetros. Se coloca el micrómetro objetivo sobre la platina y se enfoca el microscopio hasta que las líneas de la escala graduada aparezcan nítidas. Luego se hace superponer la escala del ocular y se toma como referencia las primeras divisiones en que una línea del micrómetro objetivo y una línea del micrómetro ocular coincidan o se superpongan exactamente.

Luego, por simple regla de tres, se calcula el valor en mieras de cada división ocular. Veamos un ejemplo. Si 9 divisiones del micrómetro objetivo (0,09 mm) equivalen a 30 divisiones del micrómetro ocular, cada división del ocular equivaldrá a:
0,09 mm/30 = 0,003 mm = 3 µm

Quiere decir que para el objetivo calibrado y el ocular utilizado, cada división del micrómetro ocular equivale a 3 µm. Una vez obtenido este dato para cada objetivo en la forma que hemos expuesto, teniendo el microscopio ocular podrían hacerse todas las mediciones que se deseen. Para medir, por ejemplo, un Paramecium de una preparación, procedemos así: haremos coincidir los extremos del microorganismo con las divisiones del micrómetro ocular. Si la longitud del organismo es de 75 divisiones del micrómetro ocular, y cada división equivale a 3 µm, la longitud del Paramecium será 75x3= 225 µm. También se pueden efectuar mediciones en el microscopio con cámara clara y utilizando una regla. En realidad, estas medidas no son tan exactas como cuando se utilizan micrómetros por errores que se introducen superponiendo imágenes.

El microscopio debe estar protegido del polvo, humedad y otros agentes que pudieran dañarlo. Mientras no esté en uso debe guardarse en un estuche o gabinete, o bien cubrirlo con una bolsa plástica o campana de vidrio. 

Las partes mecánicas deben limpiarse con un paño suave; en algunos casos, éste se puede humedecer con xilol para disolver ciertas manchas de grasa, aceite de cedro, parafina, etc. Que hayan caído sobre las citadas partes. 

La limpieza de las partes ópticas requiere precauciones especiales. Para ello debe emplearse papel de óptico que expiden las casas distribuidoras de material de laboratorio ó fotografía o utilizar un paño de algodón. Para el polvillo se puede utilizar un perilla con pincel de pelo de camello. Nunca deben tocarse las lentes del ocular, objetivo y condensador con los dedos; las huellas digitales perjudican la visibilidad, y cuando se secan resulta trabajoso eliminarlas.

Para una buena limpieza de las lentes puede humedecerse el papel de óptica, envolviendo un palillo con algo de punta con una solución de éter/alcohol (70-30 %) y luego pasarlo por la superficie cuantas veces sea necesario. La limpieza de la óptica ha de realizarse en espiral, desde el centro hacia el exterior. El aceite de cedro que queda sobre la lente frontal del objetivo de inmersión debe quitarse inmediatamente después de finalizada la observación. Para ello se puede pasar el papel de óptica impregnado con una gota de xilol. En caso de estar seco debe ponerse la zona a remojo con la solución señalada.

Para guardarlo se acostumbra colocar el objetivo de menor aumento sobre la platina y bajado hasta el tope; el condensador debe estar en su posición más baja, para evitar que tropiece con alguno de los objetivos. Guárdese en lugares secos, para evitar que la humedad favorezca la formación de hongos. Ciertos ácidos y otras sustancias químicas que producen emanaciones fuertes, deben mantenerse alejados del microscopio.

Dos lentes convexas bastan para construir un microscopio. Cada lente hace converger los rayos luminosos que la atraviesan. Una de ellas, llamada objetivo, se sitúa cerca del objeto que se quiere estudiar. El objetivo forma una imagen real aumentada e invertida. Se dice que la imagen es real porque los rayos luminosos pasan realmente por el lugar de la imagen. La imagen es observada por la segunda lente, llamada ocular, que actúa sencillamente como una lupa. El ocular está situado de modo que no forma una segunda imagen real, sino que hace divergir los rayos luminosos, que al entrar en el ojo del observador parecen proceder de una gran imagen invertida situada más allá del objetivo. Como los rayos luminosos no pasan realmente por ese lugar, se dice que la imagen es virtual...



</doc>
<doc id="22259" url="https://es.wikipedia.org/wiki?curid=22259" title="Apertura numérica">
Apertura numérica

En óptica, la apertura numérica (AN) de un sistema óptico es un número adimensional que caracteriza el rango de ángulos para los cuales el sistema acepta luz. Recíprocamente, también está relacionado con el ángulo de salida del sistema. La definición exacta del término varía según diferentes áreas de la óptica.

En la mayor parte de las áreas de la óptica, especialmente en microscopía, la apertura numérica de un sistema óptico tal como una lente queda definido por la siguiente ecuación:

donde "n" es el índice de refracción del medio en el que la lente se encuentra (1 para el aire, 1,33 para el agua pura, y hasta 1,56 para algunos aceites), y "θ" es la mitad del ángulo de aceptancia máximo que puede entrar o salir de la lente. La AN se mide generalmente con respecto a un objeto o a un punto de una imagen y varía con la posición del punto.

En microscopía, la apertura numérica es importante porque indica el poder de resolución de una lente. El tamaño del detalle más pequeño que puede ser visualizado es proporcional a λ/AN, donde λ es la longitud de onda de la luz empleada. Una lente con una apertura numérica grande será capaz de visualizar más detalles que una lente con una apertura numérica más baja no podrá. Además, las lentes con aperturas numéricas grandes aceptan más luz y dan una imagen más brillante.

La apertura numérica es una medida del diámetro de la apertura comparada con la distancia focal. En fotografía, esta relación viene dada habitualmente con el número f, "f/#", que para una lente delgada enfocando a un objeto en el infinito vale

En la física de los láseres, la apertura numérica se define de una forma ligeramente diferente. Los haces del láser divergen mientras se propagan, aunque muy lentamente. Lejos de la parte más estrecha del haz, la dispersión aumenta linealmente con la distancia — el haz del láser forma un cono de luz. La misma relación nos da la apertura numérica
pero "θ" se define de forma diferente a como se hacía antes. Los haces láser no suelen tener bordes abruptos como el cono de luz que pasa por una lente. En los láseres, la irradiancia decae gradualmente según nos vamos alejando del centro del haz. El perfil de decaimiento es típicamente Gaussiano. Los físicos de láseres llaman "θ" a la divergencia del haz: más allá de λ/4, el ángulo entre la dirección de propagación y el ángulo para el cual la irradiancia cae a 1/e veces la irradiancia máxima. La AN de un haz láser Gaussiano está relacionada con la mancha de spot mínima con la siguiente expresión
donde "λ" es la longitud de onda de la luz, y "D" es el diámetro del haz láser en su punto más estrecho, medido entre los puntos de irradiancia 1/e veces la irradiancia máxima. Nótese que esta expresión implica que un haz láser que enfoca con una mancha de spot pequeña divergirá rápidamente según se mueve del foco, mientras que a un haz láser muy ancho no le ocurrirá.

Ángulo de Aceptancia: es el máximo ángulo en el cual el rayo de luz incidente es atrapado por las paredes de la fibra. En este caso el rayo de luz se refleja totalmente en el recubrimiento de la misma, por lo que el ángulo de transmisión, sobre el recubrimiento de la fibra, es 90º.

Las fibras ópticas monomodo sólo guían la luz que entra en la fibra dentro de un determinado cono de aceptancia. La mitad del ángulo de este cono es el ángulo de aceptancia, "θ". Para fibras con perfil de salto de índice multimodo, este ángulo de aceptancia viene determinado por la siguiente expresión
donde "n" es el índice de refracción del núcleo de la fibra, y "n" es el índice de refracción de la cubierta.

Debido al gran parecido de esta expresión con las definiciones de AN de otras áreas de la óptica, es habitual llamar así al término de la derecha de la ecuación anterior, definiendo finalmente la apertura numérica de una fibra como
donde "n" es el índice de refracción del eje central de la fibra. Nótese que cuando se usa esta definición, la relación entre la apertura numérica y el ángulo de aceptancia es una mera aproximación. En particular, los fabricantes suelen dar la AN para fibras monomodo basándose en esta expresión, aunque para este tipo de fibras el ángulo de aceptancia es algo diferente y no depende solamente de los índices de refracción de núcleo y cubierta.


</doc>
<doc id="22261" url="https://es.wikipedia.org/wiki?curid=22261" title="Revolución francesa">
Revolución francesa

La Revolución francesa fue un conflicto social y político, con diversos periodos de violencia, que convulsionó Francia y, por extensión de sus implicaciones, a otras naciones de Europa que enfrentaban a partidarios y opositores del sistema conocido como el Antiguo Régimen. Se inició con la autoproclamación del Tercer Estado como Asamblea Nacional en 1789 y finalizó con el golpe de estado de Napoleón Bonaparte en 1799.

Si bien, después de que la Primera República cayera tras el golpe de Estado de Napoleón Bonaparte, la organización política de Francia durante el siglo XIX osciló entre república, imperio y monarquía constitucional, lo cierto es que la revolución marcó el final definitivo del feudalismo y del absolutismo en ese país, y dio a luz a un nuevo régimen donde la burguesía, apoyada en ocasiones por las masas populares, se convirtió en la fuerza política dominante en el país. La revolución socavó las bases del sistema monárquico como tal, más allá de sus estertores, en la medida en que lo derrocó con un discurso e iniciativas capaces de volverlo ilegítimo.

Según la historiografía clásica, la Revolución francesa marca el inicio de la Edad Contemporánea al sentar las bases de la democracia moderna, lo que la sitúa en el corazón del siglo XIX. Abrió nuevos horizontes políticos basados en el principio de la soberanía popular, que será el motor de las revoluciones de 1830, de 1848 y de 1871.

Los escritores ilustrados del siglo XVIII, filósofos, politólogos, científicos y economistas, denominados comúnmente "philosophes", y a partir de 1751 los enciclopedistas, contribuyeron a minar las bases del Derecho Divino de los reyes. La filosofía de la 'Ilustración' ha desempeñado pues un rol significativo en el giro que tomaron estos eventos históricos pero su influencia debe relatarse de modo más matizado: acordarle demasiada importancia a los preceptos filosóficos nacidos durante ese siglo se revelaría como una carencia mayúscula de fidelidad historiográfica.

La corriente de pensamiento vigente en Francia era la Ilustración, cuyos principios se basaban en la razón, la igualdad y la libertad. La Ilustración había servido de impulso a las "Trece Colonias" norteamericanas para la independencia de su metrópolis europea. Tanto la influencia de la Ilustración como el ejemplo de los Estados Unidos sirvieron de «trampolín» ideológico para el inicio de la revolución en Francia.

En términos generales fueron varios los factores que influyeron en la Revolución:

Desde el punto de vista político, fueron fundamentales ideas tales como las expuestas por Voltaire, Rousseau, Diderot o Montesquieu (como por ejemplo, los conceptos de libertad política, de fraternidad y de igualdad, o de rechazo a una sociedad dividida, o las nuevas teorías políticas sobre la separación de poderes del Estado). Todo ello fue rompiendo el prestigio de las instituciones del Antiguo Régimen, ayudando a su desplome.

Desde el punto de vista económico, la inmanejable deuda del Estado fue exacerbada por un sistema de extrema desigualdad social y de altos impuestos que los estamentos privilegiados, nobleza y clero no tenían obligación de pagar, pero que sí oprimía al resto de la sociedad. Hubo un aumento de los gastos del Estado simultáneo a un descenso de la producción agraria de terratenientes y campesinos, lo que produjo una grave escasez de alimentos en los meses precedentes a la Revolución. Las tensiones, tanto sociales como políticas, mucho tiempo contenidas, se desataron en una gran crisis económica a consecuencia de los dos hechos puntuales señalados: la colaboración interesada de Francia con la causa de la independencia estadounidense (que ocasionó un gigantesco déficit fiscal) y el aumento de los precios agrícolas.

El conjunto de la población mostraba un resentimiento generalizado dirigido hacia los privilegios de los nobles y del alto clero, que mantenían su dominio sobre la vida pública impidiendo que accediera a ella una pujante clase profesional y comerciante. El ejemplo del proceso revolucionario estadounidense abrió los horizontes de cambio político entre otros.

Los Estados Generales estaban formados por los representantes de cada estamento. Estos estaban separados a la hora de deliberar, y tenían sólo un voto por estamento. La convocatoria de 1789 fue un motivo de preocupación para la oposición, por cuanto existía la creencia de que no era otra cosa que un intento, por parte de la monarquía, de manipular la asamblea a su antojo. La cuestión que se planteaba era importante. Estaba en juego la idea de soberanía nacional, es decir, admitir que el conjunto de los diputados de los Estados Generales representaba la voluntad de la nación.

El tercer impacto de los Estados Generales fue de gran tumulto político, particularmente por la determinación del sistema de votación. El Parlamento de París propuso que se mantuviera el sistema de votación que se había usado en 1614, si bien los magistrados no estaban muy seguros acerca de cuál había sido en realidad tal sistema. Sí se sabía, en cambio, que en dicha asamblea habían estado representados (con el mismo número de miembros) la nobleza (Primer Estado), el clero (Segundo Estado) y la burguesía (Tercer Estado). Inmediatamente, un grupo de liberales parisinos denominado «Comité de los Treinta», compuesto principalmente por gente de la nobleza, comenzó a protestar y agitar, reclamando que se duplicara el número de asambleístas con derecho a voto del Tercer Estado (es decir, los «Comunes»). El gobierno aceptó esta propuesta, pero dejó a la Asamblea la labor de determinar el derecho de voto. Este cabo suelto creó gran tumulto.

El rey Luis XVI y una parte de la nobleza no aceptaron la situación. Los miembros del Tercer Estamento se autoproclamaron Asamblea Nacional, y se comprometieron a escribir una Constitución. Sectores de la aristocracia confiaban en que estos Estados Generales pudieran servir para recuperar parte del poder perdido, pero el contexto social ya no era el mismo que en 1614. Ahora existía una élite burguesa que tenía una serie de reivindicaciones e intereses que chocaban frontalmente con los de la nobleza (y también con los del pueblo, cosa que se demostraría en los años siguientes).

Cuando finalmente los Estados Generales de Francia se reunieron en Versalles el 5 de mayo de 1789 y se originaron las disputas respecto al tema de las votaciones, los miembros del Tercer Estado debieron verificar sus propias credenciales, comenzando a hacerlo el 28 de mayo y finalizando el 17 de junio, cuando los miembros del Tercer Estado se declararon como únicos integrantes de la Asamblea Nacional: ésta no representaría a las clases pudientes sino al pueblo en sí. La primera medida de la Asamblea fue votar la «Declaración de los Derechos del Hombre y del Ciudadano». Si bien invitaron a los miembros del Primer y Segundo Estado a participar en esta asamblea, dejaron en claro sus intenciones de proceder incluso sin esta participación.

La monarquía, opuesta a la Asamblea, cerró las salas donde ésta se estaba reuniendo. Los asambleístas se mudaron a un edificio cercano, donde la aristocracia acostumbraba a jugar el juego de la pelota, conocido como "Jeu de paume". Allí es donde procedieron con lo que se conoce como el «Juramento del Juego de la Pelota» el 20 de junio de 1789, prometiendo no separarse hasta tanto dieran a Francia una nueva constitución. La mayoría de los representantes del bajo clero se unieron a la Asamblea, al igual que 47 miembros de la nobleza. Ya el 27 de junio, los representantes de la monarquía se dieron por vencidos, y por esa fecha el Rey mandó reunir grandes contingentes de tropas militares que comenzaron a llegar a París y Versalles. Los mensajes de apoyo a la Asamblea llovieron desde París y otras ciudades. El 9 de julio la Asamblea se nombró a sí misma «Asamblea Nacional Constituyente».

El 11 de julio de 1789, el rey Luis XVI, actuando bajo la influencia de los nobles conservadores al igual que la de su hermano, el Conde D'Artois, despidió al ministro Necker y ordenó la reconstrucción del Ministerio de Finanzas. Gran parte del pueblo de París interpretó esta medida como un auto-golpe de la realeza, y se lanzó a la calle en abierta rebelión. Algunos de los militares se mantuvieron neutrales, pero otros se unieron al pueblo.

El 14 de julio el pueblo de París respaldó en las calles a sus representantes y, ante el temor de que las tropas reales los detuvieran, asaltaron la fortaleza de la Bastilla, símbolo del absolutismo monárquico, pero también punto estratégico del plan de represión de Luis XVI, pues sus cañones apuntaban a los barrios obreros. Tras cuatro horas de combate, los insurgentes tomaron la prisión, matando a su gobernador, el Marqués Bernard de Launay. Si bien sólo cuatro presos fueron liberados, la Bastilla se convirtió en un potente símbolo de todo lo que resultaba despreciable en el Antiguo Régimen. Retornando al Ayuntamiento, la multitud acusó al alcalde Jacques de Flesselles de traición, quien recibió un balazo que lo mató. Su cabeza fue cortada y exhibida en la ciudad clavada en una pica, naciendo desde entonces la costumbre de pasear en una pica las cabezas de los decapitados, lo que se volvió muy común durante la Revolución.

La Revolución se fue extendiendo por ciudades y pueblos, creándose nuevos ayuntamientos que no reconocían otra autoridad que la Asamblea Nacional Constituyente. La insurrección motivada por el descontento popular siguió extendiéndose por toda Francia. En las áreas rurales, para protestar contra los privilegios señoriales, se llevaron a cabo actos de quema de títulos sobre servidumbres, derechos feudales y propiedad de tierras, y varios castillos y palacios fueron atacados. Esta insurrección agraria se conoce como "La Grande Peur" (el Gran Miedo).

La noche del 4 de agosto de 1789, la Asamblea Constituyente, actuando detrás de los nuevos acontecimientos, suprimió por ley las servidumbres personales (abolición del feudalismo), los diezmos y las justicias señoriales, instaurando la igualdad ante el impuesto, ante penas y en el acceso a cargos públicos. En cuestión de horas, los nobles y el clero perdieron sus privilegios. El curso de los acontecimientos estaba ya marcado, si bien la implantación del nuevo modelo no se hizo efectiva hasta 1793. El rey, junto con sus seguidores militares, retrocedió al menos por el momento. Lafayette tomó el mando de la Guardia Nacional de París y Jean-Sylvain Bailly, presidente de la Asamblea Nacional Constituyente, fue nombrado nuevo alcalde de París. El rey visitó París el 27 de julio y aceptó la escarapela tricolor.

Sin embargo, después de estos actos de violencia, los nobles, no muy seguros del rumbo que tomaría la reconciliación temporal entre el rey y el pueblo, comenzaron a salir del país, algunos con la intención de fomentar una guerra civil en Francia y de llevar a las naciones europeas a respaldar al rey. Estos fueron conocidos como los "émigrés" («emigrados»).

La revolución se enfrentó duramente con la Iglesia católica que pasó a depender del Estado. En 1790 se eliminó la autoridad de la Iglesia de imponer impuestos sobre las cosechas, se eliminaron también los privilegios del clero y se confiscaron sus bienes. Bajo el Antiguo Régimen la Iglesia era el mayor terrateniente del país. Más tarde se promulgó una legislación que convirtió al clero en empleados del Estado. Estos fueron unos años de dura represión para el clero, siendo comunes la prisión y masacre de sacerdotes en toda Francia. El Concordato de 1801 entre la Asamblea y la Iglesia finalizó este proceso y establecieron normas de convivencia que se mantuvieron vigentes hasta el 11 de diciembre de 1905, cuando la Tercera República sentenció la separación definitiva entre la Iglesia y el Estado. El viejo calendario gregoriano, propio de la religión católica fue anulado por Billaud-Varenne, en favor de un «calendario republicano» y una nueva era que establecía como primer día el 22 de septiembre de 1792.

En una Asamblea que se quería plural y cuyo propósito era la redacción de una constitución democrática, los 1200 constituyentes representaban las diversas tendencias políticas del momento.




En ese primer periodo constituyente, los líderes indiscutibles de la Asamblea eran Mirabeau y el abad Sieyès.

El 27 de agosto de 1789 la Asamblea publicó la "Declaración de los Derechos del Hombre y del Ciudadano" inspirándose en parte en la Declaración de Independencia de los Estados Unidos y estableciendo el principio de libertad, igualdad y fraternidad. Dicha declaración establecía una declaración de principios que serían la base ineludible de la futura Constitución.

La Asamblea Nacional Constituyente no era sólo un órgano legislativo sino la encargada de redactar una nueva Constitución. Algunos, como Necker, favorecían la creación de una asamblea bicameral en donde el senado sería escogido por la Corona entre los miembros propuestos por el pueblo. Los nobles, por su parte, favorecían un senado compuesto por miembros de la nobleza elegidos por los propios nobles. Prevaleció, sin embargo, la tesis liberal de que la Asamblea tendría una sola cámara, quedando el rey sólo con el poder de veto, pudiendo posponer la ejecución de una ley, pero no su total eliminación.

El movimiento de los monárquicos para bloquear este sistema fue desmontado por el pueblo de París, compuesto fundamentalmente por mujeres (llamadas despectivamente «Las Furias»), que marcharon el 5 de octubre de 1789 sobre Versalles. Tras varios incidentes, el rey y su familia se vieron obligados a abandonar Versalles y se trasladaron al Palacio de las Tullerías en París.

Los electores habían escogido a los miembros de los Estados Generales por un periodo de un año, pero de acuerdo al Juramento del Jeu de paume, los miembros del Tercer Estado, también llamados los «comunes», acordaron no abandonar la Asamblea en tanto no se hubiera elaborado una Constitución.

Durante 1790 se produjeron movimientos antirrevolucionarios, pero sin éxito. En este periodo se intensificó la influencia de los «clubes» políticos entre los que destacaban los Jacobinos y los Cordeliers. En agosto de 1790 existían 152 clubes jacobinos.

A principios de 1791, la Asamblea consideró introducir una legislación contra los franceses que emigraron durante la Revolución ("émigrés"). Se pretendía coartar la libertad de salir del país para fomentar desde el extranjero la creación de ejércitos contrarrevolucionarios, y evitar la fuga de capitales. Mirabeau se opuso rotundamente a esto. Sin embargo, el 2 de marzo de 1791 Mirabeau fallece, y la Asamblea adopta esta draconiana medida.

El 20 de junio de 1791, Luis XVI, opuesto al curso que iba tomando la Revolución, huyó junto con su familia de las Tullerías. Sin embargo, al día siguiente cometió la imprudencia de dejarse ver, fue arrestado en Varennes por un oficial del pueblo y devuelto a París escoltado por la guardia. A su regreso a París el pueblo se mantuvo en silencio, y tanto él como su esposa, María Antonieta, sus dos hijos (María Teresa y Luis-Carlos, futuro Luis XVII) y su hermana (Madame Elizabeth) permanecieron bajo custodia.

El 3 de septiembre de 1791, fue aprobada la primera Constitución de la historia de Francia. Una nueva organización judicial dio características temporales a todos los magistrados y total independencia de la Corona. Al rey sólo le quedó el poder ejecutivo y el derecho de vetar las leyes aprobadas por la Asamblea Legislativa. La asamblea, por su parte, eliminó todas las barreras comerciales y suprimió las antiguas corporaciones mercantiles y los gremios; en adelante, los individuos que quisieran desarrollar prácticas comerciales necesitarían una licencia, y se abolió el derecho a la huelga.

Aun cuando existía una fuerte corriente política que favorecía la monarquía constitucional, al final venció la tesis de mantener al rey como una figura decorativa. Jacques Pierre Brissot introdujo una petición insistiendo en que, a los ojos del pueblo, Luis XVI había sido depuesto por el hecho de su huida. Una inmensa multitud se congregó en el Campo de Marte para firmar dicha petición. Georges Danton y Camille Desmoulins pronunciaron discursos exaltados. La Asamblea pidió a las autoridades municipales guardar el orden. Bajo el mando de La Fayette, la Guardia Nacional se enfrentó a la multitud. Al principio, tras recibir una oleada de piedras, los soldados respondieron disparando al aire; dado que la multitud no cedía, Lafayette ordenó disparar a los manifestantes, ocasionando más de 50 muertos.

Tras esta masacre, las autoridades cerraron varios clubes políticos, así como varios periódicos radicales como el que editaba Jean-Paul Marat. Danton se fugó a Inglaterra y Desmoulins y Marat permanecieron escondidos.

Mientras tanto, la Asamblea había redactado la Constitución y el rey había sido mantenido, aceptándola. El rey pronunció un discurso ante la Asamblea, que fue acogido con un fuerte aplauso. La Asamblea Constituyente cesó en sus funciones el 29 de septiembre de 1791.

Bajo la Constitución de 1791, Francia funcionaría como una monarquía constitucional. El rey tenía que compartir su poder con la Asamblea, pero todavía mantenía el poder de veto y la potestad de elegir a sus ministros.

La Asamblea Legislativa se reunió por primera vez el 1 de octubre de 1791. La componían 264 diputados situados a la derecha: "feuillants" (dirigidos por Barnave, Duport y Lameth), y "girondinos", portavoces republicanos de la gran burguesía. En el centro figuraban 345 diputados independientes, carentes de programa político definido. A la izquierda 136 diputados inscritos en el club de los "jacobinos" o en el de los "cordeliers", que representaban al pueblo llano parisino a través de sus periódicos "L´Ami du Peuple" y "Le Père Duchesne", y con Marat y Hebert como portavoces. Pese a su importancia social y el apoyo popular y de la pequeña burguesía, en la Asamblea era escasa la influencia de la izquierda, pues la Asamblea estaba dominada por las ideas políticas que representaban los girondinos. Mientras los jacobinos tienen detrás a la gran masa de la pequeña burguesía, los "cordeliers" cuentan con el apoyo del pueblo llano, a través de las secciones parisienses.

Este gran número de diputados se reunían en los clubes, germen de los partidos políticos. El más célebre de entre éstos fue el partido de los jacobinos, dominado por Robespierre. A la izquierda de este partido se encontraban los cordeleros, quienes defendían el sufragio universal masculino (derecho de todos los hombres al voto a partir de una determinada edad). Los "cordeliers" querían la eliminación de la monarquía e instauración de la república. Estaban dirigidos por Jean-Paul Marat y Georges Danton, representando siempre al pueblo más humilde. El grupo de ideas más moderadas era el de los girondinos, que defendían el sufragio censitario y propugnaban una monarquía constitucional descentralizada. También se encontraban aquellos que formaban parte de «el Pantano», o «el Llano», como eran llamados aquellos que no tenían un voto propio, y que se iban por las proposiciones que más les convenían, ya vinieran de los jacobinos o de los girondinos.

En los primeros meses de funcionamiento de la Asamblea, el rey había vetado una ley que amenazaba con la condena a muerte a los "émigrés", y otra que exigía al clero prestar juramento de lealtad al Estado. Desacuerdos de este tipo fueron los que llevaron más adelante a la crisis constitucional.

Mientras tanto, dos potencias absolutistas europeas, Austria y Prusia, se dispusieron a invadir la Francia revolucionaria, lo que hizo que el pueblo francés se convirtiera en un ejército nacional, dispuesto a defender y a difundir el nuevo orden revolucionario por toda Europa. Durante la guerra, la libertad de expresión permitió que el pueblo manifestase su hostilidad hacia la reina María Antonieta (llamada «la Austriaca» por ser hija de un emperador de aquel país y «Madame Déficit» por el gasto que había representado al Estado, que no era mayor que la mayoría de los cortesanos) y contra Luis XVI, que casi siempre se negaba a firmar leyes propuestas por la Asamblea Legislativa.

El 10 de agosto de 1792, las masas asaltaron el Palacio de las Tullerías, y la Asamblea Legislativa suspendió las funciones constitucionales del rey. La Asamblea acabó convocando elecciones con el objetivo de configurar (por sufragio universal) un nuevo parlamento que recibiría el nombre de Convención. Aumentaba la tensión política y social en Francia, así como la amenaza militar de las potencias europeas. El conflicto se planteaba así entre una monarquía constitucional francesa en camino de convertirse en una democracia republicana, y las monarquías europeas absolutas. El nuevo parlamento elegido ese año abolió la monarquía y proclamó la República. Creó también un nuevo calendario, según el cual el año 1792 se convertiría en el año 1 de su nueva era.

El gobierno pasó a depender de la Comuna insurreccional. Cuando la Comuna envió grupos de sicarios a las prisiones, asesinaron a 1.400 víctimas, y pidió a otras ciudades de Francia que hicieran lo mismo, la Asamblea no opuso resistencia. Esta situación persistió hasta el 20 de septiembre de 1792, en que se creó un nuevo cuerpo legislativo denominado Convención, que de hecho se convirtió en el nuevo gobierno de Francia.

El poder legislativo de la nueva República estuvo a cargo de la Convención, mientras que el poder ejecutivo recayó sobre el Comité de Salvación Nacional.

En el Manifiesto de Brunswick, los Ejércitos Imperiales y de Prusia amenazaron con invadir Francia si la población se resistía al restablecimiento de la monarquía. Esto ocasionó que Luis XVI fuera visto como conspirador con los enemigos de Francia. El 17 de enero de 1793, la Convención condenó al rey a muerte por una pequeña mayoría, acusándolo de «conspiración contra la libertad pública y la seguridad general del Estado». El 21 de enero el rey fue ejecutado, lo cual encendió nuevamente la mecha de la guerra con otros países europeos. La reina María Antonieta, nacida en Austria y hermana del Emperador, fue ejecutada el 16 de octubre del mismo año, iniciándose así una revolución en Austria para sustituir a la reina. Esto provocó la ruptura de toda relación entre ambos países.

El mismo día en el que se reunía la Convención (20 de septiembre de 1792), todas las tropas francesas (formadas por tenderos, artesanos y campesinos de toda Francia) derrotaron por primera vez a un ejército prusiano en Valmy, lo cual señalaba el inicio de las llamadas Guerras Revolucionarias Francesas.

Sin embargo, la situación económica seguía empeorando, lo cual dio origen a revueltas de las clases más pobres. Los llamados "sans-culottes" expresaban su descontento por el hecho de que la Revolución francesa no sólo no estaba satisfaciendo los intereses de las clases bajas sino que incluso algunas medidas liberales causaban un enorme perjuicio a éstas (libertad de precios, libertad de contratación, Ley Le Chapelier, etc.). Al mismo tiempo se comenzaron a gestar luchas antirrevolucionarias en diversas regiones de Francia. En la Vandea, un levantamiento popular fue especialmente significativo: campesinos y aldeanos se alzaron por el rey y las tradiciones católicas, provocando la llamada Guerra de Vandea, reprimida tan cruentamente por las autoridades revolucionarias parisinas que se ha llegado a calificar de genocidio. Por otra parte, la guerra exterior amenazaba con destruir la Revolución y la República. Todo ello motivó la trama de un golpe de estado por parte de los jacobinos, quienes buscaron el favor popular en contra de los girondinos. La alianza de los jacobinos con los "sans-culottes" se convirtió de hecho en el centro del gobierno.

Los jacobinos llevarían en su política algunas de las reivindicaciones de los sans-culottes y las clases bajas, pero no todas sus reivindicaciones serían aceptadas, y jamás se cuestionó la propiedad privada. Los jacobinos no pusieron nunca en duda el orden liberal, pero sí llevaron a cabo una democratización del mismo, pese a la represión que desataron contra los opositores políticos (tanto conservadores como radicales).

Se redactó en 1793 una nueva Declaración de los derechos del hombre y del ciudadano, y una nueva constitución de tipo democrático que reconocía el sufragio universal. El Comité de Salvación Pública cayó bajo el mando de Maximilien Robespierre y los jacobinos desataron lo que se denominó el Reinado del Terror (1793-1794). No menos de 10 000 personas fueron guillotinadas ante acusaciones de actividades contrarrevolucionarias. La menor sospecha de dichas actividades podía hacer recaer sobre una persona acusaciones que eventualmente la llevarían a la guillotina. El cálculo total de víctimas varía, pero se cree que pudieron ser hasta 40 000 los que fueron víctimas del Terror.

En 1794, Robespierre procedió a ejecutar a ultrarradicales y a jacobinos moderados.

La Convención aprobó una nueva Constitución el 17 de agosto de 1795, ratificada el 26 de septiembre en un plebiscito. La nueva Constitución, llamada Constitución del Año III, confería el poder ejecutivo a un Directorio, formado por cinco miembros llamados directores. El poder legislativo sería ejercido por una asamblea bicameral, compuesta por el Consejo de Ancianos (250 miembros) y el Consejo de los Quinientos. Esta Constitución suprimió el sufragio universal masculino y restableció el sufragio censitario.

La nueva Constitución encontró la oposición de grupos monárquicos y jacobinos. Hubo diferentes revueltas que fueron reprimidas por el ejército, todo lo cual motivó que el general Napoleón Bonaparte, retornado de su campaña en Egipto, diera el 9 de noviembre de 1799 un golpe de estado (18 de Brumario) instalando el Consulado.

La Constitución del Año VIII, redactada por Pierre Daunou y promulgada el 25 de diciembre de 1799, estableció un régimen autoritario que concentraba el poder en manos de Napoleón Bonaparte, para supuestamente salvar la república de una posible restauración monárquica. Contrariamente a las Constituciones anteriores, no incluía ninguna declaración sobre los derechos fundamentales de los ciudadanos. El poder ejecutivo recaía en tres cónsules: el primer cónsul, designado por la misma Constitución, era Napoleón Bonaparte, y los otros dos sólo tenían un poder consultivo. En 1802, Napoleón impuso la aprobación de un senadoconsulto que lo convirtió en cónsul vitalicio, con derecho a designar su sucesor.

El cargo de cónsules lo ostentaron Napoleón Bonaparte, Sieyès y Ducos temporalmente hasta el 12 de diciembre de 1799. Posteriormente, Sieyés y Ducos fueron reemplazados por Jean Jacques Régis de Cambacérès y Charles-François Lebrun, quienes siguieron en el cargo hasta el 18 de mayo de 1804 (28 de floreal del año XII), cuando un nuevo senadoconsulto proclamó el Primer Imperio y la extinción de la Primera República, cerrando con esto el capítulo histórico de la Revolución francesa.

Los tres colores azul, blanco y rojo eran ya frecuentes en diversos pabellones, uniformes y banderas de Francia antes del siglo XVIII. El azul y el rojo eran los colores de la villa de París desde el siglo XIV, y el blanco era en aquella época el color del reino de Francia, y por extensión de la monarquía borbónica.

Cuando Luis XVI visitó a la recién creada Guardia Nacional en el Ayuntamiento de París el 17 de julio de 1790, aparece por primera vez la escarapela tricolor, ofrecida al Rey por el comandante de la Guardia, el marqués de La Fayette. Unía la escarapela de la Guardia Nacional que llevaba los colores de la capital, con el color blanco del reino. No fue sin embargo hasta el 20 de marzo de 1790 que la Asamblea Nacional mencionó en un decreto los tres colores como "colores de la nación: azul, rojo y blanco". Pero la escarapela no era aún un símbolo nacional, y el primer emblema nacional como tal fue la bandera diseñada para la popa de los buques de guerra, adoptada por decreto de la Asamblea Nacional el 24 de octubre de 1790. Constaba de una pequeña bandera roja, blanca y azul en la esquina superior izquierda de una bandera blanca. Esta bandera fue modificada posteriormente por la Convención republicana el 15 de febrero de 1794, a petición de los marineros de la marina nacional que exigieron que se redujera la predominancia del blanco que simbolizaba todavía la monarquía. La bandera adoptó entonces su diseño definitivo, y se cambió el orden de los colores para colocar el azul cerca del mástil y el rojo al viento por motivos cromáticos, según los consejos del pintor Louis David.

Otro símbolo de la Revolución francesa es el gorro frigio (también llamado gorro de la libertad), llevado en particular por los Sans-culottes. Aparece también en los Escudos Nacionales de Francia, Haití, Cuba, El Salvador, Nicaragua, Colombia, Bolivia, Paraguay y Argentina.

El himno «La Marsellesa», letra y música de Claude-Joseph Rouget de Lisle, capitán de ingenieros de la guarnición de Estrasburgo, se popularizó a tal punto que el 14 de julio de 1795 fue declarado himno nacional de Francia; originalmente se llamaba «Chant de guerre pour l'armée du Rhin» («Canto de guerra para el ejército del Rin»), pero los voluntarios del general François Mireur que salieron de Marsella entraron a París el 30 de julio de 1792 cantando dicho himno como canción de marcha. Los parisinos los acogieron con gran entusiasmo y bautizaron el cántico como «La Marsellesa».

El lema "Liberté, égalité, fraternité" («Libertad, igualdad, fraternidad»), que procede del lema no oficial de la Revolución de 1789 "Liberté, égalité ou la mort" («Libertad, igualdad o la muerte»), fue adoptado oficialmente después de la Revolución de 1848 por la Segunda República Francesa.

Uno de los acontecimientos con mayor alcance histórico de la revolución fue la declaración de los derechos del hombre y del ciudadano. En su doble vertiente, moral (derechos naturales inalienables) y política (condiciones necesarias para el ejercicio de los derechos naturales e individuales), condiciona la aparición de un nuevo modelo de Estado, el de los ciudadanos, el Estado de Derecho, democrático y nacional. Aunque la primera vez que se proclamaron solemnemente los derechos del hombre fue en los Estados Unidos (Declaración de Derechos de Virginia en 1776 y Constitución de los Estados Unidos en 1787), la revolución de los derechos humanos es un fenómeno puramente europeo. Será la Declaración de Derechos del Hombre y del Ciudadano francesa de 1789 la que sirva de base e inspiración a todas las declaraciones tanto del siglo XIX como del siglo XX.

El distinto alcance de ambas declaraciones es debido tanto a cuestiones de forma como de fondo. La declaración francesa es indiferente a las circunstancias en que nace y añade a los derechos naturales, los derechos del ciudadano. Pero sobre todo, es un texto atemporal, único, separado del texto constitucional y, por tanto, con un carácter universal, a lo que hay que añadir la brevedad, claridad y sencillez del lenguaje. De ahí su trascendencia y éxito tanto en Francia como en Europa y el mundo occidental en su conjunto.

La declaración sin embargo excluyó a las mujeres en su consideración de ciudadanas y se olvidó de las mujeres en su proyecto igualitario. Dos años más tarde de la redacción de la Declaración de Derechos del Hombre y del Ciudadano la activista política Olympe de Gouges escribió la Declaración de los Derechos de la Mujer y la Ciudadana (1793) que se convierte en uno de los primeros documentos históricos que plantea la equiparación jurídica y legal de las mujeres en relación a los varones.

Las mujeres ocupan la calle durante las semanas precedentes a la insurrección y tuvieron un papel protagonista en el inicio de la Revolución. El 5 de octubre de 1789 fueron ellas quienes iniciaron la marcha hacia Versalles a buscar al rey. Sin embargo cuando las asociaciones revolucionarias dirigen el alzamiento las mujeres quedan excluidas del pueblo deliberante, del pueblo armado -la guardia nacional- de los comités locales y de las asociaciones políticas.

Al no poder participar en las asambleas políticas toman la palabra en las tribunas abiertas al público y crean los clubes femeninos en los que leen y debaten las leyes y los periódicos. Entre los más reconocidos estaba la Sociedad Patriótica y de Beneficencia de las Amigas de la Verdad (1791-1792) fundada por Etta Palm en el que se reclamaba educación para las niñas pobres, divorcio y derechos políticos.

Entre las revolucionarias más destacadas esta la dramaturga y activista política considerada precursora del feminismo, Olympe de Gouges que escribió la Declaración de los Derechos de la Mujer y la Ciudadana (1793) reivindicando la equiparación de derechos entre hombres y mujeres. Olympe se enfrentó a Robespierre y publicó la carta "Pronostic de Monsieur Robespierre pour un animale amphibie" que la llevó a ser acusada de intrigas sediciosas. Fue juzgada, condenada a muerte y guillotinada.

El 30 de septiembre de 1793 se prohibieron los clubes femeninos. En 1794 se insistió en la prohibición de la presencia femenina en cualquier actividad política y en mayo de 1795, la Convención prohibió a las mujeres asistir a las asambleas política ordenando que se retiraran a sus domicilios bajo orden de arresto si no cumplían lo prescrito. Finalmente el Código Napoleónico aprobado en 1804 consagró la derrota femenina en la lucha por la igualdad, libertad y fraternidad que la revolución significó para los varones.


"Este artículo incorpora material de las siguientes fuentes bajo dominio público:




</doc>
<doc id="22262" url="https://es.wikipedia.org/wiki?curid=22262" title="Microscopía">
Microscopía

La microscopía (o también sin tilde: «microscopia») es el conjunto de técnicas y métodos destinados a hacer visible los objetos de estudio que por su pequeñez están fuera del rango de resolución del ojo normal.

Si bien el microscopio es el elemento central de la microscopía, el uso del mismo se requiere para producir las imágenes adecuadas, de todo un conjunto de métodos y técnicas afines pero extrínsecas al aparato. Algunas de ellas son, técnicas de preparación y manejo de los objetos de estudio, técnicas de salida, procesamiento, interpretación y registro de imágenes, etc.

Exceptuando técnicas especiales como las utilizadas en microscopio de fuerza atómica, microscopio de iones en campo y microscopio de efecto túnel, la microscopía generalmente implica la difracción, reflexión o refracción de algún tipo de radiación incidente en el sujeto de estudio.

Antonie van Leeuwenhoek (Holanda, 1632-1723), un vendedor de telas, aficionado a pulir lentes, logró fabricar lentes lo suficientemente poderosas como para observar bacterias, hongos y protozoos, a los que llamó "animálculos".

El primer microscopio compuesto fue desarrollado por Zacharias Janssen. A partir de éste, los avances tecnológicos permitieron llegar a los modernos microscopios de nuestro tiempo, los que existen de varios tipos y son usados con diferentes fines. Cincuenta años después de la creación del microscopio, el inglés Robert Hooke perfecciona el microscopio; Hooke utiliza un microscopio compuesto para estudiar cortes de corcho y describe los pequeños poros en forma de caja, a los que él llamó "células". Publica su libro Micrographia.

Microscopía óptica ("microscopía de luz clásica"), consiste en hacer pasar luz visible de una fuente ("difractada, reflejada o refractada en el sujeto de estudio") a través de lentes ópticos simples o múltiples, para lograr una vista ampliada de la muestra. La imagen resultante puede ser detectada directamente por el ojo humano, impresa en una placa fotográfica o registrada y mostrada digitalmente ("y eventualmente almacenada en algún soporte digital"). En la fig. mo1 puede verse un microscopio estereoscópico ("adecuado principalmente para una visión binocular directa").



</doc>
<doc id="22263" url="https://es.wikipedia.org/wiki?curid=22263" title="Apertura angular">
Apertura angular

La apertura angular de una lente es el ángulo aparente de la apertura de la lente visto desde el punto focal:
formula_1
donde formula_2 es la distancia focal y formula_3 el diámetro de la apertura.



</doc>
<doc id="22267" url="https://es.wikipedia.org/wiki?curid=22267" title="Ernst Ruska">
Ernst Ruska

Ernst August Friedrich Ruska (Heidelberg, 25 de diciembre de 1906- Berlín, 25 de mayo de 1988). Fue un físico alemán que ganó el Premio Nobel de Física en 1986 por su trabajo en óptica electrónica, incluyendo el diseño del primer microscopio electrónico.

Ruska nació en Heidelberg, fue educado en la Universidad Técnica de Múnich de 1925 a 1927, luego ingresó a la Universidad Técnica de Berlín, donde postuló que los microscopios que usan electrones con longitudes de onda 1000 veces más corta que la de la luz visible, pueden proveer imágenes más detalladas de los objetos que los microscopios que utilizan luz, en los cuales la magnificación es limitada por el tamaño de las longitudes de onda. En 1931 demostró que una bobina magnética podría actuar como una lente electrónica, y usó varias bobinas en una serie para construir el primer microscopio electrónico en 1933.

Después de completar su doctorado en 1933, Ruska continuó trabajando en el campo de ópticas electrónicas, primero en Fernseh Ltd in Berlin-Zehlendorf, y luego desde 1937 en Siemens AG. En Siemens, se involucró en desarrollar el primer microscopio electrónico producido comercialmente en 1939. Al igual que desarrollando la tecnología del microscopio electrónico mientras en Siemens, Ruska también trabajó en otras instituciones científicas y animó a Siemens a establecer un laboratorio para investigadores visitantes que fue encabezado inicialmente por el hermano de Helmut Ruska, un médico que desarrolló el uso del microscopio electrónico para aplicaciones médicas y biológicas. Después de salir de Siemens en 1955, Ruska se desempeñó como director del Instituto de Microscopía Electrónica en el Instituto Fritz Haber de la Sociedad Max Planck hasta el año de 1974. Al mismo tiempo, trabajó en el instituto y como profesor en la Universidad Técnica de Berlín, desde 1957 hasta su retiro en 1974.

En 1986, fue galardonado con la mitad del Premio Nobel en Física por sus muchos logros en ópicas electrónicas; Gerd Binnig y Heinrich Rohrer ganaron un cuarto del premio para cada uno por su diseño del microscopio de efecto túnel. Ruska murió en Berlín Occidental en 1988.



</doc>
<doc id="22268" url="https://es.wikipedia.org/wiki?curid=22268" title="Roncesvalles">
Roncesvalles

Roncesvalles (en euskera Orreaga y oficialmente Orreaga/Roncesvalles) es un municipio español de la Comunidad Foral de Navarra, situado en la merindad de Sangüesa, en la comarca de Auñamendi y a 47,7  km de la capital de la comunidad, Pamplona. Su población en era de  habitantes (INE).

El puerto de Roncesvalles correspondía antiguamente al collado axial de Ibañeta (1066 m), vía de paso natural que se utilizó desde la prehistoria para acceder a la península ibérica. El punto de mayor altitud del municipio es la cima del monte Orzanzurieta, con 1567 msnm.

Las casas e instituciones religiosas y de atención a los peregrinos jacobeos se hallan en el pueblo de Roncesvalles, situado al pie de Ibañeta, donde arranca la famosa llanada en la que los cantares de gesta ubican la batalla contra los carolingios. Roncesvalles, al cabo del tiempo, sigue siendo enclave fundamental para los peregrinos del Camino de Santiago. Por Ibañeta y Roncesvalles entra el llamado Camino Francés, el mismo que recorrió Aymeric Picaud en el siglo XII, el cual se funde en la villa de Obanos, muy cerca de Puente la Reina, con el otro que procede del Somport de Huesca, también en los Pirineos, conocido por Camino Aragonés.

El municipio de Roncesvalles limita por el norte, en el collado de Ibañeta, con el de Valcarlos, municipio navarro transpirenaico, y por el sur con el de Burguete, la que fue primera población de la comarca, llamada entonces Burgo de Roncesvalles. Al este limita con el municipio de Orbaiceta.

Roncesvalles fue de siempre vía de paso para entrar en la península ibérica. Por Roncesvalles penetraron fundamentalmente los celtas, los bárbaros (409), los godos que se establecieron a lo largo de la cuenca del Duero, y naturalmente el rey Carlomagno con el más poderoso ejército del siglo VIII, camino de la ciudad de Zaragoza. Carlomagno, dado que fue derrotado en Zaragoza, decidió, camino de su reino, reducir a ruinas la capital de los vascones, Pamplona. Fue al regreso, en los Pirineos, entre el collado de Ibañeta y la hondonada de Valcarlos, donde hubo de sufrir una contundente emboscada por partidas de nativos vascones, a los que les resultó fácil provocar un descalabro general a base de lanzar rocas y dardos. La "Chanson de Roland", escrita en algún lugar de Francia hacia finales del siglo XI, concibió el desastre en el llano, entre Roncesvalles y la villa de Burguete, y los atacantes ya no eran vascones, sino sarracenos, quienes en realidad nunca llegaron a expandir sus dominios tan al norte.

Estos son los últimos alcaldes de Roncesvalles:
La economía de Roncesvalles se basa en las rentas que se obtienen de las tierras del municipio (explotación forestal principalmente), de alguna explotación agrícola-ganadera existente en el municipio y sobre todo de las actividades hosteleras.

La economía local está hoy en día orientada a la atención de los peregrinos y turistas, debido a la condición de Roncesvalles como tradicional primera etapa en España del Camino de Santiago.

A pesar de su escasa población, el pueblo cuenta con una notable oferta de hospedaje: un hotel, un edificio de apartamentos turísticos, dos hostales y un refugio de peregrinos que depende de la Colegiata. Roncesvalles posee una oficina de turismo, un museo y una oficina del peregrino. El hotel y los hostales poseen además bar-restaurante.

En cualquier caso, el mantenimiento del complejo histórico-artístico excede los recursos municipales y exige la aportación económica de instancias oficiales.

Roncesvalles está comunicada por carretera por medio de autobuses con origen y destino en Pamplona y otras localidades de Navarra. Además, hay servicio de taxis de varias localidades de alrededor.

El Hospital fundado por el obispo de Pamplona Sancho Larrosa, con la colaboración de Alfonso I el Batallador y algunos nobles. Los papas lo tomaron desde un principio bajo su protección. Desde su fundación lo ha regido un Cabildo de canónigos regulares de San Agustín. En 1984 pasó a depender del arzobispado de Pamplona. El Prior sigue ostentando el título medieval de Gran Abad de Colonia. El cargo de «hospitalero» lo lleva un canónigo. En el siglo XVII se repartían 25 000 raciones anuales entre los peregrinos.

El hospital que existe actualmente fue diseñado en 1792 por el arquitecto José Poudez, levantándose entre 1802 y 1807 con los criterios de la arquitectura neoclásica. Consiste en un gran bloque horizontal con tres plantas hacia el patio y cuatro hacia el este, apenas marcado por ventanales cuadrangulares, y al que se accede por un portal con arco de medio punto enmarcado por pilastras, friso y frontón triangular.

También conocida como Silo de Carlomagno por suponerse que su origen se debe al enterramiento de combatientes francos caídos en el 778, lo que no es inverosímil. Se remonta al siglo XII, por lo que está considerada la edificación más antigua de Roncesvalles.

El Sancti Spiritus hay que considerarlo templo funerario, pero no fue lugar de enterramiento perpetuo en el medievo. Era el recinto en que se oficiaban misas por los peregrinos fallecidos en el hospital que, enterrados en otro lugar, una vez transcurrido un tiempo sus restos eran depositados en el osario bajo la capilla exenta.

Pequeña iglesia gótica del siglo XIII, situada junto al Silo de Carlomagno. Es una sencilla fábrica de planta rectangular con dos tramos que incluyen la cabecera recta y bóveda de crucería simple. Unas columnas de fuste cilíndrico sirven de soporte para la cubierta. En su interior hay una figura del Apóstol Santiago. El exterior tiene muros de sillar irregular, sin contrafuertes, con una portada de arco apuntado y Crismón.

Fue utilizada como parroquia hasta el siglo XVIII. Quedó sin culto durante un largo periodo hasta que fue restaurada por Florencio Ansoleaga en el siglo XX, quien abrió un pequeño óculo sobre la puerta e incorporó la mítica y legendaria campana que orientaba a los peregrinos en la capilla que había en el collado de Ibañeta: la capilla de San Salvador.

La iglesia colegiata de Santa María es la fábrica más lujosa de Roncesvalles y el mejor ejemplo navarro del gótico, no sólo francés, sino del más puro de l'Île de France. Acoge una preciosa imagen de la Virgen del siglo XIV.
El templo actual se construyó gracias a Sancho el Fuerte (1194-1234), quien lo eligió como lugar de enterramiento. No hay datos concretos sobre las fechas de la construcción de la iglesia, pero se sabe que fue a principios del siglo XIII, entre 1215 y 1221.

La Colegiata sufrió importantes desperfectos, ocasionados principalmente por varios incendios ocurridos en 1445, 1468 y 1626. A comienzos del siglo XVII, su estado de deterioro y casi abandono propició su reconstrucción, que abarcó todo el recinto colegial, especialmente a la iglesia y al claustro. Se enmascaró el interior gótico y se le dio forma barroca salvo en el presbiterio y el tramo de nave que le precede, donde quedaron a la vista los elementos góticos.

De planta cuadrada cubierta con bóveda de terceletes con ligaduras de nervios más elaborados que los de la iglesia y las claves decoradas. La bóveda se apoya en cuatro ménsulas de gran tamaño que representan unos ángeles.

El exterior se presenta como un bloque cúbico de sillería, con cierto aspecto de fortaleza; de ahí que en ocasiones se le denomine torre de San Agustín. Unos contrafuertes adosados a las esquinas que llegan hasta la cubierta piramidal refuerzan el conjunto, que data del siglo XIV. En el centro de la capilla se sitúa el sepulcro de Sancho VII el Fuerte.

También hay que reseñar una serie de esculturas relacionadas con las obras del claustro de la Catedral de Pamplona. Se trata de dos capiteles que representan el Pecado original y la Expulsión del Paraíso, que cabe pensar formaron parte del claustro gótico.

A unos cien metros de la capilla del Sancti Spiritus, al borde de una pequeña hondonada de verdes pastizales, se halla una edificación envuelta en misterio. Domenico Laffi había escrito que la capilla funeraria estaba muy cerca del hospital de peregrinos. La situaba a occidente, lo que parece coincidir con el emplazamiento de Itzandegia. «Es un gran y bello hospital en el que los peregrinos pueden permanecer tres días. Pueden comer y dormir, y los tratan muy bien».

El edificio es una casona de piedra de 32 x 12 metros. Consta de nave única de seis tramos, cuya techumbre sostienen cinco arcos apuntados que descansan sobre los muros, que a su vez se apoyan en diez contrafuertes, cinco por cada lado. Tiene dos accesos, uno mayor, ancho como para el paso de carros, vuelto de espaldas a Roncesvalles, alzado casi un metro sobre el suelo, desnivel que no existiría hasta tanto no fue edificada la casa casi adosada. La otra puerta, menor, en el lateral derecho, permite el paso a la única planta, que en otro tiempo debió de contar con otra superior.

Es edificio ciego, salvo la escasa luz que dejan pasar seis aspilleras, verticales y estrechas, en lo alto del muro que da al mediodía. La última restauración terminó con el Xacobeo 1993. La mayor parte de los contrafuertes exteriores había desaparecido al igual que los arcos de la bóveda, «recuperándose sobre el modelo de los tres que aún se conservaban», anotaron los profesores Miranda y Ramírez. Pero no parece que fuese así, porque por algunas fotografías de antaño puede comprobarse cómo la estructura medieval había sido alterada, adaptada a los requerimientos del caserío rural en que se había convertido, con amplios ventanales, puertas para distintos cometidos y dependencias adyacentes. Ni siquiera estaban los gruesos contrafuertes.

Ocupan un edificio yuxtapuesto a la Casa Prioral formando un bloque horizontal. Consta de tres niveles en altura y un reducido ático de óculos. En el segundo cuerpo se abre una arcada sobre pilastras acanaladas.

La Biblioteca

Comprende más de 15 000 volúmenes de todo tipo de materias, aunque destacan las obras sobre cuestiones teológicas, filosóficas y de historia eclesiástica. Hay volúmenes en distintas lenguas: hebreo, griego, latín, vascuence e incluso chino. Algunas de las piezas más interesantes, como el códice "La Pretiosa" (del siglo XIV), se exponen en el Museo de la Colegiata.

Todavía conserva una importante sección de Archivo Histórico conformado a lo largo de los casi nueve siglos de existencia del hospital, que incluye pergaminos, libros de administración, documentos relativos a la historia interna y las repercusiones exteriores de la vida capitular, etc.

El Museo
Situado en la planta baja del edificio, recoge gran cantidad de objetos de arte representativos de la Colegiata, que incluyen escultura, pintura y orfebrería, así como muebles, tapices, monedas y libros de gran interés bibliográfico.

En escultura destacan la estatua de una figura femenina sedente, gótica del siglo XIV, y una talla de San Miguel fechada en el segundo tercio del siglo XVI. Asimismo, algunos relieves y estatuas que formaban parte del retablo mayor de la Colegiata, realizado entre 1618 y 1624.

En pintura lo más destacable es el tríptico de la Crucifixión (escuela noreuropea del siglo XVI), cedido a la Colegiata en 1720 por doña Jerónima Jiménez de Esparza, de origen aparentemente flamenco. Una tabla de la Sagrada Familia, realizada por Luis de Morales, que guarda semejanza con la de la Catedral Nueva de Salamanca. El lienzo del martirio de San Lorenzo, barroco de la primera mitad del siglo XVII, así como otro de Judith portando la cabeza de Holofernes.

En orfebrería hay que resaltar una hermosa arqueta de plata dorada, cubierta de fina labor de filigrana y fechada entre 1274 y 1328. Existe otra arqueta de plata parcialmente dorada, que se ha fechado en el siglo XVI, cuyo interés radica en que aprovecha medallones y relieves de la época medieval.

Figuran también los relicarios y especialmente el denominado «Ajedrez de Carlomagno», llamado así por su disposición en damero. Esta pieza, adscrita al gótico de la segunda mitad del siglo XIV, está formada por un alma de madera, forrada de láminas de plata parcialmente dorada, esmaltes translúcidos y vidrio.

La famosa esmeralda de Miramamolín, que se identifica, según la leyenda, con la que Sancho VII el Fuerte arrebató del turbante al rey moro en la Batalla de las Navas de Tolosa, a raíz de la cual se incorporó, como un símbolo, al escudo de Navarra. El evangelario en el que juraban fidelidad los reyes de Navarra, obra de orfebrería románica.

Los topónimos latinos y romances, empleados desde la Edad Media para referirse al enclave pirenaico, son muchos y variados, si bien algunos hay que considerarlos parcialmente erróneos a causa de malas interpretaciones de copistas y de personajes muy alejados de Navarra, a grafías derivadas de otras equivocadas o a intentos por enmendar lo que se suponía que estaba mal escrito. Sin agotar la lista, he ahí los concernientes a Roncesvalles:

Ibañeta también concitó múltiples referencias, mucho más imprecisas debido a las reminiscencias legendaristas de paraje tan especial:

El alto de Valcarlos se conoció como:
Este topónimo no debe confundirse con el Port de Cize propuesto por Picaud, relacionado con la travesía romana de las cumbres. El Valcarlos propiamente dicho deriva de Vallis Caroli y Karlestal, espacio que cabe encuadrar entre la frontera internacional de Arnéguy y el portillo de Moccosalia, donde la tradición supuso acampando a Carlomagno mientras los vascones aniquilaban a la retaguardia.

Existe una importante devoción por la Virgen de Roncesvalles en todo el Pirineo navarro. El 8 de septiembre se festeja el Día de la Virgen de Roncesvalles, patrona de Roncesvalles.

Durante los domingos y festivos de mayo y junio suelen acudir en romería las parroquias de los valles y pueblos del entorno e incluso de Pamplona.






</doc>
<doc id="22269" url="https://es.wikipedia.org/wiki?curid=22269" title="Termómetro">
Termómetro

El termómetro (del griego θερμός ["thermos"], «calor», y μέτρον ["metron"], «medida») es un instrumento de medición de temperatura. Desde su invención ha evolucionado mucho, principalmente a partir del desarrollo de los termómetros electrónicos digitales.

Inicialmente se fabricaron aprovechando el fenómeno de la dilatación, por lo que se prefería el uso de materiales con elevado coeficiente de dilatación, de modo que, al aumentar la temperatura, su estiramiento era fácilmente visible. La sustancia que se utilizaba más frecuentemente en este tipo de termómetros ha sido el mercurio, encerrado en un tubo de vidrio que incorporaba una escala graduada, pero también alcoholes coloreados en termómetros grandes.

El creador del primer termoscopio fue Galileo Galilei; este podría considerarse el predecesor del termómetro. Consistía en un tubo de vidrio terminado en una esfera cerrada; el extremo abierto se sumergía boca abajo dentro de una mezcla de alcohol y agua, mientras la esfera quedaba en la parte superior. Al calentar el líquido, este subía por el tubo. 

La incorporación, entre 1611 y 1613, de una escala numérica al instrumento de Galileo se atribuye tanto a Francesco Sagredo como a Santorio Santorio, aunque es aceptada la autoría de este último en la aparición del termómetro.

En España se prohibió la fabricación de termómetros de mercurio en julio de 2007, por su efecto contaminante.

En América latina, los termómetros de mercurio siguen siendo ampliamente utilizados por la población. No así en hospitales y centros de salud donde por regla general se utilizan termómetros digitales.

La escala más usada en la mayoría de los países del mundo es la Celsius (°C) en honor a Anders Celsius (1701-1744) que se llamó "centígrado" hasta 1948. En esta escala, el cero (0 °C) y los cien (100 °C) grados corresponden respectivamente a los puntos de congelación y de ebullición del agua, ambos a la presión de 1 atmósfera.

Otras escalas termométricas son:


Para medir ciertos parámetros se emplean termómetros modificados, tales como los siguientes:

El termógrafo es un termómetro acoplado a un dispositivo capaz de registrar, gráfica o digitalmente, la temperatura medida en forma continua o a intervalos de tiempo determinado.

La siguiente cronología muestra los avances en las tecnologías de medición de temperatura:




</doc>
<doc id="22280" url="https://es.wikipedia.org/wiki?curid=22280" title="Fotografía analógica">
Fotografía analógica

La fotografía analógica, de rollo o de carrete, también conocida como fotografía tradicional, argéntica o química, es el retrónimo con el que se describe al proceso fotográfico tradicional, en comparación con la fotografía digital, de aparición más reciente. Se basa habitualmente en un proceso físicoquímico que involucra el uso de un material fotosensible activo (aplicado sobre placas de vidrio o sobre una película flexible de material traslúcido, actualmente plástico) y su estabilización (revelado), para la obtención y el procesado de las imágenes.

Para la obtención de imágenes fotográficas se emplea un soporte conocido como película fotográfica, en donde el elemento sensible a la luz es el halogenuro de plata, el cual es el compuesto activo presente en la emulsión fotográfica; esta, a su vez, es un coloide en suspensión, sobre una base de gelatina muy pura. El tamaño y cantidad de los cristales de halogenuro de plata determinan la sensibilidad de la película, conocida también como velocidad (término que puede confundirse con la velocidad de obturación), la cual está normalizada y se expresa en una escala de sensibilidad fotográfica estandarizada por la ISO. Cuando se abre el obturador por un breve instante, la luz que pasa por el objetivo incide sobre la película, y deja sobre ella la impresión de la imagen, que en este punto recibe el nombre de imagen latente; ésta se irá descomponiendo a partir de ese momento, hasta ser revelada. En realidad la luz da inicio a un proceso físicoquímico, produciendo un punto de sensibilidad en el compuesto, obteniendo así una imagen latente, lo que a la postre, cuando la película se sumerja en el revelador, mediante un proceso de reducción-oxidación, ocurrirá la descomposición del halogenuro en plata metálica negra, obteniéndose así una imagen visible.

El proceso de revelado consta de dos pasos básicos: revelado y fijado, los cuales se dividen a su vez en pasos intermedios según el tipo de película a tratar. La imagen así obtenida tiene sus valores de luz invertidos respecto a la captura original, por lo cual se conoce a la película tratada como negativo.

Una vez seca, de esta película o "negativo" se pueden hacer copias de la imagen sobre papel o bien sobre otra película, en cuyo caso obtendremos una diapositiva o positivo traslúcido, que nos permitirá observar la fotografía por proyección o transparencia. Las imágenes obtenidas, al invertir nuevamente los valores de luz, por ampliación o contacto, nos dan como resultado un "positivo".
A este proceso se le llama positivado.

Si utilizamos en la cámara una película especialmente tratada, "para diapositivas", obtendremos las imágenes directamente en positivo al revelar la película.

El formato más popular de película química es la película de 35mm (también conocida como película 135), utilizada en la mayoría de cámaras réflex y compactas hasta el final del siglo XX. Después de dicho formato, los más populares son el formato medio (120, 220), Polaroid (de revelado instantáneo), y los grandes formatos (4x5 ", 5×7" y 8×10" principalmente). Último formato en aparecer Advanced Photo System (conocido mejor por su acrónimo, APS); Que permitía exponer su película en formatos C / H / P . Aunque posiblemente el menos popular de todos los formatos y sin embargo, sus dimensiones se usaron de base para los primeros sensores en fotografía digital. SLR hasta su evolución a FULL FRAME.



</doc>
<doc id="22283" url="https://es.wikipedia.org/wiki?curid=22283" title="Arnedo">
Arnedo

La ciudad de Arnedo está situada en La Rioja (España), en la comarca de la Rioja Baja, con 14.597 habitantes (INE 2015). Está bañada por el río Cidacos (afluente del Ebro). La economía depende fundamentalmente de la industria del calzado. Su nombre proviene posiblemente del latín "Arenetum", colectivo de "arena".

La ciudad de Arnedo está situada en el valle medio del Cidacos, río afluente del Ebro por su derecha, en la comarca de La Rioja Baja. Con 14.597 habitantes (INE 2015) es la tercera ciudad en población de La Rioja, de cuya capital, Logroño, dista 48 km.
El término municipal de Arnedo tiene 86,8 km² que se extienden a ambas márgenes del río Cidacos, aunque su núcleo urbano se halla en la izquierda. Incluye desde 1975 el anterior municipio de Turruncún, hoy deshabitado. Su altitud media sobre el nivel del mar es de 523 m que marcan el comienzo de la sierra hacia el sur.

Arnedo se halla en la denominada Hoya de Arnedo, en la que penetra el río Cidacos por Arnedillo y sale por Autol, determinando dos márgenes desiguales: la derecha mucho más extensa y escarpada está dominada por la peña Isasa, de 1.474 m de altura; y la izquierda con paredes arcillosas de un rojizo característico, en la que a pesar de su poco espacio se sitúan la mayoría de las poblaciones y de las cuevas artificiales horadadas por el hombre.

La Hoya está enmarcada y cerrada por un conjunto de sierras de conglomerados y areniscas que pueden ser el origen del nombre de la ciudad Arenetum:

El clima de Arnedo es de transición entre mediterráneo y atlántico. Se caracteriza por ser templado-frío, lleno de contrastes y varía de un año a otro. En general es un clima agradable, aunque se pueden registrar temperaturas superiores a los 35 grados en julio y agosto e inferiores a los 0 grados en enero.

El cierzo (viento norte) y el bochorno (viento sur) son los vientos propios de la zona. En lo que se refiere a la velocidad, aunque predominan las jornadas de vientos débiles y en calma, hay días en que pueden alcanzar rachas importantes.

A 1 de enero de 2015 la población del municipio ascendía a 14.597 habitantes, 7.222 hombres y 7.375 mujeres. A continuación se expone gráficamente cómo ha evolucionado el número de habitantes de la ciudad a lo largo de la época estadística: 

En su término municipal se encuentra el despoblado de Turruncún.

Parece indudable que Arnedo ha estado poblado desde tiempos muy antiguos; los restos arqueológicos se remontan al neolítico.

Hay historiadores que afirman que el nombre de Arnedo en tiempos prerromanos era Sadacia o Sidacia, nombre que habría quedado en el río Cidacos, pero su actual denominación deriva etimológicamente del término latino «Arenetum», que viene a significar «lugar de arena», y hace referencia a la plataforma arenosa en la que se asienta la ciudad.

Durante la prehistoria se ubican a la orilla del río y en las montañas, los más antiguos asentamientos que se conocen. Son el de San Pedro Mártir y el Valpineda, con restos de época neolítica, quizá del Bronce inicial: materiales líticos de sílex y cerámicas hechas a mano.

De la época prerromana (del 3000 a. C. al siglo II a. C.) los testimonios arqueológicos en los cerros de San Miguel y El Raposal parecen indicar un hábitat concentrado por pobladores celtíberos, quizás berones o pelendones, que desaparecerían a la llegada de los romanos en el siglo II a. C. Del pueblo celtíbero quedan importantes testimonios: un poblado, un horno y restos de cerámicas aparecidos en la colina de San Miguel.

En la época romana (del siglo II a. C. al siglo V) no se cita Arnedo en su historiografía y hay que recurrir a la arqueología, así se supone que el poblado se traslada al pie del cerro del Castillo, donde aparecieron restos de terra sigillata, allí los romanos podrían haber levantado una fortificación (base del futuro castillo medieval) que protegía un importante nudo de comunicaciones; pues en Arnedo se cruzaban las calzadas romanas que unían Calahorra con Numancia, y la de Contrebia Leucade (yacimiento de Inestrillas, en la cuenca del río Alhama) con Varea, la Vareia romana.

De la época visigótica se conservan restos de una iglesia rupestre del siglo VI en las afueras del pueblo, junto al Monasterio de Nuestra Señora de Vico, y la Cueva de los Cien Pilares, bajo el cerro de San Miguel, donde hubo un monasterio.

Arnedo pasó a ser ocupada por los árabes, muestra de ello es el Castillo, que se eleva en un imponente cerro controlando y dominando la ciudad y sus límites. Según el geógrafo árabe Idrisí, Arnedo fue capital de una de las 26 provincias árabes de España en el siglo VIII y desde donde se desempeñó un importante papel en las luchas de conquista y Reconquista.

La falta de documentación cristiana hace de las crónicas musulmanas la mejor fuente de información para este periodo, en el que la población fue reconquistada por Sancho Garcés I entre los años 908-909. Se le concedió fuero propio y surgió la leyenda de la aparición de la Virgen de Vico al moro Kam. En 1264 quedó vinculado a la Corona de Castilla.

A finales del siglo XIV Arnedo adquiere renombre al celebrarse en 1385 conferencias diplomáticas que son el origen del título de Príncipe de Asturias, y firmarse en 1388 el llamado Tratado de Arnedo entre Francia y Castilla para defensa mutua. En el siglo XV se fundan en Arnedo una de las primeras cajas de ahorro que se conocen en el mundo, así como el monasterio de Vico.

En los siguientes siglos varios reyes distinguieron a la población: Felipe III le concede el derecho de fielazgo y garapitería, Felipe IV la exime de alojar gente de guerra y concede el Título de ciudad, y Carlos III concede una feria de... "nueve días y mercado el día lunes de cada semana".

En 1654 recibió el título de Ciudad concedido por el entonces rey Felipe IV.

En el siglo XIX la ciudad contaba ya con una industria alpargatera y otras actividades fabriles: fábricas de jabón y aguardientes, tenerías, alfarerías, una imprenta, etc.

Durante el pasado siglo XX en los aspectos sociales y políticos (véase Sucesos de Arnedo) la ciudad ha seguido los acontecimientos del resto de España; pero se ha caracterizado particularmente por el inicio en los años veinte de la industria del calzado con un gran desarrollo posterior y de otras industrias auxiliares: prefabricados, caucho, cartonajes, etc.; causa de la llegada de población desde otros lugares de España y en los últimos años desde otros países.

Se conoce de esta manera a los acontecimientos que se produjeron entre 1931 y 1932 a partir de unos despidos en una fábrica de calzados y terminaría con la muerte de once personas por disparos de la Guardia Civil. Estos hechos causaron una gran polémica, añadidos a otros casos de matanzas de campesinos y obreros desarmados por parte de efectivos de la Guardia Civil como en Castilblanco (anterior) o Casas Viejas (posterior). Arnedo se convirtió en un símbolo para anarquistas y republicanos de izquierda radical.

La ciudad de Arnedo basa su economía principalmente en la industria del calzado, siendo utilizado este pretexto como eslogan de la ciudad (Arnedo, ciudad del calzado) y como atracción turística (museo del calzado).

Tiene un importante y amplio tejido industrial relacionado con el calzado, del que existen diversas variedades. Entre ellas se encuentran una gran cantidad de marcas populares propias de Arnedo: firmas como Pitillos, Flossy, Callaghan, Fluchos, Chiruca (Fal), Gorila, etc. También esta ciudad es líder nacional en la fabricación de Calzado de Seguridad; todo ello representado en la Asociación de Industrias del Calzado y Conexas de La Rioja, AICCOR. Arnedo cuenta desde el 2007 con el Centro Tecnológico del Calzado de La Rioja, que mejora la competitividad y aplica la I+D+i al sector.

Esta economía se complementa con otras como son las bodegas (Bodega Cooperativa Nuestra Señora de Vico), cartonaje, transformación de caucho, trujal (Trujal 5 valles), comercios de servicio... La agricultura se dirige al autoconsumo, por lo que destaca el regadío (sobre todo cerca del río), la vid, los olivos y los almendros.

Asimismo, en lo que a energía se refiere, en el municipio de Arnedo se encuentra uno de los mayores parques solares de España, construido por Isolux-Corsán y propiedad de T-Solar.

El concepto de deuda viva contempla sólo las deudas con cajas y bancos relativas a créditos financieros, valores de renta fija y préstamos o créditos transferidos a terceros, excluyéndose, por tanto, la deuda comercial.

El cerro del Castillo que domina el este de la población, se debió fortificar desde época romana al pasar por allí diversas calzadas. Al llegar los árabes en el 714 reconstruyen el castillo sobre esa hipotética fortaleza romana anterior, y tuvo un importante papel durante la Edad Media en las luchas de conquista y reconquista reparándose en el s. XIX. Con él se pueden relacionar los túneles o minas que atraviesan el subsuelo de Arnedo.
La ciudad posee tres iglesias situadas en el Casco Antiguo. Cada una de ellas destaca por distintos valores artísticos:

La ciudad conserva un gran número de casas-palacio en su mayoría barrocas que se sitúan en el Casco Antiguo que merecen la pena conocer. Entre ellas destacan el Palacio del Arzobispo Argaiz, actual Casa de Cultura (biblioteca municipal, salas de exposiciones...) y el Palacio de la Baronesa, conocido como La Casa de Arte en la que se desarrollan actividades musicales y artísticas.


La gastronomía de la zona recoge la unión entre las gastronomías de las diferentes culturas que han estado conviviendo en el territorio. Cabe destacar la importancia de los árabes los cuales dejaron importantes postres como los fardelejos.

También destacan en la gastronomía de la población los frutos de sus huertas. Las familias de Arnedo suelen poseer pequeñas parcelas con hortalizas y las cultivan como afición y para su autoabastecimiento. En las fiestas del municipio los manjares se disfrutan en la calle y es usual ver a gente realizando unas chuletas asadas al sarmiento, o unos ricos ranchos.

En las fiestas de marzo, fácilmente se puede hallar en las brasas de las chuletadas un hueco para asar ajos, cebollas y huevos. Actualmente se celebra cada año en Arnedo el Festival del ajo asado, generalmente en marzo y abril, el mes depende de cuando caiga semana santa.

En julio también se celebra "A que te leo". Una especie de festival de lecturas que tiene lugar por las noches en el parque que hay junto al río Cidacos.

Las fiestas patronales de San Cosme y San Damián comienzan el día 26 de septiembre y terminan el día 2 de octubre. El día 27, Arnedo recibe a los navarros que vienen a animar el día cantando por la mañana de madrugada, y asistiendo después a misa.

La razón de estas fiestas es que (según la leyenda) los navarros creen ser dueños de los Santos San Cosme y San Damián alegando que los arnedanos se los robaron. Todos los años un grupo de valientes navarros (provenientes de pueblos cercanos como Andosilla, San Adrián) intentan recuperar «sus» santos. Este acto se realiza en un recorrido con diferentes estaciones en las que un orador navarro utiliza su labia para dar razones de que los santos están mejor en Navarra. Con el grito «¡A Navarra con ellos!», los navarros intentan zafarse con los santos pero un grupo de arnedanos los detienen y no los dejan escapar. Con el ánimo de volver a intentarlo al próximo año los navarros culminan la jornada.

Las fiestas de Arnedo se organizan por medio de las peñas y asociaciones como Lubumbas, La Chispa, Tao y Quincalla. Por las mañanas se ofrecen desayunos gratis y pasacalles con las charangas antes del comienzo del encierro. También hay concurso de subir por el mayo o cucaña a por el jamón, marionetas para los niños, deportes tradicionales... y el Zapato de Oro.

El Zapato de Oro es una gran novillada codiciada por los novilleros. Se suele decir que para ser un gran novillero y tomar la alternativa, tener el Zapato de Oro es un buen comienzo. Se celebraba en la centenaria antigua plaza de toros de Arnedo. Este trofeo lo poseen celebridades como Enrique Ponce, Jesulín de Ubrique, Diego Urdiales, etc.


Denominado tradicionalmente como feudo socialista, por las sucesivas y abultadas victorias de José María León Quiñones desde 1987, en las elecciones del año 2003 se produjo un empate a concejales entre el PSOE y el PP, lo que permitió al Partido Riojano pactar con el PP y desbancar al PSOE del Ayuntamiento.

En 2007, el PP conseguiría hacerse con la mayoría absoluta (9 ediles), que no pudo revalidar en 2011, aunque si accedió al gobierno con sus 8 concejales de la mano nuevamente del Partido Riojano (PR) con dos.

En 2015, el Partido Socialista recuperaría de nuevo la alcaldía con Javier García Ibáñez, que con 32 años es el Alcalde más joven de la democracia en Arnedo.





</doc>
<doc id="22292" url="https://es.wikipedia.org/wiki?curid=22292" title="TGV">
TGV

El TGV ("Tren de Gran Velocidad", del francés, "Train à Grande Vitesse") es una serie de trenes de alta velocidad desarrollada por Alsthom (actualmente Alstom) y la compañía de ferrocarriles nacional francesa SNCF, y operada principalmente por la propia SNCF. También se denomina TGV a los servicios realizados con este tipo de trenes, y en general, a la alta velocidad en Francia.

La inauguración del servicio de este tren tuvo lugar con el trayecto entre París y Lyon en 1981. Actualmente la red del TGV conecta París con otras ciudades de Francia y con sus países vecinos. Se dedica principalmente al transporte de pasajeros, aunque existe una versión postal. Entre 1981 y 2013 la red de TGV transportó dos mil millones de pasajeros.

El TGV es uno de los trenes convencionales más veloces del mundo; en algunos tramos opera a velocidades de hasta 320 km/h. Tiene el récord de mayor velocidad en condiciones especiales de prueba cuando el 3 de abril de 2007 superó su propio registro al llegar a los 574,8 km/h en la línea París-Estrasburgo.

El éxito de la primera línea favoreció la expansión del servicio, con nuevas líneas construidas hacia el sur, oeste y noreste de Francia. Deseosos de imitar el éxito de la red francesa, algunos países vecinos como Bélgica, Italia o Países Bajos construyeron también sus propias líneas de alta velocidad. Actualmente, las líneas francesas de alta velocidad enlazan con las de Bélgica, Alemania, Países Bajos, Luxemburgo, Italia, Suiza, España e incluso, con las del Reino Unido. Otras redes de alta velocidad utilizan trenes derivados de los TGV (KTX, S100...).

Son trenes diseñados para circular por líneas de alta velocidad, que disponen de algunas características especiales, como capacidad para alimentar trenes de gran potencia, traviesas y aparatos de vía especiales o sistemas de señalización en cabina que eliminan la necesidad del maquinista de identificar las señales a gran velocidad. También pueden circular por líneas convencionales a menor velocidad.

El TGV ha absorbido una gran cantidad de los desplazamientos nacionales que antes se realizaban en avión, debido a la reducción del tiempo de viaje, especialmente para los trayectos de menos de 3 horas. El viaje en tren se completa en un tiempo menor por la ausencia de tiempos de espera propios de los aeropuertos (facturación, controles de seguridad, embarque, retrasos...). También le beneficia la localización de las estaciones, situadas dentro de las ciudades. Por otra parte, el TGV es un medio de transporte muy seguro, que no ha tenido apenas accidentes al circular por líneas de alta velocidad.

La idea de implantar el TGV fue propuesta por primera vez en los años 1960, después de que Japón hubiese empezado la construcción del Shinkansen en 1959. El japonés Shinkansen fue el primer tren de alta velocidad conectando Tokio con Osaka y se puso en servicio el 1 de octubre de 1964, aproximadamente 17 años antes que los primeros TGV. Al mismo tiempo, el gobierno francés favorecía las investigaciones en nuevas tecnologías, profundizando en campos como la producción del hovercraft o los trenes magnéticos, como el aerotrén. Simultáneamente, SNCF empezó a investigar acerca de la operatividad de trenes de alta velocidad sobre vías tradicionales.

Originalmente estaba planeado que el TGV, entonces acrónimo de "très grande vitesse" (muy alta velocidad) o "turbine grande vitesse" (turbina de gran velocidad), estuviese propulsado por locomotoras de gas turboeléctricas. Las turbinas de gas fueron seleccionadas por su reducido tamaño, su buena relación de potencia por peso y la capacidad de administrar una gran salida de potencia durante un largo período. El primer prototipo, el TGV 001 fue el único TGV construido con este tipo de motor, debido a la gran subida en el precio del petróleo durante la crisis energética de 1973, las turbinas de gas fueron calificadas como impracticables y el proyecto dio un giro hacia la electrificación de alta tensión en las líneas del tren. La electricidad iba a ser generada por las nuevas centrales nucleares de Francia.

Sin embargo, el TGV 001 no fue un prototipo inservible. Su turbina de gas turboeléctrica fue sólo una de las muchas tecnologías necesarias para los viajes a alta velocidad sobre raíles. También se probaron los frenos de alta velocidad que fueron necesarios para disipar la gran cantidad de energía cinética del tren operando a gran velocidad, así como la aerodinámica para alta velocidad y la señalización. El diseño de este tren tenía los remolques articulados, lo que significa que entre dos remolques, se comparte un bogie. Este modelo alcanzó los 318 km/h y esta velocidad aún permanece como el récord del mundo de velocidad para un tren a turbina de gas. Su interior y exterior fueron diseñados por el británico Jack Cooper, que trabajó en las formas básicas de los siguientes diseños del TGV, incluyendo el afilado morro de los coches de cabina.

Cambiar las especificaciones del TGV para incorporar la tracción eléctrica requirió una significativa revisión en el diseño. El primer prototipo eléctrico, apodado "Zebulon", fue terminado en 1974. Se probaron características como el novedoso montaje del motor en la carrocería, los pantógrafos, la suspensión y los frenos. El montaje de los motores en la propia carrocería permitió reducir el peso de los coches motores del tren en unas 3 t. Este prototipo viajó durante más de 1.000.000 km en su periodo de pruebas.

En 1976 el gobierno francés fundó el proyecto TGV y poco después se comenzó la construcción de la LGV Sud-Est, la primera línea de alta velocidad, la línea se denominó LN1, "Ligne Nouvelle 1" (Nueva línea 1). LGV es el acrónimo francés "Ligne à Grande Vitesse" usado para las Línea de alta velocidad, LAV en castellano.

Tras las dos pre-producciones de los trenes, estos fueron rigurosamente probados y sustancialmente modificados, la primera versión para producción se entregó el 25 de abril de 1980. El servició TGV comenzó a funcionar para el público entre París y Lyon el 27 de septiembre de 1981. Los pasajeros para los que inicialmente estaba destinado este tren eran gente de negocios que necesitaba viajar entre estas dos ciudades. Y como medio de transporte el TGV era considerablemente más rápido que los trenes convencionales, automóviles y aviones. De manera que este tren empezó a volverse popular muy pronto, incluso fuera de su objetivo de mercado inicial. El viaje rápido y práctico entre estas ciudades tuvo una amplia y rápida aceptación en el mercado del transporte de viajeros.

Desde entonces, se han construido nuevas líneas de gran velocidad en Francia, actualmente existen 6 líneas en este país:

Además hay conexiones con líneas en Bélgica, Países Bajos, Alemania y el Reino Unido.

El Eurostar comenzó a operar en 1994 uniendo el continente europeo con Londres a través del famoso Eurotúnel con trenes derivados directamente de los TGV. El servicio Eurostar usa la Línea de Alta Velocidad Norte (LN3) en Francia. En el Reino Unido los trenes circularon inicialmente por la vía convencional que une Folkestone con Londres a velocidades muy inferiores a sus posibilidades, y utilizando el tercer carril a 750 V como medio de alimentación. Ante esta realidad el gobierno británico encargó la construcción de una nueva línea de alta velocidad que uniera la salida del Eurotúnel con la ciudad de Londres. La primera fase de esta nueva Línea de Alta Velocidad se inauguró en 2003 reduciendo el tiempo de viaje entre París y Londres en 2.35 h (servicios sin paradas intermedias). La segunda fase del proyecto es el acceso a la ciudad de Londres (casi totalmente subterráneo) y se encuentra operativo desde 2007. Ahora se tardan 2h para unir Londres con Bruselas y 2.15h para unir Londres con la capital gala.

El 28 de noviembre de 2003 el TGV transportó a su pasajero 1.000.000.000, número de viajeros sólo superado por el Shinkhansen japonés que alcanzó los 5 mil millones de pasajeros en el 2000. Se espera que se alcancen los dos mil millones en 2010.

El TGV circula en líneas de alta velocidad permitiéndole alcanzar velocidades de hasta 320 km/h en las líneas más nuevas. Originalmente, las líneas de alta velocidad estaban definidas como líneas para permitir velocidades mayores de 200 km/h aunque esta directiva fue redefinida para alcanzar los 250 km/h. Los trenes TGV también pueden circular por trazados convencionales aunque para mantener la seguridad en estas líneas, su velocidad máxima es de 220 km/h. Esta es una ventaja del TGV sobre otros trenes, como por ejemplo los trenes de levitación magnética ya que los TGV pueden dar servicio a más destinos aunque no exista una línea específica, actualmente tienen 200 destinos en Francia y alrededores.

La construcción de las líneas de alta velocidad es bastante similar a la de las líneas tradicionales, pero con una pequeña diferencia, el radio de las curvas es mayor para que los trenes puedan atravesar las curvas a mayores velocidades sin aumentar la fuerza centrífuga que sentirán los pasajeros. El radio de los trazados de alta velocidad históricamente estaba limitado a curvas de más de 4 km de radio pero en las nuevas líneas tienen que tener un radio mínimo de 7 km para los futuros aumentos en la velocidad.

Si la línea únicamente se usa para el tráfico de alta velocidad, las líneas pueden tener pendientes más pronunciadas. Esto facilita la planificación de las líneas de alta velocidad y reduce los costes de construcción. El considerable momento lineal de los trenes TGV a alta velocidad les permite poder ascender por estas pendientes sin un gran incremento en el consumo de energía. También pueden superar los descensos, mejorando la eficiencia del consumo. Las características de la LN1 (Paris-Sud-Est) alcanza pendientes de hasta el 3,5% y trazados como la línea alemana entre Colonia y Fráncfort alcanzan el 4%.

La alineación de las vías es más precisa que en los trazados convencionales y el balasto está colocado más profundamente que en el perfil común, el resultado es un incremento de la carga que pueden soportar las vías y una mejora en la estabilidad. Las vías están ancladas por más traviesas por kilómetro de lo común y todas tienen unas características especiales. El carril es del tipo UIC 60, (60 kg/m) y traviesa de hormigón. El uso de soldadura continua en los carriles en vez de las soldaduras cortas para unirlos aumenta la comodidad del viaje a alta velocidad, evitando el traqueteo que producen las uniones de los raíles en una línea convencional.

El diámetro de los túneles también debe de ser mayor de lo normal, especialmente la entrada; la finalidad de este aumento de diámetro es reducir los efectos de los cambios de presión de aire que pueden ser más problemáticos en los trenes de alta velocidad por las velocidades alcanzadas por el TGV.

Normalmente, los trenes que no son capaces de alcanzar una alta velocidad no deberían circular por las LAV, líneas reservadas especialmente para los TGV. La justificación para esta restricción es la severa disminución de la capacidad de la línea cuando circulan trenes de diferentes velocidades a la vez, ya que los rápidos alcanzan a los lentos y es necesario detener a los rápidos o dejar mucho espacio entre ellos. El tráfico de mercancías y el de pasajeros en conjunto también constituye un riesgo de seguridad, ya que la carga de los coches de mercancías puede desestabilizarse debido a las turbulencias de aire que acompañan a los rápidos TGV en los cruces. El mantenimiento en las líneas de alta velocidad se realiza por la noche, cuando no hay TGVs en circulación.

Por otra parte las pendientes de estas líneas podrían limitar el peso de los lentos trenes de mercancías, para resolver estos problemas debería aumentarse aún más el radio de las curvas con el incremento de coste asociado.

Las líneas de alta velocidad de la red TGV están electrificadas con corriente alterna a 25 kV y 50 Hz. Los hilos de la catenaria se mantienen a una tensión mayor que en las líneas normales, ya que el pantógrafo provoca oscilaciones en ellos, y la onda debe viajar más rápido que el tren para evitar producir ondas estacionarias que pudieran causar la rotura del cable. Este fue uno de los problemas que se presentaron cuando se consiguió el récord de velocidad sobre raíles en 1990. La tensión de la catenaria tuvo que incrementarse aún más para permitir al tren velocidades superiores a los 400 km/h, no sin antes también cambiar el transformador-reductor del interior de los coches motores. En los trazados de alta velocidad, sólo está en funcionamiento uno de los dos pantógrafos del tren, el trasero es el que está elevado, impidiendo la ampliación de las oscilaciones que provocaría el pantógrafo delantero. La tensión para los motores del coche motor delantero se transfiere mediante un cable de alta tensión que recorre el tren por el techo. Sin embargo, los trenes Eurostar, al ser más largos, no tienen este problema, debido a que las oscilaciones se reducen lo suficiente entre la cabeza y la cola del tren como para que ambos pantógrafos puedan estar alzados con seguridad, evitando de este modo que ese cable de alta tensión atraviese el tren. En las líneas clásicas, al plantearse velocidades mucho menores, no existen los problemas de oscilación, de manera que en las líneas de corriente continua ambos pantógrafos están alzados.

Las diversas series de trenes TGV admiten además varias electrificaciones para poder acceder a las líneas convencionales de diferentes países, que utilizan sistemas diferentes.

Las líneas de alta velocidad están cercadas durante todo su trazado para prevenir que animales o personas atraviesen las vías. Los pasos a nivel no están permitidos y los puentes sobre la línea están equipados con sensores que detectan la caída de objetos a las vías.

Todos los cruces de una LAV se producen por pasos a desnivel, ya sea mediante túneles o pasos elevados evitando de este modo la necesidad de cruzar frontalmente el trazado.

Debido a la alta velocidad de los TGV, los maquinistas de estos trenes no son capaces de ver y reaccionar a las señales de ferrocarril como ocurre en las líneas normales. Por eso los TGV incorporan un sistema de señalización en cabina. En la red francesa se utiliza el sistema TVM("Transmission Voie-Machine", ó "transmisión vía a tren") usado para la señalización de las LAV. La información se transmite al tren mediante la transmisión de pulsos eléctricos a través de circuitos de vía, informando de la velocidad, velocidad máxima o indicaciones de parada y arranque directamente al tablero de mandos de la cabina del tren. Esta automatización no exime al maquinista del control de conducción del tren, pero es un sistema de seguridad que sí puede detener el tren en caso de que el maquinista esté cometiendo un error.

La línea está dividida en bloques de señales cada 1,5 km. Los límites están marcados por unos tableros de fondo azul con un triángulo amarillo. En el tablero de mandos de la cabina se muestra la máxima velocidad permitida para los trenes en el tramo en uso y también la velocidad objetivo basada en el perfil de la línea. La velocidad máxima permitida se determina sobre la base de factores como la proximidad con los trenes de adelante (con un descenso de la velocidad en función del número de bloques de distancia con el tren de adelante), la colocación de los desvíos, restricciones de velocidad, la velocidad máxima del tren y la distancia hasta el final de la línea. Como los trenes normalmente no pueden parar en un único bloque (la distancia de frenado va desde unos cientos de metros a unos pocos kilómetros), los maquinistas son alertados para reducir la velocidad gradualmente durante varios bloques antes del stop requerido.

Existen dos versiones de los sistemas de señalización TVM, el TVM-430 y el TVM-300, que son utilizados en las líneas de alta velocidad del TGV. El TVM-430 es un sistema más nuevo, que fue instalado por primera vez en la LGV Nord del túnel del Canal y Bélgica, y que proporciona a los trenes más información que el TVM-300. Entre otros beneficios, el TVM-430 permite a la computadora de a bordo generar un control continuo de la curva de velocidad en caso de la activación del freno de emergencia, forzando al maquinista a reducir la velocidad con más seguridad sin soltar el freno.

El sistema de señalización normalmente es permisivo: el maquinista del tren puede entrar en un bloque ocupado por otro tren sin obtener ninguna autorización, pero la velocidad en esta situación está limitada a 30 km/h y si la velocidad supera los 35 km/h los frenos de emergencia se activan hasta que el tren se pare. Si el tablero de mandos ha marcado la entrada al bloque con el código Nf, no se permite la entrada al bloque y el conductor deberá obtener una autorización desde el PAR ("Poste d'Aiguillage et de Régulation", "Centro de control de señalización") antes de entrar. Una vez que la circulación está habilitada o el PAR ha aceptado la autorización se enciende una lámpara blanca sobre el tablero para informar al maquinista. Éste confirma la autorización usando un botón en el panel de control del tren. Esto desactiva la frenada de emergencia que se produciría al pasar por el circuito de vía adyacente al bloque de paso no permitido.

Cuando los trenes entran o salen de las LAV desde las líneas clásicas, el tren pasa sobre un circuito de vía que automáticamente cambia un indicador del tablero del maquinista para activar el sistema de señalización apropiado. Por ejemplo, un tren saliendo de una LAV a una línea clásica debería desactivar su sistema de señalización TVM y activar el tradicional sistema francés KVB ("Contrôle Vitesse par Balise", "Control de velocidad por baliza").

Los TGV también incorporan otros sistemas de seguridad, propios de aquellos países en donde se efectúan instalaciones.

Una de las principales ventajas del TGV sobre otras tecnologías de transporte rápido sobre raíles como los trenes de levitación magnética es que los TGVs tienen la ventaja de las infraestructuras ya existentes. Esto permite conectar los centros de las ciudades (como París-Gare de Lyon a Lyon Perrache) realizando un mínimo gasto económico en las estaciones; Los TGVs a menudo usan vías interurbanas y estaciones construidas para trenes de velocidad lenta.

Sin embargo, los diseñadores de las rutas de alta velocidad han tenido cuidado de construir nuevas estaciones en áreas suburbanas o en zonas periféricas de las ciudades. Esto permite a los TGVs parar sin incurrir en una gran penalización de tiempo. En algunos casos, las estaciones han sido construidas a medias entre dos comunidades. Las estación que sirve a Montceau-les-Mines y Le Creusot es un ejemplo de este enfoque y otro ejemplo más controvertido es la estación de Haute Picardie (Alta Picardía), entre Amiens y San Quintín. La prensa y las autoridades locales criticaron que Haute Picardie estaba demasiado lejos para la conveniencia de la población y demasiado lejos de las conexiones ferroviarias para ser útil a los pasajeros. La estación fue llamada "la gare des betteraves" traducido como "estación de las remolachas" porque está rodeada por campos de remolacha. Este apodo se aplica ahora a las estaciones que están en condiciones similares, por la lejanía del centro de las ciudades independientemente de que estén rodeadas por campos de cultivo de remolacha o no.

Pero también han sido construidas un gran número de nuevas estaciones para los servicios del TGV, algunas de ellas son grandes logros arquitectónicos. La estación del TGV de Avignon, inaugurada en 2001 ha sido una de las que más alabanzas ha recibido de toda la red, con una espectacular cúpula de cristal de 340 m que ha sido comparada con una catedral.

Los coches del TGV se diferencian de otro tipo de trenes en que están unidos de manera semipermantente. Los bogies están situados entre dos coches, disponiendo cada uno de ellos de dos bogies en los extremos y cada bogie es compartido con otro coche, excepto en el caso de los coche motores de cabeza y de cola, que además del bogie compartido tienen uno propio. Por lo tanto tienen n+1 bogies, siendo n el número de coches que dispone el tren.

Este diseño utilizado hace más de 60 años por Talgo en sus trenes articulados tiene múltiples ventajas, durante un descarrilamiento el coche motor que es el primero de la composición y se mueve independientemente de los remolques de pasajeros se mantendrá sin volcar con este sistema, en los trenes normales puede romperse el enganche y que los primeros coches vuelquen o salgan en cualquier dirección.

Una desventaja de este diseño es la dificultad para separar la composición, mientras los coches motores del TGV pueden ser desenganchados del resto del tren y trasladados por otros trenes mediante acoples especiales entre el gancho de husillo, y el scharfemberg, los coches deben ser desmontados de los trenes con un sistema que levante el tren completo de una vez. Una vez desemparejados cada uno de los coches se queda con un único bogie impidiendo que pueda remolcarse por las vías.

SNCF opera una flota de unos 400 TGVs. Tienen 7 tipos de TGV o derivados del TGV que actualmente operan en la red ferroviaria francesa; estos son el TGV Sud-Est (pasajeros y la variedad postal "La Poste"), TGV Atlantique, TGV Reseau, el Thalys PBA, Eurostar, TGV Duplex y Thalys PBKA, y un séptimo tipo, TGV POS (que unirán Francia con el sur de Alemania) está actualmente en pruebas

Todos los TGV son al menos "bi-tensión", que significa que pueden operar a 25 kV 50 Hz CA en las líneas nuevas (LAVs)y a 1,5 kV CC en las líneas antiguas. Los trenes que cruzan la frontera de Alemania, Suiza, Bélgica, Países Bajos y Reino Unido son tritensión, o politensión para poder circular pos las líneas extranjeras, en el caso de Italia y Bélgica, los Thalys, y TGV que circulan por esos países van equipados con una mejora en el transformador principal de los coches motores que les permite utilizar los 3 Kv CC de las líneas de ambos países, y los que circulan por Suiza, o Alemania, van equipados con un transformador especial que permite además de éstas tres tensiones, los 15 Kv 16 (2/3) Hz de esos países. Todos los TGVs están equipados con dos pares de pantógrafos, dos para las tensiones alternas y otros dos para las tensiones continuas, y las ramas especiales para circular por Suiza, llevan pantógrafos de gálibo suizo, es decir, más pequeños, para poder adaptarse a los muchos túneles de la red suiza.

Cuando pasan entre áreas de diferentes tensiones, la cabina, recuerda al maquinista que desactive la potencia de los motores de tracción, que baje los pantógrafos, ajuste en cabina el sistema de tensión adecuado y que eleve de nuevo los pantógrafos.

Los pantógrafos, así como el control de su altura se seleccionan automáticamente de acuerdo en el sistema de tensión elegido por el maquinista. Una vez que el tren detecta en sus transformadores que el suministro es correcto, se indica en cabina mediante una luz y el maquinista puede dar tensión nuevamente a los motores de tracción.

La flota Sud-Est fue construida entre 1978 y 1988 con el objetivo de encargarse del primer servicio del TGV, que comenzó a funcionar entre París y Lyon en 1981. Actualmente hay 107 unidades de pasajeros operativas, de las cuales, nueve son tri-tensión (la tercera tensión son los 15 kV, 16 2/3 Hz CA para las líneas suizas) y el resto bi-tensión. Hay también 7 composiciones bi-tensión sin asientos preparadas para transportar correo postal para La Poste entre París y Lyon. Estos trenes son bastante peculiares, debido en parte a sus colores amarillentos.

Cada configuración consta de 2 cabezas tractoras y 8 remolques de pasajeros con 345 plazas sentadas. Con un bogie con tracción en cada uno de los coches adyacentes a las cabezas. Miden 200 m de longitud y 2,81 m de ancho, su peso es de 385 ton y tienen una potencia de salida de 6450 kW a 25 kV de tensión.

Inicialmente, este modelo fue construido para viajar a 260-270 km/h, pero la mayoría fueron actualizados para los 300 km/h durante su restauración a mitad de vida, preparándolos para la apertura de la "LGV-Mediterranée". Unas pocas configuraciones que aún mantienen la velocidad máxima de 270 km/h operan en esas rutas, que en comparación con una LAV, suponen una distancia relativamente corta, como es el caso del enlace con Suiza a través de Dijón. SNCF no cree que deba realizarse un desembolso económico destinado a la actualización de estos modelos, ya que el aumento de la velocidad apenas reduciría el tiempo de viaje.

La flota del TGV Atlantique (TGV-A) fue construida entre 1988 y 1992, consta de 105 composiciones bi-tensión que fueron construidas para la LAV Atlantique que entró en servició en 1989. Estas composiciones miden 237 m de longitud y tienen un ancho de 2,9 m. Su masa es de 444 tm, están compuestas por dos cabezas motrices y diez remolques de pasajeros con una capacidad de 485 plazas sentadas. Fueron construidos para alcanzar una velocidad máxima de 300 km/h y ofrecen una potencia de 8.800 kW bajo una tensión de 25 kV.

La unidad 325 con una serie de modificaciones consiguió el 18 de mayo de 1990 el récord de velocidad del mundo en la nueva LAV antes de su inauguración. Entre las modificaciones había varias mejoras aerodinámicas, ruedas de mayor diámetro, un sistema de frenado mejorado para habilitar los tests de velocidad sobre los 500 km/h. La composición fue reducida a 2 cabezas motrices y 3 remolques de pasajeros para aumentar la relación de potencia/masa, siendo la masa de esta composición de 250 ton. Los tres remolques, incluido el coche bar en el centro es la composición mínima posible por la forma en la que están articulados. Para alcanzar este récord también se aumentó la tensión en la catenaria, y también la tensión del hilo de la catenaria para que el convoy tuviese una mayor potencia.

La primera composición Réseau (TGV-R) empezó a funcionar en 1993. 50 composiciones bi-tensión fueron encargadas en 1990 y otras 40 tri-tensión en 1992 y 1993. Diez de los trenes tri-tensión componen Thalys y son también conocidos como Thalys PBA ("París" "Bruselas" "Ámsterdam"). Mientras que los trenes bi-tensión solo pueden utilizar las dos tensiones estándar de Francia (1,5 kV CC y 25 kV CA), los tri-tensión funcionan también a 3 kV CC, que es la tensión utilizada en Bélgica y otros países.

Están formados por 2 remolques de tracción que administran una potencia de 8.800 kW a 25 kV, como el TGV Atlantique, y 8 remolques de pasajeros, que suponen una capacidad de 377 plazas sentadas. La velocidad máxima es de 300 km/h. Tienen 200 m de longitud y una anchura de 2,9 m. Las configuraciones bi-tensión tienen un peso de 383 toneladas y las configuraciones tri-tensión, habilitadas para circular en Bélgica, tienen una serie de modificaciones, para respetar la máxima carga por eje de las líneas de ese país, entre las modificaciones se ha sustituido el acero por aluminio, en ejes huecos reduciendo la carga a 16 t/eje

Debido a las quejas de la incomodidad de los cambios de presión cuando se entra a gran velocidad en los túneles de la LGV-Atlantique, las composiciones Réseau ahora tienen un aislamiento de presión.

El tren Eurostar es esencialmente un TGV largo, modificado para poder funcionar en el Reino Unido y en el Eurotúnel. Las diferencias incluyen menor anchura, para poder ajustarse al gálibo británico. El diseño británico tiene motores de tracción asíncronos y una gran protección contra incendios en caso de un incendio en el túnel.

En el Reino Unido, siguiendo la clasificación TOPS que utilizan, este tren se denomina clase 373 y en las etapas de planificación del proyecto se denominaba como "TransManche Super Train". Los trenes fueron construidos por GEC-Alsthom en La Rochelle (Francia), Belfort (Francia) y Washwood Heath (Inglaterra), entrando en funcionamiento en el año 1993.

Fueron construidos dos tipos:

Todas las configuraciones de ambos tipos consisten en dos mitades idénticas no articuladas en el remolque central, de este modo, en caso de una emergencia en el Eurotúnel una de las mitades podría desacoplarse y de este modo salir del túnel. Cada mitad está numerada de un modo distinto.

Hay 38 composiciones completas, más una cabina de repuesto, de estas unidades: 16 fueron pedidas por SNCF, 4 por NMBS/SNCB y las 18 restantes por British Rail, de éstas 7 composiciones consisten en la configuración "Norte de Londres". En la privatización de los ferrocarriles británicos por parte del Gobierno del Reino Unido, las composiciones de la BR fueron compradas por London & Continental Railways que es subsidiaria de Eurostar (U.K.) Ltd. que está controlada por National Express Group (40%), SNCF (35%), SNCB (15%) y British Airways (10%).

La composición "Tres Capitales" opera a una velocidad máxima de 300 km/h, con una potencia nominal de 12.240 kW. Tiene una longitud de 394 m con capacidad para 766 plazas sentadas y el peso es de 752 ton. La composición "Norte de Londres" tiene una capacidad de 558 plazas sentadas. Todos los trenes son al menos tri-tensión y son capaces de operar a 25 kV 50 Hz CA (LAVs, incluido el enlace con el Eurotúnel), 3 kV CC ("líneas clásicas" de Bélgica) y 750 V CC (en la red de la región sur británica que dispone del tercer carril, usado para alimentar los trenes). El sistema del tercer carril será innecesario a partir de 2007 cuando se complete la 2.ª fase del enlace con el Eurotúnel desde Londres. Cinco de los modelos "Tres Capitales" que posee la SNCF son cuatri-tensión añadiendo la capacidad para soportar los 1,5 kV CC de las líneas convencionales de Francia.

Tres de las "Tres Capitales" que posee SNCF son para uso Francés e incluso llevan los colores plata y azul de los servicios TGV. Las composiciones "Norte de Londres" no han sido utilizadas nunca para uso internacional, pero estaban previstas para dar un servicio directo desde el continente europeo a las ciudades del norte de Londres, usando los corredores de la costa este y el de la costa oeste, pero estos servicios nunca llegaron a fructificar por las tarifas económicas ofrecidas por las aerolíneas del Reino Unido. Unas pocas de estas composiciones fueron cedidas a la GNER para usar su servicio "White Rose" entre Londres y Leeds, dos de estas llevan los colores azul oscuro de GNER pero la cesión terminó en diciembre de 2005.

El actual presidente de Eurostar, Richard Brown, ha sugerido que los trenes podrían ser reemplazados por trenes de dos pisos similares a los del TGV Duplex cuando se vayan retirando. Una flota de Eurostars de dos pisos podría llevar a 40 millones de pasajeros al año entre el Reino Unido y el continente europeo, lo cual, sería equivalente a añadir una pista extra en uno de los aeropuertos de Londres.

El TGV Duplex (TGV-D) fue construido para aumentar la capacidad de los TGV sin aumentar la longitud del tren ni el número de trenes. Cada remolque tiene dos pisos, con un único acceso a través de las puertas de la parte baja que se aprovechan de la baja altura de los andenes franceses. Una escalera permite el acceso al piso de arriba, donde están localizadas las pasarelas entre remolques. Esta distribución permite una capacidad de 512 asientos a cada composición (135 plazas más que los TGV-R). En las líneas más ocupadas, como la París-Marsella, salen composiciones de dos trenes con lo que se logra una capacidad de 1.024 plazas. Cada composición tienen una compartimento para el acceso con personas con movilidad reducida.

Tras una largo desarrollo que comenzó en 1988 (en el que se conocían como TGV-2NG), se construyeron en dos tandas, 30 entre 1995-1998 y 34 más entre 2000-2004. Su peso es de 386 tm, miden 200 m, cada tren se compone de 2 cabezas tractoras más 8 remolques de dos pisos. La gran cantidad de aluminio y carbono utilizada supone que la masa de éstos no es mucho mayor que de los TGV Réseau. Son también modelos bi-tensión y poseen una potencia nominal total de 8.800 kW y se les ha incrementado la velocidad máxima hasta los 320 km/h

A diferencia del Thalys PBA (París Bruselas Ámsterdam), el Thalys PBKA ("París Bruselas Colonia Ámsterdam") se usa exclusivamente para los servicios Thalys. Son tecnológicamente similares a los TGV Duplex, pero en vez de soportar dos tensiones diferentes, son cuadrifásicos, pudiendo operar bajo 25 KV 50 Hz CA (línea de alta velocidad), 15 kV 16,7 Hz CA (Alemania, Suiza), 3 KV CC (Bélgica) y 1500 V CC (Líneas convencionales de Francia y Países Bajos).

Su velocidad máxima es de 300 km/h bajo la catenaria de 25 kV, pero la potencia nominal desciende hasta 4.460 kW con una pobre relación de potencia/peso en las NBS alemanas. Tienen 2 cabezas tractoras y 8 remolques con lo que su longitud totaliza en 200 m, siendo su masa total de 385 tm y su capacidad es de 377 plazas sentadas.

De los 17 trenes fabricados, 9 son de SNCB, 7 de SNCF y dos de NS. La Deutsche Bahn contribuyó financiando dos de las composiciones de SNCB.

Los trenes TGV POS, destinados para París-Ostfrankreich-Süddeutschland (París-Este de Francia y Sur de Alemania) están en la fase de pruebas para ser usados en la LGV Est, actualmente están en construcción.

Los 19 trenes consisten en dos coches motores y 8 remolques del tipo TGV Réseau reconstruidos, la potencia nominal de estos trenes es de 9.600 kW y la velocidad máxima de 320 km/h. A diferencia de los TGV-A, TGV-R y TGV-D, sus motores son asíncronos-trifásicos, por lo que en caso de fallo, se puede aislar un motor individualmente en el bogie de tracción. Su peso es de 383 t

El TGV Dasye es una evolución de la serie TGV Duplex.

Son trenes de doble piso con una velocidad máxima en servicio comercial de 320 km/h. Están pensados para cubrir servicios internacionales, ya que admiten sistemas de seguridad y electrificación que se utilizan fuera de Francia. "Dasye" proviene de «"Duplex ASYnchrone ERTMS"». Son clasificadas como serie 700 de la SNCF.

El TGV 2N2, también denominado Euroduplex, es una nueva versión de tren de alta velocidad de la serie TGV. Sucede al TGV Dasye.

Francia tiene alrededor de 1.200 km de LAV construidas en los últimos 20 años, con cuatro líneas más planificadas o en construcción.




Ámsterdam y Colonia ya están conectadas por los TGV Thalys funcionando por líneas convencionales, aunque estas líneas están siendo mejoradas con carriles de alta velocidad. Londres también tiene servicio a través de los trenes Eurostar circulando a alta velocidad por el enlace casi terminado del Eurotúnel que será completamente funcional cuando se termine la segunda sección del enlace con el Eurotúnel en 2007.

La tecnología de los TGV ha sido adoptada en diversos países que no están interconectados con la red francesa:

SNCF y Alstom están investigando nuevas tecnologías que podrían ser usadas para el transporte de alta velocidad en Francia.

El desarrollo de los trenes TGV está buscando el modo de conseguir los Automotrice à grande vitesse ("inglés": (AGV), unidades de alta velocidad autopropulsadas. El diseño incluye coches de tracción distribuida, los motores se encuentran debajo de cada remolque. Las investigación están orientándose hacia la producción de trenes al mismo coste que los TGV existentes, con los mismos estándares de seguridad. Pero los AGV de misma longitud que los TGV podrían tener una capacidad de 450 asientos y la velocidad sería de 350 km/h.

En pocas palabras, la idea que está siendo considerada para incrementar la capacidad de los TGV en un 10% pasa por reemplazar las coches de cabeza y cola por coches con capacidad para transportar pasajeros, al estilo del ICE-3 de la Deutsche Bahn. Todos los remolques tendrían bogies motorizados debajo del coche como sucede actualmente con el primer y último coche. Además la pérdida de potencia es menor.

Otra área que está siendo investigada es la levitación magnética. Esta tecnología requiere un coste de implantación de la tecnología maglev muy alto. Además, debería construirse una nueva red entera, ya que los trenes maglev requieren vías diseñadas específicamente para su uso. El nuevo sistema sólo llegaría, previsiblemente, hasta las afueras de las ciudades.

En más de dos décadas de funcionamiento de la alta velocidad, el TGV no ha tenido ninguna víctima mortal a causa de un accidente mientras circulaba a alta velocidad. Ha habido varios accidentes, incluyendo tres descarrilamientos por encima de los 270 km/h, pero en ninguna de las ocasiones volcó ningún remolque. Esto se debe en parte la rigidez del diseño articulado del tren. Ha habido, sin embargo, accidentes graves mientras el TGV circulaba sobre líneas convencionales, donde los trenes están expuestos a los mismos peligros que los trenes normales, como los pasos a nivel.



Después del número de accidentes producidos en los pasos a nivel, se ha hecho un esfuerzo en eliminar todos los pasos a nivel de las líneas tradicionales usadas por los TGV. Como resultado, el trazado convencional entre Tours y Burdeos al final del a LGV Atlantique no tienen ningún paso a nivel.

Las primeras protestas en contra de la construcción de una línea de alta velocidad en Francia ocurrieron en mayo de 1990 durante la etapa de planificación de la LGV Méditerranée. Los manifestantes bloquearon un viaducto ferroviario para protestar en contra de la ruta planificada para la línea, argumentando que la nueva línea era innecesaria y que los trenes podrían usar las líneas existentes para llegar a Marsella desde Lyon.

Lyon Turin Ferroviaire (LTF-SAS) compañía que pretende conectar el TGV a la red de alta velocidad Italiana también ha sido el objetivo de manifestaciones en Italia. Mientras la mayoría de los políticos italianos están de acuerdo con la construcción de esta línea, los habitantes de las ciudades por donde se construiría la línea están muy en contra. Las quejas de los manifestantes sobre todo se centran en el peligro de almacenar los materiales extraídos de la montaña como el asbesto y el uranio, al aire libre. Estos serios peligros para la salud podrían evitarse usando técnicas más apropiadas pero más caras, para manejar los materiales radioactivos. Se retrasó en seis meses el comienzo de las obras para estudiar otras soluciones. Un movimiento NIMBY nacional en contra del TAV italiano está tratando de alarmar a los habitantes para que critiquen y se preocupen por el desarrollo de la LAV en toda Italia.

Las principales quejas se sitúan en el ruido de los TGV al pasar cerca de las ciudades y pueblos, lo que ha llevado a la SNCF a construir pantallas de protección acústica en grandes secciones de las LAVs para reducir las molestias a los residentes, pero las protestas todavía tienen lugar donde la SNCF no ha tomado estas decisiones.





</doc>
<doc id="22293" url="https://es.wikipedia.org/wiki?curid=22293" title="Litio">
Litio

El litio es un elemento químico de símbolo Li y número atómico 3. En la tabla periódica, se encuentra en el grupo 1, entre los elementos alcalinos. En su forma pura, es un metal blando, de color blanco plata, que se oxida rápidamente en aire o agua. Su densidad es la mitad de la del agua, siendo el metal y elemento sólido más ligero.

Al igual que los demás metales alcalinos es univalente y muy reactivo, aunque menos que el sodio, por lo que no se encuentra libre en la naturaleza. Acercado a una llama la torna carmesí pero, si la combustión es violenta, la llama adquiere un color blanco brillante.

Se emplea especialmente en aleaciones conductoras del calor, en baterías eléctricas y, sus sales, en el tratamiento del trastorno bipolar.

El litio toma su nombre del griego λίθoς -ου, ‘piedra’. El nombre del elemento proviene del hecho de haber sido descubierto en un mineral, mientras que el resto de los metales alcalinos fueron descubiertos en tejidos de plantas.

El litio fue descubierto por Johann Arfvedson en 1817. Arfvedson encontró este elemento en la espodumena y lepidolita de una mina de petalita, LiAl (SiO), de la isla Utö (Suecia) que estaba analizando. En 1818 C. G. Gmelin fue el primero en observar que las sales de litio tornan la llama de un color rojo brillante. Ambos intentaron, sin éxito, aislar el elemento de sus sales, lo que finalmente consiguieron William Thomas Brande y "sir" Humphrey Davy mediante electrólisis del óxido de litio.

En 1923 la empresa Alemana Metallgesellschaft AG comenzó a producir litio mediante la electrólisis del cloruro de litio y cloruro de potasio fundidos.

En el 2010, las baterías de litio se han convertido en el método principal para reemplazar a los contaminantes combustibles fósiles. El “triángulo del litio” compuesto por el salar de Uyuni en Bolivia, el salar de Atacama en Chile y el salar del Hombre Muerto en Argentina, concentran aproximadamente entre el 50 y el 85 % de ese mineral. El crecimiento acelerado en el uso del ion-litio ha provocado que la tonelada de litio suba su precio, desde los 450 dólares que costaba en 2003 hasta los 3000 dólares en 2009.

Por su elevado calor específico, el litio se emplea en aplicaciones de transferencia de calor, y por su elevado potencial electroquímico constituye un ánodo adecuado para las baterías eléctricas. También se le dan los siguientes usos: 

El litio es un elemento moderadamente abundante y está presente en la corteza terrestre en 65 partes por millón (ppm). Esto lo coloca por debajo del níquel, cobre y wolframio y por encima del cerio y estaño, en lo referente a abundancia.

Argentina, Bolivia y Chile tienen el 85% de reservas de litio del planeta. Se encuentra disperso en pequeña proporción en ciertas rocas volcánicas y sales naturales, pero nunca libre, dada su gran reactividad en el Salar de Uyuni en Bolivia o el Salar del Hombre Muerto en Argentina Salar de Atacama en Chile (5 % de las reservas). Hay otros salares de menor tamaño en, Manaure (Colombia) y otros yacimientos importantes localizados recientemente en Afganistán. Desde 2010 se investigan en Afganistán, unas reservas cuya magnitud todavía está por determinarse con precisión, pero que podrían cambiar radicalmente la evaluación de los porcentajes antes mencionados y la evolución de los acontecimientos políticos y económicos de aquel país.

El litio, junto al hidrógeno y al helio, es uno de los únicos elementos obtenidos en el Big Bang. Todos los demás fueron sintetizados a través de fusiones nucleares en estrellas en la secuencia principal o durante estallidos de supernovas. Industrialmente se obtiene mediante la electrólisis del cloruro de litio fundido (LiCl).

Desde la Segunda Guerra Mundial la producción de litio se ha incrementado enormemente, separándolo de las rocas de las que forma parte y de las aguas minerales. Los principales minerales de los que se extrae son lepidolita, petalita, espodumena y ambligonita. En Estados Unidos se obtiene de las salinas de California y Nevada principalmente.

Los isótopos estables del litio son dos, Li-6 y Li-7, siendo este último el más abundante (92.5 %). Se han caracterizado seis radioisótopos siendo los más estables el Li-8 con un periodo de semidesintegración de 838 milisegundos y el Li-9 con uno de 178.3 ms. El resto de isótopos radiactivos tienen periodos de semidesintegración menores de 8,5 ms. También se da, en laboratorio, el isótopo inestable Li-11-

Los pesos atómicos del litio varían entre 4.027 y 11.0348 uma del Li-4 y el Li-11 respectivamente. El modo de desintegración principal de los isótopos más ligeros que el isótopo estable más abundante (Li-7) es la emisión protónica (con un caso de desintegración alfa) obteniéndose isótopos de helio; mientras que en los isótopos más pesados el modo más habitual es la desintegración beta, (con algún caso de emisión neutrónica) resultando isótopos de berilio.

El Li-7 es uno de los elementos primordiales, producidos por síntesis nuclear tras el "big bang". Los isótopos de litio se fraccionan sustancialmente en una gran variedad de procesos naturales, incluyendo la precipitación química en la formación de minerales, procesos metabólicos, y la sustitución del magnesio y el hierro en redes cristalinas de minerales arcillosos en los que el Li-6 es preferido frente al Li-7, etc.

Al igual que otros metales alcalinos, el litio puro es altamente inflamable y ligeramente explosivo cuando se expone al aire y especialmente al agua. Es además corrosivo por lo que requiere el empleo de medios adecuados de manipulación para evitar el contacto con la piel. Se debe almacenar en un hidrocarburo líquido como vaselina o aceite mineral. Puede generar hipotiroidismo al impedir la entrada del yodo a la hormona tiroidea. El litio no es sustrato para la bomba sustrato sodio potasio ATPasa que impide el paso de los iones de sodio, reemplazando la concentración del sodio, lo cual en altas concentraciones puede resultar tóxico. 

Se sabe que puede sustituir al sodio a nivel de las membranas biológicas, pero se ha encontrado que incrementa la permeabilidad celular y actúa sobre los neurotransmisores, favoreciendo la estabilidad del estado anímico. Por lo que se utiliza para tratar y prevenir episodios de manía en la enfermedad bipolar.





</doc>
<doc id="22294" url="https://es.wikipedia.org/wiki?curid=22294" title="Flúor">
Flúor

El flúor es el elemento químico de número atómico 9 situado en el grupo de los halógenos (grupo 17) de la tabla periódica de los elementos. Su símbolo es F.

Es un gas a temperatura ambiente, de color amarillo pálido, formado por moléculas diatómicas F. Es el más electronegativo y reactivo de todos los elementos. En forma pura es altamente peligroso, causando graves quemaduras químicas al contacto con la piel.

El flúor es el elemento más electronegativo y reactivo y forma compuestos con prácticamente todo el resto de elementos, incluyendo los gases nobles xenón y radón. Su símbolo es F. Incluso en ausencia de luz y a bajas temperaturas, el flúor reacciona explosivamente con el hidrógeno. El flúor diatómico, F, en condiciones normales es un gas corrosivo de color amarillo casi blanco, fuertemente oxidante. Bajo un chorro de flúor en estado gaseoso, el vidrio, metales, agua y otras sustancias, se queman en una llama brillante. Siempre se encuentra en la naturaleza combinado y tiene tal afinidad por otros elementos, especialmente silicio, que no se puede guardar en recipientes de vidrio.

En disolución acuosa, el flúor se presenta normalmente en forma de ion fluoruro, F. Otras formas son fluorocomplejos como el [FeF], o el HF.

Los fluoruros son compuestos en los que el ion fluoruro se combina con algún resto cargado positivamente.


A causa de ser tan reactivo y peligroso, el flúor no fue aislado hasta tiempos relativamente recientes, puesto que en estado puro es sumamente peligroso y es necesario manejarlo con extremo cuidado.

El primer compuesto de flúor (del latín "fluere", que significa "fluir") que se conoce data de los años 1500, en Alemania. Se trata de la fluorita (CaF), por entonces llamada flúores, después espato de flúor. Es un mineral raro, que se funde fácilmente y era utilizado como fundente, para fundir otros minerales con mayor facilidad al mezclarlo con flúores. El mineralogista Georgius Agricola describió el mineral en 1529.

En 1670, Enrique Schwandhard descubrió que al someter al mineral a algunos ácidos, desprendía un vapor muy corrosivo, que incluso corroía el vidrio. Utilizó esta propiedad para elaborar dibujos sobre el vidrio, por lo que mantuvo en secreto la forma de obtenerlo.

Solo muy lentamente se avanzó en el estudio de este mineral. En 1768, Andrés Segismunod Sargraf estudió el mineral y obtuvo nuevamente el extraño vapor, informando sobre la característica que ataca al vidrio.

Sin embargo, el primero en estudiar el gas fue Carlos Sabéele en 1780. A él se le atribuye el descubrimiento del ácido fluorhídrico. Murió a los 44 años, muy probablemente a causa de una intoxicación sistemática con los productos que manejaba.

En 1813, Ampère hizo la hipótesis de que el ácido fluorhídrico era un compuesto de hidrógeno con un elemento todavía no descubierto. Esta hipótesis la hizo por la analogía que tiene este ácido con el muriático, del que se descubrió el cloro apenas tres años antes. Comunicó su hipótesis a Humphry Davy. Ampère sugirió el nombre de "pthor" al nuevo elemento, pero Davy se inclinó por el nombre "flúor".

Desde entonces se sucedió una serie de intentos de aislar el flúor, todos fallidos, y la mayoría con accidentes de intoxicación. Comenzó el mismo Davy por medio de electrólisis, descomponiendo el fluoruro cálcico, pero no lo logró debido a que una vez aislado el flúor en el electrodo positivo, se combinaba rápidamente con cualquier elemento que estuviese cerca. En el proceso se intoxicó y probablemente a causa de eso tuvo una muerte temprana.

En 1830 los hermanos Tomás y Jorge Knox intentaron aislar el flúor por medios químicos usando cloro. No lo lograron y también se intoxicaron seriamente.

P. Louyel también lo intentó en la misma época, fracasando también, pero en esta ocasión la intoxicación le causó la muerte.

Edmond Frémy (inicialmente ayudante de Louyel) abordó el tema con mucha mayor cautela y seguridad, lo que le valió librarse de la intoxicación. Regresó a la electrólisis y en el proceso fue el primero en obtener ácido fluorhídrico puro (anteriormente solo se lo obtenía mezclado con agua), pero tampoco logró el objetivo.

El químico francés Henri Moissan, inicialmente ayudante de Frémy, continuó con el intento. probó métodos químicos (usando fluoruro de fósforo) pero fracasó por lo que decidió intentar con electrólisis. Usó fluoruro arsénico pero al comenzar a intoxicarse paso al ácido fluorhídrico, continuando la labor de su maestro. Para que condujera la electricidad agregó fluoruro de potasio al ácido fluorhídrico puro y logró la electrólisis. Para que el flúor no atacara al electrodo positivo, usó una aleación de platino e iridio, apoyado en fluorita como aislante, y adicionalmente realizó la electrólisis a 50 grados bajo cero. Finalmente, el 26 de junio de 1886, Moissan fue el primero que obtuvo flúor en forma pura, lo que le valió el Premio Nobel de Química de 1906.

La primera producción comercial de flúor fue para la bomba atómica del Proyecto Manhattan, en la obtención de hexafluoruro de uranio, UF, empleado para la separación de isótopos de uranio. Este proceso se sigue empleando para aplicaciones de energía nuclear.

El flúor es el halógeno más abundante en la corteza terrestre, con una concentración de 950 ppm. En el agua de mar esta se encuentra en una proporción de aproximadamente 1,3 ppm. Los minerales más importantes en los que está presente son la fluorita, CaF, el fluorapatito, Ca(PO)F y la criolita, NaAlF.

El flúor se obtiene mediante electrólisis de una mezcla de HF y KF. Se produce la oxidación de los fluoruros:
En el cátodo se descarga hidrógeno, por lo que es necesario evitar que entren en contacto estos dos gases para que no haya riesgo de explosión

El oxígeno combustiona mejor con los HC porque siempre se forma CO, en cambio con flúor pueden formarse perfluorcadenas que son bastante inertes.
El compuesto más oxidante puede ser el O)F) o bien el ion XeF+. El flúor se puede obtener químicamente en reacciones de ácidos de Lewis.

El fluoruro de hidrógeno es extremadamente corrosivo y reacciona violentamente con los alcalinos y al amoníaco anhidro.Destruye el tejido hasta el hueso, más peligroso que el sulfúrico y nítrico.
Las disoluciones de HF son mortales aunque sean diluidas.
dichos compuestos son muy reactivos el ClF, es aún más reactivo que el flúor así como BrF, 
El HF anhidro y el ácido nítrico mezclados disuelven a la mayoría de los metales de transición, incluido al tántalo.

Aunque el flúor es demasiado reactivo para tener alguna función biológica natural, se incorpora a compuestos con actividad biológica. Compuestos naturales organofluorados son raros, el ejemplo más notable es el fluoroacetato, que funciona como una defensa contra los herbívoros de plantas en al menos 40 plantas en Australia, Brasil y África. La enzima adenosil-fluoruro sintasa cataliza la formación de 5'- desoxi-5'-fluoroadenosina. El flúor no es un nutriente esencial, pero su uso tópico en la prevención de la caries dental es bien reconocida. El efecto es tópico (aplicación sobre la superficie del esmalte), aunque antes de 1981 se consideró principalmente sistémico (por ingestión). Su uso sistémico está actualmente desaconsejado por muchos autores.

El flúor tiene un único isótopo natural, el F. Este isótopo tiene un número cuántico de espín nuclear de 1/2 y se puede emplear en espectroscopia de resonancia magnética nuclear. Se suele emplear como compuesto de referencia el triclorofluorometano, CFCl o el trifluoroacético TFA.

El F es un isótopo artificial emisor de positrones (emisor β), que puede obtenerse por medio de un ciclotrón a partir del O (bajo la forma química de HO). El F, por su emisión radiactiva (positrones, que al aniquilarse con los electrones del medio producen dos rayos gamma de 511 keV), se utiliza en el diagnóstico por tomografía por emisión de positrones (PET, de sus siglas en inglés), la cual tiene aplicaciones en Oncología, Neurología y Cardiología. El F se incorpora a moléculas orgánicas (proceso denominado "marcación con F"). Las mismas son aplicadas al paciente por medio de inyectables y el patrón de su distribución en el organismo permite el diagnóstico de tumores, zonas de baja perfusión cardíaca o cerebral, entre otras.

El flúor y el HF deben ser manejados con gran cuidado y se debe evitar totalmente cualquier contacto con la piel o con los ojos.
El HF anhidro hierve a 19 °C, sus vapores son muy irritantes y tóxicos, sus descubridores murieron por su acción. Nunca ha de mezclarse con metales alcalinos ni con amoniaco. En presencia de SbF, se convierte en un superácido (el HF anhidro).
La capacidad de protonación es tan grande que oxida a metales como el cobre y protona al metano etc.
Tanto el flúor como los iones fluoruro son altamente tóxicos. El flúor presenta un característico olor acre y es detectable en unas concentraciones tan bajas como 0,02 ppm, por debajo de los límites de exposición recomendados en el trabajo.

La toxicidad del flúor viene por su afinidad a unirse al zinc (básico para el aprendizaje, la memoria y la formación de anticuerpos), y al yodo (básico para la tiroides y el sistema hormonal del cuerpo y otras funciones, siendo además el yodo quelante de mercurio), esto es similar al mercurio que se amalgama con el yodo y el zinc). Además, el exceso de flúor puede producir malformaciones óseas, aparte de un "endurecimiento y fragilidad" de los huesos con una mayor facilidad a su rotura. En definitiva, el flúor puede dañar el sistema de aprendizaje, memoria, salud, sistema hormonal, huesos, y así de energía y productividad de las personas.

En la característica de unión con el yodo, se usa el flúor para tratar hipertiroidismos (un hiperdesarrollo de la tiroides, entre otras cosas por exceso de yodo). Al eliminar el yodo del cuerpo, el flúor reduce la tiroides, reduciendo su tamaño y actividad, siendo esto muy dañino para personas con tiroides normales (que hace una parte vital del sistema hormonal del cuerpo), y especialmente para las personas con una tiroides débil o hipotiroidismo. Esto se ve agravado si la persona está expuesta a contaminación por mercurio (amalgama de los dientes 55 % mercurio, lámparas halógenas/fluorescentes-cuando se funden o parpadean, contaminación minera, pescado contaminado, aire contaminado con altos niveles de diésel y del mercurio expulsado por su combustión, etc.), pues el mercurio también se une al zinc y al yodo, inutilizando sus funciones, se refuerza en el daño con el flúor.

Un síntoma de intoxicación por flúor fácilmente perceptible en la población infantil (pues sus dientes están en formación), es la presencia de manchas blancas en los dientes.

A nivel histórico, como anécdotas, en los primeros experimentos de refinamiento de uranio para hacer la bomba atómica, se pensaba que toda la toxicidad del proceso venía del uso de flúor.



</doc>
<doc id="22295" url="https://es.wikipedia.org/wiki?curid=22295" title="Juan Pablo II">
Juan Pablo II

Juan Pablo II (en latín: "Ioannes Paulus II"), de nombre secular Karol Józef Wojtyła (Wadowice, Polonia, 18 de mayo de 1920-Ciudad del Vaticano, 2 de abril de 2005), fue el papa 264 de la Iglesia católica y soberano de la Ciudad del Vaticano desde el 16 de octubre de 1978 hasta su muerte en 2005. Fue canonizado en 2014, durante el pontificado de Francisco.

Tras haber sido obispo auxiliar (desde 1958) y arzobispo de Cracovia (desde 1962), se convirtió en el primer papa polaco de la historia, y en el primero no italiano desde 1523. Su pontificado de casi 27 años fue el tercero más largo en la historia de la Iglesia católica, después del de san Pedro (se cree que entre 34 y 37 años, aunque su duración exacta es difícil de determinar) y el de Pío IX (31 años).

Juan Pablo II fue aclamado como uno de los líderes más influyentes del siglo XX, recordado especialmente por ser uno de los principales símbolos del anticomunismo, y por su lucha contra la expansión del marxismo por lugares como Iberoamérica, donde combatió enérgicamente al movimiento conocido como la teología de la liberación, con la ayuda de su mano derecha y a la postre sucesor, Joseph Ratzinger.

Jugó asimismo un papel decisivo para poner fin al comunismo en su Polonia natal y, finalmente, en toda Europa, así como para la mejora significativa de las relaciones de la Iglesia católica con el judaísmo, el islam, la Iglesia ortodoxa oriental, y la Comunión anglicana.

Entre los hechos más notorios de su pontificado destacó el intento de asesinato que sufrió el 13 de mayo de 1981, mientras saludaba a los fieles en la plaza de San Pedro, a manos de Mehmet Ali Ağca, quien le disparó a escasa distancia entre la multitud. Tiempo después el terrorista fue perdonado públicamente por el pontífice en persona. A este se sumó otro atentado ocurrido en Fátima en la noche del 12 al 13 de mayo de 1982 a manos del sacerdote ultraconservador Juan María Fernández Krohn, hecho que no trascendió hasta después de la muerte del pontífice.

Fue uno de los líderes mundiales más viajeros de la historia, visitó 129 países durante su pontificado. Hablaba los siguientes idiomas: italiano, francés, alemán, inglés, español, portugués, ucraniano, ruso, croata, esperanto, griego antiguo y latín, así como su idioma madre polaco. Como parte de su especial énfasis en la llamada universal a la santidad, beatificó a y canonizó a 483 santos, más que la cifra sumada de sus predecesores en los últimos cinco siglos. El 19 de diciembre de 2009, Juan Pablo II fue proclamado "venerable" por su sucesor, el papa Benedicto XVI, quien posteriormente presidió la ceremonia de su beatificación el 1 de mayo de 2011 (el Domingo de la Divina Misericordia), y fue canonizado junto con el papa Juan XXIII el 27 de abril de 2014 (otra vez el Domingo de la Divina Misericordia) por el papa Francisco.

Karol Józef nació el 18 de mayo de 1920 en Wadowice, un pueblo de Polonia cercano a Cracovia.

Era el menor de los tres hijos del matrimonio integrado por Karol Wojtyła y Emilia Kaczorowska. Su madre era una ferviente católica, y se las arregló para que su hijo naciera cerca de un templo, pues quería que lo primero que oyera su hijo fueran los «cánticos a Dios». Cuando Karol aún era muy pequeño, su madre le decía a otras mujeres: "Verán que mi pequeño Karol será una gran persona". Su madre falleció en 1929, cuando él tenía nueve años. Su hermana Olga había muerto antes de que él naciera. Su hermano mayor Edmund, que era médico, murió en 1932 por contagio de una enfermedad cuando curó a un hombre de condición humilde. Junto con su padre, Karol se trasladó a Cracovia para iniciar sus estudios en la Universidad Jagellónica. Su padre, un suboficial del ejército polaco, murió en 1941 durante la ocupación de Polonia por la Alemania nazi. Su padre siempre lo guió en el camino de la fe y el amor cristiano.

Al terminar sus estudios de educación media, una época en la que destacó como consumado ajedrecista (llegando a proclamarse vencedor en varios campeonatos estudiantiles), se matriculó en la Universidad Jagellónica de Cracovia y también en una escuela de teatro. Cuando las fuerzas de alemanas cerraron la Universidad, en septiembre de 1939, el joven Karol tuvo que trabajar en una cantera y luego en una fábrica química (Solvay), para ganarse la vida y evitar que lo deportaran a Alemania. Fichado por la Gestapo, se refugió en una buhardilla de Cracovia. En esa época se unió al grupo del célebre actor polaco Mieczysław Kotlarczyk, creador del teatro Rapsódico, con el cual interpretó papeles de contenido patriótico.

Durante la ocupación alemana de Polonia, cultivó especialmente la cultura, el teatro y las amistades, en el contexto del grupo Unia, formado por jóvenes católicos que pretendían resistir, tanto de forma pacífica (así Wojtyła) como de acción (ayudando directamente a los judíos o usando la violencia), a la ocupación nazi. Posteriormente, su situación se complicó y debió refugiarse en los subterráneos del arzobispado de Cracovia.

Importante para su crecimiento espiritual fue la persona de un sastre, Jan Tyranowski, quien le dio a leer a San Juan de la Cruz. Se conocieron en 1940; Tyranowski reunía a un grupo de jóvenes.

Uno de los sitios donde más le gustaba ir a rezar y descansar era Kalwaria Zebrzydowska, donde habían trabajado su abuelo y bisabuelo como guías de los peregrinos que iban allí.

En 1943 ingresó en el seminario clandestino que había fundado monseñor Adam Stefan Sapieha, cardenal arzobispo de Cracovia, iniciando la carrera de Teología. A comienzos de 1945 los soviéticos entraron en Cracovia y el futuro papa salvó la vida de una curiosa manera, casi milagrosa, gracias a Vasily Sirotenko, un universitario ruso que, antes de ser enviado a liberar Cracovia como oficial, estudiaba el último curso de Historia; la orquesta roja (espías prosoviéticos infiltrados en el ejército alemán) informó entonces de que los alemanes iban a asesinar a unos obreros polacos esclavizados por ellos; atacado ese grupo por los rusos y obligado a rendirse, estos descubrieron entre los ochenta obreros polacos liberados en una cantera de la fábrica Solvay a 18 seminaristas. Siguiendo las directrices de Stalin todos fueron enviados a un gulag de Siberia de donde no regresaron, pero no el futuro papa, ya que el comandante necesitaba a alguien como él que conociese idiomas y le tradujese los libros en latín y alemán que había estado compilando para seguir su carrera tras la guerra; es más, Wojtyla sabía incluso ruso por ser su madre de etnia rutena, según señala Pedro Beteta López en su libro "Recordando a Juan Pablo II" (2009). Sirotenko impidió así su expatriación a Siberia, incluso a pesar de la oposición de un comisario político ruso. Sin duda este trágico hecho debió reforzar su antiestalinismo.

Fue ordenado sacerdote el 1 de noviembre de 1946 en la capilla privada arzobispal. Poco después se trasladó a Roma para asistir a los cursos de la Facultad de Filosofía del Pontificio Ateneo "Angelicum", y obtuvo el doctorado en Teología con la tesis "El acto de fe en la doctrina de San Juan de la Cruz".

En 1948 regresó a Polonia y ejerció su primer ministerio pastoral como vicario coadjutor de la parroquia de Niegowić, en los alrededores de Cracovia, durante trece meses. En noviembre de ese mismo año obtuvo la habilitación para ejercer la docencia en la Facultad de Teología de la Universidad Jagellónica. El 17 de agosto de 1949 se trasladó como vicario a la parroquia de San Florián, en Cracovia, donde ejerció el ministerio durante dos años, alternándolo con su trabajo de consejero de los estudiantes y graduados de la universidad estatal de esa ciudad.

Era muy popular entre los estudiantes, con los que iba muchas veces de excursión, cosa que no era común en aquellos tiempos, pues podía llamar la atención de las autoridades policiales.

Nombrado profesor de Teología moral y Ética social del seminario metropolitano de Cracovia el día 1 de octubre de 1953, comenzó en 1954 a impartir clases de Ética en la Facultad de Filosofía de la Universidad Católica de Lublin, en la que dos años después fue nombrado director de dicha cátedra.

El 4 de julio de 1958, el papa Pío XII lo consagró obispo auxiliar de la arquidiócesis de Cracovia, bajo el administrador apostólico, arzobispo Eugeniusz Baziak.

A partir del 11 de octubre de 1962, comenzó a tomar parte activa en el Concilio Vaticano II. Destacan sus puntualizaciones sobre el ateísmo moderno y la libertad religiosa. Realizó una importante contribución a la elaboración de la constitución "Gaudium et spes". El cardenal Wojtyła participó también en las cinco asambleas del Sínodo de los Obispos, anteriores a su Pontificado. El 8 de diciembre de 1965 pasó a formar parte de las congregaciones para los Sacramentos y para la Educación Católica, y del Consejo para los Laicos. En 1962, al morir el arzobispo Baziak, fue nombrado vicario capitular y el 30 de diciembre siguiente el papa Pablo VI lo consagró arzobispo de Cracovia. El 29 de mayo de 1967 fue nombrado cardenal, lo que le convirtió en el segundo más joven de la época, con 47 años de edad.

Durante el sínodo de obispos sobre la catequesis celebrado en octubre de 1977 en Roma, coincidió por primera vez con Joseph Ratzinger, entonces cardenal de Múnich.

El 28 de septiembre de 1978 murió Juan Pablo I en circunstancias no aclaradas, tras un pontificado de 33 días. El 16 de octubre de 1978, tras dos días de deliberaciones del cónclave, Wojtyła fue elegido sucesor de San Pedro. Adoptó el nombre de "Johannes Paulus pp II" (Juan Pablo II) y se convirtió, con 58 años, en el papa más joven del siglo XX y en el primero no italiano desde el neerlandés Adriano VI (1522-1523). El 5 de noviembre visitó Asís, en el primero de sus 144 viajes por Italia.

El 25 de enero de 1979 emprendió el primero de sus 104 viajes fuera de Italia: República Dominicana y México. El último fue el 14 de agosto de 2004 al santuario mariano de Lourdes, en Francia. En total visitó 129 países diferentes, algunos de ellos varias veces.

Juan Pablo II se propuso el gran objetivo de posicionar a la Iglesia como faro y guía del mundo contemporáneo, en cinco direcciones:





A lo largo de sus casi 27 años de pontificado nombró a un total de 232 cardenales.

Como papa, Wojtyła impuso un nuevo estilo al desechar la silla gestatoria usada por sus antecesores para mostrarse en público, se acercó a la calle y a las multitudes, mostrando sus simpatías por niños y jóvenes. Debido a sus múltiples viajes al extranjero fue conocido entre los medios de comunicación, en particular en Hispanoamérica, como «el atleta de Dios», «el caminante del Evangelio», el «papa viajero» o el «papa peregrino».

Durante su prolongado mandato, Juan Pablo II superó numerosas marcas: no solo fue el pontífice más viajero hasta el momento, sino también el que proclamó más santos y beatos durante su pontificado (el número de santos y beatos elevados a los altares por él equivale al llevado a cabo en los cuatrocientos años anteriores).

Antes de ser elegido papa, Wojtyła también mostró su capacidad como poeta, filósofo y dramaturgo. Entre sus escritos destaca la obra teatral "El taller del orfebre", convertida más tarde en ópera rock. La obra se publicó por primera vez en Varsovia en 1960, cuando Wojtyla era obispo auxiliar de Cracovia, mientras que en España se editó por primera vez en 1980, tras su elección como papa.

El 13 de mayo de 1981 Mehmet Ali Ağca disparó contra el papa, mientras éste se desplazaba por la Plaza de San Pedro en un vehículo abierto. El pontífice fue herido en la mano, brazo y abdomen. Pocos años más tarde en diciembre de 1983, el papa lo visitó a la cárcel de Rebibbia, conversó con él y le otorgó el perdón. El atentado motivó la construcción de un vehículo especial con cristales blindados diseñado especialmente para este tipo de actos y que fue popularmente bautizado como papamóvil. Un año después, en la noche del 12 al 13 de mayo de 1982, Juan Pablo II sufrió un nuevo atentado en Fátima (Portugal) adonde había llegado para agradecer a la Virgen María por haber salvado su vida. En esa ocasión un sacerdote español ultraconservador, Juan Fernández Krohn, quiso ensartarlo con una bayoneta pero fue inmovilizado apenas a tiempo, aunque llegó a visualizarse la presencia de sangre en la vestimenta papal, todo lo cual fue revelado por el cardenal Stanislaw Dziwisz años después.
Desde la agresión de Mehmet Ali Ağca comenzó a sufrir diversos problemas de salud: además de las dificultades que tuvo para recuperarse de las heridas de bala que sufrió en el estómago y en una mano, padeció distintos accidentes y dolencias (ver la sección sobre sus dolencias físicas).

A fines de los años 1980, a pesar de sus dolencias físicas, su actuación en Polonia y su influencia en los acontecimientos que se produjeron en el entonces bloque comunista contribuyeran de modo considerable a la caída del comunismo soviético y a la democratización de Europa oriental, según coinciden distintos historiadores y escritores.

El 1 de julio de 1986, Juan Pablo II visitó Colombia —como consecuencia de la tragedia de Armero en Tolima— y fue al lugar de los hechos, y frente a una gran cruz oró por un rato y nombró el sitio como "lugar santo" en honor a los 25 000 muertos de esa trágica escena que tuvo que vivir el pueblo colombiano una semana después del holocausto de la toma del Palacio de Justicia en Bogotá, en el cual murieron 80 personas (o incluso más).

Más de una década después, y pese a su implacable deterioro físico, en marzo de 2003 Juan Pablo II se opuso con todas sus fuerzas y autoridad a la invasión estadounidense de Irak. En esa misión evidenció la misma determinación que había mostrado al inicio de su pontificado para mediar el Conflicto del Beagle entre Argentina y Chile en 1978, cuando se encontraban al borde de un enfrentamiento.

Entre los principales episodios de su pontificado está la primera visita de un Papa a una iglesia luterana (Roma, 1983), la primera a una sinagoga (Roma, 1986), la Jornada Mundial de Oración por la Paz (Asís, 1986) y la excomunión del obispo Marcel Lefebvre (1988). Ese año se produjo un hecho histórico: Juan Pablo II visitó un país ortodoxo, Grecia, y entró en una mezquita, la de Damasco (Siria), fue la primera vez que un Pontífice católico pisaba una mezquita y oraba en su interior.
Asimismo, figuran el primer encuentro de un papa con una comunidad musulmana (Casablanca, 1985), el Jubileo de la Redención de 1983, a partir del cual creó las Jornadas Mundiales de la Juventud, celebradas en Roma (varias veces), Buenos Aires (Argentina), Santiago de Compostela (España), Denver (Estados Unidos), Manila (Filipinas), Czestochowa (Polonia), París (Francia), Toronto (Canadá), Colonia (Alemania), Sídney (Australia), Madrid (España), Río de Janeiro (Brasil), Cracovia (Polonia) y, próximamente en 2019, Panamá.

También destaca el encuentro con el último presidente de la URSS, Mijaíl Gorbachov, en diciembre de 1989, la normalización de la Iglesia católica en los países europeos hasta entonces comunistas, y la visita realizada en enero de 1998 a Cuba, donde fue recibido con todos los honores por Fidel Castro.

Aparte de sus catorce encíclicas, con Juan Pablo II se han publicado los nuevos Códigos de Derecho Canónico Latino (1983) y Oriental, así como el Catecismo Universal de la Iglesia Católica (1992), fruto del sínodo especial de obispos de 1985, dedicado al Concilio Vaticano II.

Juan Pablo II pidió perdón por los errores cometidos por la Iglesia Católica entre ellos, el del científico italiano Galileo Galilei ([1564-1642) a quien la Inquisición le hizo retractarse de sus teorías heliocéntricas el 22 de junio de 1633.

Su gran deseo, que materializó, fue llegar al año 2000, abrir la Puerta Santa de la Basílica de San Pedro e introducir la Iglesia en el tercer milenio con el Jubileo del año 2000. En la primavera de 2000 pudo por fin pisar Tierra Santa. Visitó el Monte Nebo, donde (según la Tanaj o Antiguo Testamento) el profeta Moisés vio la Tierra Prometida antes de morir; Belén, Jerusalén, Nazaret y varias localidades de Galilea.

Durante ese viaje, Juan Pablo II, el primero en reconocer en 1986 los derechos nacionales del pueblo palestino y entablar relaciones diplomáticas plenas con Israel en 1994, ofició misa en la plaza del Pesebre de Belén, pidió perdón en el Muro de las Lamentaciones y en el Museo del Holocausto por los errores cometidos por los cristianos que persiguieron a los judíos y celebró misa en el Santo Sepulcro.

Al concluir su pontificado con su muerte, Juan Pablo II dejó pendientes dos viajes: uno a Moscú, ante la oposición del patriarca ortodoxo Alejo II, que acusaba a la Iglesia católica de "proselitismo" en su área de influencia y otro a China, donde el régimen comunista prohíbe la obediencia de la Iglesia católica china a la Santa Sede, con quien además tuvo conflictos a causa del reconocimiento de Taiwán desde 1949.

Juan Pablo II fue el primer pontífice que salió de la Ciudad del Vaticano para ser hospitalizado. Desde el atentado del 13 de mayo de 1981, fue internado en el Policlínico Agostino Gemelli en varias oportunidades: el 20 de junio del mismo año, por una infección derivada de la herida sufrida; el 15 de julio de 1992, en que se le practicó una colecistectomía, con extirpación adicional de treinta centímetros de intestino por presencia de un adenoma tubulovelloso benigno; el 11 de noviembre de 1993, por una luxación del hombro; el 28 de noviembre de 1995 por una fractura femoral; el 8 de septiembre de 1996 para una operación de apendicitis. El avance de la enfermedad de Parkinson lo debilitó hasta la indefensión, limitando su capacidad de habla. Su sucesor Joseph Ratzinger, señaló que en los últimos años del pontificado de Juan Pablo II, el sufrimiento que padeció fue casi una forma de gobierno:

El deterioro físico de Juan Pablo II se incrementó hasta su fallecimiento en 2005. En aquel año tuvo que ser hospitalizado por un síndrome de dificultad respiratoria. Se le realizó una traqueotomía a mediados de marzo. Hacia finales del mismo mes su estado se agravó y entre el 31 de marzo y el 1 de abril sufrió una septicemia por complicación de una infección de vías urinarias.

Falleció el 2 de abril de 2005 a las 21:37 (la noche previa al Domingo de la Divina Misericordia). Pocos minutos después, Monseñor Leonardo Sandri anunció la noticia a las personas congregadas en la Plaza de San Pedro y al mundo entero. Los días después de su muerte, algunos periódicos publicaron que su última palabra fue ""Amén"", sin embargo la Santa Sede desmintió esta versión y afirmó que las últimas palabras fueron en polaco: ""Pozwólcie mi iść do domu Ojca"" ("Déjenme ir a la casa de mi Padre"). La muerte fue comprobada por el cardenal camarlengo Eduardo Martínez Somalo. El Camarlengo comunicó la muerte al cardenal Camillo Ruini, como «Vicario para la Urbe» y el Cardenal-Decano del Colegio cardenalicio, Joseph Ratzinger, informó oficialmente a todos los Cardenales convocándolos al Cónclave, al declararse la Sede Vacante.

Al ser anunciada su muerte, en medio del rezo del Rosario, el público presente en la Plaza de San Pedro prorrumpió en nutridos aplausos. Las luces de su habitación en el Palacio Apostólico se apagaron por un instante para comunicar de esta manera el momento de su fallecimiento, pero luego fueron encendidas nuevamente y así permanecieron.

Su muerte se produjo debido a una septicemia y a un colapso cardiopulmonar irreversible, agravado por su enfermedad de Parkinson. Tenía 84 años y 11 meses. En su agonía, le dictó a su secretario, Stanisław Dziwisz, una carta en la que decía:

""Soy feliz, séanlo también ustedes. No quiero lágrimas. Recemos juntos con satisfacción. En la Virgen confío todo felizmente"". El portavoz del papa, Joaquín Navarro Valls afirmó inicialmente que el pontífice, en sus últimos momentos, dedicó unas palabras a la multitud, sobre todo gente joven, reunida en la Plaza de San Pedro ("Yo los he buscado y ahora ellos vienen a buscarme, les doy las gracias"), haciendo el gesto de la bendición hacia la ventana de sus aposentos, hacia los fieles apostados en la Plaza de San Pedro. Sin embargo, el médico que certificó la muerte ha señalado que el papa permaneció inconsciente durante los últimos cincuenta minutos de su vida y que, por lo tanto, tales frases tuvo que decirlas al menos una hora antes de su fallecimiento.

Los funerales pusieron de manifiesto el alto grado de aprecio hacia Juan Pablo II, no solo de parte de mandatarios de muchos países, sino también de gente de toda condición social. Tuvieron una alta resonancia política por algunos gestos inesperados, como el saludo entre los mandatarios de Israel, Irán y Siria.

Después de su muerte, católicos reconocidos, desde el cardenal británico Cormac Murphy-O'Connor, hasta el arzobispo Harry Joseph Flynn y el obispo Thomas G. Doran, su sucesor en el pontificado, Benedicto XVI, como también el periódico "L'Osservatore Romano", se han referido a Juan Pablo II como Juan Pablo Magno. Aún no se sabe si este póstumo título se impondrá, ya que no existe ningún procedimiento formal para asignar este apelativo.

Muchos seguidores del pontífice demandaron que fuese canonizado tan pronto como fuera posible, gritando "Santo subito" ("Santo ya") durante los actos de exposición pública de sus restos mortales y misas de funeral.

El 13 de mayo de 2005, el cardenal Camillo Ruini, vicario para la ciudad de Roma, dio formalmente por iniciado el proceso de beatificación de Juan Pablo II; para ello, Benedicto XVI concedió el 28 de abril dispensa del plazo de cinco años de espera después de la muerte requerido por el derecho canónico para iniciar el proceso de beatificación, de modo similar a como hizo el mismo Juan Pablo II con el proceso de beatificación de la Madre Teresa de Calcuta.

El 2 de abril de 2007, dos años después de su muerte, concluyó la fase diocesana del proceso de beatificación, reuniéndose todos los testimonios sobre su vida y los presuntos milagros, entre los que destaca el de la monja francesa Marie Simon Pierre, quien aseguró haber sido curada de la enfermedad de Parkinson gracias a la intercesión del Pontífice, que había fallecido dos meses antes.

En una misa que se celebró en la Plaza de San Pedro el mismo día, el papa Benedicto XVI aseguró que el proceso avanza «con rapidez». En tal fecha, finalizada la primera fase de su proceso de canonización, le fue concedido el título de "Siervo de Dios".

El 19 de diciembre de 2009, Benedicto XVI lo declaró "venerable". Un milagro atribuido a su intercesión fue analizado y considerado inexplicable según la ciencia, por lo que tras diversas reuniones, el papa Benedicto XVI autorizó la beatificación de Juan Pablo II en enero de 2011. La ceremonia de beatificación se llevó a cabo el 1 de mayo de 2011 (Domingo de la Divina Misericordia).

A principios de 2011 el padre Federico Lombardi, portavoz de la Casa Pontificia, anunció la fecha de la beatificación y el traslado de sus restos mortales, que hasta entonces se encontraban en la cripta vaticana, hasta la capilla de San Sebastián de la Basílica de San Pedro, contigua a la de la Piedad de Miguel Ángel.

Este proceso de beatificación ha sido catalogado como el más corto de la historia moderna de la Iglesia Católica, ya que duró seis años y 30 días, superando en un mes el proceso de beatificación de Teresa de Calcuta. A pesar de ello, y según las declaraciones de Lombardi, el proceso se ha hecho de manera minuciosa, con completos estudios sobre el milagro de la curación de sor Marie Simon y la propia vida del Pontífice.

El 5 de julio de 2013 el papa Francisco firmó el decreto que autorizó la canonización de Juan Pablo II y de Juan XXIII, realizada en una ceremonia histórica en la Ciudad del Vaticano el 27 de abril del 2014.

Al inicio del pontificado de Juan Pablo II, la Santa Sede tenía relaciones diplomáticas con 84 estados. Al fallecer este papa, las tenía con 173. Igualmente, participa como miembro de pleno derecho o como observadora en varios organismos internacionales y regionales.

Las 104 visitas internacionales de Juan Pablo II fueron realizadas mayoritariamente en su doble calidad de jefe de estado y el de cabeza de la Iglesia católica. Por ello el gesto del jefe de estado del país receptor (si es de cultura cristiana) de saludarle primero con la mano (tratándose del encuentro de dos jefes de estado) y eventualmente después con la clásica reverencia y besamanos. El primer viaje que Su Santidad Juan Pablo II hizo fue a Santo Domingo, República Dominicana desde donde se trasladó a México, al que él llamaba «México siempre fiel», una frase que se ha vuelto inmortal. Los países de América Latina que más visitó, fueron México en cinco ocasiones, Brasil en cuatro, República Dominicana y Guatemala en tres (que le servían incluso de sedes para visitar otros países cercanos), y Nicaragua en dos ocasiones, al igual que Perú, El Salvador y Venezuela.

Juan Pablo II demostró además ser un hábil diplomático. Recién asumido su pontificado, debió enfrentar en diciembre de 1978 la crisis prebélica existente entre Argentina y Chile a causa de la aplicación del Laudo Arbitral dictado por la reina Isabel II de Inglaterra referente al conflicto del canal Beagle. En momentos en que ambas naciones tenían sus tropas desplegadas a lo largo de la frontera —existen incluso evidencias que indicarían el inicio de las operaciones militares—, Juan Pablo II, aprovechando los vínculos de los militares con la Iglesia, influyó decisivamente en impedir el inicio de las hostilidades enviando al cardenal Antonio Samoré como su representante, obteniendo la separación de las fuerzas y el inicio de un proceso de mediación que culminó el 29 de noviembre de 1984 con la firma del Tratado de Paz y Amistad entre ambos países.

Fue un extraordinario políglota. No solo llegó a dominar el polaco, esperanto, griego clásico, latín, italiano, francés, español, portugués, inglés y alemán, sino que también tuvo suficientes conocimientos del checo, lituano, ruso y húngaro; además, tenía conocimientos de japonés, tagalo y varias lenguas africanas. Fue un deportista ávido y un esquiador experto en su juventud.

Ha sido el primer papa en hacer un uso intensivo de los medios de comunicación y, en especial, de Internet para hacer que llegue su mensaje, además de tener acercamientos con líderes de religiones tales como la judía, musulmana, ortodoxa y tibetana (a través del Dalái Lama), entre otras.

Con uno de los pontificados más largos de la historia, son muchos los hechos significativos en él. Respecto de la política mundial, poco antes de su muerte, la BBC comentó, refiriendo una significativa toma de postura de Mijail Gorbachov: "El papa —le dijo Gorbachov entonces a su esposa Raisa— es la autoridad moral más importante del mundo y es eslavo". El entendimiento entre ambas personalidades sin duda facilitó el camino hacia la democracia en el bloque oriental".
En palabras de Wojciech Jaruzelski, último mandatario en la Polonia comunista, la visita de Juan Pablo II a Polonia en 1979, fue el "detonador" de los cambios. Con ocasión de su fallecimiento, el presidente del Parlamento Europeo, el socialista José Borrell, escribía:

El canciller alemán Gerhard Schröder, declaraba que el papa había «influido en la integración pacífica de Europa de muchas formas. Por sus esfuerzos y por su impresionante personalidad, ha cambiado nuestro mundo».

Se destaca también su empeño en pro de los derechos humanos: "Su empeño como pontífice fue no solo el difundir el Evangelio, sino el transformar el papado romano en el portavoz de los derechos humanos".

El balance de su vida, desde un punto de vista religioso y personal, lo trazó el entonces Cardenal Ratzinger —luego Benedicto XVI— en el funeral por Juan Pablo II:

Entre distintas condecoraciones, fue considerado dos veces Una de las 100 personas más influyentes de la revista Time y en 1994, fue nombrado "Persona del año".

Juan Pablo II tuvo que afrontar durante su pontificado y también tras su muerte diferentes controversias tanto a nivel interno de la Iglesia como en el encuentro con el mundo contemporáneo. Entre estas controversias la más célebres fueron:


Los extensos y trabajados documentos de la Congregación destacan aquellos puntos que son incompatibles con la doctrina católica.








Juan Pablo II redactó las 14 encíclicas siguientes (ordenadas cronológicamente y con enlace al texto completo).











Juan Pablo II fue el primer papa que recurrió a los medios fonográficos para divulgar su mensaje, bien sea en forma de discursos, oraciones (como el Rosario) y cantos gregorianos entonados por él mismo. He aquí una relación aproximada de sus grabaciones:







</doc>
<doc id="22298" url="https://es.wikipedia.org/wiki?curid=22298" title="Delta Air Lines">
Delta Air Lines

Delta Air Lines es una aerolínea comercial estadounidense cuya base está situada en Atlanta, Georgia. Es miembro fundador, junto con Aeroméxico, Air France y Korean Air, de SkyTeam, una alianza de aerolíneas globales que ofrece a los clientes un gran número de destinos alrededor del mundo, vuelos y otros servicios. Incluyendo a sus socios de SkyTeam, así como socios a nivel mundial de código compartido.

Desde el 30 de octubre de 2008, Northwest Airlines forma parte de Delta, creando así la aerolínea más grande del mundo, desplazando a American Airlines, volando a 375 destinos en 66 países, transportando unos 170 millones de pasajeros al año y con cerca de 75.000 empleados. El proceso de integración de NWA a Delta tardará entre 12 y 24 meses, para finalmente operar bajo el mismo código y un solo certificado.

Delta es la aerolínea estadounidense más grande en vuelos transatlánticos, llegando a más destinos en Europa y Asia que ninguna otra aerolínea y es el segundo operador más grande de los Estados Unidos en América Latina luego de American Airlines.

Su sede principal es el Aeropuerto Internacional Hartsfield-Jackson, en Atlanta, que en 2004 transportó a más de 50 millones de pasajeros, convirtiéndose en el aeropuerto de mayor tráfico internacional del mundo seguido de Chicago O´hare, el principal hub de United Airlines y uno de los principales hubs de American.

Además de Atlanta, Delta tenía establecidos 3 hubs secundarios en Cincinnati, Nueva York-JFK y Salt Lake City, tras la fusión con Northwest se amplían a 8 incluyendo ahora los aeropuertos de Detroit, Memphis, Minneapolis-St. Paul y Tokyo-Narita

Delta ha utilizado distintos tipos de aviones a través de su historia, como el DC-3, Boeing 747, MD-11, Lockheed L-1011 TriStar, Boeing 727, Boeing 737, Boeing 757, Boeing 767, McDonnell Douglas DC-9, DC-8, Airbus A319 y Airbus A320. Delta fue la primera compañía en utilizar el DC-8 y el DC-9.

El entretenimiento a bordo es una de las principales estrategias de Delta, ya que está implementando los servicios con los que contaban los aviones de Song Airlines, pantallas individuales con diversos servicios multimedia. Delta cuenta con este servicio en aviones de su flota Boeing 737-800, Boeing 757-200, Boeing 767-400, Boeing 777-200 y el Airbus A319

Para determinar el servicio de comidas y bebidas que recibirán a bordo los pasajeros, se utilizan diversos criterios, incluidos la hora de vuelo, la distancia y la clase de servicio.

Recientemente esta compañía fue denunciada ante los "los juzgados de Hawaii y recogida por la web de noticias jurídicas CourtHousenews.com , los hechos sucedieron en julio de 2012, cuando se desplazó hasta Massachusetts en un avión de la compañía con el objetivo de asistir a una conferencia".

A partir del 30 de octubre de 2008, Northwest Airlines hace parte de Delta Air Lines, formando así la aerolínea más grande del mundo, volando a 375 destinos en 66 países, transportado algo más de 170 millones de pasajeros al año, con cerca de 75,000 empleados a nivel mundial. El proceso de integración de NWA a Delta, tardará entre 12 y 24 meses, para finalmente operar bajo el mismo código y un solo certificado.

En el Año 2006, la aerolínea estadounidense US Airways lanzó una oferta de comprar a Delta Air Lines por 8,400 millones de dólares (unos 6,258 millones de euros) entre líquidos y acciones, en una operación que se llevaría a cabo tras la salida de Delta de la quiebra y que desembocaría en una de las mayores aerolíneas del mundo.

US Airways señaló que la unión de las dos compañías generaría unas sinergias anuales de 1,650 millones de dólares (1, 290,5 millones de euros), y que los consumidores tendrían las ventajas de una compañía "de gran tamaño que ofrece un servicio completo".
Una vez que Delta salga de la protección por quiebra, los acreedores recibirían 4,000 millones de dólares (3,128.5 millones de euros) en metálico y 78.5 millones de acciones del capital de US Airways.

El consejero delegado de Delta, Gerald Grinstein, señaló que la propuesta de US Airways "por supuesto iba a ser analizada", pero agregó que los planes de su compañía "siempre han sido salir de la quiebra en el primer semestre de 2007 como una aerolínea fuerte que se mantenga en solitario".

El Tribunal de Quiebras ha garantizado a Delta el derecho exclusivo de crear el plan de reestructuración hasta el 15 de febrero de 2007. Delta dijo: Continuaremos avanzando con decisión hacia ese objetivo", añadió Grinstein.

La compañía resultante de la ya negada fusión operaría con el nombre de Delta, informo US Airways. Si se hubiera cerrado la operación, el nuevo gigante de la aviación volaría a más de 350 destinos de los cinco continentes y se convertiría en la aerolínea con más vuelos transatlánticos y la segunda en tráfico en el área del Caribe.
US Airways está integrada en la alianza Star Alliance, mientras que Delta pertenece a Sky Team.

El comunicado de US Airways no aclaro si la nueva aerolínea tendría su sede en el hogar de esta compañía, en Tempe (Arizona) o en el de Delta, en Atlanta (Georgia).

A Principios del 2007, US Airways sube oferta por Delta a 10,200 millones de dólares frente a 8,400 millones de dólares anteriores.
US Airways Group Inc. anunció que a principios de dicho año que aumento su oferta sobre Delta Air Lines Inc hasta los 10,200 millones de dólares frente a los 8,400 millones ofertados anteriormente.

Citigroup, el asesor de US Airways, estimo que esta nueva propuesta ofrecerá a los acreedores no asegurados de Delta entre 12,700 y15,400 millones de dólares en valor, lo que representa un prima significativa de entre 9,400 y 12,000 millones de dólares respecto al plan de Delta. 

La operación seria creativa en los beneficios por acción de US Airways en el primer año después de completar la fusión.

En enero de 2007, la aerolínea estadounidense US Airways Group retiró su oferta de compra a Delta Airlines, luego de que el comité oficial de los acreedores de Delta le informara que no apoyará su propuesta.

La compañía US Airways anunció que retiraba su oferta de USD 10,200 millones para adquirir la aerolínea rival Delta, al no haber recibido una respuesta positiva del comité de acreedores de ésta aerolínea.

Para marzo de 2008 Delta Airlines puso en marcha un plan de retiro voluntario dirigido a 30,000 trabajadores, más de la mitad de su plantilla. Las bajas incentivadas no afectaron a los pilotos de Delta, que tienen un convenio sindical propio, como tampoco a los empleados de Comair, la aerolínea regional del grupo, con sede en Kentucky.

En un comunicado enviado al regulador bursátil estadounidense (SEC), Delta anunciaba una serie de medidas destinadas a compensar los precios récord del petróleo y la desaceleración de la economía estadounidense.

El plan de retiros, que fue lanzado en abril de 2008, afecta a más de la mitad de su plantilla, que ascendía a 55,000 trabajadores a finales de 2007, y a 47,000 personas según los datos más recientes del grupo aéreo.

La aerolínea había estimado un precio para la compra de petróleo a 90 dólares por barril, pero tras una subida del 20% en los meses comprendidos entre enero a marzo de 2008, ha llegado a alcanzar una cotización récord de 111 dólares en Nueva York (Cotización a marzo de 2008).

La notificación de Delta Airlines también tenía previsto recortar en un 5 % su capacidad en el mercado estadounidense durante 2008. Debido a esto Delta empezó el plan de fusión con Northwest Airlines.

Katherine Lee Hinton (nacida en 1974), también conocida como Katherine Lee, y también por su apodo Deltalina, es una azafata americana para Delta Air Lines y, desde 2008, principal presentadora de videos de seguridad a bordo de Delta. El primer video de seguridad con Deltalina fue lanzado en febrero de 2008 y rápidamente se hizo popular, no sólo a bordo, sino que también se convirtió en uno de los videos más vistos en YouTube poco después de su lanzamiento.

Con sus pómulos altos y una amplia sonrisa de bienvenida, Lee fue rápidamente comparada con Angelina Jolie; el apodo Deltalina (de "Delta" + "-lina") apareció en los blogs y los comentarios de YouTube.

Ella también apareció en:



En 2016, se mostró el plan para realizar una alianza con la aerolínea mexicana Aeroméxico. Esta alianza consiste en la compartición de rutas entre México y Estados Unidos. Así mismo el poder usar slots entre las dos compañías en el Aeropuerto Internacional de la Ciudad de México. La alianza fue aprobada a finales de 2016 y, posteriormente, Delta lanzó una oferta de compra por el 39.8% de acciones de Aeroméxico.

El vuelo 191 de Delta Airlines fue un vuelo nacional que cubría la ruta Aeropuerto Internacional Fort Lauderdale-Hollywood - Aeropuerto Internacional de Los Ángeles, con escala en el Aeropuerto Internacional de Dallas-Fort Worth el 2 de agosto de 1985. El vuelo se estrelló en este aeropuerto en pleno aterrizaje, golpeando un tanque de agua cercano a la pista de aterrizaje 17L. 128 pasajeros y 8 miembros de la tripulación (incluidos los pilotos y el ingeniero de vuelo) murieron junto con un conductor en tierra, William Mayberry, que pasaba por el aeropuerto; 3 tripulantes de cabina y 24 pasajeros sobrevivieron al accidente.

El accidente fue causado por microrráfagas, un fenómeno meteorológico que consiste en densas nubes de tormenta eléctrica y vientos de cola fuertes, y que para 1985 ya se había cobrado víctimas en accidentes similares a éste, siendo difíciles de detectar con radares meteorológicos comunes. Actualmente los radares Doppler equipados en aviones e instalaciones en tierra detectan estos fenómenos.

El vuelo 1141 de Delta Airlines era un vuelo nacional entre Dallas-Fort Worth y Salt Lake City que se estrelló al despegar el 31 de agosto de 1988, matando a 14 personas a bordo y dejando heridas a 76, debido a que el avión que realizaba este vuelo no tenía los flaps desplegados para esta maniobra.

El vuelo 101 de Delta Air Lines, con ruta Aeropuerto Internacional Hartsfield-Jackson - Aeropuerto Internacional Ministro Pistarini, sufrió un severo desperfecto técnico al romperse un ala intentando aterrizar en el Aeropuerto Ministro Pistarini. El piloto tuvo que abortar el aterrizaje en medio de la fuerte tormenta y volar con el ala averiada, para después efectuar un aterrizaje de emergencia en el Aeropuerto Internacional de Carrasco.

El vuelo 415 de Delta Air Lines fue un vuelo con ruta Aeropuerto de Madrid-Barajas - Aeropuerto Internacional John F. Kennedy de Nueva York. El 5 de diciembre de 2013, el aparato, un Boeing 767-300ER, despega de Madrid; seis minutos después un fallo hidráulico parcial obligó a la tripulación a buscar un lugar donde aterrizar. El avión consiguió aterrizar de emergencia en el Aeropuerto de Madrid, dejando partes del fuselaje y restos de neumáticos en la pista de aterrizaje 32L. Aunque el avión quedó destrozado y fuera de la pista, ningún pasajero ni tripulante fue herido.

La flota de aviones de Delta Airlines tiene una edad promedio de 16,9 años y a finales de diciembre de 2017 está compuesta de las siguientes aeronaves: 



Delta fue patrocinadora oficial de los Juegos Olímpicos de Atlanta 1996 y los Juegos Olímpicos de Salt Lake City 2002. Asimismo, el estadio del equipo de baloncesto Utah Jazz se denominó Delta Center desde 1991 hasta 2006. Precisamente, Delta tiene sede en Atlanta y tiene un centro de conexiones en Salt Lake City.



</doc>
<doc id="22301" url="https://es.wikipedia.org/wiki?curid=22301" title="Aeropuerto Internacional Hartsfield-Jackson">
Aeropuerto Internacional Hartsfield-Jackson

El Aeropuerto Internacional Hartsfield-Jackson (IATA: ATL, ICAO: KATL), conocido localmente como Aeropuerto de Atlanta, Aeropuerto Hartsfield y Hartsfield-Jackson, se localiza a 11 kilómetros (7 millas) al sur del Distrito financiero de Atlanta, Georgia, Estados Unidos. Es el segundo aeropuerto más transitado del mundo por tráfico de pasajeros después del Aeropuerto de Londres-Heathrow pero el primero por aterrizajes y despegues. 

El aeropuerto es el principal Centro de conexiones de Delta Air Lines, GeorgiaSkies, Southwest Airlines, Delta Connection bajo el nombre de Shuttle America y la subsidiaria de Delta Connection, Atlantic Southeast Airlines; el centro de conexiones de Delta es el centro más grande del mundo. En Delta Air Lines volaron el 55,4% de los pasajeros del aeropuerto en el 2008, en AirTran volaron el 19,27% y en Atlantic Southeast Airlines voló el 12,94% de los pasajeros.
El aeropuerto cuenta con 151 puertas domésticas y 28 internacionales.

El Hartsfield-Jackson mantuvo su clasificación como el aeropuerto más ocupado en el 2008, tanto en términos de pasajeros como en número de vuelos, atendiendo a 90.0 millones de pasajeros y a 978.824 vuelos respectivamente. Muchos de estos vuelos se originan en los Estados Unidos, ya que Atlanta sirve como un punto de transferencia mayor para vuelos desde y hacia pequeñas ciudades del Sur de Estados Unidos.

El Aeropuerto Internacional Hartsfield-Jackson tiene servicio internacional hacia América del Norte, América del Sur, América Central, Europa, Asia y África. Como pasarela internacional hacia los Estados unidos, el Hartsfield-Jackson se encuentra en el séptimo lugar; el Aeropuerto Internacional John F. Kennedy en Nueva York es el primero.

Sin embargo, el aeropuerto se está convirtiendo en una importante puerta de entrada para el embarque de pasajeros con vuelos a otros países. En el 2008, el Aeropuerto de Atlanta vio un incremento en el tráfico internacional de 3,18 por ciento sobre el año anterior. Más de 4,6 millones de pasajeros abordaron vuelos internacionales en el Hartsfield-Jackson en el 2008.

El aeropuerto se localiza parcialmente en la ciudad de College Park, Georgia que está al sur del límite con la ciudad de Atlanta, pero se localiza mayoritariamente en áreas desincorporadas en los condados Fulton y Clayton, los límites de las ciudades de Atlanta, College Park, East Point y Hapeville se extienden por el aeropuerto. Al aeropuerto se puede llegar por el MARTA.

El aeropuerto de Atlanta tiene más vuelos sin escala y destinos que ningún centro de conexiones de aerolínea en el mundo. Cuenta con 261 destinos sin escala, incluyendo 83 destinos internacionales en 54 países, desde 196 puertas distribuidas en seis salas: T, A, B, C, D y E. Entre ellos también se encuentran los vuelos de carga.

La Sala T (originalmente "Puertas T"; están directamente conectadas al edificio Terminal) tiene 15 puertas: T1-T15

La Sala A tiene 34 puertas: A1-A34

La Sala B tiene 35 puertas: B1-B34, B36

La Sala C tiene 48 puertas : C1-C22, C30-C53, C55-C57.

La Sala C es el centro de conexiones principal de Southwest Airlines y de Delta Connection

La Sala D tiene 36 puertas: D1, D1A, D2-D8, D8A, D9-D11, D11A, D12-D16, D21-D37.

La sala D también posee puertas de desembarque, sin asignación, que pueden ser usadas por cualquier línea aérea.

La Sala Internacional E tiene 28 puertas: E1-E12, E14-E18, E26-E36

Debido a restricciones de acceso, GeorgiaSkies actualmente opera desde el FBO Atlantic Aviation y provee transporte para pasajeros que conectan a las terminales principales de pasajeros.




</doc>
<doc id="22847" url="https://es.wikipedia.org/wiki?curid=22847" title="Nicanor Duarte Frutos">
Nicanor Duarte Frutos

Óscar Nicanor Duarte Frutos (Coronel Oviedo, 11 de octubre de 1956) es un político paraguayo, fue el 53º presidente de la República desde el 15 de agosto de 2003 hasta el 15 de agosto de 2008, aunque él presentara su renuncia al cargo el 23 de junio de ese mismo año, con el objetivo de asumir su banca como Senador de la República. Fue el primer presidente paraguayo que no profesó la religión católica siendo adulto, sino más bien se mantuvo cercano al protestantismo (neo-cristianismo), lo cual generó una serie de polémicas para un país tradicionalmente católico. 

En octubre de 2013 fue invitado por el Presidente Horacio Cartes a representar a la República del Paraguay como Embajador ante la República Argentina. Ante la propuesta del nombramiento, el Senado paraguayo, de forma unánime, prestó su acuerdo. Ejerció dicho cargo hasta enero de 2016.

Nació en la ciudad de Coronel Oviedo, capital del departamento de Caaguazú el 11 de octubre de 1956. Sus padres de ascendencia mestiza procedían del campo y eran militantes del Partido Colorado. Casado con Gloria María Penayo y padre de seis hijos.

Se tituló como abogado en la Universidad Católica de Asunción en 1984, obtuvo una Licenciatura en Filosofía por la Universidad Nacional de Asunción en 1989 y un Posgrado en Ciencias Políticas en la Universidad Nacional y Fundación Hans Seidel en 1992. Ejerció como periodista y abogado antes de ser Ministro de Educación durante los gobiernos de Juan Carlos Wasmosy (1993-1998) y Luis Ángel González Macchi (1999-2003).

Desde el comienzo de la campaña electoral, Duarte gozó de la condición de favorito y las encuestas le distanciaron de Julio César Franco, por el PLRA, y del empresario Pedro Fadul, estrella en su momento de la nueva oposición paraguaya, al frente del Partido Patria Querida. Estas mismas encuestas indicaban que Duarte podía ser desbancado por un candidato unitario de la oposición, pero las conversaciones a varias bandas entre liberales, la izquierda, y los oviedistas agrupados en el partido UNACE, lista independiente montada por el general Lino Oviedo como culminación del primer gran cisma en el coloradismo, no llegaron a buen término.

Sin sorpresas, el 27 de abril de 2003, Nicanor Duarte Frutos se proclamó presidente con el 37,1% de los votos, seguido por Julio César Franco con el 24% y Pedro Fadul con el 21,3%. En los comicios al Congreso, el Partido Colorado acaparó algo menos de la tercera parte de los sufragios y recibió 37 de los 80 diputados y 16 los 45 senadores. Los resultados, por tanto, estuvieron lejos de ser impresionantes, y de hecho fueron los más flojos cosechados por el partido en su historia, perdiendo mayoría en ambas cámaras.

Duarte, tomó posesión de su mandato para el período presidencial 2003-2008 el 15 de agosto, en una ceremonia a la que asistieron nueve presidentes latinoamericanos, convirtiéndose en el undécimo presidente consecutivo de su partido. Autocalificado luego de las elecciones de "líder emergente, de ruptura de antiguos paradigmas que han empobrecido al Paraguay y fracturado a la sociedad", y de "presidente que llega al Ejecutivo sin haber pactado con las oligarquías económicas familiares parasitarias de mi partido y del país", en su discurso de toma de posesión Duarte se adjudicó intenciones poco menos que revolucionarias y prometió virtualmente todo lo que podía prometerse a una nación que clamaba por cambios drásticos y regeneraciones de todo tipo. Con varias críticas al neoliberalismo, del que dijo haber sido "un fracaso" porque "avasalla la dignidad humana".

Nicanor fue presidente del Paraguay desde 2003 hasta 2008, su mandato se caracterizó por un crecimiento sostenido en materia macroeconómica, con fuertes inclinaciones al estatismo, sus estrechos lazos con la izquierda latinoamericana, además de pertenecer a una nueva generación de la clase dirigente del Partido Colorado, sin un pasado vinculado al régimen del dictador Alfredo Stroessner. Su administración se vio rezagada por las grandes disputas internas al interior de su propia agrupación política. Con un discurso populista y combativo, buscó hasta el último momento de su mandato la posibilidad de una enmienda constitucional que lo habilite para pugnar por la reelección, sin embargo, no obtuvo una mayoría en el Congreso que le otorgara vía libre a la reelección. 

A los pocos meses de asumir inicia una renegociación de la deuda externa, alcanzando un acuerdo por 318 millones de dólares.

Para hacer operativo el plan, en 2006 el Gobierno lanzó la Estrategia Nacional de Lucha contra la Pobreza. A consecuencia de esta, la inversión en desarrollo social se incrementó de USD 400 millones en 2002 a USD 1.507 millones en 2007 , dinamizando el mercado interno lo que llevó a un crecimiento económico promedio de 4,8%.

Esto fortaleció a una oposición ya en crecimiento. Se convocó a una marcha de protesta que agrupó a los principales partidos políticos de la oposición, las cinco centrales sindicales y más de un centenar de asociaciones civiles que terminó con caída de su gobierno y de su partido en 2008. Pese a su discurso progresista, las inversiones sociales eran casi inexistentes.

Durante su presidencia se realizaron obras de 700 kilómetros de asfaltado.

En 2008, el Gobierno creó – mediante un decreto presidencial– la Coordinadora Ejecutiva para la Reforma Agraria -CEPRA- cuyo objetivo principal es el de coordinar y promover el desarrollo económico, social, político y cultural, además de impulsar la gestión de las políticas públicas en asentamientos creados y contribuir al logro de la reforma agraria.

En 2006, con motivo de la visita de Estado realizada a Paraguay por los Reyes de España Juan Carlos I y Sofía, los días 6 y 7 de noviembre, le fue concedido el Collar de la Orden de Isabel la Católica y a su esposa María Gloria Penayo de Duarte, la Gran Cruz de la misma Orden (B.O.E. 28/10/2006).
A inicios de 2003 la economía de Paraguay se encontraba en una situación crítica: a puertas de un default selectivo en relación a sus obligaciones internacionales, es decir, sin poder asumir sus compromisos de pagos en el exterior (en el 2002 el pago de la deuda externa llegó a representar el 44,2% del Producto Interno Bruto-PIB), con las reservas internacionales cayendo a USD 600 millones, con un ingreso per capita inferior a los USD 1.000 y sin dinero suficiente para pagar sueldos de funcionarios públicos, que en algunas instituciones inclusive tenían atrasos salariales de 3 meses. Es importante advertir además que en el 2002 la Pobreza en general representaba el 49.7% de la población.

Ante esta situación, y a partir de su asunción al mando en agosto de 2003, Nicanor Duarte Frutos impulsó una serie de reformas que iniciaron el despegue de la economía paraguaya. En primer lugar planteó una reforma tributaria conocida, como el “triple 10”, reduciendo el impuesto a la renta de las empresas del 30% al 10%, continuando con el IVA a 10% e introduciendo el Impuesto a la renta personal (IRP) del 10%, impuesto este que iba a ser abonado solo por lo que ganacen más de 120 salarios mínimos anuales, es decir, un impuesto que pagarían los que más ganen. Luego, inicia la despolitización del Banco Central del Paraguay y recupera la confianza hacia el sistema financiero, el cual había sufrido la quiebra de varios bancos en el país, años anteriores a su asunción. Al respecto, es importante mencionar que la desconfianza en el sistema financiero paraguayo había llevado a que las tasas de intereses bancarios se dispararan a 58% inclusive.

Seguidamente, el gobierno de Duarte Frutos implementó, entre otras, las siguientes medidas:

Con las citadas reformas, el Gobierno de Duarte Frutos logró:

El Informe del Banco Mundial del año 2010 sobre la Pobreza en el Paraguay concluyó, entre otras cosas, que del 2003 al 2008 (gobierno de Duarte Frutos):

Asimismo, en el año 2012 el Banco Mundial emitió un comunicado de prensa titulado ""Nuevo Informe del Banco Mundial revela que la clase media en Paraguay aumentó en la última década."" Dicho informe menciona que ""...la clase media en Paraguay aumentó en un 45 por ciento pasando de 1 millón a 1.5 millones de personas en la última década y representando el 24% de la población en el año 2009."" Seguidamente pasa a detallar que ""...la clase media de la región creció hasta comprender unos 152 millones de personas en 2009, comparado con 103 millones en 2003, un aumento del 50 por ciento. Para Paraguay, la clase media aumentó en ese periodo en casi 500.000 personas más, cifra que representa un crecimiento de 7.6 por ciento de la población total del país que han podido acceder a la clase media en la última década.""

Durante los años posteriores a su Asunción al poder y más precisamente durante la etapa final de su mandato, se suscitaron algunas controversias entre Duarte y los principales medios de comunicación del país, manejados por grandes grupos conservadores que hacían de oposición a su gobierno. Sus críticas iban dirigidas principalmente al diario Última Hora y al canal Telefuturo, a quienes los acusaba de hacer campaña política a favor de su vicepresidente, Luis Alberto Castiglioni, quien se había convertido en su principal oponente.

En sus críticas, Nicanor hacía evidente su crispación en contra de los directivos de medios, acusándolos de mal manejo periodístico y empresarial. En numerosas ocasiones puso en duda la fortuna de sus propietarios y cómo estos llegaron a disponer de semejantes sumas monetarias en su patrimonio.

Durante un acto político, Nicanor manifestó cuanto sigue:

En fecha 14 de febrero de 2008, Duarte Frutos denunció inicialmente que, durante su estadía rutinaria en el comando paraguayo de fuerzas militares, lo intentaron matar por envenenamiento con ácido muriático. Esta denuncia la hace en un contexto de emergencia sanitaria nacional por brotes endémicos de fiebre amarilla en una ciudad aledaña a la capital de Paraguay y regiones deforestadas del interior de dicho país.

El 23 de junio de 2008 Duarte Frutos presentó su renuncia ante el Presidente del Senado, con la intención de tomar posesión como Senador por voto popular y no como vitalicio, sin embargo su renuncia fue rechazada por el congreso y si bien no pudo jurar aún como senador activo.

En octubre de 2013 fue designado embajador del Paraguay ante la República Argentina, luego de roces en las relaciones durante la presidencia de Federico Franco y la suspensión de Paraguay del Mercosur. 



</doc>
<doc id="22848" url="https://es.wikipedia.org/wiki?curid=22848" title="Chozo">
Chozo

Chozo (o pequeña choza) es un refugio de ramaje o piedra que se construía tanto a la intemperie en zonas montañosas como en los sotos, baldíos o dehesas de los campos, y que era utilizado por pastores y agricultores para pernoctar junto al rebaño o protegerse de las inclemencias del tiempo, durante las labores campesinas.

De estructura circular y acabado en forma cónica para impedir que el agua de la lluvia penetre en el chozo. La cubierta puede estar hecha con diferentes tipos de escoba, impermeable y fácil de sujetar. En España, fue un recurso tradicional de los cabreros y pastores trashumantes. En amplias zonas de las provincias de Cáceres, Soria, Navarra, Álava, La Rioja y Pontevedra hay un tipo de chozo o choza construido íntegramente de piedra en las tierras de labor o pastoreo alejadas del pueblo y que servía para almacenar las herramientas y guarecerse del mal tiempo, y como almacén provisional de los productos cosechados (patatas, castañas, centeno, maíz, etc).

Un tipo singular de chozo, común en casi todo el mundo, es el utilizado por los pastores para dormir junto a apriscos o rediles improvisados en el terreno, cambiando cada noche de lugar para dormir; en concreto en parte de España se trataba de un cajón de madera fácil de transportar.



</doc>
<doc id="22850" url="https://es.wikipedia.org/wiki?curid=22850" title="Abdomen">
Abdomen

El abdomen, panza o vientre es una cavidad del cuerpo humano situada entre la cara inferior del tórax y la cara superior de la pelvis y las extremidades inferiores, en los mamíferos, separada de la caja torácica por el diafragma. Casi todas las vísceras que contiene la cavidad abdominal pertenecen al aparato digestivo, localizadas en los dos tercios frontales del abdomen. Otros órganos, como el riñón, la glándula suprarrenal y el aparato genital femenino, son intraabdominales. El tercio posterior del abdomen comprende las vértebras lumbares, el hueso sacrococcígeo y los huesos ilíacos.

En la definición del "Diccionario de la lengua española", el vientre es la cavidad del cuerpo de los animales vertebrados, en la que se contienen los órganos principales del aparato digestivo y del genitourinario, y que es la región exterior del cuerpo, correspondiente al abdomen, que es anterior en el hombre e inferior en los demás vertebrados.

La cavidad abdominal está dividida en dos partes: una recubierta interiormente por una membrana de tipo seroso, llamada peritoneo, que forma una cavidad virtual denominada cavidad peritoneal, que comunica a su vez de forma libre con la cavidad pélvica y que contiene a los órganos del sistema digestivo; la otra se denomina cavidad retroperitoneal o simplemente retroperitoneo, y alberga a los riñones y a las glándulas suprarrenales.

En el centro del abdomen se encuentra un punto conocido como ombligo, el cual es una importante referencia anatómica para la división topográfica superficial del mismo.

El plegamiento ventral del disco embrionario trilaminar durante la cuarta semana de gestación trae como consecuencia la fusión de las hojas del mesodermo lateral, lo cual hace que se forme la cavidad celómica (que tras sucesivas fusiones de membranas formará el primordio de la cavidad corporal única). La hoja esplacnopleural rodea al derivado del endodermo y mantiene su unión con la pared posterior, formando el “meso” (mesenterio, mesogastrio, mesocolon) que es por donde entrará el aporte vascular y nervioso.

Los órganos abdominales se encuentran suspendidos en la cavidad abdominal por mesenterios, o situadas entre dicha cavidad y/o incrustadas en la pared musculoesquelética. Las vísceras abdominales son:

El esófago es un conducto músculo-membranoso (un tubo muscular), ubicado en la parte media del tórax, que se extiende desde la faringe hasta el estómago. A través del esófago pasan los alimentos hasta el estómago. Su función consiste en ser precisamente el conducto de unión entre la boca y el estómago y permitir que los alimentos lleguen a éste.

Desde la parte superior hasta la porción donde el esófago se une con el estómago hay unos 40 cm. El esófago empieza en el cuello, atraviesa todo el tórax y pasa al abdomen a través del hiato esofágico del diafragma. Habitualmente es una cavidad virtual (es decir que sus paredes se encuentran unidas y sólo se abren cuando pasa el bolo alimenticio).

El esófago está formado por:





El estómago es un reservorio muscular interpuesto entre el esófago y el duodeno, es la porción más dilatada del tubo digestivo y tiene forma de "J", pero su forma y orientación cambian constantemente, según los tiempos de la digestión y según la posición del cuerpo. Ocupa casi todo el hipocondrio izquierdo y una gran parte del epigastrio, por encima del mesocolon transverso, debajo del hígado y del diafragma. El estómago se divide en cuatro regiones:


El intestino delgado es la parte del aparato digestivo que conecta el estómago con el intestino grueso. Se divide en tres porciones: duodeno, yeyuno e íleon.

El intestino grueso se inicia a partir de la válvula ileocecal en un fondo de saco denominado ciego de donde sale el apéndice vermiforme y termina en el recto. Desde el ciego al recto describe una serie de curvas, formando un marco en cuyo centro están las asas del yeyuno íleon. Su longitud es variable, entre 120 y 160 cm, y su calibre disminuye progresivamente, siendo la porción más estrecha la región donde se une con el recto o unión rectosigmoidea donde su diámetro no suele sobrepasar los 3 cm, mientras que el ciego es de 6 o 7 cm.

Es una glándula íntimamente relacionada con el duodeno, es de origen mixto, segrega hormonas a la sangre para controlar los azúcares y jugo pancreático que se vierte al intestino a través del conducto pancreático, e interviene y facilita la digestión, sus secreciones son de gran importancia en la digestión de los alimentos.

La vesícula biliar es un órgano que forma parte del aparato digestivo de los seres humanos y animales cuadrúpedos (excepto en los caballos y en los ciervos). Está situada por debajo del hígado. Su nombre en latín es "vesica fellea".

El bazo es un órgano de tipo parenquimatoso, aplanado, oblongo y muy friable, situado en el cuadrante superior izquierdo de la cavidad abdominal, relacionado con el páncreas, el diafragma y el riñón izquierdo. Aunque su tamaño varía de unas personas a otras suele tener una longitud de 12 cm, una anchura de 8 cm y un grosor de 4 cm así como un peso de 200 g aproximadamente. Su función principal es la destrucción de células sanguíneas rojas viejas, producir algunas nuevas y mantener una reserva de sangre.


En condiciones normales, las vísceras abdominales ocupan totalmente la cavidad abdominal, siendo la cavidad peritoneal un espacio virtual, el peritoneo visceral de los órganos y el peritoneo parietal de la pared abdominal adyacente se deslizan uno sobre otro libremente. Los órganos abdominales pueden clasificarse mediante dos criterios:



La vascularización arterial de las vísceras abdominales proviene de la porción abdominal de la arteria aorta (que llega al abdomen a través del hiato del diafragma, a nivel de la vértebra T12), inmediatamente emite las arterias diafragmáticas inferiores y las arterias suprarrenales medias. Más caudalmente da tres gruesos troncos impares para las vísceras intraperitoneales, de los que surgen otras muchas arterias. El más craneal es el tronco celíaco (irriga hígado, estómago y bazo), seguido de la arteria mesentérica superior (intestino delgado) y de la arteria mesentérica inferior (intestino grueso).

Para el espacio retroperitoneal emite pares de arterias como las arterias renales (para los riñones), lumbares (pared abdominal posterior) y gonadales (testicular u ovárica). Gradualmente se divide en las dos arterias ilíacas comunes (externa e interna) y en la arteria sacra media.

El retorno venoso del abdomen corresponde a la vena cava inferior, que resulta de la fusión de las dos venas ilíacas comunes. Recibe las venas renales, lumbares y gonadales, y atraviesa el diafragma sobre el hígado. El retorno venoso de las asas intestinales –por llevar sangre que lleva incorporados los productos de la digestión y que no puede pasar así al torrente circulatorio sistémico– confluye en la venas mesentéricas, superior e inferior, las cuales junto con las venas gástricas y la vena esplénica forman el tronco de la vena porta que entra en el hígado. La vena porta se ramifica en el interior del parénquima hepático (sistema porta, que se ramifica dos veces). Una vez que la sangre de la vena porta es tratada por el hígado, en los sinusoides hepáticos, confluye en las venas hepáticas que desembocan en la vena cava inferior, que la conduce a la aurícula derecha. En otras palabras, la sangre que recoge los productos de la digestión no es conducida directamente hacia la vena cava, sino que lo hace a través del hígado mediante el sistema porta. Gran parte de los problemas hepáticos provienen de una alteración en este sistema porta hepático.

En cuanto al drenaje linfático de esta región, ésta recoge la linfa procedente del aparato digestivo y de las extremidades inferiores en la denominada cisterna del quilo, localizada en la pared abdominal posterior, entre la arteria aorta y la columna vertebral a nivel de T12-L1. Pasa al mediastino posterior por el orificio aórtico. Además, presentan una serie de ganglios linfáticos que acompañan en general a las grandes arterias, o se localizan en el hilio de los órganos.

La inervación de las vísceras abdominales corre a cargo de los dos componentes del sistema nervioso autónomo simpático y parasimpático. El sistema nervioso simpático de las vísceras abdominales procede preferentemente de los nervios esplácnicos.

Los nervios esplácnicos son tres nervios a cada lado. Se originan en la cadena simpática del tórax: el mayor de los ganglios T5-T9 o T10, el menor de T10-T11, y el inferior (conocido también como imo) de T12. Terminan, bien haciendo sinapsis o pasando hacia la víscera, en los ganglios celíacos (a ambos lados de la salida del tronco celíaco de la aorta), mesentéricos o renales.
Los nervios esplácnicos lumbares (L1-L5) y los ganglios mesentéricos superior e inferior completan la inervación simpática del abdomen. Alcanzan las vísceras con las arterias, caminando las fibras en la adventicia.
La inervación parasimpática de todos los derivados del intestino anterior y medio embrionario proviene del nervio vago, desde el esófago hasta el extremo izquierdo del colon transverso. La parte distal del colon transverso, el colon descendente, el sigmoides y el recto reciben inervación parasimpática procedente del núcleo parasimpático sacro situado en la médula espinal sacra, situado entre los segmentos espinales S2 a S4.

Los músculos transversos del abdomen son unos músculos largos, angostos y con forma triangular ubicados a los costados del abdomen por debajo del músculo oblicuo interno. Estos se originan en la cara interna de las séptima a duodécima costillas, la fascia lumbar, cresta ilíaca y el ligamento inguinal hasta que se insertan detrás del músculo recto mayor del abdomen, confundiéndose con este.

Los músculos recto mayor del abdomen son músculos que se extienden desde la línea media del pubis hasta el borde inferior de la caja torácica. Se insertan por medio de un tendón aplanado y corto, el cual tiene dos haces musculares, externo e interno, que están separados por una tira de tejidos conectivos llamada línea alba.

Los músculos piramidales del abdomen son músculos que están ubicados en la parte anterolateral del abdomen. Es un músculo de forma triangular y de tamaño reducido que tiene origen en el pubis, insertándose en la línea alba.

Con fines clínicos, como la descripción del dolor, tumores e incisiones, el abdomen se divide en regiones que se definen por líneas en la superficie de la pared abdominal anterior. Por lo general, se delinean nueve regiones cortadas por dos líneas horizontales y dos verticales:

Usando estas cuatro líneas se definen nueve regiones anatómicas que son:

Esta relación entre región anatómica externa del abdomen y vísceras intraabdominales no es exacta, porque las vísceras abdominales se mueven y sobrepasan los límites mencionados, pero sirve como indicador general. Por otra parte, es de utilización frecuente en la clínica el referir dolor en alguna de las regiones apuntadas, aunque hay que tener en cuenta que la localización del dolor visceral es pobre y se puede dar el fenómeno del dolor referido, en el que duele una zona alejada de la víscera responsable.

El abdomen de los invertebrados está constituido por una serie de placas superiores conocidas como tergitos y placas inferiores conocidas como esternitos, el conjunto se mantiene unido por una dura membrana estirable.

El abdomen contiene el tracto digestivo del insecto y los órganos reproductivos, que consta de once segmentos en la mayoría de los órdenes de insectos, aunque el undécimo segmento está ausente en el adulto de la mayoría de las órdenes superiores. El número de estos segmentos varía de una especie a otra con el número de segmentos visibles reducidos sólo a las siete de la abeja común. En los colémbolos del abdomen tiene sólo seis segmentos.

El abdomen es a veces altamente modificado. En Apocrita (abejas, hormigas y avispas), el primer segmento del abdomen se fusiona con el tórax y se llama el propodeo. En las hormigas el segundo segmento constituye el pecíolo estrecho. Algunas hormigas tienen un segmento postpetiole adicional, y los segmentos restantes forman el gáster bulboso. El pecíolo y Gáster (segmentos abdominales 2 y en adelante) se les denomina colectivamente metasoma.

A diferencia de otros artrópodos, insectos poseen apéndices sobre el abdomen solo en la forma adulta, aunque el Protura tienen apéndices rudimentarios de ida como en los tres primeros segmentos abdominales y archaeognatha poseen pequeñas, articulado "agujas" que a veces se considera como apéndices rudimentarios. Muchos insectos larvales incluyendo los lepidópteros y la Symphyta (moscas de sierra) tienen apéndices carnosos llamados propatas en sus segmentos abdominales (así como sus patas torácicas más familiares), que les permiten el agarre en los bordes de las hojas de las plantas, para caminar alrededor.

En arácnidos (arañas, escorpiones y familiares), el término "abdomen" se usa indistintamente con "opistosoma" ("cuerpo trasero"), que es la sección del cuerpo posterior a la que lleva las piernas y la cabeza (el prosoma o cefalotórax).


</doc>
<doc id="22851" url="https://es.wikipedia.org/wiki?curid=22851" title="Estómago">
Estómago

El estómago (del latín "stomachus", derivado del griego ["stomachos"], a partir del prefijo ["stoma"], «boca») es la primera porción del aparato digestivo en el abdomen, excluyendo la pequeña porción de esófago abdominal. Funcionalmente se puede decir que almacena y procesa los alimentos y nutrientes consumidos, una vez bien mezclado en el estómago. Es un ensanchamiento del tubo digestivo de diámetro entre los 8 y 11 cm situado a continuación del esófago. Sirve para que el bolo alimenticio se transforme en una papilla que de ahí en adelante será llamada quimo. 
El estómago está compuesto de dos sistemas o unidades gástricas. La primera puede denominarse unidad gástrica proximal, que incluye el estómago proximal, el esófago distal y el hiato esofágico del diafragma. La segunda es la unidad gástrica distal y comprende el antro gástrico y el píloro, aunados a la primera porción del duodeno.

Su estructura y disposición hay que entenderlos teniendo en cuenta su desarrollo embrionario. El estómago en el segundo mes de vida embrionaria comienza como una simple dilatación del intestino anterior. A continuación sufre una rotación sobre un eje longitudinal de tal modo que la cara izquierda del estómago se hace anterior, y la parte derecha se hace posterior. Por esta razón el tronco vagal del lado izquierdo, que en el tórax desciende por el lado izquierdo del esófago, pasa a una localización anterior, mientras que el derecho se sitúa en el estómago en la parte posterior. El estómago tiene además otra rotación sobre un eje posterior, de tal modo que la parte inferior, por la que se continúa con el duodeno, asciende y se coloca a la derecha, bajo el hígado. Hay que tener presente que el estómago tiene en esta fase de la vida un meso en la parte posterior (mesogastrio dorsal) y otro en la parte anterior (mesogastrio ventral) que alcanza hasta la porción superior del duodeno.

Ambos mesos también sufren las rotaciones anteriores de tal modo que determinan una serie de pliegues en el peritoneo visceral que los recubre. El mesogastrio dorsal forma el omento mayor (tras fusionarse con el meso del colon transverso), lo que determina el cierre por la parte inferior de la bolsa omental. El mesogastrio ventral da origen al omento menor, que se extiende entre el borde derecho del estómago y la porción superior del duodeno hasta el hígado y la porta hepática.

El estómago se localiza en la parte alta del abdomen. Ocupa la mayor parte de la celda subfrénica izquierda. La parte de estómago que queda oculta bajo las costillas, recibe el nombre de Triángulo de Traube, mientras que la porción no oculta se denomina Triángulo de Labbé.

Topografía:
Hipocondrio izquierdo y epigastrio.
El cardias (extremo por donde penetra el esófago) se localiza a nivel de la vértebra T11, mientras que el píloro lo hace a nivel de L1. Sin embargo, hay considerable variación de unos individuos a otros.

El esófago determina la incisura cardial, que sirve de válvula para prevenir el reflujo gastroesofágico. Hacia la izquierda y arriba (debajo de la cúpula diafragmática) se extiende el fundus [tuberosidad mayor] (ocupado por aire y visible en las radiografías simples), que se continúa con el cuerpo, porción alargada que puede "colgar" más o menos en el abdomen, luego progresivamente sigue un trayecto más o menos horizontal y hacia la derecha, para continuar con la porción pilórica, que consta del antro pilórico y del conducto pilórico cuyo esfínter pilórico lo separa del duodeno. En este punto la pared se engrosa de manera considerable por la presencia de abundantes fibras circulares de la capa muscular que forman el esfínter pilórico.

La forma aplanada del estómago en reposo determina la presencia de una cara anterior, visible en el "situs abdóminis", y una cara posterior que mira a la transcavidad de los epiplones (cavidad omental), situada detrás. Asimismo, determina la presencia de un borde inferior (curvatura mayor) que mira abajo y a la izquierda, y un borde superior (curvatura menor) que mira arriba y a la derecha. Como consecuencia de los giros del estómago en período embrionario, por la curvatura mayor se continúa el estómago con el omento (epiplón) mayor, y la menor con el omento (epiplón) menor.

El aparato digestivo es una serie de órganos huecos que forman un largo y tortuoso tubo que va de la boca al ano.

La luz del estómago tiene la presencia de unos pliegues de mucosa longitudinales, de los cuales los más importantes son dos paralelos y próximos a la curvatura menor que forman el canal del estómago o calle gástrica. Los pliegues disminuyen en el fundus y en la porción pilórica.

La pared gástrica consta de una serosa que recubre tres capas musculares (longitudinal, circular y oblicua, citadas desde la superficie hacia la profundidad). La capa submucosa da anclaje a la mucosa propiamente dicha, que consta de células que producen moco, ácido clorhídrico y enzimas digestivas.

El estómago tiene unos sistemas de fijación en sus dos extremos, los cuales quedan unidos por la curvatura menor a través del omento (epiplón) menor. A nivel del cardias existe el ligamento gastrofrénico por la parte posterior, que lo une al diafragma.

Por la parte pilórica queda unido a la cara inferior del hígado por el ligamento gastrohepático, parte del tumulto menor. Estos sistemas de fijación determinan sus relaciones con otros órganos abdominales. Sin embargo, y debido no solo a los giros del estómago, sino también al desarrollo embrionario del hígado, las relaciones del estómago se establecen a través de un espacio que queda por detrás, la cavidad omental o transcavidad de los epiplones,

La irrigación corre a cargo de ramas de la aorta abdominal. El tronco celíaco da lugar a la arteria gástrica izquierda, que recorre la curvatura menor hasta anastomosarse con la arteria gástrica derecha, rama de la arteria hepática propia (que sale de la arteria hepática común, rama del tronco celíaco); estas dos arterias llegan a formar lo que es la coronaria gástrica superior. De la arteria hepática común surge también la arteria gastroduodenal, que da lugar a la arteria gastroepiploica derecha que recorre la curvatura mayor hasta anastomosarse con la arteria gastroepiploica izquierda, rama de la arteria esplénica (que proviene del tronco celíaco); estas forman lo que es la coronaria gástrica inferior. Esta irrigación viene complementada por las arterias gástricas cortas que, procedentes de la arteria esplénica, alcanzan el fundus del estómago.

El retorno venoso es bastante paralelo al arterial, con venas gástricas derecha e izquierda, además de la vena prepilórica, que drenan en la vena porta; venas gástricas cortas y gastroepiploica izquierda que drenan en la vena esplénica; vena gastroepiploica derecha que termina en la mesentérica superior. A través de las venas gástricas cortas se establece una unión (anastomosis) entre el sistema de la vena porta y de la vena cava inferior por medio de las venas de la submucosa del esófago. En casos de hipertensión portal (la sangre que penetra en el hígado por medio de la vena porta no puede alcanzar la cava inferior, por lo que se acumula retrógradamente en las venas que drenan y forman la vena porta), la sangre dilata estas anastomosis normalmente muy pequeñas, dando lugar a las varices esofágicas. Si estas varices se rompen pueden dar una hemorragia mortal.

El drenaje linfático viene dada por cadenas ganglionares que recorren la curvatura mayor (nódulos gastroepiploicos derechos e izquierdos y nódulos gástricos derecho e izquierdo). Se complementan con los ganglios linfáticos celíacos y pilóricos. Estos ganglios tienen gran importancia en el cáncer gástrico, y hay que extirparlos en caso de extensión del cáncer. Existen distintas técnicas de resección con diferentes extensiones de la linfadenectomía. La más común es la linfadenectomía D1 pese a que en países asiáticos con alta incidencia de la enfermedad, los cirujanos expertos realizan una linfadenectomía D2 de rutina (más amplia pero con mayor morbilidad postoperatoria). La extirpación se hace de acuerdo a las barreras ganglionares, existen 16 grupos ganglionares que son:

La extirpación oncológica siempre debe obtener la última barrera ganglionar libre.

La pared del estómago está formada por las capas características de todo el tubo digestivo:

La túnica mucosa del estómago presenta múltiples pliegues (rugae), crestas (mamelones) y foveolas (criptas gástricas). Presenta a su vez tres capas:

Epitelio superficial: es un epitelio cilíndrico simple mucíparo, que aparece bruscamente en el cardias, a continuación del epitelio plano estratificado no queratinizado del esófago. En el polo apical de estas células aparece una gruesa capa de moco gástrico, que sirve de protección contra las sustancias ingeridas, contra el ácido estomacal y contra las enzimas gástricas.

Glándulas del cardias: están situadas alrededor de la unión gastroesofágica. Las células endocrinas que posee en el fondo producen gastrina.

Glándulas oxínticas, gástricas o fúndicas: se localizan sobre todo en el fondo y cuerpo del estómago y producen la mayor parte del volumen del jugo gástrico. Están muy juntas unas con otras, tienen una luz muy estrecha y son muy profundas. Se estima que el estómago posee 15 millones de glándulas oxínticas, que están compuestas por cinco tipos de células:

Glándulas pilóricas: están situadas cerca del píloro. Segrega principalmente secreción viscosa y espesa, que es el mucus para lubricar el interior de la cavidad del estómago, para que el alimento pueda pasar, protegiendo así las paredes del estómago.

Lámina propia de la mucosa: formada por tejido conectivo laxo, posee glándulas secretoras de mucus y enzimas.

Lámina muscular de la mucosa: que presenta dos capas, poco diferenciadas entre sí.

Formada por tejido conjuntivo moderadamente denso (tejido de sostén que conecta o une las diversas partes del cuerpo), en el cual se encuentran numerosos vasos sanguíneos, linfáticos y terminaciones nerviosas. Está debajo de la mucosa y forma el plexo de Meissner.

Dentro de ella se encuentran tres capas de músculo liso que son: interna u oblicua, medio o circular y externa o longitudinal.
La túnica muscular está formada de adentro hacia afuera por fibras oblicuas, el estrato circular y el estrato longitudinal. La túnica muscular gástrica puede considerarse como el "músculo gástrico" porque gracias a sus contracciones, el bolo alimenticio se mezcla con los jugos gástricos y se desplaza hacia el píloro con los movimientos peristálticos.

La túnica muscular posee sus fibras en distintas direcciones, desde más interno a más externo, teniendo fibras oblicuas, un estrato circular y un estrato longitudinal. En un corte transversal se distingue claramente esta diferencia en la disposición de las fibras musculares. Se puede observar que el estrato circular, en algunos lugares está engrosado formando los esfínteres que regulan el paso de los alimentos.

La túnica serosa, constituida por tejido conectivo laxo tapizado por una capa epitelial llamada mesotelio, envuelve al estómago en toda su extensión, expandiéndose en sus curvaturas para formar el omento menor, el omento mayor y el ligamento gastrofrénico.

El estómago está controlado por el sistema nervioso autónomo, siendo el nervio vago el principal componente del sistema nervioso parasimpático. La acidez del estómago está controlada por varias moléculas entre las que se encuentran la acetilcolina, la histamina, la gastrina, la secretina y la prostanglandina E2.


Históricamente, se creía que el ambiente sumamente ácido del estómago mantendría el estómago inmune de la infección. Sin embargo, un gran número de estudios ha indicado que la mayor parte de casos de úlceras de estómago, gastritis, linfoma e incluso el cáncer gástrico son causados por la infección de Helicobacter pylori. Uno de las causas por la que esta bacteria es capaz de sobrevivir en el estómago es por la producción de una determinada enzima llamada ureasa que metaboliza el amoniaco y el dióxido de carbono para neutralizar el ácido clorhídrico producido por el estómago.

El estómago es un órgano animal con muchas aplicaciones para el uso humano. La mayor parte (sino todas) las formas de utilizarlo son artesanales ya que es demasiado complicado el diseño de máquinas que puedan aprovecharlo, ya sea por cuestiones mecánicas o históricas.

Algunas culturas antiguas, en épocas previas al descubrimiento del hule, ya conocían formas de entretenerse con algo muy parecido a algunos deportes actuales como el fútbol o rugby. Muchas de ellas encontraban en el estómago de un animal (vacas en Europa, bisontes en Norteamérica, camellos en el norte de África) el material adecuado para la elaboración de pelotas. Si bien no eran completamente esféricas, resultaban muy útiles para proporcionar un rato de diversión a los estresados soldados o aburridos niños.

En la zona de la Cuenca del Plata (en Sudamérica), existen deliciosas comidas muy baratas y ricas en nutrientes cuya base de elaboración es el estómago. La más conocida es el «estofado de mondongo con garbanzos» (mondongo es como se le dice allí al estómago vacuno que en España se conoce como "Callos" y en México como "menudo"). También se utiliza en el locro, comida tradicional Argentina. Tradicionalmente, se utilizaba el estómago de cordero como cuajo para elaborar queso.

Aunque la forma precisa y tamaño del estómago varía ampliamente entre diferentes especies de vertebrados, las posiciones relativas de las aberturas esofágicas y duodenales se mantienen relativamente constantes. Como resultado, el órgano siempre presenta curvas, un poco a la izquierda antes de curvarse hacia atrás para cumplir con el esfínter pilórico. Sin embargo, las lampreas, mixinos, quimeras, peces pulmonados, y algunos peces teleósteos no tienen el estómago en absoluto, con la apertura del esófago directamente en el ano. Estos animales todos consumen dietas que, o bien requieren poco almacenamiento de alimentos, o ningún pre-digestión con jugos gástricos, o ambos.

El revestimiento gástrico se suele dividir en dos regiones, una porción anterior bordeado por las glándulas fúndicas y una posterior con glándulas pilóricas. Glándulas cardiacos son únicos para los mamíferos, e incluso entonces están ausentes en una serie de especies. La distribución de estas glándulas varían entre las especies, y no siempre se corresponden con las mismas regiones como en el hombre. Además, en muchos mamíferos no humanos, una porción de la parte anterior del estómago a las glándulas cardíacas se alinea con epitelio esencialmente idéntica a la del esófago. Los rumiantes, en particular, tienen un estómago complejo, las tres primeras cámaras de los cuales están alineados con la mucosa esofágica. 

En las aves y cocodrilos, el estómago se divide en dos regiones. En sentido anterior es una región tubular estrecha, el proventrículo, bordeada por las glándulas fúndicas, y conectando el verdadero estómago para el cultivo. Más allá se encuentra la molleja muscular de gran alcance, bordeada por las glándulas pilóricas, y, en algunas especies, que contiene piedras que el animal se traga para ayudar a triturar los alimentos.


</doc>
<doc id="22852" url="https://es.wikipedia.org/wiki?curid=22852" title="Intestino delgado">
Intestino delgado

El intestino delgado es la sección del aparato digestivo que conecta el estómago con el intestino grueso. Se divide en tres porciones: duodeno, yeyuno e íleon.
Cumple las funciones de digestión, absorción, barrera y además inmunidad.

Es uno de los órganos con mayor número de recambio de células de todo el organismo, ya que toda su superficie interna se renueva cada cinco días.

El intestino delgado absorbe los nutrientes necesarios para el cuerpo. Posee una longitud que oscila entre 3 y 5 metros, dependiendo de numerosas variables como la talla del individuo. En el cadáver, como consecuencia de la hipotonía del músculo liso, su longitud aumenta y su promedio es de 6.5 m.

Se localiza entre dos esfínteres: el pilórico, y el ileocecal, que lo comunica con el intestino grueso.

El quimo que se crea en el estómago, formado por el bolo alimenticio mezclado con el ácido clorhídrico, pepsinógeno y otras sustancias a partir de movimientos peristálticos, se mezcla a su vez con las secreciones biliar y pancreática (además de la propia duodenal) para no romper las capas del intestino delgado (ya que este tiene un pH altamente ácido) y es llevado al duodeno. El tránsito alimenticio continúa por este tubo a lo largo del cual se completa el proceso de la digestión, el quimo se transforma en quilo y se efectúa la absorción de las sustancias útiles.

El fenómeno de la digestión y de la absorción depende en gran medida del contacto del alimento con las paredes intestinales, por lo que cuanto mayor sea éste y en una superficie más amplia, tanto mejor será la digestión y absorción de los alimentos. Esto nos da una de las características morfológicas más importantes del intestino delgado que son la presencia de numerosos pliegues que amplifican la superficie de absorción como:


El duodeno se caracteriza por su relación con el estómago, es la porción principal donde llega el jugo pancreático y hepático pero el yeyuno y el íleon son más difíciles de distinguir y no hay una separación entre ambos.

En general, se pueden distinguir porque:


Topográficamente tanto el yeyuno como el íleon ocupan el espacio infracólico, aunque:


El final del intestino delgado es el íleon terminal que desemboca en el ciego por medio de la válvula ileocecal.

En la constitución de la pared intestinal, además de las capas usuales de mucosa, submucosa, muscular y serosa, destaca la presencia de acúmulos de tejido linfoide que alcanzan hasta la submucosa. Se localizan en el borde antimesentérico y su número es de 30 ó 40, y miden hasta 2, 5 cm de diámetro. Como se ha mencionado anteriormente, son más numerosos en el íleon.

Toda la longitud del intestino delgado queda unida a la pared posterior a través de la raíz del mesenterio. Esta unión del mesenterio a la pared posterior comienza a nivel de la vértebra L2, cruza el gancho del páncreas (por donde penetra la arteria mesentérica superior), cruza delante de la cava inferior, sigue externamente a los vasos ilíacos comunes y externos para terminar en la fosa ilíaca derecha, a nivel del promontorio, lateral a la articulación sacroilíaca derecha, a unos 6 cm, de la línea media del intestino.

La irrigación proviene de la arteria mesentérica superior, rama de la aorta, que camina dentro del mesenterio y de la que nacen las arterias:

El drenaje venoso es bastante similar, corriendo a cargo de la vena mesentérica superior, la principal constituyente de la vena porta, junto con la vena mesentérica inferior y la vena esplénica.

Es el encargado a través de nervios tanto aferentes como eferentes, respectivamente de la motilidad y la sensibilidad del intestino.

La mucosa intestinal está especializada en la digestión y la absorción de nutrientes y para ello tiene que aumentar su superficie que da a la luz, de tres maneras:


El epitelio intestinal de la mucosa está formado por diferentes tipos celulares que son:


La lámina propia presenta un tejido conectivo laxo, con vasos y nervios. Está invadido por una población linfocítica y por fibras musculares lisas provenientes de la capa muscular de la mucosa. Se le denomina músculo de Brucke y es el músculo motor de las vellosidades.

El conducto lacteal o quilífero central es un vaso linfático central de la vellosidad. Se encuentra en todo corte transversal de la vellosidad. El revestimiento del quilífero es discontinuo.

El glicocálix es fundamental en la finalización del proceso digestivo, en cuanto a que es el último eslabón de la degradación. De los elementos absorbidos, las grasas van al quilífero central, y las demás a la sangre.

Si hay glándulas mucosas en la submucosa., nos encontramos en un duodeno, y si no en un yeyuno íleon. El duodeno presenta estas glándulas que secretan una mucina que neutraliza el pH ácido del quimo.

En el tubo digestivo es característica la presencia de MALT, tejido linfoide asociado a mucosa. Este tejido linfoide se encuentra en el corion o lámina propia de la mucosa. Es por lo general un tejido linfoide difuso o nodular. Junto a este tejido linfoide se encuentran generalmente plasmocitos. En el íleon el tejido linfoide es especialmente notorio por su disposición en placas, denominadas placas de Peyer. El nódulo linfático produce una modificación en el epitelio de revestimiento.

Las glándulas de Brunner son las glándulas de la submucosa duodenal, que son características de él.

La cantidad de células caliciformes aumenta desde el duodeno al recto, las células absortivas disminuyen de duodeno a recto. En el estómago no hay células caliciformes, ya que el propio epitelio es mucígeno.

Las enfermedades vasculares del intestino delgado corresponden a diferentes etiologías. Por este motivo no existe un sistema de clasificación universalmente aceptado. No obstante todas estas etiologías pueden manifestarse en forma de hemorragia. La anomalía vascular observada con más frecuencia es la angiodisplasia, que se define como un complejo vascular dilatado que se localiza en la superficie del tracto gatrointestinal, estando presente en un 40% de las hemorragias de origen indeterminado.

En el intestino delgado la carcinogénesis se da a una velocidad significativamente más baja que en otras zonas del tracto gastrointestinal, así, a pesar de ser la superficie más grande de este, los tumores primarios del intestino delgado representan solamente el 2% del total de los tumores gastrointestinales. Este hecho se debe a varios factores, como el transcurrir rápido de un contenido principalmente líquido y con pocas bacterias. No obstante, se han identificado hasta 40 tipos diferentes de tumores del intestino delgado, el 75% de los cuales son benignos. El diagnóstico de estos sin embargo se revela complicada pues los síntomas iniciales pueden ser confundidos con otras enfermedades. 

La celiaquía o enfermedad celíaca es una enfermedad autoinmune producida por una intolerancia permanente al gluten, conjunto de proteínas presentes en el trigo, avena, cebada y centeno (TACC) y derivados, en personas con predisposición genética. Considerada tradicionalmente como un trastorno únicamente digestivo, actualmente se sabe que es una enfermedad sistémica,

Las lesiones que la enfermedad celíaca provoca en el intestino delgado no se limitan a la presencia de atrofia de las vellosidades intestinales, sino que con frecuencia consisten en cambios mínimos sin atrofia vellositaria, con inflamación leve o moderada, especialmente en los niños mayores de dos años y los adultos.

El diagnóstico es complicado, especialmente en niños mayores de dos años y adultos,

Es una enfermedad inflamatoria intestinal que puede afectar a todo el grosor de la pared intestinal, que es lo que se llama afectación transmural, y que puede aparecer de manera simultánea en varios segmentos del tubo o tracto digestivo. 

El íleon terminal (la última porción del intestino delgado) es el lugar más frecuente de afectación (hasta un 40% - 50% del total de personas con enfermedad de Crohn), seguido por el colon.




</doc>
<doc id="22854" url="https://es.wikipedia.org/wiki?curid=22854" title="Hígado">
Hígado

El hígado es un importante órgano que está presente tanto en el ser humano, como en todos los animales vertebrados. El hígado humano tiene un peso medio de 1500 g, está situado en la parte superior derecha del abdomen, debajo del diafragma, segrega la bilis esencial para la digestión de las grasas y cuenta con otras muchas funciones, entre ellas la síntesis de proteínas plasmáticas, función desintoxicante y almacenamiento de vitaminas y glucógeno. Es responsable de eliminar de la sangre diferentes sustancias que puedan resultar nocivas para el organismo entre ellas el alcohol, convirtiéndolas en inocuas. La ausencia de hígado o su falta de funcionamiento es incompatible con la vida.

La palabra hígado no deriva de su homónimo en latín "jecur", ni del griego "hepatos". Proviene de la expresión latina "ficatum jecur" que significa literalmente "hígado cebado con higos". En la antigüedad los habitantes de Roma tenían la costumbre de alimentar a ciertas aves con higos con la finalidad de obtener una delicia gastronómica, pues el hígado de estos animales adquiría de esta forma un sabor delicioso. Con el tiempo "ficatum jecur" pasó a significar simplemente hígado y la expresión fue abreviándose, transformándose primero en "ficatum", después en "fégado" y finalmente en hígado. Por lo tanto hígado e higo tienen la misma etimología en español.

El hígado tiene una forma triangular, color rojo pardo, superficie lisa y consistencia blanda y depresible. En el adulto humano mide por término medio 26 cm de ancho, 15 cm de alto y 8 cm de espesor a nivel del lóbulo derecho, su peso aproximado es 1,5 kg.

El hígado se localiza en la región superior derecha del abdomen, por debajo del diafragma, ocupa el hipocondrio derecho y una parte del epigastrio. En condiciones normales no sobrepasa el límite del reborde costal. Llena el espacio de la cúpula diafragmática, donde puede alcanzar hasta la quinta costilla, y está próximo al corazón del cual se encuentra separado por el diafragma. Está recubierto por una cápsula fibrosa, la cápsula de Glisson, sobre la cual se aplica el peritoneo.

El hígado se encuentra rodeado por el peritoneo visceral y presenta dos caras:

En la base del hígado se encuentra la vesícula biliar y el hilio hepático, que es la zona de entrada de la vena porta, la arteria hepática y la salida del conducto hepático. La estructura del hígado va a seguir las divisiones de la vena porta hepática. Tras la división de ramos segmentarios, las ramas de la vena porta, acompañadas de las de la arteria hepática y de las divisiones de los conductos hepáticos, se encuentran juntas en el espacio porta.

El hígado se divide por el ligamento falciforme en dos lóbulos principales, derecho e izquierdo. Existen otros dos lóbulos más pequeños el lóbulo cuadrado y el lóbulo caudado que para muchos anatomistas pertenecen al lóbulo izquierdo, aunque otros textos consideran que el hígado tiene cuatro lóbulos.
Existen variantes anatómicas frecuentes como el Lóbulo Hepático de Riedel donde hay una prolongación infracostal derecha que se puede confundir con hepatomegalia (aumento del tamaño hepático).

La clasificación de Couinaud divide el hígado en ocho segmentos que son funcionalmente independientes, cada uno de estos segmentos dispone de una rama de la vena porta hepática, una rama de la arteria hepática, una rama venosa de salida que tributa a las venas hepáticas y un conducto biliar por el que la bilis llega al conducto hepático. Los segmentos 5,6,7 y 8 corresponden al lóbulo derecho, 2,3 y 4 al lóbulo izquierdo y 1 al lóbulo caudado.

El hígado está cubierto por el peritoneo visceral, tiene varias conexiones con el peritoneo parietal que se llaman ligamentos del hígado, los cuales no son en realidad auténticos ligamentos, sino tractos fibrosos que dan soporte al hígado y lo sustentan sobre las estructuras adyacentes. Estos ligamentos hepáticos son los siguientes: 

La sangre llega al hígado a través de la vena porta y la arteria hepática. El sistema porta constituye el 70-75 por ciento del flujo sanguíneo y contiene sangre poco oxigenada y rica en nutrientes proveniente del tracto gastrointestinal y del bazo. La sangre arterial llega a través de la arteria hepática, rama del tronco celíaco que contiene la sangre oxigenada. La sangre de ambas procedencias se mezcla en los sinusoides hepáticos y abandona el órgano a través de las venas hepáticas, también llamadas suprahepáticas, que finalmente drenan en la vena cava inferior.

El drenaje linfático del hígado corre a cargo de vasos que desembocan en la vena cava inferior o en los ganglios linfáticos que siguen el recorrido inverso de la arteria hepática.

El hígado recibe nervios del plexo celíaco, de los nervios neumogástrico izquierdo y derecho y también del frénico derecho, por medio del plexo diafragmático. El aporte nervioso también le viene del plexo celíaco que inerva al hepático, mezcla de fibras simpáticas y parasimpáticas. Estos nervios llegan al hígado junto a la arteria hepática.

Clásicamente se considera al lobulillo hepático como la unidad funcional del órgano, un hígado humano contiene entre 50 000 y 100 000 lobulillos. Cada lobulillo tienen forma hexagonal, en el centro del hexágono se encuentra la vena centrolobulillar y en las esquinas los espacios porta. Entre las esquinas del hexágono y el centro se encuentran los sinusoides hepáticos y los hepatocitos que se disponen en forma radiada en torno a cada vena centrolobulillar. En el lobulillo hepático se mezcla la sangre arterial y venosa procedente de los espacios porta para desembocar en la vena central de cada lobulillo. Dentro del lobulillo hepático se pueden distinguir las siguientes estructuras:


Las principales células que forman parte del lobulillo hepático son las siguientes:

El hígado es un órgano o víscera presente en los vertebrados y en algunos otros animales. Es la glándula más voluminosa de la anatomía y una de las más importantes en cuanto a la actividad metabólica del organismo. Desempeña funciones únicas y vitales, entre ellos la síntesis de proteínas plasmáticas, función desintoxicante y almacenamiento de vitaminas y glucógeno. Además elimina de la sangre muchas sustancias que pueden resultar nocivas para el organismo, transformándolas en otras inocuas.
A continuación se resumen las principales funciones del hígado.

La bilis es necesaria para la digestión de los alimentos, contiene sales biliares formadas por el hígado a partir del ácido glicocólico y ácido taurocólico que a su vez derivan de la molécula de colesterol. La bilis es excretada hacia la vía biliar y se almacena en la vesícula biliar de donde se expulsa al duodeno cuando se ingieren alimentos. Gracias a la bilis es posible la absorción de las grasas contenidas en los alimentos.

Las funciones metabólicas del hígado son muy numerosas.





Algunas de las enfermedades del hígado son:

El hígado en los animales mamíferos tiene una estructura y función muy similar a la del hombre, sin embargo no puede metabolizar las mismas sustancias. En perros y gatos determinados medicamentos como el paracetamol no pueden ser metabolizados fácilmente por el hígado, por lo que resultan tóxicos con dosis muy pequeñas. Por otra parte los gatos pueden presentar una enfermedad específica del hígado que no existe en otros animales, la lipidosis hepática felina.



</doc>
<doc id="22855" url="https://es.wikipedia.org/wiki?curid=22855" title="Vía biliar">
Vía biliar

La vía biliar es un conjunto de ductos intra y extrahepáticos por los que discurre la bilis producida en el hígado hasta desembocar en la segunda porción del duodeno.
La bilis que excreta el hígado es recolectada por finos canalículos bilíferos que van confluyendo en los canales bilíferos y otros de mayor calibre hasta la porta hepática. Cada porción hepática tiene su conducto biliar (derecho e izquierdo); ambos se funden en un conducto hepático común, que se une al conducto cístico –procedente de la vesícula biliar– para formar el conducto colédoco encargado de llevar la bilis hasta la porción descendente del duodeno.

La vesícula biliar está alojada en la fosa de la vesícula biliar, en la cara visceral del hígado. Consta de un fondo, cuerpo, infundíbulo y cuello que se continúa con el ducto cístico. La túnica mucosa es sumamente irregular, en forma de panal de abeja, antes de continuarse con el ducto cístico. En la vesícula la secreción biliar se almacena hasta que un estímulo adecuado causa su liberación por la contracción de su pared muscular sin embargo

La bilis, a partir de la unión entre el ducto cístico y el ducto hepático común, sigue por el ducto colédoco que discurre por el borde libre del momento menor (ligamento hepatoduodenal). Luego se coloca por detrás de la porción superior del duodeno, atraviesa la cabeza del páncreas para drenar finalmente en la porción descendente del duodeno en la papila duodenal unido al ducto pancreático. La arteria hepática derecha emite la arteria cística destinada a la irrigación de la vesícula biliar. Las relaciones entre la arteria cística y el ducto cístico son de importancia quirúrgica en la extirpación de la vesícula biliar (colecistectomía).

La vía biliar puede visualizarse gracias a:



</doc>
<doc id="22873" url="https://es.wikipedia.org/wiki?curid=22873" title="Biblioteconomía">
Biblioteconomía

La "biblioteconomía" es un campo interdisciplinario o multidisciplinario que aplica las prácticas, perspectivas y herramientas de gestión, la tecnología de la información, la educación, la recopilación, organización, preservación y difusión de los recursos de información y la economía política de información a las bibliotecas.

"Etimológicamente biblioteconomía es resultado de biblion libro, theke almacén, nomo administración o descripción. Literalmente significa administración o descripción de los almacenes de libros. Por lo que a la biblioteconomía se le adjudica el significado de ser la disciplina o ciencia encargada de la administración de las bibliotecas. Bibliotecología es resultado de la unión de biblion libro, theke almacén y logos tratado o estudio. Lo que quiere decir que bibliotecología es el tratado o estudio del almacén de los libros, y se trata de la ciencia de las bibliotecas"

Por lo anterior la biblioteconomía no debe ser confundida con la bibliotecología. Esta última es la ciencia que estudia las bibliotecas, haciendo de éstas el objeto externo de su estudio. La biblioteconomía, en cambio, se refiere más al conjunto de técnicas y conocimientos necesarios para, internamente, regir la gestión y la ordenación de libros y documentos en el seno de una biblioteca.

Para Rodríguez Gallardo la bibliotecología “es una disciplina que corresponde al ámbito de las humanidades, pues reúne las características principales propias de ellas; se ocupa del estudio del hombre y de sus obras, cuenta con un cuerpo de valores específicos y establece sus principios y reglas a partir de investigaciones en las que analiza no sólo causas y efectos, sino también la esencia misma de la cultura” 

José A. Gómez Hernández cita a Domingo Buonocore mencionar que es el "Conjunto de conocimientos teóricos y técnicos relativos a la organización y administración de una biblioteca. Comprende una parte doctrinaria que estudia la teoría de la selección y adquisición de libros, catalogación, clasificación y el régimen económico administrativo de la biblioteca: recursos, local y 30 mobiliario, personal, conservación de los libros y uso de la biblioteca, y una parte que se relaciona propiamente con el arte de administrarla, de gobernarla, para realizar con la mayor eficacia y el menor esfuerzo los fines específicos de la institución".

La biblioteconomía se enmarca dentro de las ciencias sociales, dentro de las ciencias relacionadas con la Interacción social, pues su objetivo es la modelización y satisfacción de las necesidades en información, servicios y espacio demandadas por la sociedad.

En concreto la bibliotecología es «la disciplina que estudia la naturaleza, las conductas y las necesidades de la información, las tecnologías de información y comunicación, los sistemas de información, la información en las sociedades contemporáneas como son: la sociedad del conocimiento y de la información; la industria de la información y el flujo de la información».

La Biblioteconomía y Estudios de la Información se ocupan de la técnica cuyo campo lo constituyen las colecciones de libros y las publicaciones periódicas (revistas y periódicos), la información y la gestión del conocimiento. Este último rol a lo largo del tiempo se ha ido ampliando al manejo de las tecnologías de la información. La biblioteconomía también se interesa por otros formatos o recursos de información, como por ejemplo, publicaciones periódicas en línea, los discos compactos (CD-ROM) y DVD, las microfichas, las filmaciones (vídeo-cassetes, microfilm), las cintas de audio, etc.

El formato electrónico, en especial el disco compacto y las bases de datos en línea, han revolucionado el mundo de las bibliotecas, como la especializada que aparecieron con unos rasgos característicos que las diferenciaban de las tradicionales. Sus usuarios eran personas interesadas en materias concretas, con una necesidad de información muy específica y sus colecciones son muy variadas por lo cual ha sido necesario adaptar los tradicionales catálogos de fichas o tarjetas a los nuevos formatos, surgiendo los catálogos electrónicos, también conocidos como catálogo en línea de acceso público (OPAC = "Online Public Access Catalogs"). Un ejemplo es OCLC (Online Computer Library Center).

El concepto de "colección" se ha transformado con el surgimiento de las bases de datos electrónicas, que no necesitan estar alojadas físicamente en la biblioteca para ser accesibles a los usuarios, y además por la volatilidad de la información que brindan. Esto ha producido un profundo cambio en las políticas de colección de documentos (ya sea en formato electrónico o impreso) de las bibliotecas modernas.

La biblioteconomía se puede dividir en "teórica" y "aplicada". En la primera incluimos temas como la teoría de la información y la gestión del conocimiento, el estudio de la necesidad de información y cómo satisfacerla al mundo, los factores externos que influyen en la interpretación de los conocimientos, etc. La biblioteconomía "aplicada" se ocupa de temas tales como el desarrollo y mantenimiento de las colecciones, servicios técnicos (adquisición, catalogación, préstamo y descarte o depuración) de las colecciones, cooperación interbibliotecaria, derechos de autor, libertad de información (derecho al acceso a la información), conservación, gerenciamiento de la biblioteca o unidad de información, etcétera. Junto con la bibliometría está la informetría o medida de la capacidad potencial de transferencia de información documental de un sistema documental o bibliotecario. Mide la cantidad de información posible con un "algoritmo de búsqueda" basado en las palabras clave o códigos descriptores que el lector usa en lenguaje natural y los equivalentes y sinónimos que el sistema o base de datos ha utilizado para catalogar los documentos (términos contenidos en las categorías en el caso de la Wikipedia). Si sumamos las interacciones u ocurrencias y establecemos un índice de calidad de la búsqueda por el logaritmo natural, que sería 1, 2 y 3 el óptimo para la recuperación de información conseguida. Es semejante al número de referencias que descarga un motor de búsqueda en Internet.

Puede entenderse el rol del bibliotecario referencista como un intermediario entre el usuario y la colección de una unidad de información. Con el objeto de poder cumplir bien su función, el bibliotecario realiza la denominada entrevista de referencia, para poder asistir al usuario en el planeamiento y ejecución de su búsqueda de información. El bibliotecario como técnico, estudia algoritmos de búsqueda, tesauros, índices de citación, recopilación de novedades bibliográficas, trato y atención al usuario y solución de sus consultas, contactos con colegas, etc. Actualmente y de acuerdo al desarrollo de las tecnologías de información y la Web 2.0, el bibliotecario es un facilitador de información.

Existen diversos sistemas de clasificación que se utilizan, con el objetivo primario de organizar "físicamente" las colecciones de las bibliotecas en los estantes, siendo los más empleados el de Dewey o la Clasificación Decimal Universal en Europa, el sistema de Clasificación de la Biblioteca del Congreso de los Estados Unidos, BLISS, clasificación facetada de Ranganathan, etc. Mediante el empleo de estas clasificaciones, es posible asignar al material en forma individual y al organizarse en los estantes (ejemplo, un libro) un código numérico o alfanumérico que refleja su contenido (temas o materias), y que sirve para ubicar juntos en los estantes los materiales que tienen contenidos relacionados.

Por otra parte, es tarea clásica del bibliotecario crear una ficha de cartulina con los datos principales del libro: título, autor, editor, editorial, año de publicación, edición, número de páginas, y tema, el que se extrae de alguno de los sistemas de clasificación en uso. El conjunto de estas fichas constituye el catálogo manual de una biblioteca. Existen varios métodos para organizar las fichas así creadas y aumentar las posibilidades de búsqueda en el catálogo manual. Las fichas representan a los materiales conservados en la biblioteca (ej. libros), y se las considera verdaderos subrogantes o representantes de estos materiales.

Sin embargo, hay que recordar que el bibliotecario no solo trabaja las fichas catalográficas. Exclusivamente para los libros, ya que, las publicaciones periódicas, como las revistas, periódicos, diarios, gacetas, y también los mapas. En general, otras fuentes de información en papel, además de aquellas creadas en otros formatos (como los discos de acetatos, cd-roms, dvds) son organizadas de la misma manera con los kardex, colocando los datos indispensables para que lleguen o sean rescuperados por el usuario.

Con el advenimiento de las computadoras, los catálogos manuales y la práctica de la asignación de un único o limitado número de categorías temáticas a cada material se han transformado en actividades obsoletas, puesto que actualmente es posible asignar un elevado número de descriptores a cada material, para mejorar la representación de su contenido; para así facilitar y ampliar las posibilidades de la búsqueda.

La organización de una biblioteca, esto es, la organización física por materias de un catálogo de libros, ha llevado al estudio del cómo estructurar el conocimiento humano: catalogación y clasificación. El estudio de la arquitectura de la información incluye una especialidad denominada «Bibliometría», que se ocupa por ejemplo de los índices de citas o "citation index", el factor de impacto o "impact factor", el número de veces que un artículo es citado en otros artículos, etc. El enfoque hacia el mundo empresarial se expresa en la Gestión del conocimiento, que busca calidad o relevancia y difusión selectiva del conocimiento, para hacer frente a la sobrecarga de información. El mundo de las bibliotecas y centros de documentación es muy sensible hacia el conocimiento de sus usuarios y autores, lo que supone estudiar las necesidades de los clientes mediante encuestas y entrevistas (métodos obstructivos) o mediante el análisis de los patrones de búsqueda de los clientes, como puede ser el análisis de los weblogs de los catálogos (métodos no obstructivos).

Para Jesse H. Shera, la evolución que se ha producido desde la documentación y biblioteconomía como ciencia unida a la independización de ambos, sigue los siguientes puntos:
1) Hasta finales del siglo XIX, la biblioteconomía y la documentación se consideraban la misma ciencia
2) Cuando la biblioteconomía se dejó llevar por el culto a la educación universal y el autoperfeccionismo, la documentación se aventuró sola en el mundo de las bibliográfias.
3) Los documentalistas perfeccionaron las técnicas de documentación y las ampliaron a; organización, utilización y reproducción del material.
4) Al convertirse los documentalistas en pioneros, un abismo cada vez mayor paso a separarlos de los bibliotecarios.

En España se puede estudiar Biblioteconomía y Documentación en más de una decena de universidades; con la adaptación a Grado con el nuevo plan de estudios Bolonia, el nombre de la titulación pasó a denominarse Información y Documentación. Los profesionales de la Biblioteconomía y Documentación, -bibliotecónomos y documentalistas- participan en asociaciones tanto a nivel nacional y regional como internacional.

Diplomatura en Biblioteconomía y Documentación

Titulación de primer ciclo, perteneciente al área de Ciencias Sociales y Jurídicas.

Estos estudios pretenden formar un profesional capacitado para ocupar puestos de trabajo, tanto técnicos como de gestión, en las instituciones documentales y en las organizaciones empresariales y administraciones públicas: profesionales especializados en la búsqueda, selección, clasificación y almacenamiento de todo tipo de información, sea cual sea el soporte en el que esté contenida.

Para lograr este fin, se estudian todas las normas y técnicas de catalogación, indización y análisis de contenidos que se utilizan para recopilar y organizar la información que se genera en bibliotecas, archivos y centros de documentación. Asimismo, también son materias de estudio en esta carrera la organización y administración de bibliotecas, hemerotecas, archivos, centros de documentación... Tan importante como saber dónde encontrar la información es poder recuperarla para que sea de fácil consulta. Las nuevas tecnologías de la información y comunicación (TIC) han influido de manera decisiva en esta profesión en los últimos años. Actualmente, es casi imposible llevar a cabo este tipo de trabajo sin conocer detalladamente el funcionamiento de bases de datos, o motores de búsqueda en Internet.

Descripción del título (Diplomatura en Biblioteconomía y Documentación)
Titulación de primer ciclo, perteneciente al área de Ciencias Sociales y Jurídicas.

Estos estudios pretenden formar un profesional: suficientemente cualificado para desempeñar con éxito sus labores profesionales dentro de la Sociedad de la Información y el Conocimiento en la que nos encontramos inmersos; capacitado para ocupar puestos de trabajo, tanto técnicos como de gestión, en las instituciones documentales y en las organizaciones empresarialesy administraciones públicas; profesionales especializados en la búsqueda, selección, clasificación y almacenamiento de todo tipo de información, sea cual sea el soporte en el que esté contenida.

Para lograr este fin, se estudian todas las normas y técnicas de catalogación, indización y análisis de contenidos que se utilizan pararecopilar y organizar la información que se genera en bibliotecas, archivos y centros de documentación. Asimismo, también son materias de estudio en esta carrera la organización y administraciónde bibliotecas, hemerotecas, archivos, centros de documentación...

Tan importante como saber dónde encontrar la información es poder recuperarla para que sea de fácil consulta. Las nuevas tecnologías de la información y comunicación (TIC)han influido de manera decisiva en esta profesión en los últimos años. Actualmente, es casi imposible llevar a cabo este tipo de trabajo sin conocer detalladamente el funcionamiento de bases de datos, o motores de búsqueda en Internet.

Salidas profesionales
El trabajo en la biblioteca (Bibliotecas generales, bibliotecas universitarias, bibliotecas de fundaciones y cualquier otro tipo de bibliotecas) continúa siendo una de los principales vías de salida laboral para estos titulados, en la actualidad las perspectivas profesionales de este colectivo se presentan mucho más amplias.

Medianas y grandes empresas e institucionesque necesitan y demandan expertos en biblioteconomía y documentación.
Organización de los archivos en empresas medianas y grandes, especialmente las que trabajan con información, tales como periódicos...

Buscar la documentación necesaria en cada momento para cubrir las necesidades de información de la empresa.
Contexto empresarial
las empresasnecesitan organizar y gestionar su información de forma eficaz y dinámica,
los centros tecnológicos basan gran parte de su éxito en la capacidad y profesionalidad de sus centros de documentación.
la creación de los contenidos en el desarrollo de portales en Internet.
los productores de bases de datos jurídicas, de información administrativa o bases de datos de prensa.

Másteres relacionados

Este grado da acceso a los másteres impartidos por la Facultad de Comunicación y Documentación.




</doc>
<doc id="22874" url="https://es.wikipedia.org/wiki?curid=22874" title="Batalla de Roncesvalles">
Batalla de Roncesvalles

La batalla de Roncesvalles tuvo lugar el 15 de agosto de 778 (según otros autores, en alguna fecha no identificada de 808), posiblemente en Valcarlos, en las proximidades del desfiladero de Roncesvalles del Pirineo Navarro, en la que la retaguardia del ejército de Carlomagno mandada por Roldán fue diezmada en una emboscada efectuada por vascones, como tesis más probable. La misma ocurrió en el contexto de los intentos de realizar en la zona una Marca Hispánica carolingia, que en el territorio pamplonés se logró únicamente durante 10 años, del 806 al 816.

La ubicación exacta del lugar de la batalla se desconoce, ya que los cronistas carolingios no mencionan con un topónimo el puerto por donde pasaron las tropas ni el desfiladero donde aconteció la emboscada. La historiografía del siglo IX y tradicionalmente la población la situó en "Luçayde" (actual Valcarlos, Luzaide en euskera, la última procedente de "Vallis-Karoli" en relación a la mención explícita del valle de Carlos). Es a raíz de la "Canción de Roldán", en el siglo XII (hacia 1150) cuando se localiza más al sur, en "Rozaballes" o "Renzeval" (Roncesvalles). En estudios recientes se dan distintas posibilidades. Así Rita Lejeune lo situó en el paso del Perthus, en los Pirineos orientales gerundenses actuales. El historiador Antonio Ubieto concluyó que fue en el puerto del Palo en el valle de Ansó del actual Pirineo oscense, cuando utilizaban la calzada romana de Zaragoza al Bearne. José María Jimeno Jurío en sus conclusiones, posteriores a los anteriores, se decanta por la hondonada de Valcarlos-Luzaide. Iñaki Sagredo por su parte amplía el campo de búsqueda y no descarta un ataque de desgaste y posterior engaño. La ruta del ejército la sitúa en dirección Belate-Baztán, con desplazamiento al Bidasoa. Sus conclusiones se refuerzan con la idea de que el ataque se produjo al atardecer tras arrastrarlos a un valle fuera de la ruta.

El interés de Carlomagno en los asuntos hispánicos le movió a apoyar una rebelión en el Vilayato de la Marca Superior de al-Ándalus, de Sulaymán al-Arabi, valí de Barcelona, que pretendía alzarse a emir de Córdoba con el apoyo de los francos, a cambio de entregar al emperador franco la plaza de Saraqusta (Zaragoza).

Entre mayo y junio de 778 Carlomagno, rey de los francos, se había adentrado en tierras hispanas, acudiendo a la llamada del gobernador de Zaragoza, Sulaymán al-Arabi, quien se había rebelado contra Abderramán I un año antes, para apoyarle en su sublevación a cambio de la plaza de Zaragoza. En su avance, Carlomagno llegó a Pamplona que capituló. La conjura fue un desastre, pues Sulaymán se negó a su llegada a entregar Zaragoza y Carlomagno puso asedio a la ciudad. Dado que llegó la noticia de la sublevación de los sajones, los francos levantaron el cerco e iniciaron la retirada llevando consigo como rehén al propio Sulaymán al-Arabi. Sulayman, que marchaba junto a sus tropas a unirse a las fuerzas rebeldes al emir y al ejército de Carlomagno, fue capturado por este frente a Saraqusta. Al llegar de nuevo a Pamplona, arrasa las murallas como se describe en los "Anales regios" y en los "Annales de Gestis Caroli Magni" del Poeta Sajón, y además destruye totalmente la ciudad para abandonarla y retornar al Pirineo por el mismo camino que en la venida.

Al paso por el desfiladero de Valcarlos (transformación etimológica de "Vallis-Karoli"), la retaguardia del ejército franco, unos 20 000 soldados acaudillados por Roldán, sobrino de Carlomagno, y por el resto de los Doce Pares de Francia, fue desbaratada el 15 de agosto de 778 por unas huestes formadas probablemente por contingentes de tribus vasconas. Sulaymán fue liberado en esta batalla.

Los primeros textos relatan que el ataque se efectuó sobre la cola de la retaguardia, como punto más débil, y que utilizaron la estrechez del camino ("angustiae viae"), lo angosto de los parajes ("angustus locus") y los tupidos bosques ("opacitas silvarum"). Se lanzaron dardos y piedras y cayeron rodando pesadas rocas por las laderas que sorprendieron al ejército creando pánico que les hizo precipitarse por el barranco sin tiempo para reaccionar (Anales Regios hasta 829). Murieron un gran número de caballeros francos entre los que destacaban Oliveros y Roldán.En los textos iniciales no se dice nada de la agonía y muerte de Roldán, no encontrándose su cadáver.

No se conoce con exactitud quiénes fueron los vencedores. Los historiadores manejan tres hipótesis. La primera dice que una coalición de vascones y musulmanes; la segunda, una combinación de vascones de ambas laderas del Pirineo y, la tercera, vascones ultrapirenaicos descontentos con el fortalecimiento del régimen franco en Aquitania. En una redacción casi coetánea de la época, en los "Anales regios", hacen protagonistas de la emboscada únicamente a los vascones. Es en la "Canción de Roldán" y otros del siglo XII donde se sustituye a los atacantes por sarracenos, ya que describe un enorme ejército de cuatrocientos mil sarracenos distribuido en escuadrones a las órdenes de los doce Pares musulmanes (equivalente a la organización franca). Ramón Menéndez Pidal concluye que en la celada tomaron parte vascones y musulmanes juntos, en el contexto entre alianzas y relaciones familiares entre los primeros caudillos pamploneses y la familia Banu Qasi del valle del Ebro que arrancan en el 734.

No existen relatos sobre estos hechos de los vascones del siglo VIII, cuyos caudillos establecieron en los años siguientes el reino de Pamplona, que evolucionó al reino de Navarra.

Los principales textos carolingios recogidos en los "Anales regios" (hasta 829), "Annales Mettenses priores", "Vita Karoli Magni imperatoris" de Eginhardo, "Annales de Gestis Caroli Magni" del Poeta Sajón y "Vita Hludowici imperatoris" del Astrónomo Lemosín recogen estos hechos en los años siguientes a la batalla.

Los "Anales Mettenses Priores" (hasta el 805) son anónimos y fueron escritos en Metz a los 25 años de la masacre, son los más cercanos en el tiempo y aunque «silencian el desastre son valiosísimos por cuanto anotan expresamente la ruta seguida por Carlomagno entre Aquitania y Pamplona», como refiere José María Jimeno Jurío.

Los "Anales Regios" también anónimos, fueron escritos a los 50 años de los hechos:
Eghinardo, que era el biógrafo de Carlomagno en el relato "Vita Karoli Magni", realizado 50 años después, describe:
El astrónomo Lemosín, biógrafo de Ludovico Pío:
El Poeta Sajón un siglo después de la batalla, cuenta que el rey iba por delante y que ya había pasado los puertos cuando se produjo el ataque:
Este suceso histórico también dio lugar a relatos y poemas épicos con versiones en el siglo X que llevarían a la leyenda recogida en la versión más antigua del "Cantar de Roldán" conocido como el manuscrito de Oxford del siglo XII, compuesta por 4002 versos agrupados en estrofas relatando una batalla abierta localizada en Roncesvalles, en vez de una emboscada como en realidad fue y que cuenta que Roldán hizo sonar su olifante de marfil en el vértice de Ibañeta para advertir al grueso del ejército, que descansaba en Valcarlos. En la versión legendaria de la "Chanson de Roland", Carlomagno creyó oír el olifante de Roldán pidiendo ayuda, pero Ganelón le convence de que no tiene importancia. Cuando caen heridos los doce paladines imperiales y Roldán, éste arrojó al agua su gloriosa espada, «Durandarte», a fin de que no cayera en manos del enemigo.el berra

Antes de esta versión existe una variante recogida por la "Nota Emilianense", fechada por su descubridor Dámaso Alonso entre 1065 y 1075, donde aparecen los "duodecim neptis" (los futuros doce pares de Francia) con los nombres de Rodlane, Bertlane, Oggero Spatacurta, Ghigelmo Alcorbitunas, Olibero y del obispo Turpín. En esta "Nota Emilianense" emplaza la muerte de Roldán en "Rozaballes".

En 1066, durante la batalla de Hastings, el juglar Incisor Ferri o Taillefer animó a los franceses cantando las hazañas de los héroes muertos en Roncesvalles:

En el libro IV del "Codex Calixtinus" (también denominado "Historia Turpini" y "Pseudo Turpín"), en tiempos de las cruzadas en Tierra Santa y la reconquista de al-Ándalus, se cuenta que Carlomagno en siete años conquistó toda la Hispania mora, excepto Zaragoza. La gesta de Roldán en Roncesvalles se relata en el capítulo XXI. La ubicación que realiza copia a la "Canción de Roldán" en la zona donde se fundará el hospital de Santa María en el 1132, pero como novedad utiliza el "Vallis Karoli" por donde camina la vanguardia francesa, que denota el uso ya popular de esta denominación. Hay un reconocimiento explícito de la utilización del camino de Luzaide/Valcarlos.



</doc>
<doc id="22875" url="https://es.wikipedia.org/wiki?curid=22875" title="Radioisótopo">
Radioisótopo

Se llama radioisótopo o radionúclido a aquel isótopo que es radiactivo. Son radiactivos ya que tienen un núcleo atómico inestable (isótopo padre) y emite energía y partículas cuando cambia de esta forma a un isótopo más estable (isótopo hijo). La palabra isótopo, del griego "en mismo sitio", se usa para indicar que todos los tipos de átomos de un mismo elemento químico se encuentran en el mismo sitio de la tabla periódica. Los átomos que son isótopos entre sí, son los que tienen igual número atómico (número de protones en el núcleo), pero diferente número másico (suma de número de neutrones y el de protones en el núcleo). Los distintos isótopos de un elemento, difieren pues en el número de neutrones.




</doc>
<doc id="22876" url="https://es.wikipedia.org/wiki?curid=22876" title="Josef Albers">
Josef Albers

Josef Albers (19 de marzo de 1888 - 25 de marzo de 1976) fue un artista y profesor alemán cuyo trabajo, tanto en Europa como en los Estados Unidos, creó la base de algunos de los programas de educación artística más influyentes del siglo XX. Su esposa fue la también artista Anni Albers.

Albers nació en Bottrop, Westfalia. Desde 1901 hasta 1905 estudió en la universidad de magisterio de Büren y luego enseñó como profesor de escuela primaria. En 1908 vio por primera vez las obras de Paul Cézanne y Henri Matisse en Museo Folkwang en Essen. Inspirado por Piet Mondrian en 1913 pintó su primer cuadro abstracto.

Albers estudió arte en la Real Escuela de Arte en Berlín de 1913 a 1915, y en la Escuela de Arte en Essen de 1916 a 1919, después estudió en la Academia de las Artes de Prusia con Franz von Stuck y de 1919 a 1920 en el Academia de Bellas Artes en Múnich hasta que en 1920 entró en la Escuela de la Bauhaus en Weimar (a partir de 1925 en Dessau, de 1932 a 1933 en Berlín). Comenzó a enseñar en el Departamento de Diseño en 1923 y fue ascendido a profesor en 1925, el mismo año en que la Bauhaus se mudó a Dessau. Su trabajo en este momento incluía el diseño de muebles y de trabajo con el vidrio. En esa época se casó con Anni Albers que era entonces una estudiante de la Bauhaus.

Con la clausura de la Bauhaus bajo la presión nazi en 1933, Albers emigró a los Estados Unidos y se incorporó a la facultad de Black Mountain College (Carolina del Norte), encargándose del programa de arte hasta 1949, siendo varios los latinoamericanos que se formaron en esta escuela, destacando la alumna cubano-mexicana Clara Porset.

Entre sus alumnos más importantes en esta institución, están John Cage, Robert Rauschenberg, Cy Twombly, Ray Johnson, Susan Weil, Donald Judd y Merce Cunningham. De 1934 a 1936 fue miembro del grupo de artistas de París: Abstracción-Creación.

En 1935, viajó, con su mujer, por primera vez a Cuba y México, y quedaron muy impresionados por la arquitectura y su influencia fue duradera. Asimismo, conocieron al arquitecto Luis Barragán y su obra.

De 1950 a 1958, Albers dirigió el Departamento de Diseño de la Universidad de Yale, en New Haven (Connecticut), donde, entre otras cosas, enseñó a Richard Anuszkiewicz, Eva Hesse y Richard Serra. En 1953 regresó como profesor invitado a la Escuela de Diseño de Ulm, Alemania. En 1962, como becario de la Universidad de Yale, recibió una subvención de la Graham Foundation para una exposición y conferencia sobre su trabajo.

Tras jubilarse en Yale en 1958, Albers continuó viviendo y trabajando en New Haven con su esposa, la artista textil Anni Albers, hasta el día de su fallecimiento.

Consumado diseñador, fotógrafo, tipógrafo y poeta, Albers es principalmente recordado como pintor abstracto. Favoreció un acercamiento muy disciplinado a la composición y publicó varios libros y artículos sobre la teoría de la forma y el color, así como realizó diversas obras pictóricas en este sentido. Las más conocidas quizás sean sus rigurosas series "Homenaje al cuadrado", que empezó en 1949, donde explora las interacciones cromáticas entre cuadrados de distintos colores organizados concéntricamente en el lienzo. Sus teorías sobre el arte y su enseñanza fueron de gran valor formativo para la siguiente generación de artistas.

El trabajo de Albers representa una transición entre el arte europeo tradicional y el arte norteamericano. Su trabajo incorpora las influencias europeas de los constructivistas y el movimiento Bauhaus, y su intensidad y la pequeñez de escala son típicamente europeos. Sin embargo, su influencia se redujo fuertemente en los artistas americanos de la década de 1950 y 1960.

Albers experimentó con una gran cantidad de efectos de colores, formas, líneas y áreas entre sí, con la subjetividad de la percepción visual: "Sólo las apariencias no engañan". Con sus dibujos sobre la base de las ilusiones ópticas, fue junto a Victor Vasarely fundador del Op-art. En este contexto, sus series más famosas son parte de"Homenaje al cuadrado", En “Homenaje al cuadrado intentó explorar la interacción del color en un formato regular dado. Y demostró por si hiciera falta, que el color es un fenómeno completamente relativo; los colores se cambian constantemente según su yuxtaposición y relación con otros colores. "Un mismo tono puede parecer diferente cuando se coloca sobre diferentes fondos, y diferentes colores pueden parecer casi iguales cuando se asocian a distintos fondos. Es así que un mismo color permite innumerables lecturas". Por lo tanto, Albers es también uno de los representantes de Hard edge.

Albers también colaboró con el arquitecto y profesor de Yale Rey-lui Wu en la creación de diseños decorativos para algunos de los proyectos de Wu. Entre estas se encuentran las chimeneas geométricas distintivo de las casas de Rouse (1954) y DuPont (1959), la fachada de la Sociedad del Manuscrito, uno de los sociedades secretas de Yale (1962), y un diseño para la Iglesia Bautista Bethel (1973). Además, en este tiempo trabajó en su constelación de piezas estructurales. En 1963 publicó "Interacción del Color" que presentaba su teoría de que los colores se rigen por una lógica interna y engañosa. También durante este tiempo, creó la cubierta abstracta del álbum del director de orquesta Enoch Light de la compañía discográfica Command Lp Records.

Sus obras más conocidas son:


Josef Albers participó en la Documenta celebrada en los años 1955 y 1968 en Kassel. En 1970 se convirtió en ciudadano honorario de su ciudad natal de Bottrop, a la que más tarde donó una gran parte de sus bienes con los que se estableció el Museo de Josef Albers Quadrat.

En 1971 (casi cinco años antes de su muerte), fundó la Fundación Josef Albers y Anni Albers, una organización sin ánimo de lucro que esperaba que fuera ""la revelación y la evocación de la visión a través del arte"". Hoy en día, esta organización no sólo sirve como la oficina de bienes tanto de Josef Albers y su esposa, Anni Albers, sino que también realiza exposiciones y publicaciones centradas en las obras de Albers. El edificio oficial de la Fundación está ubicado en Bethany, Connecticut e ""incluye una central de investigación y un centro de almacenamiento de archivos para dar cabida a las colecciones de arte de la Fundación, biblioteca y archivos, y oficinas, así como estudios de residencia para artistas visitantes.""

El representante de derechos de autor de EE.UU. para la Fundación Josef y Anni Albers es la Artists Rights Society. El director ejecutivo de la fundación es Nicholas Fox Weber, autor de catorce libros.

"Ver página de discusión."




</doc>
<doc id="22879" url="https://es.wikipedia.org/wiki?curid=22879" title="Laguardia">
Laguardia

Laguardia es un municipio situado en el sur de la provincia de Álava, España, a 64 km de la capital Vitoria, que pertenece a la comunidad autónoma del País Vasco. Está enclavado en la comarca de la Rioja Alavesa.

Se halla en un altozano y está rodeada por una muralla que mandó levantar el rey Sancho el Fuerte de Navarra. Todavía se conservan cinco puertas de acceso a la ciudad. Sus nombres son: Mercadal, Carnicerías, Páganos, San Juan y Santa Engracia. Sus calles y rincones conservan un gran sabor medieval. Su economía está basada en la industria del vino, con elaboración propia y numerosas bodegas.

Durante la Edad Media recibió nombres como "Leguarda", "Gardia", "Guardia", "Guoardia", "Lagarde", "Lagardia" y "Laguoardia", hasta que quedó fijado en el actual Laguardia, siendo el nombre completo "La Guardia de la Sonsierra Navarra".

Existe cierta controversia sobre el nombre euskérico de la localidad. A finales del siglo XIX se extendió la creencia de que antes de la concesión de la carta de villazgo en 1164, la población de Laguardia se había llamado Biasteri. Muchos vieron en "Biasteri" un topónimo de origen euskérico y se hicieron populares etimologías como "bi haitz herri" ('pueblo de las dos peñas'). Ello llevó a utilizar "Biasteri" hasta fechas recientes como nombre en euskera de la localidad. Sin embargo, en los últimos años del siglo XX filólogos e historiadores llegaron a la conclusión consensuada de que "Biasteri" había sido el nombre antiguo de la vecina localidad de Viñaspre, no de Laguardia, y que por tanto no cabía realizar dicha asociación.
Otros defienden que el nombre euskérico original era Legarda. Para ello se basan en que en escrituras medievales anteriores a 1164, año en que la villa recibió sus fueros, entre los límites de Logroño y Marañón figuran los lugares de 'Legarda' y 'Legarde' respectivamente. 
En la actualidad la Real Academia de la Lengua Vasca considera que el nombre correcto en euskera de la localidad es Guardia.


Laguardia posee tres barrios separados del casco urbano:

En el lugar denominado La Hoya existe un importantísimo yacimiento arqueológico protohistórico. Se trata de un asentamiento prerromano de etnia berona cuyo sustrato abarca un extenso período de más de mil años, aproximadamente desde el siglo XII a. C. hasta el siglo II a. C..

En el año 1164 la villa recibió sus fueros durante el reinado del rey navarro Sancho VI "El Sabio". La demarcación inicial comprendía desde las Conchas de Haro hasta el Soto de Íñigo Galíndez, en el actual término de Viana. Fue la cabecera de la comunidad de "Villa y Tierra" hasta que se fueron creando nuevas villas en su entorno, desgajando el territorio en favor de San Vicente, Labraza y Viana. Aun así, en época medieval fue la principal plaza de la Sonsierra de Navarra.

La economía de Laguardia gira en torno al mundo de la viticultura (cultivo de la vid, industria vitivinícola y enoturismo). Laguardia es la capital de una de las comarcas vitivinícolas más conocidas de España, la Rioja Alavesa. En Laguardia, así como en las localidades de su entorno, se produce vino de la Denominación de Origen Calificada Rioja.




La localidad de Laguardia siempre ha sido conocida por tener entre sus habitantes músicos de gran talento en todas sus vertientes. De esta forma, varias generaciones de gaiteros han dado lugar a que se celebre el Día del Gaitero, una de las fiestas más importantes, que se celebra en homenaje a gaiteros ilustres de esta villa y alrededores. También son parte importante de la historia musical del pueblo sus charangas y su banda municipal, que lleva en activo desde el año 1881.

La localidad cuenta con un equipo de fútbol de División Regional. Una formación anterior competía en la Liga Regional de La Rioja.

En 1967 comenzó en Laguardia la decimosexta etapa de la Vuelta ciclista a España, una contrarreloj con final en Vitoria. El ganador fue el francés Raymond Poulidor.

Desde el año 2012, se celebra una carrera popular organizada por la peña "Los Zaborricos".




</doc>
<doc id="22880" url="https://es.wikipedia.org/wiki?curid=22880" title="Álava">
Álava

Álava (; oficialmente Araba/Álava) es una provincia y territorio histórico de España, situada en la comunidad autónoma del País Vasco. Su capital es Vitoria, que también es sede de las instituciones del gobierno autonómico. 

Tiene una superficie de 3.037 km² (según el INE), siendo la más extensa de las tres provincias vascas. En contaba con una población de habitantes (INE), ocupando el puesto 41 entre las provincias españolas y el último lugar entre las de la comunidad autónoma.

Las cumbres principales de Álava son el Gorbea, con 1482 msnm (cumbre compartida con Vizcaya); el Aratz, con 1443 msnm; el Palomares, con 1436 msnm y la Sierra de Toloño, con 1271 msnm.

Los ríos principales que discurren de forma total o parcial por Álava son el Zadorra, el Bayas, el Nervión, el Ayuda, el Omecillo, el Ega, el Ebro y el Inglares.
A pesar de que ambas formas son eusquéricas, de acuerdo con la legislación autonómica del País Vasco, la denominación oficial de Álava, como territorio histórico, es bilingüe:


En junio de 2011, el acuerdo presupuestario alcanzado entre el PSOE y el PNV en el Congreso de los Diputados incluyó el cambio de denominación oficial de la provincia, mediante el cual, la denominación oficial sería la de "Araba/Álava".

El territorio está dividido en siete cuadrillas, conforme recuerda el dicho "Siete Cuadrillas hacen Álava una - Zazpi talde Araba Bat":

Álava fue habitada en la época prerromana por autrigones, caristios, várdulos y berones.
A la época romana se adscriben los importantes yacimientos encontrados tanto en la parte más septentrional (Aloria) como en la central (Iruña-Veleia, Arkaia, San Román) —por la que transita la calzada Ab Asturica Burdigalam (de Astorga a Burdeos)— y en la meridional (Campezo). La primera referencia escrita a este nombre aparece en el siglo IX, en la crónica de Sebastián, de tiempos del monarca astur-leonés Alfonso III.

En opinión del fallecido lingüista alavés Henrike Knörr, su nombre en euskera procede de "alaba", derivado de "lau", 'llanura', más el artículo, tal y como recoge la propia web de la Diputación Foral. Otras teorías defienden el origen romance de Álava vinculado con el topónimo "Alba". Algunas otras propuestas apuntan a que su origen tiene que ver con el patronímico "Guevara", subrayando que algunas fuentes árabes se refieren a la Llanada como Al-guebala, aunque los cronistas árabes suelen utilizar mayoritariamente la expresión Alaba wa-l-Qila “Álava y los castillos”, entendiendo probablemente a Álava no como la provincia actual, sino solo la llanada. En euskera también ha sido denominada como "Araba-herria" o "Alaba-herria" (forma utilizada por Axular).

Los ataques musulmanes contra Álava fueron muy numerosos, siendo quizá la región que más razzias sufrió en toda la península.

En el año 767, con Abderramán I, según las crónicas árabes, hubo una expedición contra Álava mandada por Bedr, partiendo, como la mayoría, de La Rioja, y entraron por Pancorbo y la llanada de Miranda.
En el año 791, con Hixem I, el ejército musulmán a las órdenes del general Ubayd Allah ben Uthmán.

En el año 792, recién subido al trono Alfonso II de Asturias, esta vez los musulmanes al mando del general Abd al-Málik ben Mugith, saquean toda la Llanada.

El año 794, los musulmanes atacan dirigidos por Abd al-Karim, pero después caen derrotados por Alfonso II en Lutos, en tierras asturianas.

En el 796, se da otro ataque, organizado por el fallecido Hixem I. 

En el 801, dirigidos por el príncipe Moawia, hermano del emir, atacando Álava y Castilla, pero el ejército musulmán cae derrotado en un desfiladero cerca de Miranda, en una emboscada preparada por los alaveses; es la batalla de Lapuebla de Arganzón.

En el 803 recibe Álava y Castilla otro ataque del ejército dirigido por uno de los hermanos Ben Mugith; se tienen pocas fuentes de este ataque y puede que fuera un fracaso, en el Wadi-Aroun, muy posible el río Orón.

Tras unos años de calma, tras morir Alhakén I, llega al poder Abderramán II, que ejecuta el ataque más violento registrado en Álava, según fuentes árabes, que indican que entran por una garganta llamada Guerniq, detrás de la cual había una llanura donde tenía el enemigo sus almacenes y provisiones. Cayeron las tropas árabes sobre aquellos llanos y los tomaron, apoderándose de todas las provisiones. Todos aquellos lugares los encontraron desiertos por la huida de sus habitantes. La incursión la realizó en el verano del 822. 'Abd al-Karim invade las tierras de Álava, saqueándolas. Tras recibir promesa de sumisión por parte de castellanos y alaveses, 'Abd al-Karim volvió a Córdoba llevando en garantía numerosos rehenes. Esta terrible expedición afectó casi únicamente a la región de Álava.

Otra expedición contra Álava es relatada por el historiador árabe Ibn Hayyan. Cuenta que en el año 825 una tropa cordobesa dirigida por Ubayd Allah, con la colaboración de los vascones de Pamplona, aliados entonces del emir, penetraron en el mes de agosto en la llanada alavesa llegando hasta el «monte de los Madchus o adoradores del fuego», donde tras duros combates destrozaron a las fuerzas cristianas. Esas montañas que les cerraron el paso bien pudieron ser las montañas que van desde el Gorbea hasta el macizo de Aitzgorri.

En el año 838, dirigido por el príncipe Saíd, y en el 839 por Musa ibn Musa.

La última razzia contra Álava durante el reinado de Alfonso II fue en el verano del 842, año del fallecimiento de este rey astur.
Durante el reinado de Ordoño I de Asturias, hacia el año 854, pudo haber otra razzia contra Álava, pero no se tienen muchos detalles de ella. 

En el año 863, en el que un ejército dirigido por el general Abd al-Málik ibn al-Abbás invadió, arrasó y pasó a cuchillo la zona alavesa.

En el año 867 el príncipe Al-Hakkam atacará de nuevo, ocupando el castillo de Guerniq (o Yarniq), el mismo de la campaña del 823. Se discute la ubicación real del castillo, si en la zona del actual puerto de Azáceta o en una zona entre Etxabarri-Ibiña y Miñano Mayor, llamada Guernika.
En el año 882 el ejército cordobés cae sobre Álava partiendo desde La Rioja. Intentaron entrar los musulmanes por Castro Cellorigo pero fueron rechazados por el conde Vela Jiménez, segundo conde de Álava; poco después lo intentaron por Pancorbo y fracasaron igualmente. 
En el 883, se repite el resultado del año anterior, con los mismos contendientes. Es el último año de las razzias cíclicas.

Álava se libra de las constantes razzias, salvo la de Almanzor del año 1000, al descender la frontera con las reconquistas y repoblaciones, en época del rey Alfonso III de Asturias.

En el siglo VIII el territorio alavés ya se encontraba en la órbita del reino astur desde el reinado de Fruela I de Asturias, pero sería en el siglo IX cuando la monarquía astur organizó Álava bajo la forma política de condado, la cual no desaparecería hasta fines del siglo XII cuando Sancho VI de Navarra desplegó iniciativas para introducir en el territorio alavés nuevas formas de organización jurídica y política otorgando fueros a poblaciones como Vitoria.

Entre los siglos IX y XI el territorio que hoy cubre Álava formó parte del Condado de Castilla, que pertenecía al reino leonés hasta el año (en 932 d.C., en la que Castilla se independizó para formar más tarde el Reino de Castilla (hacia 1065). Álava se unió brevemente a Castilla cuando, aprovechando los problemas sucesorios, un noble local llamado Eglyón o Elyón se subleva para sacar provecho de la situación. Rodrigo de Castilla, el primer conde de Castilla fue el encargado de sofocar la rebelión que pareció acabarse (867 u 868) sin ni siquiera sacar la espada. Este pudo ser el motivo por el cual a partir de ese momento el conde Rodrigo va a extender sus dominios también sobre Álava. Aunque su nombre sigue sin aparecer en los documentos firmados en los dominios del obispado de Valpuesta, sí aparece en un documento de donación de Obarenes (870) y en una carta del monasterio alavés de San Millán de Salcedo (18 de abril de 873), en el valle de Cuartango junto con el señor Sarracín Muñoz, que pudiera ser el lugarteniente de Rodrigo en tierras alavesas.

Fernán González, a finales del siglo X se convierte en conde de Castilla y Álava (c. 931 - 944) unificando los territorios y logrando la autonomía del Condado de Castilla existiendo también al oeste del actual territorio alavés el condado de Lantarón. 

Durante la mayor parte del siglo XII el condado de Álava estará vinculado al Reino de Navarra, pero el año 1199 el rey Sancho VII de Navarra pierde Vitoria y la mayor parte de Álava en favor del rey castellano Alfonso VIII de Castilla. En el contexto de la guerra entre Castilla y Navarra la Cofradía de Arriaga acabaría por "entregarse voluntariamente" a Castilla. 

La Rioja Alavesa, nucleada en torno a la villa de Laguardia, continuaría formando parte del Reino de Navarra hasta finales del siglo XV, momento en que en el marco de la Guerra Civil de Navarra Castilla conquistó el territorio de la Sonsierra de Navarra pasando a integrarse prácticamente en su totalidad en Álava. 

Los precedentes de la provincia de Álava se dieron a través de dos hermandades promovidas por Vitoria dentro de la Corona de Castilla. En 1296 la Hermandad de Haro, Vitoria la promueve junto a las actuales Cuadrilla de Añana, Cuadrilla de Campezo-Montaña Alavesa, Cuadrilla de Laguardia-Rioja Alavesa, Cuadrilla de Salvatierra, enclave de Treviño, Miranda de Ebro y las comarcas de Rioja Alta y Rioja Media. No así Zuya ni Ayala. La Hermandad de Álava se fundó el 4 de octubre de 1463 en Rivabellosa (Álava). En ella quedaron integradas las villas de Vitoria, Miranda de Ebro, Salvatierra, Pancorbo y Sajazarra; 26 hermandades locales y 2 juntas, la de San Millán y Arana.

La provincia es conocida por la Batalla de Vitoria, dentro de la Guerra de la Independencia Española, contra la invasión francesa del país comandada por Napoleón Bonaparte. Tras la restauración de la monarquía borbónica en España, se empiezan a dar los enfrentamientos entre liberales y carlistas, dando lugar a las Guerras Carlistas. Durante este siglo XIX hubo dos liberales alaveses que llegaron a presidir el Gobierno de España, Salustiano Olózaga y Miguel Ricardo de Álava.

Álava, como el resto de provincias que constituyen el País Vasco, es un territorio histórico que cuenta como instituciones forales propias a la Diputación Foral de Álava y a las Juntas Generales de Álava.

La Diputación es el órgano de gobierno y administración provincial que, por un lado, desempeña las funciones que en el resto de provincias españolas ejercen las diputaciones provinciales y que, por otro, es depositaria también de competencias específicas (carreteras, servicios sociales, miñones, etc.) derivadas de los derechos históricos reconocidos a Álava y a los demás territorios forales. Entre estas competencias forales, adquiere especial relevancia la administración del sistema fiscal propio que, en su actual forma, data de 1876 cuando el gobierno central abolió el sistema foral de las provincias vascas, pero poniendo en marcha casi al mismo tiempo el sistema de concierto económico por el que la Diputación recauda los tributos de los alaveses efectuando después una aportación al Estado y al País Vasco para contribuir a los gastos comunes del gobierno nacional y del de la comunidad autónoma. La Diputación, en tanto que órgano colegiado con poderes ejecutivos, está dirigida por el Diputado General, cargo actualmente desempeñado por Ramiro González Vicente (PNV).

Las Juntas Generales de Álava son la institución que tiene atribuido el poder normativo de primer orden en Álava a través de la aprobación de "normas forales". Están compuestas por 51 representantes llamados procuradores o junteros, que son elegidos en las elecciones forales celebradas de manera simultánea a las elecciones municipales en España. 
Funcionan como un verdadero parlamento provincial que elige al Diputado General, aprueba los presupuestos y controla la actividad del ejecutivo foral alavés. En la actualidad el presidente de las Juntas Generales de Álava es el procurador del PNV Pedro Elósegui González de Gamarra.

A día 31 de diciembre de 2008 el endeudamiento en el que había incurrido la Diputación Foral de Álava ascendía a 76 millones de euros, según las cifras publicadas por el Ministerio de Economía y Hacienda. Un año después (31 de diciembre de 2009), esta cifra se había incrementado hasta los 180 millones de euros, es decir, más de un 236% respecto al año anterior. Finalmente, según los últimos datos disponibles (31 de diciembre de 2010), el endeudamiento siguió ascendiendo hasta alcanzar los 338 millones de euros, es decir, más de un 187% respecto a finales del 2009, y un acumulado del 444% desde diciembre del 2008. En otras palabras, la deuda en dos años se ha multiplicado por 4,5 veces, alcanzando más de 1000 euros por cada alavés residente. 

Comparativamente, la Diputación Foral de Guipúzcoa, incurre a 31/12/2010 en un endeudamiento de 285 millones de euros, o 403 euros por habitante. Y la Diputación Foral de Vizcaya, incurre a 31/12/2010 en un endeudamiento de 949 millones de euros, o 823 euros por habitante.

La provincia de Álava excluyendo su capital, Vitoria, suma cerca de 80.000 habitantes, cerca de 325.000 incluyendo la capital. Se da una marcada macrocefalia ya que más del 70% de la población total de la provincia reside en la ciudad de Vitoria. La macrocefalia alavesa es el resultado de la escasez de poblaciones de una cierta entidad a excepción del valle de Ayala con ciudades como Llodio o Amurrio ambas con más de 10.000 habitantes. En la zona central de Álava, es donde se concentra la mayor parte de la población por la ubicación de Vitoria en esa parte de la provincia rodeada de poblaciones con cierta importancia con las que se alcanzan los 260.000 habitantes. Municipios como Zuya, Cigoitia, Legutiano y Arrazua-Ubarrundia limitando al norte con la capital y las localidades de Salvatierra, Alegría de Álava e Iruña de Oca en el eje del ferrocarril Madrid-Irún que las conecta con el centro de Vitoria son las más importantes de la zona central. Álava es la provincia en que existe un mayor porcentaje de habitantes concentrados en su capital (73,82 %, frente a una media estatal de 31,96 %).

Vitoria:
Su capital, Vitoria, cuenta con un gran número de monumentos y lugares de interés, entre los que destaca la Catedral de Santa María de Vitoria en pleno casco medieval. Es además la European Green Capital o (Capital Verde Europea) 2012.

Valle de Ayala:

Cuadrilla de Zuya:

Cuadrilla de Salvatierra:

Cuadrilla de Añana:

Cuadrilla de Campezo - Montaña Alavesa:

Cuadrilla de Laguardia - Rioja Alavesa:

Existen varias líneas de autobuses interurbanos que recorren la provincia y zonas limítrofes, las cuáles, en su mayoría, tienen como punto de salida, la capital, Vitoria. Fueron puestas en marcha en 2015 por la Diputación Foral de Álava a través del Departamento de Infraestructuras Viarias y Movilidad. 

Es posible el pago con un descuento usando la tarjeta BAT, que es la misma que se puede utilizar en los autobuses urbanos y el tranvía de Vitoria.

Su santo patrón es San Prudencio, festejado anualmente el 28 de abril, y su patrona la Virgen de Estíbaliz, festejándose el segundo domingo de septiembre.




</doc>
<doc id="22882" url="https://es.wikipedia.org/wiki?curid=22882" title="Alcaraz">
Alcaraz

Alcaraz es un municipio español situado al sureste de la península ibérica, en la provincia de Albacete, dentro de la comunidad autónoma de Castilla-La Mancha. Se halla al pie de la serranía homónima en el extremo septentrional de ésta, en la falda este del cerro llamado de San Cristóbal. 

Es la cabeza de partido de la comarca. Alcaraz tiene como coordenadas geográficas los 2° 39' 34" de longitud Oeste y 38° 40' 5" de latitud Norte. Incluye las pedanías de Canaleja, El Cepillo, Escondite, Escorial, El Horcajo, La Hoz, El Jardín, La Mesta, Salinas de Pinilla ("antiguo Real Salero de Pinilla") y Solanilla.


Alcaraz tiene la peculiaridad de encontrarse en cuatro cuencas hidrográficas (Guadalquivir, Guadiana, Júcar y Segura), siendo uno de los pocos lugares de la geografía española con esta singularidad, constituyendo así uno de los puntos del divortium aquarum peninsulares.





La Sierra de Alcaraz forma parte de la Cordilleras Béticas, que son un conjunto de sistemas montañosos que se extienden por el sur de la península ibérica, desde el golfo de Cádiz hasta Alicante y se subdivide en las cordilleras Prebética, Subbética y Penibética. Dentro de estas divisiones, la Sierra de Alcaraz forma parte de la Cordillera Prebética.

Su pico más alto es el Pico Almenara, de 1.796 metros, y el segundo pico más importante es el Pico de la Sarga, de 1.769 metros.

Ubicación de la antigua población originaria, se le concía en la Edad Media con el nombre de Alcaraz Viejo. También es el lugar donde se ubicaba la antigua ciudad íbera de Urcesa. Lugar de bataneo de los tejidos que configuraban la antigua industria textil medieval alcaraceña, adopta pues por esta razón su actual topónimo.

Actualmente es una Micro-reserva natural a 3 km del núcleo urbano, en la carretera de La Mesta. Bajo impresionantes riscos confluyen el Río de la Mesta y el Río del Escorial, dando lugar al Río Guadalmena en una impresionante cascada llamada “Salto del Caballo”. En las paredes de estos tajos hay cuevas con pinturas rupestres que datan del Neolítico y ha sido a lo largo de los tiempos la cantera de pedra caliza que ha nutrido las construcciones monumentales de la población. Los Batanes destacan por tener una flora y fauna de mucho valor ecológico, destacando, entre otras, la planta carnívora Pinguicula Batanis, conocida comúnmente como “GRASILLA".

Los restos arqueológicos aquí existentes son evidentes a simple vista. No han sido nunca excavados de forma legal por ninguna institución, lo que no ha impedido el continuo saqueo de los mismos. En el Museo Arqueológico de Albacete hay, afortunadamente, algunas piezas originarias de este paraje.

En cuevas o salientes al sudeste de la ciudad, en la confluencia de los ríos de La Mesta y Escorial, a unos 3 km del actual núcleo urbano, hay restos de pinturas neolíticas que evidencian la temprana ocupación humana de estos parajes. Muy cerca de este lugar, en lo alto de un cerro, hay unas evidentes ruinas de lo que pudo ser un gran núcleo de población, actualmente no excavado ni estudiado. Hay estudiosos que sitúan aquí la ciudad íbera de Urcesa.

De la época romana es el Puente del Canto así como la vía que sobre él transitaba y que viene desde la antigua Mentesa Oretana (actual Villanueva de la Fuente) en dirección al Alcaraz Viejo (actual paraje de Los Batanes) donde se encuentran restos evidentes de una antigua población bastante saqueados y muy poco estudiados.

En el Museo Arqueológico de Albacete se encuentran algunas piezas expuestas, originarias de este lugar.

Alcaraz viejo se encontraba en el centro geográfico de la provincia de Orospeda. Posiblemente se le conocía con el nombre de Castaom o Castam. De este Alcaraz Viejo hay referencias en numerosos documentos medievales, y podría haber sido un reducto mozárabe bastante tardío.

Se conservan en la iglesia arciprestal de la Santísima Trinidad y Santa María, unas estelas funerarias visigodas en piedra magníficamente conservadas. También existe un jarrón litúrgico ricamente labrado, que se conserva en el Museo Arqueológico Nacional en Madrid con el número de inventario 61745, presentado en la exposición "Los caminos de la Luz" en Albacete el 19 de diciembre de 2000.

En el siglo XII ya aparece citada Alcaraz con su ubicación actual, existiendo indicios de que se empezó a construir en época califal por los siglos X y XI. Parece ser que recibió el nombre por encontrarse en la sierra así llamada y no al contrario como pudiera pensarse, pues hay conocimiento de la existencia anterior del topónimo "Sierra de Alcaraz" الكرز الجبل (Sierra de los cerezos). En esta época comienzan a cobrar fama las alfombras y telares de Alcaraz.

El tesoro de Canaleja, hallado en esta pedanía, es de época califal y está compuesto por numerosas monedas de plata.

En 1212 tiene lugar la batalla de Las Navas de Tolosa, y tras esta victoria cristiana, el rey castellano Alfonso VIII se dirige a la fortaleza de Alcaraz, y tras un azaroso asedio, conquista la plaza y entra en ella el 23 de mayo de 1213 por la puerta de Granada junto al Obispo de Toledo Ximénez de Rada. La población estaba gobernada por Aben Hamet, que tras su rendición y los acuerdos de la misma, fue escoltado por las tropas castellanas hasta Jaén.

A partir de esta fecha, serán los castellanos quienes hostiguen a los musulmanes, y se van repoblando los territorios vecinos, desde la fortaleza de Alcaraz, formándose el poderoso y extenso Alfoz de Alcaraz. Al mismo tiempo, se procede a la organización del concejo alcaraceño y se le dota de un fuero basado en el Fuero de Cuenca.

En la década de los treinta del siglo XIII, las tropas de la Orden de Santiago empezaron a incorporar varias villas de la actual Sierra de Segura, en la frontera norte del emirato murciano y del Campo de Montiel, al sur de La Mancha. A esta presión militar le siguieron una serie de campañas bélicas; a la conquista de Chinchilla de Monte-Aragón en 1242 le siguió un fuerte avance por las estribaciones segureñas.

En el Tratado de Alcaraz (1243) entre los representantes de Ibn Hud al-Dawla, el emir de Murcia, y de Fernando III "el Santo", se determinaba la entrada de tropas castellanas en los principales castillos murcianos.

Alfonso X "el Sabio" concede en 1256 a Alcaraz un nuevo fuero, donde le daba al concejo el control de su territorio. Con este fuero refuerza la autoridad real en los concejos y se interpone entre éstos y los grandes señores feudales. De las prolongadas estancias del monarca y su corte en Alcaraz, según Juan Torres Fontes, da fe una de sus Cantigas, "El niño de Alcaraz".

En julio de 1379 la villa envía procuradores a las Cortes de Burgos a instancia de nuevo rey de Castilla Juan I, recién fallecido Enrique II de Trastámara, su padre. El mismo monarca, organizando la defensa del reino frente a las pretensiones del Duque de Lancaster, convoca las Cortes de Segovia de 1386, donde vuelven a hacer acto de presencia, junto al resto de representantes de las ciudades castellanas, los procuradores del territorio de Alcaraz.

Alcaraz recibe el título de ciudad en el año 1429 por concesión de Juan II. Su situación geográfica es estratégica y justifica los intentos de los nobles de apoderarse de la ciudad en los siglos XIV y XV.

Punto digno de resaltar es la profunda convicción de autonomía en las gentes de Alcaraz. Fue este el afán que impulsó al concejo a perderse en una lucha sin fin contra los intereses de la Orden de Santiago, y mantenerse firme ante las pretensiones de anexión del marqués de Villena y dirigir en 1444 un orgulloso alegato al príncipe don Enrique en petición de que la ciudad no fuese cedida. Sin embargo, no pudieron evitar que el rey entregara la fortaleza al marqués de Villena, hecho que se produjo en 1470. Cinco años después, en marzo de 1475, los habitantes de Alcaraz se levantan en armas contra el marqués, siendo la primera ciudad de Castilla en posicionarse en favor de los Reyes Católicos en su lucha por el trono contra Juana la Beltraneja apoyada por el marquesado de Villena.

Cada una de las partes mandó un ejército a estas tierras, venciendo los Reyes Católicos. Años después los Reyes autorizaban al derribo de las murallas y de la fortaleza para que no fuese jamás cedida a noble alguno. Se firmó en 1480 la capitulación de Diego López Pacheco y Portocarrero que hizo que Alcaraz recobrase sus aldeas: El Bonillo, Munera y Lezuza. No pudo con Villarrobledo pues ésta obtuvo el título de Villa en recompensa a su fidelidad.

Los Reyes Católicos le conceden el título de Muy Noble y Muy Leal, visitando la reina Isabel "la católica" la ciudad en el año 1495. 

"En el mes de marzo de 1475 los vecinos de Alcaraz tomaron las armas y cercaron la fortaleza, dominada por los hombres del marqués ... El rey envía un ejército de 500 caballeros y 300 peones al mando del maestre de Santiago, así como 500 lanzas. Por su parte el marqués reunió un ejército de 2000 lanzas y 4000 peones. Las tropas reales sitian el castillo, pero son sitiadas a su vez por las del marqués... llegan refuerzos reales y la fortaleza es tomada para la corona de los Reyes Católicos."

"El día 10 de mayo el alcázar se entregaba al magnífico señor don Rodrigo Manrique, Maestre de la cavallería de Santiago, y al dicho señor Adelantado Pedro Fajardo, y al Obispo de Ávila, que en el cerco de la dicha fortaleza avían estado y estaban al servicio de los dichos señores Reyes."

A principios del siglo XVI Alcaraz goza de numerosos privilegios otorgados por los Reyes Católicos. Destaca en esta época la construcción de la actual Plaza Mayor, una de las más bellas de España. Igualmente es de destacar el nacimiento en Alcaraz del genial arquitecto renacentista Andrés de Vandelvira. En esta época, en la actual provincia de Albacete solo había dos poblaciones con más de 1.000 vecinos: Alcaraz y Albacete. No obstante esta importancia, fue inevitable que Alcaraz fuera perdiendo progresivamente varias de sus poblaciones: Peñas de San Pedro (1537), El Bonillo (1538), Munera (1548), Lezuza (1553), Barrax (1564), Ayna (1565), Bogarra (1573) y El Ballestero (1694).

Durante la Guerra de las Comunidades la ciudad de Alcaraz se mantuvo leal al rey e hizo caso omiso a los mensajeros que Toledo envío reclamándoles apoyo. Eso sí, sus vecinos e inclusos sus regidores se opusieron a que el Corregidor Francisco de Mendoza impusiese sobrecargas tributarias para mandar tropas y pertrechos al ejército realista que combatía a los rebeldes en Castilla. Por otro lado, envío a la localidad de Peñas de San Pedro dos regidores acompañados de un pequeño contingente con el fin de evitar allí una inminente sublevación comunera, regresando aquellos a Alcaraz una semana después. El temor a un levantamiento dentro de la propia urbe llevó a que a inicios de septiembre de 1520 el procurador síndico pidiese al concejo de la ciudad que se tomasen medidas de seguridad, tales como reparar los muros y recoger en un lugar seguro del Ayuntamiento las armas que estaban en poder de partículares.

Por lo demás, consta que el Corregidor pudo finalmente recaudar la imposición extraordinaria citada, reforzando así las huestes realistas del prior de San Juan (que combatía a las fuerzas comuneras en el reino de Toledo desde comienzos de 1521) con 100 caballeros y 500 peones. 

A la vuelta del rey Carlos I en 1522, Alcaraz intentaría sacar partido de su lealtad en el conflicto haciendo una serie de peticiones acerca del reconocimiento de ciertos privilegios dados por los Reyes Católicos, como eran la celebración de dos ferias anuales y un mercado franco semanal, entre otros, así como una antigua costumbre de poder envíar procuradores a la Cortes de Castilla. Estos pedidos, a excepción del último, y junto con la exención de pagar la contribución extraordinaria de 1518, fueron cumplidos por Carlos I, pero las crecientes dificultades financieras de la Corona les quitaron en ciertos casos utilidad práctica.

En 1526, Carlos I entrega una rica dote de bodas a su esposa Isabel de Portugal. Entre estas rentas están las de la ciudad de Alcaraz y como complemento también las de la cercana villa de Albacete. Así reconocía el concejo a Isabel como señora de Alcaraz.

Durante el reinado de los Austrias, y más tarde con los Borbones, Alcaraz contó con una de las grandes circunscripciones político-administrativas del país, lo que fue el llamado Corregimiento de la ciudad de Alcaraz.

En el siglo XIX Alcaraz fue partícipe dentro del foco de resistencia durante la invasión francesa, siendo en 1812 sede, tras un período inicial en Elche de la Sierra y antes de su definitiva ubicación en Ciudad Real, de la edición del semanario "la Gaceta de Junta Superior de la Mancha", así como capital provisional de la antigua provincia de La Mancha en rebeldía contra el invasor francés y cuartel general de un batallón de seis compañías del Regimiento de Infantería "Murcia 42" perteneciente al Tercer Ejército (15 de agosto a 5 de diciembre de 1812). Por su parte el gobierno afrancesado de José Bonaparte, a imagen de los modos territoriales franceses, estableció la Prefectura de La Mancha dividida en dos Subprefecturas; la de Ciudad Real y la de Alcaraz.

A partir de 1833 y tras la creación de la nueva provincia de Albacete, Alcaraz deja de poseer esa elevada importancia que hasta entonces había tenido, conservando como único privilegio el mantenimiento de la cabecera de partido judicial, que todavía hoy ostenta.

 Como en la mayoría de pueblos del sur de la provincia de Albacete, la población creció significativamente en la primera mitad de siglo XX, hasta alcanzar su máximo en 1950, y luego disminuyó de forma muy acentuada, hasta la década de 1980. Desde entonces, con ligeros altibajos, la población se mantiene estable, por lo general sin alcanzar los dos mil habitantes.

Alcaraz es la sede del Juzgado de Primera Instancia e Instrucción Único de Alcaraz, que a su vez tiene las funciones de Registro Civil.

Es la cabeza del partido judicial nº 2 de la provincia de Albacete, que incluye las localidades de Alcaraz, El Ballestero, Bienservida, Bogarra, Casas de Lázaro, Cotillas, Masegoso, Paterna del Madera, Peñascosa, Povedilla, Riópar, Robledo, Salobre, Vianos, Villapalacios, Villaverde de Guadalimar y Viveros.

El tramo de la carretera nacional N-322 de Albacete a Bailén comunica por carretera a Alcaraz con la capital de su provincia. También estuvo unida a la capital provincial por el ferrocarril Albacete-Linares ahora desmantelado (la parte andaluza no se terminó).

Alcaraz es sede de un "Parque de Bomberos de Zona" del SEPEI Albacete.

Son monumentos de este conjunto: La Plaza Mayor, la Portada del Alhorí, la iglesia de la Trinidad, la Casa del Inquisidor, la Antigua Casa de Justicia, la Calle Mayor, la Casa de Galiano (o de Pedro Vandelvira), la iglesia de San Miguel, el convento de Santa María Magdalena, el convento de San Francisco, los alrededores de la iglesia arciprestal, y las ruinas del acueducto y del alcázar.

Declarada Conjunto Histórico-Artístico por Decreto de 28 de diciembre de 1945. Domina el conjunto histórico-artístico de Alcaraz, siendo una de las plazas más bellas de España. Es un rectángulo irregular, con arquerías en tres de sus lados (las lonjas). Tiene un estilo renacentista uniforme, de clarísima influencia italiana, que hace que el conjunto sea excepcional. Tiene los siguientes edificios:

Esta iglesia de estilo gótico y renacentista data de los siglos XIV y XV. Es la única parroquia que queda de las doce que hubo en su momento y en la actualidad es sede arciprestal. Su nombre compuesto se explica por la unión de la más nueva de la parroquias alcaraceñas de la Santísima Trinidad, con la primitiva de Santa María que estaba ubicada a intramuros del alcázar. En su interior alberga esculturas góticas y tablas del siglo XVI, así como numerosa imaginería borgoñona, napolitana y sobre todo, salcillesca, conformando el rico museo parroquial. La obra gótica fue finalizada antes de 1492 y en ella trabajó Pedro Cobo. El templo consta de tres naves y cuatro tramos sin crucero, separados por pilares fasciculados. La torre, que se encuentra a los pies de la iglesia, posee un cuerpo, el primero, también de estilo gótico. La portada principal es de un estilo gótico muy avanzado, con decoración flamígera en las arquivoltas.

La parte del siglo XIV fue derruida por el terremoto de Portugal con epicentro en Lisboa. Fue reconstruida posteriormente y tiene tres capillas renacentistas: las capillas de D. Pedro I el Grande y la capilla de los Ballesteros son obra de Vandelvira; la tercera, la Capilla de San Sebastián o Baptisterio, pertenece a la escuela de Andrés de Vandelvira, con una portada de formas manieristas; realizada tras la muerte del arquitecto alcaraceño y cuyo destino era el de sala de solemnidades del concejo, concretamente estrenó su cometido en la jura de lealtad a Carlos I y la proclamación de la emperatriz Isabel como señora de la ciudad.

La iglesia posee además un claustro renacentista que imita fielmente las arcadas de las lonjas de la Plaza Mayor.

La Torre del Tardón (del reloj) se eleva lindera con la Lonja de Santo Domingo. Se alza justo al lado de la Torre de la Trinidad y forma la imagen más conocida de Alcaraz y una de las más bellas de Castilla.

Al más puro estilo de la ‘logia’ italiana del ‘cinquecento’, constituye el resto del antiguo convento de este nombre. Esta lonja tiene cinco arcos, y destaca sobre el arco central una galería superior con un enorme escudo de Felipe II.

Con dos galerías de 12 arcos cada una.

Es el edificio del actual Ayuntamiento. Consta de dos galerías de 5 arcos cada una.

La Portada del Alhorí se levanta en la Calle Mayor, contigua al palacio del Ayuntamiento. Es de labra plateresca. Se trata de un arco de medio punto flanqueado por columnas labradas y rematados por elementos arquitectónicos igualmente labrados de forma primorosa. Posiblemente es el monumento más atractivo e importante de la ciudad. Es obra del genial arquitecto alcaraceño Andrés de Vandelvira.

Actualmente da a las dependencias municipales que acogen la sede de la Oficina de Turismo de Alcaraz.

Se sabe de la existencia del castillo de Alcaraz en el siglo XI, aunque podría haber sido construido un siglo antes, primero como torre de vigía que controlaba el Valle del Guadalmena y más tarde como ciudadela fortificada. Dicha ciudadela llega a gozar de la protección de tres murallas, que hoy corresponden a las calles Comedias-Mayor, Barrera y la del mismo alcázar

Tuvo siete puertas principales: Puerta de Granada (actual calle de Granada), Puerta de las Torres, (actual calle de las Torres, aproximadamente a la altura de la Cruz Verde), Puerta de Murcia (actual calle de la Puerta de Morcil) y la Puerta Nueva, única que se conserva actualmente y se encuentra al inicio de la Calle Mayor, Puerta de Santa Ana (en los Cantones), Puerta de Montiel (en el recinto del Alcázar), y el Postigo de los Frailes (calle Postigo). 

Ello da una idea de lo impresionante de la construcción y del tremendo esfuerzo que hubo de suponer el asedio y la toma por Alfonso VIII en 1213. Tras la Batalla de Alcaraz, el enclave se configuró como un punto estratégico fundamental en el control castellano del suroeste peninsular.

El acueducto de Alcaraz se alza en la entrada del pueblo, en el collado existente entre el cerro de San Cristóbal al pie del Castillo y el Cerro de Santa Bárbara (donde actualmente se eleva el monumento al Sagrado Corazón de Jesús). Es uno de los monumentos más característicos de Alcaraz, formando parte de su fisonomía propia, pues uno de sus grandes arcos se yergue majestuoso sobre el acceso a la población, siendo bien visible desde la carretera N-322.

En 1493 el Concejo de Alcaraz adjudica a Juan de Cózar y a Pedro de Cobo la construcción de unas fuentes, encargando a Gil Díaz las obras, que se realizaron sobre los siglos XVI y XVII pero tuvieron innumerables problemas económicos.

La obra comienza en el río de la Mesta, al principio es una acequia, luego hay un túnel de unos 100 metros de largo, llamado en su época "la contramina" bajo el saliente de La Molata, después continuaba hasta el collado posterior de los cerros de Santa Bábara y Matacaballos donde se levantaban las arcadas del acueducto de los que hoy sólo quedan unos pilares y un arco completo.

Lamentablemente se encuentra en estado ruinoso.

El castillo de Cortes se encuentra a las afueras del municipio de Alcaraz. Data del siglo XIII (1222); reformas en el siglo XVIII. Se encuentra muy reformado. Pertenece a la Iglesia católica y se destina a la celebración de actos religiosos. El acceso es libre.

Está protegido por la declaración genérica del Decreto de 22 de abril de 1949, y la Ley 16/1985 sobre el Patrimonio Histórico Español.

La torre de Gorgojí se encuentra junto al río Guadalmena, en el término municipal de Alcaraz. Data del siglo XIII.

Para llegar a ella se ha de tomar la carretera autonómica CM-412 desde la carretera nacional N-322 en dirección a la localidad de Villapalacios.

Se derribó cuando iba a comenzar su restauración.

Es de uso particular y se utiliza como vivienda, pero el acceso es libre.

Está protegida por de la declaración genérica del Decreto de 22 de abril de 1949, y la Ley 16/1985 sobre el Patrimonio Histórico Español.

La iglesia de San Miguel, al parecer fue fundada tras la conquista de Alcaraz en el siglo XIII; según el historiador alcaraceño Padre Pareja se fundó en 1227. Su fachada principal da a una recoleta plaza en mitad de la Calle Mayor, es de arco de medio punto, con pilastras a ambos lados, coronado por un entablamento en cuyos extremos se elevan unas pirámides y en el centro se eleva una hornacina, actualmente vacía.

En la fachada lateral a la calle Mayor hay tres ventanas saeteras de indudable sentido militar, no obstante es un resto de la antigua muralla. La iglesia es de planta rectangular, con tres naves, la central más ancha. En su interior destaca la Capilla del Rosario, gótica. Aquí fue bautizado Andrés de Vandelvira y se supone que en la capilla del Rosario están enterrados sus padres.

Es un paraje situado a unos 6 km del núcleo urbano, donde se encuentran los restos de un antiguo poblado íbero, no explorado ni excavado; también hay ruinas de una iglesia gótica y unas tumbas talladas en la roca. Es un paraje singular donde el visitante retrocede en el tiempo.

El Real Monasterio y Santuario de Cortes, sumergido en atisbos legendarios, se ubica en el entorno donde los lugareños sitúan una antigua fortificación popularmente conocida como "La Atalaya".

Las crónicas cuentan, además, que dicha Atalaya era en realidad el castillo de Carriz, donde el magno rey, don Alfonso VIII, se reunió con los reyes don Pedro II y don Sancho VII, en las primeras Cortes conjuntas de la primigenia España (de diez comunidades autónomas actuales), acordando importantes estrategias y compromisos para la batalla definitiva contra los almohades en las Navas de Tolosa (1212), y la posterior conquista de la fortaleza de Alcaraz (que quedaría a retaguardia). Así fue como dicho asedio duró desde primeros de marzo hasta la segunda mitad del mes de mayo de 1213, consiguiendo, con no poco esfuerzo, tomar para siempre dicha plaza.

Nueve años después, el 1 de mayo de 1222, fue cuando parece que se produjo la aparición de Nuestra Señora, al pastor Francisco Álvarez, natural de Solanilla, en el hueco de una encina. En recuerdo de aquel singular evento se erigió el santuario actual. Otros investigadores afirman que la imagen apareció en Alcaraz Viejo, actual paraje de Los Batanes, y fue trasladada al castillo de Cortes, donde el rey Alfonso X "el Sabio" estableció su corte durante un tiempo, preparando la definitiva conquista de los territorios murcianos, y construyó en él una bella capilla en la que se celebraban los oficios religiosos para su numeroso séquito palatino.

Existe, además, la posibilidad de que en el mismo castillo, se instalase una nueva comunidad monástica -de ahí que su apelativo de monasterio- de la Orden de la Santísima Trinidad y de Redención de Cautivos (fundación de origen provenzal, de inicios del siglo XIII, pionera para aquel tiempo), con gran éxito como intermediadora de los sucesivos conflictos entre las tropas cristianas y moriscas (la “Cruz Roja” del medievo). Orden ampliamente extendida por España y por toda La Mancha y Murcia, que difundió la actual imagen con iconografía característica “trinitaria” (capa triangular y cuernos a los pies), a semejanza de otras imágenes parecidas: Virgen de Los Llanos de Albacete, Virgen de Los Remedios de La Roda, Virgen de la Fuensanta de la ciudad de Murcia, etc.

Hasta hace unos años ha albergado la comunidad religiosa de Dominicas de la Unidad y su Instituto Ecuménico, ahora trasladado a la capital. El lugar es de máximo abolengo popular y tradicional como lugar de peregrinación desde los distintos puntos de La Mancha y de la Andalucía septentrional, que cada 8 de septiembre, concentra a una numerosa multitud en romería junto a este santuario.


La antigua línea ferroviaria Baeza-Utiel, que nunca llegó a utilizarse, ha sido reconvertida en vía verde. En la actualidad parte desde Alcaraz hasta la capital, Albacete, más o menos paralela a la carretera nacional N-322. Tiene una longitud de 106,150 km. Consta de 25 túneles (el más largo de 750 m) y 4 viaductos. Comunica Los Llanos de La Mancha con la sierra albacetense y es de especial interés para los romeros que acuden al Real Monasterio de Cortes.

Francisco Ríos González, alias "El Pernales" ( Estepa, provincia de Sevilla 23 de julio de 1879 - Villaverde de Guadalimar, provincia de Albacete 31 de agosto de 1907), fue el último bandolero andaluz y cobró fama a principios del S. XX. La reciente tradición y el folklore popular albacetense de los años 70 del pasado siglo XX, lo describen como un “bandolero bueno”, creándole incluso un famoso romance, aunque muy alejado de la realidad.
El caso es que esta figura está unida a la localidad de Alcaraz únicamente por las circunstancias de su muerte. Ante el acoso de la Guardia Civil, el Pernales, junto a su compañero El Niño del Arahal, intentan huir a Valencia y embarcarse con destino a América, para lo cual, decide salir de Andalucía por las fragosas sierras de Cazorla, Segura y Alcaraz. En agosto de 1907, en las inmediaciones de la localidad de Villaverde de Guadalimar el Pernales es reconocido y denunciado a la Guardia Civil. El 31 de agosto de 1907 en el paraje del Arroyo del Tejo, cerca de esta población, en lo más profundo de la Sierra de Alcaraz, ambos bandoleros fueron sorprendidos por el Teniente Haro y sus hombres mientras comían, y tras un tiroteo por ambas partes, cayeron los dos bandidos muertos a tiros. Los cuerpos sin vida de ambos son llevados a Bienservida y posteriormente a Alcaraz, donde se inicia el correspondiente proceso judicial. Ambos son enterrados en el Cementerio Municipal de Alcaraz, y todavía hoy se puede ver su tumba, en uno de los muros de dicho cementerio, en el antiguo Castillo. Hay una tradición en Alcaraz que dice que su tumba siempre se encuentra con flores.





</doc>
<doc id="22884" url="https://es.wikipedia.org/wiki?curid=22884" title="Almansa">
Almansa

Almansa es un municipio y una ciudad española situada al sureste de la península ibérica, en la provincia de Albacete, dentro de la comunidad autónoma de Castilla-La Mancha. El término municipal limita con la Comunidad Valenciana (provincias de Alicante y Valencia) y con la Región de Murcia. En 2017 contaba con 24 566 habitantes según las cifras oficiales del INE.

El topónimo «Almansa» deriva del término árabe المنصف («al-manṣaf») «la mitad del camino», nombre que hace posiblemente referencia al tramo de la Vía Augusta que partiendo de la zona de Ad Turres (probablemente Fuente la Higuera) se dirigía a Saltigi (Chinchilla de Monte-Aragón) camino de Gades (Cádiz).

Desde el punto de vista geográfico, el municipio se encuentra en el Levante interior, en la zona donde se unen los sistemas montañosos ibérico y bético, que han dado lugar a una llanura situada a unos 700 msnm, con unas montañas que la ciñen, pero que dejan unos pasillos de acceso hacia la Meseta y el Levante configurando el llamado Corredor de Almansa, considerado como uno de los 17 pasos naturales de la península ibérica.

La extensa sierra del municipio forma un conjunto con el Macizo del Caroche (o sierra de Enguera). Existe otra sierra al sur, que limita con los términos de Caudete y Yecla: la sierra de Oliva (o Santa Bárbara). Y el emblemático «El Mugrón» (1209 msnm), donde en una de sus cimas se encuentra el poblado ibérico Castellar de Meca (en la parte valenciana, en Ayora).

Limita con los municipios de Ayora, Enguera, Fuente la Higuera, Villena, Caudete, Yecla, Montealegre del Castillo, Bonete y Alpera.

Esta especial situación geográfica ha originado la dotación de una importante infraestructura de comunicaciones, que configura al municipio como un estratégico enclave de comunicaciones desde el centro peninsular a la zona levantina.

Esta obligada y favorable comunicación en ambos sentidos ha reunido en Almansa las influencias del Levante y de la Meseta, forjando la peculiar personalidad del municipio.

Los testimonios más relevantes del pensamiento de los grupos humanos prehistóricos corresponden a los dos abrigos con pinturas rupestres prehistóricas conocidos hasta el presente: «Barranco del Cabezo del Moro» y «Cueva de Olula».

El primero fue descubierto en 1984 por José Luis Simón y el segundo, que se halló de forma casual hacia 1990, por Pedro Más Guereca. Este último contó con una nueva aportación, una representación femenina, localizada por Alexandre Grimal Navarro, en 2000. Con la presencia de varios arqueros, algún animal y la representación de hasta tres mujeres, estas importantes muestras de las creencias de los grupos cazadores (10 000 años antes del presente) forman parte del llamado arte levantino.

Además de ser Bienes de Interés Cultural desde el 17 de febrero de 1997 (Identificadores de los bienes otorgados por el Ministerio de Cultura: RI-51-0009642 y RI-51-0009643), su valor inestimable como documento de la Prehistoria determinó a la Unesco su declaración como Patrimonio de la Humanidad, en 1998, formando parte del conjunto del arte rupestre del arco mediterráneo de la península ibérica.

En época romana hubo una villa llamada «Ad Aras», que algunos historiadores sitúan en Almansa o cerca de ella, pero no se ha podido confirmar.

Almansa dejó de pertenecer a la taifa de Murcia probablemente durante el reinado de Fernando III «el Santo», padre de Alfonso X «el Sabio», hacia el año 1244, en virtud del tratado de Alcaraz (año 1243) por el que la Corona de Castilla imponía su dominio sobre la taifa musulmana.

Fue precisamente el 28 de marzo de 1244 cuando se rubricó el tratado de Almizra entre el infante don Alfonso de Castilla y el rey Jaime I de Aragón, por el que se establecieron los límites entre los dos reinos cristianos, «quedando Almansa, Jorquera y su entorno del Júcar, el valle de Ayora con Cofrentes y Jarafuel, Caudete, Villena y Sax de parte castellana». Sin embargo, una vez ocupada definitivamente la taifa de Murcia, Almansa pasó a formar parte del Reino de Murcia según se observa en un documento del año 1257, en el que se dice: «... son de parte del Regno de Murçia, Xorquera, e Ayora, e Almansa, a Vees, e Chinchilla...».

En 1294 don Juan Manuel heredó de su padre (Manuel de Castilla) el señorío de Villena, que más tarde pasaría a ser marquesado, en el que estaba incluido Almansa. Don Juan Manuel concedería numerosos privilegios y mercedes a la entonces villa de Almansa, ratificados por distintos monarcas castellanos y aún continuados por los marqueses de Villena.

El marquesado, al igual que el señorío tras el tratado de Almizra, perteneció a la Corona de Castilla, hasta el año 1395 en que el marquesado pasó a formar parte de la Corona de Aragón, dentro del Reino de Valencia.

En 1444, el príncipe Enrique (futuro Enrique IV de Castilla), ante la confusa situación jurídica del señorío de Villena, autorizaba a Alfonso Téllez Girón a ocupar las villas que lo conformaban (Almansa, Chinchilla de Monte-Aragón, Villena, etcétera). Posteriormente otorgaría el dominio del territorio al hijo de éste, y futuro valido durante su reinado, Juan Pacheco.

El 20 de diciembre de 1452, Juan II de Castilla le confirmó en el señorío de la villa de Almansa, cuya merced había concedido el 3 de septiembre de 1445 a su padre Alfonso Téllez Girón «con derecho a heredamiento», aunque esta donación no fue a petición de su padre y se menciona que la merced fue hecha «por los muchos y buenos, leales y señalados servicios que vos D. Juan Pacheco, hijo mayor, legítimo heredero de Alfonso, mi vasallo que fue de mi consejo.»

Juan Pacheco fue el I marqués de Villena y su figura fue trascendental para la villa de Almansa, pues fue él quien le dio la morfología actual al castillo de Almansa.

Los Reyes Católicos anexionaron definitivamente este marquesado a la Corona de Castilla, dentro del Reino de Murcia en 1476. Aun así, el marquesado fue un auténtico «estado medieval» hasta finales del siglo XVI.

Es sabido que en su término municipal se libró una importante batalla el 25 de abril de 1707, la batalla de Almansa, durante el conflicto internacional de la Guerra de Sucesión Española. Carlos II falleció sin descendencia, y dos príncipes extranjeros pretendían la, por aquellos años, prestigiosa Monarquía Hispánica, para así afianzar su hegemonía tanto en Europa como en América.

El príncipe Felipe de Francia (duque de Anjou) y el archiduque Carlos de Austria (ambos extranjeros) querían acceder al trono, y se inició la guerra, contando ambos con la intervención de sus respectivos aliados europeos.

Como resultado de la batalla de Almansa, Felipe de Anjou vio abierto el camino hacia Valencia, aunque el triunfo no fue decisivo pues la guerra no concluyó hasta 1713, con el Tratado de Utrecht.

Desde 1778 y por merced de Carlos III esta antigua villa pasó a titularse ciudad.

En 1833 se creó la provincia de Albacete, incluyendo en ella a Almansa. Esta provincia formó parte de la región histórica de Murcia, hasta la creación de la comunidad autónoma de Castilla-La Mancha, en 1982.

A mitad del siglo dos acontecimientos cambiarían el desarrollo del municipio definitivamente, dejando atrás una sociedad agrícola y ganadera de subsistencia, y naciendo una nueva clase obrera, además de la creación de un incipiente comercio, transformando la ciudad no solo desde el punto de vista económico, sino también social y político.

Por un lado la construcción del ferrocarril. Entre 1857 y 1859 el municipio quedó comunicado por este medio con las ciudades de Madrid, Alicante y Valencia. La estación de Almansa fue una de las más importantes del sureste español, generando mucha mano de obra y surgiendo una clase obrera.

Por otro lado la puesta en marcha de la fábrica Calzados Coloma. Desde entonces, la fabricación de calzado de caballero de alta calidad para la exportación fue el motor económico del municipio. En 1930 esta empresa empleaba a más de 1200 trabajadores, de los cuales la mitad eran mujeres, creándose más clase obrera.

Tras su desaparición en 1954 (la empresa existió durante casi cien años) permitió que sus trabajadores crearan sus propias fábricas de calzado.

Alcaldes durante la II República (1931-1936) y la Guerra Civil (1936-1939)

La tradicional industria del municipio es la fabricación de calzado de caballero de alta calidad para la exportación, heredada de la fábrica Calzados Coloma (se creó a mitad del siglo XIX) siendo un referente tanto a nivel nacional como en toda Europa. Ya en 1913 Almansa se convirtió en el segundo productor de calzado de toda España tan solo superado por Barcelona. En 1930 "Calzados Coloma" empleaba a más de 1.200 trabajadores, de los cuales la mitad eran mujeres. Tras su desaparición en 1954 (la empresa existió durante casi 100 años) permitió que sus operarios fundaran sus propias factorías de calzado. 

Actualmente existen diferentes empresas del sector de reconocido prestigio tanto a nivel nacional como internacional, siendo el principal mercado los Estados Unidos.

Desde la segunda mitad de la década de 1980, y con la construcción del polígono industrial «El Mugrón», se diversificó en parte la actividad económica.

Desde inicios del siglo XXI la industria del calzado entró en una gran decadencia, elevando considerablemente el desempleo en el municipio, unido al cierre de muchas otras empresas de diferentes sectores.

El número de desempleados inscritos en el Servicio Público de Empleo Estatal (SEPE) ascendía a 2.418 personas a fecha 31 de marzo de 2018. Esta cifra supone un porcentaje sobre la población activa muy superior a la media nacional.

El sector terciario y comercial está bastante desarrollado, ofreciendo múltiples servicios al municipio y su área de influencia. La población activa de este sector supera al manufacturero.

El municipio y su comarca disponen de unos excelentes viñedos, amparados bajo la Denominación de Origen Almansa.

Desde 1857 el municipio está comunicado por ferrocarril con Madrid, desde 1858 con Alicante, a través de la línea Madrid-Alicante, que fue la primera línea de largo recorrido que se construyó en España. En 1859, se terminó el tramo Almansa-Játiva, lo que permitió viajar hasta Valencia. Años después la estación de Almansa fue una de las más importantes del sureste español.

Desde la estación de Almansa se puede llegar a Albacete, Madrid, Alicante, Valencia, Ciudad Real, etc.

Por carretera, y debido a la situación geográfica del municipio, las comunicaciones son inmejorables. Por autovías y autopistas se puede viajar hasta Madrid (320 km), Valencia (110 km), Alicante (95 km), Murcia (140 km), etc.

La estación de autobuses está justo al lado de la del ferrocarril, en un inmueble rehabilitado de mitad del siglo XIX. Es el antiguo muelle de carga de mercancías de la estación del tren.

También gracias a estas autovías quedan muy cerca los aeropuertos de Alicante-Elche, Valencia-Manises, Adolfo Suárez Madrid-Barajas y los puertos marítimos de Alicante y Valencia.

Almansa dispone de diversos servicios públicos tanto para el municipio como para su área de influencia.

La Administración General del Estado presta, entre otros, los siguientes servicios: la Tesorería General de la Seguridad Social, el Instituto Nacional de la Seguridad Social (INSS) o el Servicio Público de Empleo Estatal (SEPE).

Almansa es el partido judicial nº 3 de su provincia y tiene la sede de dos juzgados de primera instancia e instrucción. Los municipios que pertenecen a este partido judicial son: Alatoz, Almansa, Alpera, Bonete, Carcelén, Caudete, Fuente-Álamo, Higueruela y Montealegre del Castillo.

La Junta de Comunidades de Castilla-La Mancha es la encargada de, entre otras, las siguientes prestaciones:




Asimismo, el municipio es sede de un Parque de Bomberos Comarcal dependiente del SEPEI.

El municipio cuenta con 5 escuelas infantiles públicas, dispone de 8 colegios de educación primaria, de los cuales dos son concertados, y 3 Institutos de Educación Secundaria.

Cuenta además con otras instituciones educativas oficiales y públicas: una extensión de la Universidad Nacional de Educación a Distancia (UNED), conservatorio profesional de música,Escuela Oficial de Idiomas (inglés, francés y alemán)y un centro de educación de personas adultas (CEPA).

El municipio dispone también de un centro de educación especial, 2 bibliotecas públicas y una universidad popular.

Almansa cuenta con dos teatros municipales: el Teatro Regio (inaugurado en 1930) y el Teatro Principal (inaugurado a finales del siglo XIX).

La construcción original correspondió a los almohades. Estos edificaban utilizando la técnica del tapial, de los cuales se conservan algunos restos primitivos, sobre todo en las partes más cercanas a la roca y restaurados en 2008, siendo esta parte la más antigua (siglo XII).

En el siglo XIV la fortaleza pasó a manos del infante don Juan Manuel, quien aprovechando la construcción almohade, mandó reconstruir algunos de sus elementos y murallas, diferenciándose la mampostería cristiana del siglo XIV del tapial árabe del siglo XII. Así lo indican algunos documentos publicados por Aurelio Pretel. En uno de ellos, de 1338, sobre el aprovechamiento de aguas de Alpera, se establecen varias penas que, en todo o en parte se habían de destinar para el castillo de Almansa. En 1346, cuando don Juan Manuel hace merced al concejo de Almansa de tierras de riego de su posesión y del agua que le correspondía, pide a cambio:

Comparando ambos textos pudiera ser que el primero se refiera a fortificaciones existentes con anterioridad y que después en 1346 se iniciarán sobre aquellas las obras de un nuevo castillo.

Juan Pacheco, I marqués de Villena, le dio la morfología actual al castillo, con la construcción de la torre del homenaje, las torres semicirculares de las murallas y la barbacana defensiva. Sus armas aparecen en las claves de las bóvedas de crucería sencilla de su torre del homenaje.

A principios del siglo XX y gracias a los informes realizados por la Real Academia de la Historia y la Real Academia de Bellas Artes de San Fernando, el castillo de Almansa fue declarado por Real Orden del 2 de febrero de 1921 Monumento Histórico-Artístico Nacional. Identificador del bien otorgado por el Ministerio de Cultura: RI-51-0000190.

En la plaza de Santa María se alza la imponente iglesia arciprestal de la Asunción. Desde esta plaza se accede al castillo de Almansa por unas amplias escalinatas.

En el centro de la plaza se encuentra la popular «fuente de los patos» (en realidad son cisnes), que ya aparece en fotografías de principios del siglo XX.

El monumento es el resultado de varias etapas constructivas, desde el siglo XVI al XIX. En la puerta de la iglesia aparece inscrita la fecha de 1639.

En el interior del templo cabe destacar su espectacular bóveda de cañón, y las capillas laterales de bóvedas de crucería, de estilo gótico.

La cabecera de la nave está inspirada en el Palacio de Versalles.

Una única torre de ladrillo visto, barroca, de la segunda mitad del siglo XVIII, se alza en la fachada en el lado del Evangelio. La segunda torre no se construyó por falta de presupuesto.

Anexa a la nave principal, ya en la calle Virgen de Belén, se encuentra la capilla de la Comunión, fechada en 1763 y de estilo barroco. Tiene planta de cruz latina y cúpula en el crucero, con portada enteramente rococó finamente labrada.

A los pies de la iglesia arciprestal se celebran dos de los actos de las Fiestas Mayores de Almansa, la «conversión del moro» al Cristianismo y la «Serenata a la Patrona», la imagen de la Virgen de Belén.

Fue declarada Monumento Histórico-Artístico el 13 de abril de 1983. Identificador del bien otorgado por el Ministerio de Cultura: RI-51-0004848.

El palacio de los condes de Cirat es un edificio del siglo XVI, llamado también la «Casa Grande». 

Está situado en la plaza de Santa María, prácticamente anexo a la iglesia arciprestal de la Asunción.

El palacio fue construido para Alfonso de Pina, miembro influyente de la nobleza almanseña. Desde el siglo XV la familia de Ximén de Pina se encontraba en Almansa, provenientes de los caballeros aragoneses que acompañaron al rey Jaime I de Aragón en la conquista del Reino de Valencia.

En 1793 perteneció al conde de Cirat, Miguel de Catalá y Calatayud y, después, al marqués de Montortal. 

En 1992 fue adquirido por el ayuntamiento y, después de ser restaurado, es la sede del mismo desde 1996.

De especial interés es su fachada con un acusado carácter manierista en todos sus detalles. Ofrece dos cuerpos con columnas fajadas y almohadilladas en alternancia que parecen sacadas de tratados de arquitectura italianos, en especial de Sebastiano Serlio. 

Pérez Sánchez relaciona la decoración de esta fachada con la obra del jienense Francisco del Castillo.

Fue declarado Bien de Interés Cultural el 2 de noviembre de 1990. Identificador del bien otorgado por el Ministerio de Cultura: RI-51-0006984.

A 14 km de Almansa se encuentra el «Santuario de Nuestra Señora de Belén», el cual alberga la imagen de la patrona de Almansa en las épocas estivales. En él destaca su templo barroco, levantado en el siglo XVII, de planta rectangular, coro alto a los pies y magnífico retablo barroco de principios del siglo XVIII.

Fue declarado Bien de Interés Cultural el 11 de enero de 1991. Identificador del bien otorgado por el Ministerio de Cultura: RI-51-0007015.

A finales del siglo XX tomó gran importancia la difusión de los diferentes Caminos de Santiago que recorren la provincia de Albacete, entre ellos la «Ruta de la Lana». Este camino une la ciudad de Alicante con la de Burgos, donde se une con el Camino Francés, y recorre la provincia de Albacete desde Almansa hasta Villamalea, pasando también por los términos municipales de Bonete, Alpera, Alatoz, Alcalá del Júcar y Casas-Ibáñez.

Otro «Camino de Santiago» que atraviesa la provincia de Albacete es el denominado . Este camino une la ciudad de Valencia con la de Zamora, donde se une con la Ruta Jacobea de la Plata, y recorre la provincia de Albacete desde Almansa hasta Minaya, pasando también por los términos municipales de Higueruela, Hoya-Gonzalo, Chinchilla de Monte-Aragón, Albacete, La Gineta y La Roda.

Del 30 de abril al 6 de mayo se celebran las Fiestas Mayores en honor a la patrona, la imagen de la Virgen de Belén. Fiestas declaradas de Interés Turístico Nacional en 2008. Éstas consisten en la tradicional fiesta levantina de moros y cristianos. Destaca la espectacular embajada mora nocturna a los pies del emblemático castillo de Almansa, acto único en todas las fiestas de moros y cristianos de España.

La imagen de la Virgen de Belén es patrona de Almansa por bula del papa Urbano VIII emitida en 1644 y recibió la coronación canónica y pontificia (papa Pío XI) el 5 de mayo de 1925. Su fiesta principal es el día 6 de mayo.

El campo de fútbol, de césped natural, inaugurado en 1976, recibe el nombre de «Campo de Fútbol Polideportivo Municipal Paco Simón», en honor a un emblemático periodista deportivo almanseño ya fallecido, y tiene una capacidad para 3.500 espectadores. Las dimensiones del terreno de juego son 104 x 66. Además, está adaptado para competiciones internacionales de categoría sub-21. El 11 de octubre de 2002 se disputó un partido de clasificación para el XIV Campeonato de Europa sub-21 entre la selección española y la de Irlanda del Norte con victoria del combinado nacional por 1-0.

Un pabellón cubierto inaugurado en 1982, para las competiciones de baloncesto, voleibol, fútbol sala, etc., con capacidad para 1.500 espectadores. El Club Baloncesto Almansa juega en este pabellón, llamado popularmente «La Bombonera». El equipo milita, actualmente (temporada 2017-18), en la Liga EBA.

Una piscina cubierta y climatizada con las mejores dotaciones de toda su provincia.


Almansa está hermanada con varias poblaciones internacionales con características similares, ya sea por número de habitantes u otros motivos:


</doc>
<doc id="22885" url="https://es.wikipedia.org/wiki?curid=22885" title="Akira (manga)">
Akira (manga)

El largometraje homónimo se separa de la línea argumental del manga por causas claras: la película fue estrenada dos años antes de la conclusión del manga. Akira se ambienta en la ciudad futurista de Neo-Tokio, representada con profundo detalle en la película de animación (se invirtieron cerca de siete millones de dólares sólo en los decorados).

1988: El mundo está al borde de la destrucción absoluta. La tecnología avanzada fue la causa de una terrible explosión que desencadenó una guerra nuclear y devastó las grandes ciudades del planeta. Treinta años después, sobre las ruinas de Tokio, se alza la megalópolis de Neo-Tokio, una ciudad opresiva e inhumana cargada de problemas como el desempleo, la violencia, la droga y el terrorismo. Las sectas religiosas y los grupos extremistas, aprovechándose de la insatisfacción de los ciudadanos, cultivan el mito de Akira, un "niño cobaya" depositario de la "energía absoluta" cuya resurrección significaría para Japón el amanecer de una nueva era.

La historia se desarrolla en el año 2019 en Neo-Tokio, una ciudad reconstruida tras sufrir los devastadores efectos de una presunta explosión nuclear que desencadena la tercera guerra mundial. El gobierno ejerce un control represivo sobre la ciudad y experimenta sobre unos niños con poderes psíquicos latentes, aplicándoles fármacos para potenciarlos, estos contribuyen con predicciones para mantener la paz. Kaneda y Tetsuo son miembros de una pandilla de motociclistas llamada "The capsules" que tienen entre otras aficiones participar en peleas callejeras contra otras bandas, enfrentándose continuamente contra otra pandilla llamada "The clowns" montados sobre potentes motos. En el manga, Tetsuo sufre un accidente causado por un extraño niño con aspecto de anciano (en el filme, era durante una pelea callejera en moto, en el manga, es mientras conducían de regreso a la ciudad). A partir de ese accidente, Tetsuo no vuelve a ser el mismo. El gobierno lo secuestra, y en un análisis descubren que su potencial psíquico es uno de los más grandes que hayan detectado, comparable al de un sujeto extraordinario reclutado tiempo atrás. Empiezan a experimentar con él y éste comienza a desarrollar poderes psíquicos rápidamente, los cuales exacerban sus miedos y frustraciones, transformando patológicamente su personalidad. Por otro lado, se encuentran Kay y Ryu, miembros de la resistencia y dirigidos por su jefe Nezu, estos intentan averiguar qué ocurre en las instalaciones del ejército situadas en la zona cero, lugar donde explotó la bomba nuclear que destruyó la antigua ciudad. En este lugar es donde se encuentran Kiyoko, Takashi y Masaru, niños de extraña apariencia y poseedores de estos poderes psíquicos, sin olvidar a Akira que se descubre como el auténtico responsable de la explosión acontecida años atrás, al alcanzar el poder absoluto.

Producto de los experimentos del gobierno, Tetsuo empieza a sufrir alucinaciones y desarrolla poderes paranormales más allá de todo lo conocido. Esto lo lleva a creerse un dios y a enfrentarse al ejército mismo buscando cualquier evidencia de la existencia de Akira, ya que se cree su sucesor y superior a él. Luego la lucha por controlar el poder que tanto anheló se desata y lo lleva por el camino de la autodestrucción.

Por otro lado Kaneda, líder de su pandilla, se relaciona con Kay, de la cual termina enamorándose y se enfrenta con Tetsuo, del cual había sido sobreprotector. Este último desarrolla un sentimiento de inferioridad y odio hacia Kaneda, al que cuestiona como jefe, que se ve potenciado por sus nuevos poderes.

A todo esto surgen sectas y grupos que adoran a Akira y toman a Tetsuo como el nuevo salvador que sacara a Neo-Tokio del caos y la opresión.














Pandilla de motociclistas, enemigos de la banda de moteros de Kaneda. Liderados por Joker, y luego por Tetsuo. En el manga, tienen una acción más importante.

En España, en el año 2012 Norma editorial reunió en un cofre especial por los 30 años de la serie la obra completa en 6 tomos, incluido el "artbook" Akira Club y un set de postales. Aunque en el 2005 Norma ya había publicado el manga, y en los años 90 había sido publicado en tres formatos (formato comic, manga (blanco y negro) y en color) por la editorial Ediciones B.





</doc>
<doc id="22889" url="https://es.wikipedia.org/wiki?curid=22889" title="Lantánido">
Lantánido

Los lantanoides (nombre recomendado por la "IUPAC") o lantánidos son un grupo de elementos que forman parte del periodo 6 de la tabla periódica de los elementos. Estos elementos son llamados «tierras raras» debido a que se encuentran en forma de óxidos, y también, junto con los actínidos, forman los «elementos de transición interna».

El nombre procede del elemento químico lantano, que suele incluirse dentro de este grupo, dando un total de 15 elementos, desde el de número atómico 57 (el lantano) al 71 (el lutecio). Aunque se suela incluir en este grupo, el lantano no tiene electrones ocupando ningún orbital "f", mientras que los catorce siguientes elementos tienen este orbital 4"f" parcial o totalmente lleno (véase configuración electrónica).
Estos elementos son químicamente bastante parecidos entre sí puesto que los electrones situados en orbitales "f" son poco importantes en los enlaces que forman, en comparación con los de "p" y "d". También son bastante parecidos a los lantánidos los elementos itrio y escandio, debido a que tienen un radio similar y, al igual que los lantánidos, su estado de oxidación más importante es el +3. Éste es el estado de oxidación más importante de los lantánidos, pero también presentan el estado de oxidación +2 y +4.

La abundancia de estos elementos en la corteza terrestre es relativamente alta, en minerales como por ejemplo la monacita, en la cual se encuentran distintos lantánidos e itrio.

En la tabla periódica, estos elementos se suelen situar debajo del resto, junto con los actínidos, dando una tabla más compacta que si se colocaran entre los elementos del bloque "s" y los del bloque "d", aunque en algunas tablas periódicas sí que se pueden ver situados entre estos bloques, dando una tabla mucho más ancha.

El radio de los lantánidos va disminuyendo conforme aumenta el número atómico; no son variaciones grandes, pero se van acumulando. Esto provoca que los elementos del bloque "d" de la segunda y tercera serie de transición presenten radios similares dentro de un grupo: deberían aumentar al bajar en un grupo, pero al haberse intercalado los lantánidos, este aumento en el radio por bajar dentro de un grupo se ve contrarrestado por la disminución del radio por la presencia de los lantánidos. Esto se conoce como "contracción de los lantánidos".

Varios de los aspectos del comportamiento magnético y espectral de los lantánidos difieren fundamentalmente de los del bloque de correspondiente a los elementos de transición. La razón básica de estas diferencias reside en que los electrones que son responsables de las propiedades de los iones lantánidos son electrones 4f, y que los orbitales 4f están protegidos muy efectivamente de la influencia de fuerzas externas en las capas externas 5s y 5p. Es por ello que los estados que se originan desde las diversas configuraciones 4f sólo son ligeramente afectados por el medio que rodea a los iones y permanecen prácticamente invariables para determinado ion en todos sus compuestos.

Las constantes de acoplamiento de spin-órbita son bastante grandes. Esto tiene por consecuencia que, salvo unas cuantas excepciones, los iones lantánidos posean estados fundamentales con un solo y bien definido valor del momento angular total J, con el siguiente estado inferior de J, y con energías muchas veces mayores que el valor de KT, y por consiguiente el estado superior está virtualmente no poblado.

Los colores y estados electrónicos fundamentales de los iones M se dan en la tabla que se encuentra a continuación; la consecuencia de los colores en la serie del lantano al gadolinio se repiten accidentalmente en la serie del lutecio al gadolinio. Como ha quedado implícito en las explicaciones anteriores, los colores se deben a transiciones f-f , las cuales son virtualmente independientes del entorno exterior de iones.




</doc>
<doc id="22890" url="https://es.wikipedia.org/wiki?curid=22890" title="Yasir Arafat">
Yasir Arafat

Mohammed Yasir Abdel Rahman Abdel Raouf Arafat al-Qudwa al-Husseini () (El Cairo, Egipto, 24 de agosto de 1929-Clamart, Francia, 11 de noviembre de 2004), más conocido como Yasir Arafat () o por su kunya Abu Ammar (), fue un líder nacionalista palestino, presidente de la Organización para la Liberación de Palestina, presidente de la Autoridad Nacional Palestina y líder del partido político secular Fatah, que fundó en 1959. Arafat pasó gran parte de su vida luchando contra Israel en nombre de la autodeterminación de los palestinos. Aunque se había opuesto a la existencia de Israel, en 1988 cambió de posición y aceptó la Resolución 242 del Consejo de Seguridad de Naciones Unidas.

En 1994, recibió el junto con Shimon Peres e Isaac Rabin, por sus esfuerzos a favor de la paz en Oriente Próximo.

Arafat y su movimiento operaron desde varios países árabes. A finales de la década de 1960 y principios de la de 1970, Fatah se enfrentó a Jordania en una breve guerra civil. Al ser forzado a huir de Jordania hacia el Líbano, Arafat y Fatah fueron objetivos importantes en las invasiones que Israel llevó a cabo en 1978 y 1982. Mientras la mayoría de los palestinos, con independencia de su ideología política, le veían como un guerrillero y mártir que simbolizaba sus aspiraciones nacionales, muchos israelíes le describían como terrorista a causa de los ataques que su facción llevó a cabo contra civiles.

Arafat entabló negociaciones con el gobierno de Israel para terminar con el conflicto de décadas entre este país y la OLP. Estos incluyeron la Conferencia de Paz de Madrid, los Acuerdos de Oslo y la Cumbre de Camp David de 2000. Sus rivales políticos, incluyendo a los islamistas y varios izquierdistas de la OLP, le criticaban a menudo por ser corrupto o demasiado sumiso en sus concesiones al gobierno de Israel. En 1994, Arafat recibió el , junto a Isaac Rabin y Shimon Peres, por las negociaciones de Oslo. Durante este tiempo, Hamás y otras organizaciones militares tomaron el poder y sacudieron los cimientos de la autoridad de Fatah que Arafat había establecido en los territorios Palestinos.

A finales de 2004, tras llevar confinado más de dos años en su complejo de Ramala por el ejército israelí, Arafat cayó enfermo y entró en coma. Los médicos hablaron de púrpura trombocitopénica idiopática y cirrosis, pero no se hizo ninguna autopsia. Murió el 11 de noviembre de 2004 a los 75 años.

Yasir Arafat nació en El Cairo, hijo de padres palestinos. Su padre, Abdel Raouf al-Qudwa al-Huseini, era de Gaza; y su abuela paterna era egipcia. El padre de Arafat era comerciante textil en El-Sakakini, un distrito de El Cairo con gran mezcla de religiones, y un hermano de su padre, Amin al-Husayni, fue el Gran Muftí de Jerusalén, elegido en 1922 presidente del Consejo Supremo Musulmán, y que encabezó las revueltas árabes de 1929 y de 1936 que dieron lugar a sendas masacres contra los judíos de Palestina, la primera de ellas conocida como matanza de Hebrón.

Arafat fue el segundo más joven de siete hijos y, junto con su hermano pequeño Fathi, fue el único que nació en El Cairo. Su madre, Zahwa Abul Saud, procedía de una familia de Jerusalén. Murió de una dolencia renal en 1933 cuando Arafat tenía cuatro años.

La primera visita de Arafat a Jerusalén se produjo cuando su padre, incapaz de criar a siete hijos él solo, le envió a él y a su hermano Fathi junto con la familia de su madre al Barrio Marroquí de la Ciudad Vieja. Vivieron allí durante cuatro años con su tío Salim Abul Saud. En 1937, su padre los reclamó para que quedaran al cuidado de su hermana mayor, Inam. La relación de Arafat con su padre se estaba deteriorando; cuando éste murió en 1952, Arafat no asistió a su funeral. Tampoco visitó la tumba de su padre tras su regreso a Gaza.

En 1944, Arafat se matriculó en la Universidad del Rey Fuad I y se graduó en 1950. Más tarde declaró haber hallado una mejor comprensión del judaísmo y el sionismo al entablar discusiones con judíos y leer publicaciones de Theodor Herzl y otros sionistas prominentes. Al mismo tiempo, se hizo nacionalista árabe y comenzó a pasar armas de contrabando hacia el antiguo Mandato británico de Palestina, destinadas a los irregulares del Alto Comité Árabe y las milicias del Ejército de la Guerra Santa. Durante la guerra árabe-israelí de 1948, Arafat dejó la universidad y junto con otros árabes se propuso entrar en Palestina para unirse a las fuerzas árabes que luchaban contra las tropas israelíes. Sin embargo, en lugar de unirse a las filas de los fedayín palestinos, Arafat luchó junto a los Hermanos Musulmanes, aunque no se unió a la organización. Tomó parte en los combates de la zona de Gaza, que era el campo de batalla principal de las fuerzas egipcias durante el conflicto. A principios de 1949 la guerra se estaba decantando a favor de Israel y Arafat volvió a El Cairo por falta de apoyo logístico.

Tras volver a la universidad, Arafat estudió ingeniería civil y ejerció de presidente de la Unión General de Estudiantes Palestinos (UGEP) desde 1952 hasta 1956. Durante su primer año como presidente de la unión, la universidad cambió de nombre para llamarse Universidad de El Cairo, después de que el Movimiento de Oficiales Libres diera un golpe de estado, destronando al rey Faruq I. Por aquel entonces, Arafat ya había obtenido el título de grado en ingeniería civil y fue llamado a filas para luchar con las fuerzas egipcias durante la guerra del Sinaí; sin embargo, nunca llegó a luchar en ese campo de batalla. Más tarde ese mismo año, en una conferencia en Praga, vistió una kufiyya de color blanco liso, diferente de la estampada a cuadros que adoptó más tarde en Kuwait, que se convertiría en su emblema.

El nombre original de Arafat era Mohammed Abdel Rahman Abdel Raouf Arafat al-Qudwa al-Husseini. Su primer nombre era Mohammed Abdel Rahman; Abdel Raouf era el nombre de su padre y Arafat el de su abuelo. El nombre de su tribu era Al-Qudwa, que pertenecía al clan al-Husseini. Debe señalarse que el clan de Arafat, al-Husseini, estaba situado en Gaza y no debe confundirse con el conocido clan al-Husayni de Jerusalén.

Aunque Arafat creció en El Cairo, era común la tradición de quitar Mohammed o Ahmad del primer nombre; egipcios famosos como Anwar el-Sadat y Hosni Mubarak también lo hicieron. Sin embargo, Arafat también quitó las partes de Abdel Rahman y Abdel Raouf. A principios de los años 1950, Arafat adoptó el nombre de Yasir y, durante sus primeros años en la guerrilla, adoptó el "nom de guerre" de Abu Ammar. Ambos nombres están relacionados con Ammar ibn Yasir, uno de los primeros compañeros de Mahoma. Aunque se despojó de la mayoría de sus nombres heredados, conservó Arafat debido a su trascendencia en el islam.

Tras la crisis de Suez de 1956, el presidente egipcio Gamal Abdel Nasser, un líder del Movimiento de Oficiales Libres, acordó permitir a la Fuerza de Emergencia de las Naciones Unidas que se estableciera en la península de Sinaí y en la Franja de Gaza, causando la expulsión de todas las fuerzas de la guerrilla o "fedayín" que había allí, incluyendo a Arafat. En principio Arafat se esforzó por obtener una visa para Canadá y más tarde para Arabia Saudita, pero no lo consiguió en ninguno de los casos. En 1957 solicitó una visa para Kuwait (en aquel momento un protectorado británico), que le fue concedida sobre la base de su trabajo en ingeniería civil. Allí se encontró con dos amigos palestinos: Salah Khalaf ("Abu Iyad") y Khalil al-Wazir ("Abu Jihad"), ambos miembros oficiales de los Hermanos Musulmanes Egipcios. Arafat había conocido a Abu Iyad durante su periodo en la Universidad de El Cairo y a Abu Jihad en Gaza. Ambos se convertirían en la mano derecha de Arafat durante su vida política posterior. Abu Iyad viajó con Arafat a Kuwait a finales de 1960; Abu Jihad, trabajando también como profesor, había estado viviendo allí desde 1959. Tras establecerse en Kuwait, Abu Iyad ayudó a Arafat a conseguir un trabajo temporal como profesor de colegio.

Mientras Arafat entablaba amistad con otros refugiados palestinos (algunos de los cuales conoció durante sus días en El Cairo), él y otros fundaron de manera gradual un grupo que terminó conociéndose como Fatah. Se desconoce la fecha exacta del nacimiento de Fatah. Sin embargo, en 1959, la existencia del grupo quedó atestiguada en las páginas de una revista nacionalista palestina, "Filastununa Nida al-Hayat" (Nuestra Palestina, La Llamada de la Vida), que escribía y editaba Abu Jihad. FaTaH es un acrónimo inverso del nombre árabe "Harakat al-Tahrir al-Watani al-Filastini", cuya traducción es "El Movimiento de Liberación Nacional Palestino". Fatah también es una palabra que se usaba al principio de los tiempos islámicos para referirse a la «conquista».

Fatah se dedicó a la tarea de liberar Palestina mediante la lucha armada por parte de los propios palestinos. Esto difería de otras organizaciones políticas y guerrilleras de Palestina, la mayoría de las cuales creía firmemente en una respuesta árabe unida. La organización de Arafat nunca abrazó las ideologías de los gobiernos nacionales árabes más importantes de la época, en contraste con otras facciones palestinas, que a menudo se convertían en satélites de naciones como Egipto, Irak, Arabia Saudita, Siria y otras.

De acuerdo con su ideología, Arafat en general rechazaba donaciones a su organización de parte de gobiernos árabes importantes, con el fin de actuar de forma independiente a ellos. Sin embargo, no quería alienarlos, y buscó su apoyo unánime evitando alianzas con grupos leales a otras ideologías. Si embargo, trabajó duro en Kuwait para establecer las bases del apoyo económico futuro a Fatah consiguiendo contribuciones de los muchos palestinos adinerados que trabajaban allí y en otros países del Golfo, como Catar (donde conoció a Mahmud Abbas en 1961). Estos hombres de negocios y trabajadores del petróleo contribuyeron generosamente a la organización de Fatah. Arafat continuó este proceso en otros países árabes como Libia y Siria.

En 1962, Arafat y sus compañeros más cercanos emigraron a Siria —un país que hace frontera con Israel—, que recientemente se había separado de la efímera unión con el Egipto de Nasser. En esa época, Fatah tenía aproximadamente trescientos miembros, pero ninguno de ellos era combatiente. Sin embargo, en Siria consiguió reclutar miembros con mayores ingresos para embarcarse en su lucha armada contra Israel. Las fuerzas de Fatah aumentaron más después de que Arafat decidiera ofrecer salarios mucho mayores a los miembros del Ejército por la Liberación de Palestina, la fuerza militar regular de la Organización para la Liberación de Palestina, que fue creada por la Liga Árabe en el verano de 1964. El 31 de diciembre de ese mismo año, un pelotón de Al-'Asifah, el brazo armado de Fatah en aquella época, intentó infiltrarse en Israel, pero fueron interceptados y detenidos por fuerzas de seguridad libanesas. A este incidente siguieron varias incursiones más por parte de los guerrilleros de Fatah, mal entrenados y equipados. Algunos tuvieron éxito, mientras que otros fracasaron. A menudo Arafat lideró personalmente estas incursiones.

Arafat y su ayudante principal, Abu Jihad, fueron detenidos en Siria cuando se produjo el asesinato de un líder palestino pro-sirio, Yusuf Orabi. Horas antes de su asesinato, Arafat estaba discutiendo con él acerca de maneras de unificar sus facciones y para solicitar el apoyo de Orabi contra los rivales de Arafat dentro del liderazgo de Fatah. Poco después de que Arafat abandonara la reunión, a Orabi le arrojaron por la ventana de un edificio de tres pisos y la policía siria leal a Hafez al-Asad (Assad y Orabi eran "amigos íntimos") sospechaba que Arafat estaba implicado en el incidente. Assad organizó un jurado, que encontró a Arafat y a Abu Jihad culpables de asesinato. Sin embargo, ambos fueron perdonados por el presidente sirio Salah Jadid. No obstante, el incidente deterioró las relaciones entre Assad y Arafat, algo que saldría a flote más tarde cuando Assad alcanzó la presidencia de Siria.

El 13 de noviembre de 1966, Israel lanzó un gran ataque contra el pueblo cisjordano de as-Samu, administrado por Jordania, en respuesta a un ataque con bomba de carretera, implementado por Fatah, que había matado a tres miembros de las fuerzas de seguridad de Israel, cerca del sector meridional de la Línea Verde. En la escaramuza que se produjo como consecuencia del ataque israelí murieron decenas de miembros de las fuerzas de seguridad jordanas y 125 casas quedaron arrasadas. Esta incursión fue uno de los factores que condujeron a la Guerra de los Seis Días de 1967.

La Guerra de los Seis Días comenzó cuando Israel lanzó un ataque aéreo preventivo contra la fuerza aérea de Egipto el 5 de junio de 1967. La guerra terminó con una derrota árabe y la ocupación de varios territorios árabes por parte de Israel, incluyendo Cisjordania y la Franja de Gaza. Aunque Nasser y sus aliados árabes resultaron derrotados, Arafat y Fatah se pudieron apuntar una victoria, ya que la mayoría de los palestinos, que hasta entonces tendían a alinearse y simpatizar con gobiernos árabes particulares, a partir de ese momento empezaron a aceptar que era indispensable una solución «palestina» a su dilema. Muchos partidos políticos principalmente palestinos, incluyendo el Movimiento Nacionalista Árabe de George Habash, el Alto Comité Árabe de Amin al-Husayni, el Frente de Liberación Islámica y varios grupos respaldados por Siria, que en la práctica estaban desmoronados con la derrota de los gobiernos que los financiaban. Apenas una semana después de la derrota, Arafat cruzó el Río Jordán disfrazado y entró en Cisjordania, estableció centros de reclutamiento en Hebrón, la zona de Jerusalén y Nablus, y empezó a atraer a luchadores y patrocinadores para su causa.

Al mismo tiempo, Nasser contactó con Arafat a través de Mohammed Heikal (uno de los consejeros de Nasser), tras lo cual Nasser declaró que Arafat era el «líder de los Palestinos». En diciembre, Ahmed Shukeiri dimitió de su puesto como presidente de la OLP. Yahya Hammuda ocupó su lugar e invitó a Arafat a que se uniera a la organización. A Fatah se le concedieron 33 de los 105 asientos del Comité Ejecutivo de la OLP, mientras que se dejaron 57 asientos para otras facciones de la guerrilla.

A lo largo de 1968, Fatah y otros grupos armados palestinos fueron el objetivo de una importante operación militar israelí en la aldea jordana de Karamé, donde se situaba la sede de Fatah, además de un campo de refugiados palestinos de tamaño medio. El nombre del pueblo es la palabra árabe para «dignidad», lo cual elevaba su poder simbólico a los ojos de los árabes, sobre todo después de la derrota árabe de 1967. La operación se encontró con fuertes ataques en el interior de Cisjordania, incluyendo lanzamientos de cohetes por parte de Fatah y otras milicias palestinas. Según Said Aburish, el gobierno de Jordania y cierto número de comandos de Fatah informaron a Arafat de que se estaba preparando un ataque a gran escala de Israel al pueblo, instando a los grupos fedayín, como el recientemente formado Frente Popular para la Liberación de Palestina de George Habash y la organización disidente de Nayef Hawatmeh, el Frente Democrático por la Liberación de Palestina, a que retiraran sus fuerzas del pueblo. Aunque un general de división jordano favorable a Fatah le aconsejó que retirara a sus hombres y su sede hacia las colinas cercanas, Arafat se negó, afirmando: «Queremos convencer al mundo de que en el mundo árabe hay quien no se rinde ni huye.» Aburish escribe que fue por las órdenes de Arafat que Fatah se quedó en su sitio, y que el Ejército Jordano acordó respaldarles si se producían combates fuertes.

En la noche del 21 de marzo, las Fuerzas de Defensa Israelíes atacaron Karamé con armamento pesado, vehículos acorazados y aviones caza. Fatah mantuvo su posición, para sorpresa del ejército israelí. Al intensificar las fuerzas israelíes el ataque entró en juego el Ejército Jordano, provocando que los israelíes se retiraran para evitar una guerra a gran escala. Al final de la batalla murieron cerca de 150 militantes de Fatah, veinte soldados jordanos y veintiocho soldados israelíes. A pesar del gran número de víctimas mortales árabes, Fatah se consideró victorioso por la rápida retirada del ejército israelí. El propio Arafat estuvo en el campo de batalla, pero no están claros los detalles sobre su implicación. Sin embargo, sus aliados (además de la inteligencia israelí) confirmaron que durante la batalla estuvo animando a sus hombres a que mantuvieran sus posiciones y continuaran luchando.

La batalla estuvo cubierta en gran detalle por la revista "Time", y la cara de Arafat apareció en la portada del 13 de diciembre de 1968, llevando por primera vez su imagen a todo el mundo. Tras la guerra, la reputación de Arafat y Fatah recibió un espaldarazo, y Arafat fue considerado un héroe nacional que se atrevía a enfrentarse a Israel. Además de los elogios masivos por parte del mundo árabe, las donaciones económicas aumentaron significativamente y mejoraron el armamento y equipamiento de Fatah. Muchos jóvenes árabes, incluyendo miles de no palestinos, comenzaron a engrosar las filas de Fatah.

En el Consejo Nacional Palestino de El Cairo del 3 de febrero de 1969, Yahya Hammuda renunció a la presidencia de la OLP y Arafat ocupó su lugar. Dos años después se convirtió en el comandante en jefe de las Fuerzas Revolucionarias Palestinas, y en 1973 se convirtió en el líder del brazo político de la OLP.

A finales de los años 1960 aumentaron de manera importante las tensiones entre los palestinos y el gobierno jordano; elementos de resistencia árabe, fuertemente armados, habían creado un virtual «estado dentro de un estado» en Jordania, controlando varias posiciones estratégicas del país. Tras su victoria en la batalla de Karamé, Fatah y otras milicias palestinas empezaron a tomar el control de la vida civil de Jordania. Establecieron controles policiales, humillaban públicamente a las fuerzas policiales jordanas, molestaban a las mujeres y recaudaban impuestos ilegales; todo lo cual Arafat consentía o ignoraba. El rey Hussein consideró que esto era una amenaza creciente a la soberanía y seguridad de su reino y trató de desarmar a las milicias. Sin embargo, para evitar una confrontación militar con las fuerzas de la oposición, Hussein despidió a varios de sus ministros anti-OLP, incluyendo algunos miembros de su propia familia, e invitó a Arafat a convertirse en primer ministro de Jordania. Arafat lo rechazó citando su creencia en la necesidad de un Estado palestino con un liderazgo palestino.

A pesar de la intervención de Hussein, las acciones militantes continuaron en Jordania. El 15 de septiembre de 1970, el FPLP secuestró cinco aviones e hizo aterrizar a tres de ellos en Dawson's Field, situado a 48 km al este de Amán. Cuando los pasajeros fueron trasladados a otro lugar, hicieron explotar tres de los aviones. Esto dañó la imagen de Arafat en muchos países occidentales, incluyendo Estados Unidos, que le tenía por responsable de controlar las facciones palestinas que pertenecían a la OLP. Arafat, cediendo a la presión de los gobiernos árabes, condenó públicamente los secuestros y puso en suspenso al FPLP de cualquier acción guerrillera durante unas semanas. (Había actuado igual cuando el FPLP atacó en Aeropuerto de Atenas). El gobierno jordano se movió para retomar el control sobre su territorio y, el día siguiente, el Rey Hussein declaró la ley marcial. Ese mismo día Arafat se convirtió en el comandante supremo del ELP.

Tras encrudecerse el conflicto, otros gobiernos árabes intentaron negociar una salida pacífica. Gamal Abdel Nasser formó parte de este esfuerzo organizando la primera cumbre de emergencia de la Liga Árabe en El Cairo, el 21 de septiembre. El discurso de Arafat atrajo las simpatías de los líderes árabes asistentes. Otras cabezas de estado se alinearon contra Hussein, entre ellos Muamar el Gadafi, que se burló de él y de su padre esquizofrénico, el rey Talal. Por tanto, fracasó el intento de establecer un acuerdo de paz entre los dos bandos. Nasser murió de un infarto al corazón horas después de la cumbre.

Para el 25 de septiembre, el ejército jordano había conseguido dominar y, dos días después, Arafat y Hussein acordaron un alto el fuego en Amán. El ejército jordano provocó muchas bajas entre los palestinos —incluyendo civiles—, que sufrieron aproximadamente 3500 muertes. Tras repetidas violaciones del alto el fuego tanto por la OLP como por el ejército jordano, Arafat hizo un llamamiento para derrocar al rey Hussein. En respuesta a esta amenaza, en junio de 1971, Hussein ordenó a su ejército expulsar a todos los guerrilleros palestinos que quedaban en el norte de Jordania, lo cual consiguieron. Arafat y varios de sus hombres, incluyendo dos comandantes de alto rango, Abu Iyad y Abu Jihad, fueron arrinconados en el rincón más septentrional de Jordania. Tomaron posiciones cerca del pueblo de Gerasa, cerca de la frontera con Siria. Con la ayuda de Munib Masri, un miembro del gabinete jordano favorable a los palestinos, y Fahd al-Khomeimi, el embajador saudí en Jordania, Arafat consiguió entrar en Siria con cerca de dos mil de sus hombres. Sin embargo, debido a la hostilidad entre Arafat y el presidente sirio Hafez al-Asad (que había expulsado al presidente Salah Jadid), los luchadores palestinos cruzaron la frontera hacia el Líbano para unirse a las fuerzas de la OLP de ese país, donde establecieron su nueva sede.

Desde el final de la Segunda Guerra Mundial participó en el incipiente movimiento palestino, que aspiraba a construir un Estado árabe independiente en el entonces Mandato Británico, lo cual chocaba con las aspiraciones judías sobre el mismo territorio.

Arafat se unió en 1944 a la Liga de Estudiantes Palestinos, de la cual fue presidente desde 1952 hasta 1956. En 1957 o 1959 (las fuentes difieren al respecto) participó en Kuwait en la fundación de la organización Fatah, que reunida en 1964 con otros movimientos y partidos políticos crearon la Organización para la Liberación de Palestina. El 3 de febrero de 1969 pasó a presidir ambas formaciones, simbolizando desde entonces las aspiraciones palestinas de recuperar su patria perdida a costa de la invasión territorial de Israel y frente a las ambiciones de sus vecinos árabes. 

Durante los Juegos Olímpicos de Múnich de 1972, el grupo conocido como Septiembre Negro secuestró y asesinó a once atletas israelíes, en lo que se denominó la masacre de Múnich. Según diversas fuentes, Arafat estaba informado sobre los planes del secuestro en Múnich. Otros autores que sostienen esta información son Mohammed Oudeh (Abu Daoud), uno de los cerebros de la masacre de Múnich, y Benny Morris, un importante historiador Israelí.
Como máximo líder del movimiento, fue rechazado en muchos países occidentales por sus vinculaciones con el terrorismo árabe; pero tuvo también momentos de aceptación, como su famosa alocución ante las Naciones Unidas en virtud del reconocimiento de la OLP como legítima representante del pueblo palestino (1974), o su admisión como miembro de la Liga Árabe (1976).

En 1981 fue recibido en Madrid por el presidente del gobierno de España, Adolfo Suárez, con honores de Jefe de Estado. Fue la primera vez que Arafat era tratado de esta forma en una nación europea.

El ataque israelí al Líbano entre 1982 y 1985 privó a la OLP de las bases desde donde había realizado sus acciones armadas contra Israel y obligó a Arafat a refugiarse con su organización en Túnez, aunque esto no evitó que la aviación israelí bombardeara sus cuarteles generales en este país en 1985. El protagonismo de la lucha árabe pasó entonces al interior, a las poblaciones de los territorios ocupados, que desde 1988 crearon un clima de rebelión permanente contra las autoridades israelíes (la Intifada).

Arafat intentó capitalizar ese movimiento proclamando simbólicamente el 15 de noviembre de 1988 la creación del Estado de Palestina (cuyo gobierno en el exilio presidía él mismo), que obtuvo el reconocimiento de más de sesenta países. Pero las sucesivas derrotas militares de los árabes acabaron por convencerle, a raíz de la desaparición de la Unión Soviética y de la guerra del Golfo en los primeros años noventa, de la necesidad de llegar a un entendimiento con Israel.

El impulso de Estados Unidos a la apertura de un proceso de paz en Oriente Medio le dio la ocasión para iniciar conversaciones secretas con representantes israelíes, que condujeron a los acuerdos firmados en Washington en 1993: Arafat regresó a Cisjordania como titular de un gobierno autónomo (la Autoridad Nacional Palestina) que inicialmente solo tenía poder sobre Gaza y Jericó (después se iría extendiendo a otras ciudades de Cisjordania).

Los retrasos y discrepancias en el plan de retirada israelí de los territorios ocupados añadían dificultad al proceso, viciado por problemas de fondo, como la falta de entendimiento sobre el futuro de Jerusalén (reclamada como capital tanto por los israelíes como por los palestinos) o la falta de apoyo por parte de Siria.

Los esfuerzos de Arafat fueron reconocidos con la concesión, junto a Rabin, del y del Premio Príncipe de Asturias de la Concordia en 1994. El proyecto de paz enfrentó grandes dificultades debido a la oposición de los radicales de ambos bandos. Extremistas palestinos cometieron varios atentados y el 4 de noviembre de 1995 Rabin fue asesinado por un ultranacionalista israelí.

El 20 de enero de 1996 fue elegido presidente de la Autoridad Nacional Palestina, con el 87% de los votos. Desde 2001 vivió en Ramallah bajo arresto domiciliario por autoridades israelíes, violando los acuerdos de paz de Oslo de 1993.

Arafat tuvo una vida muy inestable, y esto también se vio reflejado en sus relaciones. Se lo vincula con una joven egipcia y con otra jordana. También fue pareja de la periodista uruguaya Isabel Pisano, quien le dedicó una biografía íntima y ha publicado polémicas declaraciones relativas a su muerte. Se casó en 1990 con su secretaria Suha Tawil, con quien tuvo una hija, Zahwa, nacida el 24 de julio de 1995 en París.

Según distintas fuentes, Yasir Arafat habría mantenido vínculos homosexuales, aunque habría tratado de ocultar este aspecto a la opinión pública.

En 2004 fue trasladado a Francia, donde se encontraba su esposa, al Hôpital d'Instruction des Armées Percy, un hospital militar francés en Clamart (cerca de París), donde estuvo hospitalizado desde el 29 de octubre y en coma desde el 3 de noviembre.

Murió en la madrugada del 11 de noviembre de 2004 a las 3.30 hora local (2.30 UTC) debido a una hemorragia cerebral según los medios de prensa; a las 4.40 (3.40 UTC) según el comunicado oficial de la Autoridad Nacional Palestina. Pese a que no se facilitó información sobre las causas de su muerte, ciertos rumores, sostenidos recientemente también por Ahmad Jibril, líder y fundador del Frente Popular por la Liberación de Palestina, en una entrevista al canal Al-Manar vinculado a Hezbolá, estiman que se debió al sida. Otras fuentes afirman que murió por envenenamiento urdido por los servicios secretos israelíes, aunque fuera portador del VIH.

Claude Goasguen, parlamentario francés, reclamó una investigación parlamentaria para acallar los rumores sobre el presunto asesinato de Arafat. El Gobierno francés señaló que no había evidencias de que Arafat fuese envenenado, de otro modo se habría abierto una investigación criminal.

En julio de 2012, la cadena de noticias catarí Al Jazeera publicó una investigación de nueve meses en la que distintas pruebas realizadas por el prestigioso Centro de Medicina Legal del Hospital Universitario de Lausana (Suiza) determinaron que las pertenencias de Arafat, especialmente las que habían estado en contacto con sus fluidos corporales, contenían un nivel extremadamente alto de polonio 210, un material radioactivo, y que esto no era explicable por causas naturales, lo que sugiere un envenenamiento como posible causa de su muerte. El mismo mes, un experto israelí citado por el diario Jerusalem Post aseguró que los restos radiactivos del polonio 210 se desvanecen a menos de la mitad en cuatro meses, por lo que los altos niveles descubiertos en las ropas de Arafat indican que fueron "implantados" mucho tiempo después de su muerte.

El presidente de la Autoridad Nacional Palestina, Mahmud Abbas, ordenó al comité encargado de investigar las causas de la enfermedad y la muerte de Arafat que tratase de averiguar la verdad sobre este tema y aceptó que se exhumase el cadáver con ese fin.

Unas horas antes, Suha Arafat, la viuda de Yasir, ya había pedido la exhumación de los restos mortales de su marido. Finalmente a fecha de 27 de noviembre de 2012, se procedió a exhumar el cadáver de Yasir Arafat, bajo supervisión rusa, para esclarecer si fue envenenado. Expertos forenses suizos, rusos y franceses tomaron muestras de los restos para su estudio. En noviembre de 2013, el equipo suizo determinó que las muestras tenían niveles de polonio 210 dieciocho veces superiores a lo normal. En diciembre de ese mismo año, según una fuente anónima que filtró las conclusiones a la prensa, una investigación de la justicia francesa mantenida secreta concluyó que «Arafat no fue envenenado, sino que murió de causas naturales», como consecuencia de una «infección generalizada». Expertos rusos confirmaron posteriormente las conclusiones de la investigación francesa.

En enero de 2018 el periodista e investigador israelí Ronen Bergman en el libro "Rise and Kill First" (Levántate y mata primero) menciona el uso por Israel de radiación para matar al líder palestino.



</doc>
<doc id="22894" url="https://es.wikipedia.org/wiki?curid=22894" title="Piezoelectricidad">
Piezoelectricidad

La piezoelectricidad (del griego "piezein", "estrujar o apretar") es un fenómeno que ocurre en determinados cristales que, al ser sometidos a tensiones mecánicas, en su masa adquiere una polarización eléctrica y aparece una diferencia de potencial y cargas eléctricas en su superficie.

Este fenómeno también ocurre a la inversa: se deforman bajo la acción de fuerzas internas al ser sometidos a un campo eléctrico. El efecto piezoeléctrico es normalmente reversible: al dejar de someter los cristales a un voltaje exterior o campo eléctrico, recuperan su forma. 

Los materiales piezoeléctricos son cristales naturales o sintéticos que carecen de centro de simetría. Una compresión o un cizallamiento provocan disociación de los centros de gravedad de las cargas eléctricas, tanto positivas como negativas. Como consecuencia, en la masa aparecen dipolos elementales y, por influencia, en las superficies enfrentadas surgen cargas de signo opuesto.

En 1824, Sir David Brewster demostró efectos piezoeléctricos utilizando sal de La Rochelle, decidiendo nombrar el efecto piroelectricidad.

Existen dos grupos de materiales:

La propiedad de la piezoelectricidad fue observada por primera vez por Pierre y Jacques Curie en 1881 estudiando la compresión del cuarzo. Al someterlo a la acción mecánica de la compresión, las cargas de la materia se separan. Esto propicia una polarización de la carga, lo cual causa que salten chispas.

Para que en la materia ocurra la propiedad de la piezoelectricidad debe cristalizar en sistemas que carezcan de centro de simetría (que posean disimetría) y, por lo tanto, de eje polar. De las 32 clases cristalinas, en 21 no existe el centro mencionado. En 20 de estas clases ocurre la propiedad piezoeléctrica, en mayor o menor medida. Los gases, los líquidos y los sólidos con simetría no poseen piezoelectricidad. 

Si se ejerce presión en los extremos del eje polar se produce polarización: flujo de electrones se dirige hacia un extremo y genera en él una carga negativa, mientras que en el extremo opuesto se induce una carga positiva. 

Cuando se utilizan láminas de cristal estrechas y de gran superficie, el alto voltaje obtenido –necesario para que salte la chispa– es mayor. Las láminas estrechas se cortan de manera que el eje polar cruce perpendicularmente dichas caras.

La corriente generada es proporcional al área de la placa y a la rapidez de la variación de la presión aplicada ortogonalmente a la superficie de la placa. 

Otra aplicación importante de la piezoelectricidad resulta por cumplirse la propiedad inversa:


La primera aplicación práctica de la piezoelectricidad, que surge de la cualidad de transformar una señal mecánica (presión) en una señal eléctrica (corriente eléctrica), es la del sónar. 

Al final de la Primera guerra mundial se descubrió que las ondas sonoras producidas por los submarinos podían ser detectadas por un trozo de cuarzo sumergido en el agua, en el que se medían las corrientes generadas y posibilitaba la detección de la dirección proveniente del sonido. 

El sónar consta de una sonda (piezoeléctrico) que es un transductor; es decir: funciona según la sucesión de eventos siguiente: 


Dentro de los 32 grupos cristalográficos existen 21 que no tienen centro de simetría. De estos, unos 20 exhiben directamente piezoelectricidad (la número 21 es la clase cúbica 432). Diez de ellos son polares; es decir: presentan polarización instantánea, debido a que en su celda unidad contienen un dipolo eléctrico, y el material exhibe piroelectricidad. De estos –cuando la dirección del dipolo puede invertirse mediante aplicación de un campo eléctrico– algunos son además ferroeléctricos. Las clases cristalográficas son:


Las ecuaciones constitutivas de los materiales piezoeléctricos combinan tensiones, deformaciones y comportamiento eléctrico:

"D" es la densidad de flujo eléctrico, formula_2 es la permitividad y "E" es el campo eléctrico:

"S" es la deformación y "T" es la tensión.

Estas ecuaciones pueden combinarse en una sola ecuación donde se considera la relación entre carga y deformación:

"d" representa las constantes piezoeléctricas del material, y el superíndice "E" indica que la magnitud está medida bajo campo eléctrico constante o cero, y el superíndice "T" señala que se trata de una forma traspuesta de matriz.

Esto se puede reescribir en forma matricial así:

Uno de los usos más extendidos de este tipo de cristales sucede en los encendedores eléctricos. En su interior llevan un cristal piezoeléctrico al cual golpea bruscamente el mecanismo de encendido. Este golpe seco provoca una elevada concentración de carga eléctrica, capaz de crear un arco voltaico o chispa, que enciende el mechero.

Otra aplicación importante de un cristal piezoeléctrico es su utilización como sensor de vibración. Cada una de las variaciones de presión producidas por la vibración provoca un pulso de corriente proporcional a la fuerza ejercida.

Fácilmente se ha convertido una vibración mecánica en una señal eléctrica lista para amplificar. Basta conectar un cable eléctrico a cada una de las caras del cristal y enviar esta señal hacia un amplificador. Por ejemplo, en pastillas piezoeléctricas de guitarra.

Una aplicación adicional muy importante de la piezoelectricidad, pero en este caso al revés, sucede en los inyectores de combustible de los motores de combustión interna. Al aplicarse una diferencia de potencial a un material piezoeléctrico se consigue abrir el inyector, lo cual permite al combustible, a muy alta presión, entrar en el cilindro. El uso de inyectores piezoeléctricos posibilita controlar, con enorme precisión, los tiempos de inyección y la cantidad de combustible que se introduce en el motor. Ello redunda en mejoras en consumo, prestaciones y rendimiento de los motores.

Materiales utilizados en electrónica:





</doc>
<doc id="22897" url="https://es.wikipedia.org/wiki?curid=22897" title="Isógona">
Isógona

El término isógona (del griego "isos", "igual" y "gonios", "ángulo") puede referirse a:


</doc>
<doc id="22899" url="https://es.wikipedia.org/wiki?curid=22899" title="Toro Sentado">
Toro Sentado

Tatanka Iyotanka (en lakota: "Tȟatȟaŋka Iyotȟaŋka"), más conocido como Toro Sentado (en inglés "Sitting Bull", Grand River, Dakota del Sur; "ca." 1831 – ibídem, 15 de diciembre de 1890), fue un jefe nativo americano de la tribu de los sioux.

Era considerado un líder espiritual de los lakota, y también fue elegido como jefe supremo de toda la nación sioux, cuando se incrementaba el acoso del ejército estadounidense sobre sus tierras ancestrales. Sin embargo, la rendición de los nativos era inevitable, por lo que decidió refugiarse en Canadá en 1877, aunque regresó a los Estados Unidos cuatro años después para entregarse a las autoridades gubernamentales.

Pasó los últimos años de su vida en la reserva de Standing Rock, y formó parte del espectáculo de Buffalo Bill. Fue asesinado mientras un grupo de policías lakota le detenían, ya que se le acusaba de instigar una nueva rebelión de los nativos.

Toro Sentado nació en el territorio del Grand River en Dakota del Sur, en el seno de la tribu hunkpapa. Fueron sus padres Jumping Bull y Her-Holy-Door, quienes le pusieron por nombre Jumping Badger (Tejón Saltarín), cuando nació. Su infancia transcurrió sin sobresaltos, y era llamado por sus amigos «Slow» (el sosegado), ya que tenía una conducta muy meticulosa.Sin embargo, a los doce años demostró su intrepidez cuando montó un joven búfalo que había tratado de embestirle, y por esa hazaña su padre organizó una fiesta en su honor.

A los catorce años, su padre le regaló una macana. Dicho objeto tenía un significado especial para los nativos, ya que si el joven lograba golpear a un enemigo en batalla, le podría dar mucho prestigio. Tejón Saltarín tuvo esa oportunidad cuando enfrentó a un bando crow en su primer combate, y en la refriega logró apalear a un contrincante, por lo que su coraje quedó demostrado. El padre, henchido de orgullo, le renombró Sitting Bull o Toro Sentado (Tatanka-Iyotanka) en la ceremonia que ganó el estatus de guerrero. El apelativo hace alusión a la tozudez del animal cuando se encuentra sentado en sus ancas.

A la edad de quince años, el joven guerrero sufrió su primera herida en batalla. Esto sucedió durante un asalto sobre los caballos de la tribu crow, cuando recibió un disparo en el pie izquierdo que le dejó cojo de por vida; sin embargo, él pudo eliminar al causante con una herida mortal con su cuchillo.

Alrededor de los 25 años de edad, Toro Sentado tenía su prestigio bien cimentado. Había logrado expandir los territorios de caza de los siux, y llegó a convertirse en caudillo de los grupos tribales, especialmente de los Silent Eaters que se caracterizaban por sus virtudes guerreras. Precisamente, en el campo de batalla había demostrado su fiereza, pero también su sabiduría y generosidad eran notables, las cuales demostraba con el aprecio a los niños y desfavorecidos, el esmero por buscar la solución pacífica a los conflictos y el cariño a los animales. Por ello, en 1857 se ganó la designación de jefe tribal. Además, se hizo conocedor de la espiritualidad lakota, por lo que también se le reconoció como chamán y curandero. Todos esos méritos le convertían en un líder espiritual.

Entre los años 1863 y 1868, el ejército estadounidense realizaba incursiones en los campos de caza del territorio de los lakota, lo que provocaba continuos conflictos. De hecho, a raíz de una rebelión de la tribu siux santee en Minesota, se realizó una intensa campaña militar en la que Toro Sentado tuvo su primera batalla contra las tropas gubernamentales en junio de 1863. En 1864, volvió a pelear en la batalla de la montaña Killdeer, y también dio albergue a los sobrevivientes de la masacre de Sand Creek en el Territorio de Colorado, que había sido ejecutada por los militares en detrimento de las tribus cheyenne y arapajó.Ese mismo año, la señorita Fanny Kelly cayó como rehén de los siux por cinco meses, y pudo conocer a Toro Sentado. Ella daría testimonio de la hospitalidad del jefe tribal con estas palabras: «Era cortés y afectuoso con su esposa e hijos, y se comportaba de igual manera con los forasteros. En el tiempo que pasé con ellos, la comida escaseaba, y tanto Toro Sentado como su esposa preferían pasar hambre con tal de alimentarme. Mantengo un lugar para ambos en mi corazón». 

Para 1865, mientras se encontraba en el Territorio del río Powder, Toro Sentado lideró una ofensiva contra el fuerte Rice en Dakota del Norte. Dos años después —ya respetado y reconocido por su arrojo, a lo que se sumaban las cualidades innatas de diplomático y buen orador—, fue elegido como el jefe máximo de la nación siux, mientras que Caballo Loco le seguía en el mando.

En la primavera del año 1868, el gobierno de los Estados Unidos pactó un tratado de paz con los siux en Fort Laramie. En dicho convenio se pactó que los nativos se establecieran en la reserva de Black Hills en el Territorio de Dakota.De hecho, Black Hills era un sitio sagrado para los siux, y se reconocía como parte de la denominada Gran Reserva Siux. A los encuentros no asistió Toro Sentado, a pesar que sería persuadido por el sacerdote Pierre-Jean de Smet para que firmase el tratado.

Aunque en el tratado de fort Laramie se garantizaba la protección del Estado sobre la propiedad de los nativos,el descubrimiento de oro en Black Hills provocó la llegada de aventureros que invadían sus campos de caza. Para 1875, se estima que había mil colonos establecidos en el lugar, y cuando fallaron los intentos gubernamentales de comprar la zona —y en contra de lo establecido en el tratado— se dispuso que los nativos se asentaran en las reservas antes del 31 de enero de 1877. Los que no obedecieran dicha orden, se considerarían infractores de la ley. Ante la amenaza, algunos jefes tribales decidieron vender sus tierras, como lo hicieron aquellos que ya se habían asentado en las reservas como Spotted Tail y Nube Roja;Toro Sentado, por el contrario, decidió defender lo suyo.

Para el mes de marzo de 1876, tres unidades militares se desplegaron en el área y arrasaron los asentamientos de Caballo Loco y del cheyenne Two Moon. Los nativos que lograron escapar fueron recibidos por Toro Sentado. Precisamente, los lakota consideraron que no podían enfrentarse a los militares por sí solos, por lo que Toro Sentado hizo un llamamiento a otras tribus —entre ellos los cheyenne, arapajó, miniconjou, sans arc y brulé— al sitio de Rosebud Creek en el Territorio de Montana. Se estima que el número de nativos reunidos llegó a quince mil almas.Asimismo, el jefe tribal aprovechó la reunión para realizar una plegaria:

El supremo jefe tribal cumplió su promesa y realizó la Danza del Sol. Como parte de la ceremonia, su asistente Jumping Bull le hizo cincuenta cortes en cada brazo, a manera de sacrificio. Posteriormente, Toro Sentado comenzó a danzar: Se desplazaba con un movimiento rítmico de sus pies y, sin comida ni agua, también oraba y observaba la trayectoria del sol. Se dice que terminó el ritual al mediodía siguiente cuando cayó exhausto.En medio del ofuscamiento, describió una visión en la que una multitud de soldados y nativos caían del cielo. Toro Sentado expresó que los soldados eran ofrenda de Wakan Tanka, por lo que exhortó a su gente a que los aniquilaran en la batalla. Sin embargo, les previno que no debían tomar sus armas, caballos, ni cualquier otro despojo, porque de lo contrario sería la perdición de los nativos.
Inspirado por la visión, el guerrero Caballo Loco armó un contingente de 500 combatientes. El 17 de junio los nativos provocaron la retirada de la tropa del mayor George Crook en la batalla de Rosebud, y posteriormente los vencedores acamparon en Little Big Horn, donde llegaron otros tres mil nativos que habían abandonado sus reservas para unirse a Toro Sentado. El 25 de junio, las tropas gubernamentales al mando de George Armstrong Custer lanzaron un ataque contra los nativos, pero fueron abatidos en la batalla de Little Big Horn, que ha sido considerada la peor derrota de las fuerzas armadas en las Guerras Indias.Se dice que Toro Sentado se encargó de trasladar a un lugar seguro a las mujeres y niños mientras se desarrollaba el combate.Por otra parte, los nativos ignoraron su advertencia y se dieron a la tarea de hacerse de los despojos de los militares.Un mes después, Toro Sentado sostuvo una entrevista con el coronel Nelson Miles, que terminó en un intercambio de disparos entre ambos bandos.
<br><br>

La tragedia de Little Big Horn desató la ira de los estadounidenses, y desde entonces más tropas asediaron a los lakota, y la imagen del indio salvaje y bárbaro se afianzó en todo el país. Para el mes de septiembre, mientras los nativos se habían dispersado por la caza del búfalo, el general George Crook atacó una villa siux ubicada en Slim Buttes, cercana a los campos mineros de Black Hills, en la que se ejecutó indiscriminadamente a hombres, mujeres y niños.Cuando Toro Sentado se presentó en el lugar, cayó en sus manos una nota del militar en la que le dejaba un recado: Que hasta el último de los nativos sería eliminado o hecho prisionero, por lo que reclamaba la rendición de los siux para evitar el riesgo en las vidas de sus mujeres y niños. 

Poco a poco, los jefes tribales comenzaban a rendirse —entre ellos Caballo Loco— y eran obligados a ceder sus tierras. Su situación se había agravado desde que el Congreso del país había decidido interrumpirles el abastecimiento de provisiones, con el objetivo de someterlos a las disposiciones gubernamentales.Toro Sentado decidió resistir. En 1877, encabezó una marcha hacia el territorio de Canadá con unos mil seguidores, fuera del alcance de las tropas y cuyo número ascendería a cinco mil en los meses siguientes con la llegada de otros refugiados. Para el mes de agosto, el general Alfred Terry se desplazó a Canadá para ofrecerle una amnistía a cambio de refugiarse en una reserva, pero Toro Sentado rechazó la propuesta. Por otra parte, los periódicos canadienses comenzaron a difundir noticias falsas de una supuesta confabulación de Toro Sentado con otras tribus locales, para organizar un ataque en los Estados Unidos y generar otros disturbios.

En 1881, sin embargo, tomó la decisión de retornar a los Estados Unidos. La dificultad de alimentar a su gente por la disminución de las manadas de búfalo, la negativa de los canadienses de otorgarles una reserva y darles raciones, la desaparición de varios jefes tribales y el ataque a las familias siux hacían la situación insostenible.El 19 de julio se dirigió al fuerte Buford, junto a su joven hijo Crow Foot, a quien entregó su rifle para que a su vez se lo diera al comandante del lugar como gesto de rendición. También se firmó un acuerdo, en el que se le «otorgaba» el perdón por sus acciones. James Walsh, oficial canadiense que se relacionó con Toro Sentado en ese país, se expresó acerca del jefe siux en estos términos: «No era el criminal que algunos informes describían. No era un hombre cruel. Era afable. No era deshonesto. Era honrado. Amaba a su gente y extendía su mano a cualquiera que correspondiese su amistad».

En Estados Unidos, Toro Sentado reclamó para su gente el derecho de libre tránsito hacia el territorio de Canadá, además de una reserva en Little Missouri River cerca de Black Hills. Sin embargo, el gobierno ignoró su petición y terminaron siendo enviados a la reserva de Standing Rock. Por su parte, y debido al temor que su presencia pudiera provocar un alzamiento, el líder tribal fue trasladado al fuerte Randall, aguas abajo del río Misuri. Allí se le mantuvo como prisionero de guerra por dos años, aunque en realidad tenía una vida relativamente tranquila, ya que era respetado por los soldados, recibía a otros líderes tribales que necesitaban de sus consejos y leía la correspondencia que le llegaba de sus simpatizantes.El 10 de mayo de 1883 se le trasladó a la agencia india de Standing Rock, donde pudo reencontrarse con su gente.

En dicho lugar, el agente a cargo, James McLaughlin, se empeñó en denigrar el estatus de jefe tribal de Toro Sentado, pues le obligaba a trabajar en campo abierto. No obstante, él mantenía su autoridad. Así lo demostró cuando se negaba, aunque sin éxito, a la petición de una comisión de senadores del congreso estadounidense para que parte de la reserva siux se adjudicase a colonos.

En 1885, Toro Sentado obtuvo el permiso para acompañar el show de Buffalo Bill que recreaba las gestas del Viejo Oeste. Con esta compañía logró viajar a través de los Estados Unidos, Canadá y Europa, y ganaba cincuenta dólares semanales por montar a caballo;aparte que se hacía de algún dinero por firmar autógrafos. Sin embargo, se mantuvo en el espectáculo apenas cuatro meses. Se dice que no soportaba la sociedad «civilizada» del hombre de piel blanca, principalmente cuando observaba el sinnúmero de pordioseros que vivían en las calles, a quienes él mismo daba limosna. Además, recibía una que otra ofensa por parte del público que no olvidaba lo ocurrido en Little Big Horn.Pese a todo, en ese tiempo logró entrevistarse con el presidente Grover Cleveland.

Toro Sentado se alojó en una cabaña en la zona del Grand River, en la reserva de Standing Rock en el fuerte Yates, cerca del mismo lugar donde nació. Allí vivía con dos esposas, lo que demostraba que se negaba a dejar sus costumbres y su rechazo a los mandamientos cristianos. También despreciaba el poderío militar del gobierno y se mostraba escéptico ante cualquier promesa gubernamental. No obstante, decidió que sus hijos se educaran en una escuela cristiana, ya que era de la opinión que a las nuevas generaciones de lakotas les sería útil leer y escribir. 

En Standing Rock, el jefe tribal mantenía su influencia entre los suyos. Ellos también tenían en alta estima su sabiduría y recibían sus consejos, pues Toro Sentado les instaba a no regalar sus tierras y a ser precavidos en cuanto a lo que recibían de la cultura del hombre de piel blanca. Además, rechazaba con denuedo los acuerdos de 1888 y 1889 que cedían la mitad de la Gran Reserva Siux y reducían el resto a seis porciones separadas para los nativos. Sin embargo, sabía que era inevitable el traspaso de las tierras, por lo que propuso el precio de 1,25 dólares por acre; pero los enviados gubernamentales hallaron la manera de escoger jefes tribales elegidos por ellos mismos para adquirir dichas parcelas a un precio irrisorio. Por otra parte, en ese tiempo Toro Sentado recibía la asistencia de la neoyorquina Catherine Weldon, quien le proporcionaba ayuda económica y aprendía su idioma, así como ella le enseñaba el idioma inglés.

En 1890, Toro Sentado recibió la visita de Kicking Bear, quien le informó de la "Danza de los Espíritus", una ceremonia que profetizaba el retorno de las antiguas tradiciones nativas y la recuperación de las tierras tomadas por el hombre de piel blanca. El jefe tribal se mostraba escéptico del ritual, pero de todos modos dejó que su gente la practicase si lo consideraban necesario. 

Pronto la ceremonia cobró tanto auge entre los nativos, que fue considerada como un brote de rebelión por el gobierno. Se mandaron tropas a las reservas, y el mismo Toro Sentado, quien no tenía nada que ver con el hecho, fue considerado como el poder detrás de la «maligna práctica religiosa».De inmediato, el agente gubernamental a cargo de los lakotas envió una patrulla conformada por nativos para arrestar a Toro Sentado y obligarle a terminar la ceremonia. Cabe agregar que el jefe tribal había tenido una visión tras su retorno a la reserva, en la que un pajarillo posado en una colina le había anunciado que sería asesinado por los mismos lakotas.

Antes del amanecer del 15 de diciembre de 1890, cuarenta y tres patrulleros lakotas se presentaron en la cabaña de Toro Sentado, quien aún dormía. El jefe tribal asintió a acompañarles y mandó que su caballo fuese ensillado. Sin embargo, ya cuando se marchaban, un grupo de fieles de la "Danza de los Espíritus" retó a los policías. Uno de ellos, de nombre Catch-The-Bear, sacó su rifle y disparó al teniente Bull Head, quien al responder al ataque hirió a Toro Sentado y posteriormente el agente Red Tomahawk le remató con un disparo en la cabeza. Luego los policías entraron en la cabaña donde se encontraba Crow Foot, el hijo del jefe tribal, quien también fue asesinado. La refriega terminó con la vida de otros policías y la de trescientos miembros de la tribu, en especial mujeres y niños.

El cuerpo de Toro Sentado fue enterrado en el cementerio del fuerte Yates. En 1953, un grupo de ciudadanos de Mobridge (Dakota del Sur), con el permiso de los descendientes de Toro Sentado y del gobierno, trasladaron los restos a una colina, donde se erigió un busto del jefe tribal cuyo autor fue Korczak Zoilkowski. Sin embargo, existe disputa de si los restos son verdaderos.Por otra parte, en cuanto a la vida privada de Toro Sentado, se estima que desposó a cinco mujeres, con las que procreó doce hijos.



</doc>
<doc id="22906" url="https://es.wikipedia.org/wiki?curid=22906" title="Cumbre Mundial sobre la Sociedad de la Información">
Cumbre Mundial sobre la Sociedad de la Información

La Cumbre Mundial sobre la Sociedad de la Información (CMSI) fue un evento internacional organizado por la Unión Internacional de Telecomunicaciones (UIT) centrado en los aspectos sociales de la Sociedad de la Información.
Eliminar la brecha digital ("digital divide" en inglés) existente en el acceso a las tecnologías de la información y las comunicaciones en el mundo, específicamente las Telecomunicaciones e Internet, y preparar planes de acción y políticas para reducir dicha desigualdad.

Gobiernos nacionales, el sector privado, organizaciones representantes de la sociedad civil, Naciones Unidas y sus organismos especializados.

La cumbre tuvo dos fases:

En la Declaración del Milenio Naciones Unidas establece, entre otros principios, una serie de decisiones tomadas respecto a la eliminación de la pobreza en el mundo, las acciones a tomar consideran la eliminación de la Brecha Digital, como uno de los objetivos importantes en la lucha contra el subdesarrollo. Algunos de estos dicen:

La Resolución 73 de la Conferencia de Plenipotenciarios de la UIT (Minneapolis, 1998) resolvió encargar al Secretario General de la UIT (Unión Internacional de Telecomunicaciones), inscribir en el orden del día del Comité Administrativo de Coordinación (CAC), que pasó a denominarse Junta de Jefes Ejecutivos del Sistema de las Naciones Unidas para la Coordinación (CEB), la cuestión de la celebración de una Cumbre Mundial sobre la Sociedad de la Información, e informar al ente directivo de la UIT, el Consejo, sobre los resultados de dicha consulta. El Secretario General indicó en su informe a la sesión del Consejo de 1999, que el CAC tuvo una reacción positiva y que la mayoría de las otras organizaciones y organismos especializados de Naciones Unidas expresaron interés en asociarse con la preparación y la celebración de la Cumbre. Se decidió que la Cumbre sería celebrada bajo los auspicios de la Secretaría General de la ONU, y que la UIT asumiría la intervención principal en los preparativos.

En 2001, el Consejo de la UIT decidió celebrar una Cumbre en dos etapas, la primera en Ginebra (Suiza), del 10 al 12 de diciembre de 2003; y la segunda en Túnez, en 2005.

En la Resolución 56/183 de la Asamblea General de las Naciones Unidas se aprobó el marco de la Cumbre adoptado por el Consejo de la UIT así como la función principal de la Unión en la Cumbre y su proceso preparatorio, en cooperación con otras organizaciones y asociados interesados.

La encomienda que se encarguen los preparativos de la Cumbre a un Comité preparatorio intergubernamental de composición abierta, que establecería el programa de la Cumbre, decidiría las modalidades de participación de otros interesados en la Cumbre y concluiría la redacción del proyecto de declaración y el proyecto del plan de acción. Se invita a la UIT a asumir la función administrativa principal de la Secretaría Ejecutiva de la Cumbre y, así mismo, se invita a los gobiernos a participar activamente en el proceso preparatorio de la Cumbre y a enviar a ella representantes del más alto nivel.

Finalmente, en la resolución la Asamblea General alienta asimismo a todos los organismos competentes de las Naciones Unidas y, en particular, al grupo especial de las Naciones Unidas sobre las TIC, a aportar contribuciones. Alienta además a otras organizaciones intergubernamentales y, en particular, a las instituciones internacionales y regionales, las organizaciones no gubernamentales, la sociedad civil y el sector privado; a participar activamente en el proceso preparatorio intergubernamental de la Cumbre y en la propia Cumbre.



</doc>
<doc id="22910" url="https://es.wikipedia.org/wiki?curid=22910" title="Gerónimo">
Gerónimo

Gerónimo (Arizpe, Sonora; 16 de junio de 1829 - Fort Sill, Oklahoma; 17 de febrero de 1909), (en idioma chiricahua fue Goyaałé (AFI [], transcrito en inglés como "Goyathlay", que significa «el que bosteza») fue un destacado jefe militar de los apaches Bendoke. Entre 1858 y 1886 luchó contra los ejércitos mexicano y estadounidense a lo largo del territorio norte de México, junto a Juh, Victorio y Lozen.

Los ataques liderados por Gerónimo son continuación de aquellos iniciados por Mangas Coloradas; dichos ataques son parte de las guerras apaches.

Gerónimo, hijo de Hermenegildo Monteso y Catalina Chagori, fue bautizado el primero de junio de 1829, en la parroquia de la Asunción de María, en Arizpe, Sonora, lugar donde nació.

En el año 1858 las tropas del gobernador militar de Sonora asesinaron a su esposa, a sus tres hijos y a su madre. Gerónimo juró entonces vengarse y se asoció con Cochise, el jefe de los apaches chiricahua. Juntos atacaron Sonora, donde murieron numerosos soldados enemigos. En los años siguientes se sucedieron los ataques a diversas ciudades mexicanas.

Al morir Cochise, su hijo, Naiche, proclamó a Gerónimo jefe de la tribu. No obstante, en 1876 se le obligó a ingresar en una reserva india. Gerónimo rechazó permanecer en ese pedazo de tierra árido y se marchó a México en 1885, acompañado de un grupo de guerreros entre los que estaban Chihuahua Mangas (hijo de Mangas Coloradas) y Nachez. A partir de entonces, iba y venía entre ambos lugares, arengando a su gente para que no aceptaran estar confinados en una reserva y vivir como prisioneros.

En 1886, después de una fuga más de Gerónimo, en esta ocasión junto a aproximadamente una treintena de apaches más, se dio la orden de búsqueda y captura contra Gerónimo, enviándose 5000 soldados (la tercera parte del Ejército estadounidense de la época) y ofreciéndose una recompensa de 2000 dólares estadounidenses.

Gerónimo fue encontrado en la Sierra Madre y decidió entregarse. Mientras los apaches (tanto seguidores de Gerónimo como los que sirvieron al ejército estadounidense) fueron enviados al fuerte Marion, en Florida, EE. UU., en donde las condiciones causaron numerosas muertes por enfermedades, Gerónimo fue recluido en la prisión de Fronteras (Sonora), en donde se conservan valiosos recuerdos en el museo que hoy lleva su nombre, donde permaneció tres años. Transcurrido este tiempo, fue trasladado a una reserva india en Oklahoma, sin que tuviese la ocasión de ver de nuevo a su pueblo. Allí pasó los últimos años de su vida, en los que fue lo que se llamaba por aquel entonces un «indio ejemplar», participando en un desfile presidencial y en la Exposición Universal de San Luis (1904). "En la Exposición panamericana de Búfalo (1901) los organizadores reservaron dentro del Midway, o sitio destinado al entretenimiento, un espacio para la Villa India, donde se representaban las costumbres de los pueblos originales de Norteamérica. Cerca de setecientos indígenas en representación de cuarenta y dos tribus conformaban el Congreso Indio, entre ellos Crazy Snake y Gerónimo, líderes de la resistencia hechos prisioneros por el Gobierno Federal quienes acudieron a la cita fuertemente custodiados por soldados. A todos ellos podía vérseles situados de modo contiguo a un caballo que sumaba y restaba y a un chimpancé que entre sus muchas habilidades estaban las de usar cubiertos, montar en bicicleta y tocar el piano]] (fuente: Ricardo QUIZA MORENO (2007) "Babel revisitada:
exposiciones, globalización y modernidad (1851-1905)" Revista de Historia Contemporánea. Número 7. "Gerónimo estuvo también en la exposición Universal de Saint Louis (1904) donde vendía arcos y flechas y autografiaba fotografías de sí mismo." (Fuente: Verner Bradford, Phillips and Harvey Blume: Ota Benga: The Pygmy in the Zoo. St. Martin's Press, New York, 1992). Murió a los 79 años de edad.

Seis miembros de la sociedad secreta de Yale llamada "Skull and Bones", incluyendo a Prescott Bush, servían como voluntarios del ejército en Fort Sill durante la Primera Guerra Mundial. Varios partidos y organizaciones de Estados Unidos los han acusado de haber robado la calavera de Gerónimo y algunos artículos personales de este jefe apache, incluyendo sus riendas de plata, del Cementerio Apache de Prisioneros de Guerra en Fort Sill, Oklahoma. Alexandra Robbins afirmó que estos objetos estaban en la sede de la secta en Yale.

En 1986, el expresidente apache de San Carlos Ned Anderson recibió una carta y una foto con la calavera en la sede de "Skull & Bones". "Skull & Bones" se ha reunido en varias ocasiones con oficiales para discutir el rumor; el fiscal del grupo, Endicott P. Davidson, niega que tengan la calavera y alega que la profanación y el robo de 1918 son un mito, rechazándolo. En 2006, Marc Wortman descubrió una carta de 1918 del miembro de los "Skull & Bones" Winter Mead, dirigida a F. Trubee Davison, que confirmaba el robo:

Pero Mead no estaba en Fort Sill, y el profesor de historia de la Universidad Cameron David H. Miller afirmó que, por entonces, la tumba de Gerónimo no estaba marcada. La revelación condujo a que Harlyn Geronimo, de Mescalero, Nuevo México, escribiera al presidente George H. W. Bush (también miembro de los "Skull & Bones") pidiéndole el regreso de los restos:
En 2009, Ramsey Clark representó a los descendientes de Gerónimo en un juicio para lograr el retorno de los restos, contra Barack Obama, Robert Gates, y los Skull and Bones, exigiendo el retorno de los huesos de Gerónimo Un artículo en "The New York Times" afirma que Clark "reconoce que no tiene pruebas sustanciales de que la historia sea cierta" Ramsey Clark, ex Fiscal General de los Estados Unidos, que representa a la familia de Gerónimo reconoce que no tienen mayores datos, pero espera que la corte investigue.

Investigadores del rango de Cecil Adams a Kitty Kelley rechazan la historia. Un vocero de Fort Sill afirmó a Adams: "No existen evidencias concretas de que los huesos estén en ninguna otra parte que en la tumba." Jeff Houser, presidente de la tribu apache de Fort Sill, Oklahoma, también llamó a la historia un mito.

Existe actualmente una petición ante el Congreso de los Estados Unidos para repatriar la calavera de Gerónimo


Gerónimo aparece mencionado en las siguientes obras:




</doc>
<doc id="22914" url="https://es.wikipedia.org/wiki?curid=22914" title="Jerónimo">
Jerónimo

Jerónimo o Gerónimo es un nombre propio proveniente del griego antiguo "Ἱερώνυμος" (Hierốnymos) formado a partir de "ἱερός" (hierόs) (sagrado) y "ὄνομα" (ónoma) (nombre).




</doc>
<doc id="22928" url="https://es.wikipedia.org/wiki?curid=22928" title="Romeo y Julieta">
Romeo y Julieta

Romeo y Julieta ("Romeo and Juliet" o "The Most Excellent and Lamentable Tragedie of Romeo and Juliet", 1597) es una tragedia de William Shakespeare. Cuenta la historia de dos jóvenes enamorados que, a pesar de la oposición de sus familias, rivales entre sí, deciden casarse de forma clandestina y vivir juntos; sin embargo, la presión de esa rivalidad y una serie de fatalidades conducen a que la pareja elija el suicidio antes que vivir separados. Esta relación entre sus protagonistas los ha convertido en el arquetipo de los llamados amantes desventurados o "star-crossed lovers". La muerte de ambos, sin embargo, supone la reconciliación de las dos familias.

Se trata de una de las obras más populares del autor inglés y, junto a "Hamlet" y "Macbeth", la que más veces ha sido representada. Aunque la historia forma parte de una larga tradición de romances trágicos que se remontan a la antigüedad, el argumento está basado en la traducción inglesa ("The Tragical History of Romeus and Juliet", 1562) de un cuento italiano de Mateo Bandello, realizada por Arthur Brooke, que se basó en la traducción francesa hecha por Pierre Boaistuau en 1559. Por su parte, en 1582, William Painter realizó una versión en prosa a partir de relatos italianos y franceses, que fue publicada en la colección de historias "Palace of Pleasure". 

Shakespeare tomó varios elementos de ambas obras, aunque, con el objeto de ampliar la historia, creó nuevos personajes secundarios como Mercucio y Paris. Algunas fuentes señalan que comenzó a escribirla en 1591, llegando a terminarla en 1595. Sin embargo, otras mantienen la hipótesis de que la terminó de escribir en 1597.

La técnica dramática utilizada en su creación ha sido elogiada como muestra temprana de la habilidad del dramaturgo. Entre otros rasgos, se caracteriza por el uso de fluctuaciones entre comedia y tragedia como forma de aumentar la tensión, por la relevancia argumental que confiere a los personajes secundarios y por el uso de subtramas para adornar la historia. Además, en ella se adscriben diferentes formas métricas para los distintos personajes, que, en ocasiones, terminan cambiando de acuerdo con la evolución de los mismos personajes; por ejemplo, Romeo se va haciendo más experto en el uso del soneto a medida que avanza la trama. La tragedia ha sido adaptada en numerosas ocasiones para los escenarios, el cine, los musicales y la ópera.

Se desconoce la fecha exacta en que Shakespeare comenzó a escribirla, aunque en ella se hace referencia a un terremoto que, supuestamente, habría ocurrido once años antes de los hechos que se narran. Dado que, efectivamente, Inglaterra fue sacudida en 1580 por un sismo, se supone que Shakespeare pudo haber comenzado a redactar los primeros borradores hacia 1591. No obstante, la existencia de otros terremotos en años diferentes, impide emitir una conclusión definitiva al respecto. Desde un punto de vista estilístico, las semejanzas de "Romeo y Julieta" con "El sueño de una noche de verano", así como con otras obras de entre 1594 y 1595, inciden en la posibilidad de que pudiese haber sido escrita entre 1591 y 1595.

La primera edición de "Romeo y Julieta" es de 1597 y fue publicada por John Danter en formato de cuarto (de ahí el tecnicismo Q1 con el que es conocida). Las diversas diferencias que presenta su texto respecto de ediciones posteriores, ha sido motivo para que haya sido catalogada como una mala versión; T. J. B. Spencer, un editor del siglo XX, describió su texto como "detestable. Una reconstrucción a partir de los recuerdos imperfectos de uno o dos actores", sugiriendo que se trata de una copia ilegal. Se ha aducido también que sus defectos derivan de que, al igual que ocurre con otros textos teatrales de la época, pudo haber sido publicado antes de su representación. No obstante, su aparición respalda la hipótesis de que 1596 es la última fecha posible para la composición de "Romeo y Julieta".

La segunda edición, conocida como Q2, llevaba como título "La excelente y lamentable tragedia de Romeo y Julieta". Fue publicada en 1599 por Cuthbert Burby y editada por Thomas Creede. Respondiendo a lo indicado en la portada (el texto ha sido "corregido, aumentado y revisado"), incluye unos 800 versos más que el texto de Q1. 

Algunos especialistas creen que Q2 está basada en el borrador de la primera escenificación, porque contiene rarezas textuales tales como diferentes nombres asignados a un mismo personaje y "comienzos falsos" en los discursos que, se presume, podrían haber sido suprimidos por el autor, pero preservados erróneamente por el editor. Así las cosas, Q2 presenta un texto más completo y fiable que su predecesor. Esta versión fue reeditada en 1609 como (Q3), en 1622 como (Q4) y en 1637 como (Q5). Por lo demás, es el texto que se sigue en las ediciones modernas.

En 1623 apareció en la recopilación conocida como Primer Folio, con el texto basado en Q3 y con algunas correcciones realizadas sobre la base de un apunte escénico. 

Años después, se publicaron otras ediciones del Primer Folio: en 1632 (F2), en 1664 (F3) y en 1685 (F4). 

Las primeras versiones modernas, basadas en las ediciones en cuarto y en el Primer Folio y sus reediciones, estuvieron a cargo de Nicholas Rowe en 1709 y de Alexander Pope en 1723, quien inició la tradición de editar el texto agregando cierta información adicional y detalles artísticos que no aparecen en Q2, pero sí en Q1. 

La reedición de la obra ha sido constante desde entonces y, a partir de la era victoriana, su edición ha ido acompañada de notas explicativas sobre sus fuentes y el contexto cultural y social en que fue producida.

Verona —ciudad que Shakespeare escogió para su obra— es una de las más prósperas del norte de Italia. El lugar atrae con mayor frecuencia a parejas jóvenes y matrimonios principalmente por haberse ganado el distintivo de la "Ciudad de Romeo y Julieta". Además, se caracteriza por poseer un patrimonio arquitectónico e histórico muy bien conservado, en el que destacan un anfiteatro romano, un castillo de la Edad Media, así como una serie de palacios e iglesias provenientes de la época medieval. Junto a estos atractivos y edificaciones como el Museo de Castelvecchio, Verona posee un edificio denominado la casa de Julieta que, aunque no existe ninguna prueba de que los Capuleto vivieran allí, atrae a muchos visitantes. Su construcción se inició en el siglo XVIII, y podría haber pertenecido a la familia Cappello. En el interior de la construcción existe una estatua de bronce de Julieta, frescos de la obra y una especie de contador con la biografía de Shakespeare. Es conocida también la leyenda de que quien toque el pecho derecho de la estatua tendrá suerte en el amor.

La cuestión de la existencia histórica de Romeo y Julieta es difícil de dirimir. Existen documentos en los que Girolamo della Corte, un italiano que vivió en la época de Shakespeare, afirma que la relación de los dos jóvenes amantes había ocurrido realmente en 1303, aunque ello no ha podido ser comprobado con certeza. Lo único que puede afirmarse es que las familias Montesco y Capuleto sí que existieron realmente, aunque no se sabe si vivieron en la península itálica y tampoco se puede certificar que hayan sido rivales. Otra fuente literaria que menciona a las dos familias es la "Divina Comedia", del italiano Dante Alighieri. En este poema, Dante cita a los Montesco y a los Capuleto como participantes de una disputa comercial y política en Italia. En el mismo testimonio, ambas familias se encuentran en el purgatorio, tristes y desoladas. Para el historiador Olin Moore, eran dos importantes partidos políticos que se hallaban enfrentados en territorio italiano: güelfos y gibelinos. Secundando el mismo aspecto se encuentra Luigi da Porto. Sin embargo, varios académicos consideran que estas familias nunca existieron; Lope de Vega y Mateo Bandello creían que la gente había enriquecido la "creencia" de su existencia con el paso del tiempo.

No hay evidencia alguna respecto a estas sospechas, ya sea en la literatura italiana o en la biografía de William Shakespeare. Sin embargo, para ciertas personas como el historiador Rainer Sousa, el "amor trágico y desmedido de Romeo y Julieta", parece instaurar un arquetipo del amor ideal, muchas veces distante de las experiencias afectivas cotidianamente experimentadas. Tal vez por eso, varios acreditan que el amor sin medida, como el del caso shakespeariano, es real».

Junto a "Hamlet", "Romeo y Julieta" es una de las obras más escenificadas de Shakespeare. Asimismo, sus numerosas adaptaciones han pasado a convertirla en una de sus historias más famosas y perdurables. Incluso era extremadamente popular en la época del autor, añadiendo, que el académico Gary Taylor la denominó como la sexta más famosa de sus obras, tomando en consideración el período consecuente a la muerte de Christopher Marlowe y Thomas Kyd, y preliminar a la popularidad de Ben Jonson, autor del Renacimiento. Taylor predispuso de este lapso, al tener en cuenta que era la época en que consideraban a Shakespeare como al dramaturgo más importante de Londres. 

Se desconoce cuándo se realizó su primera escenificación; la primera edición (Q1) de 1597 dice que: "ha sido teatralizada públicamente [y con muchos aplausos]", deduciendo entonces que ya se había puesto en escena desde antes de que fuese publicado el texto. No obstante, se sabe con certeza que la compañía teatral Lord Chamberlain's Men fue la primera en escenificarla. Tomando en cuenta sus importantes conexiones con el dramaturgo, en la segunda edición (Q2) aparece publicado en una línea del Acto V el nombre de uno de sus actores, William Kempe, en lugar de Pedro, nombre de uno de los sirvientes de la familia Capuleto. Igualmente, se considera que Richard Burbage interpretó por primera vez a Romeo (en ese entonces era el actor principal de Lord Chamberlain's Men), mientras que el joven Robert Goffe asumió por primera ocasión el rol de Julieta. El hecho de que un hombre interpretara a un personaje femenino se debe a que por entonces las leyes prohibían que las mujeres actuaran en el teatro. Además, se estima que la obra tuvo su debut en los teatros isabelinos The Theatre y The Curtain, acompañada en este último de otras producciones recién estrenadas. Debido a que en 1604 se estrenó una versión simplificada en la localidad alemana Nördlingen, también es una de las primeras obras de Shakespeare en haber sido escenificada fuera del territorio inglés.

El gobierno puritano clausuró todos los teatros ingleses el 6 de septiembre de 1642. Tras la restauración de la monarquía, en 1660, se erigieron dos compañías de teatro (King's Company y Duke's Company), por lo que todo el repertorio teatral existente hasta ese momento quedó dividido entre ambas. De esta forma, William Davenant (de Duke's Company) montó una nueva versión de "Romeo y Julieta" en 1662, en la que Henry Harris desempeñó el papel de Romeo, Thomas Betterton el de Mercucio y Mary Saunderson (esposa de Betterton) el de Julieta. Se considera por ello que Saunderson fue la primera mujer en haber interpretado el rol de Julieta de forma profesional. Otra versión paralela a la adaptación de Davenant fue producida por la misma empresa, pero a diferencia de la original ésta consistió en una tragicomedia hecha por James Howard, en donde los protagonistas no morían al final.

En 1680, se estrenó "The History and Fall of Caius Marius" (Historia y final de Caius Marius) de Thomas Otway, catalogada como una de las adaptaciones de Shakespeare más extremas de la Restauración. Esta versión mostraba varias diferencias respecto al guion original, entre las que destaca el haber cambiado a Verona como escenario primordial por la Roma Antigua, la modificación de los nombres de los amantes (Romeo era Marius, mientras que Julieta era Lavinia), la sustitución del enfrentamiento entre las familias italianas por la lucha de clases entre patricios y plebeyos, y el propio final (Lavinia despierta justo antes de que Romeo muera). Contrario a lo que pudiera pensarse, esta representación se convirtió en un éxito tan considerable, que continuó siendo teatralizada durante las siguientes siete décadas. Es importante destacar que el elemento más perdurable de esta versión era la escena final, que continuaría usándose durante los próximos dos siglos, sobresaliendo la adaptación de Theophilus Cibber (hijo de Colley Cibber) de 1744 y la escenificación de David Garrick de 1748, que emplearon variaciones de la misma obra. Asimismo, ambas adaptaciones eliminaron los elementos que eran considerados como inapropiados en esa época. Por ejemplo, en esta última se transfirió a Julieta todo el lenguaje que describía originalmente a Rosalina, con el propósito de acrecentar la noción de la fidelidad y minimizar el concepto de amor a primera vista. En 1750, comenzó una "Batalla de Romeos", con Spranger Barry y Susannah Maria Arne (esposa de Theophilus Cibber) del Teatro Real de Ópera contra David Garrick y George Anne Bellamy del Teatro Drury Lane. 

La primera producción conocida en Estados Unidos fue una versión "amateur" que se estrenó el 23 de marzo de 1730. Información que se conoce debido a un anuncio publicitario en el periódico "Gazette", donde el médico Joachimus Bertrand promocionaba una producción en la que él interpretaría al boticario. Las primeras adaptaciones profesionales en la misma región, fueron producidas por la compañía teatral Hallam Company.

La versión de Garrick se volvió muy popular, llegando a escenificarse a lo largo de casi todo el siglo XVIII y parte del XIX. En 1845 se retomó la obra original de Shakespeare en Estados Unidos, con las hermanas Susan y Charlotte Cushman interpretándola. Dos años después, ocurrió lo mismo en Gran Bretaña con Samuel Phelps, en el Sadler's Wells Theatre. La adaptación de las hermanas Cushman contó con ochenta y cuatro representaciones en total; varios críticos alabaron la interpretación de Susan interpretando a Romeo, llegando inclusive a considerarla como "perfecta". El diario "The Times", escribió al respecto 

A su vez, la reina Victoria escribió en su diario que: 

De esta forma, el éxito de la versión Cushman rompió con la tradición de Garrick, alentando a escenificaciones futuras para que retomasen el guion original de su creador.

Las teatralizaciones profesionales de Shakespeare en la mitad del siglo XIX tenían dos características en lo particular: primeramente, consistían en producciones que tenían el fin principal de mejorar la trayectoria artística de sus protagonistas, por lo que se acostumbraba omitir algunos papeles secundarios con tal de mantener cierta prominencia en los personajes principales. El rasgo restante de esa época se definía por el concepto "ilustrado", calificativo que hacía referencia a la espectacularidad de los montajes elaborados en donde se pretendían llevar a cabo las escenificaciones. Este último factor ocasionaba que existieran largas pausas en plena obra para poder cambiar el escenario cada vez que lo requería el contexto. Asimismo, se usaban pinturas vivientes de manera constante. En 1882, se estrenó la producción del actor Henry Irving en el Lyceum Theatre de Londres. En dicha versión, Irving interpretaba a Romeo, mientras que la actriz Ellen Terry ejercía el papel de Julieta. Cabe mencionar que la versión habría de ser catalogada como uno de los arquetipos del estilo ilustrado, mencionado anteriormente. Tiempo después, en 1895, cuando Irving viajó al continente americano para realizar una gira teatral, Johnston Forbes-Robertson asumió el papel de Romeo en el Lyceum Theather, y su dramatización del personaje fue más natural y realista en comparación a la de Irving; su visión pasó a popularizarse desde entonces. Durante su labor, Forbes-Robertson evitó hacer uso de la espectacularidad de Irving, pues trató de dar a conocer una descripción más realista de Romeo expresando el diálogo poético como prosa realista y evitando florituras melodramáticas.

Por otra parte, los actores estadounidenses comenzaron a competir con sus contra partes británicos; Edwin Booth (hermano de John Wilkes Booth) y Mary McVicker (quien luego se convertiría en la esposa de Edwin) hicieron su propia interpretación de los jóvenes amantes, estrenando su producción el 3 de febrero de 1869 en el suntuoso Booth's Theatre (de la propiedad del primero), en Nueva York. El edificio se caracterizó por contar con maquinaria teatral de estilo europeo, así como un sistema de aire acondicionado único en la ciudad. Algunos reportes informativos mencionan que era una de las producciones más elaboradas de la obra que jamás se hubiera presenciado en América. La escenificación se convirtió en una de las más populares de la época durante las seis semanas que se representó, recaudando más de 60.000 dólares en total. La primera página del programa de la producción apuntaba que "la interpretación sería producida en estricta concordancia con la propiedad histórica, y con todo el respeto, siguiendo cercanamente al texto de Shakespeare".

Profesionalmente, la primera representación japonesa, pudo haber sido la producción que hizo la empresa de George Crichton Miln, la cual viajó a Yokohama como parte de una gira internacional en 1890. En conclusión, a lo largo del siglo XIX la obra había pasado a convertirse en la más popular de Shakespeare, tomando en cuenta las diversas representaciones profesionales llevadas a cabo en ese período. En el siglo XX, solamente la superó "Hamlet".

En 1935, la producción de John Gielgud (estrenada en el New Theatre de Westminster, Londres) tenía como protagonistas al propio Gielgud como Romeo y a Laurence Olivier como Mercucio (intercambiándose mutuamente los papeles durante seis semanas), y a Peggy Ashcroft como Julieta. A manera de inspiración, el actor utilizó una combinación académica de las primeras dos ediciones (Q1 y Q2), para organizar el montaje y los vestuarios acorde a la época isabelina. Al final, sus esfuerzos le valieron un considerable éxito comercial, dando lugar adicionalmente a un realismo histórico sin precedentes en la trayectoria de las escenificaciones del guion. Más tarde, Olivier comparó su actuación con la de Gielgud diciendo:

Con la versión de 1947 de Peter Brook, comenzó un nuevo ciclo en el modo de llevar a cabo los montajes en la interpretación, ya que este buscó enfocarse en un punto donde la trama original pudiera conectarse con la sociedad contemporánea. En sus propias palabras, «una producción sólo puede ser adecuada si es exacta, y buena si es exitosa». Un detalle notable en la adaptación de Brook, consistió en la remoción de la reconciliación final entre los Capuleto y los Montesco.

A lo largo del siglo, la influencia del cine comenzó a preferir actores jóvenes para interpretarla debido a que se asociaron las características de los personajes con menores de edad. Ante esto, los productores contrataron actores jóvenes para que asumiesen los roles protagónicos, destacando las interpretaciones de John Stride y de Judi Dench en la versión que llevó a cabo Franco Zeffirelli, en el teatro Old Vic en 1960. Zeffirelli tomó prestadas algunas ideas de Brook, retirando de igual modo una tercera parte del guion original con tal de hacerlo más accesible para los espectadores. En una entrevista realizada por "The Times", el productor concluyó que «los temas idénticos del amor y la ruptura total del entendimiento entre dos generaciones, tiene relevancia contemporánea». Asimismo, las siguientes escenificaciones se centraron en un contexto coetáneo. Por ejemplo, en 1986, la Royal Shakespeare Company llevó a cabo una producción ambientada en la Verona moderna; las navajas reemplazaron a las espadas, el baile formal se cambió por una fiesta de rock y Romeo se suicidaba con una aguja hipodérmica. En 1997, se estrenó una producción cuya trama sucede en un típico entorno suburbano, en el que Romeo se infiltra en una reunión de parrillada de la familia Capuleto para conocer a Julieta, mientras que ésta se entera en su escuela de la muerte de Teobaldo. Analizando los aspectos anteriormente mencionados, se percibe un interés por parte de las empresas teatrales por adaptar el guion original a un período específico, con el propósito único de permitirle a las audiencias reflejarse en los conflictos subyacentes de la trama. Así, se tiene noción de adaptaciones en donde los sucesos ocurren en medio del conflicto árabe-israelí, en la era del "apartheid" en Sudáfrica o bien en la etapa de rebelión de los indios pueblo. De forma parecida, la versión cómica de Peter Ustinov de 1956, "Romanoff and Juliet", ocurre en un pueblo ficticio de Europa durante los eventos de la Guerra Fría. En otra producción de los años 1980, "The Life and Adventures of Nicholas Nickleby", se realizó una versión burlesca de la escena final de "Romeo y Julieta" usándose para ello algunos elementos de la época victoriana (la escena citada concluía con un final feliz en el que Romeo, Julieta, Mercucio y Paris reviven, mientras que Benvolio revela su disfraz y se descubre que es Benvolia, declarando su amor a Paris). Por otra parte, "Shakespeare’s R&J" de Joe Calarco adoptó un giro inédito en la que se exploró un despertar homosexual de la juventud. Una de las más recientes producciones en este mismo orden fue la comedia musical de Chicago "The Second City's Romeo and Juliet Musical: The People vs. Friar Laurence, the Man Who Killed Romeo and Juliet".

En los siglos XIX y XX, "Romeo y Julieta" pasó a convertirse en la opción preferida de todo el legado de Shakespeare para inaugurar nuevas compañías teatrales. Lo anterior puede ejemplificarse en el debut de la obra en el teatro de Edwin Booth (en 1869), en la nueva estructura del teatro Old Vic (estrenada en 1929, con John Gielgud, Martita Hunt y Margaret Webster en los roles principales), así como en la apertura de Riverside Shakespeare Company de Nueva York, en 1977.

La primera traducción al castellano se debe a Manuel García Suelto, en traducción de la adaptación francesa de Le Tourneur (1783), y con el título de "Julia y Romeo" (1803). La siguiente versión se debe al dramaturgo cordobés Dionisio Solís en 1817, de la versión francesa de Jean François Ducis. La primera representación que consta de esta obra, se celebró en el Teatro del Príncipe de Madrid el 14 de diciembre de 1818, formando el elenco de actores Manuela Molina, María Maqueda, Andrés Prieto, Fernando Avecilla, Joaquín Caprara, Ramón López y Manuel Prieto.

El 17 de abril 1849 Víctor Balaguer publicó el drama trágico en cinco actos "Julieta y Romeo", inspirado en el original. Nueve años más tarde hacía lo propio Ángel María Dacarrete, con una obra de igual título que la anterior y que se representó en el Teatro Novedades de Madrid con interpretación de José Calvo el 29 de mayo de 1858.

También del francés es la traducción de Manuel Hiraldez de Acosta (1868). La primera traducción directa del inglés corresponde a Matías de Velasco y Rojas y se publicó en 1872. Posteriormente llegarían, entre otras, las de Jaime Clark (1873), Guillermo Mcpherson (1880), Marcelino Menéndez y Pelayo (1881), Roviralta Borrell (1909), Cipriano Montoliu (1910), Gregorio Martínez Sierra (1918) y Pablo Neruda (1964).

La obra se representó en varias ocasiones a lo largo del siglo XIX. Ya en el siglo XX puede mencionarse la representación en el Teatro Novedades de Barcelona en 1913, con interpretación de Ricardo Calvo y Lola Velázquez. En 1943 se representó en el Teatro Español, dirigida por Cayetano Luca de Tena, y con José María Seoane en el papel de "Romeo", Mercedes Prendes en el de "Julieta" y Alfonso Muñoz. La versión de Neruda se puso en escena en el Teatro Fígaro de Madrid, con interpretación de María José Goyanes, Eusebio Poncela, Rafaela Aparicio y Luis Peña.

Para Televisión española se ha versionado en dos ocasiones: La primera emitida el 22 de diciembre de 1967 en el espacio "Teatro de siempre", en adaptación de Antonio Gala, dirigida por Luis Lucia e interpretada por Federico Illán ("Romeo"), Enriqueta Carballeira ("Julieta"), Ana María Noé, Mayrata O'Wisiedo, Estanis González, José Luis Pellicena y Andrés Mejuto. La segunda en el espacio "Estudio 1" en octubre de 1972, con dirección de José Antonio Páramo e interpretación de Tony Isbert ("Romeo"), Ana Belén ("Julieta"), Laly Soldevila, Agustín González, Carlos Lemos y Víctor Valverde.

Al menos unas veinticuatro óperas se han basado en "Romeo y Julieta". La más antigua, "Romeo und Julie", apareció en 1776 al estilo de "singspiel" (pequeña ópera popular) por Georg Benda. Esta producción omitió gran parte de la acción relatada en el guion, así como a la mayoría de los personajes, contando asimismo con un final feliz. Ocasionalmente, se retomó en la sociedad contemporánea. Por otro lado, la ópera más conocida es "Roméo et Juliette" de Charles Gounod, estrenada en 1867 (el libreto fue escrito por Jules Barbier y Michel Carré). Tras su debut, pasó a ser considerada como un "triunfo" por la crítica. A partir de entonces, se ha interpretado a menudo a "Roméo et Juliette". La versión lírica "I Capuleti e i Montecchi" de Vincenzo Bellini también ha pasado por la misma situación, aunque en ocasiones se la ha criticado negativamente por sus diferencias con el guion de Shakespeare. Para la producción, Bellini y su libretista, Felice Romani, retomaron algunos elementos culturales de Italia citados en un libreto que Romani originalmente redactó para una ópera de Nicola Vaccai. 

La sinfonía de Hector Berlioz ("Romeo y Julieta") es una "composición dramática" de gran escala dividida en dos partes, una para solistas, y otra para coro y orquesta. Se estrenó en 1839. La obra homónima de Piotr Ilich Chaikovski, escrita en forma de obertura-fantasía y estrenada en 1869, es un poema sinfónico de considerable extensión que contiene la famosa melodía conocida como "tema de amor". Chaikovski sugirió que se repitiera esa pieza musical en las escenas del baile, el balcón, la recámara de Julieta y la tumba. Usando el mismo recurso de Chaikovski (repetir varias veces la pieza a lo largo de una obra como un "leitmotiv"), Nino Rota creó su propia melodía que luego habría de ser introducida en la película de 1968, al igual que la canción "Kissing You" de Des'ree en la cinta de 1996. Otros compositores clásicos que se vieron influenciados por "Romeo y Julieta" son Johan Svendsen ("Romeo og Julie", de 1876), Frederick Delius ("A Village Romeo and Juliet", 1899-1901) y Wilhelm Stenhammar ("Romeo och Julia", de 1922). 

La versión más conocida para ballet corrió a cargo de Sergéi Prokófiev. La compañía Ballet Mariinski, con su versión de "Romeo y Julieta" de Prokófiev, fue rechazada por la empresa hasta en dos ocasiones diferentes: una cuando Prokófiev intentó añadirle un final feliz a la trama, y la otra debido a la naturaleza experimental de su música. Con el paso del tiempo ha ganado una vasta reputación, llegando a ser coreografiada por John Cranko en 1962 y por Kenneth MacMillan en 1965, entre otros. 

"Romeo y Julieta" ha influido a varias producciones de "jazz", entre las cuales sobresalen la interpretación de Peggy Lee, «Fever», en 1956, y la melodía "The Star-Crossed Lovers" (incluida en el álbum "Such Sweet Thunder") del compositor Duke Ellington. En esta última los protagonistas son representados por un saxofón tenor y un saxo alto; los críticos percibieron que el saxo de Julieta sobresale en la pieza, más allá de ofrecer una imagen de equidad con el saxo tenor. Asimismo, la obra ha inspirado a diversos exponentes de la música popular. Entre ellos se encuentran The Supremes, Bruce Springsteen, Tom Waits y Lou Reed. Igualmente, el grupo musical My Chemical Romance hace alusión a "Romeo y Julieta" en su canción «The Sharpest Lives», mientras que el sencillo «Mademoiselle Juliette» de la cantante francesa Alizée presenta a una Julieta cansada de vivir en un entorno donde todo resulta fallido. No obstante, la pista más famosa del conjunto es el tema «Romeo and Juliet» de la banda de rock Dire Straits. Cabe mencionar también a Liv Kristine con la canción «In the heart of Juliet».

Por otra parte, el musical de teatro más famoso es "West Side Story", musicalizado por Leonard Bernstein y escrito por Stephen Sondheim. La producción debutó en Broadway en 1957, siendo estrenada en el distrito inglés West End al año siguiente. Tres años después, en 1961, se adaptó exitosamente en una película. La versión cinematográfica trasladó los sucesos del musical a una ciudad de Nueva York de mediados del siglo XX, mientras que las familias rivalizadas se convirtieron en pandillas. Otros musicales notables son la producción de rock de 1999, "William Shakespeare's Romeo and Juliet" (por Terrence Mann), la versión de Gérard Presgurvic, Roméo et Juliette, de la Haine à l'Amour (estrenada en 2001) y "Giulietta e Romeo" de Riccardo Cocciante, de 2007.

Su composición y trama ha tenido una profunda influencia en la literatura posterior. Anteriormente, el amor no solía ser visto como un elemento digno de una tragedia. En palabras de Harold Bloom, Shakespeare "inventó la fórmula de que lo sexual se convierte en lo erótico cuando se cruza con la sombra de la muerte". De las obras de Shakespeare, es la que más ha generado variaciones, ya sean trabajos producidos en versos narrativos o en prosa, pinturas, dramas, óperas y composiciones corales, orquestales y de ballet, así como distintas versiones para cine y televisión. En la lengua inglesa, al igual que en muchos países de habla hispana, la palabra "Romeo" se considera como sinónimo de "amante masculino". Respecto a parodias, "Romeo y Julieta" fue satirizada en "Las Dos Furiosas Mujeres de Abingdon" (1598) de Henry Porter, y "Blurt, Master Constable" (1607) de Thomas Dekker, específicamente en la escena del balcón, donde una heroína virgen recita palabras indecentes. Desde otra perspectiva, la obra shakesperiana influenció también a ciertos trabajos literarios, destacándose el texto "Nicholas Nickleby", de Charles Dickens.

De la misma manera, la obra ha sido ilustrada en innumerables ocasiones. La primera ilustración conocida es una xilografía representando la escena del balcón, que se atribuye a Elisha Kirkall, y que pudo haber sido creada probablemente en 1709 para una edición de las obras de William Shakespeare producida por Nicholas Rowe. En el siglo XVIII, la Galería Boydell Shakespeare encomendó cinco pinturas de la obra que retratasen cada uno de los cinco actos de la tragedia. Por otro lado, la tradición de las producciones "pictóricas" del siglo XIX llevó a los productores a recurrir a pinturas con el propósito de tomar inspiración para sus adaptaciones; estas obras terminaban influenciando a los pintores para representar a los actores y escenarios ideales para cada versión de teatro. En el siglo XX, los íconos visuales de las obras empezaron a derivarse de las producciones cinematográficas de la época.

En la historia del cine se la considera como la tragedia más adaptada de todos los tiempos. La versión original de Shakespeare se filmó por primera vez en la era del cine mudo por Georges Méliès, aunque la película está considerada como "perdida". Así, se estima a "The Hollywood Revue of 1929", protagonizada por John Gilbert y Norma Shearer, como la primera versión cinematográfica con audio. Por otra parte, Renato Castellani ganó el León de Oro en el Festival de Venecia por su película homónima de 1954. En esa versión, el actor experimentado Laurence Harvey interpretó a Romeo, mientras que el rol de Julieta recayó en Susan Shentall.

Las producciones cinematográficas más famosas son el filme de 1936 (nominado a cuatro premios Óscar) y dirigido por George Cukor, la versión de 1968 del director Franco Zeffirelli y "Romeo + Julieta" de Baz Luhrmann en 1996. En su época, estas dos últimas se convirtieron en las cintas basadas en el legado de Shakespeare más exitosas de la industria. La película de Cukor (que no recibió una entusiasta recepción durante su exhibición, siendo criticada por brindar una imagen "superficial" de la trama, aspecto contrastante con la producción previa de Warner Bros., "El sueño de una noche de verano") se caracterizó por haber sido protagonizada por Norma Shearer y Leslie Howard, quienes por entonces sumaban más de setenta y cinco años; la versión de "Romeo y Julieta" de Zeffirelli contó con jóvenes atractivos en los roles estelares, mientras que "Romeo + Julieta" estuvo dirigida a audiencias juveniles.

El experto Stephen Orgel describe a la película de Franco Zeffirelli como "llena de gente hermosa y joven; las cámaras y luces exuberantes contribuyen a la energía sexual y atractivo de los actores". Cabe mencionarse que los protagonistas Leonard Whiting y Olivia Hussey, a pesar de ser jóvenes (Whiting tenía dieciocho años y Olivia sólo quince), habían participado en otros proyectos anteriores a "Romeo y Julieta". Sin embargo, Zeffirelli argumentó que la razón de haberlos elegido como protagonistas era su inexperiencia y juventud. Visto desde una perspectiva general, el director recibió elogios por dicha producción, especialmente por la escena del duelo donde expresa cómo una situación puede "salirse de control". No obstante, generó cierta controversia por las tomas de los protagonistas desnudos en la escena de luna de miel, puesto que Olivia Hussey era menor de edad en aquel entonces. "Romeo + Julieta" (), junto a su banda sonora, cautivó a toda una generación conformada por jóvenes que se vieron "conectados" con la trama expuesta. Un poco menos "oscura" que la versión de Zeffirelli, la adaptación de Luhrmann se sitúa en una "sociedad grosera, violenta y superficial" de las ficticias Verona Beach y Sycamore Grove. Protagonizada por Leonardo DiCaprio y Claire Danes, consiguió ser elogiada por la crítica especializada. Resultó además notable la actuación de Danes como Julieta, calificada como "perfecta y espontánea". Otras películas basadas en el mismo concepto son "Romeo Must Die" y "Chicken Rice War", ambas de 2000, así como la versión independiente de Lloyd Kaufman, "Tromeo and Juliet". De la misma forma, la producción mexicana "Amar te duele" utiliza el tema del amor prohibido para relatar la historia de sus protagonistas.

Respecto a las adaptaciones televisivas, en 1960 Peter Ustinov realizó una parodia de la Guerra Fría ("Romanoff and Juliet"), inspirada en la obra de Shakespeare. Asimismo, el musical de 1961 "West Side Story" denota a los Montesco y Capuleto como los ficticios Jets (la población blanca) y Sharks (nativos de Puerto Rico). En 2006 la película "High School Musical" de Walt Disney Pictures utilizó la trama de "Romeo y Julieta" sustituyendo a las familias rivales por dos "pandillas" escolares. Igualmente, la telenovela argentina "Romeo y Julieta", de 2007, hace uso de la trama mediante una adaptación ambientada en la época contemporánea. En el campo de la animación, destacan la serie japonesa "Romeo x Juliet" y la británica "", al igual que la película "" dirigida por Phil Nibbelink.

Peculiarmente, varios directores acostumbran incorporar escenas de sus actores interpretando a "Romeo y Julieta". El concepto de cómo desarrolló Shakespeare su tragedia del amor prohibido ha sido también utilizada en varias producciones, destacando la versión de John Madden "Shakespeare in Love", estrenada en 1998, donde se reconstruye el ambiente del teatro isabelino.

Para la creación de "Romeo y Julieta" el dramaturgo se basó en varios elementos provenientes de una antigua tradición de relatos trágicos sobre el amor. Uno de ellos es "Píramo y Tisbe" de "Las Metamorfosis" de Ovidio, el cual posee algunas similitudes con la tragedia de Shakespeare —las dos tramas se enfocan en los desacuerdos existentes entre los padres de los jóvenes enamorados y la falsa creencia por parte de Píramo de que su amada, Tisbe, estaba muerta—. De la misma manera, la novela griega "Habrócomes y Antía", escrita por Jenofonte de Éfeso en el siglo III, narra una historia semejante, pues incluye la obligada separación de los protagonistas, así como la poción que induce al "sueño profundo".

La primera edición conocida fue el relato trigésimo tercero de "Il Novellino", obra del autor Masuccio Salernitano publicada en 1476. Esta novela italiana se desarrolla en Siena, siendo particularmente referida como "un relato acontecido en la época del autor". Algunos de sus elementos narrativos (la boda secreta, el fraile bondadoso, el exilio de Mariotto, el matrimonio forzado de Gianozza, el veneno y el importante mensaje que nunca llega a su destinatario) son mayormente conocidos por la obra de Shakespeare. No obstante, cuenta con grandes diferencias hacia el final del relato: Mariotto es capturado y decapitado, mientras que Gianozza muere de tristeza.

Cinco décadas después, Luigi da Porto adaptó "Il Novellino" en una nueva edición titulada "Giulietta e Romeo", lanzada en 1530 con la denominación original de "Historia novellamente ritrovata di due Nobili Amanti" ("Novela del encuentro de dos nobles amantes"). Para redactar su escrito, Da Porto se inspiró en "Píramo y Tisbe" y en el libro de cuentos "El Decamerón" de Giovanni Boccaccio. Algunas fuentes insisten en que "Giulietta e Romeo" se convirtió en la primera obra en incluir la mayoría de los elementos característicos de "Romeo y Julieta", citando entre ellos a los nombres de los protagonistas y los de las familias rivales, así como la sede de la tragedia en Verona. Además, Da Porto introdujo a los personajes originales de Mercucio, Teobaldo y el conde Paris, a los cuales Shakespeare acabó desarrollando. La primera edición hecha sobre su obra la publicó como "historia verídica", insistiendo en que los sucesos presentados en la tragedia habían sucedido durante el reinado de Bartolomeo II della Scala en el siglo XIII. En esa misma época se tiene constancia de la existencia de los Montesco y de los Capuleto como facciones políticas, sin embargo su única interacción apareció plasmada en "Cantos del Purgatorio" de Dante Alighieri. Otra de las similitudes es la manera en que Giulietta se atraviesa el pecho con la daga de Romeo, quien antes había muerto tras beber el veneno.

En 1554, Matteo Bandello publicó su propia versión de "Giuletta e Romeo", siendo incluida en el segundo volumen de poemas de la colección "Novelle". Su adaptación opta por ahondar en la depresión de Romeo al comienzo de la novela original de Da Porto, al igual que en la rivalidad de los Montesco y los Capuleto; él fue quien introdujo por primera vez a la nodriza de Julieta y a Benvolio. Cinco años después, en 1559, su historia se tradujo al francés por Pierre Boaistuau, incluyéndola en su volumen "Histories Tragiques". En la traducción de Boaistuau se añadió más sentimentalismo a la trama, mientras que la retórica de los personajes adquirió una fuerza impetuosa que le habría de conferir una cualidad vehemente.

Luego seguiría la adaptación de Arthur Brooke, quien partió de una traducción a partir de la versión francesa de Boiastuau para llegar a conformar un poema, "The Tragical History of Romeus and Juliet", influenciado por "Troilo y Crésida" de Geoffrey Chaucer. En esa época, había una intensa fascinación por las novelas italianas —los cuentos italianos se habían vuelto muy populares entre los guionistas y actores de teatro—, por lo que se considera que Shakespeare pudo haberse familiarizado con la colección de cuentos de William Painter, "Palace of Pleasure", de 1567. Esta colección incluía una versión en prosa de "Romeo y Julieta", titulada ""The goodly History of the true and constant love of Rhomeo and Julietta"" ("La grandiosa historia del constante amor verdadero de Romeo y Julieta"). A partir de lo anterior, el autor inglés optaría por redactar una serie de novelas provenientes de relatos italianos, entre ellas "El mercader de Venecia", "Mucho ruido y pocas nueces", "A buen fin no hay mal tiempo" y "Medida por medida". Finalmente, su interpretación de "Romeo y Julieta" se convertiría en una dramatización del poema de Brooke, así como en una extensión de la trama a partir de los personajes protagonistas y secundarios (especialmente, la nodriza y Mercucio).

Tanto el poema mitológico "Hero y Leandro" como "Dido, reina de Cartago", ambos del autor Christopher Marlowe, se redactaron durante la época en que Shakespeare comenzó a escribir "Romeo y Julieta". Dichas obras son consideradas como influencia indirecta de esta última, pudiendo ser las responsables de la atmósfera en la que la historia del "amor trágico" logra concretarse en el bien conocido guion.

El relato se extiende entre las relaciones de dos sobresalientes familias de Verona, Italia, además del gobierno municipal. Su historia acontece en cinco actos.



La representación y puesta en escena comienza con una disputa callejera entre los Montesco y los Capuleto. El príncipe de Verona, Della Escala, interviene entre ellos y declara un acuerdo de paz que en caso de ser violado habría de ser pagado con la muerte. Después de los sucesos, el conde Paris se reúne con el señor Capuleto para conversar sobre la idea de contraer matrimonio con su hija, pero Capuleto le pide que espere durante un plazo de dos años más, tiempo tras el cual Julieta cumpliría quince años. Aprovechando el ofrecimiento, le sugiere que organice un baile familiar de carácter formal para celebrar tal acontecimiento. Mientras tanto, la señora Capuleto y la nodriza de Julieta intentan convencer a la joven de que acepte casarse con Paris.

En diferentes circunstancias, Benvolio habla con su primo Romeo, hijo de los Montesco, sobre su más reciente depresión. Convencido de que la tristeza de su primo se debe al amor no correspondido de una joven llamada Rosalina —sobrina del señor Capuleto—, Benvolio le informa acerca del baile familiar de los Capuleto. Finalmente, Romeo acepta acudir sin invitación a la ceremonia, esperando encontrarse con Rosalina. No obstante, cuando llega al hogar de los Capuleto, se encuentra con Julieta y se enamora perdidamente de ella. Tras concluir el baile, en la secuencia conocida como "la escena del balcón", Romeo se infiltra en el patio de los Capuleto y escucha secretamente a Julieta, quien está en el balcón de su dormitorio, admitiendo su amor por él a pesar de la hostilidad entre su familia y los Montesco. 

Con el paso del tiempo, el joven comienza una serie de encuentros con la muchacha, hasta llegar al momento en que ambos deciden casarse. Con la asistencia de Fray Lorenzo, quien espera reconciliar a los grupos rivales de Verona a través de la unión de sus hijos, al día siguiente del juramento de amor, los enamorados se casan en secreto. Ofendido por la intromisión de Romeo en el baile familiar, Teobaldo —primo de Julieta— reta al joven a un duelo. Sin embargo, Romeo evade el combate. Impaciente tanto por la insolencia de Teobaldo como por la "cobarde sumisión de Romeo", Mercucio —amigo de Romeo— acepta el duelo, aunque resulta mortalmente herido por Teobaldo. Dolido ante la muerte de su amigo, Romeo retoma el enfrentamiento y logra asesinar al primo de Julieta. A consecuencia de lo anterior, el príncipe exilia al joven de la ciudad, reiterando que si regresa, "sería lo último que haría en su vida". Malinterpretando la tristeza de su hija, el señor Capuleto decide ofrecerla en matrimonio al conde Paris, intentando convencerla de aceptarlo como esposo y convertirse en su "feliz consorte". Finalmente, la joven acepta bajo la condición de prolongar la boda, aun cuando su madre se niega terminantemente. Mientras tanto, Romeo pasa la noche secretamente en la alcoba de Julieta, donde ambos tienen relaciones sexuales.

Julieta visita a Fray Lorenzo para pedirle sugerencias, y éste conviene en ofrecerle una pócima que la induciría a un intenso coma con duración de "cuarenta y dos horas". Una vez que la joven acepta llevar a cabo la farsa, el fraile le promete enviar un mensaje a Romeo, informándole sobre su plan secreto, por lo que podría volver cuando ella despierte. La noche anterior a la boda, Julieta ingiere la poción y sus familiares, al creerla muerta, depositan su cuerpo en la cripta familiar. 

A pesar de su promesa incondicional, el mensaje de Fray Lorenzo nunca llega a Romeo y, en cambio, éste se encuentra con Baltasar (uno de sus sirvientes), quien le informa de la repentina muerte de Julieta. Frustrado ante semejante noticia, Romeo decide comprarle al boticario de la ciudad un eficaz veneno, antes de acudir a la cripta donde se encuentra Julieta. Al llegar, se encuentra con Paris, quien momentos antes había estado llorando sobre el "cuerpo inerte" de su amada. Creyendo que Romeo es un saqueador de tumbas, el conde lo enfrenta pero muere asesinado por Romeo. Convencido todavía que su amada está muerta, Romeo procede a beber el veneno. Al despertar del coma inducido, Julieta se encuentra con los cadáveres de Romeo y Paris en la cripta; incapaz de hallar una solución a tales circunstancias, determina atravesarse el corazón con la daga de su esposo. Tiempo después, los Montesco y los Capuleto, acompañados del príncipe, se percatan de la muerte de los jóvenes y del conde. Absorto por la trágica escena, Fray Lorenzo comienza a relatar la historia completa del "amor prohibido" entre Romeo y Julieta. Su revelación consigue terminar con la rivalidad entre ambas familias.

"Romeo y Julieta" finaliza con la elegía de Della Escala sobre el "amor imposible" de los jóvenes: "Nunca ha habido una historia más trágica / que ésta, la de Julieta y su Romeo..."

La mayoría de los académicos no ha podido asignar una temática específica a la obra. Existe una propuesta derivada del estudio de los personajes en la que el ser humano, sin ser completamente bueno o malo, posee rasgos de ambos aspectos. Aunque esta propuesta no ha contado con suficiente apoyo, y no se ha podido encontrar un tema central, hay varios temas secundarios que se enredan de manera compleja en la trama. Sus diferentes interpretaciones continúan siendo estudiadas por diversos académicos y expertos en la vida de Shakespeare.

El amor intemporal es uno de los elementos representativos de "Romeo y Julieta". Con el paso del tiempo, sus protagonistas han pasado a ser considerados como iconos del "amor joven destinado al fracaso". Varios estudiosos han explorado el lenguaje y contexto histórico existentes en el trágico romance.
En su primer encuentro, Julieta y Romeo utilizan una forma de comunicación (metáfora) recomendada por varios autores convencionales durante la época en que vivió Shakespeare. Haciendo uso de ellas al implicar las palabras "santo" y "pecado", Romeo fue capaz de evaluar los sentimientos de Julieta hacia él, en un modo inusual. Este método también fue apoyado por el escritor italiano Baltasar de Castiglione (cuyas obras, en su mayoría, fueron traducidas al idioma inglés). Además Castiglione aconsejó que, en caso de que un hombre usara una metáfora a manera de invitación para una mujer, ella podría fingir que no lo entendió, con lo que su pretendiente podría retirarse sin perder su honor. Contrariamente, Julieta participa en la metáfora, expandiéndola. Los términos religiosos "sepulcro", "senda" y "santo" eran muy populares en la poesía de ese entonces, mostrando una propensión al tono romántico, más que a una indirecta blasfemia —el término "santidad" fue asociado con el catolicismo algunos años antes del escrito—. Más adelante, en el mismo texto, Shakespeare determinó eliminar la más clara referencia a la resurrección de Cristo y la Pascua, elementos presentes en "The Tragical History of Romeus and Juliet".

En la "escena del balcón" de Shakespeare, Romeo escucha discretamente el soliloquio de Julieta. No obstante, en la versión de Brooke, ella hace su declaración de amor estando sola. Al introducir a Romeo en la escena donde escucha a escondidas, el autor se deslinda de la secuencia normal del cortejo. Normalmente, se solicitaba que una mujer siguiera un patrón de conducta basado en la modestia y la timidez, con tal de asegurarse de que su pretendiente fuese honesto. La razón de desviarse de la secuencia mencionada, se debe a que Shakespeare quiso agilizar un poco la trama. De esta forma los jóvenes enamorados se vuelven aptos para evadir parte del proceso de cortejo, desplazando el relato, que inicialmente se halla centrado en el desarrollo de su relación sentimental, a un contexto mayormente enfocado en su decisión de contraer matrimonio (tras descubrir sus sentimientos mutuos en una sola noche). En la escena final del suicidio, existe una contradicción en el vínculo con la religión católica, pues los suicidios son considerados por ésta como un pecado que debe ser castigado en el infierno, aunque quienes recurren a éstos con tal de estar con su enamorado ("amor cortés") se vuelven acreedores al paraíso, en donde estarán acompañados de su amante. Es así como el amor entre Romeo y Julieta tiende a ser más platónico que religioso. Otro punto a considerar es la consumación del amor (relaciones sexuales) citada en el escrito original; aun cuando el amor entre ambos era apasionado, la pareja sólo consuma su amor después de casados, cosa que les previene de perder la simpatía del público.

Es posible que "Romeo y Julieta" funcione como una ecuación del amor y el sexo, con la muerte. A lo largo de la tragedia, tanto él como ella (junto con otros personajes secundarios) fantasean con esta "igualdad fulminante", normalmente atribuida a un amante. Por ejemplo, el señor Capuleto es quien se percata primero de la "muerte" de Julieta, comparando este factor con el desvirgamiento de su hija. Además, un poco más adelante Julieta compara, eróticamente, a Romeo con la muerte. Justo antes de suicidarse, decide emplear la daga de éste, diciendo, "¡Oh, feliz daga! Este es tu filo. Corróeme entonces, y déjame morir".

Las opiniones de varios estudiosos difieren en lo que respecta al rol que desempeña el destino en "Romeo y Julieta". En modo de aserción, todavía no hay consenso alguno que decida si los personajes verdaderamente están destinados a morir juntos, o si los sucesos ocurridos se deben a una serie de eventos desafortunados. En los argumentos a favor de la importancia del destino se suele describir a Romeo y Julieta con el término "Star-crossed lovers". 
La anterior frase establece que "las estrellas han predeterminado el futuro de ambos". John W. Draper indica la similitud entre la creencia de los cuatro humores y los personajes principales de la trama (a manera de ejemplo, Teobaldo representaría el enojo). Tras interpretar el texto mediante esta creencia, se reduce considerablemente la cantidad de texto atribuido por audiencias contemporáneas al azar, un rasgo característico observado por audiencias contemporáneas. A pesar de la comparación, otros investigadores ven la historia como una serie de eventos desafortunados, al no mirarlo íntegramente como una tragedia sino como un melodrama emocional. Según Ruth Nevo, el continuo énfasis de la causalidad en el argumento hacen que Romeo y Julieta sea una tragedia no tan nefasta del azar, pero tampoco considera que sea una tragedia de los personajes. Por ejemplo, el hecho de que Romeo desafiara a Teobaldo no es resultado de una acción compulsiva, sino la consecuencia esperada ante el homicidio de Mercucio. En esta misma escena, Nevo observa en Romeo una actitud de perspicacia ante los peligros derivados de las normas sociales, la identidad y los compromisos. Por eso, éste decide matar; no es un producto de "hamartia", sino de una circunstancia determinada.

A lo largo del escrito trágico de Shakespeare, diversos estudiosos han identificado el uso frecuente de imaginería o elementos relacionados con la luz y la oscuridad. Caroline Spurgeon considera a la luz como "un símbolo de la belleza natural del amor joven", concepto a partir del cual varios críticos han expandido la interpretación de este elemento en "Romeo y Julieta". Por ejemplo, Julieta y Romeo se miran recíprocamente como una manifestación conjunta de la luz en un entorno oscuro. Él la describe "similar al sol, más brillante que una antorcha, una joya destellante en medio de la noche, y un ángel iluminado entre nubes oscuras". Incluso cuando ella permanece estática sobre la tumba, aparentemente muerta, él exclama, "Tu belleza hace / de esta bóveda un lugar lleno de luz". Julieta describe a Romeo como "el día en la noche" y como algo "más blanco que la nieve en el lomo de un cuervo". Este contraste de luz y oscuridad pudiera ser entendido, simbólicamente, como el amor y el odio, la juventud y la madurez en una forma metafórica. En ocasiones, estas metáforas crean una especie de ironía dramática. Lo anterior puede evidenciarse en la asimilación del amor entre Romeo y Julieta como "una luz en medio de una oscuridad producida por el odio que los rodea". No obstante, todas sus actividades como pareja son realizadas durante la noche, mientras que la contienda llega a cumplirse en pleno día. Esta paradoja de la imaginería brinda una nueva atmósfera al dilema moral de los jóvenes enamorados: lealtad a la familia o lealtad al amor. Al final de la historia, cuando la mañana se ensombrece y el sol está ocultando su rostro de tristeza, la luz y la oscuridad han regresado a sus lugares apropiados: la oscuridad externa ahora refleja la verdadera lobreguez interior de la disputa familiar, más allá del pesar por el trágico desenlace de los amantes. Cada uno de los personajes reconoce su locura en el día, y finalmente las cosas regresan a su orden natural, debido a la revelación del verdadero amor entre Romeo y Julieta. La luz, como elemento temático en el guion, juega también un papel imprescindible al estar involucrada con el tiempo, concluyendo entonces que Shakespeare la utilizó como una manera conveniente de expresar el transcurso del tiempo a través de las descripciones del sol, la luna y las estrellas.

La percepción del tiempo juega un papel importante en el lenguaje y la trama de la obra. Tanto Romeo como Julieta luchan por mantener un mundo imaginario ausente del transcurso del tiempo frente a las duras realidades que los rodean. Por ejemplo, cuando Romeo jura su amor a Julieta teniendo a la luna de fundamento, ella dice "O no jures por la luna, la inconstante luna, / que mensualmente cambia en su órbita circular, / a menos de que el amor pueda demostrarse igual de variable". Desde un comienzo, los jóvenes son catalogados como "un par de enamorados con estrellas opuestas" ("Star-crossed lovers"), situación que refiere a un vínculo entre las creencias astrológicas y el tiempo. Se pensaba que las estrellas controlaban el destino de la humanidad, y con el paso del tiempo, se movían progresivamente en el cielo trazando junto con su movimiento el destino de la vida humana. En las primeras líneas del escrito, Romeo habla de un presentimiento que tiene sobre la traslación de estos cuerpos celestes, por lo que al enterarse de la muerte de Julieta, desafía a las estrellas preguntando qué es lo que tienen destinado para él.

Otro tema central de "Romeo y Julieta" es la precipitación. Al contrario del poema de Brooke que expande el relato sobre un período de nueve meses, la obra de Shakespeare se desarrolla entre cuatro y seis días. Estudiosos como G. Thomas Tanselle creen que el tiempo era "especialmente necesario para Shakespeare", sobre todo tratándose de los eventos acontecidos en la trágica historia de "Romeo y Julieta". Asimismo, perciben que el autor utilizó referencias "a corto plazo" para la relación de los jóvenes, un concepto contrario a las alusiones "a largo plazo" existentes para describir a las "generaciones más antiguas", con el propósito primordial de resaltar "una carrera destinada a la perdición". Julieta y Romeo se enfrentan al tiempo para propiciar que su amor se extienda por toda la eternidad. Al final, la única manera perceptible en la que ellos pueden vencer al tiempo es con la muerte, aspecto que los vuelve inmortales a través del arte. 

Generalmente, en el plano literario, se considera que el tiempo está vinculado con la luz y la oscuridad. En la época de Shakespeare, las obras teatrales eran usualmente representadas al mediodía, en plena luz del día. Esto pudo haber obligado al autor a que usara palabras que crearan una ilusión dual del día y la noche en sus escritos. Además, Shakespeare utilizó referencias para las estrellas, la luna, el sol y el día junto a la noche para poder crear esta percepción. De forma similar, hizo que algunos de sus personajes refirieran a los días de la semana y a horas específicas para ayudar a que la audiencia comprendiera cuánto tiempo ha transcurrido en su historia. En conjunto, no menos de 103 referencias se han encontrado en la obra para ayudar a entender este paso del tiempo.

Más allá del hecho de que los críticos le han encontrado muchos puntos débiles, continúa siendo considerada como una de las mejores obras de Shakespeare. La primera apreciación conocida al respecto proviene del diarista Samuel Pepys, en 1662, el cual dice: 

Diez años después, el poeta John Dryden elogió el material así como la comicidad de su personaje Mercucio diciendo: 

Ciertamente, el análisis crítico de la trama en el siglo XVIII fue menos escaso, aunque no menos dividido. El dramaturgo Nicholas Rowe fue el primero en reflexionar sobre la temática de la obra, la cual percibió como "el justo castigo de dos familias enfrentadas". A mediados del siglo, el escritor inglés Charles Gildon y el filósofo escocés Lord Kames, sostuvieron que la obra era un fracaso en la medida de que no seguía las reglas básicas del drama: la tragedia debe ocurrir a causa de alguna "hamartia", no de un accidente del destino. En contraste, el escritor y crítico Samuel Johnson consideró que: 

En la última parte del siglo XVIII, y durante el siglo XIX, las opiniones se centraron en debates sobre el mensaje moral del escrito. El actor y dramaturgo David Garrick, en su adaptación de 1748, excluyó a Rosalina pues consideró que la situación de Romeo abandonándola por Julieta era algo muy voluble y temerario. Algunos críticos como Charles Dibdin sostenían que Rosalina había sido incluida a propósito en el guion para mostrar qué tan temerario era el héroe, siendo esta la razón de su final trágico. Otros argumentaron que Fray Lorenzo pudo haber sido el portavoz de Shakespeare en sus advertencias contra la precipitación obsesiva. Con el advenimiento del siglo XX, estos argumentos morales quedaron en disputa por analizadores como Richard Green Moulton, quien mencionó que los accidentes condujeron a la muerte de los amantes, descartando los defectos de los personajes como causa de los sucesos finales ("hamartia").

En "Romeo y Julieta", Shakespeare emplea varias técnicas dramáticas que han recibido aclamación entre la crítica. El principal rasgo elogiado al respecto es el cambio repentino de la comedia a la tragedia, situación que puede ejemplificarse en la conversación calambur (juego de palabras) de Romeo y Mercucio momentos antes de que llegue Teobaldo. Previo a la muerte de Mercucio en el Acto III, el guion tiende hacia una postura más cómica. Sólo después de ese momento, adopta un tono serio y trágico. Aun cuando Romeo es desterrado, y no ejecutado, mientras Fray Lorenzo le sugiere un plan a Julieta para que ella pueda reunirse con su amado, la audiencia todavía puede esperar a que todo finalice bien entre ellos. De forma imperceptible, el público queda en un "intenso estado de suspense" para cuando inicia la última escena en la tumba: si Romeo se retrasa lo suficiente como para que Fray Lorenzo pueda llegar a tiempo, el primero y Julieta podrían salvarse. Estas permutaciones que van desde la esperanza hasta la desesperación, para continuar con el indulto y un nuevo sentimiento de optimismo, sirven para enfatizar la tragedia en el final, donde la última esperanza ha quedado descartada y ambos protagonistas mueren en la última escena. 

El autor también hace uso de argumentos secundarios para ofrecer una visión más clara de las acciones desarrolladas por cada uno de los personajes principales. Por ejemplo, al comienzo del relato, Romeo está enamorado de Rosalina, quien se había mantenido indiferente ante sus insinuaciones románticas. La infatuación de Romeo provocada por los rechazos de ella, está en un contraste evidente respecto a su posterior enamoramiento con Julieta. Lo anterior, proporciona una comparación a través de la cual, la audiencia puede observar la seriedad de la relación entre los amantes. El amor de Paris por Julieta, establece igualmente una disparidad entre los sentimientos que la moza tiene por él y la afectividad que al mismo tiempo tiene por Romeo. El lenguaje formal que ella utiliza con Paris, así como la manera en que habla de él con su nodriza, muestra que sus sentimientos están sólo con Romeo. Más allá de todo esto, la historia complementaria del enfrentamiento entre las familias Montesco y Capuleto se agrava en lo sucesivo, suministrando una atmósfera de odio fundamental que, a últimas instancias, se convierte en el principal factor para que la historia finalice trágicamente.

El dramaturgo utiliza una variedad de formas poéticas a lo largo del relato; inicia con un prólogo de catorce líneas en forma de soneto shakesperiano, el cual es narrado por un coro. No obstante, la mayor parte de "Romeo y Julieta" está escrita en verso blanco, redactada en estrictos pentámetros yámbicos, con una menor variación rítmica que en las obras posteriores del mismo autor. En cuanto a la elección de formas poéticas, Shakespeare va relacionando cada una de ellas a un personaje específico. Tales son los casos de Fray Lorenzo utilizando el sermón y la pena de "latae sententiae", como la nodriza haciendo uso del verso blanco, hecho que muestra una tendencia al lenguaje coloquial. Igualmente, cada una de estas formas, se moldea y adapta a la emoción inherente en la escena donde participa el personaje. Por ejemplo, cuando Romeo habla sobre Rosalina en las líneas iniciales, intenta emplear el soneto de Petrarca. Usualmente, esta corriente la aplicaban los hombres para exagerar la belleza de las mujeres, cualidad que les era imposible alcanzar tal y como se describe en la situación de Romeo y Rosalina. Esta forma poética la usa también la Señora Capuleto cuando le describe a Julieta la apariencia física de Paris, a quien califica como "atractivo". 

En el momento en que la joven pareja se conoce, Shakespeare cambia el petrarquismo de sus respuestas a un estilo de soneto más contemporáneo, mediante el uso de metáforas relacionadas con «santos» y «peregrinos». Finalmente, cuando ambos se encuentran en el balcón, Romeo utiliza un soneto para expresarle su amor, pero Julieta lo interrumpe con el interrogante infalible, "¿Me amas?". Al recurrir ella a esto, busca la expresión verdadera, más que el uso de una exageración poética acerca de su amor. El mismo personaje dispone de monosílabas con Romeo, aunque utiliza un lenguaje formal con Paris.

Otras formas poéticas existentes en la obra incluyen un epitalamio de Julieta, una rapsodia compuesta por Mercucio para definir a la Reina Mab y una elegía hecha por Paris. Shakespeare conserva su típico estilo de prosa para delimitar las expresiones de la gente común, aun cuando en ocasiones la utiliza en personajes como Mercucio. Asimismo, el humor juega un rol indispensable: la investigadora Molly Mahood identificó, por lo menos, 175 juegos de palabras en el texto. La mayor parte de estos son de naturaleza sexual, primordialmente existentes en la relación de Mercucio y la nodriza.

Algunos críticos psicoanalistas se han percatado de que el problema de Romeo y Julieta, en términos de la impulsividad de Romeo, proviene de una «agresión mal controlada y parcialmente encubierta» que, en última instancia, conduce tanto a la muerte de Mercucio como al doble suicidio. Asimismo, consideran que la trama no es psicológicamente muy compleja; mediante un análisis disciplinario, la experiencia trágica masculina resulta equivalente a una enfermedad. En 1966, Norman Holland consideró el sueño de Romeo como un auténtico «deseo lleno de fantasía, tanto en términos de su mundo adulto como de su hipotética infancia en las etapas oral, fálica y edípica», reconociendo que un personaje dramático no es un ser humano con procesos mentales separados de los que presenta el autor. Otros críticos de esta disciplina, tales como Julia Kristeva, se enfocan en el odio existente entre las familias. Así, argumentan que el odio provoca a su vez la pasión mutua de Julieta y Romeo. Igualmente, esa aversión se manifiesta directamente en el lenguaje de los amantes: por ejemplo, Julieta recita que «mi único amor solo surge de mi único odio», en ocasiones expresando su pasión por medio de una anticipación de la muerte de Romeo. Lo anterior conduce a una especulación sobre la psicología del dramaturgo, en particular sobre la consideración del dolor de Shakespeare por la muerte de su hijo, Hamnet.

Varias críticas feministas sostienen que el fundamento del enfrentamiento entre las familias reside en la sociedad patriarcal de Verona. Para Coppélia Kahn, la etiqueta masculina de violencia extrema se impone en la personalidad de Romeo como la fuerza principal que impulsa que toda la serie de acontecimientos finalice en una tragedia. Cuando Teobaldo mata a Mercucio, Romeo se vuelve violento, lamentándose por haberse «afeminado» a causa de Julieta. Desde esta perspectiva, los jóvenes varones «se vuelven hombres» solo al emplear la violencia en nombre de sus padres o, en el caso de los sirvientes, de sus amos. La pelea también denota virilidad, como demuestran los numerosos chistes sobre la virginidad hallados a lo largo de la obra. Por otra parte, Julieta presenta una etiqueta femenina de docilidad al permitir que personajes como el fraile resuelvan sus problemas personales. Otras críticas de esta corriente, tales como Dympna Callaghan, advierten en el feminismo del texto un ángulo histórico donde el orden feudal estaba siendo cuestionado por un gobierno cada vez más centralizado, añadiendo el factor de la llegada del capitalismo como una de las circunstancias principales. Al mismo tiempo, las nuevas ideas del purismo sobre el matrimonio en comparación con otras ideologías más antiguas habían restado importancia a los «males de la sexualidad femenina», adoptando un enfoque a favor del matrimonio por amor; por ello, las evasiones de Julieta respecto a los intentos de su padre por obligarla a contraer matrimonio con un hombre al que ella no ama son un ejemplo de la forma en que Julieta desafía el orden patriarcal de una manera que posiblemente no hubiera sido posible en tiempos pasados.

Diversos análisis provenientes de la teoría "Queer" cuestionan las orientaciones sexuales de Mercucio y de Romeo, comparando su amistad con una forma de amor sexual. En una conversación, Mercucio hace mención del falo de Romeo, sugiriendo indicios de homoerotismo. Citando a manera de ejemplo, el primero dice: «El que se hiciera surgir algún espíritu de extraña naturaleza en el círculo de su adorada y que allí se lo mantuviera hasta que ella, por medio de exorcismos, lo volviese a la profundidad». El homoerotismo de Romeo puede encontrarse en su actitud hacia Rosalina, a quien ve como una mujer distante e indisponible, incapaz de darle esperanza alguna de continuar con su descendencia. Asimismo, tal como añade Benvolio al respecto, «ella es sustituida por alguien con mayor reciprocidad». Los sonetos de procreación de Shakespeare describen a otro joven que, al igual que Romeo, tiene problemas de tipo sexual, por lo cual pudiera ser visto como un homosexual. Críticos especializados en el tema creen que Shakespeare pudo haber introducido a Rosalina como una manera viable de expresar los problemas vinculados con la procreación. En este marco, cuando Julieta dice «[...] eso que conocemos como rosa [o Rosalina] / Con otro nombre olería más dulce», posiblemente está planteándose si hay alguna diferencia entre la belleza de un hombre y la de una mujer.








</doc>
<doc id="22938" url="https://es.wikipedia.org/wiki?curid=22938" title="Cambridgeshire">
Cambridgeshire

Cambridgeshire es uno de los cuarenta y siete condados de Inglaterra, Reino Unido, con capital en Cambridge. Ubicado en la región Este limital al norte con Lincolnshire, al este con Norfolk y Suffolk, al sur con Essex y Hertfordshire, y al oeste con Bedfordshire y Northamptonshire. Ocupa un área de 3389 km² y su población en el año 2011 era de 806 700 habs.

Aunque la mayoría de los terrenos del condado son muy bajos, el punto más elevado se sitúa en la localidad de Great Chishill con una altura de 146 m s.n.m.. Otras colinas elevadas son Little Trees Hill y Wandlebury Hills.

El condado actual es producto de diferentes unificaciones realizadas por el gobierno local. En 1888, cuando se establecieron los consejos de condados, se establecieron dos divisiones administrativas en este condado: una en la zona sur, alrededor de Cambridge, y otra en la Isla de Ely. 

En 1965, estas dos divisiones se fusionaron para formar "Cambridgeshire y la Isla de Ely". En 1974 se les unieron Huntingdon y Peterborough para formar el actual condado de Cambridgeshire. Desde 1998, la ciudad de Peterborough está administrativamente separada del condado y forma una autoridad unitaria. Sin embargo, sigue asociada con Cambridgeshire para diferentes cuestiones como son la policía o los bomberos.

Dos personajes famosos nacidos en el condado de Cambridgeshire son el político y militar Oliver Cromwell y el ex primer ministro británico John Major.




</doc>
<doc id="22944" url="https://es.wikipedia.org/wiki?curid=22944" title="Cochise">
Cochise

Cochise, de nombre apache Shi-Kha-She (c. 1812 - 9 de junio de 1874), fue un jefe de los apaches chiricahua en Norteamérica que lideró una revuelta en 1861, dando así principio al episodio conocido como Guerras apaches. 

Cochise nació en la región de la sierra de Chiricahua, cuando esta pertenecía a México. En ese territorio se habían producido enfrentamientos importantes entre los mexicanos y los apaches desde 1831, hasta que fue anexado a los Estados Unidos después de la Intervención estadounidense en 1847. A partir de ese año comenzó un periodo de relativa tranquilidad. Cochise trabajaba como leñador en una parada de diligencias.

La paz finalizó cuando en 1861 un grupo de apaches se llevó el ganado de un colono y secuestró a su hijo de 12 años. Cochise fue erróneamente considerado el causante del incidente, por el teniente Bascom, un oficial inexperto de 25 años que sólo llevaba cuatro meses destacado en territorio indio. Al mando de 50 soldados de infantería citó a Cochise en Apache Pass para declarar. Cochise, que ignoraba el incidente del secuestro realizado ya que se encontraba a más de 100 kilómetros del lugar en el que se había realizado, creyendo que la invitación de Bascom era para concretar la convivencia entre indios y blancos en el territorio del noreste de Arizona, se presentó acompañado por su mujer, un hermano, dos sobrinos y sus dos hijos pequeños. Bascom estaba convencido de que el secuestro había sido obra de Cochise, lo acusó, no aceptó la explicación de éste de que nada tenía que ver con el mismo y lo arrestó junto con sus acompañantes.

Cochise se resistió y consiguió huir, herido por un disparo en una pierna. Tomó rehenes para poder negociar la entrega de sus compañeros que se encontraban todavía en prisión pero se crearon malentendidos, la desconfianza acabó creando una gran tensión y ambas partes acabaron matando a sus rehenes. Cochise se unió entonces a su suegro Mangas Coloradas, o "Colorado", un jefe de los apaches mimbreños, y juntos llevaron a cabo numerosas incursiones en los terrenos de los colonos. Aquello ocasionó numerosas muertes en ambos lados. Cuando los apaches empezaron a dominar la situación, el mando militar estadounidense envió una expedición en busca de Cochise y su suegro.

En 1862, en el puerto de montaña denominado "Apache Pass", Cochise y Colorado, con 500 hombres, defendieron su posición frente a 3.000 voluntarios estadounidenses. Finalmente, el general en jefe hizo traer cañones, con los que consiguió tomar la posición de los apaches, que huyeron. Colorado fue apresado y asesinado en 1863. Cochise quedó solo como jefe de la insurrección apache. Él y sus hombres se retiraron hacia las montañas, donde pudieron esconderse. En la primavera de 1871 el comisionado de asuntos indios de Washington no logró reunirse con Cochise, que temió una traición. 

En abril de ese mismo año, unos apaches robaron ganado cerca de Tucson, causando la muerte a cuatro blancos. Los habitantes de la ciudad, al mando de William S. Oury, marcharon sobre el campamento de los apaches aravaipas en Camp Grant, matando a sus 144 habitantes, incluyendo mujeres y niños. La matanza provocó la enérgica protesta del presidente Ulysses S. Grant, que envió al comisionado Vicent Colyer a tratar de negociar con Cochise. Además, se transfirió al general George Crook a Arizona para tomar el mando militar del sudoeste. Crook no logró capturar a Cochise, que se refugió en Nuevo México rompiendo el cerco que le rodeaba junto con sus hombres.

Al año siguiente, el general Granger hizo saber a los chiricahua que se dispondría de una reserva para los apaches en los montes Mogollones, pero ellos se negaron a abandonar las tierras de sus antepasados, cuya propiedad les había sido garantizada mediante un tratado. Cochise consiguió escapar de nuevo a Arizona y reanudó las incursiones contra los colonos blancos. En septiembre de 1872, el general Oliver Otis Howard contactó con Cochise y le transmitió los deseos del presidente Grant de llegar a un acuerdo definitivo. Le acompañó un viejo amigo de Cochise, Tom Jeffords. El acuerdo alcanzado permitió que los apaches conservasen una reserva que comprendía los montes Chiricahua y el valle de Sulphur Spring, muy cerca de las Dragon Mountains.

En la primavera de 1874 murió Cochise de causas naturales y lo sucede Gerónimo como jefe de los apaches.






</doc>
<doc id="22948" url="https://es.wikipedia.org/wiki?curid=22948" title="Deadweight">
Deadweight

«Deadweight» —en español: "«Peso muerto»"— es una canción incluida en la banda sonora de la película "A Life Less Ordinary" correspondiente al músico estadounidense Beck. Fue lanzada como sencillo el 27 de octubre de 1997 a través de la discográfica Geffen. Fue nominada a "Mejor Canción de una Película" en los . La canción fue incluida posteriormente en una edición especial en 2008 del álbum "Odelay", así como también una versión alternativa de la canción "Jack-Ass", titulada "Strange Invitation" y una versión en español titulada "Burro". El video musical fue dirigido por Michel Gondry. Entrelaza imágenes de "A Life Less Ordinary" con imágenes de Beck, viviendo en un mundo paradójico.

Beck grabó "Deadweight" con The Dust Brothers entre los álbumes "Odelay" y "Mutations". Fue lanzado en la banda sonora de "A Life Less Ordinary" a finales de 1997. En contraste con la melodía boyante, animada, Beck agrega letras de mala suerte, al estilo de Gram Parsons, sobre el juego, Las Vegas, y la soledad. Beck ha mencionado que esta canción era una parte de su "trilogía brasileña". Sin embargo, a partir de 2001, sólo ha completado dos partes, "Deadweight" y "Tropicalia". A diferencia de "Tropicalia", que es una canción de bossa nova, "Deadweight" contiene una influencia mas funky. Como Beck dijo en el periódico "USA Today", "estoy tratando de llegar a un lugar en donde esta fusión de estilos sea tan fluida y natural que no se observe los distintos estilos, una conciencia musical donde no hay ideas preconcebidas". Una versión editada sin la coda de ruidos también se editó como sencillo.

Beck grabó la mayor parte de la canción por sí mismo, tocando el bajo, teclados, caja de ritmos y todas las guitarras; Aunque es muy probablemente que el scratching haya sido hecho por uno de los Dust Brothers, que estaban contribuyendo con Beck en aquel tiempo. Las restantes dos pistas, "Erase the Sun" y "SA-5", proporcionan enlaces directos a "Mutations" con fragmentos líricos que terminan casi palabra por palabra en futuras canciones.




</doc>
<doc id="22970" url="https://es.wikipedia.org/wiki?curid=22970" title="The Juniper Tree">
The Juniper Tree

The Juniper Tree es una película en blanco y negro realizada en 1990 y dirigida por Nietzchka Keene.

La película es protagonizada por la cantante y compositora islandesa Björk Guðmundsdóttir, Bryndis Petra Bragadóttir y Godrun S. Gisladóttir.

Si bien "The Juniper Tree" fue realizada y actuada por islandeses, la película se rodó en inglés para llegar a un público más amplio.

La trama de la misma es acerca de dos hermanas que han escapado de su casa en Islandia después de haber sobrevivido a la muerte de su madre, que había sido quemada en la hoguera acusada de practicar la brujería. La película se enfoca en la historia de las dos hermanas brujas: Katla y Margit –siendo Margit la bruja buena. En su camino, Katla hechiza a Jóhann, un viudo campesino que tiene un hijo, Jóhas. El hechizo hace que Jóhann se enamore de Katla para luego casarse, pero Jóhas ve las intenciones de Katla y trata de convencer a su padre para que la abandone. Por otro lado, la madre de Mirgit, que quiere ayudar a Jóhass, se le aparece a Margit en visiones y le da un amuleto para proteger al joven, y juntos luchan para deshacerse de Katla.
Véase también: 


</doc>
<doc id="22971" url="https://es.wikipedia.org/wiki?curid=22971" title="Becilla">
Becilla

Becilla puede referirse a:



</doc>
<doc id="22972" url="https://es.wikipedia.org/wiki?curid=22972" title="Becerril de la Sierra">
Becerril de la Sierra

Becerril de la Sierra es un municipio y localidad españoles de la provincia y Comunidad de Madrid.

La historia de Becerril de la Sierra está marcada por la ausencia de documentos, pues muchas de las fuentes primordiales para su estudio, como el Archivo Municipal, están desaparecidas. Por ello, la historia de Becerril que se conoce está enmarcada dentro del conjunto histórico del Real de Manzanares.

A pesar de la ausencia de datos exactos acerca de su fundación, los historiadores sitúan el surgimiento de Becerril de la Sierra en el periodo de la Reconquista de la península ibérica, durante los siglos XII y XIII. Más concretamente, el lugar fue fundado en el periodo reconquistador del Rey Don Alfonso.

La primera mención documental de Becerril de la Sierra se encuentra en 1385, fecha en la que se funda un Mayorazgo sobre el Real de Manzanares, en cuyo privilegio están citados todos los lugares, aldeas y villas ubicadas dentro de la jurisdicción del Real. Entre ellos está Becerril de la Sierra. Desde entonces pasó a estar bajo la protección, tanto judicial como militar, del Consejo del Excelentísimo Señor Duque del Infantado, ofreciendo a su Majestad el Rey Felipe IV la petición administrativa con titularidad de Villazgo hecha por el Concejo, Justicia y vecinos de Becerril. Finalmente, el lugar fue declarado como Villazgo en 1636 y confirmado en 1658 por el VII Duque del Infantado, Marqués de Santillana y Conde del Real de Manzanares, Don Rodrigo de Mendoza y Sandoval.

El primer censo de población encontrado en Becerril de la Sierra contaba con 39 varones, ocho de ellos eran solteros y dos viudos. Por aquel entonces, la población de Becerril se dedicaba, principalmente, a la agricultura, al pastoreo y a la cría de ganado. Otro censo posterior, el conocido censo de Castilla de 1594, refleja que Becerril se componía de 69 vecinos, cuya aproximación en habitantes ascendía a 276.

Tras el descubrimiento de América, llegó una época de fuerte emigración española y Becerril de la Sierra también sufrió sus consecuencias. A pesar de esto, hubo habitantes que se quedaron. Estas gentes dedicaban sus labores y oficios a las necesidades de la comunidad vecinal, destacando el campo de labranza a causa del duro clima. Asimismo, también se ocupaban varios meses del año de la conducción de piedra, carbón y leñas a la Corte y Villa de Madrid. Otro sector de población se ocupaba del pastoreo y del ganado, principalmente vacuno, lanar y cabrío. Finalmente, otro menor número de trabajadores se repartía entre los jornaleros, criados, canteros, fabriqueros de carbón, tejedores, mampostería, cirujano, maestro de primeras letras y un sacristán.

Una época crítica para Becerril de la Sierra, así como para otros muchos pueblos de la comarca, fue lo acontecido en el año 1841, cuando Becerril fue afectado por las leyes desamortizadoras y de expropiación. Por aquel entonces, la pobreza y el aislamiento se acentuaron en toda la región.

Por el contrario, en lo que se refiere a otra época de crisis, la Guerra Civil española de 1936, la localidad no fue de las que más sufrió sus nefastas consecuencias. Becerril vivió la contienda de otro modo, como lugar de acogida de vecinos de las poblaciones limítrofes, como Guadarrama y Los Molinos, dañadas y muy castigadas por los bombardeos y los efectos de la guerra.

Actualmente, Becerril de la Sierra es un municipio desarrollado, acogedor y de gran belleza natural y artística. Sus vecinos ya no sólo se dedican a actividades de ganadería y agricultura sino que muchos de ellos tienen como fondo de ingresos los establecimientos –restaurantes, hoteles, bares, tiendas...- surgidos del turismo. Precisamente, éste hace que en verano y fines de semana la población se multiplique y aumente hasta los 18 000 habitantes.

Limita por el oeste con Navacerrada y Collado Mediano, por el este con el Boalo y por el sur con Moralzarzal.

Está situado a 50 kilómetros de la capital de la Comunidad de Madrid.

Becerril de la Sierra ofrece ejemplos de patrimonio y cultura que merecen una visita. La arquitectura religiosa –con dos iglesias- y la civil –con la plaza de la Constitución y varias fuentes- conviven perfectamente y muestran parte de la tradición e historia de la localidad. 

Situado en la plaza de la Constitución, el Ayuntamiento es un edificio porticado, de tres alturas, coronado con techo de pizarra y un reloj. 

Está construida en piedra y presenta dos estrechos caños que vierten sus agua a un pilón con forma rectangular. La fuente está coronada con una bola pétrea. Se encuentra en la Plaza del Espejo.

Fue construida en 1955 en piedra. Presenta un pequeño pilón ovalado en cuyo centro se asienta un elemento vertical que divide en dos el pilón donde se recogen las aguas. La fuente está coronada por una bola pétrea. Se encuentra en la plaza Fuente de los Cielos.

Esta fuente circular con dos caños fue construida en granito en 1889 y reconstruida en 1982. Está situada en la avenida de Calvo Sotelo, cerca del Ayuntamiento. 

Fue construida a finales de la década de 1960 gracias a la iniciativa de un grupo de personas que pasaba parte de sus vacaciones en Becerril de la Sierra. La iglesia fue diseñada para que se integrara perfectamente en el entorno donde se ubica. De hecho, la forma de su estructura y los colores del hormigón simbolizan la montaña y los campos de Becerril. 

Tanto la planta como el alzado de la iglesia de Nuestra Señora del Valle se caracterizan por sus formas triangulares y por los muros inclinados. El interior del templo y su ornamentación se adaptan al estilo arquitectónico de la iglesia.

De estilo barroco rural y con influencias herrerianas, su construcción, en piedra, está fechada entre finales del siglo XVI y principios del XVII. La iglesia -que presenta una nave de planta rectangular con algunas irregularidades- destaca por su torre de tres cuerpos separados por impostas y por su pórtico sobre columnas toscanas. 

El atrio lateral es uno de los elementos más valiosos de la iglesia y su cubierta a una sola agua se asienta sobre seis columnas toscanas que, a su vez, reposan sobre un muro de baja altura. El atrio es también el lugar donde se encuentra el acceso al interior del templo, a través de un arco de medio punto construido con dovelas. Hay autores que creen que este atrio fue añadido a finales del siglo XVIII.

Al finalizar la Guerra Civil española, la iglesia de San Andrés Apóstol estaba bastante deteriorada por lo que la Junta Nacional de Reconstrucción de Templos Parroquiales se hizo cargo de su reconstrucción. Esta iglesia también vivió reformas a finales de los años 1960 y también en 1995.

En datos de INE de 2011 la población del municipio de Becerril alcanzaba los 5231 habitantes, de los cuales 2585 eran varones y 2646 mujeres. La mayor parte de ellos, 4655, residentes en la localidad del mismo nombre.

La evolución de la población en la primera década del siglo XXI ha sido significativa ya que en el año 2000 la población ascendía a 3246 habitantes.

Becerril de la Sierra dispone de una guardería (pública) y de un colegio público bilingüe llamado Juan Ramón Jiménez, en honor al genial poeta español. En él se imparte educación infantil, primaria y secundaria.



</doc>
<doc id="22974" url="https://es.wikipedia.org/wiki?curid=22974" title="Batrachomorpha">
Batrachomorpha

Los batracomorfos (Batrachomorpha) (literalmente, «forma de sapo») es un clado que incluye a los anfibios extintos y actuales que no están relacionados con los reptiles. Los límites del grupo han variado con los años, pero en un sentido cladístico reúne a todas las especies actuales, su ancestro común y sus parientes extintos.

 ,____________________________________ Nectridea +


</doc>
<doc id="22978" url="https://es.wikipedia.org/wiki?curid=22978" title="Zucaina">
Zucaina

Zucaina (en valenciano, "Sucaina") es un municipio de la provincia de Castellón, perteneciente a la Comunidad Valenciana, España. Situado en la comarca del Alto Mijares.
Desde Castellón de la Plana se accede a esta localidad a través de la CV-16 y en Alcora se toma la CV-190.

La población está asentada en lo alto de una hoya repleta de campos de avellanos, nogales, almendros y otros tipos de cultivos y bajo las faldas del macizo de Peñagolosa. El término municipal cuenta con numerosas masías que hasta hace bien poco estaban habitadas todo el año como Fuentes, El Plano Herrera, El Mas del Rebollo, Chirivilla... También existen parajes de singular belleza como son Santa Ana, El Chorrador de Zucaina, La Peña Villanueva, El Pinar, Los Picayos, El Corral del Pino, El Corral de Onofre, La Carrasca de los Tres Pies...

Al llegar el otoño en los bosques del término, siempre y cuando el final del verano y principios del otoño hayan sido generosos en lluvias, se suelen encontrar numerosas setas, siendo los níscalos las setas más abundantes y apreciadas, conocidas en Zucaina con el nombre de rebollones. 

Villahermosa del Río, Cortes de Arenoso, Arañuel, Ludiente y Castillo de Villamalefa, todas de la provincia de Castellón.

Población de origen árabe. Su nombre posiblemente proviene de la palabra "sukaina", el cual significa casita al lado de la fuente. La población está al lado de una fuente y de ahí posiblemente el nombre. Del pozo que alimenta esta fuente se extrae parte del agua que abastece a la población.

Hay que destacar que existen numerosos restos de poblados íberos en su término municipal, en los que se han encontrado numerosos restos de cerámica ibérica en la Escudilla y los Cabañiles de los siglos VI y V aC.

Su historia corre paralela a la de los moriscos, constituyendo un ejemplo de pueblo árabe de montaña en estas tierras, y sufriendo tras la expulsión de estos en el 1609 una despoblación de la que ha ido recuperándose poco a poco, en dura competencia con el despoblamiento que sufren las tierras del interior por la crisis agraria.

Economía basada por una parte en la agricultura, teniendo el avellano y más recientemente el nogal como cultivos principales y por otra parte en la cría de cerdos y gallinas, contando con numerosas granjas.

El concepto de deuda viva contempla solo las deudas con cajas y bancos relativas a créditos financieros, valores de renta fija y préstamos o créditos transferidos a terceros, excluyéndose, por tanto, la deuda comercial.

La deuda viva municipal por habitante en 2014 ascendía a 388,00 €.

El municipio, que tiene una superficie de 51,57 km², cuenta según el padrón municipal para del INE con habitantes y una densidad de  hab./km².















</doc>
<doc id="22980" url="https://es.wikipedia.org/wiki?curid=22980" title="Zazuar">
Zazuar

Zazuar es una localidad y un situados en la provincia de Burgos, comunidad autónoma de Castilla y León (España), comarca de La Ribera, partido judicial de Aranda, ayuntamiento del mismo nombre. 

El municipio de Zazuar se ubica en la Ribera del Duero, a 12 km al noreste de Aranda de Duero (Burgos), al sur de la provincia de Burgos, a 86 km de la capital, en la comunidad autónoma de Castilla y León (España). Las coordenadas del municipio son: latitud: 41º 41' 42" N, longitud: 3º 33' 18" O. Tiene una extensión de 22,5 km² y se encuentra a 839 metros sobre el nivel del mar según el Instituto Geográfico Nacional.

Por su término municipal pasa el río Arandilla, que acabará dejando sus aguas en el Duero.

El punto más alto del término municipal de Zazuar está en el "Alto de la Casa", con una altura de 916 msnm.

La climatología de la Ribera del Duero se caracteriza por una pluviometría moderada-baja (450 mm de lluvia al año) que, unida a sus veranos secos (35 - 40 °C) e inviernos largos y rigurosos (de hasta -15 °C), y con acusadas oscilaciones térmicas a lo largo de las estaciones, la enmarcan dentro de un clima mediterráneo, con más de 2.400 horas de sol, cuyo carácter primordial es la continentalidad.

En "Pueblos y apellidos de España: diccionario etimológico", de Julián Aydillo San Martín, se indica que Zazuar provendría de "Aza-or", significando "altura rocosa", o "en una colina", según Pascual Madoz.

En "Santo Domingo de Caleruega, en su contexto socio-político, 1170-1221", dedicado a Domingo de Guzmán, se menciona un documento fechado en Burgos, a 30 de septiembre de 1177, firmado por Alfonso VIII de Castilla y elaborado por dos clérigos, uno de ellos el arcipreste de "Sozuuar", identificándose el topónimo con el actual Zazuar.

En las Elecciones municipales de España de 1991 fue elegido alcalde Tomás Causín Aguilera del Partido Popular.
Obtuvo 138 votos de los 249 contabilizados.


Cuadrada, de 1:1 tricolor, en vertical 0,3 de rojo, 0,1 onda de azul y 0,6 de blanco. En el corazón de la bandera campeará el escudo municipal.

Escudo y Bandera definidos en BOCYL Nº 233 a 30 de noviembre de 2001.

Zazuar tiene un único núcleo de población, con un total de 260 habitantes.



En el término municipal de Zazuar se puede encontrar una amplia variedad de Flora:



La "Danza del Milano" es un baile típico del pueblo.

Gran variedad de productos entre los que cabe destacar los de origen porcino a partir de matanzas tradicionales así como el cordero (preparado asado) de gran calidad ya sea como lechazo asado en horno de leña o como chuletas a la parrilla. Para beber, vinos de la tierra, con Denominación de Origen Ribera del Duero.

La economía de Zazuar está principalmente basada en el cultivo de vid y remolacha así como en la industria existente en Aranda de Duero. El pueblo cuenta con una panadería, un autoservicio y dos bares que normalmente no sirven comidas. En cuanto a hostelería, Zazuar cuenta también con tres casas de turismo rural.

La Cooperativa San Andrés comercializa el vino «Vegazar» en distintas variedades: blanco, rosado, joven, roble y crianza. Recientemente ha empezado a comercializar una reserva bajo la denominación «Señorío de Zazuar». El «Vegazar» rosado ha ganado los siguientes premios:
El «Tinto Crianza 2005» ha obtenido el premio «Zarcillo de Oro» en la edición de 2009.

Carreteras


Ferrocarril más cercano

Aeropuertos cercanos

Sede de la Mancomunidad Río Arandilla, presidente, Alfonso Martínez Herrera.


</doc>
<doc id="23006" url="https://es.wikipedia.org/wiki?curid=23006" title="Aeropuerto Internacional de Narita">
Aeropuerto Internacional de Narita

El , también conocido anteriormente como Aeropuerto Internacional de Tokio-Narita o Aeropuerto Internacional de Nuevo Tokio, es uno de los dos aeropuertos internacionales que sirve al área del Gran Tokio en Japón. Se encuentra a unos 60 kilómetros (37 millas) al este del centro de Tokio, en la prefectura de Chiba, en la frontera entre la ciudad de Narita y la ciudad adyacente de Shibayama.

Narita es el aeropuerto internacional predominante en Japón, manejando alrededor del 50% del tráfico de pasajeros internacional del país y el 60% del tráfico internacional de carga aérea. En 2013, Narita fue el segundo aeropuerto más transitado de Japón (tras el aeropuerto de Haneda en Tokio), y fue el décimo mayor centro de carga aérea del mundo. Su pista de 4.000 metros (13.123 pies) es la pista que ostenta el récord de pista más larga en Japón junto a la segunda pista del Aeropuerto Internacional de Kansai en Osaka.

Actúa como centro de conexión para Japan Airlines, All Nippon Airways, Delta Airlines y United Airlines.

Los planes para la construcción de este aeropuerto se iniciaron en 1962 debido a la congestión del Aeropuerto Internacional de Haneda (en Tokio) y no estuvo exenta de conflictos con los agricultores cuyas tierras se expropiaron. El aeropuerto finalmente entró en funcionamiento en 1978 con el nombre de New Tokyo International Airport. El 1 de abril de 2004 se le puso el nombre actual (Narita International Airport).

En 1986, el aeropuerto empezó una segunda fase de construcción, que culminó con la inauguración de la segunda pista en abril de 2002, justo a tiempo para el Mundial de fútbol de Corea y Japón.

Narita es un aeropuerto de gran tráfico. Debido a ello, las autoridades japonesas han limitado el número de vuelos que cada compañía puede operar desde Narita, convirtiéndolo en un aeropuerto caro tanto para las compañías como para los usuarios. Recientemente, el Aeropuerto Internacional de Haneda en Tokio, fue autorizado a recibir nuevos vuelos internacionales provenientes de Asia para aligerar la carga de Narita.

En 2003, una Ley de la "Narita International Airport Corporation" pasó a disponer la privatización del aeropuerto. Como parte de este cambio, el 1 de abril de 2004, el aeropuerto internacional de New Tokyo pasó a llamarse oficialmente aeropuerto internacional de Narita, lo que refleja su denominación popular desde su apertura. El aeropuerto pasó del control del gobierno a la autoridad de una nueva empresa denominada Narita International Airport Corporation.

El aeropuerto de Narita fue uno de los primeros aeropuertos del mundo en alinear sus terminales en torno a las tres grandes alianzas de aerolíneas internacionales. Desde el año 2006, el aeropuerto ha organizado el aeropuerto para que SkyTeam pueda utilizar el Ala Norte de la Terminal 1, las aerolíneas de Star Alliance el Ala Sur de la Terminal 1, y las de Oneworld la Terminal 2.
La terminal 1 utiliza un diseño de terminal de satélite dividido en un Ala Norte (北ウイング "kita-uingu"), Edificio Central ("chūō-biru") y un Ala Sur. Dos satélites circulares, Satélite 1 (puertas 11-18) y Satélite 2 (puertas 21-24), están conectadas al ala norte. Satélites 3 y 4 (puertas 26-38 y 41-47) puertas componen una explanada lineal conectada al edificio central. La terminal Satélite 5 (puertas 51-58) está conectada al ala sur.

El check-in se procesa en el cuarto piso, y la zona de salidas y el control de inmigración están en el tercer piso. Los pasajeros llegan por el puesto de inmigración del segundo piso, recogen su equipaje y pasan la aduana en el primer piso. La mayoría de tiendas y restaurantes se encuentran en el cuarto piso del Edificio Central. El Ala Sur incluye un centro comercial libre de impuestos llamado "Narita Nakamise", el mayor centro comercial de boutique de aeropuerto de la marca en régimen de franquicia en Japón.

Japan Airlines utilizaba Terminal 1 antes de unirse a Oneworld.

El Ala Norte ha servido como un centro para la alianza SkyTeam desde 2007, cuando Delta Air Lines y Aeroflot se unieron a Air France, KLM, Korean Air, Aeroméxico y a otras aerolíneas de SkyTeam que ya operaban allí. A partir de 2015, Aircalin es la única aerolínea no perteneciente a SkyTeam que opera desde el ala norte.

El Ala Sur y la Satélite 5 abrieron en junio de 2006 como una terminal para las aerolíneas de Star Alliance. La construcción del Ala Sur llevó casi una década y más del doble del área de la Terminal 1, con un total de 440.000 metros cuadrados. En la actualidad, todos los miembros de Star Alliance utilizan esta ala, junto con los no miembros Air Busan, MIAT, Uzbekistan Airways y Etihad Airways.
La terminal 2 se divide en un edificio principal (h"onkan") y el satélite, los cuales están diseñados alrededor de explanadas lineales. Los dos estaban conectados por un sistema de traslado ("Narita Airport Terminal 2 Shuttle System"), que fue diseñado por Otis y fue el primer sistema de traslado por cable de Japón. Una nueva pasarela entre los principales y la satélite se construyó, empezando a funcionar el 27 de septiembre de 2013, y el sistema de transporte se suspendió.

El registro de entrada, salidas y control de la inmigración para los pasajeros que llegan es en el segundo piso, mientras que la recogida de equipaje y las aduanas están en el primer piso.

Para los vuelos nacionales, hay tres puertas (65, 66 y 67) en el edificio principal que conectan tanto a las principales salidas como a una instalación de check-in doméstica separada. Los pasajeros de conexión entre vuelos nacionales e internacionales deben salir del área a la zona de facturación, y luego hacer el "check in" para su vuelo de conexión.

Japan Airlines es actualmente el principal operador en la T2. La terminal ha servido como centro de la alianza Oneworld en Narita desde 2010, algunas aerolíneas que operan allí son British Airways, Cathay Pacific, American Airlines y la más reciente incorporación de Iberia, que desde octubre de 2016 opera vuelos directos a Madrid. Otras aerolíneas también comenzaron a utilizar la terminal, incluyendo las aerolíneas de SkyTeam China Airlines y China Eastern Airlines, así como Air India de Star Alliance, y otras no afiliadas como Air Macau, Air Niugini, Asia Atlantic Airlines, Eastar Jet, Emirates, Pakistan International Airlines, Philippine Airlines y Scoot.

Vanilla Air, una aerolínea de bajo coste, tiene su sede dentro de la Terminal 2, pero opera desde la Terminal 3.

All Nippon Airways y otras aerolíneas de Star Alliance utilizaban la Terminal 2 antes de la apertura del Ala Sur de la Terminal 1 en 2006.
Una tercera terminal de aerolíneas de bajo coste se inauguró el 8 de abril de 2015. Situada a 500 m al norte de la Terminal 2, la nueva terminal incorpora una serie de medidas de reducción de costes, incluyendo el uso de las etiquetas en lugar de señales direccionales luminosas y el uso de puertas y escaleras al aire libre ("airstairs") en lugar de "fingers", que están destinadas a reducir los costes de las instalaciones para las aerolíneas y sus pasajeros en un 40% en vuelos internacionales y en un 15% en vuelos nacionales. Jetstar Japan, Vanilla Air y otras tres aerolíneas de bajo coste utilizan la terminal. La terminal también incluye una zona de comidas abierta 24 horas que es el mayor área de comidas en un aeropuerto en Japón, y una sala de oración islámica. Fue construido a un costo de 15 mil millones de yenes y cubre 66.000 metros cuadrados de superficie.

Nippon Cargo Airlines (NCA) tiene su sede en los terrenos del aeropuerto de Narita, en el "NCA Line Maintenance Hangar."

Japan Airlines opera también en el aeropuerto como centro de carga, en el centro de operaciones "Japan Airlines Narita Operation Center". La aerolínea subsidiaria JALways también tuvo su sede en el edificio. All Nippon Airways también tiene un edificio "Sky Center" de operaciones adyacente a la Terminal 1, que sirve como la sede de "ANA Air Service Tokyo", un proveedor de servicios de tierra que es una empresa conjunta entre ANA y la autoridad aeroportuaria.

El aeropuerto está conectado por una tubería de 47 kilometros al puerto de la ciudad de Chiba y a un terminal de combustibles en Yotsukaido. El oleoducto se inauguró en 1983 y se habían bombeado 130 mil millones de litros de combustible al aeropuerto de Narita para su trigésimo aniversario de operaciones en 2013.

Fuente: "Narita International Airport Corporation(NAA)", Situación operativa del aeropuerto

El Narita Express es un servicio de ferrocarril de Japan Railways que conecta el aeropuerto con la ciudad de Tokio, en un trayecto que tarda 53 minutos en llegar a la estación de Tokio, 80 a la de Shinjuku y 90 a Omiya y Yokohama. Es el método más rápido, pero también el más caro. Cada billete cuesta entre 3.000 y 4.500 yenes en clase turista. 

Japan Railway ofrece también un servicio rápido hasta la estación de Tokio que tarda 90 minutos pero con un coste menor.

También existen líneas de la compañía privada Keisei que tienen como destino la estaciones de Nippori y Ueno, tanto en el servicio expreso (los llamados trenes Skyliner que no realizan paradas intermedias) como los expresos limitados que tienen frecuentes paradas a lo largo del recorrido. Estos últimos suelen ser la alternativa más económica para llegar a la capital (unos 1.000 yenes).

Diversas compañías ofrecen también servicio de autobuses, llamados Limousine Bus, pero éstos son mucho más lentos que las diversas líneas de ferrocarril, debido al siempre colapsado tráfico de Tokyo.

También hay servicio de autobús nocturno a Kioto y Osaka. Los autobuses también viajan a las bases militares de Estados Unidos, incluyendo la Base Naval de Yokosuka y la Base Aérea de Yokota.

Existe también un servicio de taxi, pero su precio es prohibitivo. Las carreras cuestan entre 14.000 y 20.000 yenes (con un extra de 1.450 yenes para los peajes de las autopistas).

"Mori Building City Air Service" ofrece un servicio de chárter en helicóptero entre Narita y el complejo Ark Hills en Roppongi, tardando 35 minutos y con coste de 280.000 yenes por trayecto para hasta cinco pasajeros.

Imágenes de distintas aeronaves en su paso por el aeropuerto:




</doc>

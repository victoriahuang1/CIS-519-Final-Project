<doc id="6536" url="https://es.wikipedia.org/wiki?curid=6536" title="Sistema de Información Contable de la Seguridad Social de España">
Sistema de Información Contable de la Seguridad Social de España

La carencia de una adecuada informatización de la contabilidad se hacía notar en todos los estamentos de la seguridad social española desde mediados de la década de 1980, en la medida en la que la importancia relativa de la Seguridad Social respecto del conjunto de la economía nacional había ido creciendo sistemáticamente desde finales de la década de 1970.

Por ello, resultaba imprescindible establecer un sistema contable que, como el SICOSS (Sistema de Información Contable de la Seguridad Social), respetara las peculiaridades que presentan todas y cada una de estas entidades gestoras, permitiese un tratamiento homogéneo de sus operaciones y la consolidación de sus datos para poder ofrecer una imagen correcta de su actuación como grupo.

La informatización de la contabilidad de la Seguridad Social siguió varias etapas: 


</doc>
<doc id="6538" url="https://es.wikipedia.org/wiki?curid=6538" title="Continente">
Continente

Se considera como continente a una gran extensión de tierra que se diferencia de otras menores o sumergidas por conceptos geográficos, como son los océanos, y culturales, como la etnografía.

Según la definición o modelo de continente, se reconocen entre cuatro y siete continentes: Asia, Antártida, Europa, África, Oceanía (o el continente australiano) y América.

La palabra continente viene del latín "continente", que significa «mantener juntos» y deriva del "continentes tierra", «las tierras continuas». Literalmente, el término se refiere a una gran extensión de tierra firme en la superficie del globo terrestre. Sin embargo, esta definición estrictamente geográfica es frecuentemente modificada de acuerdo a criterios históricos y culturales. Así, hay algunos modelos de continentes que consideran Europa y Asia como dos continentes, mientras que Eurasia no es más que una extensión de tierra, y otros lo hacen a la inversa.

La definición poco clara de continente ha dado lugar a la existencia de varios modelos y actualmente se reconocen entre cuatro y siete continentes. Pero esto no ha sido siempre así y esos modelos han variado a lo largo de la historia y el descubrimiento de nuevos territorios.

En su acepción común, la zona continental incluye también las islas pequeñas situadas a corta distancia de la costa, pero no las que están separadas por brazos de mar importantes. Desde una perspectiva científica, el continente también incluye las islas vinculadas a las placas continentales, del modo en que las islas británicas pertenecen a Eurasia.

En realidad no existe una única forma de fijar el número de continentes y depende de cada área cultural determinar si dos grandes masas de tierra unidas forman uno o dos continentes, y en concreto, decidir los límites entre Europa y Asia (Eurasia) por una parte y América del Norte y América del Sur (América o "las Américas") por otra. Los principales modelos son los siguientes:






Los continentes de los distintos modelos son los siguientes:


La primera distinción semejante al concepto de continente fue hecha por los antiguos navegantes griegos, que dieron el nombre de Europa y Asia a las tierras localizadas a ambos lados de la extensión de agua que formaban el mar Egeo, el estrecho de los Dardanelos, el mar de Mármara, el estrecho del Bósforo y el mar Negro. Esos nombres se emplearon en principio para designar a las tierras cercanas a la costa, y sólo después se extendió su uso al interior del país. Los pensadores de la Grecia antigua debatieron a continuación si África (entonces llamada Libia) debería ser considerada como una parte de Asia o como una tercera parte independiente del mundo y finalmente se impuso la división de la Tierra en tres partes. Desde la perspectiva griega, el mar Egeo era el centro del mundo, con Asia al este, Europa al oeste y al norte, y África al sur.

Los límites entre los continentes no quedaron fijados. Desde el principio, la frontera de Europa con Asia fue tomada partiendo del mar Negro a lo largo del río Rioni (conocido entonces como Phasis) en Georgia. Más tarde, se consideró que la frontera iba desde el mar Negro por el estrecho de Kerch, el mar de Azov y el río Don (llamado entonces Tanais), en Rusia.

El río Nilo se ha considerado generalmente desde antiguo la frontera entre Asia y África. Heródoto, en el siglo V antes de Cristo, sin embargo, ya se oponía a esta situación, que emplazaba Egipto a caballo entre dos continentes, y hacía coincidir el límite entre Asia y África con la frontera occidental de Egipto, dejando Egipto en Asia. También cuestionó la división en tres de lo que era en realidad una sola tierra continua, un debate que continúa casi dos milenios y medio más tarde.

Eratóstenes, en el siglo III antes de Cristo, señaló que algunos geógrafos dividían los continentes siguiendo los ríos (el Nilo y el Don), considerándolos por tanto como «islas», mientras que otros los dividían por istmos, siendo así que entonces serían «penínsulas». Estos geógrafos fijaban la frontera entre Europa y Asia en el istmo entre el mar Negro y el mar Caspio y la frontera entre Asia y África, en el istmo entre el mar Rojo y la desembocadura del lago Bardawil en el mar Mediterráneo.

En la época romana y en la Edad Media, algunos escritores tomaron el istmo de Suez como el límite entre Asia y África, pero la mayoría de los autores siguieron considerando que el río Nilo, o la frontera occidental de Egipto, eran el límite entre continentes. En la Edad Media, el mundo se representaba en un mapa de T en O, con la T aludiendo a las aguas de la división de los tres continentes.

Cristóbal Colón atravesó el océano Atlántico hasta llegar a las Antillas el 12 de octubre del 1492, allanando el camino para la exploración y colonización europea de América. A pesar de sus cuatro viajes hacia el oeste, Colón nunca llegó a saber que había arribado a un continente nuevo y pensaba que había alcanzado las costas de Asia. En 1501 Américo Vespucio viajó a América como piloto de una expedición que navegó a lo largo de la costa de Brasil. Los miembros de la expedición recorrieron un largo camino en dirección sur a lo largo de la costa de América del Sur, lo que confirmó que la tierra que bordeaban tenía proporciones continentales. De regreso a Europa, Vespucio publicó un relato de su viaje titulado "Mundus Novus" en 1502 o 1503, pero parece que ha habido adiciones o modificaciones por otro escritor. Cualquiera que sea el autor de esas palabras que se pueden leer en "Mundus Novus", «He descubierto un continente en aquellas regiones meridionales que está habitado por más numerosos pueblos y animales que nuestra Europa, o Asia o África» es la primera identificación explícita de América, como un continente distinto de los otros tres.

Desde que Vespucio anunciara el hallazgo del nuevo continente, éste había recibido varios nombres, cuya aplicación y aceptación era generalmente regional. Así, los castellanos lo llamaban «Indias» o «La gran Tierra del Sur»; los portugueses, «Vera Cruz» o «Tierra de Santa Cruz». Algunos cartógrafos empleaban «Tierra del Brasil» (que sin embargo aludía a una isla imaginaria), «Tierra de Loros», «Nueva India», o simplemente «Nuevo Mundo». Después de unos años, el nombre de "Nuevo Mundo" comienza a aparecer como un nombre para América del Sur en los mapas y cartas, como en la Oliveriana (Pesaro) que data de poco después de 1503. Los mapas de la época, como los planisferios de Cantino y Caverio, muestran claramente América del Norte conectada con Asia y América del Sur como una tierra inexplorada, de la que sólo se dibuja la costa norte y este de Brasil, o bien, como en el "Planisferio de Sylvanus" en forma de una gran isla independiente.

En 1507, Martin Waldseemüller publicó un mapa del mundo, "Cosmographia Universalis", que fue el primero en mostrar que América del Norte y América del Sur estaban separadas de Asia y rodeadas de agua. Una pequeña nota sobre la carta principal, dice, por primera vez, que América se encuentra al este de Asia y están separadas por un océano, nota colocada para evitar confusión ya que América se encuentra en el extremo izquierdo del mapa y Asia en el extremo derecho. En el libro de acompañamiento, "Cosmographiae Introductio", Waldseemüller anotó que la tierra se dividía en cuatro partes, Europa, Asia, África y la cuarta parte que formó a partir del nombre de Américo Vespucio. En el mapa, la palabra «América» se colocó en una parte de América del Sur.

La voz tenía tal eufonía y guardaba tanta consonancia con las palabras «Asia» y «África» que inmediatamente se afincó en las lenguas noreuropeas. Sin embargo tardó en ser adoptado en la península ibérica y sus colonias, donde el nombre mayoritariamente usado siguió siendo por bastante tiempo el de «Indias occidentales».

Desde finales del siglo XVIII algunos geógrafos comenzaron a considerar que América del Norte y América del Sur eran dos partes del mundo, lo que hacía un total de cinco partes. Sin embargo, la división en cuatro fue en general la prevalente en el siglo XIX. Los europeos descubrieron Australia en 1606, pero durante algún tiempo, fue visto como una parte de Asia. A finales del siglo XVIII algunos geógrafos lo consideraron un continente en sí mismo, por lo que paso a ser el quinto (o sexto para aquellos quienes creen que América son dos continentes). En 1813, Samuel Butler (1774-1839) escribió en relación a Australia que «New Holland, an immense island, which some geographers dignify with the appellation of another continent» [Nueva Holanda, una isla inmensa, que algunos geógrafos dignifican con la apelación de otro continente] y el Oxford English Dictionary es igualmente equívoco algunas décadas más tarde.

La Antártida fue oficialmente descubierta hacia 1820 (aunque ahora se sabe que ya fue conocida desde 1599 o 1603) y descrita ya en 1838 como un continente por Charles Wilkes (1798-1877) de la United States Exploring Expedition (expedición Wilkes) (1838-42). Fue el último continente en ser identificado, aunque la existencia de un gran territorio Antártico había sido considerada desde hacia milenios. En la Antigüedad los pensadores griegos dedujeron que si la Tierra era esférica, por simetría, debía tener una contrapartida continental a la masa continental del hemisferio norte en el hemisferio sur en las latitudes polares, de este modo el cosmógrafo Claudio Ptolomeo confeccionó un célebre planisferio en el cual aparecía un inmenso territorio que en latín fue llamado "Terra Australis Incognita" [Tierra Austral Desconocida], aunque la extensión del supuesto continente incluía zonas que corresponden no solo a la Antártida propiamente dicha sino también a Australia, Nueva Zelanda y grandes extensiones oceánicas. En 1520 Magallanes al descubrir el estrecho que lleva hoy su nombre, estrecho de Magallanes, creyó que la isla de Tierra del Fuego era un sector de esa "Terra Australis Incognita". La exploración de Francisco de Hoces descubrió el gran pasaje marítimo que separa América de la Antártida y posteriormente el nombre de "Terra Australis Incognita" quedó reservado para Australia. En 1849, un atlas informó ya que la Antártida era un continente, pero pocos más lo hicieron hasta después de la Segunda Guerra Mundial.

En el siglo XIX, el mayanista Augustus Le Plongeon (1825-1908) propuso la hipótesis de un nuevo continente, llamado Mu, que habría existido y desaparecido en el océano Pacífico. Se basaba en la traducción —más tarde contestada— del "Códice Tro-Cortesiano" realizada por Brasseur de Bourbourg. En un momento en que la geología estaba menos avanzada que ahora, se propuso la existencia de muchos continentes hipotéticos. Este fue el caso de Lemuria, un continente considerado para explicar la desaparición de algunas especies de mamíferos, y también la Atlántida, mencionada por primera vez en el siglo IV antes de Cristo, aunque este hipotético continente hundido sigue siendo aún hoy fruto de especulaciones.

Desde mediados del siglo XIX los atlas de los Estados Unidos tratan a menudo América del Norte y América del Sur como dos continentes, lo que es coherente con el entendimiento de la geología y la tectónica de placas. Pero no es raro que los atlas americanos los traten como un único continente, por lo menos hasta la Segunda Guerra Mundial. Es esta última visión la que todavía prevalece hoy en día en algunos países de Europa.
La bandera olímpica, diseñada en 1913, tiene cinco anillos que representan los cinco continentes habitados, considerando América como uno solo y sin considerar a la Antártida.
Sin embargo, en los últimos años ha habido un impulso para que Europa y Asia, que tradicionalmente se consideraron como dos continentes, sean ahora un único continente, llamado Eurasia, para hacerlo compatible con los conocimientos derivados de la geología y la tectónica de placas. En este modelo, el mundo estaría dividido en seis continentes.

No existe una definición única de continente y por ello en distintos ámbitos culturales y científicos hay listas diferentes de continentes. En general, un continente debe ser una gran área conformada por tierras con importantes límites geológicos. El criterio de origen para la designación de un continente, el criterio geográfico, es ignorado muchas veces en favor de otros criterios más arbitrarios, de índole histórica y cultural. Aunque algunos consideran que solo hay cuatro o cinco continentes, modernamente se suele considerar que hay seis o siete.

Convencionalmente, se entiende que los «continentes son masas de tierra grandes, continuas y discretas, a ser posible separadas por extensiones de agua». Muchos de los siete continentes comúnmente reconocidos por convención no son masas de tierra discretas separadas por agua (realmente sólo la Antártida y Oceanía están verdaderamente separados de los otros continentes por agua). Del mismo modo, el criterio ideal que cada uno sea una masa de tierra continua con frecuencia es ignorado por la inclusión de la plataforma continental y las islas oceánicas. Las principales masas de la Tierra están bañadas por un único y continuo océano mundial, que está dividido en los principales componentes oceánicos por los continentes y otros criterios geográficos diferentes.

El sentido más estricto de continente como un área de tierra continua, con su costa y cualquier frontera terrestre que forme el borde del continente. En este sentido, la referencia a «Europa continental» se utiliza para referirse a la parte continental de Europa con exclusión de las islas como Gran Bretaña, Irlanda e Islandia, y el «continente australiano» puede referirse a la parte continental de Australia, con excepción de Tasmania. Del mismo modo, «Estados Unidos continental» se refiere a los 48 estados contiguos de Estados Unidos del centro de América del Norte y puede incluir a veces Alaska, en el noroeste del continente (separadas ambos por Canadá), pero excluyendo Hawái localizada en medio del océano Pacífico.

Desde el punto de vista de la geología o la geografía física, un continente puede extenderse más allá de los confines de la tierra firme continua a fin de incluir las aguas poco profundas, las áreas adyacentes sumergidas (la plataforma continental) y las islas de la plataforma (islas continentales), ya que son parte estructural del continente. Desde esta perspectiva el borde de la plataforma continental es el verdadero límite del continente, ya que las costas varían con los cambios del nivel del mar. En este sentido, Madagascar es parte de África, las islas de Gran Bretaña e Irlanda son parte de Europa, y Australia y la isla de Nueva Guinea, juntos forman un continente (Australia-Nueva Guinea).

Como construcción cultural, el concepto de continente puede ir más allá de la plataforma continental a fin de incluir las islas oceánicas y otros fragmentos continentales. De esta manera, Islandia se considera parte de Europa. Extrapolando el concepto al extremo, algunos geógrafos consideran Australia, Nueva Zelanda y todas las islas de Oceanía (algunas veces Australasia) como equivalente a un continente, consiguiendo así que toda la superficie de la Tierra se divida en continentes o cuasi-continentes.

Dado que la definición de continente es a menudo arbitraria, las separaciones entre ellos no siempre están claramente definidas.

El estrecho de Gibraltar de forma convencional marca la frontera entre África y Europa.

El límite entre Asia y África es en general fijado en el istmo de Suez, lo que excluye la península del Sinaí de África. Egipto se encuentra a caballo entre dos continentes, aunque algunos geógrafos proponen desplazar el límite entre ambos continentes a la frontera israelo-egipcia.

Por comodidad, la frontera entre Asia y América se ha fijado en la frontera ruso-estadounidense, a lo largo del estrecho de Bering. Las islas del Comandante son asiáticas, mientras que el resto de las islas Aleutianas son americanas.
La separación entre Oceanía y Asia es todavía hoy objeto de controversias. En 1831, el geógrafo y explorador Jules Dumont d'Urville a través de criterios geográficos dividió Oceanía en cuatro regiones: Polinesia, Micronesia, Melanesia e Insulindia (entonces llamada Malaya), de esta forma el límite entre Asia y Oceanía transcurre a través de los estrechos de Malaca y Luzón. La debilidad de los argumentos que sustentan este límite llevó a algunos geógrafos a repensar esa frontera. En 1860 el geógrafo Alfred Wallace propuso un nuevo límite basado en criterios biogeográficos que transcurre entre los estrechos de Macasar y Lombok conocido como la línea de Wallace. Algunos piensan que sería más apropiado utilizar la línea de Wallace como el verdadero límite entre los dos continentes. Posteriormente otros investigadores han propuestos nuevos límites como es el caso de Richard Lydekker en 1895 y Max Weber en 1907.

La frontera más controvertida es, sin duda, entre Asia y Europa, ya que las fronteras de Europa han ido desplazándose durante siglos y no están claramente definidas. En el siglo XVIII, el zar Pedro el Grande quiso convertir a Rusia en una potencia europea. El geógrafo Vassili Tatichtchev propuso en 1703 que los montes Urales, el río Ural y el Cáucaso constituyesen la frontera entre Europa y Asia en lugar del río Don, que situaba en esa época a Rusia en Asia.

Con la reciente ampliación de la Unión Europea hasta territorios situados en las puertas de Asia, tanto en los Balcanes como en la antigua Europa del Este, se plantea de nuevo el problema de la ubicación exacta de la frontera entre Europa y Asia. Algunos geógrafos, por conveniencia, extenderían el límite más allá del Cáucaso a fin de incluir en Europa a países como Armenia y Georgia. Otros, en cambio, fijarían el límite en la depresión de Kuma-Manych situada al norte del Cáucaso para incluir a los pueblos turcos del Cáucaso en Asia.

Las distintas partes del mundo se consideran como continentes en un sentido amplio. Por un lado, toda isla debe pertenecer a alguna parte del mundo ya que el conjunto de todas las partes del mundo debe contener todas las tierras emergidas, pero las islas no son parte de los continentes (en el sentido común o sentido científico) ya que su territorio no es continuo con el resto del continente. Sin embargo, en general se consideran como pertenecientes al continente del que estén más próximas. Por ejemplo, las islas Canarias, aunque españolas, están vinculadas a África; las islas Baleares forman parte de Europa y casi todas las islas del Pacífico pertenecen a Oceanía. Igual pasa con la isla de Reunión e isla Mauricio, que, a pesar de la distancia que las separa de África, son consideradas como islas africanas.

Algunas partes de los continentes son reconocidas como subcontinentes, en especial aquellas situadas en las diferentes placas tectónicas que dividen los continentes. Los más notables son el subcontinente indio, la península arábiga y Europa tomando el modelo que considera como continente a Eurasia. Groenlandia, sobre la placa norteamericana, también se considera como un subcontinente. Cuando América se considera un solo continente, a su vez se entiende dividida en dos subcontinentes (América del Norte y América del Sur), o en las diversas regiones.

Algunas áreas de la corteza continental están en gran parte cubiertas por el mar, pero se pueden considerar continentes sumergidos. Este es el caso de Zealandia, emergiendo del mar en Nueva Zelanda y Nueva Caledonia, o incluso la casi completamente sumergida meseta Kerguelen, en el sur del océano Índico.

Algunas islas se encuentran en las secciones de la corteza continental que se han fracturado y se alejan a la deriva de uno de los grandes continentes. Aunque no son considerados como continentes debido a su tamaño relativamente pequeño, pueden ser considerados microcontinentes. Madagascar, el ejemplo más extendido, es generalmente considerado parte de África, pero ya ha sido descrito como "el octavo continente".

Los geólogos utilizan el término «continente» de manera distinta a los geógrafos. Más que simplemente identificar grandes masas de tierra sólida, los geólogos usan un criterio distinto para identificarlos. Para los geólogos, en la superficie de la Tierra existen, de forma simplificada, dos elementos estructurales distintos: la corteza continental, compuesta principalmente por granito y rocas asociadas, y la corteza oceánica, formada por basalto y gabro. Además, el límite entre el dominio continental y el dominio oceánico se encuentra por debajo de la superficie del mar: es entonces cuestión de la «plataforma continental» que a veces se extiende varios kilómetros más allá de la línea de costa. Durante la última edad de hielo (en la época de la glaciación de Würm, hace unos años), en Europa Occidental, la plataforma continental se extendía a varias decenas de kilómetros al oeste de la costa actual.

A comienzos del siglo XX, Alfred Wegener se dio cuenta de que por la disposición de los continentes, la costa este de América del Sur parecía encajar exactamente en la costa occidental de África. Otros antes que él ya lo habían advertido, pero fue el primero en sugerir a continuación, a partir de esta observación, la teoría de la deriva continental: un supercontinente, llamado Pangea, se habría fragmentado a principios de la era secundaria y, desde entonces, las masas continentales de esta fragmentación derivarían sobre la superficie de la Tierra.

Durante el siglo XX los geólogos aceptaron que los continentes se movían sobre la superficie de la Tierra, en una escala del tiempo geológica. Este proceso se conoce como «deriva de los continentes» y se explica por la tectónica de placas. La superficie de la Tierra está formado por siete placas tectónicas mayores (y muchos más de menor importancia). Estas son las que derivan, separadas y reunidas para formar con el tiempo los continentes que conocemos hoy en día.

En consecuencia, existieron otros continentes en el pasado geológico, los paleocontinentes. Se ha podido determinar que ha habido períodos en la historia de la Tierra en que únicamente había un gran continente en su superficie. El más reciente fue Pangea, hace 180 millones años. El próximo «continente único» deberá de aparecer en unos 250 millones de años, resultado de la combinación de África, Eurasia y América, que sería la Pangea Última.

Los geólogos consideran que un "continente" se define por la corteza continental: una plataforma de rocas metamórficas y rocas ígneas, en gran medida de composición granítica. Algunos geólogos restringen el término continente a las porciones de la corteza construidas en torno al estable Escudo Precámbrico, por lo general desde 1500 hasta 3800 millones años de edad, llamado cratón. El cratón en sí mismo es un complejo de acreción de los antiguos cinturones móviles (cinturones de montaña) de los ciclos anteriores de subducción, colisión continental y ruptura de las placas tectónicas. Un engrosamiento saliente hacia el exterior de rocas sedimentarias más jóvenes y mínimamente deformadas cubre gran parte del cratón. Los márgenes de los continentes geológicos se caracterizan por la actividad actual o relativamente reciente de los cinturones móviles y por profundos sedimentos marinos o deltaicos. Más allá del margen, puede haber o bien una plataforma continental que cae sobre la cuenca oceánica basáltica o el margen de otro continente, dependiendo de la actual placa tectónica del continente. Una frontera continental no tiene porque ser un cuerpo de agua. En el tiempo geológico, los continentes quedan periódicamente sumergidos bajo grandes mares epicontinentales, y las colisiones continentales dan como resultado en un continente nuevo conectado a otro continente. La era geológica actual es relativamente anómala ya que gran parte de las zonas continentales son "altas y secas" (high and dry) en comparación con gran parte de la historia geológica.

Algunos sostienen que los continentes son "balsas" de corteza acrecionales, que, a diferencia de la corteza basáltica más densa de las cuencas oceánicas, no están sujetas a la destrucción a través del proceso de subducción de la placa tectónica. Esto explicaría la gran antigüedad de las rocas comprendidas en los cratones continentales. Según esta definición, Europa Oriental, India y otras regiones podrían ser considerados como masas continentales distintas del resto de Eurasia, porque tienen áreas de escudo antiguas separadas (es decir, cratón de Europa del Este y cratón de la India). Cinturones móviles más jóvenes (como los montes Urales y los Himalayas) marcan los límites entre estas regiones y el resto de Eurasia.

Hay muchos microcontinentes que se han construido de la corteza continental, pero que no contienen un cratón. Algunos de ellos son fragmentos de Gondwana y otros antiguos continentes cratónicos: Zealandia, que incluye a Nueva Zelanda y Nueva Caledonia; Madagascar; la norte meseta de Mascareñas, que incluye las islas Seychelles; etc. Otras islas, como varias del mar Caribe, se componen principalmente de roca granítica, pero todos los continentes tienen la corteza tanto de granito como basalto, y no hay un límite claro de que islas podrían ser consideradas microcontinentes según esa definición. La meseta Kerguelen, por ejemplo, es en gran medida volcánica, pero se asocia con la ruptura de Gondwana y se considera un microcontinente, mientras que las volcánicas Islandia y Hawái no lo son. Las islas Británicas, Sri Lanka, Borneo y Terranova son los márgenes del continente laurasiano que sólo están separadas por mares interiores que han inundado sus márgenes.

La tectónica de placas ofrece otra forma de definir los continentes. Hoy en día, Europa y la mayor parte de Asia comprenden la unificada placa Euroasiática, que es aproximadamente coincidente con el continente euroasiático geográfico, con exclusión de la India, Arabia, Rusia y Extremo Oriente. India contiene un escudo central, y el geológicamente reciente cinturón móvil de los Himalayas forma su margen norte. América del Norte y América del Sur son continentes distintos y el istmo de conexión es en gran medida el resultado de la actividad volcánica de la relativamente reciente subducción tectónica. Las rocas continentales de América del Norte se extienden hasta Groenlandia (una parte del Escudo Canadiense), y en términos de límites de placas, la placa Norteamericana incluye la parte más oriental de la masa continental de Asia. Los geólogos no utilizan estos datos para sugerir que el Asia oriental sea parte del continente de América del Norte (aunque el límite de la placa se extiende hasta allí) y utilizan también la palabra continente generalmente en su sentido geográfico y en las nuevas definiciones («rocas continentales», «límites de placas») se utilizan según el caso.

Lista de países por continente según los países hispanohablantes:






</doc>
<doc id="6540" url="https://es.wikipedia.org/wiki?curid=6540" title="Contabilidad presupuestaria">
Contabilidad presupuestaria

La contabilidad pública en España es el sistema contable que utiliza el el sector público español que se haya sujeto la Ley General Presupuestaria (LGP) española.

El presupuesto público español presenta los gastos e ingresos de los poderes públicos durante el año. Un gasto es toda transacción que implica una aplicación financiera (uso de fondos) y un recurso (también denominado ingreso) es toda operación que implica la utilización de un medio de financiamiento (fuente de fondos).


Pueden expresarse de distintas maneras. Para cumplir con los objetivos de Claridad y Programación que tiene el presupuesto, se elaboran las clasificaciones presupuestarias. Se trata de instrumentos normativos los recursos y gastos de acuerdo a ciertos criterios, cuya estructuración se basa en el establecimiento de aspectos comunes y diferenciados de las operaciones administrativas. Las clasificaciones presupuestarias facilitan la toma de decisiones por parte de las autoridades en todas las etapas del proceso presupuestario.

El grado de agregación de las cuentas públicas se corresponde con los niveles de gestión: el funcionario que administra determinado programa gubernamental precisará trabajar con una mayor desagregación que el Ministro. Siguiendo este criterio, existen dos tipos de clasificaciones:


Los estados de gastos de los presupuestos públicos en España aplican las clasificaciones orgánica, funcional agregada en programas y económica.

La estructura de programas diferencia los programas de carácter finalista (con las letras A a L) y los programas instrumentales y de gestión (con las letras M a Z).

La previsión de ingresos de los presupuestos públicos en España emplea la clasificación económica, distinguiendo su origen, es decir si se trata de impuestos directos o indirectos, tasas, transferencias corrientes o de capital, enajenación de inversiones o de activos, incremento de pasivos, etc.

El modelo de clasificación económica que se emplea en los Presupuestos Generales del Estado en España, es el siguiente:




</doc>
<doc id="6541" url="https://es.wikipedia.org/wiki?curid=6541" title="Sistema Europeo de Cuentas">
Sistema Europeo de Cuentas

El Sistema Europeo de Cuentas (también denominado "SEC 2010" o, simplemente, "SEC") constituye el marco contable comparable a escala internacional, cuyo fin es realizar una descripción sistemática y detallada del total de una economía (una región, un país o un grupo de países), sus componentes y sus relaciones con otras economías. 

La elaboración de las políticas en la Unión Europea y la supervisión de las economías de los Estados miembros y de la unión económica y monetaria (UEM) exigen que se cuente con datos comparables, actualizados y fiables sobre la estructura de la economía y la evolución de la situación económica de cada Estado.

Un Sistema de Cuentas Nacionales constituye un marco contable que define las reglas precisas para la elaboración de la contabilidad nacional, establece las definiciones y conceptos de las operaciones económicas, la estructura ordenada de cuentas, etcétera. En definitiva, esa «normativa» no es más que una técnica de representación que permite obtener una descripción cuantitativa y simplificada de la actividad económica de un país, mostrando cómo se alcanza el equilibrio entre las principales magnitudes agregadas de esa economía.

El sistema de cuentas adoptados en la Unión Europea, denominado SEC 2010, fue aprobado por el Reglamento (CE) 549/2013, relativo al Sistema Europeo de Cuentas Nacionales y Regionales, que sustituyó al Reglamento (CE) 2223/96.

La Contabilidad nacional agrupa a las unidades que constituyen la economía nacional por categorías normalizadas. La clasificación de los sectores institucionales según el SEC es la siguiente:

El sistema de contabilidad nacional adoptado en la Unión Europea (SEC-95) sirve para realizar un análisis en profundidad de las operaciones económicas que se realizan entre los sujetos que intervienen en la economía. Se describe el ciclo económico en una serie ordenada de Cuentas relacionadas entre sí mediante las operaciones económicas. Así, una operación como puede ser la exportación de mercancías va a tener incidencia en la "cuenta de bienes y servicios" y en la "cuenta del resto del mundo". El sistema distingue tres tipos de cuentas:

La Contabilidad Nacional de España elabora un conjunto del cuentas, cuadros y datos, fijados en dicho Reglamento de aprobación del Sistema europeo de cuentas. En los años ochenta se empezó a elaborar el denominado subsistema de Cuentas Regionales (CRE) y al inicio de los años noventa, la Contabilidad Nacional Trimestral (CNTR).

Las principales aplicaciones del sistema de contabilidad europeo de contabilidad son:




</doc>
<doc id="6544" url="https://es.wikipedia.org/wiki?curid=6544" title="Gasto">
Gasto

Un gasto es un egreso o salida de dinero que una persona o empresa debe pagar para acreditar su derecho sobre un artículo o a recibir un servicio. 
Sin embargo, hay sustancial diferencia entre el dinero que destina una persona (porque ella no lo recupera) del dinero que destina una empresa. Porque la empresa sí lo recupera al generar ingresos, por lo tanto «no lo gasta» sino que lo utiliza como parte de su inversión.

En contabilidad, se denomina gasto o egreso a la anotación o partida contable que disminuye el beneficio o aumenta la pérdida de una sociedad o persona física. Se diferencia del término costo porque precisa que hubo o habrá un desembolso financiero (movimiento de caja o bancos). 

El gasto es una salida de dinero que «no es recuperable», a diferencia del costo, que sí lo es, por cuanto la salida es con la intención de obtener una ganancia y esto lo hace una inversión que es recuperable: es una salida de dinero y además se obtiene una utilidad.
Podemos decir también que el gasto es la corriente de recursos o potenciales de servicios que se consumen en la obtención del producto neto de la entidad: sus ingresos.
El gasto se define como expiración de elementos del activo en la que se han incurrido voluntariamente para producir ingresos.
También podemos definir el gasto como la inversión necesaria para administrar la empresa o negocio, ya que sin eso sería imposible que funcione cualquier ente económico; el gasto se recupera en la misma medida que el cálculo del precio de la venta del bien o servicio se tenga en cuenta. 
Las pérdidas son expiraciones involuntarias de elementos del activo que no guardan relación con la producción de ingresos.

</doc>
<doc id="6545" url="https://es.wikipedia.org/wiki?curid=6545" title="Gasto público">
Gasto público

El gasto público es el total de gastos realizados por el sector público, tanto en la adquisición de bienes y servicios como en la prestación de subsidios y transferencias. En una economía de mercado, el destino primordial del gasto público es la satisfacción de las necesidades colectivas, mientras que los gastos públicos destinados a satisfacer el consumo público solo se producen para remediar las deficiencias del mercado. También tiene una importancia reseñable los gastos públicos de transferencia tendientes a lograr una redistribución de la renta y la riqueza.

La autorización de gasto público, es la operación contable que refleja el acto, en virtud del cual, la autoridad competente para gestionar un gasto con cargo a un crédito acuerda realizarlo, determinando su cuantía en forma cierta o de la forma más aproximada posible, cuando no puede hacerse de forma cierta, reservando, a tal fin la totalidad o una parte del crédito presupuestado. Este acto no implica aún relación sin interesado ajenos a la entidad, pero supone la puesta en marcha del proceso administrativo
Gasto público: recoge aquellos bienes y servicios adquiridos por la Administración Pública, bien para su consumo (material de oficina, servicios de seguridad y limpieza...), bien como elemento de inversión (ordenadores, construcción de carreteras, hospitales...). También incluye el pago de salarios a los funcionarios.

No incluye, sin embargo, el gasto de pensiones: cuando paga el salario a un funcionario compra un servicio, su trabajo (hay una transacción económica), mientras que cuando paga una pensión se trata simplemente de una transferencia de rentas (no recibe nada a cambio), por lo que no se contabiliza en el PIB.

Los gastos realizados por el gobierno son de naturaleza diversa. Van desde cumplir con sus obligaciones inmediatas como la compra de un bien o servicio hasta cubrir con las obligaciones incurridas en años fiscales anteriores. Sin embargo, muchos de ellos están dirigidos a cierta parte de la población para reducir el margen de desigualdad en la distribución del ingreso.

Por lo tanto, saber en qué se gasta el dinero del presupuesto público resulta indispensable y sano, pues a través de este gasto se conoce a quienes se ayuda en forma directa e indirecta. En esta sección encontrará diversos documentos que dan luz sobre cómo se gasta el dinero público. Además según el modelo keynesiano existe un mecanismo conocido como "multiplicador del gasto" por el cual los rendimientos económicos de una cierta cantidad de gasto superan a la cantidad gastada, vía reactiviación de la actividad económica.

Desde un punto de vista económico se distinguen tres tipos de gasto público:




</doc>
<doc id="6546" url="https://es.wikipedia.org/wiki?curid=6546" title="Inmovilizado">
Inmovilizado

En contabilidad se entiende por inmovilizado, en sentido genérico, el conjunto de elementos patrimoniales reflejados en el activo, con carácter permanente y que no están destinados a la venta. Se distinguen dos tipos de inmovilizados:




</doc>
<doc id="6547" url="https://es.wikipedia.org/wiki?curid=6547" title="Pago">
Pago

El pago es uno de los modos de extinguir las obligaciones, y consiste en el cumplimiento efectivo de la prestación debida.

El pago es el cumplimiento de la obligación, a través del cual se extingue ésta, satisfaciendo el interés del acreedor y liberando al deudor. El pago de la deuda debe ser completo (excepto en casos en donde se acuerde un cumplimiento parcial). 

Algunos autores como Díez-Picazo lo entiende como el acto debido y otros como Ferrara como un acto jurídico. 

El pago debe hacerlo, en primer lugar, el deudor. Este debe tener capacidad para enajenar y libre disposición de dar si la obligación es de dar y legitimación.
Teniendo en cuenta en las obligaciones de hacer se da el pago solo cuando el objeto o acción mandada a realizar se realiza o se entrega el objeto al acreedor desprendiéndose de la obligación y liberando así al deudor de la obligación; y en las de no hacer, el cumplimiento del pago es que el sujeto deudor no haga la obligación hasta el término pactado por la obligación.
En las obligaciones de dar es el entregar el objeto convenido y en las de dinero es cumplir con la deuda.
Muchos hablan de la satisfacción por parte del acreedor al cumplir el deudor con el pago, lo cual es irrelevante porque si el acreedor no se encuentra satisfecho con el pago no habría pago ya que no extinguirá la obligación.

También puede hacer el pago un tercero en nombre del deudor, con consentimiento cabría supererogación, sin consentimiento (ignorando el pago) no cabría subrogación pues el deudor debe tener conocimiento o en contra del deudor, en cuyo caso no podrá repetir en aquello que le hubiese sido útil.
El pago por un tercero siempre extingue la obligación pagada pero hace nacer otras obligaciones.

El tercero paga en nombre y representación del deudor o paga con conocimiento y autorización del deudor, se crea entonces una nueva obligación. El tercero tiene toda la protección del sistema jurídico para exigir el pago.
El deudor no tiene idea de que se realiza el pago.
La tercera persona paga en contra de la voluntad del deudor. 
El derecho romano señala que el tercero carece de acción para exigir al deudor que le regrese su pago (obligación natural)

Dentro del CCDF se llega a la conclusión de que el pago puede ser hecho por cualquier persona, tanto por el deudor como por un tercero, ya que el acreedor se encuentra obligado a aceptar el pago.

El pago debe hacerse al acreedor o a quien lo represente legalmente. Este último caso puede ser la persona a quien le ha otorgado un poder o aquella que tiene la representación legal de un incapaz (padre o tutor de un menor de edad, curador de un demente declarado en juicio. etc.).

Debe coincidir con el contenido de la obligación. Si la obligación consistía en la entrega de una cosa determinada y ésta se hubiese deteriorado sin culpa del deudor, el acreedor debe aceptarla en el estado que se encuentre.

Sin embargo, el deudor puede cumplir con una protección distinta siempre que el acreedor dé su consentimiento. A esta modalidad de pago se le llamó dación en pago ("datio in solutio"). Ante esta pregunta, el pago debe hacerse tal cual se estipuló en el contrato.

Hay casos en que el deudor realiza el pago sin coincidir con lo estipulado. Hoy día en materia de derecho privado no es común. Pero en materia pública se prestan cláusulas de dación en pago.


También es necesario que se constaten las características de la mercancía antes de realizar el pago.

Será el estipulado en el contrato, en caso contrario se siguen las siguientes reglas.
Si se trataba de cosas inciertas (genéricas) o de cosas fungibles (cosas que pueden ser reemplazadas unas por otras), el cumplimiento debe hacerse en el domicilio del deudor, donde el acreedor podía reclamarlo judicialmente, si por el contrario se trataba de la entrega de un bien inmueble o de otra cosa cierta (específica), el lugar era aquel en donde estuvieran los bienes. Hoy día en materia procesal si nada se ha dicho, para cosas genéricas o fungibles en el domicilio del deudor si se trataba de la entrega de un bien inmueble en donde está ese bien.
En lo que respecta al tiempo del pago este debe cumplirse en el tiempo estipulado en la obligación, pero si no lo hubieran establecido las partes, se aplica la regla de que la prestación se debe cumplir desde el día en que nace la obligación. 
No obstante, lo anterior, el cumplimiento de la obligación estará sujeta a la naturaleza y al alcance de la propia prestación. De manera tal que el deudor debe cumplirla cuando razonablemente pudiera hacerlo. Ejemplo: Al comprometerse hacer un puente, no se estipula la fecha, no se puede cumplir en un solo día.
Aunque para pintar un cuadro no se establezca un plazo de tiempo, no significa que se pueda demorar un mes cuando se puede realizar el trabajo en unos cuantos días.

En caso de insolvencia declarada judicialmente (concurso de acreedores o quiebra) las obligaciones pendientes se tornan exigibles en forma inmediata.

Si la obligación es pura (no depende de ninguna condición), es desde el momento en que nazca la obligación; si es condicional, cuando se cumpla la condición; y si es a plazos (con día cierto), será exigible cuando el día llegue a no ser que el deudor haya perdido su derecho. 

Acto por el que el deudor tiene varias deudas de la misma especie a favor de un solo acreedor; determina a cuál de ellas atribuye el pago.

El único requisito es la aceptación por el acreedor, el cual una vez acepte el pago no podrá impugnarlo posteriormente, salvo que se pruebe causa invalidante del contrato. Pero tiene límites; si la deuda devenga intereses, el pago no podrá entenderse imputado al principal mientras no estén cubiertos los intereses. 

Si ni acreedor ni deudor efectúan la imputación usamos las reglas generales:

Todo pago supone la existencia de una deuda antecedente. De ahí que al pagar una deuda que nunca existió, se le permite al que ha pagado recuperar su dinero. 

Hay subrogación cuando un acreedor sustituye a otro en el derecho de una deuda. La deuda en sí no sufre modificación. Existen dos tipos de subrogación: la "convencional" se da cuando el acreedor recibe de un tercero el pago de la deuda, y es así sustituido en sus derechos. La "legal" se da de pleno derecho en distintas disposiciones de la ley. Por ejemplo, el coobligado solidario al que se le exige toda la obligación, tendrá el derecho de exigirle su parte a los demás obligados como si fuera un acreedor.

La responsabilidad del deudor no disminuye ni aumenta por la subrogación; sigue siendo exactamente la misma.

Casos de subrogación convencional:

La subrogación convencional es aquella que se produce cuando el acreedor recibe el pago de un tercero y el acreedor decide voluntariamente subrogarlo en todos los derechos que tiene como tal, lógicamente opera cuando no opera la subrogaciones legal.
para que se dé la subrogación convencional debe estar expresa mediante una carta de pago a efectos de la subrogación convencional y legal al nuevo a creedor se le traspasan todos los derechos, privilegios, prendas e hipotecas para hacerlas efectivas tanto al deudor principal como al subsidiario.

Se produce cuando el deudor queda liberado, cuando pese a sus esfuerzos,el pago no ha podido efectuarse, debido a que el acreedor se negare a recibirlo sin razón, estuviere ausente o incapacitado para recibirlo. Se produce depositando los bienes muebles a la autoridad judicial. 

El objetivo es detener el curso de los intereses, transferir al acreedor el riesgo de la cosa y hacer recaer sobre éste los gastos de conservación. El acreedor deberá pagar al deudor los gastos de conservación de la cosa así como los gastos del juicio de consignación.

Para operar la consignación, se necesita que el deudor haya hecho repetidos intentos de pago al acreedor, de toda la obligación, y que un notario de fe de los repetidos intentos de pago por parte del deudor.

La Ley establece esta modalidad de pago, como medio de defensa que tiene el deudor contra su acreedor que no quiere recibir el pago o que se encuentra en un estado de repugnancia del mismo y que no manifiesta las razones por las cuales no le recibe dicho pago al deudor.


</doc>
<doc id="6548" url="https://es.wikipedia.org/wiki?curid=6548" title="Pasivo">
Pasivo

En contabilidad financiera, mientras el activo comprende los bienes y derechos financieros de la empresa, que tiene la persona o empresa, el pasivo recoge sus obligaciones, es decir, es el financiamiento provisto por un acreedor y representa lo que la persona o empresa debe a terceros. como el pago a bancos, proveedores, impuestos, salarios a empleados, etcétera.

Según las Normas Internacionales de Contabilidad, un pasivo financiero es todo aquel que incluye:


El pasivo está agrupado según su exigibilidad, es decir, a su mayor y menor urgencia. Así, existen pasivos a corto plazo y pasivos a largo plazo. Los pasivos cuyo pago es más urgente producen más tensión sobre el efectivo, por lo que las empresas suelen hacer una lista de sus pasivos en el orden en que se vence la fecha de pago. El poder saber qué cantidad de los pasivos de la empresa son a corto plazo y que cantidad son a largo plazo, permite a los acreedores evaluar la factibilidad de su empresa de obtener efectivo.




</doc>
<doc id="6549" url="https://es.wikipedia.org/wiki?curid=6549" title="Transferencia">
Transferencia

El término transferencia puede referirse a:


</doc>
<doc id="6550" url="https://es.wikipedia.org/wiki?curid=6550" title="Balance general">
Balance general

El balance general, balance de situación o estado de situación patrimonial es un informe financiero contable que refleja la situación económica y financiera de una empresa en un momento determinado.

El estado de situación financiera se estructura a través de tres conceptos patrimoniales, el activo, el pasivo y el patrimonio neto, desarrollados cada uno de ellos en grupos de cuentas que representan los diferentes elementos patrimoniales.

El activo incluye todas aquellas cuentas que reflejan los valores de los que dispone la entidad. Todos los elementos del activo son susceptibles de traer dinero a la empresa en el futuro, bien sea mediante su uso, su venta o su cambio. Por el contrario, el pasivo: muestra todas las obligaciones ciertas del ente y las contingencias que deben registrarse. Estas obligaciones son, naturalmente, económicas: préstamos, compras con pago diferido, etc.

El patrimonio neto puede calcularse como el activo menos el pasivo y representa los aportes de los propietarios o accionistas más los resultados no distribuidos. Del mismo modo, cuando se producen resultados negativos (pérdidas), harán disminuir el Patrimonio Neto. El patrimonio neto o capital contable muestra también la capacidad que tiene la empresa de autofinanciarse.

La ecuación básica de la contabilidad relaciona estos tres conceptos:

Patrimonio neto = Activo - Pasivo

que dicho de manera sencilla es:

"Lo que se es = Lo que se tiene - Lo que se debe"

El balance de situación forma parte de las cuentas anuales (estados financieros) que deben elaborar todas las sociedades cada ejercicio contable (habitualmente tiene una duración anual). Otros componentes de las cuentas anuales son:


Las partidas de balance es un estado financiero que muestra de manera sintetizada la situación financiera del emprendimiento al finalizar un periodo contable son agrupadas y ordenadas de acuerdo a criterios fijados que faciliten su interpretación y homologación.
En el activo normalmente se ordenan los elementos en función de su liquidez, es decir en función de la facilidad que tiene un bien para convertirse en dinero, el dinero depositado en la caja es el más líquido que hay. En España según establece el Plan General de Contabilidad se colocan en primer lugar los activos menos líquidos y en último los más líquidos, así en primer lugar se sitúa el Activo no corriente y después el Activo corriente. En muchos países de Hispanoamérica y Estados Unidos el orden es el inverso al expuesto, los activos se ordenan de mayor a menor liquidez, en primer lugar se colocan los activos más líquidos para dejar al final los menos líquidos.

El patrimonio neto y pasivo se suelen ordenar en función de su exigibilidad; un elemento será más exigible cuanto menor sea el plazo en que vence. El capital es el elemento menos exigible, mientras que las deudas con proveedores suelen ser exigible a muy corto plazo. De acuerdo con este criterio, en España, se ordenan de menor a mayor exigibilidad, se colocan en primer lugar el patrimonio neto, después el pasivo no corriente y por último el pasivo corriente. En países de Hispanoamérica es al contrario y se ordenan de mayor exigibilidad a menor exigibilidad.



</doc>
<doc id="6552" url="https://es.wikipedia.org/wiki?curid=6552" title="Subvención">
Subvención

Una subvención es la entrega de dinero o bienes y servicios realizada por una administración pública a un particular, persona física o jurídica, sin que exista la obligación de reembolsarlo. Suelen utilizarse en actividades consideradas de interés público, o en circunstancias de interés social. Como técnica de intervención administrativa, pertenece al conjunto de instrumentos propios de la actividad de fomento.

La subvención crea una relación jurídica que vincula a la Administración y al beneficiario. El beneficiario tiene, cumplidas las condiciones legales, un derecho a recibir la subvención, obligándose en consecuencia a realizar la actividad beneficiada.

La Administración, que está obligada a entregar las sumas pertinentes, se reserva para sí un haz de potestades.

Numerosas actividades económicas son subvencionadas hoy en día. Así, por ejemplo, la educación concertada, el transporte, la agricultura, las producciones cinematográficas, las actividades de I+D+I de las empresas, los programas de cooperación al desarrollo de la ONG, la producción de energía "renovable", etc.

También numerosas circunstancias sociales, personales y familiares son subvencionadas.

La subvención es una parte importante de la actividad financiera del sector público, con la que se pretende dar respuesta a demandas sociales y económicas de personas y entidades públicas o privadas. 

Desde el punto de vista de la teoría económica, el fenómeno subvencional encuentra su justificación en la función de reasignación que debe cumplir la actividad financiera de la hacienda pública, y en la teoría de los fallos del mercado. Según esta teoría, las subvenciones se justifican por la necesidad de internalizar los beneficios de determinadas conductas, producciones y actividades, que generan externalidades positivas para la sociedad, beneficios que el mercado, por sus fallos, no atribuye directamente a sus ejecutores, promotores o participantes.

Desde la perspectiva administrativa, las subvenciones son una de las herramientas más importantes empleadas por las Administraciones para el fomento de sus políticas públicas de interés general e incluso un procedimiento de colaboración entre la Administración Pública y los particulares para la gestión de actividades de interés público. Desde la perspectiva financiera, constituyen una modalidad de gasto público que debe estar sujeto a las reglas para la ejecución de los créditos del presupuesto de gastos destinados a las subvenciones. Desde el punto de vista contable, las subvenciones recibidas no reintegrables se califican como ingresos contabilizados, con carácter general, como patrimonio neto que se imputarán posteriormente a la cuenta de resultado económico patrimonial sobre una base sistemática y racional de forma correlacionada con los gastos derivados de la subvención, mientras que las subvenciones recibidas reintegrables se registran como un pasivo hasta que adquieran la condición de no reintegrables.

El Diccionario de la lengua española, en su 23.ª edición, define “subvención” como “la acción y efecto de subvenir o subvencionar” o “la ayuda económica que se da a una persona o institución para que realice una actividad considerada de interés general”, y “subvenir” como “venir en auxilio de alguien o acudir a las necesidades de algo”.

Según el Diccionario Económico y Financiero (Bernard-Colli) subvención es “"el gasto otorgado a título definitivo a una persona pública o privada a fin de aligerar o compensar una carga o fomentar una actividad determinada"” y, de modo más especializado, son “"las transferencias efectuadas por una colectividad pública en provecho de otras colectividades públicas, instituciones sociales o empresas"”.

En el ámbito supranacional, son múltiples las definiciones del concepto subvención.

El Acuerdo de la Organización Mundial del Comercio sobre Subvenciones y Medidas Compensatorias somete a disciplina la utilización de subvenciones, y reglamenta las medidas que los países pueden adoptar para contrarrestar los efectos de las subvenciones. El citado Acuerdo, en su artículo 1, recoge una definición del término “subvención”, que comprende:

El Reglamento (UE, Euratom 966/2012, de 25 de octubre de 2012, sobre las normas financieras aplicables al presupuesto general de la Unión y por el que se deroga el Reglamento (CE, Euratom) 1605/2002 del Consejo define las subvenciones como contribuciones financieras directas a cargo del presupuesto que se conceden a título de liberalidad con objeto de financiar cualquiera de las actividades siguientes:

Completa esta definición una relación de supuestos excluidos de la naturaleza de subvenciones recogidos en su artículo 121.

Por su parte, el Sistema Europeo de Cuentas Nacionales y Regionales de la Unión Europea, regulado en el Reglamento (UE) 549/2013 del Parlamento Europeo y del Consejo, de 21 de mayo de 2013, define las subvenciones como pagos corrientes sin contrapartida que las administraciones públicas o las instituciones de la Unión Europea efectúan a los productores residentes, citando como objetivos de estas subvenciones:

Asimismo, el citado Reglamento excluye del concepto de subvenciones una serie de pago sin contrapartida (4.38) como las transferencias corrientes de las administraciones públicas a los hogares en su calidad de consumidores o las transferencias corrientes entre las diferentes administraciones públicas en su condición de productores de bienes y servicios no de mercado.

La doctrina coincide en señalar tres notas características de las subvenciones:



</doc>
<doc id="6553" url="https://es.wikipedia.org/wiki?curid=6553" title="Desplazamiento patrimonial">
Desplazamiento patrimonial

Desplazamiento patrimonial es un término jurídico. Es más concreto que el recogido en la definición de "atribución patrimonial", ya que requiere que la citada ventaja o beneficio se materialice en un bien dinerario o no dinerario, que cambia de titularidad, dejando así al margen toda atribución patrimonial que suponga para el beneficiario la evitación de un gasto.



</doc>
<doc id="6554" url="https://es.wikipedia.org/wiki?curid=6554" title="Atribución patrimonial">
Atribución patrimonial

Una atribución patrimonial es, en derecho, un acto jurídico por medio del cual una persona proporciona a otra una ventaja o un beneficio de carácter patrimonial.



</doc>
<doc id="6555" url="https://es.wikipedia.org/wiki?curid=6555" title="Activo (contabilidad)">
Activo (contabilidad)

Activo es un sistema construido con bienes y servicios, con capacidades funcionales y operativas que se mantienen durante el desarrollo de cada actividad socio-económica específica. Los activos de las empresas varían de acuerdo con la naturaleza de la actividad desarrollada.

Es el conjunto de bienes económicos, derechos a cobrar que posee un comerciante o una empresa y aquellas erogaciones que serán aprovechadas en ejercicios futuros.

El Marco Conceptual para la Información Financiera del IASB (Junta de Normas Internacionales de Contabilidad), emitido el 1 de enero de 2012, establece la siguiente definición:

«Un activo es un recurso controlado por la entidad como resultado de sucesos pasados,del que la entidad espera obtener, en el futuro, beneficios económicos».

En las registraciones o registros contables cuando se produce una variación de un elemento de activo, ésta puede ser de dos tipos:


Su saldo al finalizar el ejercicio contable es siempre deudor o cero.

Ejemplos de activos: Caja, Valores a depositar, Rodados, Marcas Registradas, Mercaderías, Deudores por Venta.


Según define el Marco conceptual del Plan General de Contabilidad español, los activos son los bienes, derechos y otros recursos controlados económicamente por la empresa, resultantes de sucesos pasados de los que se espera obtener beneficios o rendimientos económicos en el futuro.

Según el apartado 5º del Marco conceptual del PGC, los activos deben reconocerse en el balance cuando sea probable la obtención a partir de los mismos de beneficios o rendimientos económicos para la empresa en el futuro, y siempre que se puedan valorar con fiabilidad. El reconocimiento contable de un activo implica también el reconocimiento simultáneo de un pasivo, la disminución de otro activo o el reconocimiento de un ingreso u otros incrementos en el patrimonio neto. dependiendo el contexto de la contabilidad

Criterios de valoración de activos utilizados en el PGC:

De acuerdo con el Plan General Contable, el activo se desglosa como suma del activo corriente y no corriente.
Activo total es la suma del activo corriente y del activo no corriente. con pasivo

http://www.travelbymexico.com/loscabos/atractivos/?nom=mcabplayamed&don=5




</doc>
<doc id="6556" url="https://es.wikipedia.org/wiki?curid=6556" title="Resultado contable">
Resultado contable

En contabilidad pública, el resultado contable es el resultado económico-patrimonial; es la variación de los fondos propios de una entidad, producida en determinado período como consecuencia de sus operaciones de naturaleza presupuestaria y no presupuestaria. Este resultado se determina considerando la diferencia entre los ingresos y los gastos producidos en el período de referencia.

El resultado presupuestario es la diferencia entre la totalidad de ingresos presupuestarios realizados durante el ejercicio, excluidos los derivados de la emisión y creación de pasivos financieros, y la totalidad de gastos presupuestarios del mismo ejercicio, excluidos los derivados de la amortización y reembolso de pasivos financieros.


</doc>
<doc id="6559" url="https://es.wikipedia.org/wiki?curid=6559" title="Circuito (desambiguación)">
Circuito (desambiguación)

Por circuito puede entenderse:





</doc>
<doc id="6560" url="https://es.wikipedia.org/wiki?curid=6560" title="Río de la Plata">
Río de la Plata

El Río de la Plata es un estuario o bahía del Cono Sur de América formado por la unión de los ríos Paraná y Uruguay. Puede ser dividido en dos sectores: el sector "interior", compuesto por los tramos superior y medio, el cual es de poca profundidad y se encuentra desprovisto de intrusión salina, y el sector "exterior", comprendido entre Punta del Este y la bahía de Samborombón, tramo de mayor profundidad, y con una importante influencia marina al ser ya un estuario del océano Atlántico. Tiene una forma que tiende a la triangular de 320 km de largo, sirviendo de frontera en todo su recorrido entre Argentina y Uruguay. Posee un rumbo general noroeste-sureste, volcando en el océano Atlántico la escorrentía de su cuenca hidrográfica más la de sus afluentes, sumando alrededor de 3 200 000 km². A pesar de que, sobre todo en Argentina, se lo llame comúnmente río, el Río de la Plata es considerado por varios geógrafos como un golfo o mar marginal del océano Atlántico. Para aquellos que lo consideran un río es el río más ancho del mundo, debido a su anchura máxima de 219 km.

El Río de la Plata tiene una extensa cuenca que recoge las aguas de los ríos Paraná, Paraguay, Uruguay, sus afluentes y diversos humedales, como el Pantanal, los Esteros del Iberá y el Bañado la Estrella. Es la segunda cuenca más extensa de Sudamérica, sólo superada por la del río Amazonas.

Fluye de noroeste a sureste desde el paralelo de punta Gorda —que marca el punto extremo del Río de la Plata (km 320)— hacia el sur y del río Uruguay hacia el norte. Al sur de este paralelo se hallan las secciones insulares 1° y 4° pertenecientes a la provincia argentina de Entre Ríos.

Según la Organización Hidrográfica Internacional, su desembocadura está determinada por la línea que une Punta del Este con el cabo San Antonio, —siendo más precisos es la punta norte de dicho cabo, llamada punta Rasa—.

De acuerdo con el Tratado del Río de la Plata, suscripto por la Argentina y Uruguay, su límite exterior está determinado por la línea imaginaria de 219 km que une Punta del Este (Uruguay) con punta Rasa del cabo San Antonio (Argentina).

La salinidad —halinidad— del agua se mide en gramos de «sal» por litro de solución, donde 10 g/L es igual a 10 psu, a 10 ppt, a 10 ‰ o a 1 %. El promedio de halinidad en los mares del mundo es de 35 ‰ y en la región del Atlántico donde desemboca este río es de algo más de 32 ‰, considerándose aguas marinas a partir de los 30 ‰ y hasta los 50 ‰.

La influencia de las aguas de este río se hace sentir aún muy adentro de las aguas oceánicas, llegando a los 32 ‰ de halinidad recién a más de 100 km del límite exterior fluvial convenido en el Tratado del Río de la Plata. En cuanto a ambas riberas, Punta del Este supera levemente los 32 ‰, mientras que en la costa argentina solo se alcanza a la altura de Nueva Atlantis, 60 km al sur de punta Rasa, contando dicha punta con 29 ‰, el mismo tenor que posee en la ribera opuesta la zona de El Pinar, en la boca del arroyo Pando, aunque entre estos dos últimos puntos una lengua extiende ese grado de halinidad hasta 45 km mar adentro hacia el sudeste de la línea del Tratado. Contorneando dicha lengua, el límite marino de 30 ‰ se logra en la banda uruguaya en Costa Azul, y en la argentina en Las Toninas, aunque esta frontera marina se logra recién 75 km mar adentro del límite del Tratado.

La intrusión halina hacia el interior del río alcanza hasta la zona entre punta Piedras y la boca del río Santa Lucía, pues allí la halinidad promedio anual apenas ronda los 0,5 ‰.

Los principales puertos son, el de Buenos Aires, (Argentina), en el margen sur, y el de Montevideo, (Uruguay), en el margen norte.

Tanto en el estuario como en la costa atlántica se pueden encontrar delfines de la especie franciscana, una especie de río que prefiere las aguas salobres. En ocasiones este delfín de especie franciscana remonta las aguas de los ríos Paraná y Uruguay. También se encuentran tres especies de tortugas marinas: la tortuga verde ("Chelonia mydas"), la tortuga cabezona ("Caretta caretta") y la tortuga siete quillas ("Dermochelys coriacea").

Físicamente el Río de la Plata se divide en tres zonas geográficas:

Desde el paralelo de Punta Gorda Latitud: 33º55'00" 10 S, hasta la línea imaginaria que une al faro de Colonia, con Punta Lara, la que se caracteriza por un sustrato de arena fina, limo y arcilla. Es la prolongación del Delta del Paraná bajo el Plata, donde se depositan los sedimentos más gruesos.

Es el tramo entre la línea que une Colonia, con Punta Lara hasta otra que une Punta Espinillo, en Montevideo con Punta Piedras. Aquí se produce el transporte de los sedimentos finos en suspensión.

Es el tramo entre esa segunda línea hasta el límite exterior, la línea imaginaria que une al Hito Faro de Punta del Este con el Hito Faro Punta Rasa. Es el único de los tramos donde se presenta influencia marina; las aguas son salobres con tenores de halinidad variados y cambiantes, según las mareas, vientos, etc. Los sedimentos son pelíticos con un importante cuerpo arenoso.

Cada año son transportados hasta el Río de la Plata unos 160 millones de toneladas de sedimento. Esta carga está compuesta por: limo —el 56 %— (90 millones ton/año), arcilla —el 28 %— (45 millones ton/año), y arena —el 16 %— (25 millones ton/año). De toda la carga, el 90 % viaja en suspensión (145 millones ton/año). El material suspendido está integrado por la totalidad de las arcillas y limos que llegan al Plata, más 10 millones ton/año de la arena más fina. Mediante carga de fondo también son transportados 15 millones ton/año de arena más gruesa. En el sector del Río de la Plata que enfrenta al delta se deposita toda la arena transportada (los 25 millones ton/año) y una porción de los limos.

La fuente principal de los materiales finos es el sur de Bolivia y las provincias del noroeste de la Argentina, regiones que son drenadas por la alta cuenca del río Bermejo, por la cual primero los sedimentos son tributados al río Paraguay, luego por este pasan al río Paraná, y finalmente este último los transporta hasta el Plata, donde allí se depositan, luego de un recorrido de casi 2000 km desde su origen.

El cauce del río está dominado por la presencia de extensos bancos de baja profundidad que dificultan la navegación con embarcaciones de calado, que debe hacerse siguiendo diversos canales naturales y artificiales, muchos de los cuales, en especial la ruta que comunica Buenos Aires con el océano Atlántico, son objeto de constante dragado para evitar la acumulación de sedimentos y así mantenerlos abiertos a la navegación. Los principales bancos de arena son Ortiz, Arquímedes, Inglés y Rouen.

La morfología del lecho del área específica de Martín García se caracteriza por un bajío (la formación Playa Honda) atravesado por canales y compuesto por sedimentos grises-marrones, arenosos hacia el norte, arenas limosas y limos arenosos hacia el sur y limos arcillosos (lodos) hacia la margen argentina. El delta sufre un continuo avance de 15 m por año y se produce una disminución de las profundidades del lecho con la formación de bancos que luego se transforman en islas.

La región responde a la «falla de Punta del Este», y a la «subfalla del Río de la Plata»; con sismicidad baja; y su última expresión se produjo el de silencio sísmico), a las 3.20 UTC-3, con una magnitud de 5,5 en la escala de Richter. (Terremoto del Río de la Plata de 1888).

En cuanto a las costas del río, éstas presentan características muy diversas.

La costa uruguaya pertenece a la formación geológica del macizo de Brasilia, con costas altas y playas de arena bordeadas de dunas separadas por cabos rocosos.

Los principales afluentes por la ribera uruguaya son los ríos San Juan, Rosario, Santa Lucía, y el arroyo Solís Grande.

La costa argentina corresponde a la cuenca sedimentaria de la Pampa, formada por mesetas de limo que alternan con planicies barrosas. Es, por lo general, baja, formada por limos, siendo abundantes los juncales. En ella se destaca la bahía de Samborombón, cuya costa tiene unos 180 km de longitud. En esta bahía desembocan varios cursos de agua, muchas veces canalizados, siendo los principales los ríos Samborombón y Salado.

Esta isla, a diferencia de las cercanas islas deltaicas, posee un núcleo rocoso, al ser un afloramiento de rocas arcaicas del macizo de Brasilia, con una antigüedad de 1800 millones de años. Tiene una superficie de 184 ha y una altura máxima de 27 msnm. La sedimentación en el lecho del área específica de la isla Martín García es muy activa, formándose bancos que luego se transforman en islas bajas, como ocurrió con el crecimiento de la isla Timoteo Domínguez y de bancos como el de Santa Ana y del Medio.

La isla Martín García —en su superficie histórica— es de soberanía argentina, pero desde la entrada en vigencia del Tratado del Río de la Plata de 1973 quedó situada como un exclave en aguas de uso común para los dos países, pero rodeada del sector del Río de la Plata cuyo lecho y subsuelo fue adjudicado a Uruguay por el tratado. Por esta razón los depósitos aluvionales que la rodean —incluso la isla Timoteo Domínguez— pertenecen a Uruguay, con excepción de los aluviones que se depositen en las costas que miran al "canal de Martín García" (o "Buenos Aires"), y al "canal del Infierno", los cuales son de uso común para ambos países, en virtud del artículo 46 del tratado.

Desde el acuerdo del 18 de junio de 1988, se ha establecido la única "frontera seca" del límite argentino-uruguayo, entre las islas Martín García y Timoteo Domínguez.

Según diversas fuentes, el estuario fue descubierto por Américo Vespucio en 1501, el cual lo llamó "Río Jordán" (no confundir con el original y pequeño río Jordán del Cercano Oriente asiático ni con otros ríos también llamados "Jordán"). La conquista de Río de la Plata, nombre que entonces se dio a la región de la que hoy son parte: Argentina, Paraguay y Uruguay, la inició el español Juan Díaz de Solís, quien buscando un paso para llegar a Oriente, se internó en el actual Río de la Plata en 1516 al cual llamó "Mar Dulce", en referencia a la baja salinidad del agua.

El rey de España dispuso que se crearan colonias en la región del Río de la Plata. El 2 de febrero de 1536, el español Pedro de Mendoza fundó la ciudad de Santa María de Buenos Aires, situada en la costa occidental del Río de la Plata en la desembocadura del Río de la Plata. El 15 de agosto de 1537, Juan de Salazar fundó, la ciudad de Asunción, cerca del río Paraguay.

El primer no americano en navegar sus aguas fue Juan Díaz de Solís, en 1516, mientras intentaba hallar un pasaje desde el océano Atlántico al océano Pacífico. Desembarcó junto a un grupo de hombres en con los españoles para, finalmente y al cabo de un tiempo, volver a la nación indígena. Otras versiones señalan a Américo Vespucio como su descubridor.

El nombre “de la Plata” se refiere a la mitológica “Sierra de Plata” en el país del “rey Blanco” que buscaron Alejo García, Sebastián Caboto y otros, remontando los ríos de la Plata, Paraná, Paraguay y Uruguay y realizando expediciones terrestres hacia el Chaco y Chiquitos. Es posible que la Sierra de Plata haya sido un remoto influjo del Cerro Rico de Potosí que los indígenas pasaban de boca en boca, aunque es más probable que el mito del "rey Blanco" haya llegado por influencia de la civilización incaica. Según Dick Edgar Ibarra Grasso y otros investigadores y aficionados, basándose en gran medida en las Crónicas del Inca Garcilaso de la Vega, los monarcas del Tawuantinsuyu eran "blancos", llegando Grasso a suponer que descendientes de algunas remotas expediciones europeas precolombinas; aunque lo más probable es que el adjetivo "blanco" fuera una metonimia por la blanca plata. En 1525 Sebastián Caboto dio con algunos de los acompañantes indígenas de Alejo García, quienes llevaban plata que recogieron en la expedición, pensó que en la zona abundaba la plata y desde entonces muchos quisieron expedicionar hacia el “Río de la Plata”.

En 1536 los españoles al mando de Pedro de Mendoza fundaron a la ciudad de Santa María de los Buenos Aires en la costa sur y oeste del Río de la Plata; tal ciudad fue momentáneamente abandonada hasta que fue definitivamente refundada en 1580 por el español Juan de Garay. En 1680, transgrediendo el Tratado de Tordesillas los portugueses fundaron, sobre la ribera oriental del río, la Colonia del Sacramento, frente a Buenos Aires. Colonia fue objeto de disputas entre españoles y portugueses por cerca de un siglo, cambiando varias veces de manos y obligando a los españoles a colonizar la Banda Oriental, hasta entonces mayormente ignorada. Colonia fue tomada por las fuerzas virreinales españolas el mismo año de su fundación, para ser devuelta a Portugal en 1681. Para detener el avance lusitano, el rey español Felipe V ordenó la construcción de Montevideo y sus fortificaciones sobre la banda oriental del río. Esta tarea se realizó entre 1724 y 1730.

En los años 1806-1807 el Río de la Plata fue testigo de las Invasiones Inglesas al Virreinato que llevaba su nombre.

El 5 de junio de 1888 se produjo el terremoto del Río de la Plata, a las 3,20, con una magnitud en la escala de Richter de 5,5, su epicentro estuvo en , a 30 m de profundidad. Jamás se tomaron medidas mínimamente antisísmicas. Afectó a todas las poblaciones de la costa del Río de la Plata, especialmente a las ciudades de Buenos Aires y de Montevideo. Produjo leves daños y su epicentro se localizó en el centro de dicho río.

El 13 de diciembre de 1939 se desarrolló la Batalla del Río de la Plata, frente a Punta del Este, primer enfrentamiento importante de navíos alemanes y británicos durante la Segunda Guerra Mundial.

En aguas del Río de la Plata se han producido numerosos naufragios. Se lo considera de navegación difícil por la intensidad y la rapidez con la cual se forman tormentas.

La sudestada es un fenómeno climático que se caracteriza por vientos persistentes, regulares a fuertes del sudeste, temperaturas relativamente bajas y generalmente acompañado por lluvias de variada intensidad.

Esto ocurre cuando después del pasaje de un sistema frontal frío, un sistema de alta presión, cuyo centro se ubica al sudoeste de la provincia de Buenos Aires produce por su circulación vientos persistentes del mencionado sector sobre la costa del Río de La Plata.

Se genera por el efecto combinado de dos sistemas, uno de alta presión ubicado sobre el océano Atlántico, frente a las costas de la Patagonia central, que transporta aire frío y de origen marítimo hacia el Este de la provincia de Buenos Aires, extremo sur del Litoral y Sur del Uruguay y un sistema de baja presión, localizado sobre el Centro y sur de la Mesopotamia y la región occidental del Uruguay y que por su circulación produce un aporte de aire cálido y húmedo sobre la región.

Cuando se profundiza la depresión, se intensifica la circulación del viento del sector sudeste produciéndose este fenómeno climático. Durante una sudestada, el río puede alcanzar una altura de 3,96 metros (sobre el plano de marea altura cero) y oleaje intenso, lo que provoca anegamientos en toda la zona ribereña.

El Pampero es un viento que sopla del sudoeste, precisamente del Anticiclón del Pacífico Sur hasta el Ciclón del Río de la Plata, que se caracteriza por tener una masa de aire fría y seca gracias a las lluvias de convección que provocó en la cordillera de Los Andes al atravesarla. En verano causa tormentas intensas seguidas por un marcado descenso de la temperatura y cielo despejado.

Dentro de las actividades deportivas más antiguamente practicadas en el Río de la Plata se encuentran el nado en aguas abiertas y el "yachting". En ambas márgenes del río, los clubes náuticos desarrollan regatas de distintas clases de veleros, siendo una de las más importantes la regata “Buenos Aires-Punta del Este”.

El Río de la Plata permite la práctica del windsurf y, en especial, embravecido, también del surf, presentando olas de unos dos metros. También se puede practicar "kite surfing".

En Montevideo, las playas donde se "surfea" son Playa Honda, Playa Verde y Playa Malvín.

En la margen argentina, la práctica del deporte está muy extendida en la zona de Buenos Aires y alrededores. En prácticamente toda la costa, destacando la localidad de San Isidro, se practica "windsurf", "kite surfing", kayak y "outrigger" (canoas hawaianas).

La navegación en kayak tomó relevancia en los últimos años en el Río de la Plata y delta del Paraná por su bajo costo (un kayak es, en comparación con otras, una embarcación muy barata) y por la casi inexistencia de requisitos legales para su adquisición, tenencia y manejo, requisitos abundantes en casi toda otra embarcación. El "Encuentro Anual de Kayakistas" tiene lugar durante la Semana Santa de cada año, tratándose de un encuentro libre; quienes concurren se reúnen espontáneamente en la Isla Martín García a la cual arriban piloteando sus kayaks.
En los ríos Paraná, San Antonio y Canal del Este se puede practicar esquí acuático, "wake board" y "wake surf".

Las aguas del Río de la Plata están muy contaminadas principalmente en las cercanías de la ciudad de Buenos Aires y el conurbano bonaerense. El río recibe efluentes del Riachuelo, que es uno de los ríos más contaminados del planeta, y del río Reconquista, otro río muy contaminado, entre otros. Por esta razón, el baño está prohibido en sus aguas en las costas de la ciudad de Buenos Aires, Vicente López, San Isidro y Avellaneda, entre otros.

La contaminación que recibe consiste principalmente en desechos cloacales y residuos industriales sin tratamiento.

Del lado uruguayo, los gobiernos de los departamentos de Montevideo y Canelones monitorean semanalmente la calidad del agua de sus playas, permitiendo el baño en aquellas no alcanzadas por esta contaminación.




</doc>
<doc id="6561" url="https://es.wikipedia.org/wiki?curid=6561" title="Verso">
Verso

El verso es una de las unidades en que puede dividirse un poema, superior generalmente al pie e inferior a la estrofa. En la literatura en lenguas romances, los testimonios en verso preceden a los testimonios en prosa. Aunque ambas formas de expresión manifiestan históricamente una tendencia innegable a la especialización (el verso para la lírica, la prosa para la narrativa,y los textos argumentativos y expositivos), no faltan ejemplos tanto de verso no lírico (épico, narrativo en general, dramático o expositivo, como en la poesía didáctica grecolatina) como de prosa lírica. Es una forma especial de expresarse, es un conjunto de palabras sujetas a medida, ritmo y cadencia.


En función de su medida y cadencia cabe distinguir múltiples tipos de verso. Una clasificación elemental es la que distingue versos de arte menor (de dos a ocho sílabas) bisílabos, trisílabos, tetrasílabos, pentasílabos, hexasílabos, heptasílabos, octosílabos y de arte mayor (de nueve o más). Los versos pares de arte mayor, como el decasílabo, el dodecasílabo y el alejandrino, suelen dividirse en dos mitades, generalmente iguales, llamadas hemistiquios.

La manera de colocar los acentos marca el ritmo del verso. Dentro de un mismo tipo de verso caben varias disposiciones acentuales, que reciben nombres específicos. Así, un verso puede ser trocaico (_U), si los acentos de palabra caen en las sílabas impares; yámbico (U_), si caen en las pares; o mixto, si se mezclan las dos cadencias.




</doc>
<doc id="6562" url="https://es.wikipedia.org/wiki?curid=6562" title="Prosa">
Prosa

Prosa es la forma que toma naturalmente el lenguaje (tanto el oral como el escrito) para expresar los conceptos, y no está sujeta, como el verso, la medida y cadencia determinadas. Se identifica con lo contrapuesto al ideal y la perfección. Coloquialmente, "prosa" es equivalente a "palabrería".

La prosa es una forma de la lengua escrita, definida por oposición al verso, con figuras que se agrupan en el llamado paralelismo. Se ha definido la prosa por oposición al verso, porque aquella no tiene ni ritmo métrico, ni repetición (formas fijas) ni periodicidad (rima) como aquél (Ducrot y Todorov, 1974). Ritmo, repetición y periodicidad son justamente los elementos caracterizadores de la oralidad.

El término se originó en la expresión latina "prosa oratio" "discurso directo" (sin los ornamentos del verso), donde "prosa" es el femenino de "prosus", antes prorsus "directo", del antiguo latín "provorsus" "(moverse) adelante", de "pro-" "adelante" + "vorsus" "vuelto", participio pasado de "vertere" "volverse".

En la liturgia de la misa, tras la aleluya o el tracto se "cantaba" una secuencia denominada "prosa".


El origen de la prosa proviene de la Jonia en el siglo VI a. C., primero por una prosa narradora para describir lugares, costumbres o relatos, en una lengua más racionalista, diferente de la lírica. Sin embargo, su mayor desarrollo se efectuaría en Atenas durante los siglos V y IV a. C.

Por primera vez se dispuso de un instrumento lingüístico capaz de servir al pensamiento abstracto: el "estilo imperiódico" de los logógrafos (literalmente, "los que escriben en prosa").

Los principales autores de la historiografía griega en prosa son:

En la cultura romana la prosa no se ligó a los géneros narrativos sino a la oratoria. Marco Tulio Cicerón en "Oratore" distingue tres niveles de estilo: bajo, medio y elevado, y profundiza los caracteres musicales de la prosa estableciendo reglas relativas a la disposición de las partes de la frase, el ritmo y sobre todo las cláusulas del periodo, disponiendo la parte final según métricas análogas a las de la poesía. A través de Quintiliano este modelo llega a la Edad Media influyendo en el "ars dictandi" de escuelas (escuelas monásticas, escuelas episcopales, escuelas palatinas, "Studia Generalia") y cancillerías. En el siglo XIII Juan de Garlandia describe y clasifica algunos tipos de estilo prosaico, y en este periodo se elabora una prosa latina científica y filosófica, por encima del gusto del "ornatus", hace prevalecer el rigor de los esquemas lógico-demostrativos (escolástica).

El Renacimiento propone una gama más amplia de géneros en prosa: la poética del clasicismo presenta modelos a imitar en los diversos géneros literarios. La inversión de tendencia en el Barroco trae artificios espectaculares. En la Ilustración la prosa se convierte en un instrumento importante para la divulgación y la polémica narrativa, filosófica, satírica, etc. En el siglo XIX la distinción entre prosa y poesía se profundiza, creando la distinción entre prosa de función teórico-narrativa y poesía de función lírica; a esta distinción se refiere la comprensión del dominio de la prosa en el naturalismo.

"Los primeros textos plenamente escritos en lengua vulgar autóctona datan de finales del siglo XI, en Cataluña, de finales del XII, en Castilla
y Navarra, y de la primera mitad del XIII, en León, Galicia y Portugal, y, por lo general, se trata de documentos no emanados de la cancillería regia. ... Si el primer testimonio del empleo del vernáculo en la cancillería navarra es de 1169 o 1171 -excluyendo los Fueros de Estella y de Laguardia, el de la cancillería castellana es de 1194." La cancillería de Fernando III el Santo estableció la práctica de redactar los documentos en lengua vulgar (el castellano del siglo XIII).

Si la primera mitad del siglo XIX fue la edad de oro de la poesía rusa, la segunda mitad del siglo fue la edad de oro de la prosa rusa.


El primer autor castellano de nombre conocido utiliza el término "prosa" para denominar sus propios versos:

"Antes del siglo VIII el término "prosa" se usaba ya para distinguir la poesía rítmica de la poesía clásica cuantitativa."

"Las prosas no se dirigen a los ricos de los palacios, sino a los pobres de las casuchas, más bien."

Molière pone en boca a sus personajes una definición simplificada de "prosa" y "verso", con un cómico resultado:


</doc>
<doc id="6563" url="https://es.wikipedia.org/wiki?curid=6563" title="Tangente">
Tangente

En matemáticas, tangente puede referirse a:


</doc>
<doc id="6565" url="https://es.wikipedia.org/wiki?curid=6565" title="Continuidad">
Continuidad

Continuidad puede hacer referencia a:


</doc>
<doc id="6569" url="https://es.wikipedia.org/wiki?curid=6569" title="Dimensión de un espacio vectorial">
Dimensión de un espacio vectorial

La dimensión de un espacio vectorial (también llamada dimensión de Hamel de un espacio vectorial, para distinguirla de la dimensión de Hilbert en el caso de los espacios de Hilbert) es el número de vectores que forman una base [de Hamel] del espacio vectorial.

Dado un espacio vectorial pueden considerarse conjuntos de vectores "S" de un espacio vectorial "V" y se puede examinar si poseen algunas de estas propiedades:


Un conjunto que sea linealmente independiente (1) y generador del espacio vectorial (2) se dice que es una base vectorial. Puede demostrarse que todas las bases de un espacio vectorial son conjuntos con el mismo número de elementos (es decir, conjuntos que tienen el mismo cardinal). Y el número común de elementos de una base cualquiera es precisamente la dimensión del espacio vectorial.

Nótese un hecho importante, si se cambia el cuerpo de los escalares, de formula_2 a formula_3, entonces el mismo punto M será determinado por el complejo "z" ="x" + "y"i, es decir por un solo parámetro.

La dimensión de P es 1 sobre formula_3 y dos sobre formula_2:

Un plano real es por lo tanto una recta compleja. La apelación "plano complejo" para designar un plano real con escritura compleja de las coordenadas ( "x" + "y"i en vez de (x; y) ) es errónea, pero muy común.

El espacio ambiente es tridimensional y se requiere por lo tanto tres reales ("x", "y", "z") para definir un punto. No se le puede considerar como un espacio sobre formula_3.

En la teoría de la relatividad, se añade una cuarta variable: el tiempo, y un punto ("x", "y", "z", "t") de este espacio cuadridimensional corresponde a un evento o acontecimiento (las coordenadas nos dicen donde y cuando ocurrió).

En algunas teorías actuales, los físicos trabajan en un modelo del espacio con once dimensiones, pero sobre el conjunto de los enteros, y no los real. Como el conjunto formula_7 de los racionales no es un cuerpo sino un anillo, el espacio no es vectorial (se dice que es un módulo). Sin embargo, la definición de la dimensión es válida en tales espacios. En este ejemplo, la mayoría de las dimensiones son enrolladas sobre sí mismas, como una serpiente que se muerde la cola. Su curvatura es enorme, pues su radio es microscópico, menor que el de un núcleo. Los espacios vectoriales no tienen curvatura.

Más formalmente la dimensión de un espacio vectorial se define como el cardinal de una base vectorial para dicho espacio. Por el axioma de elección todo espacio tiene una base (incluso el espacio {0}, ya que el vacío es una base), y puesto que puede demostrarse que todas las bases vectoriales tienen el mismo cardinal, el concepto de dimensión está bien definido. Conviene notar que existen espacios vectoriales de tanto de dimensión finita como de dimensión infinita (el espacio vectorial de los polinomios de una variable, por ejemplo tiene dimensión formula_8.

La dimensión de un espacio coincide además con los dos cardinales siguientes:

La definición sigue siendo la misma en el caso de un subespacio, pero existe un método particular de calcularla cuando el subespacio es definido como espacio generado por sistema de vectores.
Veámoslo en un ejemplo. En el espacio formula_9, sean los vectores:

Cuatro vectores no pueden ser independientes en formula_9, por lo tanto tienen que existir relaciones de dependencia:

lo que se puede escribir en forma matricial :

Llamemos "A" a la matriz anterior, y "X" el vector columna. Esta relación significa que el vector "X" pertenece al núcleo de "A", que se nota Ker A (del alemán "Kern", núcleo). El espacio generado es el conjunto de los 

es decir de los A·X: es la imagen de A.

Resulta intuitivo que cuanto mayor es el núcleo, menor es la imagen, en términos de dimensiones.
Concretamente, si llamamos rango de A a la dimensión de su imagen: rg A = dim (Im A), tenemos la relación:

Busquemos dim (Ker A): 

formula_15
formula_16
Quedan dos ecuaciones no proporcionales, por lo tanto independientes, y cada una resta 1 a la dimensión, que vale inicialmente 4. Resulta que dim (Ker A ) = 2. 
Se puede constatarlo de otra manera: Las dos ecuaciones permiten expresar "y",luego "x" en función de "z" y "t", por consiguiente solo quedan dos variables libres, y la dimensión es 2. 

Aplicando la fórmula : rg A = 4 - 2 = 2. El subespacio es un plano.

Si U y U son subespacios de un espacio vectorial de dimensión finita, se cumple: 
satisface formula_28.




</doc>
<doc id="6570" url="https://es.wikipedia.org/wiki?curid=6570" title="Producto cartesiano">
Producto cartesiano

En matemáticas, el producto cartesiano de dos conjuntos es una operación, que resulta en otro conjunto, cuyos elementos son todos los pares ordenados que pueden formarse de forma que el primer elemento del par ordenado pertenezca al primer conjunto y el segundo elemento pertenezca al segundo conjunto. 

Por ejemplo, dados los conjuntos:

y

su producto cartesiano es:

que se representa:

El producto cartesiano recibe su nombre de René Descartes, cuya formulación de la geometría analítica dio origen a este concepto.

Un par ordenado es una colección de dos objetos distinguidos como "primero" y "segundo", y se denota como , donde es el «primer elemento» y el «segundo elemento». Dados dos conjuntos y , su producto cartesiano es el conjunto de todos los pares ordenados que pueden formarse con estos dos conjuntos:

Puede definirse entonces el cuadrado cartesiano de un conjunto como .

Sea también el conjunto de todos los números enteros . El producto cartesiano de consigo mismo es , es decir, el conjunto de los pares ordenados cuyos componentes son enteros. Para representar los números enteros se utiliza la recta numérica, y para representar el conjunto se utiliza un plano cartesiano (en la imagen).

Sean los conjuntos de tubos de pintura, y de pinceles:

El producto cartesiano de estos dos conjuntos, , contiene todos los posibles emparejamientos de pinceles y tubos de pintura. De manera similar al caso de un plano cartesiano en el ejemplo anterior, este conjunto puede representarse mediante una tabla:
Dado un número finito de conjuntos , , ..., , su producto cartesiano se define como el conjunto n-tuplas cuyo primer elemento está en , cuyo segundo elemento está en , etc.

Puede definirse entonces potencias cartesianas de orden superior a 2, como , etc. Dependiendo de la definición de n-tupla que se adopte, esta generalización puede construirse a partir de la definición básica como:

o construcciones similares.

En el caso de una familia de conjuntos arbitraria (posiblemente infinita), la manera de definir el producto cartesiano consiste en cambiar el concepto de tupla por otro más cómodo. Si la familia está indexada, una aplicación que recorra el conjunto índice es el objeto que distingue quién es la «entrada -ésima»:

donde denota la unión de todos los . Dado un , la proyección sobre la coordenada es la aplicación:

En el caso de una familia finita de conjuntos indexada por el conjunto , según la definición de n-tupla que se adopte, o bien las aplicaciones de la definición anterior son precisamente n-tuplas, o existe una identificación natural entre ambos objetos; por lo que la definición anterior puede considerarse como la más general.

Sin embargo, a diferencia del caso finito, la existencia de dichas aplicaciones no está justificada por las hipótesis más básicas de la teoría de conjuntos. Estas aplicaciones son de hecho funciones de elección cuya existencia sólo puede demostrarse en general si se asume el axioma de elección. De hecho, la existencia de funciones de elección (cuando todos los miembros de son no vacíos) es equivalente a dicho axioma.

El conjunto vacío actúa como el cero del producto cartesiano, pues no posee elementos para construir pares ordenados:
El producto cartesiano de dos conjuntos no es conmutativo en general, salvo en casos muy especiales. Lo mismo ocurre con la propiedad asociativa.
Puesto que el producto cartesiano puede representarse como una tabla o un plano cartesiano, es fácil ver que el conjunto producto es el producto de los cardinales de cada factor:

En teoría de conjuntos, la fórmula anterior de cardinal del producto cartesiano como producto de los cardinales de cada factor, sigue siendo cierta utilizando cardinales infinitos.



</doc>
<doc id="6571" url="https://es.wikipedia.org/wiki?curid=6571" title="Asociatividad (álgebra)">
Asociatividad (álgebra)

La asociatividad es una propiedad en el álgebra y la lógica proposicional que se cumple si, dados tres o más "elementos" cualquiera de un conjunto determinado, se verifica que existe una "operación": formula_1, que cumpla la igualdad:

Es decir, en una expresión asociativa con dos o más ocurrencias seguidas de un mismo "operador" asociativo, el orden en que se ejecuten las "operaciones" no altera el resultado, siempre y cuando se mantenga intacta la secuencia de los "operandos". En otras palabras, reorganizar los paréntesis en una expresión asociativa no cambia su valor final. 

La suma y el producto de números reales cumplen la propiedad asociativa, siendo válidas las igualdades:

para la suma y:

para la multiplicación.

En ambas, la ubicación de los "paréntesis" no altera el resultado. Nótese que los "operandos" han se han mantenido en su posición original dentro de la expresión. Muchas operaciones importantes son "no asociativas," por ejemplo la resta y la exponenciación. Las expresiones que contienen tanto operaciones "asociativas" como operaciones "no asociativas" dan como resultado "expresiones" "no asociativas." 

No se debe confundir la asociatividad con la conmutatividad, la cual establece que si se puede cambiar el orden de los "operandos" sin afectar el resultado final.

Sea A un conjunto en el cual se ha definido una operación binaria interna ○ tal que

Se dice que la operación formula_1 es asociativa si:
La ley asociativa también puede ser expresada en notación funcional así:

Partiendo del conjunto de los números naturales 

para la operación suma, definida como:

formula_11 tiene la propiedad asociativa, dado que:

Por ejemplo:

"Sin embargo", para la operación resta, definida como:

formula_15 "no" tiene la propiedad asociativa, dado que:

Por ejemplo:



En la lógica proposicional estándar, la "asociación", o "asociatividad" son dos reglas de reemplazo válidas. Estas reglas permiten mover los paréntesis en expresiones lógicas usadas en pruebas lógicas. Las reglas son:
y
donde "formula_30" es un símbolo metalógico que representa "puede ser reemplazado en una prueba por."

"Asociatividad" es una propiedad de algunas conectivas lógicas en las funciones de verdad de la lógica proposicional. Las siguientes equivalencias lógicass demuestran que la asociatividad es una propiedad de conectivas lógicas particulares. Son asimismo tautologías de funciones de verdad.

Asociatividad de la disyunción:
Asociatividad de la conjunción:
Asociatividad de la equivalencia:



</doc>
<doc id="6572" url="https://es.wikipedia.org/wiki?curid=6572" title="Conmutatividad">
Conmutatividad

En matemáticas, la propiedad conmutativa o conmutatividad es una propiedad fundamental que tienen algunas operaciones según la cual el resultado de operar dos elementos no depende del orden en que se toman. Esto se cumple en la adición y la multiplicación ordinarias: el "orden de los sumandos no altera la suma", o "el orden de los factores no altera el producto". 

La conmutatividad de las operaciones elementales de sumar y multiplicar ya era conocida implícitamente desde la antigüedad, aunque no fue llamada así hasta principios del siglo XIX, época en que las matemáticas contemporáneas empezaban a formalizarse. Las sucesivas ampliaciones del concepto de número (números naturales, números enteros, números racionales, números reales) ampliaron el alcance de las operaciones de sumar y multiplicar, pero en todas ellas se preserva la conmutatividad. Esta propiedad también se satisface en muchas otras operaciones, como la suma de vectores, polinomios, matrices, funciones reales, etc., o el producto de polinomios o de funciones reales.

En contraposición a la adición y la multiplicación de números, la sustracción y la división no son operaciones conmutativas. Entre las operaciones no conmutativas cabe destacar también la composición de funciones, el producto de matrices y el producto vectorial.

A pesar de ser una propiedad aplicada básicamente a las operaciones matemáticas, la conmutatividad o la no conmutatividad son relevantes en otros campos cercanos como la lógica proposicional y algunas operaciones de teoría de conjuntos, y en algunas aplicaciones físicas tales como el principio de incertidumbre de la mecánica cuántica. Fuera del ámbito científico, también se pueden encontrar ejemplos en la vida cotidiana, ya que la ejecución consecutiva de dos acciones puede tener un resultado diferente según el orden en que se ejecuten.

De hecho, la conmutatividad es un caso particular del concepto de función simétrica. En efecto, una operación binaria en "M" no es más que una aplicación μ: "M" × "M" formula_1 "M", y afirmar que esta es simétrica, μ("x","y") = μ("y","x"), es exactamente lo mismo que lo que requiere la propiedad conmutativa.

Dada una operación binaria formula_2 un conjunto "M", se dice que dos elementos "x", "y" de "M" conmutan (o que son "permutables") cuando se cumple que "x"formula_2"y" = "y"formula_2"x". Así pues, una operación es conmutativa cuando dos elementos cualesquiera conmutan.

La importancia fundamental de la propiedad conmutativa radica en el hecho de que la adición y la multiplicación de números naturales, los números que permiten contar los conjuntos finitos, son conmutativas. Por ejemplo:

Expresado de manera general: para cualesquiera "x", "y" de N:

La ampliación del sistema de los números naturales a otros sistemas numéricos: números enteros (formula_9), números racionales (formula_10), números reales (formula_11), y números complejos (formula_12), se hace extendiéndose las operaciones de adición y multiplicación, y de manera que éstas siguen siendo conmutativas. Por ejemplo:

Esto no quiere decir que cualquier ampliación de un sistema numérico necesariamente vaya a respetar las propiedades previas. El ejemplo más importante de este hecho viene dado por el cuerpo de los cuaterniones "H", que, al igual que el de los números complejos, también es una extensión del cuerpo de los números reales, pero con tres unidades imaginarias "i", "j", "k" en lugar de una. La multiplicación de "H" "no" es conmutativa, ya que por ejemplo "i"·"j" = "k", es diferente de "j"·"i" = -"k".

En contraste con las operaciones de adición y multiplicación, las operaciones que las permiten invertir, sustracción y división, son claramente "no conmutativas". Basta poner un par de ejemplos:
Nótese que para poder efectuar estos cálculos hay que trabajar en el sistema numérico apropiado: Z para poder restar, y Q para poder dividir por un número diferente de  0

Es importante destacar que para sacar provecho de la conmutatividad de una operación es necesario que ésta sea asociativa, ya que en este caso la composición de "n" elementos "x", …, "x" se puede representar (sin paréntesis) como "x"formula_2…formula_2"x". Por ejemplo
Si una operación formula_2 asociativa y dos elementos "x", "y" conmutan, entonces también conmutan sus «potencias»: formula_19, para cualesquiera "m" y "n" números naturales no nulos. En particular, todas las «potencias» formula_20 ("n"> 0) conmutan entre ellas.

Dado un conjunto "M" con una operación interna, el centro de M es el subconjunto formado por los elementos que conmutan con todos los demás; a veces se representa por Z("M"). Afirmar que la operación es conmutativa significa que el centro de "M" es todo "M".

Como consecuencia de la última de las propiedades anteriores, si la operación es asociativa entonces el centro de "M" es una parte estable para la operación (es decir, si dos elementos "x", "y" pertenecen al centro entonces x formula_2 también pertenece.)

Una estructura algebraica viene dada por uno o varios conjuntos dotados de operaciones binarias u operaciones externas. En la definición de cada tipo de estructura algebraica impone que estas operaciones cumplan ciertas propiedades, entre las que puede estar la propiedad conmutativa. Cuando en alguna de estas operaciones no se impone que satisfaga la propiedad conmutativa pero sin embargo la satisface, entonces se añade el adjetivo "conmutativo" el nombre de la estructura en cuestión.

Hay, sin embargo, un caso especial en el que el adjetivo "conmutativo" no tiene exactamente el mismo significado que en los casos anteriores:


El adjetivo "conmutativo" aparece también en el nombre de una rama del álgebra: el álgebra conmutativa, que estudia los anillos conmutativos y sus módulos.

Los primeros usos implícitos de la propiedad conmutativa se remontan a la antigüedad. Los egipcios utilizaban la propiedad conmutativa de la multiplicación para simplificar el cálculo de productos.
En la Antigua Grecia, Euclides asumió la propiedad conmutativa de la multiplicación en su obra "Elementos".
Los usos formales de la propiedad conmutativa aparecieron a finales del siglo XVIII y los inicios del XIX, cuando los matemáticos empezaron a trabajar en el campo de la teoría de funciones.

La primera utilización documentada del adjetivo "conmutativo" fue en un artículo de François Servois de 1814 los "Annales de Gergonne",
donde aparece la expresión en francés "conmutativas entre ellas" para describir, en la terminología actual, el hecho de que dos funciones conmutan.
En 1841 Duncan Farquharson Gregory usó la expresión en inglés "commutative law" en su libro "Examples of the processes of the differential and integral calculus" para referirse a la posibilidad de conmutar dos operaciones. Este uso fue recogido poco después, en 1844, por George Boole en un artículo en "Philosophical Transactions".

La propiedad conmutativa también es aplicable a algunas operaciones de la lógica proposicional. En lógica proposicional, la conmutación se encuentra en algunas reglas de sustitución:
y
donde "formula_27" es un símbolo metalógico que significa «en una demostración formal, se puede sustituir con...».

La conmutatividad es una propiedad de algunas conectivas lógicas de la lógica proposicional, que se expresa con equivalencias lógicas:

"Conmutatividad de la conjunción"
"Conmutatividad de la disyunción"
"Conmutatividad de la implicación" (también llamada ley de la permutación)
"Conmutatividad de la equivalencia" (también llamada ley conmutativa completa de la equivalencia)

La unión y la intersección de conjuntos son operaciones conmutativas. Aunque estas operaciones se pueden efectuar con familias arbitrarias de conjuntos, cuando se trata de dos conjuntos estas propiedades se expresan

La suma y el producto de cardinales son operaciones conmutativas.
Si formula_33 i formula_34 son dos cardinales, entonces
Esto implica en particular que la suma y el producto de números naturales (es decir, los cardinales de los conjuntos finitos) son conmutativas. La conmutatividad de la suma es consecuencia de la de la unión de conjuntos. La conmutatividad del producto es consecuencia de que un producto cartesiano de conjuntos tiene el mismo número de elementos independientemente de cómo se realice este producto.

En contraste con los cardinales, en general la suma y el producto de ordinales transfinitos no son conmutativas. Por ejemplo, si ω es el ordinal de N, 1 + ω ≠ ω + 1.

Además de la adición y multiplicación de números, hay otras operaciones análogas que son conmutativas.
Entre ellas destaca la adición de vectores en un espacio vectorial cualquiera, como por ejemplo el espacio euclidiano R, l'espai M(R) de las matrices "m"×"n" con coeficientes reales, o el espacio de las funciones reales formula_36("E",R) definidas en un conjunto cualquiera "E".
También se dice que el producto escalar de vectores en un espacio euclidiano es conmutativo, aunque, al no tratarse de una operación interna, sería más apropiado decir que es "simétrico".

En la vida cotidiana se pueden encontrar numerosos ejemplos de operaciones conmutativas, como por ejemplo la acción de ponerse los calcetines: no importa qué calcetín se ponga primero, de cualquiera de las dos maneras el resultado final (tener los dos calcetines puestos) es el mismo. Un ejemplo que utiliza la conmutatividad de la adición se observa cuando se paga un producto o servicio con monedas: independientemente del orden en que se den en el cajero, el total acumulado siempre es el mismo.

La composición de funciones no es una operación conmutativa. Por ejemplo, consideremos las funciones "f","g": R → R definidas por "f"("x")="x"+1, "g"("x")="x". Entonces

Un caso particular interesante es el de las biyecciones de un conjunto en sí mismo, es decir, las permutaciones, que forman un grupo dicho grupo simétrico. Este no es conmutativo cuando el conjunto tiene 3 o más elementos.

En cuanto a operaciones no conmutativas en matemáticas, y aparte de la sustracción y división ya mencionadas, algunas operaciones binarias no conmutativas son las siguientes:
La potenciación no es conmutativa, ya que, por ejemplo, 2 = 8 es diferente de 3 = 9.
La multiplicación de matrices no es conmutativa; por ejemplo,
Más generalmente, si "n"≥2, el anillo de las matrices cuadradas M(R) no es conmutativo, y su centro está formado por las matrices escalares, es decir, las matrices múltiples de la identidad.
El producto vectorial de dos vectores en el espacio tridimensional es anticonmutativa, es decir, b × a = - a × b. Así tenemos, por ejemplo, i×j = k, diferent de j×i = -k.

En la vida del día a día se pueden encontrar multitud de ejemplos de operaciones no conmutativas.
Un ejemplo sencillo podría ser el de lavar y planchar la ropa: las acciones de lavar y planchar (en este orden) producen un resultado diferente que planchar y luego lavar.
Otro ejemplo es la concatenación de textos, es decir, la acción de juntar cadenas de caracteres. No es lo mismo escribir LA y luego CA (LACA) que escribir primero CA y luego LA (CALA).
Finalmente, un último ejemplo: los movimientos del cubo de Rubik no conmutan (de hecho, todos ellos forman un grupo no conmutativo).

El conmutador da una indicación de la medida en que una cierta operación binaria no consigue ser conmutativa. Para poder definirlo, hay una cierta estructura adicional, ya sea que la operación es la de un grupo, o bien que sea la multiplicación en un anillo o álgebra.

En un grupo, el conmutador de dos elementos "x" e "y" es el elemento:

Está claro que ["x","y"] = "e" (elemento neutro del grupo) si "x" e "y" conmutan. Un grupo es conmutativo sii todos los conmutadores son el elemento neutro.

El conjunto de los conmutadores de un grupo "G" no es por lo general un subgrupo, pero genera un subgrupo normal llamado subgrupo de los conmutadores o subgrupo derivado. El cociente G / D de G por su subgrupo derivado es un grupo conmutativo llamado "grupo abelianitzat" de "G"; es el más grande de los cocientes conmutativos de "G".

En un anillo o, más generalmente, en un álgebra, el conmutador de dos elementos "x" e "y" es el elemento
De nuevo, está claro que ["x", "y"] = 0 sii "x" e "y" conmutan. Un anillo o álgebra son conmutativos si todos los conmutadores son nulos.

Si "A" es una "K"-álgebra asociativa, entonces el producto ("x","y") formula_24 ["x","y"] definido por el conmutador es alternado y satisface la identidad de Jacobi, de modo A es también una "K"-álgebra de Lie. El álgebra asociativa "A" es conmutativa si su álgebra de Lie asociada también lo es.

La propiedad asociativa está muy relacionada con la conmutativa. La propiedad asociativa de una expresión que contiene dos o más ocurrencias del mismo operador postula que el orden que se lleven a cabo las operaciones no afecta al resultado final, siempre que el orden de los términos no cambie. Por el contrario, la propiedad conmutativa dice que el orden de los términos no afecta al resultado final.

La mayoría de operaciones conmutativas que se encuentran en la práctica también son asociativas. Sin embargo, la conmutatividad no implica la asociatividad. Un contraejemplo sencillo es el siguiente:

Esta operación es claramente conmutativa (el intercambio entre "x" e "y" no afecta el resultado final porque se trata de una suma) pero no es asociativa, ya que, por ejemplo, "m"(1,"m"(2,3)) = 7/4, pero "m"("m"(1,2),3) = 9/4. Al no ser asociativa no se puede aplicar el teorema de conmutatividad, como se ve por ejemplo en que
"m"(1,"m"(2,3)) ≠ "m"(2,"m"(1,3)).

Existe una relación interesante entre asociatividad y conmutatividad. Consideremos un conjunto "M" dotado de una operación que representaremos multiplicativamente. Consideremos, para cada "a" de "M", las correspondientes traslaciones por la izquierda y la derecha:
La asociatividad de la operación significa que ("xy")"z" = "x"("yz") para cualesquiera "x,y,z". Pero esta expresión se puede escribir
R formula_44 L ("y") =
L formula_44 R ("y"),
por lo que la operación es asociativa si toda traslación por la izquierda conmuta con toda traslación por la derecha.

Algunas formas de simetría se pueden relacionar directamente con la conmutatividad. Cuando un operador conmutativo escribe como una función binaria entonces la función resultante es simétrica a lo largo de la línea "y = x". Por ejemplo, si la función "f" representa la suma (una operación conmutativa) de tal manera que "f"("x","y") = "x" + "y", entonces "f" es una función simétrica (véase la imagen de la derecha, donde se observa la simetría respecto a la diagonal).

En cuanto a relaciones entre dos variables, hay una estrecha conexión entre conmutatividad y la relación simétrica. Afirmar que una relación R es simétrica significa que formula_46.

En mecánica cuántica, tal como la formuló Schrödinger, las magnitudes observables físicas se corresponden con un cierto tipo de operadores lineales, los operadores autoadjuntos en un espacio de Hilbert apropiado. Por ejemplo, en un movimiento unidimensional la posición "x" y la cantidad de movimiento p de una partícula están representadas respectivamente por los operadores formula_47 y formula_48. Cuando el estado del sistema se representa mediante una función de onda "ψ"("x") de L(R), entonces estos operadores interpretan como formula_49 (multiplicar por "x") y formula_50 (donde "ħ" es la constante de Planck reducida). Estos dos operadores no conmutan, tal como se puede comprobar considerando el resultado de componerlos actuando sobre "ψ"("x") (omitimos el factor constante -i"ħ"):

Esta no conmutación también se puede expresar calculando su conmutador:

Según el principio de incertidumbre de Heisenberg, si los operadores que representan dos magnitudes observables no conmutan, entonces éstas no se pueden medir de forma precisa y simultánea. Así pues la posición y la cantidad de movimiento (en una dirección dada) no se pueden determinar simultáneamente. De manera más precisa, esta incertidumbre mínima viene cuantificada precisamente por el valor esperado del conmutador de los dos operadores, y en el caso que nos ocupa esto significa que las desviaciones estándares de la posición y el momento satisfacen la desigualdad σσ






</doc>
<doc id="6573" url="https://es.wikipedia.org/wiki?curid=6573" title="División (matemática)">
División (matemática)

En matemática, la división es una operación parcialmente definida en el conjunto de los números naturales y los números enteros; en cambio, en el caso de los números racionales, reales y complejos es siempre posible efectuar la división, exigiendo que el divisor sea distinto de cero, sea cual fuera la naturaleza de los números a dividir. En el caso de que sea posible efectuar la división, esta consiste en indagar cuántas veces un número (divisor) está "contenido" en otro número (dividendo). El resultado de una división recibe el nombre de cociente. De manera general puede decirse que la división es la "operación inversa" de la multiplicación, siempre y cuando se realice en un campo.

Debe distinguirse la división «exacta» (sujeto principal de este artículo) de la «división con resto» o "residuo" (la división euclídea). A diferencia de la suma, la resta o la multiplicación, la división entre números enteros no está siempre definida; en efecto: 4 dividido 2 es igual a 2 (un número entero), pero 2 entre 4 es igual a ½ (un medio), que ya no es un número entero. La definición formal de «división» , «divisibilidad» y «conmensurabilidad», dependerá luego del conjunto de definición. 

Conceptualmente, la división describe dos nociones relacionadas, aunque diferentes, la de «separar» y la de «repartir». De manera formal, la división es una operación binaria que a dos números asocia el producto del primero por el inverso del segundo. Para un número no nulo, la función «división por ese número» es el recíproco de «multiplicación por ese número». De este modo, el cociente formula_1 dividido formula_2 se interpreta como el producto formula_3 por formula_4.

Si la división no es exacta, es decir, el divisor no está contenido un número exacto de veces en el dividendo, la operación tendrá un resto o "residuo", donde:

Etimología: la palabra deriva del latín dividere: partir, separar.

En álgebra y ciencias, la división se denota generalmente a modo de fracción, con el dividendo escrito sobre el divisor. Por ejemplo formula_6 se lee: "tres dividido cuatro". También puede emplearse una barra oblicua: formula_7; este es el modo más corriente en los lenguajes de programación por computadora, puesto que puede ser fácilmente inscrito como secuencia simple del código ASCII. 

Otro modo de indicar una división es por medio del símbolo óbelo (formula_8) (también llamado "signo de la división"). Este símbolo también se usa para representar la operación de división en sí, como es de uso frecuente en las calculadoras. Otras variantes son los dos puntos (:) o el punto y coma (;).

La división no es propiamente dicho una "operación" (es decir, una ley de composición interna definida por todas partes), sus «propiedades» no tienen implicaciones estructurales sobre el conjunto de números, y deben ser comprendidas dentro del contexto de los "números fraccionarios".


Hasta el siglo XVI fue muy común el algoritmo de la división por galera, muy similar a la división larga y a la postre (sustituido por ésta como método predilecto de división). El proceso usual de división (división larga) suele representarse bajo el diagrama:

También se usa un diagrama equivalente con la línea debajo del dividendo

Y también se usa otro diagrama equivalente 

Otro método consiste en la utilización de una «tabla elemental», similar a las tablas de multiplicar, con los resultados preestablecidos.

Consideremos el conjunto ℕ = {0, 1, 2, ..."n", ...} de los números naturales y sean "a","b" no nulo, "c" números naturales, diremos que 
si 

Si es así se dirá que "a" es el dividendo; "b", el divisor; y "c", el cociente si existe.

Sin embargo, dados dos números naturales "a" y "b" ≠ 0, existen dos únicos números naturales "q" y "r" tal que se cumplen las relaciones formula_16. 

El algoritmo que permite encontrar "q" y "r", conociendo "a" y "b", se denomina "división entera", entre otros nombres.

La división no es una operación cerrada, lo cual quiere decir que, en general, el resultado de dividir dos números enteros no será otro número entero, a menos que el dividendo sea un múltiplo entero del divisor. 

Existen "criterios de divisibilidad" para números enteros (por ejemplo, todo número terminado en 0,2,4,6 u 8 será divisible entre 2), utilizados particularmente para descomponer los enteros en factores primos, lo que se usa en cálculos como el mínimo común múltiplo o el máximo común divisor.

La división en ℚ siempre es posible, toda vez que el divisor no sea nulo. Pues el cociente formula_17, no es sino el producto formula_18

En los racionales, el resultado de dividir dos números racionales (a condición de que el divisor no sea 0) puede calcularse con cualesquiera de las fracciones representativas. Se puede definir de la manera siguiente: dados "p"/"q" y "r"/"s",

Esta definición demuestra que la división funciona como la "operación inversa" de la multiplicación.

El resultado de dividir dos números reales es otro número real (siempre y cuando el divisor no sea 0). Se define como "a"/"b" = "c" si y solo si "a" = "cb" y "b" ≠ 0.

La división de cualquier número entre cero es una «indefinición». Esto resulta del hecho que cero multiplicado por cualquier cantidad finita es otra vez cero, es decir que el cero no posee un inverso multiplicativo.

El resultado de dividir dos números complejos es otro número complejo (siempre y cuando el divisor no sea 0). Se define como

en donde "r" y "s" no son ambos iguales a 0.

En la forma trigonométrica formula_22

En forma exponencial:





</doc>
<doc id="6574" url="https://es.wikipedia.org/wiki?curid=6574" title="Número trascendente">
Número trascendente

Un número trascendente, también número trascendental, es un número real o complejo que no es raíz de ninguna ecuación algebraica con coeficientes enteros no todos nulos.
Un número real trascendente no es un número algebraico, pues no es solución de ninguna ecuación algebraica con coeficientes racionales. Tampoco es número racional, ya que estos resuelven ecuaciones algebraicas de primer grado, al ser real y no ser racional, necesariamente, es un número irracional.
En este sentido, "número trascendente" es antónimo de "número algebraico". La definición no proviene de una simple relación algebraica, sino que se define como una propiedad fundamental de las matemáticas. Los números trascendentes más conocidos son π y e.

En general, si tenemos dos cuerpos formula_1 y formula_2 de forma que el segundo es extensión del primero, diremos que formula_3 es trascendente sobre formula_4 si no existe ningún polinomio formula_5 del que formula_6 es raíz (formula_7).

El conjunto de números algebraicos es numerable, mientras el conjunto de números reales es no numerable; por lo tanto, el conjunto de números trascendentes es también no numerable. O tiene la potencia del continuo.

Sin embargo, existen muy pocos números trascendentes conocidos, y demostrar que un número es trascendente puede ser extremadamente difícil. Por ejemplo, todavía no se sabe si la constante de Euler (formula_8) lo es, siendo

formula_9 formula_10 cuando formula_11.

De hecho, ni siquiera se sabe si formula_12 es racional o irracional.

La propiedad de normalidad de un número puede contribuir a demostrar si es trascendente o no.

La denominación «"trascendental"» la acuñó Leibniz cuando en un artículo de 1682 demostró que la función formula_13 no es una función algebraica de formula_14,posteriormente Euler definió los números trascendentes en el sentido moderno. La existencia de los números trascendentes fue finalmente probada en 1844 por Joseph Liouville, en 1851 mostró algunos ejemplos entre los que estaba la «constante de Liouville»:

formula_15

donde el enésimo dígito después de la coma decimal es 1 si "n" es un factorial (es decir, 1, 2, 6, 24, 120, 720, etc.) y 0 en cualquier otro caso. El primer número del que se demostró que era trascendente sin haber sido específicamente construido para ello fue "e", por Charles Hermite en 1873. En 1882, Carl Louis Ferdinand von Lindemann publicó una demostración de que π es trascendente. En 1874, Georg Cantor encontró el argumento descrito anteriormente estableciendo la ubicuidad de los números trascendentes.

El descubrimiento de estos números ha permitido la demostración de la imposibilidad de resolver varios antiguos problemas de geometría que sólo permiten utilizar regla y compás. El más conocido de ellos es el de la cuadratura del círculo, y su imposibilidad radica en que π es trascendente. No ocurre lo mismo con los otros dos "problemas griegos" más famosos, la duplicación del cubo y la trisección del ángulo, que se deben a la imposibilidad de construir con regla y compás números derivados de polinomios de grado superior a dos (véase Número construible) es significativo que estos otros dos problemas puedan resolverse con modificaciones relativamente simples del método (permitiendo marcar la regla, acción que la geometría euclídea no toleraba) o con métodos similares a la regla y compás, como el origami, en tanto que la cuadratura del círculo, al depender de la trascendencia de π, tampoco es resoluble con esos métodos.

Una lista de los números trascendentes más comunes:



</doc>
<doc id="6575" url="https://es.wikipedia.org/wiki?curid=6575" title="Conjunto difuso">
Conjunto difuso

Un conjunto difuso es un conjunto que puede contener elementos de forma parcial, es decir, que la propiedad de que un elemento formula_1 pertenezca al conjunto formula_2 (formula_3) puede ser cierta con un grado parcial de verdad. Este grado de pertenencia es una proposición en el contexto de la lógica difusa, y no de la lógica usual binaria, que sólo admite dos valores: cierto o falso.

El grado de pertenencia de formula_1 a formula_2, o el grado de verdad de pertenecer al conjunto, se mide con un número real formula_6 comprendido entre 0 y 1, ambos inclusive. De forma rigurosa, el valor correspondiente a cada elemento define una función indicatriz formula_7, donde formula_8 representa el conjunto universal del que el conjunto formula_2 toma sus elementos. Por ello se suele hablar de subconjuntos difusos y no de conjuntos difusos.

Si el valor de esta función es 0, formula_1 no pertenece a formula_2. Si es 1, entonces formula_12 totalmente, y si formula_13 entonces formula_1 pertenece a formula_15 de una manera parcial.

La teoría de los subconjuntos difusos o borrosos (palabras intercambiables en este contexto) fue desarrollada por Lofti A. Zadeh en 1965 con el fin de representar matemáticamente la imprecisión intrínseca de ciertas categorías de objetos.

Los subconjuntos difusos (o partes borrosas de un conjunto) fueron inventados para modelar la representación humana de los conocimientos (por ejemplo para medir nuestra ignorancia o una imprecisión objetiva) y mejorar así los sistemas de decisión, de ayuda a la decisión, y de inteligencia artificial.

Con los conjuntos difusos se pueden realizar las mismas acciones que con los conjunto clásico. Siendo dos conjuntos difusos formula_2 y formula_17 se definen las operaciones usuales:


Por lo tanto, un conjunto difuso equivale, en concepto de información, a una familia infinita no numerable de conjuntos clásicos. La teoría de los subconjuntos difusos es por lo tanto muy distinta y mucho más compleja que la teoría de los conjuntos usuales. Por ejemplo, un conjunto finito clásico tiene un número finito de subconjuntos clásicos, pero un número infinito de subconjuntos difusos.



</doc>
<doc id="6576" url="https://es.wikipedia.org/wiki?curid=6576" title="Identidad de Euler">
Identidad de Euler

Se llama identidad de Euler a un caso especial de la fórmula desarrollada por Leonhard Euler, notable por relacionar cinco números muy utilizados en la historia de las matemáticas y que pertenecen a distintas ramas de la misma:

donde:


La identidad es un caso especial de la Fórmula de Euler, la cual especifica que

para cualquier número real "x". (Nótese que los argumentos para las funciones trigonométricas "sen" y "cos" se toman en radianes.) En particular si

entonces

y ya que 

y que

se sigue que

Lo cual implica la identidad

Para una forma alternativa de notar que la identidad de Euler es tanto verdadera como profunda, supongamos que:

en el desarrollo polinómico de e a la potencia x:
para obtener:
simplificando (usando i = -1):
Al separar el segundo miembro de la ecuación en subseries real e imaginarias:

Se puede comprobar la convergencia de estas dos subseries infinitas, lo cual implica

El logaritmo natural de un número complejo z = a+bi (donde a y b son números reales) se define como: 

Donde formula_18 es:

Notar que con esta definición, arg(z) está en el intervalo formula_20 (el argumento en este intervalo es conocido como el "valor principal del argumento" o simplemente "argumento principal"). Esta definición no es la única posible, ya que se pudo haber definido en [0, 2π), etc.

Para logaritmos de otras bases, se tiene la siguiente relación mediante "cambio de base" :

Por ejemplo :

Y también se cumple:

Lo anterior se puede deducir de la definición. También se puede obtener formula_24 a partir de la identidad de Euler, pero no es la razón de la deducción de ln(-1). Este detalle se explicará a continuación.
Se sabe que formula_25, pero también es cierto que formula_26 o formula_27. De hecho en general:

El error que se puede cometer aquí, es que si formula_29, entonces a = b. Lo anterior es válido si a y b son números reales, pero en complejos esto no se siempre se cumple. Por ende si bien formula_30, no es cierto que formula_31. De esta forma, se puede ver que:

Antes se mencionó que si se puede obtener formula_24 con la identidad de Euler, pero no es recomendable hacerlo, porque se puede cometer errores como lo descrito más arriba, ya que no siempre se cumple el hecho de que si formula_34 entonces a = ln(b).
Otro error es lo siguiente:

El error aquí ocurre en formula_36. Esto último no es correcto y el motivo es que 

Porque formula_38 solo se cumple de manera general si a es positivo. Por un lado formula_39, pero formula_40 no es real, puesto que ln(-e) no es un número real.

El número áureo (también llamado número de oro​) es un número irracional,​ representado por la letra griega φ (phi) o Φ (Phi) = 1,61803398874988... 

Una de sus propiedades es:

formula_41

Por tanto: formula_42

Reemplazando '1' en la identidad de Euler, formula_1, se tiene:

formula_44

Por tanto:

formula_45

formula_46

formula_47

Ordenando los términos de la ecuación queda:

formula_48

De esta manera se relacionan siete números muy utilizados, cinco operaciones de las matemáticas y la ecuación cuadrática.



</doc>
<doc id="6577" url="https://es.wikipedia.org/wiki?curid=6577" title="Lógica difusa">
Lógica difusa

La lógica difusa (también llamada lógica borrosa) se basa en lo relativo de lo observado como posición diferencial. Este tipo de lógica toma dos valores aleatorios, pero contextualizados y referidos entre sí. Así, por ejemplo, una persona que mida dos metros es claramente una persona alta, si previamente se ha tomado el valor de persona baja y se ha establecido en un metro. Ambos valores están contextualizados a personas y referidos a una medida métrica lineal.

Fue formulada en 1965 por el ingeniero y matemático Lotfi A. Zadeh.

La lógica difusa ("fuzzy logic," en inglés) se adapta mejor al mundo real en el que vivimos, e incluso puede comprender y funcionar con nuestras expresiones, del tipo «hace mucho calor», «no es muy alto», «el ritmo del corazón está un poco acelerado», etc.

La clave de esta adaptación al lenguaje se basa en comprender los cuantificadores de cualidad para nuestras inferencias (en los ejemplos de arriba, «mucho», «muy» y «un poco»).

En la teoría de conjuntos difusos se definen también las operaciones de unión, intersección, diferencia, negación o complemento, y otras operaciones sobre conjuntos (ver también subconjunto difuso), en los que se basa esta lógica.

Para cada conjunto difuso, existe asociada una función de pertenencia para sus elementos, que indica en qué medida el elemento forma parte de ese conjunto difuso. Las formas de las funciones de pertenencia más típicas son trapezoidal, lineal y curva.

Se basa en reglas heurísticas de la forma SI (antecedente) ENTONCES (consecuente), donde el antecedente y el consecuente son también conjuntos difusos, ya sea puros o resultado de operar con ellos. Sirvan como ejemplos de regla heurística para esta lógica (nótese la importancia de las palabras «muchísimo», «drásticamente», «un poco» y «levemente» para la lógica difusa):


Los métodos de inferencia para esta base de reglas deben ser sencillos, versátiles y eficientes. Los resultados de dichos métodos son un área final, fruto de un conjunto de áreas solapadas entre sí (cada área es resultado de una regla de inferencia). Para escoger una salida concreta a partir de tanta premisa difusa, el método más usado es el del centroide, en el que la salida final será el centro de gravedad del área total resultante.

Las reglas de las que dispone el motor de inferencia de un sistema difuso pueden ser formuladas por expertos o bien aprendidas por el propio sistema, haciendo uso en este caso de redes neuronales para fortalecer las futuras tomas de decisiones.

Los datos de entrada suelen ser recogidos por sensores que miden las variables de entrada de un sistema. El motor de inferencias se basa en chips difusos, que están aumentando exponencialmente su capacidad de procesamiento de reglas año a año.

Un esquema de funcionamiento típico para un sistema difuso podría ser de la siguiente manera:

En la figura, el sistema de control hace los cálculos con base en sus reglas heurísticas, comentadas anteriormente. La salida final actuaría sobre el entorno físico, y los valores sobre el entorno físico de las nuevas entradas (modificado por la salida del sistema de control) serían tomadas por sensores del sistema.

Por ejemplo, imaginando que nuestro sistema difuso fuese el climatizador de un coche que se autorregula según las necesidades: Los chips difusos del climatizador recogen los datos de entrada, que en este caso bien podrían ser la temperatura y humedad simplemente. Estos datos se someten a las reglas del motor de inferencia (como se ha comentado antes, de la forma SI... ENTONCES... ), resultando un área de resultados. De esa área se escogerá el centro de gravedad, proporcionándola como salida. Dependiendo del resultado, el climatizador podría aumentar la temperatura o disminuirla dependiendo del grado de la salida.

La LDC es un modelo lógico multivalente que permite la modelación simultánea de los procesos deductivos y de toma de decisiones. El uso de la LDC en los modelos matemáticos permite utilizar conceptos relativos a la realidad siguiendo patrones de comportamiento similares al pensamiento humano. Las características más importantes de estos modelos son: La flexibilidad, la tolerancia con la imprecisión, la capacidad para moldear problemas no lineales y su fundamento en el lenguaje de sentido común. Bajo este fundamento se estudia específicamente cómo acondicionar el modelo sin condicionar la realidad.

La LDC utiliza la escala de la LD, la cual puede variar de 0 a 1 para medir el grado de verdad o falsedad de sus proposiciones, donde las proposiciones pueden expresarse mediante predicados. Un predicado es una función del universo X en el intervalo [0, 1], y las operaciones de conjunción, disyunción, negación e implicación, se definen de modo que restringidas al dominio [0, 1] se obtenga la Lógica Booleana. 

Las distintas formas de definir las operaciones y sus propiedades determinan diferentes lógicas multivalentes que son parte del paradigma de la LD. Las lógicas multivalentes se definen en general como aquéllas que permiten valores intermedios entre la verdad absoluta y la falsedad total de una expresión. Entonces el 0 y el 1 están asociados ambos a la certidumbre y la exactitud de lo que se afirma o se niega y el 0,5 a la vaguedad y la incertidumbre máximas. 
En los procesos que requieren toma de decisiones, el intercambio con los expertos lleva a obtener formulaciones complejas y sutiles que requieren de predicados compuestos. Los valores de verdad obtenidos sobre estos predicados compuestos deben poseer sensibilidad a los cambios de los valores de verdad de los predicados básicos. 

Esta necesidad se satisface con el uso de la LDC, que renuncia al cumplimiento de las propiedades clásicas de la conjunción y la disyunción, contraponiendo a éstas la idea de que el aumento o disminución del valor de verdad de la conjunción o la disyunción provocadas por el cambio del valor de verdad de una de sus componentes, puede ser “compensado” con la correspondiente disminución o aumento de la otra. Estas propiedades hacen posible de manera natural el trabajo de traducción del lenguaje natural al de la Lógica, incluidos los predicados extensos si éstos surgen del proceso de modelación. 

En la LDC, el operador conjunción, expresado como c (and) es la media geométrica.

En la LDC la modelización de la vaguedad se logra a través de variables lingüísticas, lo que permite aprovechar el conocimiento de los expertos, al contrario de lo que ocurre en otros métodos más cercanos a las cajas negras y exclusivamente basados en datos, como por ejemplo las redes neuronales.

Existen autores como Jesús Cejas Montero en su Artículo La Lógica Difusa Compensatoria publicado en el 2011 por la Revista Ingeniería Industrial del Instituto Superior Politécnico José Antonio Echeverría, que marcó un hito en la difusión de la LDC, que recomiendan el uso de funciones de pertenencia sigmoidales para funciones crecientes o decrecientes. Los parámetros de estas funciones quedan determinados fijando dos valores. El primero de ellos es el valor a partir del cual se considera que la afirmación contenida en el predicado es más cierta que falsa, por ejemplo pudiera establecerse a partir de 0.5. El segundo es el valor para el cual el dato hace casi inaceptable la afirmación correspondiente, por ejemplo pudiera establecerse a partir de 0.1.

En la actualidad existe un Sistema de Soporte a Decisiones Basado en Árboles con Operadores de Lógica Difusa cuyo nombre es Fuzzy Tree Studio 1.0, desarrollado en forma conjunta entre Universidad CAECE y la Universidad Nacional de Mar del Plata (Argentina), que posee un módulo que trabaja con la LDC. Ello permite al agente decisor despreocuparse por el trasfondo matemático y centrarse en la formulación verbal del modelo que le permita tomar una decisión. 

En general los modelos basados en LDC combinan la experiencia y el conocimiento con datos numéricos, por lo que puede ser visto como una “caja gris”. Los modelos basados en LD pueden verse como “cajas blancas”, dado que permiten ver su estructura explícitamente. En contraposición a los modelos basados en datos exclusivamente, como las Redes Neuronales, que corresponderían a “cajas negras”.
Estos modelos pueden ser optimizados cuando se dispone de datos reales numéricos. El método de optimización puede provenir de la Inteligencia Computacional. En este contexto, los Algoritmos Genéticos presentan una alternativa interesante. Este enfoque constituye el fundamento de los sistemas híbridos. 

La tendencia de las investigaciones sobre gestión empresarial, mediante las técnicas de la LDC, está orientada a la creación de sistemas híbridos que integren esta con las habilidades de las Redes Neuronales y las posibilidades de los Algoritmos Genéticos y la Lógica de Conjuntos. La creación e implementación de estos sistemas mixtos permite resolver problemas complejos y de difícil solución; en las que se usan estimaciones subjetivas sustentadas en la experiencia y en la información disponible, como son: modelos de decisión utilizados con criterios de optimización, ubicación de centros comerciales, estrategia de entrada a mercados, selección de carteras de productos y servicios, desarrollo de aplicaciones informáticas, métodos para problemas de descubrimiento de conocimiento, métodos para evaluar la eficiencia de diferentes tipos de instituciones, entre otras. 

La Lógica Difusa Compensatoria es un modelo lógico multivalente que renuncia a varios axiomas clásicos para lograr un sistema idempotente y “sensible”, al permitir la “compensación” de los predicados. En la LD el valor de verdad de la conjunción es menor o igual a todas las componentes, mientras que el valor de verdad de la disyunción es mayor o igual a todas las componentes. La renuncia de estas restricciones constituye la idea básica de la LDC.

En conclusión la LDC es un nuevo enfoque para los sistemas multivalentes basado en la Media Geométrica que, además de aportar un sistema formal con propiedades lógicas de notable interés, constituye un puente entre la Lógica y la Toma de Decisiones. La LDC entra a formar parte del arsenal de métodos para la evaluación multicriterio, adecuándose especialmente a aquellas situaciones en que el agente decisor puede describir verbalmente, frecuentemente en forma ambigua, la heurística que utiliza cuando ejecuta acciones de evaluación/clasificación multicriterio. Sin embargo, la consistencia de la plataforma lógica dota a esta propuesta de una capacidad de formalización del razonamiento que rebasa los enfoques descriptivos de los procesos de decisión. Es una oportunidad para usar el lenguaje como elemento clave de comunicación en la construcción de modelos semánticos que faciliten la evaluación, la toma de decisiones y el descubrimiento de conocimiento.

La lógica difusa se utiliza cuando la complejidad del proceso en cuestión es muy alta y no existen modelos matemáticos precisos, para procesos altamente no lineales y cuando se envuelven definiciones y conocimiento no estrictamente definido (impreciso o subjetivo). 

En cambio, no es una buena idea usarla cuando algún modelo matemático ya soluciona eficientemente el problema, cuando los problemas son lineales o cuando no tienen solución.

Esta técnica se ha empleado con bastante éxito en la industria, principalmente en Japón, extendiéndose sus aplicaciones a multitud de campos. La primera vez que se usó de forma importante fue en el metro japonés, con excelentes resultados. Posteriormente se generalizó según la teoría de la incertidumbre desarrollada por el matemático y economista español Jaume Gil Aluja.

A continuación se citan algunos ejemplos de su aplicación:

La lógica difusa es una rama de la inteligencia artificial que le permite a una computadora analizar información del mundo real en una escala entre lo falso y lo verdadero, manipula conceptos vagos, como "caliente" o "húmedo", y permite a los ingenieros construir dispositivos que juzgan la información difícil de definir. 

En Inteligencia artificial, la "lógica difusa", o "lógica borrosa" se utiliza para la resolución de una variedad de problemas, principalmente los relacionados con control de procesos industriales complejos y sistemas de decisión en general, la resolución y la compresión de datos. Los sistemas de lógica difusa están también muy extendidos en la tecnología cotidiana, por ejemplo en cámaras digitales, sistemas de aire acondicionado, lavar ropas, etc. Los sistemas basados en lógica difusa imitan la forma en que toman decisiones los humanos, con la ventaja de ser mucho más rápidos. Estos sistemas son generalmente robustos y tolerantes a imprecisiones y ruidos en los datos de entrada. Algunos lenguajes de programación lógica que han incorporado la lógica difusa serían por ejemplo las diversas implementaciones de "Fuzzy PROLOG" o el lenguaje Fril.

Consiste en la aplicación de la lógica difusa con la intención de imitar el razonamiento humano en la programación de computadoras. Con la lógica convencional, las computadoras pueden manipular valores estrictamente duales, como verdadero/falso, sí/no o ligado/desligado. En la lógica difusa, se usan modelos matemáticos para representar nociones subjetivas, como "caliente"/"tibio"/"frío", para valores concretos que puedan ser manipuladas por los ordenadores.

En este paradigma, también tiene un especial valor la variable del tiempo, ya que los sistemas de control pueden necesitar retroalimentarse en un espacio concreto de tiempo, pueden necesitarse datos anteriores para hacer una evaluación media de la situación en un período anterior.

Como principal ventaja, cabe destacar los excelentes resultados que brinda un sistema de control basado en lógica difusa: ofrece salidas de una forma veloz y precisa, disminuyendo así las transiciones de estados fundamentales en el entorno físico que controle. Por ejemplo, si el aire acondicionado se encendiese al llegar a la temperatura de 30º, y la temperatura actual oscilase entre los 29º-30º, nuestro sistema de aire acondicionado estaría encendiéndose y apagándose continuamente, con el gasto energético que ello conllevaría. Si estuviese regulado por lógica difusa, esos 30º no serían ningún umbral, y el sistema de control aprendería a mantener una temperatura estable sin continuos apagados y encendidos.

También está la indecisión de decantarse bien por los expertos o bien por la tecnología (principalmente mediante redes neuronales) para reforzar las reglas heurísticas iniciales de cualquier sistema de control basado en este tipo de lógica.



</doc>
<doc id="6584" url="https://es.wikipedia.org/wiki?curid=6584" title="Tuna">
Tuna

Una tuna o "estudiantina" es una agrupación o hermandad de estudiantes universitarios que, portando la vestimenta antigua de la universidad, a imagen de los antiguos goliardos se caracterizan por cantar, tocar y viajar por el mundo gracias a estas habilidades —a pesar de que no todos o muy pocos sean músicos de profesión— o interpretan temas musicales del folclore hispano, entendido como ibérico, hispanoamericano y filipino, haciendo uso generalmente de instrumentos de cuerda y percusión.

La tuna es una antigua tradición que surgió en España, y posteriormente gracias a su carácter viajero, se extendió a diversas partes de Europa, como Portugal y Holanda, y en América Latina en países como México, Guatemala, Colombia, Perú, Chile, etc. llegó a finales del siglo XIX a través de tunos españoles, concretamente madrileños.

Para algunos su origen se ubica entre el siglo XIII o XIV con los continuadores de la tradición goliarda: en la edad media era un tipo de clérigo itinerante que aprovechaba la tradición de hospedaje de los monasterios para vivir sin trabajar destacándose por su predilección por la música, la bebida y la comida. Otro origen, no excluyente, sería el de los estudiantes pobres que vivían de la sopa boba. Estos sopistas se valdrían de sus habilidades musicales para cubrir sus estudios y necesidades. Sea cual fuere el origen, dejarían con el tiempo una huella que se refleja ya en el siglo de oro como estereotipo del estudiante de carácter alegre y pícaro que podemos encontrar, por ejemplo, en el entremés cervantino de "La cueva de Salamanca".

Existen muchas versiones del origen de la palabra «tuna», para algunos esta deriva de la palabra “"tunar o ‘correr la tuna’," "que significa: llevar una vida viajera, vagabunda, tocando y cantando." […] "también se [cree que] deriva de la expresión francesa Roi de Thunes (Rey de Túnez), un apelativo utilizado para designar a líderes de vagabundos,"” para otros deriva de la palabra atún y hacen esta similitud de los tunantes por “la naturaleza migratoria de estos peces y el carácter ambulatorio de los Tunos”. Según esta teoría, "Tunos" serían los trabajadores estacionales que se desplazaban hacia el sur de España buscando trabajo, siendo éste proporcionado por la temporada del atún del Mediterráneo. Estos trabajadores de temporada pudieron haber inspirado a los estudiantes a llevar una vida errante.

Así mismo también se cree que la palabra tuna proviene de "tunante", que era una palabra despectiva referida a esos estudiantes nocheriegos que hacían ruido por las noches, que por uso derivó en "tuna". Esta teoría se apoya en el carácter mendicante de los "sopistas", estudiantes, no necesariamente pobres, que tras dilapidar sus mesadas, sobrevivían a expensas de la "sopa boba", distribuida gratuitamente en ese tipo albergues, de modo que, (he aquí la conexión) debieron de llamarles "tunos".

Otras teorías más ampliamente difundidas procuran situar el origen en el latín "tonare" (sonido), aunque esta evolución es contraria a la legislación de los cambios fonéticos del latín a las lenguas ibéricas. Sin embargo, existen otras teorías.

La tuna, en sus albores, la constituían estudiantes que, debido a sus escasos recursos, tenían que cantar o tocar de lugar en lugar para poder ganarse la vida, o simplemente, para sustentarse durante el viaje de vuelta a sus casas cuando llegaban las vacaciones (en algunos casos). De ahí que esa actividad se designe con un verbo específico: "tunar", o "correr la tuna".Siendo una tradición íntimamente ligada a las universidades, las tunas mantienen vivas las costumbres heredadas de los estudiantes universitarios del siglo XIII. Alfonso X el Sabio se refirió a los tunos como juglares, en su "Código de las Siete Partidas" al escribir: "Esos escolares que trovan y tañen instrumentos para haber mantenencia". Coetánea suya, también lo hizo la obra "Razón de amor con los denuestos del agua y el vino", describiendo las cintas que aún penden sobre la capa del tuno: una por cada conquista amorosa, una por cada mujer.

El Arcipreste de Hita, en "El Libro de buen amor", subraya su carácter mendicante.
La tuna en sus orígenes aglutinaba a aquellos estudiantes que por su condición económica no podían costearse su estancia en la universidad, y trovaban por las fondas y mesones para conseguir algo de dinero y un plato de sopa con los que mantenerse. Por esta razón se les conocía como "sopistas", y se decía que vivían de la sopa boba.

Para tales menesteres portaban guitarras y bandurrias, y cantaban coplas populares. También se servían de sus habilidades musicales para enamorar a las doncellas que pretendían. Constancia de ello queda en la primera referencia escrita que hay sobre las tunas, que se encuentra en el archivo de la Universidad de Lérida, y en la que se prohíbe a los estudiantes hacer rondas nocturnas bajo pena de confiscarles los instrumentos.

No puede hablarse de tunos, hasta 1538, año en que los sopistas se acogieron a las viviendas benéficas que les ofrecía la Instrucción para bachilleres de pupilos. A partir de ese momento, comenzaron a cantar sin que en ello les fuera la supervivencia. Porque, entonces, los ya ex sopistas, en calidad de estudiantes veteranos, se hicieron servir como escuderos por los "bobos", "pardillos" o estudiantes nuevos, a los que supuestamente debían apoyar, según la norma, a cambio de legarles su gaya ciencia musical.

Lo cuenta el "Guzmán de Alfarache", haciendo hincapié en el estatus de estudiante rico que así alcanzaron los otrora sopistas. Luego, en "El Buscón" de Francisco de Quevedo, se habla de las bromas que les aguantaban los estudiantes novatos, hasta cumplir el meritoriaje que les terminara equiparando a ellos.

El tuno mendicante casi desaparece de la escena española merced a la abolición de la obligatoriedad en el uso del traje talar (traje de estudiante), viéndose constreñidos los estudiantes a colgar manteos y tricornios y a utilizar ropas de gentes, imposibilitándose la identificación de quienes corren la tuna como pertenecientes a la corporación escolar en el año de 1835, y posteriormente a mediados de siglo durante la regencia de María Cristina, que permite la libre asociación, se crean asociaciones de músicos y artistas entre las que sobresalen las "estudiantinas", grupos musicales a la batuta de un director, con un formato de número musical que fue todo un éxito en la época, haciendo que estudiantinas como la "Figaro" trascendiera fronteras y continentes.

A imagen de estas estudiantinas, se recrean en las universidades españolas las primeras tunas como las vemos hoy, que evocan las otrora comparsas de estudiantes que con sotana y manteo raído recorrían ciudades y campos, pero ahora con el traje y formato musical de la estudiantina, multiplicándose sus tradicionales galanteos y rondas nocturnas.Actualmente el asociacionismo en la universidad se ha divesificado en extremo, lo que ha hecho que las asociaciones de más tradición como el coro, la tuna y el teatro universitario pierdan estudiantes, foco y respaldo en favor de otras más recientes como las de cooperación internacional, cineforum y juegos rol. Sin embargo perdura en muchas ciudades el hecho de realizar encuentros de estudiantinas como un elemento propio y algunas estudiantinas realizan recitales anuales en sus respectivas universidades con respaldo institucional.

Pese a la antigüedad de la institución o, quizá, debido a ella, la tuna despierta en la actualidad opiniones encontradas. Partidarios y detractores esgrimen distintos argumentos a favor y en contra de su existencia. Entre los argumentos de sus partidarios están que la tuna es una institución "simpática", un grupo cohesionado que defiende valores como la hermandad, la lealtad, la defensa de las tradiciones estudiantiles, que celebra la alegría de la juventud, la despreocupación estudiantil y el amor por la música y la sana diversión.

Una de las costumbres que ha levantado más polémica es la costumbre dentro de las tunas de realizar novatadas. Aunque con variaciones entre las distintas tunas, en general, los nuevos miembros de la tuna son considerados indignos de portar el traje y la beca hasta haber demostrado distintos grados de capacidad o pericia tanto musical como en los usos y forma de comportarse de los tunos veteranos. El periodo de tiempo que los novatos y pardillos (un pardillo posee traje pero no beca) deben pasar siéndolo varía, dependiendo de su velocidad de aprendizaje.

Durante ese tiempo son considerados aprendices, es por ello por lo que están al servicio de los veteranos y excluidos de "derechos", asimismo pueden ser objeto de novatadas aleatorias a capricho de cualquiera de los miembros más veteranos (siempre con el objetivo de que aprendan las habilidades propias de la tuna de esas novatadas. También es tradición que novatos y pardillos traten de librarse de ellas, hacer que otro las cumpla y/o cualquier otro menester que les aligere la carga, ayudándose mutuamente aunque acaben de conocerse, pues esto es lo que hace hermandad). Transcurrido ese periodo, los novatos que ameriten acceder al grado de veterano y portar la beca suelen pasar por un rito de paso en forma de festejo privado en el que son sometidos a diversas "bromas" más o menos vejatorias y humillantes, o que obligan al pardillo en cuestión a poner en práctica sus habilidades adquiridas en su formación. A la finalización de éste, al nuevo miembro de pleno derecho de la tuna se le impone la beca distintiva que le acredita como tal. En algunas tunas el rito de paso se realiza como condición inicial para entrar en la tuna y acceder a la condición de novato. En otras, existen grados de "impericia": pardillo, novato, etc. hasta acceder al de veterano.

Las tunas en la actualidad se suelen clasificar según la facultad universitaria a la que pertenecen sus miembros: así tendremos "Tuna de Derecho", "Tuna de Medicina", "Tuna de Peritos", etcétera. En casos de universidades con menor antigüedad o tradición, suele haber una sola tuna Universitaria, que englobe a estudiantes de varios estudios, e incluso en ciudades con varios "distritos universitarios", crean una Tuna de Distrito (también con estudiantes de varios estudios).

La indumentaria del tuno está compuesta de capa o manteo, jubón, camisa, calzas, abullonadas o cervantinas sobre éstas, zapatos o botas y finalmente la beca que es lo que identifica a cada tuna y varía su color de acuerdo a la facultad a la que pertenezca según la tradición española: rojo para Derecho, azul turquí para Ciencias, amarillo para Medicina, azul celeste para Filosofía y Letras, morado para Farmacia, verde para Empresariales, naranja para Economía, etcétera.

La primera representación iconográfica de un tuno se encuentra en la parte inferior del pasamanos de la escalera del Rectorado de la Universidad de Salamanca, antiguo hospital universitario, una pequeña talla que lo muestra con el bicornio decorado con cuchara y tenedor, un instrumento de cuerda que parece una bandurria y capa.

En las representaciones correspondientes del siglo XVIII, aparecen vestidos con calzón corto, jubón y capa, y bicornio. En la actualidad las tunas se dividen entre las de gregüesco o trusas y las de calzón corto."






Las becas tienen un largo suficiente para alcanzar (aproximadamente) desde la mitad superior de la espalda hasta que la punta de la uve se sitúe en el pecho. Las becas de las tunas de Valladolid son particularmente diferentes al resto, dado que son más largas (de forma que por la espalda se cruzan y llegan sus extremos hasta más abajo del cinturón del jubón) y tienen en uno de sus extremos un aro (quizá como recuerdo de un sombrero que se llevaba cosido a la beca).

Hay algunas curiosidades: la Tuna Compostelana (no lleva beca, porta cosida en el jubón una Cruz de Santiago bordada) y la Tuna de Distrito de Granada (lleva al pecho el escudo de la Universidad de Granada), la conocida como Tuna de Peritos de Sevilla (o Tuna de Peritos e Ingenieros Técnicos Industriales) lleva un fajín blanco impuesto en honor a su anual acto de ofrenda a la Virgen de la Inmaculada Concepción (acto que forma parte de las fiestas tradicionales de Sevilla, en la noche del 7 al 8 de diciembre).Hay otra indumentaria de tuna propia de las tunas portuguesas. Llevan el traje de estudiante negro y la capa negra. En vez de cintas dejan que les rasguen la capa personas que son importantes para ellos.

"Los instrumentos actuales de la tuna son principalmente los llamados de pulso y púa, los populares españoles: laúd, guitarra y bandurria y la pandereta, y antes la vihuela, de péñola o de arco, antecesora del violín. Estos instrumentos aparecen en el "Libro del Buen Amor.""La guitarra que se utiliza como acompañamiento armónico de la melodía. La melodía la crean las voces y los cantos, que se apoyan en la bandurria (primera voz) y el laúd español (segunda voz). El contrabajo se ha convertido en la actualidad en un instrumento habitual en muchas tunas, complementado armónicamente el conjunto de cuerdas. No nos podemos olvidar, sin embargo, de otro instrumento característico de la música estudiantil: la pandereta, así como también podemos encontrar al pandero y las castañuelas.

Además de los instrumentos básicos, sin los cuales no se podría crear música de tuna, utilizan muchos otros que le confieren una riqueza muy especial. Estos han llegado gracias a la fusión con la cultura de muchos pueblos, y también muchas veces por los propios instrumentos regionales de las localidades de origen de la tuna. Entre los más destacados encontramos el timple canario, la bandola y el charango. También suelen usarse en las tunas de todo el mundo el cuatro venezolano, tres cubano, cuatro puertorriqueño y el acordeón para acrecentar la variedad sonora. En la actualidad también se han añadido instrumentos de viento tales como la quena y la flauta traversa. En conclusión la tuna recoge la cultura musical de cada país y/o región que visita y la adopta en su repertorio.

Las Estudiantinas portuguesas suelen usar mandolina en vez de bandurria y laúd; también incluyen tradicionalmente el bandoneón y la guitarra portuguesa.

La mayoría de las tunas y estudiantinas de México, adaptan dos instrumentos básicos del mariachi como son: la vihuela mexicana y el guitarrón, esto con el fin de enriquecer más aún las melodías. Cabe mencionar que algunas estudiantinas parroquiales, también utilizan el acordeón para armonizar la pieza musical. Incluso algunas tunas y estudiantinas de México hacen uso de violines cuando interpretan piezas de folclore mexicano como las canciones de mariachi o algunos sones y huapangos, sobre todo, las tunas y Estudiantinas de los estados de Hidalgo y Veracruz.""El repertorio de la Tuna es sobre todo canción estudiantil, empezando por el género propiamente que es la canción de ronda, que aparece desde las primeras canciones líricas europeas, que tiene su versión en la música popular, donde siempre se encuentran canciones de serenata y los pasacalles etc. con sus elementos que vemos en otro lugar. Buena parte del repertorio tunantesco, las más populares, no tienen autor conocido.""Después, su cancionero se ha enriquecido con temas populares españoles, así como pasodobles y vals, y folclore regional, como jotas, isas, malagueñas, etc. También, por su carácter viajero el Tuno ha engrosado su repertorio con canciones de todo el mundo en miles de idiomas, siempre para poder sorprender y alegrar cada fiesta en la que se encuentren.

Algunas tunas son cantautoras de temas propios. Por ejemplo, la Tuna de Derecho de Sevilla es cantautora de temas como Rumor en los Balcones, Las Plazas de Mi Sevilla, Tus Ojos o Maicena, entre otras.




</doc>
<doc id="6589" url="https://es.wikipedia.org/wiki?curid=6589" title="Quevedo (desambiguación)">
Quevedo (desambiguación)

Quevedo puede referirse a Francisco de Quevedo, célebre escritor del Siglo de Oro español, o también a las siguientes personalidades:


Además, puede hacer referencia a:

</doc>
<doc id="6593" url="https://es.wikipedia.org/wiki?curid=6593" title="Vim">
Vim

Vim (del inglés "Vi IMproved") es una versión mejorada del editor de texto vi, presente en todos los sistemas UNIX. 

Su autor, Bram Moolenaar, presentó la primera versión en 1991, fecha desde la que ha experimentado muchas mejoras. La principal característica tanto de Vim como de Vi consiste en que disponen de diferentes modos entre los que se alterna para realizar ciertas operaciones, lo que los diferencia de la mayoría de editores comunes, que tienen un solo modo en el que se introducen las órdenes mediante combinaciones de teclas o interfaces gráficas.

Vim, como su antecesor vi, se utiliza desde un terminal en modo texto. Se controla por completo mediante el teclado. Esto es en parte a causa de que Vi fue desarrollado a mediados de la década de 1970, cuando los terminales se comunicaban con un ordenador principal (host) mediante una conexión serie, que no era muy rápida (20 Kbps). Las limitaciones de los terminales de la época dieron lugar al concepto de diferentes modos, idea que ha resultado muy provechosa desde entonces. Vim es casi 100 % compatible con vi, aunque tiene muchas mejoras e incluso cuenta con versiones dotadas de interfaz gráfica y menús que pueden operarse mediante el ratón (gvim o kvim), así como una versión simplificada, evim que se comporta como un editor sin diferentes modos.

Hay versiones de Vim disponibles para muchos sistemas operativos y se puede encontrar en casi cualquier sistema GNU/Linux y en todos los sistemas *BSD, donde en muchas ocasiones se puede ejecutar a través de la orden "vi", que invoca a Vim a través de un enlace simbólico o un alias. Cuando se inicia vim, lo hace en modo comando, y muestra la versión por pantalla.

Cuando Bram Moolenar compró una computadora Amiga a finales de la década de 1980, quería seguir usando el editor de Unix al que se había acostumbrado, pero los clones de vi disponibles para Amiga no le convencían. Así que en 1988 partió del clon de vi Stevie 1.0 como base para empezar a desarrollar Vim. En un principio le llamó "vi IMitation" (imitación de vi), pues al principio su objetivo principal era emular las funcionalidades de que vi disponía en su nuevo sistema. En 1991 apareció la versión 1.14 en "Fred Fish disk #591", una colección de software libre para Amiga. La versión 1.22 fue la primera que apareció con versiones para Unix y MS-DOS. Por aquellos tiempos el acrónimo que le daba nombre ya había cambiado a "Vi IMproved". 

En los años siguientes Vim experimentó grandes mejoras. Se dio un paso importante al incorporar ventanas en la versión 3.0 (1994) (Figura 1). Con vi se pueden tener varios ficheros abiertos en la misma sesión, pero sólo se puede ver uno a la vez; las ventanas de Vim permiten verlos simultáneamente. Desde la versión 4.0 (1996) está disponible una interfaz gráfica de usuario (GUI en inglés), que empezó a desarrollar Robert Webb. Desde la versión 5.0 (1998) Vim dispone de resaltado de sintaxis (Figura 2).


Vim dispone de una excelente documentación, que se rige por la máxima "Una característica no documentada es una característica inútil". La documentación, en formato texto, es muy amplia y fácil de entender. El usuario accede mediante una búsqueda a la descripción de varias funcionalidades que pueden solucionar su problema. A través del resaltado de la sintaxis propia de la ayuda de Vim se resaltan las palabras clave. (Figura 3) Mediante combinaciones de teclas ejecutadas cuando el cursor está sobre una palabra clave se puede navegar por la ayuda, volviendo atrás en caso necesario. En la versión gráfica también se puede utilizar el ratón para este propósito.
La orden codice_1 es importante, pues permite al usuario buscar una palabra en los textos de la ayuda, sin que sea necesario que se trate de una palabra clave (Figura 4). Completan la ayuda una versión en HTML disponible para su consulta en internet y una extensa lista de preguntas frecuentes (FAQ).

Vim es un editor hecho por programadores para programadores. Para facilitar la programación, Vim dispone de un modo "editar, compilar, corregir". De la misma forma que los entornos de desarrollo integrados, puede editar el código fuente además llamar a un compilador externo, e interpretar sus resultados. Si hay errores de compilación, éstos se muestran en una ventana. Los mensajes de error dirigen al usuario a la zona en la que se han encontrado para poder así corregirlos. Entonces vuelve a empezar el ciclo "editar, compilar, corregir" y, si es necesario, corregir nuevos errores. El trabajo del programador también se ve facilitado por el resaltado de sintaxis y la funcionalidad de plegado de código (véase ':help quickfix').

Comparar dos (o más) versiones de un fichero es una tarea frecuente para algunos usuarios. Vim ofrece una solución simple, las dos versiones diferentes del fichero se muestran en dos ventanas contiguas en las que se resaltan las diferencias (Figura 5). De esta forma se pueden ver coloreadas las diferencias intercaladas en las versiones. Por ejemplo, las eliminaciones aparecen en rojo y las nuevas inserciones en violeta. 

Hay órdenes que permiten situar el cursor en las diferencias siguientes y anteriores ([c y ]c), además de volcar estas diferencias hacia el otro archivo (do y dp). 

Vim dispone de un lenguaje interpretado, o de scripting, para programar nuevas funcionalidades, mediante el que se pueden automatizar operaciones particulares demasiado complejas para realizarlas con una macro. Los scripts pueden ser leídos e interpretados mediante la orden codice_2.

Vim es un "editor modal", lo que significa que se puede trabajar en diferentes modos para realizar una tarea determinada. Para ver en qué modo se encuentra Vim se debe de tener activada la opción codice_3. A continuación se describen los seis modos de Vim. Los tres primeros son los modos del vi original. Los cinco modos adicionales no deben entenderse por separado, sino en combinación con el modo base. Véase también la ayuda de Vim a este respecto: ':help vim-modes'.

Vim empieza en modo comando, también conocido como modo normal. En este modo se pueden emplear combinaciones de teclas para, por ejemplo, copiar líneas y trabajar en el formato del texto. Éste es el "modo central", desde el que se cambia a los otros modos. Si no se sabe qué se está haciendo, pulsando dos veces la tecla Escape siempre se puede volver al modo normal. Si ya se estaba en modo normal y tanto la configuración de Vim como la del terminal lo permiten, Vim emite un pitido.

En modo inserción cuando se pulsan las teclas se edita el texto como en otros editores. Se puede cambiar del modo comandos al modo inserción pulsando la tecla i. Hay un gran abanico de comandos para pasar al modo inserción, que difieren sustancialmente, pues permiten por ejemplo editar al final de la línea, en un punto concreto del texto, editar borrando una palabra, entre muchas otras. Un usuario experto puede sacar un gran provecho de la existencia de esta variedad de órdenes.

En el modo inserción todas las teclas tienen alguna función además de la mera inserción, que se activan pulsando simultáneamente las teclas Tecla control o Mayúsculas. La tecla Esc es muy importante en modo inserción, pues permite cambiar de modo inserción a modo comandos.

Cambiando al modo comandos para realizar ciertas tareas se incrementa en gran medida la eficiencia en la edición, y se puede aprovechar la potencia completa de Vim.

A este modo se accede pulsando la tecla dos puntos :. Tras los dos puntos se pueden introducir órdenes complejas, como por ejemplo buscar y reemplazar con expresiones regulares. Pulsando la tecla Esc se puede volver al modo órdenes. Las búsquedas se pueden realizar con la orden / (hacia adelante) y ? (hacia atrás). También se pueden filtrar líneas mediante !.

Este modo es una mejora respecto a vi. Mediante unas ciertas combinaciones de teclas en combinación con las teclas de movimiento del cursor, se puede marcar un área de texto, ya sea un grupo de líneas o un bloque. Una vez se tiene el texto marcado se pueden usar órdenes del modo comandos para manipularlo. Las operaciones que se pueden realizar en este modo son más simples que las del modo comandos.

Este modo empieza como el modo visual, pues hay que seleccionar un bloque de texto. Tras la selección, se puede cambiar al modo selección mediante Control-G. Una vez en el modo, si se pulsa una tecla imprimible, el texto seleccionado se borra, se termina el modo selección y aparece el símbolo correspondiente a la tecla pulsada. La selección se puede extender pulsando mayúsculas y las teclas de flechas, el comportamiento habitual en los programas de Microsoft Windows. Este modo se puede finalizar pulsando la tecla Escape.

Este modo se asemeja al modo línea de órdenes, con la diferencia de que tras la ejecución de una orden no se vuelve al modo comandos. Se entra en este modo pulsando Q y se termina con vi. En este modo Vim imita al editor de UNIX "ex", que manipulaba el texto línea a línea debido a las limitaciones de la época, en lugar de editar toda la página.

gVim es una versión gráfica del editor de textos Vim. gVim funciona con las librerías gtk. Mantiene las funcionalidades del Vim, y añade menús y un entorno gráfico (funciona "fuera" de la consola/terminal). 

Su principal ventaja, para los nuevos usuarios, los menús desplegables implican una curva de aprendizaje algo menos dura. Su principal desventaja es que no está instalado por defecto en todos los sistemas (por ejemplo, no está en los servidores sin entorno gráfico).

Una virtud de Vim es que se puede ejecutar en muchos sistemas operativos. Esto es importante para usuarios, como por ejemplo administradores de sistemas que deben trabajar en muchas plataformas distintas. Se puede ejecutar en los siguientes sistemas operativos: AmigaOS, Atari MiNT, BeOS, DOS, GNU/Linux, Mac OS, NextStep, OS/2, OSF, RISC OS, IRIX, Unix (muchas variedades, como por ejemplo BSD, AIX y HP-UX), VMS, y Windows 3.x/95/98/ME/2000/NT/XP.

Las críticas se aplican a Vi y Vim por igual, pues están basados en el mismo concepto y sus modos de operación son completamente diferentes de la mayoría los editores convencionales actuales. Sobre todo conciernen a la separación de las operaciones en distintos modos y la necesidad de aprender numerosas combinaciones de teclas, sin las que no es posible trabajar de forma eficiente.
Actualmente la falta de empleo del ratón como complemento extra a las combinaciones de teclas se considera como un anacronismo, pues puede llevar al usuario ocasional a la frustración. Sólo tras un aprendizaje prolongado se consigue aumentar la productividad.

La mayoría de los usuarios que usan Vim aseguran que este editor incrementa su productividad comparándolo con editores más simples una vez se ha superado la curva de aprendizaje. Las combinaciones de teclas se pueden memorizar empleando métodos mnemotécnicos, pues guardan relación con palabras inglesas. La complejidad intrínseca de aprender las instrucciones se ve recompensada por la mejora en la eficiencia. Los usuarios expertos pueden, usando unas pocas combinaciones de teclas, copiar texto, formatearlo u ordenarlo de muchas formas diferentes, que sólo se pueden realizar en la mayoría de editores mediante operaciones considerablemente más complejas. Basta con un poco de experiencia para notar que las combinaciones de instrucciones que permiten ediciones de texto complejas se facilitan con Vim. Por otra parte las nuevas versiones permiten emplear el ratón e incorporan menús gráficos, que facilitan trabajar con Vim de una forma similar a la de otros editores. A menudo se instala como editor base por su pequeño tamaño y su rapidez en plataformas con recursos limitados basadas en UNIX.

Vim fue el ganador de los "Readers' Choice Awards" de "Linux Journal" en la categoría "Favorite Text Editor" desde 2001 a 2005 y obtuvo el "Slashdot Bernie Award" como el "Mejor editor de texto Open Source" así como en 1999 el "Linuxworld Editors' Choice Award".

Vim se distribuye bajo una licencia Charityware (del inglés "charity": caridad) compatible con Licencia pública general de GNU. y por sus siglas en inglés GPL (General Public License). Esto significa que Vim se rige por las mismas condiciones, pero anima a los usuarios a realizar donaciones para los niños huérfanos de Uganda a través de la organización ICCF Holland.

Vim es desarrollado por Bram Moolenaar y muchos voluntarios. La página de ayuda de Vim de la versión actual menciona a más de 50 colaboradores. Además hay un gran número de personas que han ayudado, no sólo en el desarrollo del software, sino también portándolo a otros sistemas operativos, probando nuevas versiones, reportando bugs, redactando documentación y traduciendo el sistema de ayuda. También contestan a las preguntas de los usuarios, adaptando Vim a otros proyectos y muchas otras cosas.

Todos los usuarios pueden aportar realizando scripts o dando consejos. Hay una lista de correo muy activa, útil tanto para nuevos usuarios como para los experimentados en la que se dan respuestas rápidas y competentes.


"<vim@vim.org>" – véase también: Instrucciones de la Lista de Correo




</doc>
<doc id="6594" url="https://es.wikipedia.org/wiki?curid=6594" title="Centauro">
Centauro

En la mitología griega, el centauro (en griego Κένταυρος "pitoas", ‘matador de toros’, ‘cien fuertes’, plural Κένταυρι "Kentauri"; en latín "Centaurus/Centauri") es una criatura con la cabeza, los brazos y el torso de un humano y el cuerpo y las patas de un caballo. Las versiones femeninas reciben el nombre de centáurides.

Vivían en las montañas de Tesalia y se les consideraba hijos de Centauro (el hijo de Ixión y Néfele) y algunas yeguas magnesias, o de Apolo y Hebe.

Los centauros son muy conocidos por la lucha que mantuvieron con los lápitas, provocada por su intento de raptar a Hipodamía el día de su boda con Pirítoo, rey de los lapitas y también hijo de Ixión. La riña entre estos primos es una metáfora del conflicto entre los bajos instintos y el comportamiento civilizado en la humanidad. Teseo, un héroe y fundador de ciudades que estaba presente, inclinó la balanza del lado del orden correcto de las cosas, y ayudó a Pirítoo. Los centauros huyeron. (Plutarco, "Teseo", 30; Ovidio, "Las metamorfosis" xii. 210; Diodoro Sículo iv. 69, 70.) Escenas de la batalla entre los lápitas y los centauros fueron esculpidas en bajorrelieves en el friso del Partenón, que estaba dedicado a la sabia Atenea.

Como la titanomaquia, la derrota de los titanes por los dioses olímpicos, las contiendas con los centauros representan la lucha entre la civilización y el barbarismo y es conocida como centauromaquia.

El personaje general de los centauros es el de seres salvajes, sin leyes ni hospitalidad, esclavos de las pasiones animales. Dos excepciones a esta regla son Folo y Quirón, que expresaban su «buena» naturaleza, siendo centauros sabios y amables.

Entre los centauros, el tercero con una identidad individual es Neso. El episodio mitológico del centauro Neso raptando a Deyanira, la prometida de Heracles, también proporcionó a Giambologna (1529-1608), un escultor flamenco que trabajó en Italia, espléndidas oportunidades de concebir composiciones con dos formas en violenta interacción. Giambologna realizó varias versiones de Neso raptando a Deyanira, representados por los ejemplos conservados en diversos museos. Sus seguidores, como Adriaen de Vries y Pietro Tacca, continuaron esculpiendo incontables repeticiones del tema. Cuando Albert-Ernest Carrier-Belleuse abordó la misma composición de formas en el siglo XIX, la tituló "Rapto de Hipodamía".

En antiguas vasijas pintadas áticas los centauros eran representados como seres humanos de frente, con el cuerpo y las patas traseras de un caballo sujetos a la espalda. Posteriormente, fueron hombres sólo hasta la cintura. La batalla con los lápitas y la aventura de Heracles con Folo (Apolodoro, ii. 5; Diodoro Sículo, iv. li) son temas favoritos del arte griego.

Muchas leyendas sobre los centauros sostienen que son criaturas muy inconstantes, que miran con frecuencia al cielo para determinar sus destinos. Son grandes astrólogos y muy aficionados a la adivinación.

El antropólogo y escritor Robert Graves especuló con que los centauros de la mitología griega fueran una reminiscencia de una secta prehelénica que considerase al caballo un tótem. Una teoría parecida aparece en "El toro del mar" de Mary Renault.

Otras fuentes especulan con la idea de que los centauros provengan de la primera reacción de una cultura que no conociese la equitación, como el mundo egeo minoico, hacia los nómadas que sí montaban a caballo. La teoría señala que tales jinetes parecerían mitad hombres mitad caballos. La cultura de doma y monta de caballos surgió primero en las estepas del sur de Asia Central, quizá aproximadamente en la actual Kazajistán.

Otra posible etimología relacionada puede ser "matador de toros" .Algunos dicen que los griegos tomaron la constelación Centaurus, y también su nombre "toro penetrante", de Mesopotamia, donde se simboliza al dios Baal, que representa la lluvia y la fertilidad, y luchando y perforando con sus cuernos el demonio Mot que representa la sequía de verano. En Grecia, la constelación de Centaurus fue observada por Eudoxus de Cnidus en el cuarto siglo a.C. y por Aratus en el tercer siglo

Los centauros han aparecido muchas veces y en muchos lugares en obras de ficción modernas.

Aunque se dice que la palabra griega "kentauros" está compuesta por un único morfema —quizá no griego en su origen—, el sufijo "-tauro" ha sido inventado por escritores y diseñadores de juegos a finales del siglo XX para otros híbridos animal-humanos fantásticos.

Aunque las mujeres centauros, llamadas «centáurides» (κενταύριδες), no son mencionadas en la antigua literatura ni en el arte griego, aparecen ocasionalmente en la antigüedad tardía. Un mosaico macedonio del siglo IV a. C. que se halla actualmente en el museo arqueológico de la ciudad de Pella es uno de los primeros ejemplos de la presencia de centáurides en el arte. El autor romano Ovidio en sus "Metamorfosis" menciona a una centáuride llamada Hilonoma, que se suicidó cuando su amante Cílaro murió durante la guerra contra los lapitas.

En la descripción de una pintura que vio en Neápolis, el retórico griego Filóstrato el Viejo presenta a las centáurides como hermanas y esposas de los centauros masculinos que vivían en el monte Pelión con sus hijos.



Otras criaturas híbridas aparecen también en la mitología griega, siempre con alguna conexión liminal que enlaza la cultura helénicas con otras arcaicas o extranjeras:

Otras criaturas similares:





</doc>
<doc id="6598" url="https://es.wikipedia.org/wiki?curid=6598" title="Sofista">
Sofista

El término sofista, del griego "sophía" (σοφία), «sabiduría» y "sophós" (σοφός), «sabio», es el nombre dado en la Grecia clásica al que hacía profesión de enseñar la sabiduría. "Sophós" y "Sophía" en sus orígenes denotaban una especial capacidad para realizar determinadas tareas como se refleja en la "Ilíada" (XV, 412). Más tarde se atribuiría a quien dispusiera de «inteligencia práctica» y era un experto y sabio en un sentido genérico. Sería Eurípides quien le añadiría un significado más preciso como «el arte práctico del buen gobierno» (Eur. I.Á.749) y que fue usado para señalar las cualidades de los Siete Sabios de Grecia. Sin embargo, al transcurrir el tiempo hubo diferencias en cuanto al significado de "sophós": por una parte, Esquilo denomina así a los que dan utilidad a lo sabido, mientras que para otros es al contrario, siéndolo quien conoce por naturaleza. A partir de este momento se creará una corriente, que se aprecia ya en Píndaro, que da un cariz despectivo al término "sophós" asimilándolo a «charlatán».

Ya en la "Odisea", Ulises es calificado de "sophón" como «ingenioso». Por el contrario, Eurípides llama a la "sophía" «listeza» y al "sophón" «sabiduría», tratando con ello de diferenciar la intensidad y grado de conocimiento de las cosas que tienen respectivamente los hombres y los dioses.

Los sofistas eran pensadores que desarrollaron su actividad en la Atenas democrática del siglo V a. de C. Los filósofos de la naturaleza, los presocráticos, habían elaborado diferentes teorías para explicar el cosmos. Los sofistas y Sócrates van a cambiar el objeto de la filosofía. Ahora, el tema de reflexión es el hombre y la sociedad. Como los sofistas eran viajeros, conocían diferentes culturas, totalmente distintas a la griega. Por eso se plantearon problemas referidos a las costumbres y las leyes. ¿Son las costumbres y leyes un simple acuerdo, una convención, o son naturales? Así surgió la idea de relativismo.

Los sofistas eran maestros que iban de ciudad en ciudad enseñando a ser buenos ciudadanos y a triunfar en la política. El arte de hablar en público, la retórica, era esencial en la democracia griega, donde los ciudadanos participaban constantemente. Las enseñanzas de los sofistas tenían un fin práctico, saber desenvolverse en los asuntos públicos. Fueron los primeros pensadores que cobraron dinero por sus enseñanzas. Unos de los principales sofistas fue Protágoras (480-410 a. de C.).

El verbo "sophídsesthai", «practicar la "sophía"», sufrió una evolución similar al terminar por entenderse como «embaucar». La derivación "sophistés" se dio a los Siete Sabios en el sentido de «filósofos» y así llama Heródoto a Pitágoras, a Solón, y a quienes fundaron el culto dionisiaco. También se llamaba así a los "mousike" y a los poetas y, en general, a todos los que ejercían una función educadora. El uso peyorativo empezó a tomar forma en el siglo V a. C., coincidiendo con la extensión del uso del término a los prosistas. El momento coincide con un incremento de las suspicacias de los atenienses hacia los que mostraban una mayor inteligencia. Isócrates denostaba que el término «hubiera caído en deshonor» y Sófocles lo atribuye al hecho de que los educadores y maestros recibieran una remuneración por su trabajo. Esta es la tesis más extendida en la actualidad.

No obstante, era aceptado en la Grecia Antigua que los poetas cobrasen por sus servicios. El desprecio con el que los sofistas eran tratados en ocasiones no nacía del hecho mismo de recibir remuneración, sino de hacerlo, sobre todo, por la formación en la llamada "areté", el arte de la política y la ciudadanía, que incluía todas las técnicas persuasivas para hacerse un lugar en la administración de la "polis".

Platón criticaba a los sofistas por su formalismo y sus trampas dialécticas, pretendiendo enseñar la virtud y a ser hombre, cuando nadie desde un saber puramente sectorial, como el del discurso retórico, puede arrogarse tal derecho.

La primera exigencia de esa "areté" era el dominio de las palabras para ser capaz de persuadir a otros. «Poder convertir en sólidos y fuertes los argumentos más débiles», dice Protágoras. Gorgias dice que con las palabras se puede envenenar y embelesar. Se trata, pues, de adquirir el dominio de razonamientos engañosos. El arte de la persuasión no está al servicio de la verdad sino de los intereses del que habla. Llamaban a ese arte «conducción de almas». Platón dirá más tarde que era «captura» de almas.

Según algunos autores, no eran, pues, propiamente filósofos. Para quienes son de esa opinión, tenían sin embargo en común con los filósofos una actitud que sí puede llamarse filosófica: el escepticismo y relativismo. No creían que el ser humano fuese capaz de conocer una verdad válida para todos. Cada quien tiene «su» verdad.

Por el contrario, hay quien sostiene que sí lo eran, y que las ácidas críticas de Platón corresponden a una disputa por un mismo grupo de potenciales discípulos y a sus diferencias políticas y filosóficas.

De Aristóteles provendrá también el sentido peyorativo: sofista es quien utiliza del sofisma para razonar. Los más destacados miembros de la sofística fueron: Protágoras, Gorgias, Hipias, Pródico, Trasímaco, Critias y Calicles.

Frente a la tradición filosófica, algunos autores a partir del siglo XX han tratado de reivindicar la importancia filosófica de los sofistas. Por ejemplo, Giorgio Colli ha destacado que no es menor el rigor lógico de Gorgias de Leontinos que el de Platón. Además, plantea la hipótesis de que tal vez el sofista fuera el creador de la refutación por reducción al absurdo. También Michel Onfray ha tratado de destacar el papel de los sofistas en la filosofía griega.










</doc>
<doc id="6599" url="https://es.wikipedia.org/wiki?curid=6599" title="27 de abril">
27 de abril

El 27 de abril es el 117.º (centésimo decimoséptimo) día del año en el calendario gregoriano y el 118.º en los años bisiestos. Quedan 248 días para finalizar el año.








</doc>
<doc id="6604" url="https://es.wikipedia.org/wiki?curid=6604" title="Universo">
Universo

El universo es la totalidad del espacio y del tiempo, de todas las formas de la materia, la energía, el impulso, las leyes y constantes físicas que las gobiernan. Sin embargo, el término también se utiliza en sentidos contextuales ligeramente diferentes y alude a conceptos como cosmos, mundo o naturaleza. Su estudio, en las mayores escalas, es el objeto de la cosmología, disciplina basada en la astronomía y la física, en la cual se describen todos los aspectos de este universo con sus fenómenos.

La ciencia modeliza el universo como un sistema cerrado que contiene energía y materia adscritas al espacio-tiempo y que se rige fundamentalmente por principios causales. Basándose en observaciones del universo observable, los físicos intentan describir el continuo espacio-tiempo en el que nos encontramos, junto con toda la materia y energía existentes en él.

Los experimentos sugieren que el universo se ha regido por las mismas leyes físicas, constantes a lo largo de su extensión e historia. Es homogéneo e isotrópico. La fuerza dominante en distancias cósmicas es la gravedad, y la relatividad general es actualmente la teoría más exacta para describirla. Las otras tres fuerzas fundamentales, y las partículas en las que actúan, son descritas por el modelo estándar.

El universo tiene por lo menos tres dimensiones de espacio y una de tiempo, aunque experimentalmente no se pueden descartar dimensiones adicionales. El espacio-tiempo parece estar conectado de forma sencilla, y el espacio tiene una curvatura media muy pequeña o incluso nula, de manera que la geometría euclidiana es, como norma general, exacta en todo el universo.

La teoría actualmente más aceptada sobre la formación del universo, fue teorizada por el canónigo belga Lemaître, a partir de las ecuaciones de Albert Einstein. Lemaitre concluyó (en oposición a lo que pensaba Einstein), que el universo no era estacionario, que el universo tenía un origen. Es el modelo del Big Bang, que describe la expansión del espacio-tiempo a partir de una singularidad espaciotemporal. El universo experimentó un rápido periodo de inflación cósmica que arrasó todas las irregularidades iniciales. A partir de entonces el universo se expandió y se convirtió en estable, más frío y menos denso. Las variaciones menores en la distribución de la masa dieron como resultado la segregación fractal en porciones, que se encuentran en el universo actual como cúmulos de galaxias.

Las observaciones astronómicas indican que el universo tiene una edad de 13 730±120 millones de años (entre 13 610 y 13 850 millones de años) y por lo menos 93000 millones de años luz de extensión. 

Debido a que, según la teoría de la relatividad especial, la materia no puede moverse a una velocidad superior a la velocidad de la luz, puede parecer paradójico que dos objetos del universo puedan haberse separado 93 000 millones de años luz en un tiempo de únicamente 13 000 millones de años; sin embargo, esta separación no entra en conflicto con la teoría de la relatividad general, ya que esta solo afecta al movimiento en el espacio, pero no al espacio mismo, que puede extenderse a un ritmo superior, no limitado por la velocidad de la luz. Por lo tanto, dos galaxias pueden separarse una de la otra más rápidamente que la velocidad de la luz si es el espacio entre ellas el que se dilata.

Observaciones recientes han demostrado que esta expansión se está acelerando, y que la mayor parte de la materia y la energía en el universo son las denominadas materia oscura y energía oscura, la materia ordinaria (bariónica), solo representaría algo más del 5 % del total.

Las mediciones sobre la distribución espacial y el desplazamiento hacia el rojo ("redshift") de galaxias distantes, la radiación cósmica de fondo de microondas, y los porcentajes relativos de los elementos químicos más ligeros, apoyan la teoría de la expansión del espacio, y más en general, la teoría del Big Bang, que propone que el universo en sí se creó en un momento específico en el pasado.

En cuanto a su destino final, las pruebas actuales parecen apoyar las teorías de la expansión permanente del universo ("Big Freeze" o "Big Rip", Gran Desgarro), que nos indica que la expansión misma del espacio, provocará que llegará un punto en que los átomos mismos se separarán en partículas subatómicas. Otros futuros posibles que se barajaron, especulaban que la materia oscura podría ejercer la fuerza de gravedad suficiente para detener la expansión y hacer que toda la materia se comprima nuevamente; algo a lo que los científicos denominan el "Big Crunch" o la Gran Implosión, pero las últimas observaciones van en la dirección del gran desgarro.

Los cosmólogos teóricos y astrofísicos utilizan de manera diferente el término "universo", designando bien el sistema completo o únicamente una parte de él. Frecuentemente se utiliza el término "el universo" para designar la parte observable del espacio-tiempo o el espacio-tiempo entero.

Según el convenio de los cosmólogos, el término "universo" se refiere frecuentemente a la parte finita del espacio-tiempo que es directamente observable utilizando telescopios, otros detectores y métodos físicos, teóricos y empíricos para estudiar los componentes básicos del universo y sus interacciones. Los físicos cosmólogos asumen que la parte observable del espacio comóvil (también llamado nuestro universo) corresponde a una parte del espacio entero y normalmente no es el espacio entero. 

En el caso del universo observable, este puede ser solo una mínima porción del universo existente y, por consiguiente, puede ser imposible saber realmente si el universo está siendo completamente observado. Algunos cosmólogos creen que el universo observable es una parte extremadamente pequeña del universo «entero» realmente existente y que es imposible observar todo el espacio comóvil. En la actualidad se desconoce si esto es correcto, ya que de acuerdo a los estudios de la forma del universo, es posible que el universo observable esté cerca de tener el mismo tamaño que todo el espacio. La pregunta sigue debatiéndose.

El hecho de que el universo esté en expansión se deriva de las observaciones del corrimiento al rojo realizadas en la década de 1920 y que se cuantifican por la ley de Hubble. Dichas observaciones son la predicción experimental del modelo de Friedmann-Robertson-Walker, que es una solución de las ecuaciones de campo de Einstein de la relatividad general, que predicen el inicio del universo mediante un big bang.

El "corrimiento al rojo" es un fenómeno observado por los astrónomos, que muestra una relación directa entre la distancia de un objeto remoto (como una galaxia) y la velocidad con la que este se aleja. Si esta expansión ha sido continua a lo largo de la vida del universo, entonces en el pasado estos objetos distantes que siguen alejándose tuvieron que estar una vez juntos. Esta idea da pie a la teoría del "Big Bang"; el modelo dominante en la cosmología actual.

Durante la era más temprana del "Big Bang", se cree que el universo era un caliente y denso plasma. Según avanzó la expansión, la temperatura decreció hasta el punto en que se pudieron formar los átomos. En aquella época, la energía de fondo se desacopló de la materia y fue libre de viajar a través del espacio. La energía remanente continuó enfriándose al expandirse el universo y hoy forma el fondo cósmico de microondas. Esta radiación de fondo es remarcablemente uniforme en todas direcciones, circunstancia que los cosmólogos han intentado explicar como reflejo de un periodo temprano de inflación cósmica después del "Big Bang".

El examen de las pequeñas variaciones en el fondo de radiación de microondas proporciona información sobre la naturaleza del universo, incluyendo la edad y composición. La edad del universo desde el "Big Bang", de acuerdo a la información actual proporcionada por el WMAP de la NASA, se estima en unos 13.700 millones de años, con un margen de error de un 1 % (137 millones de años). Otros métodos de estimación ofrecen diferentes rangos de edad, desde 11 000 millones a 20 000 millones.

Hasta hace poco, la primera centésima de segundo era más bien un misterio, impidiendo a los científicos describir exactamente cómo era el universo. Los nuevos experimentos en el RHIC, en el Brookhaven National Laboratory, han proporcionado a los físicos una luz en esta cortina de alta energía, de tal manera que pueden observar directamente los tipos de comportamiento que pueden haber tomado lugar en ese instante.

En estas energías, los quarks que componen los protones y los neutrones no estaban juntos, y una mezcla densa supercaliente de quarks y gluones, con algunos electrones, era todo lo que podía existir en los microsegundos anteriores a que se enfriaran lo suficiente para formar el tipo de partículas de materia que observamos hoy en día.

Los rápidos avances acerca de lo que pasó después de la existencia de la materia aportan mucha información sobre la formación de las galaxias. Se cree que las primeras galaxias eran débiles "galaxias enanas" que emitían tanta radiación que separarían los átomos gaseosos de sus electrones. Este gas, a su vez, se estaba calentando y expandiendo, y tenía la posibilidad de obtener la masa necesaria para formar las grandes galaxias que conocemos hoy.

El destino final del universo tiene diversos modelos que explican lo que sucederá en función de diversos parámetros y observaciones. De acuerdo con la teoría general de la relatividad el destino final más probable dependerá del valor auténtico de la densidad de materia, en función de ese parámetro se barajan dos tipos de finales:

A partir de los años 1990 se comprobó que el universo parece tener una expansión acelerada, hecho que dentro de la relatividad general solo es explicable acudiendo a un mecanismo de tipo constante cosmológica. No se conoce si ese hecho puede dar lugar a un tercer tipo de final.

Si el universo es suficientemente denso, es posible que la fuerza gravitatoria de toda esa materia pueda finalmente detener la expansión inicial, de tal manera que el universo volvería a contraerse, las galaxias empezarían a retroceder, y con el tiempo colisionarían entre sí. La temperatura se elevaría, y el universo se precipitaría hacia un destino catastrófico en el que quedaría reducido nuevamente a un punto.

Algunos físicos han especulado que después se formaría otro universo, en cuyo caso se repetiría el proceso. A esta teoría se la conoce como la teoría del universo oscilante.

Hoy en día esta hipótesis parece incorrecta, pues a la luz de los últimos datos experimentales, el Universo se está expandiendo cada vez más rápido.

El Gran Desgarramiento o Teoría de la Eterna Expansión, en inglés Big Rip, es una hipótesis cosmológica sobre el destino último del universo. Este posible destino final del universo depende de la cantidad de energía oscura existente en el Universo. Si el universo contiene suficiente energía oscura, podría acabar en un desgarramiento de toda la materia.

El valor clave es "w", la razón entre la presión de la energía oscura y su densidad energética. A "w" < -1, el universo acabaría por ser desgarrado. Primero, las galaxias se separarían entre sí, luego la gravedad sería demasiado débil para mantener integrada cada galaxia. Los sistemas planetarios perderían su cohesión gravitatoria. En los últimos minutos, se desbaratarán estrellas y planetas, y los átomos serán destruidos.

Los autores de esta hipótesis calculan que el fin del tiempo ocurriría aproximadamente 3,5×10 años después del Big Bang, es decir, dentro de 2,0×10 años.

Una modificación de esta teoría denominada "Big Freeze", aunque poco aceptada, afirma que el universo continuaría su expansión sin provocar un "Big Rip".

Muy poco se conoce con certeza sobre el tamaño del universo. Puede tener una longitud de billones de años luz o incluso tener un tamaño infinito. Un artículo de 2003 dice establecer una cota inferior de 24 gigaparsecs (78000 millones de años luz) para el tamaño del universo, pero no hay ninguna razón para creer que esta cota está de alguna manera muy ajustada "(Véase forma del Universo)".

El universo "observable" (o "visible"), que consiste en toda la materia y energía que podía habernos afectado desde el "Big Bang" dada la limitación de la velocidad de la luz, es ciertamente finito. La distancia comóvil al extremo del universo visible ronda los 46.500 millones de años luz en todas las direcciones desde la Tierra. Así, el universo visible se puede considerar como una esfera perfecta con la Tierra en el centro, y un diámetro de unos 93000 millones de años luz. Hay que notar que muchas fuentes han publicado una amplia variedad de cifras incorrectas para el tamaño del universo visible: desde 13700 hasta 180000 millones de años luz. "(Véase universo observable)".

En el Universo las distancias que separan los astros son tan grandes que, si las quisiéramos expresar en metros, tendríamos que utilizar cifras muy grandes. Debido a ello, se utiliza como unidad de longitud el año luz, que corresponde a la distancia que recorre la luz en un año.

Anteriormente, el modelo de universo más comúnmente aceptado era el propuesto por Albert Einstein en su Relatividad General, en la que propone un universo "finito pero ilimitado", es decir, que a pesar de tener un volumen medible no tiene límites, de forma análoga a la superficie de una esfera, que es medible pero ilimitada.
Esto era propio de un universo esférico. Hoy, gracias a las últimas observaciones realizadas por el WMAP de la NASA, se sabe que tiene forma plana. Aunque no se descarta un posible universo plano cerrado sobre sí mismo. Estas observaciones sugieren que el universo es infinito.

Una pregunta importante abierta en cosmología es la forma del universo. Matemáticamente, ¿qué 3-variedad representa mejor la parte espacial del universo?

Si el universo es espacialmente "plano", se desconoce si las reglas de la geometría Euclidiana serán válidas a mayor escala. Actualmente muchos cosmólogos creen que el Universo observable está muy cerca de ser espacialmente plano, con arrugas locales donde los objetos masivos distorsionan el espacio-tiempo, de la misma forma que la superficie de un lago es casi plana. Esta opinión fue reforzada por los últimos datos del WMAP, mirando hacia las "oscilaciones acústicas" de las variaciones de temperatura en la radiación de fondo de microondas.

Por otra parte, se desconoce si el universo es conexo. El universo no tiene cotas espaciales de acuerdo al modelo estándar del Big Bang, pero sin embargo debe ser espacialmente finito (compacto). Esto se puede comprender utilizando una analogía en dos dimensiones: la superficie de una esfera no tiene límite, pero no tiene un área infinita. Es una superficie de dos dimensiones con curvatura constante en una tercera dimensión. La 3-esfera es un equivalente en tres dimensiones en el que las tres dimensiones están constantemente curvadas en una cuarta.

Si el universo fuese compacto y sin cotas, sería posible, después de viajar una distancia suficiente, volver al punto de partida. Así, la luz de las estrellas y galaxias podría pasar a través del universo observable más de una vez. Si el universo fuese múltiplemente conexo y suficientemente pequeño (y de un tamaño apropiado, tal vez complejo) entonces posiblemente se podría ver una o varias veces alrededor de él en alguna (o todas) direcciones. Aunque esta posibilidad no ha sido descartada, los resultados de las últimas investigaciones de la radiación de fondo de microondas hacen que esto parezca improbable.

Históricamente se ha creído que el Universo es de color negro, pues es lo que observamos al momento de mirar al cielo en las noches despejadas. En 2002, sin embargo, los astrónomos Karl Glazebrook e Ivan Baldry afirmaron en un artículo científico que el universo en realidad es de un color que decidieron llamar café con leche cósmico. Este estudio se basó en la medición del rango espectral de la luz proveniente de un gran volumen del Universo, sintetizando la información aportada por un total de más de 200.000 galaxias.

Mientras que la estructura está considerablemente fractalizada a nivel local (ordenada en una jerarquía de racimo), en los órdenes más altos de distancia el universo es muy homogéneo. A estas escalas la densidad del universo es muy uniforme, y no hay una dirección preferida o significativamente asimétrica en el universo. Esta homogeneidad e isotropía es un requisito de la Métrica de Friedman-Lemaître-Robertson-Walker empleada en los modelos cosmológicos modernos.

La cuestión de la anisotropía en el universo primigenio fue significativamente contestada por el WMAP, que buscó fluctuaciones en la intensidad del fondo de microondas. Las medidas de esta anisotropía han proporcionado información útil y restricciones sobre la evolución del Universo.

Hasta el límite de la potencia de observación de los instrumentos astronómicos, los objetos irradian y absorben la energía de acuerdo a las mismas leyes físicas a como lo hacen en nuestra propia galaxia. Basándose en esto, se cree que las mismas leyes y constantes físicas son universalmente aplicables a través de todo el universo observable. No se ha encontrado ninguna prueba confirmada que muestre que las constantes físicas hayan variado desde el "Big Bang".

El universo observable actual parece tener un espacio-tiempo geométricamente plano, conteniendo una densidad masa-energía equivalente a 9,9 × 10 gramos por centímetro cúbico. Los constituyentes primarios parecen consistir en un 73 % de energía oscura, 23 % de materia oscura fría y un 4 % de átomos. Así, la densidad de los átomos equivaldría a un núcleo de hidrógeno sencillo por cada cuatro metros cúbicos de volumen. La naturaleza exacta de la energía oscura y la materia oscura fría sigue siendo un misterio. Actualmente se especula con que el neutrino, (una partícula muy abundante en el universo), tenga, aunque mínima, una masa. De comprobarse este hecho, podría significar que la energía y la materia oscura no existen.
Durante las primeras fases del "Big Bang", se cree que se formaron las mismas cantidades de materia y antimateria. Materia y antimateria deberían eliminarse mutuamente al entrar en contacto, por lo que la actual existencia de materia (y la ausencia de antimateria) supone una violación de la simetría CP "(Véase Violación CP)", por lo que puede ser que las partículas y las antipartículas no tengan propiedades exactamente iguales o simétricas, o puede que simplemente las leyes físicas que rigen el universo favorezcan la supervivencia de la materia frente a la antimateria.
En este mismo sentido, también se ha sugerido que quizás la materia oscura sea la causante de la bariogénesis al interactuar de distinta forma con la materia que con la antimateria.
Antes de la formación de las primeras estrellas, la composición química del universo consistía primariamente en hidrógeno (75 % de la masa total), con una suma menor de helio-4 (He) (24 % de la masa total) y el resto de otros elementos. Una pequeña porción de estos elementos estaba en la forma del isótopo deuterio (²H), helio-3 (³He) y litio (Li). La materia interestelar de las galaxias ha sido enriquecida sin cesar por elementos más pesados, generados por procesos de fusión en la estrellas, y diseminados como resultado de las explosiones de supernovas, los vientos estelares y la expulsión de la cubierta exterior de estrellas maduras.

El "Big Bang" dejó detrás un flujo de fondo de fotones y neutrinos. La temperatura de la radiación de fondo ha decrecido sin cesar con la expansión del universo y ahora fundamentalmente consiste en la energía de microondas equivalente a una temperatura de 2725 K. La densidad del fondo de neutrinos actual es de 150 por centímetro cúbico.

Según la física moderna, el Universo es un sistema cuántico aislado, un campo unificado de ondas que entra en decoherencia al tutor de la observación o medición. En tal virtud, en última instancia, el entorno del Universo sería no local y no determinista.

Los cosmólogos teóricos estudian modelos del conjunto espacio-tiempo que estén conectados, y buscan modelos que sean consistentes con los modelos físicos cosmológicos del espacio-tiempo en la escala del universo observable. Sin embargo, recientemente han tomado fuerza teorías que contemplan la posibilidad de "multiversos" o varios universos coexistiendo simultáneamente. Según la recientemente enunciada Teoría de Multiexplosiones se pretende dar explicación a este aspecto, poniendo en relieve una posible convivencia de universos en un mismo espacio.

Científicos del King's College de Londres lograron recrear las condiciones inmediatamente seguidas al Big Bang a través del conocimiento adquirido durante dos años de la partícula de Higgs y llegaron a la conclusión de que, posiblemente, el universo colapsó, hasta dejar de existir casi tan pronto cuando empezó, lo que plantea la idea de que todo lo que vemos no existe y solo es el pasado de los astros.

A gran escala, el universo está formado por galaxias y agrupaciones de galaxias. Las galaxias son agrupaciones masivas de estrellas, y son las estructuras más grandes en las que se organiza la materia en el universo. A través del telescopio se manifiestan como manchas luminosas de diferentes formas. A la hora de clasificarlas, los científicos distinguen entre las galaxias del Grupo Local, compuesto por las treinta galaxias más cercanas y a las que está unida gravitacionalmente nuestra galaxia (la Vía Láctea), y todas las demás galaxias, a las que llaman "galaxias exteriores".

Las galaxias están distribuidas por todo el universo y presentan características muy diversas, tanto en lo que respecta a su configuración como a su antigüedad. Las más pequeñas abarcan alrededor de 3000 millones de estrellas, y las galaxias de mayor tamaño pueden llegar a abarcar más de un billón de astros. Estas últimas pueden tener un diámetro de 170 000 años luz, mientras que las primeras no suelen exceder de los 6000 años luz.

Además de estrellas y sus astros asociados (planetas, asteroides, etc...), las galaxias contienen también materia interestelar, constituida por polvo y gas en una proporción que varía entre el 1 y el 10 % de su masa.

Se estima que el universo puede estar constituido por unos 100 000 millones de galaxias, aunque estas cifras varían en función de los diferentes estudios.

La creciente potencia de los telescopios, que permite observaciones cada vez más detalladas de los distintos elementos del universo, ha hecho posible una clasificación de las galaxias por su forma. Se han establecido así cuatro tipos distintos: galaxias elípticas, espirales, espirales barradas e irregulares.

En forma de elipse o de esferoide, se caracterizan por carecer de una estructura interna definida y por presentar muy poca materia interestelar. Se consideran las más antiguas del universo, ya que sus estrellas son viejas y se encuentran en una fase muy avanzada de su evolución.

Las galaxias de este tipo fueron en su momento galaxias espirales, pero consumieron o perdieron gran parte de materia interestelar, por lo que hoy carecen de brazos espirales y solo presenta su núcleo. Aunque a veces existe cierta cantidad de materia interestelar, sobre todo polvo, que se agrupa en forma de disco alrededor de esta. Estas galaxias constituyen alrededor del 3 % de las galaxias del universo.

Están constituidas por un núcleo central y dos o más brazos en espiral, que parten del núcleo. Este se halla formado por multitud de estrellas y apenas tiene materia interestelar, mientras que en los brazos abunda la materia interestelar y hay gran cantidad de estrellas jóvenes, que son muy brillantes. Alrededor del 75 % de las galaxias del universo son de este tipo.

Es un subtipo de galaxia espiral, caracterizados por la presencia de una barra central de la que típicamente parten dos brazos espirales. Este tipo de galaxias constituyen una fracción importante del total de galaxias espirales. La Vía Láctea es una galaxia espiral barrada.

Incluyen una gran diversidad de galaxias, cuyas configuraciones no responden a las tres formas anteriores, aunque tienen en común algunas características, como la de ser casi todas pequeñas y contener un gran porcentaje de materia interestelar. Se calcula que son irregulares alrededor del 5 % de las galaxias del universo.

La Vía Láctea es nuestra galaxia. Según las observaciones, posee una masa de 10 masas solares y es de tipo espiral barrada. Con un diámetro medio de unos 100 000 años luz se calcula que contiene unos 200 000 millones de estrellas, entre las cuales se encuentra el Sol. La distancia desde el Sol al centro de la galaxia es de alrededor de 27 700 años luz (8,5 kpc)
A simple vista, se observa como una estela blanquecina de forma elíptica, que se puede distinguir en las noches despejadas. Lo que no se aprecian son sus brazos espirales, en uno de los cuales, el llamado brazo de Orión, está situado nuestro sistema solar, y por tanto la Tierra.

El núcleo central de la galaxia presenta un espesor uniforme en todos sus puntos, salvo en el centro, donde existe un gran abultamiento con un grosor máximo de 16 000 años luz, siendo el grosor medio de unos 6000 años luz.
Todas las estrellas y la materia interestelar que contiene la Vía Láctea, tanto en el núcleo central como en los brazos, están situadas dentro de un disco de 100 000 años luz de diámetro, que gira sobre su eje a una velocidad lineal superior a los 216 km/s.

Tan solo tres galaxias distintas a la nuestra son visibles a simple vista. Tenemos la Galaxia de Andrómeda, visible desde el Hemisferio Norte; la Gran Nube de Magallanes, y la Pequeña Nube de Magallanes, en el Hemisferio Sur celeste. El resto de las galaxias no son visibles al ojo desnudo sin ayuda de instrumentos. Sí que lo son, en cambio, las estrellas que forman parte de la Vía Láctea. Estas estrellas dibujan a menudo en el cielo figuras reconocibles, que han recibido diversos nombres en relación con su aspecto. Estos grupos de estrellas de perfil identificable se conocen con el nombre de constelaciones. La Unión Astronómica Internacional agrupó oficialmente las estrellas visibles en 88 constelaciones, algunas de ellas muy extensas, como Hidra o la Osa Mayor, y otras muy pequeñas como Flecha y Triángulo.

Son los elementos constitutivos más destacados de las galaxias. Las estrellas son enormes esferas de gas que brillan debido a sus gigantescas reacciones nucleares. Cuando debido a la fuerza gravitatoria, la presión y a la temperatura del interior de una estrella que sea suficientemente intensa, se inicia la fusión nuclear de sus átomos, y comienzan a emitir una luz roja oscura, que después se mueve hacia el estado superior, que es en el que está nuestro Sol, para posteriormente, al modificarse las reacciones nucleares interiores, dilatarse y finalmente enfriarse.
Al acabarse el hidrógeno, se originan reacciones nucleares de elementos más pesados, más energéticas, que convierten la estrella en una gigante roja. Con el tiempo, esta se vuelve inestable, a la vez que lanza hacia el espacio exterior la mayor parte del material estelar. Este proceso puede durar 100 millones de años, hasta que se agota toda la energía nuclear, y la estrella se contrae por efecto de la gravedad hasta hacerse pequeña y densa, en la forma de enana blanca, azul o marrón. Si la estrella inicial es varias veces más masiva que el Sol, su ciclo puede ser diferente, y en lugar de una gigante, puede convertirse en una supergigante y acabar su vida con una explosión denominada supernova. Estas estrellas pueden acabar como estrellas de neutrones. Tamaños aún mayores de estrellas pueden consumir todo su combustible muy rápidamente, transformándose en una entidad supermasiva llamada agujero negro.
Los púlsares son fuentes de ondas de radio que emiten con periodos regulares. La palabra «púlsar» significa "pulsating radio source" (fuente de radio pulsante). Se detectan mediante radiotelescopios y se requieren relojes de extraordinaria precisión para detectar sus cambios de ritmo. Los estudios indican que un púlsar es una estrella de neutrones pequeña que gira a gran velocidad. El más conocido está en la Nebulosa del Cangrejo. Su densidad es tan grande que una muestra de cuásar del tamaño de una bola de bolígrafo tendría una masa de cerca de 100 000 toneladas. Su campo magnético, muy intenso, se concentra en un espacio reducido. Esto lo acelera y lo hace emitir gran cantidad de energía en haces de radiación que aquí recibimos como ondas de radio.

La palabra «cuásar» es un acrónimo de "quasi stellar radio source" (fuentes de radio casi estelares). Se identificaron en la década de 1950. Más tarde se vio que mostraban un desplazamiento al rojo más grande que cualquier otro objeto conocido. La causa era el Efecto Doppler, que mueve el espectro hacia el rojo cuando los objetos se alejan. El primer cuásar estudiado, denominado 3C 273, está a 1500 millones de años luz de la Tierra. A partir de 1980 se han identificado miles de cuásares, algunos alejándose de nosotros a velocidades del 90 % de la de la luz.

Se han descubierto cuásares a 12 000 millones de años luz de la Tierra; prácticamente la edad del universo. A pesar de las enormes distancias, la energía que llega en algunos casos es muy grande, equivalente a la recibida desde miles de galaxias: como ejemplo, el es unas 60 000 veces más brillante que toda la Vía Láctea.

Los planetas son cuerpos que giran en torno a una estrella y que, según la definición de la Unión Astronómica Internacional, deben cumplir además la condición de haber limpiado su órbita de otros cuerpos rocosos importantes, y de tener suficiente masa como para que su fuerza de gravedad genere un cuerpo esférico. En el caso de cuerpos que orbitan alrededor de una estrella que no cumplan estas características, se habla de planetas enanos, planetesimales, o asteroides. En nuestro Sistema Solar hay 8 planetas: Mercurio, Venus, Tierra, Marte, Júpiter, Saturno, Urano y Neptuno, considerándose desde 2006 a Plutón como un planeta enano. A finales de 2009, fuera de nuestro sistema solar se habían detectado más de 400 planetas extrasolares, pero los avances tecnológicos están permitiendo que este número crezca a buen ritmo.

Los satélites naturales son astros que giran alrededor de los planetas. El único satélite natural de la Tierra es la Luna, que es también el satélite más cercano al sol. A continuación se enumeran los principales satélites de los planetas del sistema solar (se incluye en el listado a Plutón, considerado por la UAI como un planeta enano).

En aquellas zonas de la órbita de una estrella en las que, por diversos motivos, no se ha producido la agrupación de la materia inicial en un único cuerpo dominante o planeta, aparecen los discos de asteroides: objetos rocosos de muy diversos tamaños que orbitan en grandes cantidades en torno a la estrella, chocando eventualmente entre sí. Cuando las rocas tienen diámetros inferiores a 50 m se denominan meteoroides. A consecuencia de las colisiones, algunos asteroides pueden variar sus órbitas, adoptando trayectorias muy excéntricas que periódicamente les acercan la estrella. Cuando la composición de estas rocas es rica en agua u otros elementos volátiles, el acercamiento a la estrella y su consecuente aumento de temperatura origina que parte de su masa se evapore y sea arrastrada por el viento solar, creando una larga cola de material brillante a medida que la roca se acerca a la estrella. Estos objetos se denominan cometas. En nuestro sistema solar hay dos grandes discos de asteroides: uno situado entre las órbitas de Marte y Júpiter, denominado el Cinturón de asteroides, y otro mucho más tenue y disperso en los límites del sistema solar, a aproximadamente un año luz de distancia, denominado Nube de Oort.

La teoría general de la relatividad, que fue publicada por Albert Einstein en 1916, implicaba que el cosmos se hallaba en expansión o en contracción. Pero este concepto era totalmente opuesto a la noción de un universo estático, aceptada entonces hasta por el propio Einstein. De ahí que este incluyera en sus cálculos lo que denominó “constante cosmológica”, ajuste mediante el cual intentaba conciliar su teoría con la idea aceptada de un universo estático e inmutable.
Sin embargo, ciertos descubrimientos que se sucedieron en los años veinte llevaron a Einstein a decir que el ajuste que había efectuado a su teoría de la relatividad era el ‘mayor error de su vida’. Dichos descubrimientos se realizaron gracias a la instalación de un enorme telescopio de 254 centímetros en el monte Wilson (California). Las observaciones formuladas en los años veinte con la ayuda de este instrumento demostraron que el universo se halla en expansión.

Hasta entonces, los mayores telescopios solo permitían identificar las estrellas de nuestra galaxia, la Vía Láctea, y aunque se veían borrones luminosos, llamados nebulosas, por lo general se tomaban por remolinos de gas existentes en nuestra galaxia. Gracias a la mayor potencia del telescopio del monte Wilson, Edwin Hubble logró distinguir estrellas en aquellas nebulosas. Finalmente se descubrió que los borrones eran lo mismo que la Vía Láctea: galaxias. Hoy se cree que hay entre 50 000 y 125 000 millones de galaxias, cada una con cientos de miles de millones de estrellas.

A finales de los años veinte, Hubble también descubrió que las galaxias se alejan de nosotros, y que lo hacen más velozmente cuanto más lejos se hallan. Los astrónomos calculan la tasa de recesión de las galaxias mediante el espectrógrafo, instrumento que mide el espectro de la luz procedente de los astros. Para ello, dirigen la luz que proviene de estrellas lejanas hacia un prisma, que la descompone en los colores que la integran.

La luz de un objeto es rojiza (fenómeno llamado corrimiento al rojo) si este se aleja del observador, y azulada (corrimiento al azul) si se le aproxima. Cabe destacar que, salvo en el caso de algunas galaxias cercanas, todas las galaxias conocidas tienen líneas espectrales desplazadas hacia el rojo. De ahí infieren los científicos que el universo se expande de forma ordenada. La tasa de dicha expansión se determina midiendo el grado de desplazamiento al rojo.
¿Qué conclusión se ha extraído de la expansión del cosmos? Pues bien, un científico invitó al público a analizar el proceso a la inversa —como una película de la expansión proyectada en retroceso— a fin de observar la historia primitiva del universo. Visto así, el cosmos parecería estar en recesión o contracción, en vez de en expansión y retornaría finalmente a un único punto de origen.

El físico Stephen Hawking concluyó lo siguiente en su libro Agujeros negros y pequeños universos (y otros ensayos), editado en 1993: «La ciencia podría afirmar que el universo tenía que haber conocido un comienzo».
Pero hace años, muchos expertos rechazaban que el universo hubiese tenido principio. El científico Fred Hoyle no aceptaba que el cosmos hubiera surgido mediante lo que llamó burlonamente "a big bang" («una gran explosión»). Uno de los argumentos que esgrimía era que, de haber existido un comienzo tan dinámico, deberían conservarse residuos de aquel acontecimiento en algún lugar del universo: tendría que haber radiación fósil, por así decirlo; una leve luminiscencia residual.

El diario "The New York Times" (8 de marzo de 1998) indicó que hacia 1965 «los astrónomos Arno Penzias y Robert Wilson descubrieron la omnipresente radiación de fondo: el destello residual de la explosión primigenia». El artículo añadió: «Todo indicaba que la teoría [de la gran explosión] había triunfado».

Pero en los años posteriores al hallazgo se formuló esta objeción: Si el modelo de la gran explosión era correcto, ¿Por qué no se habían detectado leves irregularidades en la radiación? (La formación de las galaxias habría requerido un universo que contase con zonas más frías y densas que permitieran la fusión de la materia.) En efecto, los experimentos realizados por Penzias y Wilson desde la superficie terrestre no revelaban tales irregularidades.

Por esta razón, la NASA lanzó en noviembre de 1989 el satélite COBE (siglas de Explorador del Fondo Cósmico, en inglés), cuyos descubrimientos se calificaron de cruciales. “Las ondas que detectó su radiómetro diferencial de microondas correspondían a las fluctuaciones que dejaron su impronta en el cosmos y que hace miles de millones de años llevaron a la formación de las galaxias.”

Diferentes palabras se han utilizado a través de la historia para denotar "todo el espacio", incluyendo los equivalentes y las variantes en varios lenguajes de "cielos", "cosmos" y "mundo". El macrocosmos también se ha utilizado para este efecto, aunque está más específicamente definido como un sistema que refleja a gran escala uno, algunos, o todos estos componentes del sistema o partes. Similarmente, un microcosmos es un sistema que refleja a pequeña escala un sistema mucho mayor del que es parte.

Aunque palabras como mundo y sus equivalentes en otros lenguajes casi siempre se refieren al planeta Tierra, antiguamente se referían a cada cosa que existía (se podía ver). En ese sentido la utilizaba, por ejemplo, Copérnico. Algunos lenguajes utilizan la palabra "mundo" como parte de la palabra "espacio exterior". Un ejemplo en alemán lo constituye la palabra "Weltraum".


En inglés:


</doc>
<doc id="6608" url="https://es.wikipedia.org/wiki?curid=6608" title="28 de abril">
28 de abril

El 28 de abril es el 118.º (centésimo decimoctavo) día del año en el calendario gregoriano y el 119.º en los años bisiestos. Quedan 247 días para finalizar el año.








</doc>
<doc id="6609" url="https://es.wikipedia.org/wiki?curid=6609" title="29 de abril">
29 de abril

El 29 de abril es el 119.º (centésimo decimonoveno) día del año en el calendario gregoriano y el 120.º en los años bisiestos. Quedan 246 días para finalizar el año.








</doc>
<doc id="6613" url="https://es.wikipedia.org/wiki?curid=6613" title="Reutilización">
Reutilización

Reutilizar es la acción que permite volver a utilizar los bienes o productos desechados y darles un uso igual o diferente a aquel para el que fueron concebidos.

Este proceso hace que cuantos más objetos volvamos a reutilizar menos basura produciremos y menos recursos tendremos que gastar.

La reutilización ocupa el segundo puesto en la jerarquía de residuos, después de la prevención y por encima del reciclaje.


En España está regulada la producción y gestión de los residuos procedentes de todo tipo de obras: edificación, urbanización, demolición, reforma, etc.
Tiene por objeto fomentar, por este orden, su prevención, reutilización, reciclado y otras formas de valorización, asegurando que los destinados a operaciones de eliminación reciban un tratamiento adecuado, y contribuir a un desarrollo sostenible de esta actividad. A tales efectos es preceptiva la redacción de un Plan de Gestión de Residuos Construcción-Demolición (RCD).



</doc>
<doc id="6618" url="https://es.wikipedia.org/wiki?curid=6618" title="Telefonía móvil">
Telefonía móvil

La telefonía móvil o telefonía celular es un medio de comunicación inalámbrico a través de ondas electromagnéticas.
Como cliente de este tipo de redes, se utiliza un dispositivo denominado "teléfono móvil" o "teléfono celular". En la mayor parte de Hispanoamérica se prefiere la denominación "teléfono celular" o simplemente "celular", aunque en Cuba se dice de ambas formas, y mientras que en España es más común el término "teléfono móvil" o simplemente "móvil".

A partir del año 2000, los teléfonos móviles han adquirido distintas funcionalidades que van mucho más allá de limitarse a solo llamar, traducir o enviar mensajes de texto: se puede decir que han incorporado las funciones de los dispositivos tales como PDA, cámara de fotos, cámara de video, consola de videojuegos portátil, agenda electrónica, reloj despertador, calculadora, micro-proyector, radio portátil, GPS o reproductor multimedia (al punto de causar la obsolescencia de varios de ellos), y que también pueden realizar una multitud de acciones en un dispositivo pequeño y portátil que llevan prácticamente todos los habitantes de los países desarrollados y un número creciente de habitantes de los países en desarrollo. A este tipo de evolución del teléfono móvil se le conoce como teléfono inteligente (o teléfono autómata).

El primer antecedente técnico de la telefonía móvil fueron los servicios de comunicación públicos de radiotelefonía establecidos en algunas ciudades estadounidenses durante los años 1940. Así, AT&T estableció un servicio de ese tipo en la ciudad de San Luis (Misuri) en 1946, que usaba un único transmisor y ofrecía seis canales de transmisión. La popularidad del servicio hizo que rápidamente quedara bloqueado, pero en 1947 AT&T dio con la solución: en lugar de utilizar un único transmisor, creó una red de transmisores de baja potencia, cada uno para un área concreta o "célula" (de ahí derivó el término "teléfono celular" que en muchos países es la forma de referirse a la telefonía móvil). Sin embargo, la noción de telefonía móvil había sido ya anticipada mucho tiempo antes, así William Edward Ayrton (1847-1908), catedrático de física aplicada e ingeniería eléctrica en una conferencia en el Brittish Imperial Institute en 1897 dijo:

A finales de la década de 1950, el científico soviético Leonid Ivanovich Kupriyanovich desarrolló un sistema de comunicación móvil que culminó en el modelo KL-1, que utiliza ondas de radio y es capaz de alcanzar una distancia de 30 km y puede dar servicio a varios clientes. Este teléfono móvil se patentó el 11 de enero de 1957 con el Certificado de Patente n.º 115494. Fue la base para la investigación que Kupriyanovich comenzó el año siguiente en el Instituto de Investigación Científica de Voronezh. De esta investigación surgió el Altai, que se distribuyó comercialmente en 1963, llegó a estar presente en más de 114 ciudades de la Unión Soviética y dio servicio a hospitales y médicos. Con un Altai los usuarios se podían comunicar a otro Altai, a teléfonos fijos y a cabinas de teléfono convencionales. El sistema se extendió por otros países de Europa del Este, como Bulgaria, que lo mostraría en la Exposición Internacional Inforga.

La primera red comercial automática fue la de NTT de Japón en 1974, seguida por la NMT, que funcionaba en simultáneo en Suecia, Dinamarca, Noruega y Finlandia en 1981 usando teléfonos de Ericsson y Mobira (el ancestro de Nokia). En Estados Unidos las primeras redes de teléfonos celulares aparecieron en Chicago en 1978, donde 10 "células" comunicaban a 2000 usuarios, y al año siguiente se creó un sistema similar en Japón. Arabia Saudita también usaba la NMT y la puso en operación un mes antes que los países nórdicos. El primer antecedente respecto al teléfono móvil en Estados Unidos es de la compañía Motorola, con su modelo DynaTAC 8000X. El modelo fue diseñado por el ingeniero de Motorola Rudy Krolopp en 1983. El modelo pesaba poco menos de un kilo y tenía un valor de casi 4000 dólares estadounidenses. Krolopp se incorporaría posteriormente al equipo de investigación y desarrollo de Motorola liderado por Martin Cooper. Tanto Cooper como Krolopp aparecen como propietarios de la patente original. A partir del DynaTAC 8000X, Motorola desarrollaría nuevos modelos como el Motorola MicroTAC, lanzado en 1989, y el Motorola StarTAC, lanzado en 1996 al mercado. Básicamente podemos distinguir en el planeta dos tipos de redes de telefonía móvil, la existencia de las mismas es fundamental para que podamos llevar a cabo el uso de nuestro teléfono celular, para que naveguemos en Internet o para que enviemos mensajes de texto como lo hacemos habitualmente. La primera red es la Red de Telefonía móvil de tipo analógico (TMA), la misma establece la comunicación mediante señales vocales analógicas, tanto en el tramo radioeléctrico como en el tramo terrestre; la primera versión de la misma funcionó en la banda radioeléctrica de los 450 MHz, luego trabajaría en la banda de los 900 MHz; en países como España, esta red fue retirada el 31 de diciembre de 2003. Luego tenemos la red de telefonía móvil digital; aquí ya la comunicación se lleva a cabo mediante señales digitales, lo que nos permite optimizar tanto el aprovechamiento de las bandas de radiofrecuencia como la calidad de la transmisión de las señales. El exponente más significativo que esta red posee actualmente es el GSM y su tercera generación UMTS (ambos funcionan en las bandas de 850/900 MHz) que en el 2004, llegó a alcanzar los 100 millones de usuarios.

Martin Cooper fue el pionero en esta tecnología. A él se le considera «el padre de la telefonía celular», al introducir el primer radio-teléfono en 1973, en Estados Unidos, mientras trabajaba para Motorola, pero no fue sino hasta 1979 cuando aparecieron los primeros sistemas comerciales en Tokio, Japón, de la compañía NTT.

En 1981, los países nórdicos introdujeron un sistema celular similar a AMPS ("Advanced Mobile Phone System"). Por otro lado, en Estados Unidos, gracias a que la entidad reguladora de ese país adoptó reglas para la creación de un servicio comercial de telefonía celular, en 1983 se puso en operación el primer sistema comercial en la ciudad de Chicago.

Con ese punto de partida, en varios países se diseminó la telefonía celular como una alternativa a la telefonía convencional inalámbrica y el innovador de un nuevo medio de comunicación. La tecnología tuvo gran aceptación, por lo que a los pocos años de implantarse se empezó a saturar el servicio. En ese sentido, hubo la necesidad de desarrollar e implantar otras formas de acceso múltiple al canal y transformar los sistemas analógicos a digitales, con el objeto de darle cabida a más usuarios. Para separar una etapa de la otra, la telefonía celular se ha caracterizado por contar con diferentes generaciones. En la actualidad tienen gran importancia los teléfonos móviles táctiles.

La comunicación telefónica es posible gracias a la interconexión entre centrales móviles y públicas. Según las bandas o frecuencias en las que opera el móvil, podrá funcionar en una parte u otra del mundo. La telefonía móvil consiste en la combinación de una red de estaciones transmisoras o receptoras de radio (repetidores, estaciones base o BTS) y una serie de centrales telefónicas de conmutación de y 5.º nivel (MSC y BSC respectivamente), que posibilita la comunicación entre terminales telefónicos portátiles (teléfonos móviles) o entre terminales portátiles y teléfonos de la red fija tradicional.

La red de telefonía móvil, debemos entenderla en varios «segmentos».

En su operación, el teléfono móvil establece comunicación con una estación base y, a medida que se traslada, los sistemas computacionales que administran la red van transmitiendo la llamada a la siguiente estación base de forma transparente para el usuario. Por eso se dice que las estaciones base forman una red de celdas, sirviendo cada estación base a los equipos móviles que se encuentran en su celda.

La evolución del teléfono móvil ha permitido disminuir su tamaño y peso, desde el Motorola DynaTAC, el primer teléfono móvil en 1983 que pesaba 800 gramos, a los actuales más compactos y con mayores prestaciones de servicio. El desarrollo de baterías más pequeñas y de mayor duración, pantallas más nítidas y de colores, la incorporación de software más amigable, hacen del teléfono móvil un elemento muy apreciado en la vida moderna.

El avance de la tecnología ha hecho que estos aparatos incorporen funciones que no hace mucho parecían futuristas, como juegos, reproducción de música MP3 y otros formatos, correo electrónico, SMS, agenda electrónica PDA, fotografía digital y video digital, videollamada, navegación por Internet, GPS, y hasta Televisión digital. Las compañías de telefonía móvil ya están pensando nuevas aplicaciones para este pequeño aparato que nos acompaña a todas partes. Algunas de esas ideas son: medio de pago, localizador e identificador de personas.

Con la aparición de la telefonía móvil digital, fue posible acceder a páginas de Internet especialmente diseñadas para móviles, conocido como tecnología WAP. Desde ese momento hasta la actualidad, se creó el protocolo para el envío de configuración automática del móvil para poder acceder a Internet denominado OMA Client Provisioning.

Las primeras conexiones se efectuaban mediante una llamada telefónica a un número del operador a través de la cual se transmitían los datos, de manera similar a como lo haría un módem de línea fija para PC.

Posteriormente, nació el GPRS (o 2G), que permitió acceder a Internet a través del Protocolo de Internet. La velocidad del GPRS es de 54 kbit/s en condiciones óptimas, tarificándose en función de la cantidad de información transmitida y recibida.

Otras tecnologías más recientes permiten el acceso a Internet con banda ancha, como son EDGE, EV-DO, HSPA y 4G.

Por otro lado, cada vez es mayor la oferta de tabletas (tipo iPad, Samsung Galaxy Tab, libro electrónico o similar) por los operadores para conectarse a internet y realizar llamadas GSM (tabletas 3G).

Aprovechando la tecnología UMTS, han aparecido módems que conectan a Internet utilizando la red de telefonía móvil, consiguiendo velocidades similares a las de la ADSL o WiMAX. Dichos módems pueden conectarse a bases Wi-Fi 3G (también denominadas gateways 3G) para proporcionar acceso a internet a una red inalámbrica doméstica. En cuanto a la tarificación, aún es cara ya que no es una verdadera tarifa plana, debido a que algunas operadoras establecen limitaciones en cuanto a la cantidad de datos. Por otro lado, han comenzado a aparecer tarjetas prepago con bonos de conexión a Internet.

En 2011, el 20 % de los usuarios de banda ancha tiene intención de cambiar su conexión fija por una conexión de Internet móvil.

Según datos del tercer cuatrimestre de 2013, los resultados fueron los siguientes:

Por sistema operativo:

La denominada contaminación electromagnética, también conocida como electropolución, es la supuesta contaminación producida por las radiaciones del espectro electromagnético generadas por equipos electrónicos u otros elementos producto de la actividad humana.

Numerosos organismos como la Organización Mundial de la Salud, la Comisión Europea, la Universidad Complutense de Madrid, la Asociación Española Contra el Cáncer, el Ministerio de Sanidad y Consumo de España, o el Consejo Superior de Investigaciones Científicas de España han emitido informes que descartan daños a la salud debido a las emisiones de radiación electromagnética, incluyendo las de los teléfonos móviles.

No obstante existen estudios que indican lo contrario como el publicado en 2003 por el TNO (Instituto Holandés de Investigación Tecnológica), que afirmaba que las radiaciones de la tecnología UMTS podrían ser peligrosas, (aunque otra investigación de la Universidad de Zúrich, que utilizó hasta 10 veces la intensidad utilizada por el estudio del TNO, arrojó resultados contrarios). También hay numerosos estudios que investigan la posible asociación entre la presencia de antenas de telefonía celular y diversas enfermedades.

Las normativas en vigor en los diversos países consideran seguro vivir en un edificio con una antena de telefonía y en los que lo rodean, dependiendo del nivel de emisiones de la misma. 
No se ha podido demostrar con certeza que la exposición por debajo de los niveles de radiación considerados seguros suponga un riesgo para la salud, pero tampoco se dispone de datos que permitan asegurar que no existen efectos a largo plazo. El "Informe Steward" encargado por el Gobierno del Reino Unido aconseja que los niños no usen el teléfono móvil más que en casos de emergencia. Existen organizaciones que, aludiendo a estos posibles riesgos, reclaman que se observe el principio de precaución y se mantengan las emisiones al mínimo.

La mayoría de los mensajes que se intercambian por este medio, no se basan en la voz, sino en la escritura. En lugar de hablar al micrófono, cada vez más usuarios —sobre todo jóvenes— recurren al teclado para enviarse mensajes de texto. Sin embargo, dado que hay que introducir los caracteres en el terminal, ha surgido un lenguaje en el que se abrevian las palabras valiéndose de letras, símbolos y números. A pesar de que redactar y teclear es considerablemente más incómodo que conversar, dado su reducido coste, se ha convertido en una seria alternativa a los mensajes de voz.

El lenguaje SMS, consiste en acortar palabras, sustituir algunas de ellas por simple simbología o evitar ciertas preposiciones, utilizar los fonemas y demás. La principal causa es que el SMS individual se limita a 160 caracteres, si se sobrepasa ese límite, el mensaje individual pasa a ser múltiple, lógicamente multiplicándose el coste del envío. Por esa razón se procura reducir el número de caracteres, para que de un modo sencillo de entender, entre más texto o bien cueste menos.

Según un estudio británico, entre los usuarios de 18 a 24 años un 42 % los utilizan para coquetear; un 20 %, para concertar citas románticas, y un 13 %, para romper una relación.

A algunos analistas sociales les preocupa que estos mensajes, con su jerga ortográfica y sintáctica, lleven a que la juventud no sepa escribir bien. Sin embargo, otros opinan que “favorecen el renacer de la comunicación escrita en una nueva generación”. La portavoz de una editorial que publica un diccionario australiano hizo este comentario al rotativo "The Sun-Herald": «No surge a menudo la oportunidad de forjar un nuevo estilo [de escritura] […]; los mensajes de texto, unidos a Internet, logran que los jóvenes escriban mucho más. Necesitan tener un dominio de la expresión que les permita captar el estilo y defenderse bien con el vocabulario y el registro […] correspondientes a este género».

Algunas personas prefieren enviar mensajes de texto (SMS) en vez de hablar directamente, sobre todo por cuestiones económicas, dado que el coste de SMS es muy accesible frente al establecimiento de llamada y la duración de la llamada.

Hay restricciones en los sectores ortodoxos de la religión judía que, debido a algunas interpretaciones, los teléfonos móviles estándar no cumplen. Para resolver este problema, algunas organizaciones rabínicas han recomendado que los niños judíos no utilicen las funciones de mensajes de texto de los celulares. Estos se conocen con el nombre de teléfonos kosher, y los rabinos que practican el judaísmo ortodoxo autorizaron que los practicantes del judaísmo los utilizaran en Israel y en otros lugares. Aunque se pretende que estos teléfonos sirvan para fomentar la , algunos vendedores de los aparatos dicen haber tenido buenas ventas con adultos que prefieren la simplicidad de los dispositivos. Incluso se ha autorizado el uso de algunos teléfonos durante el sabbat, sobre todo entre trabajadores de la salud, de la seguridad y de otros servicios públicos, a pesar de que en esa fecha suele prohibirse el uso de cualquier dispositivo eléctrico.



</doc>
<doc id="6621" url="https://es.wikipedia.org/wiki?curid=6621" title="29 de julio">
29 de julio

El 29 de julio es el 210.º (ducentésimo décimo) día del año en el calendario gregoriano y el 211.º en los años bisiestos. Quedan 155 días para finalizar el año.












</doc>
<doc id="6626" url="https://es.wikipedia.org/wiki?curid=6626" title="Piel">
Piel

La piel es el mayor sistema del cuerpo humano o animal. En el ser humano ocupa aproximadamente 2 m², y su espesor varía entre los 0,5 mm (en los párpados) y los 4 mm (en el talón). Su peso aproximado es de 5 kg. Actúa como barrera protectora que aísla al organismo del medio que lo rodea, protegiéndolo y contribuyendo a mantener íntegras sus estructuras, al tiempo que actúa como sistema de comunicación con el entorno, y éste varía en cada especie. Anatómicamente se toma como referencia las medidas estándar dentro de la piel humana. También es conocido como sistema tegumentario.

La biología estudia tres capas principales que, de superficie a profundidad, son:
En medicina, en histoanatómico y dermológico, a fines prácticos se estudian dos de las capas; la epidermis y la dermis. De la piel dependen ciertas estructuras llamadas anexos cutáneos, como son los pelos, las uñas, las glándulas sebáceas y las sudoríparas.

Está compuesta de corpúsculos: de Meissner, presentes en el tacto de piel sin pelos, palmas, plantas, yema de los dedos, labios, punta de la lengua, pezones, glande y clítoris (tacto fino); de Krause, que generan la sensación de frío; de Paccini, que dan la sensación de presión; de Ruffini, que registran el calor; y de Merkel, el tacto superficial.

La piel puede sufrir de varias enfermedades distintas, denominadas dermatitis, como la seborrea. Estas son estudiadas por las disciplinas de la dermatología y la patología, principalmente.

En el ser humano, la piel del varón produce más secreción sebácea que la de la mujer debido a la mayor cantidad de andrógenos (hormona sexual masculina) que produce el varón. Como consecuencia, la piel masculina es más gruesa y grasosa.

La estructura general está compuesta por:

Existen dos tipos de piel:

La piel dentro del estudio embriológico:

La biología estudia a la piel y la divide en dos porciones:
Mientras que en corrientes médicas, como la histoanatomía y dermatología se estudia en tres estratos:

Cada una de las capas tiene funciones y componentes diferentes que causan una interrelación.
Está compuesto por: epidermis, dermis, tejido subcutáneo, y fascia profunda.

La epidermis se compone en su mayoría por queratinocitos, que se encuentran segmentados en el estrato córneo, además de un factor importante que son los melanocitos, también llamados pigmentocitos, que dan la pigmentación a la piel y que se encuentran justamente sobre el estrato germinativo. En la piel se pueden apreciar bajo cortes histológicos células de Langerhans y linfocitos, que se encargan de dar protección inmunológica, además de hallar a los mecanorreceptocitos o células de Merkel.







Las células que migran desde el estrato germinativo tardan en descamarse alrededor de 4 semanas. Esto depende de la raza y género, así como también de la especie cuando se estudia en animales. Cabe decir que la mayoría de mamíferos comparte estas características estratales. Si la descamación está por menor de 2 semanas y por mayor de 4 se le considera patológico, y puede deberse a alteraciones congénitas.

Una de las funciones vitales de la piel es el de cubrir todo el cuerpo, es este órgano el encargado de la protección del cuerpo, respiración, pasaje de la luz, reconocimiento de patógenos, etc.

La tinción especial empleada en las técnicas histológicas, es la de hematoxilina y eosina. Para el estudio de la epidermis a mayores rasgos se requieren estudios de microscopía electrónica. Otra tinción bajo microscopia óptica no muy usual es la tinción de Matoltsy y Parakkal.

La dermis es una capa profunda de tejido conjuntivo en la cual se tienen la peculiaridad de la abundancia de las fibras de colágeno y elásticas que se disponen de forma paralela y que le dan a la piel la consistencia y elasticidad característica del órgano. Histológicamente se divide en 2 capas:


En la dermis se hallan los siguientes componentes:

La dermis es 20-30 veces más gruesa que la epidermis.En ella se encuentran los anexos cutáneos, que son de dos tipos:
·córneos (pelos y uñas);
·glandulares (glándulas sebáceas y sudoríparas)

Es un estrato de la piel que está compuesto de tejido conjuntivo laxo y adiposo, lo cual le da funciones a la piel de regulación térmica y de movimiento a través del cuerpo como el que se ve cuando estiramos la piel de nuestro antebrazo hacia arriba, si no tuviera estos tipos de tejidos sería imposible moverla.

Los componentes propios que integran al tejido subcutáneo son:

La fascia profunda es una capa de tejido conjuntivo muy densa y organizada que reviste a las estructuras internas como los músculos, en los cuales crea compartimientos para que su expansión intrínseca no se propague más de lo que ella permite y así comprima a las venas.

Los tres estratos más interrelacionados de la piel son la epidermis, la dermis y el tejido subcutáneo, que se relacionan a través de las estructuras que contienen. Las estructuras con las que se relacionan son:


Glándulas sebáceas, músculos erectores del pelo y folículos pilosos:

Las glándulas sebáceas relacionan los estratos epidermis y dermis a través de la función que realizan cuando el folículo piloso, es movido por el músculo erector del pelo que comprime a la vez la glándula sebácea que suelta su secreción oleosa al exterior de la o epidermis.






Morfología de la piel o macro estructura es lo que vemos a simple vista. A simple vista parece lisa y llena, pero en realidad presenta pliegues, surcos, Hendiduras y pequeñas salientes.

a) Pliegues y surcos: más menos acentuados , están siempre presentes en todos los individuos sobre la cara dorsal de ciertas articulaciones, incluso cuando estos están en extensión completa o están en articulaciones completas. Ejemplo: codos, rodillas, dedos, muñecas, etc.

b) Arrugas: pueden ser provocadas ya se por contracción muscular, debido a un movimiento o por disposiciones estructurales de la piel. Ejemplo: pliegues de las articulaciones.

c) Poros cutáneos: son el orificio externo del canal de salida de la glándula sudorípara y sebácea, pero este último debe ser diferenciado por el nombre de "Ostium flicular".

Dentro del deterioro de la piel está lo que se llama el envejecimiento cutáneo prematuro debido a factores internos y externos.


El deterioro de la piel que se produce por causas naturales se presentan en forma de arrugas.

Las arrugas son causadas por alteraciones físico-químicas que conlleva el envejecimiento de la piel. A medida que pasa el tiempo, se pierden, gradualmente, tres elementos importantes para la piel:
Por lo demás, el sol, el humo del tabaco y de la contaminación, pueden acelerar también el proceso.

Las quemaduras de piel requieren un estudio más amplio ya que los protocolos médicos consideran grandes quemados a los pacientes a partir de un 10 % de piel afectada por quemaduras profundas y del 20 % de superficiales, tanto unos como otros requerirían ingreso hospitalario en una unidad especial. Aunque existen técnicas de piel cultivada que permiten autotrasplantes o autoinjerto, para quemaduras en sitios muy visibles o que provocan cierto rechazo y pueden provocar para el paciente problemas psicológicos.




</doc>
<doc id="6634" url="https://es.wikipedia.org/wiki?curid=6634" title="Franz Schubert">
Franz Schubert

Franz Peter Schubert (Viena, 31 de enero de 1797-ibídem, 19 de noviembre de 1828) fue un compositor austriaco, considerado el introductor del romanticismo musical y la forma breve característica pero, a la vez, también continuador de la sonata clásica siguiendo el modelo de Ludwig van Beethoven. Fue un gran compositor de "lieder" (breves composiciones para voz y piano, antecesor de la canción moderna), así como de música para piano, de cámara y orquestal.

Schubert fue uno de los principales músicos austríacos que vivió a comienzos del siglo XIX; fue el único nacido en la que fue capital musical europea a finales del siglo XVIII y principios del XIX: Viena. Vivió 31 años, tiempo durante el cual consiguió componer una obra musical excelente, en especial en la última etapa de su vida, en la cual estaba ya tremendamente enfermo. Su música no fue reconocida durante toda su vida y es a partir de finales del siglo XIX cuando la música de Schubert suena con más frecuencia. Se abre camino con pequeñas obras para piano, para más tarde empezar con sinfonías. En la historia ha estado siempre escondido, a la sombra de Beethoven, solo se dice que su música es muy bella, pero resulta que no encontró ninguna orquesta que la tocara y prácticamente solo en las "schubertiadas" podía exhibir su música. Escribió más de seiscientos "lieder", de los cuales gran parte, después de su fallecimiento, quedaron inéditos. Siempre ha sido visto como un compositor "frágil" que hacía bellas melodías, pero si analizamos su música, nos damos cuenta de que está repleta de tristeza. "No existe música alegre" -Franz Schubert.

Desde que nació, la muerte estuvo presente en la vida de este compositor. Eran trece hermanos aunque diez de ellos murieron al nacer, otro de ellos murió prematuramente y su madre falleció en su último parto cuando él tenía apenas quince años. La única familia que le quedaba era su padre y su otro hermano. Residían en el barrio de Liechtental. Su padre era profesor y gozaban de unos escasos ingresos para vivir.

Es un gran cantante e ingresa en "Stadtkonvikt", la escolanía de la Catedral de Viena como soprano, institución la cual le becaba y mantenía aparte de darle la oportunidad de tener como maestro al compositor Antonio Salieri. La necesidad de componer se reveló con fuerza en el joven Schubert durante estos años y sus primeras piezas fueron interpretadas por la orquesta de discípulos del Stadtkonvikt, de la que él mismo era violinista.

En 1813 le cambia la voz y no puede seguir estudiando en la escolanía, así que no le queda más remedio que trabajar como asistente de su padre en la escuela, a pesar de que no le agradaba la idea de tener que hacerlo, puesto que quería dedicarse a la música por completo.

1814 es una época de creatividad para Schubert, ya que conoce a su primer amor, Therese Grob. Estrena la Misa en la mayor con Therese de soprano. En estos años es cuando ven la luz sus primeras obras maestras, como el lied "El rey de los elfos", inspirado en un poema de Goethe, uno de sus escritores más frecuentados. Después de abandonar sus funciones en la escuela paterna, Schubert intentó ganarse la vida únicamente con su música, con escaso éxito en su empresa. La principal meta que tenía Schubert estos años era encontrar trabajo como fuera y ganar dinero para que el padre de la chica de la que se había enamorado le dejara estar con ella, y el único campo que podía darle dinero de verdad era la ópera. Aunque éste fue un género que Schubert abordó con insistencia a lo largo de toda su vida, bien fuera por la debilidad de los libretos escogidos o por su propia falta de aliento dramático, nunca consiguió destacar en él. Sus óperas, entre las que merecen citarse "Los amigos de Salamanca", "Alfonso y Estrella", "La guerra doméstica" y "Fierabrás", continúan siendo la faceta menos conocida de su producción.

Si Schubert no logró sobresalir en el género dramático, sí lo hizo en el lied. Un solo dato da constancia de su absoluto dominio en esta forma: sólo durante los años 1815 y 1816 llegó a componer más de ciento cincuenta lieder, la mayoría de una calidad asombrosa. Escritos muchos de ellos sobre textos de sus amigos, como Johann Mayrhofer y Franz von Schober, eran interpretados en reuniones privadas, conocidas famosamente como «schubertiadas», a las que asistía, entre otros, el barítono Johann Michael Vogl, destinatario de muchas de estas breves composiciones.

Pese a sus talentos, su padre pretendía que heredara su profesión, lo que motivó el enfrentamiento entre ellos y desencadenó la mala relación entre ambos y el abandono de la casa paterna.
Fuera del hogar y habiendo decidido ganarse la vida con la música, Schubert se refugió en la casa de Franz von Schober. Así comenzó el peregrinaje. Nunca logró mantenerse sólo con sus composiciones y necesitó de la generosidad de amigos, que lo acogían en sus respectivas casas. Schubert tampoco mantuvo una relación duradera ni tuvo hijos, pero se adscribió a un círculo íntimo de amigos que le brindaba muchas satisfacciones personales, además de constituir un público fiel y sensible a su arte.

Schubert no pudo estrenar ni publicar ninguna de sus obras operísticas u orquestales. A lo sumo se interpretaron algunas composiciones vocales o pianísticas en las célebres "schubertiadas".

En estos años Schubert contrajo sífilis. Habitualmente pasaba estrechez económica. Se volvió inseparable de sus gafas, que conformaron parte indisoluble de su apariencia y acentuaron su fisonomía tímida.

En Viena, Schubert llevó una vida bohemia rodeado de intelectuales, amante de las tabernas y de los ambientes populares, alejado de los salones y de la etiqueta nobiliaria. De este entorno procede el famoso término de "schubertiadas": reuniones de artistas de todos los ámbitos que formaban un círculo brillante y animado dedicado a la música y a la lectura.

Cuando la sífilis comenzó a trastornar su vida fue, lógicamente, cuando apareció el lado más amargo y melancólico de Schubert. Así, no puede sorprendernos que, necesitado de comprensión, escribiese en 1824 estas líneas estremecedoras a su amigo Leopold Kupelwieser:
Durante sus últimos años escribió piezas magistrales, fruto y reflejo de sus experiencias personales y siempre con el sello inconfundible de una inagotable inspiración melódica. Por ejemplo, una tensa profundidad marca la "Wanderer-Fantasie", D. 760, para piano solo (1822) o el ciclo de "lieder" "La bella molinera (Die schöne Müllerin)" (1823), estos últimos inspirados en poemas de Wilhelm Müller. En 1824 escribiría "La muerte y la doncella", uno de sus cuartetos más conocidos, y ya hacia el final de su vida el intenso dolor y el aislamiento dejaron su impronta en el "Winterreise, D. 911, Op. 89" (1827), también con textos de W. Müller.

Por aquel entonces, Schubert tenía solamente treinta y un años y acababa de matricularse para estudiar fuga. Pero una gonorrea, complicada finalmente con una fiebre tifoidea, lo condujo a la muerte el 19 de noviembre de 1828. Se decía de Schubert que hacía tiempo ya «andaba por el mal camino», se hablaba de su afición al alcohol y la «sensualidad» —que lo llevó a tener relaciones esporádicas—. Pero esa debilidad no ensombrece de ningún modo la figura de un hombre que en sus años de madurez padecía, según muchos biógrafos, de lo que actualmente llamaríamos trastorno bipolar. Esto explicaría que grandes obras quedaran incompletas sin una razón explícita.

El 30 de octubre de 1822 comenzó su "Sinfonía en si menor" pero, tras dos movimientos en una partitura de orquesta cuidadosamente pasada a limpio, y de empezar el tercero, la abandonó. El manuscrito con ambos movimientos completos pasó a manos de su amigo, An. Hüttenbrenner, quien los conservó en un cajón durante más de cuarenta años. En 1865 se los entregó al director de orquesta Johann von Herbeck, quien en diciembre de ese mismo año dirigió en Viena el estreno de la obra incompleta. 

No hay una conclusión a la cuestión sobre los motivos que condujeron a Schubert a dejarla inconclusa; una posibilidad sugiere que parte del manuscrito se perdiera. También se ha sugerido que el poderoso "Entreacto en si menor de la música de escena para Rosamunda," de 1823, fuera en realidad el último movimiento sinfónico. A favor de esta tesis: las coincidencias en orquestación con ambos movimientos existentes, incluido el añadido de los tres trombones incorporados a la orquesta clásica convencional, así como la tonalidad. A pesar de todo, la explicación más verosímil para la crítica es la que cuestiona la madurez autorial para completar dos movimientos más con la misma altura y calidad expresiva de los previos. Así, la obra queda tal como la conocemos hoy: un díptico asimétrico, pero equilibrado: primero un "Allegro moderato", en el que se contraponen la tensión dramática inicial y la naturalidad lírica, seguido de un "Andante con moto en mi mayor", pleno de un agitado y tumultuoso vagabundeo, que alcanza al final el descanso en una coda, cuya serenidad parece trascender el mundo.

La obra completa de Schubert se publicó entre 1884 y 1897 en la editorial Breitkopf & Härtel. Fue especialmente relevante, dentro de esta, la edición de las canciones, encomendada al musicólogo y compositor Eusebius Mandyczewski, quien realizó un trabajo tan meticuloso que todavía hoy es de referencia.

La numeración tradicional de las obras de Schubert fue sustituida poco a poco por la catalogación que hizo el erudito Otto Erich Deutsch y publicada por vez primera en el año 1951. La notación se compone de la letra "D" seguida por un número y, en algún caso, una letra minúscula para insertos o hallazgos posteriores. Por ejemplo, la "Sinfonía n.º 8 «Inacabada» o «Incompleta»" lleva como número de catálogo D 759.





</doc>
<doc id="6635" url="https://es.wikipedia.org/wiki?curid=6635" title="Computación distribuida">
Computación distribuida

La computación distribuida o informática en malla (grid) es un modelo para resolver problemas de computación masiva utilizando un gran número de ordenadores organizados en clústeres incrustados en una infraestructura de telecomunicaciones distribuida.

Un sistema distribuido se define como una colección de computadoras separadas físicamente y conectadas entre sí por una red de comunicaciones; cada máquina posee sus componentes de hardware y software que el programador percibe como un solo sistema (no necesita saber qué cosas están en qué máquinas). El programador accede a los componentes de software (objetos) remotos, de la misma manera en que accedería a componentes locales, en un grupo de computadoras que usan un middleware entre los que destacan (RPC) y SOAP para conseguir un objetivo. 

Los sistemas distribuidos deben ser muy confiables, ya que si un componente del sistema se descompone otro componente debe ser capaz de reemplazarlo. Esto se denomina "tolerancia a fallos". 

El tamaño de un sistema distribuido puede ser muy variado, ya sean decenas de hosts (red de área local), centenas de hosts (red de área metropolitana), o miles, o millones de hosts (Internet); esto se denomina "escalabilidad".

La computación distribuida ha sido diseñada para resolver problemas demasiado grandes para cualquier supercomputadora y mainframe, mientras se mantiene la flexibilidad de trabajar en múltiples problemas más pequeños. Por lo tanto, la computación en malla (grid) es naturalmente un entorno multiusuario; por ello, las técnicas de autorización segura son esenciales antes de permitir que los recursos informáticos sean controlados por usuarios remotos.

En términos de funcionalidad, las mallas se clasifican en computacionales (incluyendo mallas de barrido de la CPU) y en mallas de datos.
Y en los estados son:

Los servicios web basados en XML ofrecen una forma de acceder a diversos servicios/aplicaciones en un entorno distribuido. Recientemente, el mundo de la informática en malla y los servicios web caminan juntos para ofrecer la malla como un servicio web. La arquitectura está definida por la "Open Grid Services Architecture (OGSA)". La versión 3.0 de Globus Toolkit, que actualmente se encuentra en fase alfa, será una implementación de referencia acorde con el estándar OGSA. 

Las mallas ofrecen una forma de resolver grandes retos, como el plegamiento de las proteínas y descubrimiento de medicamentos, modelización financiera, simulación de terremotos, inundaciones y otras catástrofes naturales, modelización del clima/tiempo, etc. Ofrecen un camino para utilizar los recursos de las tecnologías de la información de forma óptima en una organización.

El modelo de computación de ciclos redundantes, también conocido como "computación zombi", es el empleado por aplicaciones como Seti@Home, consistente en que un servidor o grupo de servidores distribuyen trabajo de procesamiento a un grupo de computadoras voluntarias a ceder capacidad de procesamiento no utilizada. Básicamente, cuando dejamos nuestro ordenador encendido, pero sin utilizarlo, la capacidad de procesamiento se desperdicia por lo general en algún protector de pantalla, este tipo de procesamiento distribuido utiliza nuestra computadora cuando nosotros no la necesitamos, aprovechando al máximo la capacidad de procesamiento.

Otro método para crear sistemas de supercomputadoras es el "clustering". Un "cluster" o racimo de computadoras consiste en un grupo de computadoras de relativo bajo costo conectadas entre sí mediante un sistema de red de alta velocidad (gigabit de fibra óptica por lo general) y un software que realiza la distribución de la carga de trabajo entre los equipos. Por lo general, este tipo de sistemas cuentan con un centro de almacenamiento de datos único.
Los clusters tienen la ventaja de ser sistemas redundantes, al estar fuera de servicio el procesador principal el segundo se dispara y actúa como un Fail Over.

La computación en grid o en malla es un nuevo paradigma de computación distribuida en el cual todos los recursos de un número indeterminado de computadoras son englobados para ser tratados como un único superordenador de manera transparente.

Estas computadoras englobadas no están conectadas o enlazadas firmemente, es decir no tienen por qué estar en el mismo lugar geográfico. Se puede tomar como ejemplo el proyecto SETI@Home, en el cual trabajan computadoras alrededor de todo el planeta para buscar vida extraterrestre.

El punto de la seguridad es delicado en este tipo de computación distribuida pues las conexiones se hacen de forma remota y no local, entonces suelen surgir problemas para controlar el acceso a los otros nodos. Esto puede aprovecharse para un ataque de DoS, aunque la red no va a dejar de funcionar porque uno falle. Esa es una ventaja de este sistema grid.

SSI (Single System Image): en un SSI todas las computadoras vinculadas dependen de un sistema operativo común, diseñado al efecto. En cambio, un grid es heterogéneo, en el sentido en que las computadoras pueden tener diferentes sistemas operativos.

Algunos ejemplos de estos sistemas operativos son:


Cluster de computadores: En un clúster todos los nodos se encuentran en el mismo lugar, conectados por una red local para así englobar todos los recursos. En cambio, en un grid no tienen por qué estar en el mismo espacio geográfico; pueden estar en diferentes puntos del mundo.

También suele llamarse GRID al resultado obtenido de englobar las máquinas en una supercomputadora; con un clúster solo se busca mejorar el rendimiento de las máquinas englobándolas en una sola.




</doc>
<doc id="6637" url="https://es.wikipedia.org/wiki?curid=6637" title="Varón">
Varón

Varón es un ser humano de sexo masculino, independientemente de si es niño o adulto. La palabra «varón» en español deriva del latín "varo" («valiente», «esforzado»), muy probablemente relacionada con "vir" («varón», «héroe») bajo la influencia del germánico "baro" («hombre libre»).

La testosterona es una hormona androgénica propia del macho en muchas especies, que permite desarrollar los músculos del varón con poco esfuerzo y es determinante en parte de su desarrollo físico y de las características sexuales secundarias.

El aparato reproductor masculino garantiza que el varón tenga la capacidad de fecundar el óvulo femenino y en ello la transmisión de la información genética por medio de la célula espermatozoidal. Los órganos sexuales primarios del varón son exteriores, a diferencia de los de la mujer que son internos. La andrología es la ciencia que estudia el aparato reproductor masculino.

Entre las características secundarias más comunes que empiezan a desarrollarse a partir de la pubertad y la edad viril (y que no necesariamente son siempre así) sin que su ausencia vaya en contra de la identidad masculina, se cuentan las siguientes


Tanto varones como mujeres son víctimas del mismo tipo de enfermedades que afectan al género humano, pero cada género tiene una tendencia mayor a un determinado tipo. Las enfermedades que más se manifiestan en el varón son el Autismo, el Daltonismo y el Mal de Alzheimer, que ataca principalmente en la edad mayor, pero puede presentarse en varones jóvenes.

Las expectativas de vida masculina, como las femeninas, varían considerablemente de acuerdo al desarrollo de cada sociedad.

En cuanto a la tasa de mortalidad infantil a nivel global, se considera que los varones recién nacidos tienen una mayor esperanza de vida que las niñas. 

La desfase entre la población neonata masculina y femenina se equipara durante la adolescencia, tiempo en el cual aumenta en todos los continentes la morbilidad masculina por encima de la femenina debido a la mayor participación de los varones en confrontaciones armadas, guerras o simplemente en el desafío del peligro. Otros riesgos como el consumo de estupefacientes, alcohol, enfermedades de transmisión sexual y violencia urbana, mayor entre los varones que entre las muchachas, reducen la población masculina adolescente en todo el mundo.

La más popular alteración física de la constitución sexual del varón es la circuncisión, una práctica muy antigua y que tiene desde razones religiosas hasta de salud. La circuncisión es una operación que se practica por lo general al recién nacido con la remoción del prepucio de su pene. Aparte de las razones religiosas que se tienen, la circuncisión ha probado ser un método de prevención contra el cáncer de pene. Pero la circuncisión no es tan restringida a un grupo religioso como muchos piensan. Las estadísticas hablan de que en el mundo por lo menos un 20% de los varones son circuncisos, especialmente en las sociedades judías, América del Norte, las Filipinas, Corea del Sur y los países musulmanes.

La circuncisión también es vista como una forma de ablación genital masculina, entre otras . Entre las causas de que la circuncisión no provoque el mismo impacto social que otras formas de mutilación, atendería a razones de proximidad y tradición, al ser algo relativamente común y cercano, esta práctica sería vista como algo no trágico a pesar de que el debate sanitario sobre los beneficios de esta práctica no es concluyente. Aún habiendo hombres damnificados por esta práctica se ignora la repercusión negativa de la circuncisión. 

Un ser humano del género masculino es varón desde el momento en el cual es concebido: el espermatozoide contiene los cromosomas sexuales diferenciados XY, mientras la hembra tiene los cromosomas homogaméticos XX. La combinación cromosómica entre el espermatozoide y el óvulo determina el sexo del individuo concebido, lo que da como resultado que un feto pueda ser determinado como “hembra” si la combinación cromosómica es XX y como varón si es XY. La combinación genética XX es más frecuente que la combinación genética XY, mientras que la mortalidad infantil es menor en varones recién nacidos que en niñas.

El varón infante recibe el nombre de “niño” al menos hasta el inicio de su pubertad. También es popular llamarlo “mozo”, palabra que lo determina hasta su primera juventud (aproximadamente hasta los 20 años de edad). Durante este tiempo comienza todo el proceso de desarrollo físico, psicológico y social como “varón” que le permitiría desarrollar un rol determinado por la cultura a su condición humana masculina.

La prevalencia del varón en las sociedades da lugar a lo que se ha denominado como el machismo. Áreas como la política, la religión y la ciencia entre otros han sido vistas tradicionalmente como “cosas de hombres” sin que deje de ser un supuesto asumido. En general este ha sido el elemento de batalla de los grupos feministas. Pero la figura del varón se ha visto además afectada por múltiples elementos culturales entre los cuales ha jugado un papel importante el fenómeno de globalización, el feminismo, las crisis sociales y otros factores. En cuanto a los Medios de comunicación, estos, dominados especialmente por la Civilización Occidental, han impuesto la figura greco-romana del varón atlético. En tal caso, la figura del varón occidental puede verse en muchos casos reflejada en países del mundo en donde adolescentes siguen las modas de cantantes y actores especialmente.

La discusión acerca de las diferencias entre varones y mujeres, especialmente en Occidente no es unánime. Psicológicamente, la asociación tradicional de aptitudes y actitudes a un género normalmente se basa en suposiciones consolidadas por el hábito de la observación directa, de la actividad y personalidad de las personas de ambos géneros en el contexto social. Esta asociación se arraiga principalmente en la edad infantil.

Los estereotipos masculinos varían según el nivel cultural de la sociedad, la edad y el momento histórico. Por ejemplo, estudiantes y personas adultas definen de forma diferente lo que se considera masculino. Los estudiantes elaboran unos estereotipos de rol de género más claramente definidos que las personas adultas. Los estereotipos masculinos normalmente está más definido que los estereotipos femeninos. No obstante, esta asignación de características es cada vez más alejada de la realidad, por lo que los mismos estereotipos de género van cambiando paulatinamente, conforme al cambio de tareas tradicionalmente asignadas a uno de los dos sexos como, por ejemplo, la incorporación de la mujer al mundo laboral. Así mismo, el incremento de la actividad de las mujeres en los ámbitos deportivos propicia un cambio del estereotipo tradicional masculino.

Las sociedades y culturas orientales o más conservadoras, asumen muchos de esos estereotipos como lo que es o debe ser en el varón, pero la era de la globalización poco a poco los hace entrar en el debate. Entre los "estereotipos" más comunes se pueden enumerar:


Muchos de estos paradigmas tienen fundamento científico, mientras que otros no (aunque la sociedad ha hecho que muchos de estos estereotipos sean realidad como por ejemplo, el saludo de dos mujeres puede ser beso, entre hombre y mujer también, pero entre hombres es "raro" sin ninguna razón, etc). Por ejemplo, no es sencillo separar los elementos innatos de la biología masculina de aquellos que han sido influenciados por la cultura. En tal caso, la agresividad puede darse tanto en el varón como en la mujer de acuerdo al ambiente en que estos se desenvuelvan. La mayor masa corporal y muscular del varón y las culturas patriarcales contribuyen a acentuar el estereotipo de la agresividad masculina. Los grupos feministas en sus estudios señalan que en la violencia intrafamiliar, el abuso infantil, el maltrato infantil y la violencia contra la mujer, tienen como principal verdugo en la mayoría de los casos al varón tanto de países industrializados como en vías de desarrollo.

Algunos de estos estereotipos se asocian, en ocasiones erróneamente y en ocasiones acertadamente con los niveles de hormonas sexuales masculinas, como la testosterona, o la menor cantidad de hormonas sexuales femeninas, como los estrógenos. En el caso de la agresividad, tradicionalmente relacionada con el nivel de testosterona, algunos estudios indican que dicha relación no corresponde con sus resultados.

Desde su nacimiento se viste a los varones de celeste y se les enseña a creer que productividad, conquista, poder, hiperactividad y penetración son sinónimos de virilidad. De pequeños se les enseña a no llorar, a no ser vulnerables, a no quejarse, a no mostrar sus debilidades ni sus sentimientos y a ser autosuficientes y no pedir ayuda. Se les enseña a confundir acción y agresión con masculinidad, a rendir en los deportes aún a expensas de su propia salud, a exponerse a peligros y a deportes de riesgo. Las consecuencias de la adecuación a este marcado estereotipo social se las puede encontrar en los servicios de terapia intensiva de los hospitales con mayoría masculina, en la población carcelaria, donde la gran mayoría de los reclusos son varones, en las estadísticas de accidentes y en los hechos delictivos que leemos en los diarios.

La educación masculina depende en gran parte de la discusión de los estereotipos masculinos en el grado en que estos sean asumidos por una sociedad. La educación entonces que parte desde el hogar dada al niño, pasa por la formal y se expresa en las relaciones sociales y en la imagen que presentan los medios de comunicación, tiene diversos matices que dependen de la cultura del país, continente o región del mundo.

La primera educación de la sexualidad y socialización del niño parte del hogar. El padre y la madre son los encargados de transmitir la primera información sobre el rol sexual que desempeñará el niño en sociedad. En general, el padre transmitirá al hijo varón las características psicológicas de su sexualidad. En ello entran en juego los paradigmas asumidos y las maneras de ser del varón en la sociedad en la que nació. La manera de vestirse, de llevar el cabello, de hablar, de modular la voz, el tipo de juegos, los juguetes, las exigencias disciplinarias diferenciadas entre el varón y la mujer, la casi ausencia de cosméticos y otros muchos elementos, determinan poco a poco la conciencia propia del ser un varón en sociedad.
Llegada la pubertad, el papel del padre adquiere un rol más activo en la educación del hijo varón. En muchas culturas este paso entre el niño y el hombre es celebrado. Entre culturas del orden natural como tribus y clanes, el muchacho debe afrontar un número determinado de desafíos que le permitirán ser respetado en su grupo social como un varón adulto. En antiguas culturas célebres por su formación militar como los griegos (Esparta por ejemplo), China, Japón (los Samurái), los Azteca, los Quechua y los Chibcha, el paso a la edad adulta del muchacho era marcado por su capacidad de prepararse como un guerrero y su aceptación y aprecio social nacían de su coraje demostrado en las luchas, artes marciales y batallas. Pero también la religión tiene un papel del primer orden en la formación masculina del muchacho. La pubertad está marcada por un rito de iniciación que da al muchacho un estatus social y religioso. Por ejemplo, para el Judaísmo este viene representado en el bar mitzvah, celebración que le da al varón adolescente el derecho de leer los libros sagrados en la Asamblea. Para el Cristianismo ese momento viene marcado por la Confirmación.

Pasada la pubertad, el muchacho comienza un camino de desarrollo final hacia la adultez en la cual compite por demostrar la capacidad de su identidad como varón. Los deportes de competencia y fuerza física, por ejemplo, adquieren una enorme importancia, el afán por tener una pareja, el ingreso en un grupo social de adolescentes (la pandilla), la búsqueda de una vocación y otros son la preocupación del muchacho, situaciones no siempre pacíficas. Resta el peligro del consumo de drogas, alcohol, fumar, delincuencia y otros males sociales en el cual el joven ingresa en muchos casos llevado por el ánimo de una búsqueda de su propia identidad e independencia.

El rol sexual del varón adquiere su máxima plenitud en el matrimonio como marido y como padre. El rol masculino ha tenido una diversidad de influencias a lo largo de la historia. La Revolución industrial, la Revolución Femenina y otros momentos, han tenido sus consecuencias en la figura del padre y marido. Obviamente partimos de una lectura de Occidente, porque en otras culturas no occidentales, este papel puede estar marcado por una concepción más tradicionalista como la llamada Familia patriarcal en la cual la figura paterna es el centro de toda autoridad. En India y otros sitios de la tierra, se practica la dote en la cual el padre de la hija paga una cierta cantidad al padre del hijo varón. Dicha práctica trae como desventaja principal un cierto desdén en la concepción de las niñas, las cuales son vistas más como una carga y abre las puertas al infanticidio femenino. En otros países en cambio, como Camboya, la tradición es al contrario, es el padre del hijo varón quien da la dote al padre de la hija. Pero en ambos casos, la libertad de ambos jóvenes se ve restringida en la escogencia del cónyuge, la cual es decisión de sus padres. Casos similares se presentan entre las culturas musulmanas, muchas de las cuales todavía practican la poligamia, es decir, el varón puede casarse con varias mujeres.

No siempre la heterosexualidad en el varón fue vista como la única opción. De hecho, en sociedades antiguas la atracción hacia otros varones y la actividad sexual con ellos era considerada tan normal como la expresada hacia las mujeres, y esta característica predomina en la cultura grecorromana. La milicia utilizó este tipo de relaciones para unir a los guerreros con fines de autoprotección y compañerismo, mientras que ciertos autores griegos y latinos dan por hecho que todos los hombres sienten deseo homosexual en algún momento.




</doc>
<doc id="6638" url="https://es.wikipedia.org/wiki?curid=6638" title="Mujer">
Mujer

Mujer (del latín "mulĭer", "-ēris"), o fémina (lat. "femĭna"), es el ser humano femenino o hembra, independientemente de si es niña o adulta. Mujer también remite a distinciones por género de carácter cultural y social que se le atribuyen, así como a las diferencias sexuales y biológicas de la hembra en la especie humana. Cuenta con aspectos físicos que la diferencian del varón como voz más aguda, cintura más estrecha, cadera más ancha y pelvis más amplia, menos vello corporal, menos masa muscular y más tejido adiposo, mamas más grandes y estrechas y menor estatura comparada con el varón. El Día Internacional de la Mujer Trabajadora se celebra el día 8 de marzo.

La mujer ha realizado y realiza el esfuerzo de trabajo reproductivo que permite la supervivencia de individuos y sociedades. A lo largo de la historia y hasta fechas recientes, con el objeto de garantizar la supervivencia social y en un contexto de altísima mortalidad (tanto en tasa bruta de mortalidad como en mortalidad infantil), ha sido necesario mantener una muy alta natalidad (tanto en la tasa bruta de natalidad como en la tasa de fecundidad) para garantizar un reemplazo suficiente de las poblaciones.

La mejora en la alimentación, la generalización de la higiene, la sanidad y la difusión de medicamentos han sido decisivos para el fuerte crecimiento de la población mundial que ha pasado de los casi 1.000 millones en el año 1800 a más de 6.000 millones en el año 2000 y a 7.000 millones a finales de 2011.

La necesidad de una alta reproducción ha dejado de ser uno de los tradicionales problemas de las sociedades -y por supuesto del mundo en su conjunto- para incluso convertirse, para algunos autores de corte neomalthusiano, en un nuevo problema, la superpoblación.

La reducción de la tasa bruta de mortalidad es característica de la denominada transición demográfica así como una fuerte reducción de las tasa de natalidad es característica de la segunda transición demográfica junto con cambios sociológicos que afectan básicamente al papel tradicional de la mujer.

Los avances y difusión de los métodos anticonceptivos junto con la reducción de la presión social sobre la mujer para mantener la población -al alcanzarse una alta supervivencia de las poblaciones- permiten que se produzca lo que algunos autores como John MacInnes y Julio Pérez Díaz denominan revolución reproductiva. El esfuerzo reproductivo se reduce, la supervivencia de los individuos -la baja mortalidad- permite entonces reducir sustancialmente el número de hijos. En las sociedades modernas se da una alta eficiencia reproductiva que libera a la mujer de buena parte del trabajo que desarrollaba tradicionalmente y la permite incorporarse al mercado de trabajo modificándose sustancialmente las relaciones sociales antes establecidas y advirtiéndose cambios sustanciales: declive del trabajo reproductivo (fundamentalmente en la mujer), derrumbamiento del patriarcado, privatización de la sexualidad y reducción del control social sobre la sexualidad; desaparición de la punibilidad de las relaciones sexuales no reproductivas; alto control sobre la procreación con el uso de métodos anticonceptivos y apoyo intergeneracional muy amplio a hijos y nietos, reforzamiento de los lazos familiares profundos; aumento de los años vividos o madurez de masas; centralidad de la familia y reforzamiento de los lazos e importancia de la misma.

Aunque existe gran diversidad, dependiendo del tipo de sociedad matrilineal, patrilineal, cazadores recolectores, agrícola filiación, puede decirse que desde la prehistoria, las mujeres han asumido un papel cultural particular normalmente diferenciado y todas las sociedades documentadas han conocido la división sexual del trabajo, por el cual las tareas necesarias para la subsistencia eran asignadas en función del sexo de la persona. En sociedades de caza y recolección, las mujeres casi siempre eran las que recogían los productos vegetales, mientras que los varones suministraban la carne mediante la caza de animales. A causa de su conocimiento profundo de la flora, la mayor parte de los antropólogos creen que fueron las mujeres quiénes condujeron las sociedades antiguas hacia el Neolítico y se convirtieron en las primeras agricultoras. De hecho, los datos de muchas sociedades recolectoras modernas han mostrado que la mayor parte de las calorías ingeridas provienen de la recolección realizada por las mujeres.
En la antigüedad clásica, tanto griegos como romanos documentaron ampliamente que muchos otros pueblos mediterráneos y europeos de hecho no eran patriarcados. En muchos pueblos existía una organización matrilineal, esto está documentado en varias sociedades protohistóricas de Europa. Existen abundantes elementos para pensar que originalmente entre los lidios, etruscos, astures, minoicos y algunos pueblos germanos las mujeres tenían un papel mucho más preponderante que en la sociedad griega y romana. Igualmente en la antigua Persia las mujeres tenían un papel social más preponderante que las mujeres romanas. La situación en otras partes del planeta está menos clara.

En la Edad Media europea, los autores masculinos, pertenecientes a una estirpe, religiosos, tratadistas laicos y sobre todo, predicadores, hablaron de las condiciones y conductas que les exigían a las niñas, a las jóvenes y a las mayores. La conducta femenina fue pautada para cada momento y situación de la vida. Casi siempre la edad corresponde a un estado civil determinado y a una función de acuerdo a ella. Tal es así que la mujer se representaba en la imagen de la novia, la prometida, la casada, la viuda, es decir, siempre ligada inexorablemente a un varón que debía responsabilizarse de ella y su conducta. El papel más importante atribuido a la mujer era el de esposa y madre.

En la historia reciente, las funciones de las mujeres han cambiado enormemente. La burguesía trajo consigo una nueva concepción de la familia donde la mujer desempeñaba un papel restringido al hogar. Hasta entonces la mujer había participado, aunque de modo distinto al varón, en tareas de aprovisionamiento y trabajo para la supervivencia familiar fuera del domicilio u hogar. Las funciones sociales tradicionales de las mujeres de la clase media consistían en las tareas domésticas, acentuando el cuidado de niños, y no solían acceder a un puesto de trabajo remunerado. Para las mujeres más pobres, sobre todo entre las clases obreras, esta situación era a veces un objetivo, ya que la necesidad económica las ha obligado durante mucho tiempo a buscar un empleo fuera de casa, aunque las ocupaciones en que se empleaban tradicionalmente las mujeres de clase obrera eran inferiores en prestigio y salario que aquellas que llevaban a cabo los varones. Eventualmente, el liberar a las mujeres de la necesidad de un trabajo remunerado se convirtió en una señal de riqueza y prestigio familiar, mientras que la presencia de mujeres trabajadoras en una casa denotaba a una familia de clase inferior.

La mujer española durante la conquista de América, viajaba con su esposo o sino llegaba lo más pronto posible a su localización. Para el varón, estar casado era un beneficio; se respetaba a los varones casados con hijos. Igualmente para la mujer era un beneficio, especialmente si estaba con un varón de alto título, así poseía riquezas y poder. Cuando los conquistadores iban a misiones, las que se encargaban de mantener las "cosas corriendo" en los territorios conquistados eran las mujeres españolas. Estas mujeres aportaron grandemente al proceso de la conquista de América.

En el siglo XIX ocurre una transformación en los ámbitos social, político y económico. En esta etapa se movilizaron mujeres feministas en pos de la igualdad de género. Las mujeres en países de primer mundo, recibieron libertad en el sentido de expresión hasta poder ser parte del mundo laboral. En 1979, se aprobó la Eliminación de todas las formas de Discriminación contra la Mujer. Este evento, aprobado por la Asamblea de Naciones Unidas, fue un logro para las mujeres quienes lucharon por sus derechos en la sociedad. La mujer a través de la historia ha tenido que combatir muchos problemas. Con los siglos los derechos, roles y estereotipos de las mujeres han evolucionado; desde la Edad Media hasta el Siglo XXI. Los derechos humanos de la mujer, define la discriminación contra la mujer como "toda distinción, exclusión o restricción basada en el sexo.
Las mujeres nativas y africanas, se consideraban mujeres guerreras y ayudantes en el periodo de conquista. Los conquistadores españoles se enfrentaban a estas mujeres poderosas durante sus invasiones. No retrocedían al momento de batallar a los europeos contra sus armas. Al contrario, la estrategia de los europeos fue utilizar a las mujeres españolas para controlar las sociedades nativas y a la misma vez empezar la transmisión cultural. El pensar era que los varones guerreros no iban a rebelarse estando mujeres y niños presentes. 
Un descubrimiento importante de la antropología moderna fue que fuera de Europa las sociedades no siempre eran patriarcales. Así los antropólogos descubrieron durante el siglo XIX y XX, centenares de sociedades matrilineales en las que las mujeres tenían un papel mucho más destacado, que el de las mujeres de Europa y sus colonias en América. De hecho un número signififcativo de las sociedades humanas podrían haber sido matrilineales y no patrilineales como la mayor parte de sociedades históricas europeas. Un análisis contenido en el "Ethnographic Atlas" (1967) de George P. Murdock sobre 752 sociedades históricamente documentadas dio los siguientes datos:

En los datos de Murdock, se observa que algo más de una quinta parte de las sociedades tienen un régimen de filiación matrilineal, en el que los individuos reciben el nombre familiar, la herencia y el prestigio de su rama materna. De acuerdo a centenares de descripciones antropológicas, queda claro que, en general, en las sociedades matrilineales las mujeres tienen un estatus social más alto que en sociedades patrilineales. Aunque no puede decirse que las sociedades matrilineales sean matriarcados, ya que aunque la mayor parte del poder económico y familiar está en manos de las mujeres, en estas sociedades matrilineales muchas de las más altas responsabilidades políticas y legislativas están en manos de ambos sexos.

El movimiento feminista ha perseguido el reconocimiento de la igualdad de oportunidades y la igualdad de derechos entre mujeres y hombres. Como movimiento político coherente y organizado colectivamente, el feminismo arranca en las sociedades europeas del siglo XIX. Durante el siglo XX, el feminismo ha crecido notoriamente en todo el mundo, primero en América y más tarde también Oceanía, Asia y África.

Factores asociados al modo de producción como la división sexual del trabajo, además de factores históricos, en combinación con las costumbres y las tradiciones sociales y religiosas condicionaron fuertemente que especialmente en las sociedades europeas de la antigüedad, la edad media y moderna, el patriarcado fuera la forma de organización social dominante, en la que los hombres tenían un papel muy preponderante, especialmente en los asuntos públicos. Las revoluciones burguesas y proletarias de los siglos XIX y XX, ayudaron a que las tesis de los movimientos feministas tuvieran reflejo y amparo en las legislaciones de la mayor parte de estados modernos.

Actualmente, en la mayor parte de estados, debido a los cambios económicos, el apoyo del poder económico y las reivindicaciones del movimiento feminista y otros movimientos de derechos humanos, las mujeres tienen acceso a carreras profesionales y trabajos similares a los de los hombres en la mayor parte de las sociedades. En muchas sociedades modernas las mujeres tienen plena igualdad jurídica tanto en el ámbito laboral como en el familiar, pudiendo ser cabezas de familia, detentar cargos altos tanto en política como en grandes empresas. Así que se podría decir que las condiciones de las mujeres han mejorado.

Algunas corrientes feministas cambian constantemente el significado de la palabra mujer, entendiéndose que la categoría mujer está estrechamente vinculada a la expresión de genitalidad, por lo que frecuentemente se presupone que mujer es aquella cuya expresión gonádica es igual a XX. Esta articulación discursiva se soporta sobre fundamentos biológicos y esencialistas. La naturalización del concepto impide su cuestionamiento, dogmatizándolo. Sin embargo, desde diferentes corrientes feministas, esto ha sido criticado. El rol sexual y el ejercicio de la sexualidad son en sí mismo, construcciones socioculturales motivadas por un mecanismo de control social, y de una reproducción de las estructuras de poder. Además, la categoría mujer se conceptualiza en tanto que opuesta a la categoría hombre, formando así un binomio, mutuamente excluyente, a partir del cual se articula la distinción de sexo (femenino - masculino, respectivamente). En esta situación existe opresión social cuando las personas no reproducen los esquemas preestablecidos de acuerdo a lo esperado, limitando la diversidad sexual, omitiendo y dejando al margen situaciones tales como la transexualidad y la intersexualidad.
El sufragio femenino ha sido garantizado y revocado, varias veces en varios países del mundo. En muchos países, el sufragio femenino se ha garantizado antes que el sufragio universal; así, una vez concedido este, a mujeres y varones de ciertas razas, aún se les seguía negando el derecho a votar.

El primer sufragio femenino, con las mismas características propias que el masculino, se garantizó en Nueva Jersey en 1776, aunque rescindió en 1807. Pitcairn garantizó el sufragio femenino en 1838. Varios países y estados garantizaron un sufragio femenino restringido en la segunda mitad del siglo XIX, empezando por Australia del Sur en 1861. El primer sufragio femenino sin restringir, en lo que a derecho a votar se refiere, ya que a las mujeres no se les permitía presentarse a elecciones, se garantizó en Nueva Zelanda en 1893.

La primera mujer en ejercer formalmente el derecho al voto político en América Latina fue Matilde Hidalgo de Procel en 1924, en la ciudad de Loja, convirtiendo al Ecuador en el primero de la región que permitió el voto femenino. Sin embargo no se descarta anteriores brotes de lucha por la participación de la mujer en la política. Seguramente Matilde Hidalgo de Prócel, quien además sería la primera mujer en recibirse de una carrera universitaria y doctorarse en medicina en el Ecuador, abrazaría la influencia de un importante movimiento femenino chileno por el derecho al sufragio que "apoyándose en la resolución del ministro Zenteno, se inscribió para votar por Benjamín Vicuña Mackenna en las elecciones presidenciales de 1876. Al calor de la campaña antioligárquica de este candidato, las mujeres reclamaron el derecho a sufragio y, a pesar de la negativa de las autoridades, alcanzaron a inscribirse en La Serena.".

A lo largo de la historia, en la mayoría de las culturas, las mujeres han sido sometidas a estructuras patriarcales que les han negado los derechos humanos más fundamentales.
Las leyes antiguas y los sistemas tradicionales, como el cristianismo y el islamismo, antecedentes de los sistemas modernos, han provocado la dependencia de la mujer, de forma análoga a la esclavitud, a la explotación de las clases desfavorecidas y a la mano de obra.

Una de las razones podría ser el fortalecimiento y sostenimiento del poder y de la actividad económica y de igual forma se evidencia que quienes resultan sometidos son vistos, por los explotadores, como seres inferiores, inmaduros, infantiles, malvados o depravados. (Véase el artículo Prejuicio cognitivo)

La Declaración de los Derechos de la Mujer y de la Ciudadana fue un texto redactado en 1791 por Olympe de Gouges parafraseando la Declaración de Derechos del Hombre y del Ciudadano del 26 de agosto de 1789, el texto fundamental de la revolución francesa. Es uno de los primeros documentos históricos que propone la emancipación femenina en el sentido de la igualdad de derechos o la equiparación jurídica y legal de las mujeres en relación a los varones.

En algunos países la mujer ha tardado muchos siglos en conseguir igualdad, aunque solo sea teórica, ante la ley. Y aun cuando la ley hable de igualdad, suele haber un gran abismo entre la teoría y la práctica.

La publicación de las Naciones Unidas titulada The World’s Women—1970-1990 dice: “Esta brecha [en la política gubernamental] ha quedado recogida en gran parte en las leyes que niegan a la mujer la igualdad con el varón en lo que respecta a sus derechos de tenencia de tierras, solicitud de préstamos y firma de contratos”. Una mujer de Uganda declaró: “Seguimos siendo ciudadanas de segunda clase... o de tercera clase más bien, pues nuestros hijos varones van delante nuestro. Hasta los burros y los tractores reciben a veces mejor trato”.

El libro Men and Women, editado por Time-Life, dice: “En 1920, la Decimonovena Enmienda de la Constitución de Estados Unidos garantizó a las mujeres el derecho al voto, mucho después que en bastantes países europeos. Pero en el Reino Unido no se les concedió ese privilegio hasta el año 1928 ...”. Como protesta por la injusticia política a la que se sometía a las mujeres, Emily Wilding Davison, sufragista británica, se echó delante del caballo del rey en el derby de 1913, y perdió la vida. Se convirtió en una mártir en la causa de la igualdad de derechos para la mujer.

El propio hecho de que en fechas tan tardías como el año 1990 el senado de Estados Unidos promulgase el decreto Violence Against Women Act, indica que las legislaturas dominadas por el varón han sido lentas a la hora de responder a las necesidades de la mujer.




</doc>
<doc id="6641" url="https://es.wikipedia.org/wiki?curid=6641" title="Pistacia vera">
Pistacia vera

El alfóncigo, alfónsigo o pistachero (Pistacia vera L., Anacardiaceae, o algunas veces Pistaciaceae) es un árbol pequeño del género Pistacia, originario de las regiones montañosas de Grecia, Siria, Turquía, Kirguistán, Turkmenistán, Irán, Pakistán y Afganistán occidental, que produce un importante fruto para uso culinario llamado pistacho, o alfóncigo. A la "Pistacia vera" se la confunde a menudo con otras especies del género "Pistacia", pueden diferenciarse de la "P. vera" por su distribución geográfica originaria y por sus frutos, más pequeños, con un intenso sabor a trementina y una cáscara dura.
Tiene como principal productor a Irán, con 472097 toneladas de pistachos en 2011.

El alfóncigo moderno, Pistacia vera, fue plantado por primera vez en Asia occidental. Su cultivo se extendió al mundo mediterráneo pasando por Irán Central, donde ha sido una cosecha importante durante mucho tiempo. El manuscrito escrito por Anthimus, a principios del siglo VI d. C. “De observatione ciborum” (Acerca de la observación de los alimentos) indica que los pistachos ("pistacia" en latín vulgar) eran bien conocidos en Europa hacia el final de los tiempos de la dominación Romana.

Para su venta en el mundo de habla inglesa, el alfóncigo se ha cultivado más recientemente en Australia, Nuevo México y California. El Departamento de Agricultura de los Estados Unidos (USDA) introdujo este árbol a California alrededor de 1904, pero no fue promovido como un producto comercial en California hasta 1929.

El alfóncigo crece hasta los 10 metros de altura y tiene hojas pinnadas, con (1)3 a 5 folíolos de 10-20 centímetros (4-8 pulgadas) de largo, que se desprenden en la estación fría.

Es una planta desértica y por esto tiene una alta tolerancia al suelo salino. Se ha reportado que crece bien cuando se la irriga con agua que contiene 3000-4000 ppm de sales solubles. Los alfóncigos son bastante resistentes bajo las condiciones correctas, y pueden sobrevivir en temperaturas que van desde –10 °C en invierno, hasta 40 °C en verano. Necesitan estar orientados hacia el sol y en suelo bien drenado.

Los alfóncigos no se desarrollan bien en condiciones de alta humedad, sino que son susceptibles a que sus raíces se pudran durante el invierno si reciben demasiada agua y el suelo no tiene suficiente drenaje libre. Se requieren largos veranos para la adecuada maduración del fruto. Las plantas son dioicas, tienen pies masculinos y femeninos separados. Las flores son apétalas y se reúnen en inflorescencias llamadas panículas (popularmente racimos). 

El fruto es una drupa que contiene una semilla alargada, que es la porción comestible. Está cubierto por una piel carnosa fina, de color verde. En su interior, bajo una cáscara dura y blanquecina, la semilla, de un color verde pálido, está cubierta a su vez por una piel fina de tono malva, y tiene un sabor característico. Comúnmente considerada como fruto, esta semilla es para uso culinario. Cuando el fruto madura, la piel cambia del verde a un amarillo rojizo otoñal y su cáscara se rompe y abre parcialmente, de manera abrupta.

A este rompimiento se le conoce como dehiscencia/eclosión y ocurre acompañado por un sonido audible. La tendencia a esta apertura es una característica que ha sido seleccionada por los humanos. Los cultivares comerciales varían en cuanto a cómo se abren. Cada alfóncigo da en promedio 50 kg de semillas cada dos años.

Los árboles se plantan en huertos y necesitan de siete a diez años para lograr una producción considerable. La producción es alterna, o bienal, lo cual significa que la cosecha es más abundante cada dos años. La producción pico se alcanza aproximadamente a los 20 años. Por lo general, a los árboles se les poda hasta un tamaño que permita realizar la cosecha con mayor facilidad. Un árbol macho produce suficiente polen para que den frutos de ocho a doce árboles hembra. A menudo, la cosecha en los Estados Unidos se lleva a cabo mediante el uso de equipo para sacudir al árbol y hacer caer los frutos.

Los alfóncigos son vulnerables a una amplia variedad de enfermedades, entre las que destaca la infección por el hongo "Verticillium dahliae" que puede llegar a matar a la planta y por el "". Este último provoca panoja y antracnosis (es decir, mata a las flores y a los brotes jóvenes), y puede dañar huertas enteras de alfóncigos.

En California, casi todos los árboles hembra pertenecen al cultivar “Kerman” el cual suele asociarse con el cultivar macho "Peter". Una mata de un árbol maduro de estas variedades se injerta en un pie de dos años de edad. Como patrón se ha venido usando mayoritariamente otra especie del género Pistacia llamada "Pistacia atlantica", la cual está siendo sustituida en los últimos años por un híbrido de ésta con denominada UCB-1 debido a su inmunidad a la Verticilosis.

Los envíos de pistachos empacados al por mayor tienden al auto-calentamiento y a la combustión espontánea a causa de su elevado contenido de grasas y su bajo contenido de agua.

"Producción de pistachos (toneladas en 2005)"

"Pistacia vera" fue descrita por Carlos Linneo y publicado en "Species Plantarum" 2: 1025. 1753.

La palabra alfóncigo (según la Real Academia Española) deriva del árabe hispano "alfústaq", este del árabe clásico "fustuq", este del pelvi "pistag", y a su vez este del griego πιστάκη, "pistákē".

Debido al uso culinario internacional es mucho más conocido como pistacho (del italiano "pistacchio"), siendo uno de los sabores ya clásicos para las cremas heladas y de los batidos de pistacho.





</doc>
<doc id="6643" url="https://es.wikipedia.org/wiki?curid=6643" title="Panícula">
Panícula

Una panícula o panoja es una inflorescencia racimosa compuesta de racimos que van decreciendo de tamaño hacia el ápice. En otras palabras, un racimo ramificado de flores, en el que las ramas son a su vez racimos. 
Se cataloga como un racimo de racimos, posee un raquis principal que se subdivide en raquis secundarios de los cuales se desprenden flores con pedicelo.



</doc>
<doc id="6645" url="https://es.wikipedia.org/wiki?curid=6645" title="Inflorescencia">
Inflorescencia

En botánica, la inflorescencia es la disposición de las flores sobre las ramas o la extremidad del tallo; su límite está determinado por una hoja normal. 
La inflorescencia puede presentar una sola flor, como en el caso de la magnolia o el tulipán, o constar de dos o más flores como en el gladiolo y el trigo. En el primer caso se denominan inflorescencias unifloras y en el segundo se las llama plurifloras. 

Las inflorescencias unifloras pueden ser terminales como en la magnolia, o axilares como en la camelia, y constan generalmente del pedicelo y algunas brácteas. 

Los órganos constitutivos de las inflorescencias plurifloras son las flores provistas o no del pedicelo, el eje o receptáculo común, el pedúnculo y las brácteas.
El pedicelo es la parte del tallo que sostiene la flor; a veces es muy corto, y otras veces es nulo, en cuyo caso la flor se dice sentada o sésil.
El eje o raquis es la parte alargada del tallo que lleva las ramas floríferas; si es corto y está ensanchado en forma de plato se llama receptáculo común.

El pedúnculo, es la parte del tallo que soporta el raquis o el receptáculo común. El eje que sale de la base arrosetada de la planta o de un órgano subterráneo se llama escapo (por ejemplo, "amaryllis," o "Agapanthus, Taraxacum)."

Las brácteas o hipsófilos son las hojas modificadas, generalmente menores que las hojas normales, coloreadas o verdosas, que nacen sobre el ráquis o acompañan a las flores. Algunas veces faltan, como en el caso de las crucíferas, otras veces reciben nombres especiales, tales como glumas y glumelas en las poáceas y ciperáceas, o espata en las aráceas y palmeras. En otros casos las brácteas forman órganos protectores de las flores (involucros), como la cúpula de "quercus" y el erizo del "castaño".

El prófilo o bracteola es la primera bráctea de una rama axilar, está dispuesta del lado opuesto a la hoja normal. En las monocotiledóneas es bicarenada y por el dorso, cóncavo, se adosa el eje que lleva la rama. En las espiguillas de las poáceas el prófilo recibe el nombre de pálea o "glumela superior". 

Las inflorescencias plurifloras pueden ser simples, si solo constan de un eje o receptáculo común que lleva las ramitas unifloras o compuestas cuando el eje principal lleva ramas plurifloras laterales.

De acuerdo con la forma y desarrollo del eje se distinguen dos tipos diferentes de inflorescencias: las racimosas, cuyo crecimiento es indefinido, y las cimosas, de crecimiento definido.

En ambos casos pueden estar formadas por inflorescencias elementales de igual naturaleza que la inflorescencia total (por ejemplo, racimo de racimos o espiga de espigas) y se las denomina inflorescencias homogéneas. Por el contrario, pueden estar formadas de elementos de distinta naturaleza, sea del mismo tipo (por ejemplo, racimo de espigas) o de distinto tipo (por ejemplo, cima de capítulos). En el primer caso se las denomina inflorescencias heterogéneas y, en el segundo, inflorescencias mixtas.
En los casos en que la inflorescencia pluriflora simula una sola flor, tal como el capítulo de las compuestas (asteraceae), el espádice de las aráceas o el ciatio de "euphorbia," se le denomina pseudanto. 
Las inflorescencias se denominan abiertas, racimosas o racemosas cuando los meristemas apicales de los diversos ejes mantienen su actividad mientras dura el crecimiento de estas. En este tipo de inflorescencias todas las flores son laterales. El eje o raquis de la inflorescencia crece indefinidamente mientras a los costados se producen yemas florales que se abren a medida que aquel se desarrolla. Los botones apicales, o del centro de la inflorescencia, son los últimos en abrirse. La marcha de la floración es centrípeta. 
En este tipo de inflorescencias se distinguen cuatro clases diferentes que ofrecen una notable diversidad de formas: el racimo, la espiga, la umbela y el capítulo.

Las inflorescencias se denominan cerradas o cimosas cuando los meristemas apicales de los diversos ejes se consumen en la producción de flores. Por debajo de la yema terminal convertida en flor, otras yemas laterales producen nuevos ejes. En este tipo de inflorescencias todas las flores son terminales. En las inflorescencias cimosas la flor terminal del eje principal es la primera en abrirse, seguida de las flores terminales de los ejes de segundo orden, tercero, etc. 

Por su aspecto general recuerdan a inflorescencias racimosas pero el desarrollo de la floración es diferente, pues comienza por la flor central y termina en las laterales, siguiendo una marcha centrífuga. 

Es común, además, que la bráctea aparezca del lado contrario a la rama florífera; esto se debe a que cada eje que va naciendo remata en una flor y cesa de crecer, comenzando el crecimiento de otra rama en la axila formada por la hoja y la ramita floral.
El número de ramas floríferas que se desarrolla debajo de la primera flor, o donde se ha interrumpido el crecimiento vegetativo es variable entre una o más. En el caso de ser única, la inflorescencia se llama monocasio (por ejemplo el "Iris"), si son dos dicasio (por ejemplo las cariofiláceas), si son tres o más se denomina pleiocasio (ppor ejemplo "geranium)." Las inflorescencias cimosas comprenden muchas clases, entre las cuales las principales son la cima helicoidal, la cima circinada, la cima dicotómica, la cima umbeliforme, la cima corimbiforme y la cima capituliforme.

Se trata de inflorescencias que no siguen ningún patrón específico para la ramificación del eje floral. Son muy raras, y se presentan solo en algunos taxones.

Las inflorescencias se diferencian del resto del tallo vegetativo por algunas características como: 





</doc>
<doc id="6646" url="https://es.wikipedia.org/wiki?curid=6646" title="Ápice">
Ápice

En botánica o zoología, ápice designa el extremo superior o punta (del latín "apex", con el mismo significado) de la hoja, del fruto, del pólipo, etc. El adjetivo "apical" se puede aplicar a flores, frutos, o cnidarios, con el significado de "el más distal". "Distal", a su vez, es lo que se sitúa hacia el extremo opuesto a la base o parte basal del órgano en cuestión, como la zona oral de los antozoos. <br>

En un órgano, por ejemplo una hoja, hay que distinguir entre el ápice orgánico, por donde puede crecer distalmente el órgano, dotado de tejido meristemático, y el ápice geométrico, que es simplemente el punto más distanciado de la base.


</doc>
<doc id="6649" url="https://es.wikipedia.org/wiki?curid=6649" title="Baklava">
Baklava

El baklava, baklawa o baclava (del turco "baklava"), es un pastel elaborado con una pasta de nueces trituradas, distribuida en la masa filo y bañado en almíbar o jarabe de miel, existiendo variedades que incorporan avellanas y almendras, entre otros frutos secos y kaymak, procedente de la cocina turca. Puede encontrarse con diferentes nombres, en la gastronomía de Oriente Medio, del Subcontinente Indio y de los Balcanes.

La historia del baklava se remonta a la antigua Mesopotamia. Se cree, sin embargo, que los asirios, cerca del siglo VII a. C., fueron los primeros en colocar unas pocas capas de masa de pan junto a nueces trituradas entre esas capas, añadiendo un poco de miel y horneándolo en sus hornos de madera primitivos. Esta temprana versión del baklava se cocinaba sólo para ocasiones especiales, siendo de hecho considerado históricamente una comida para las clases acomodadas hasta mediados del siglo XIX.

Los marinos y mercaderes griegos que viajaban hacia Mesopotamia, pronto descubrieron las delicias del baklava y llevaron la receta a Atenas. 

El mayor aporte de los griegos a la elaboración de la masa, es la creación de una técnica de pastelería que hace posible amasarla hasta dejarla fina como una hoja, comparada a la áspera textura, similar a la del pan, de la elaboración asiria. Las capas de masa que lo forman son tradicionalmente 33 en referencia a los años de vida de Cristo en la tierra. En el siglo XV, los otomanos conquistaron Constantinopla, expandiendo sus territorios a gran parte de los antiguos territorios asirios y al reino Armenio por completo. Por cuatrocientos años, desde el siglo XVI hasta la declinación del Imperio otomano en el siglo XIX, las cocinas del Palacio Imperial Otomano en Constantinopla, se convirtieron en el cenit culinario del Imperio. Vryonis identifica a las antiguos "gastris", "kopte", "kopton", o "koptoplakous", mencionados en los "Deipnosofistas", como baklava y lo llama un "favorito Bizantino". Sin embargo, Perry muestra que aunque los "gastris" contenían un relleno de nueces y miel, no incluían masa, sino una mezcla de miel y sésamo similar al "pasteli" o "halva" moderno.

Perry muestra evidencias para demostrar que las capas de masa fueron creadas por los turcos en Asia central y argumenta que el "enlace perdido" entre las capas de masa (que no incluye nueces) y las moderna masa hojaldrada (o masa phyllo) con la pasta de nueces que constituyen el moderno baklava, es el alimento azerbaiyano "Baki pakhlavası". El desarrollo posterior habría ocurrido en las cocinas del Palacio de Topkapi, donde los jenízaros tenían una celebración anual llamada "Baklava Alayı".

Buell argumenta que la palabra "baklava" es de origen mongol, y menciona una receta en un libro de cocina chino escrito en 1330, bajo la dinastía Yuan.
Varias láminas de masa filo se barnizan con mantequilla derretida y se esparce la nuez picada sobre ellas. Sobre esta se colocan otras láminas de masa filo, repitiendo el procedimiento y una vez conseguida la altura necesaria, el pastel se corta en triángulos de igual tamaño. La masa filo se humedece con agua, para luego hornear a 180 °C por 40 minutos hasta conseguir el dorado. Se retira del horno y se deja enfriar por 15 minutos. Luego se cubre con almíbar preparado con agua, canela, azúcar y jugo de limón. Se deja reposar durante una hora, hasta que se impregne. El baklava suele servirse acompañado de té o café.

"Antep baklavası" o sea "Baklava de (la ciudad de Gazi) Antep" ha sido el primer producto alimenticio de Turquía en ser reconocido y registrado como una denominación de origen por la Comisión europea.




</doc>
<doc id="6654" url="https://es.wikipedia.org/wiki?curid=6654" title="Alfonso VI de León">
Alfonso VI de León

Alfonso VI de León, llamado «el Bravo» (1040/1041-Toledo, 1 de julio de 1109), hijo de Fernando I de León y de su esposa, la reina Sancha, fue entre 1065 y 1072 en un primer reinado, y entre 1072 y 1109 en un segundo, entre 1071 y 1072 y entre 1072 y 1109 y entre 1072 y 1109.

Durante su reinado, se produjo la conquista de Toledo (1085) y tuvieron lugar las batallas de Sagrajas y Uclés, que constituyeron sendas derrotas para las mesnadas leonesas y castellanas. En la segunda falleció el heredero del rey, el infante Sancho Alfónsez.

Hijo del rey Fernando I y de su esposa, la reina Sancha de León, Alfonso era un «infante leonés con sangre navarra y castellana». Sus abuelos paternos fueron Sancho Garcés III, , y su esposa la reina Muniadona —hija de Sancho García, conde de Castilla— y los maternos fueron el rey Alfonso V de León y su esposa la reina Elvira Menéndez. 

El año de su nacimiento no está registrado en la documentación medieval. Un texto coetáneo del cronista anónimo de Sahagún que conoció al monarca y se halló presente cuando murió, relata que falleció con 62 años de vida y 44 de reinado, por lo tanto, habría nacido en el segundo semestre de 1047 o en la primera mitad de 1048. Reilly dice que falleció con 72, por lo que habría nacido en 1037. 

Según el Silense, la primogénita, Urraca, vino al mundo cuando sus padres aún eran condes de Castilla, antes de reinar, así que habrá nacido en 1036/37. El segundogénito, Sancho, habrá nacido en el segundo semestre de 1038 o en 1039. La infanta Elvira pudo haber nacido en 1039/40, Alfonso en 1040/41, y el más pequeño de los hermanos, García, entre 1041 y el 24 de abril de 1043 cuando el rey Fernando, en una donación a la abadía de San Andrés de Espinareda, menciona a sus cinco hijos. Todos ellos, excepto Elvira, confirman un documento en el monasterio de San Juan Bautista de Corias el 26 de abril de 1046.

Todos los hijos del rey Fernando, según el Silense, fueron educados en las artes liberales y los varones también en las armas, el «arte de correr caballos al uso español» y en la caza. El clérigo Raimundo fue el encargado del aprendizaje de Alfonso en las letras. Ya siendo rey, Alfonso le nombró y se refirió a él como "magistro nostro, viro nobile et Deum timenti". Posiblemente Alfonso pasó largas temporadas en Tierra de Campos donde aprendió el arte de la guerra y lo que se esperaba de un caballero junto con Pedro Ansúrez, hijo de Ansur Díaz y sobrino del conde Gómez Díaz de Saldaña, todos del linaje de los Banu Gómez.

Como segundo hijo varón del rey de León y conde de Castilla, Fernando I, y de la reina Sancha de León, a Alfonso no le habría correspondido heredar. A finales de 1063, probablemente el 22 de diciembre, aprovechando que numerosos magnates se habían reunido en la capital del reino para la consagración de la basílica de San Isidoro de León, Fernando I convocó una "Curia Regia" para dar a conocer sus disposiciones testamentarias, en las cuales decidió repartir su patrimonio entre sus hijos, reparto que no se haría efectivo hasta la muerte del monarca con el fin de evitar que surgieran discordias después de su muerte.


El historiador Alfonso Sánchez Candeira sugiere que, aunque no se conocen las razones que llevaron al rey Fernando a dividir los reinos, heredando Alfonso el de León que llevaba implícito el título imperial, el reparto pudo ser debido a que consideró conveniente que a cada hijo varón le entregara en herencia la región donde fueron educados y donde pasaron sus primeros años.

Tras su coronación en la ciudad de León en enero de 1066, Alfonso tuvo que enfrentarse con los deseos expansionistas de su hermano Sancho quien, como primogénito, se consideraba el único heredero legítimo de todos los reinos de su padre. Los conflictos se inician cuando el 7 de noviembre de 1067 fallece la reina Sancha, suceso que abrirá un periodo de siete años de guerra entre los tres hermanos y cuyo primer acto tendrá lugar el 19 de julio de 1068 cuando Alfonso y Sancho se enfrentan en Llantada, en un juicio de Dios en el que ambos hermanos pactan que el que resultase victorioso obtendría el reino del derrotado. Aunque Sancho vence, Alfonso no cumple con lo acordado a pesar de lo cual las relaciones entre ambos se mantienen como demuestra el hecho de que Alfonso acudiera, el 26 de mayo de 1069, a la boda de Sancho con una noble inglesa llamada Alberta y donde ambos decidieron unirse para repartirse el reino de Galicia que le había correspondido a García, el menor de los hijos de Fernando I.

Con la complicidad de Alfonso, su hermano Sancho entra en Galicia en 1071 y, tras derrotar a su hermano García, lo apresa en Santarém y lo encarcela en Burgos hasta que es exiliado a la taifa de Sevilla, gobernada por Al-Mutámid. Tras eliminar a su hermano, Alfonso y Sancho se titulan reyes de Galicia y firman una tregua.

La tregua se rompe con la batalla de Golpejera en 1072. Las tropas de Sancho salen victoriosas, pero este decide no perseguir a su hermano. Alfonso fue hecho prisionero y encarcelado en Burgos. Posteriormente es trasladado al monasterio de Sahagún, donde se le rasura la cabeza y se le obliga a tomar la casulla. Gracias a la intercesión de su hermana Urraca, Sancho y Alfonso llegaron a un acuerdo para que Alfonso marchara y se refugiase en la taifa de Toledo bajo la protección de su vasallo, el rey Al-Mamún y acompañado por el fiel Pedro Ansúrez, amigo de su infancia, y sus dos hermanos Gonzalo y Fernando.

Alfonso, desde su exilio en Toledo, logra el apoyo tanto de su hermana Urraca como de la nobleza leonesa que se hacen fuertes en la ciudad de Zamora, señorío que Alfonso le había otorgado anteriormente, obligando a Sancho, en 1072, a sitiar la ciudad para someterla después de que Urraca se negara a canjearla por otras plazas que le había ofrecido Sancho, deseoso de controlar la plaza fuerte de Zamora, «clave para la futura expansión al sur del Duero». En el transcurso del asedio el rey Sancho recibió la muerte. La tradición o leyenda narra el episodio con el detalle de que durante el cerco, un noble zamorano o gallego llamado Vellido Dolfos se presentó ante el rey como desertor y, con la excusa de mostrarle los puntos débiles de las murallas, lo separó de su guardia y consiguió acabar con su vida de una lanzada. Aunque no hay constancia alguna de que la muerte de Sancho se debiera a una traición más que a un engaño, ya que Dolfos era enemigo de Sancho, su asesinato fue debido a un lance bélico propio de la situación de sitio y no se produjo en las murallas sino en un bosque cercano donde Dolfos llevó al rey castellano alejándole de su protección armada. La muerte violenta de su hermano Sancho, que no dejó descendencia, permitió a Alfonso recuperar su trono y reclamar para sí Castilla y Galicia.

Aunque Rodrigo Díaz de Vivar, hombre de confianza y portaestandarte del rey Sancho, se halló en el sitio de Zamora, no consta cual había sido su actuación. Tampoco se puede atribuir a Alfonso que estaba desterrado y alejado de los hechos, la muerte de su hermano, «pero los juglares y el romancero rellenaron este vacío con hermosas creaciones literarias desprovistas de cualquier realidad histórica». 

En este momento, la "Leyenda de Cardeña" acerca del Cid () sitúa la jura exculpatoria de la posible participación de Alfonso en el asesinato de su hermano, que tomó El Cid en la iglesia de Santa Gadea de Burgos (Jura de Santa Gadea) y que provocaría una relación de desconfianza mutua entre ambos, aunque Alfonso intentó un acercamiento al ofrecerle en matrimonio a su sobrina Jimena Díaz junto a la inmunidad de sus heredades. Estos hechos y sus consecuencias llegarían con el tiempo a ser considerados históricos por multitud de cronistas e historiadores, aunque en la actualidad la mayor parte de estos rechazan la historicidad del episodio. 

La muerte de Sancho también fue aprovechada por García para recuperar su propio trono, pero al año siguiente, el 13 de febrero de 1073, fue llamado por Alfonso a una reunión, y fue apresado y encarcelado de por vida en el castillo de Luna, donde fallecería finalmente el 22 de marzo de 1090. Eliminados los dos hermanos, Alfonso no tuvo problema en obtener la lealtad tanto del alto clero como de la nobleza de sus territorios; para confirmar esta, pasó los dos años siguientes visitándolos.

Consolidado en el trono leonés, y con el título de emperador que heredaba de la tradición neogoticista leonesa, Alfonso VI dedica los siguientes catorce años de su reinado a engrandecer sus territorios mediante conquistas como la de Uclés y los territorios de los Banu Di-l-Nun. También se tituló, desde 1072, "rex Spanie".

Su primer movimiento lo realiza en 1076, cuando al fallecer el monarca navarro Sancho Garcés IV, la nobleza navarra decide que el trono no pase a su hijo menor de edad, sino a uno de los nietos de Sancho III de Navarra: Alfonso VI o Sancho Ramírez de Aragón que invadieron el reino navarro. Tras llegar a un acuerdo, Sancho Ramírez es reconocido como rey de Navarra y Alfonso se anexiona los territorios de Álava, Vizcaya, parte de Guipúzcoa y La Bureba, adoptando en 1077 el título de "Imperator totius Hispaniae" ('Emperador de toda España'). 

Pero su gran expansión territorial la hará a costa de los reinos taifas musulmanes, para lo cual Alfonso siguió con la práctica de explotación económica mediante el sistema de parias consiguiendo que la mayor parte de los reinos de taifas de la España musulmana fuesen sus tributarios, práctica a la que unió la presión militar. En el 1074 probablemente recuperó el pago de las parias de Toledo y ese mismo año, ayudado por tropas de esta ciudad, taló las tierras de la taifa granadina, que como consecuencia comenzó también a pagar tributo a Alfonso. En el 1076, el emir de Zaragoza, que deseaba apoderarse de Valencia sin que lo estorbase Alfonso, se avino a reanudar el pago de las parias. En el 1079, se adueñó de Coria.

Una de las iniciativas de estos años, que ha pasado a la historia como la traición de Rueda, terminará en fracaso. Tuvo lugar en 1083 en el castillo de Rueda de Jalón, cuando Alfonso recibe noticias de que el alcaide de dicha fortaleza, la cual pertenecía al reino Taifa de Zaragoza, pretende rendirla al rey leonés. Las tropas que envía Alfonso son emboscadas al entrar en el castillo y mueren varios de sus principales magnates.

En 1074 había fallecido envenenado en Córdoba su vasallo y amigo, el rey de la taifa de Toledo Al-Mamún a quien sucedió su nieto Al-Qádir quien, en 1084, solicitó por segunda vez la ayuda de Alfonso ante un levantamiento que pretendía derrocarle. Alfonso aprovechó el llamamiento de ayuda del rey taifa para sitiar Toledo, ciudad que caería el 25 de mayo de 1085 y al-Qádir fue enviado como rey a Valencia bajo la protección de Álvar Fáñez. Para facilitar esta operación y recuperar el pago de las parias de la ciudad, que había dejado de pagarlas el año anterior, Alfonso asedió Zaragoza en la primavera del 1086. A comienzos de marzo, Valencia aceptó a al-Qádir; Játiva trató de resistir solicitando el socorro del reyezuelo de Tortosa y Lérida, que realizó una fallida incursión por la región antes de retirarse acosado por las huestes de Fáñez. 

Tras esta importante conquista, el monarca se tituló emperador de las dos religiones y como gesto ante la importante población musulmana de la ciudad se compromete, además de respetar las propiedades de estos, a reservarles la mezquita mayor para su culto. Esta decisión será revocada por el recién nombrado arzobispo de Toledo, el cluniacense Bernardo de Sedirac, aprovechando una ausencia del monarca de Toledo y valiéndose para ello del apoyo de la reina Constanza de Borgoña.

La ocupación de Toledo, que permite a Alfonso VI incorporar el título de rey de Toledo a los que ya ostentaba ("victoriosissimo rege in Toleto, et in Hispania et Gallecia"), llevó a la toma de ciudades como Talavera y de fortalezas como el castillo de Aledo. También ocupa la entonces ciudad de "Maŷriṭ" en 1085 sin resistencia, probablemente mediante capitulación. La incorporación del territorio situado entre el Sistema Central y el río Tajo, servirá de base de operaciones para la corona leonesa, desde donde podía emprender un mayor hostigamiento contra las taifas de Córdoba, Sevilla, Badajoz y Granada.

La conquista de la extensa y estratégica taifa toledana, el control de Valencia y la posesión de Aledo, que aisló Murcia del resto de al-Ándalus, preocuparon a los soberanos musulmanes de la península. La presión militar y económica sobre los reinos taifas hizo que los reyes de las taifas de Sevilla, Granada, Badajoz y Almería decidiesen pedir ayuda a los almorávides que, a finales de julio del 1086, al mando del emir Yusuf ibn Tasufin, cruzaron el estrecho de Gibraltar y desembarcaron en Algeciras.

En Sevilla, el ejército almorávide se unió a las tropas de los reinos taifas y juntos se dirigieron a tierras extremeñas donde, el 23 de octubre de 1086, se enfrentaron en la batalla de Zalaca a las tropas de Alfonso VI, que se había visto obligado a abandonar el sitio a que sometía a la ciudad de Zaragoza. A él se reunió también Álvar Fáñez, a quien se había llamado desde Valencia para unirse a las fuerzas del rey. La batalla se saldó con la derrota de las tropas cristianas, que regresaron a Toledo para defenderse, pero el emir no supo aprovechar la victoria, pues regresó apresuradamente a África a causa de la muerte de su hijo. El choque marcó el comienzo de una nueva etapa en la península que duró unas tres décadas, en las que la iniciativa militar pasó a los almorávides y el reino de Alfonso tuvo que mantenerse a la defensiva; logró en todo caso retener Toledo, objetivo principal de las acometidas almorávides.

Alfonso solicitó a los reinos cristianos de Europa la organización de una cruzada contra los almorávides que habían recuperado casi todos los territorios que Alfonso había conquistado, con la excepción de Toledo, ciudad en la que Alfonso se hacía fuerte. Para reforzar su posición, se reconcilió con el Cid, que acudió a Toledo a finales del 1086 o principios del 1087. Como consecuencia de la grave derrota, las taifas andalusíes dejaron de pagarle parias. El Cid, empero, logró volver a someter a las taifas levantinas a Alfonso a lo largo de los dos años siguientes.

Aunque la cruzada no llegó finalmente a organizarse, sí provocó la entrada en la península de un importante número de cruzados entre los que destacaban Raimundo de Borgoña y Enrique de Borgoña, que contrajeron matrimonio con dos hijas de Alfonso, Urraca (1090) y Teresa (1094), lo que originó la implantación de la dinastía borgoñona en los reinos peninsulares. Algunos de los cruzados sitiaron infructuosamente Tudela en el invierno del 1087, antes de retirarse. Ese mismo año, el rey aplastó una revuelta en Galicia, que pretendía liberar a su hermano García.

En 1088 Yusuf ibn Tasufin cruzó por segunda vez el estrecho, pero fue derrotado en el sitio de Aledo y sufrió la deserción de muchos de los reyes de las taifas musulmanas, lo que motivó que, en su próxima venida, el emir llegase con la decisión de destituirlos a todos y quedarse él como único rey de todo al-Ándalus. Gracias al fracaso musulmán ante Aledo, Alfonso había podido reanudar el cobro de las parias, mediante amenazas de talar el territorio granadino en el caso del soberano de esta ciudad y corriendo el territorio sevillano para recuperar la sumisión de la ciudad del Guadalquivir. Enemistado definitivamente Abd Allah ibn Buluggin de Granada con Ibn Tasufin, Alfonso se comprometió a socorrerlo a cambio de su sumisión.

En junio del 1090, los almorávides realizaron un tercer desembarco, destituyeron al rey de Granada, vencieron a al-Mamun, gobernador de Córdoba, y tras la batalla de Almodóvar del Río, entraron en Sevilla enviando al exilio a su rey al-Mutámid. En la segunda mitad del año, se apoderaron de todas las taifas sureñas, Alfonso, que se había comprometido a ayudar al soberano de Sevilla, fracasó en este propósito. El rey sufrió reveses en todos los frentes: en el este no consiguió apoderarse de Tortosa por la tardía llegada de la flota genovesa que debía participar en su toma; más al sur, al-Qádir fue depuesto en una revuelta; en el sur, su relación con Zaida, nuera del emir sevillano, no sirvió para favorecer su imagen de paladín del islam peninsular frente a los almorávides; finalmente, en el oeste, la alianza con el emir de Badajoz no bastó para librar a este de la conquista de su territorio por los magrebíes. Como precio del pacto Alfonso había obtenido Lisboa, Sintra y Santarém, pero las perdió en noviembre del 1094, cuando su yerno Raimundo, encargado de su defensa, fue derrotado por el ejército almorávide que había tomado Badajoz poco antes. La única buena nueva para Alfonso la proporcionó el Cid, que consiguió recuperar Valencia en junio y vencer al ejército almorávide que avanzó contra él en octubre en la batalla de Cuarte; esta victoria fijó la frontera oriental durante aproximadamente una década.

Seguidamente, desbarató con astucia una conjura en su contra de sus yernos Raimundo y Enrique, que deseaban repartirse el reino a su muerte. Para enemistarlos, Alfonso casó a su hija Teresa con Enrique en el 1096, y concedió al matrimonio el gobierno del condado de Portugal, hasta entonces dominado por Raimundo, que comprendía las tierras desde el Miño hasta Santarém, mientras que el gobierno de Raimundo se limitaba a Galicia.

En 1097 se produjo un cuarto desembarco almorávide. La noticia la recibió Alfonso VI cuando se dirigía a Zaragoza para prestar ayuda a su vasallo el rey Al-Musta'in II en su enfrentamiento con el recién coronado Pedro I de Aragón. El objetivo almorávide era nuevamente Toledo, en cuyo camino se encontraba el castillo de Consuegra y donde, el 15 de agosto, se encontraron con las tropas cristianas que nuevamente resultaron derrotadas en la batalla de Consuegra, lo que supuso la confirmación del periodo de decadencia del reinado de Alfonso VI que ya se había iniciado en 1086 con la derrota de Zalaca.

En el 1099, los almorávides conquistaron gran parte de los castillos que defendían la zona toledana y al año siguiente trataron de apoderarse de Toledo, infructuosamente. Dirigió la defensa de la ciudad Enrique, el yerno de Alfonso, pues este había marchado a Valencia a inspeccionar sus defensas; el Cid había fallecido el año anterior y el gobierno de la ciudad recaía entonces en su viuda, Jimena.

En 1102, Alfonso envió tropas en auxilio de Valencia frente a la amenaza almorávide. La batalla tuvo lugar en Cullera y terminó sin un claro vencedor, aunque Valencia cayó en manos almorávides ante lo costoso que resultaba para Alfonso defender esta plaza. Alfonso supervisó la evacuación de la ciudad en marzo y abril, y le prendió fuego antes de marcharse; en mayo, los almorávides se adueñaron de ella. Ese mismo año, emprendió la repoblación de Salamanca —que protegía Coria— y Ávila —que defendía el puerto de mejor acceso desde Guadarrama—, tratando de prepararse para una eventual pérdida de Toledo. Para proteger la zona por el este, cercó y tomó Medinaceli, plaza clave que permitía el ataque hacia la región toledana desde el este a lo largo del valle del Jalón, en el 1104. En el 1104, 1105 y 1106, realizó varias incursiones en el territorio andalusí; en la última alcanzó Málaga y pudo escoltar en su vuelta a mozárabes que se instalaron en su reino como repobladores.

En 1108 las tropas del almorávide Tamim, gobernador de Córdoba e hijo de Yusuf ibn Tasufin, se dirigieron nuevamente contra los territorios cristianos, pero la ciudad elegida no fue Toledo, sino Uclés. Alfonso se encontraba en Sahagún, recién casado, mayor y con una vieja herida que le impedía montar a caballo. Al mando del ejército se puso Álvar Fáñez, gobernador de las tierras de los Banu Di-l-Nun, y le acompañó el infante heredero Sancho Alfónsez. Los ejércitos se enfrentaron en la batalla de Uclés, donde las tropas cristianas sufrieron otra dura derrota y en la que, además, pereció el infante heredero al trono, lo que tuvo como consecuencia un parón de treinta años en la reconquista y la independencia del condado portugués. La situación militar también era grave, pues los almorávides se apoderaron casi de inmediato de toda la franja defensiva del Tajo de Aranjuez a Zorita y se produjeron levantamientos de la población musulmana de la región.

En 1067 se negoció su matrimonio con Ágata de Normandía, hija del rey Guillermo I de Inglaterra y de Matilde de Flandes, pero su muerte prematura frustró el proyecto. 

Según el obispo Pelayo de Oviedo, coetáneo del rey, en su "Chronicon Regum Legionensium", Alfonso VI tuvo cinco esposas y dos concubinas "nobilissimas". Las esposas fueron, según el obispo, Inés, Constanza, Berta, Isabel y Beatriz, y las concubinas Jimena Muñoz y Zaida. 

En 1069 se firmó el acuerdo de esponsales con Inés de Aquitania, hija de Guido Guillermo VIII, duque de Aquitania y y de Matilde de la Marche. Inés apenas contaba con diez años de edad y hubo que esperar hasta que cumpliese los catorce años para celebrar el matrimonio que tuvo lugar a finales de 1073 o principios de 1074. Aparece en diplomas reales hasta el 22 de mayo de 1077 y a partir de esa fecha, el rey aparece solo en la documentación. Inés falleció el 6 de junio de 1078. 

Reilly sugiere que el año anterior se había anulado el matrimonio, probablemente por la falta de hijos. Sin embargo, Gambra discrepa y opina que no existen fuentes fidedignas que avalen tal aseveración. La información sobre el supuesto repudio solo aparece en un tomo de "L'art de vérifier les dates" y, según Gambra, «Se hace imposible, a falta de mejores referencias, conceder crédito a la afirmación del repudio de Inés». Además, señala que el Tudense, en su "Chronicon Mundi", indica que la reina fue sepultada en Sahagún. Finalmente, señala que «Si realmente se hubiese producido un hecho de dicha envergadura, carecería de sentido [...] que Alfonso VI contrajese matrimonio inmediatamente con otra princesa de la familia de Inés». Inés y la siguiente esposa del rey, Constanza, eran primas en tercer grado, ambas descendientes del duque Guillermo III de Aquitania.

Por otro lado, Orderico Vital, cronista inglés del , decía que el matrimonio de Inés y el rey Alfonso había sido anulado en 1080 por razones de consanguinidad y que Inés había vuelto a casar en 1109 con Elías de la Fleche, conde de Maine. Según Jaime de Salazar y Acha, la que casó con el conde de Maine fue Beatriz, la última esposa de Alfonso VI.

Después de la muerte de Inés, el rey mantuvo una relación con Jimena Muñoz, concubina "nobilissima", según el obispo Pelayo de Oviedo de la cual nacieron dos hijas entre 1078 y 1080.


Contrajo matrimonio por segunda vez a finales de 1079 con Constanza de Borgoña, con quien aparece por primera vez el 8 de mayo de 1080, viuda, sin hijos, del conde Hugo III de Chalon-sur-Saône, e hija de Roberto el Viejo, duque de Borgoña y Hélie de Semur, y bisnieta de Hugo Capeto, rey de Francia. También era sobrina del abad Hugo de Cluny, y tía de Enrique de Borgoña. Fruto de este matrimonio, que duró hasta la muerte de la reina en 1093, nacieron seis hijos, cinco de ellos fallecidos en la niñez, y la única que sobrevivió fue:


El obispo Pelayo de Oviedo menciona a Zaida como una de las dos concubinas del rey y dice que fue hija de Al-Mu'tamid rey taifa de Sevilla. Zaida, en realidad, era su nuera, casada con su hijo Abu Nasr Al-Fath al-Ma'mun, rey de la taifa de Córdoba. En marzo de 1091 los almorávides sitiaron la ciudad de Córdoba. El marido de Zaida, que murió durante el asedio el 26/27 de ese mes, como medida de precaución, envió a su esposa Zaida y sus hijos a Almodóvar del Río. Después de enviudar, Zaida buscó la protección en la corte del rey leonés y ella y sus hijos se convirtieron al cristianismo, fue bautizada con el nombre de Isabel y se convirtió en la concubina del rey.

De esta relación nació entre 1091 y 1095, posiblemente en 1094: 

En la crónica "De rebus Hispaniae", del arzobispo de Toledo Rodrigo Jiménez de Rada, la mora Zaida se cuenta entre las esposas de Alfonso VI. Pero la "Crónica najerense" y el "Chronicon mundi" indican que Zaida fue concubina y no esposa de Alfonso VI.. 

Según Jaime de Salazar y Acha, seguido por otros autores, entre ellos, Gonzalo Martínez Díez, contrajeron matrimonio en 1100, quedando legitimado el hijo de ambos que se convirtió en príncipe heredero del reino cristiano. Para Salazar y Acha, Zaida y la cuarta esposa del rey, Isabel, son la misma persona, «Pese a los ímprobos esfuerzos de los historiadores posteriores por intentarnos demostrar que no era la mora Zaida», y también sería la madre Elvira y Sancha Alfónsez. Otra razón que esgrime el autor es el hecho que poco después de la boda del rey con Isabel, el infante Sancho comienza a confirmar diplomas regios y de no ser la nueva reina Zaida, no hubiera consentido el nuevo protagonismo de Sancho en detrimento de sus posibles futuros hijos. También cita un diploma en la catedral de Astorga del 14 de abril de 1107 donde el rey concede unos fueros y actúa "cum uxore mea Elisabet et filio nostro Sancio". Este es el único diploma donde se cita como «nuestro hijo», ya que en otros solamente figura como hijo del rey aunque también aparece la reina Isabel. 

Reilly acepta que fueron dos Isabel, la mora Zaida —bautizada Isabel— y la otra Isabel, pero argumenta que para reforzar la posición de Sancho Alfónsez, el rey Alfonso anuló el matrimonio con Isabel en marzo de 1106 y se casó con Zaida. La hipótesis que Alfonso VI se había casado con Zaida ya había sido rechazada por Menéndez Pidal y por Lévi-Provençal.

El 27 de marzo de 1106, el rey Alfonso confirmó una donación al monasterio de Lorenzana: "(...) eiusdemque Helisabeth regina sub maritali copula legaliter aderente", una fórmula inusual que confirma un legítimo matrimonio. Salazar y Acha y Reilly interpretan esta cita como prueba de que el rey había casado con Zaida, legitimando así al hijo de ambos y la relación de concubinato. Gambra, sin embargo, se opone y dice que es «una argumentación extremadamente endeble, empezando por la referencia documental, escasamente significativa. Su carácter es más bien ornamental y literario». Montaner Frutos también dice que la hipótesis es «poco verosímil y problemática» ya que no era necesario que el rey casase con Zaida para legitimar a su hijo Alfonso y que, además, Isabel la francesa falleció en 1107 según reza en su epitafio. También menciona Montaner Frutos una donación de la reina Urraca años después, en 1115, cuando donó unas propiedades a la catedral de Toledo y solamente menciona a una Isabel como la esposa del rey.

El 25 de noviembre de 1093 contrajo un tercer matrimonio con Berta, aunque en un documento del 13 de abril de 1094 no se cita lo cual «resulta extraño porque se inscribe en una época en la que ya es habitual la inclusión de la regia consorte en el tenor diplomático». El genealogista Szabolcs de Vajay, por razones onomásticas, sugiere que Berta era miembro de la casa de Saboya, hija de Amadeo II de Saboya (m. 1180), hermano de Pedro I de Saboya, sobrina nieta de Berta de Saboya, bisnieta de Berta d'Este y prima hermana de otra Berta, quien fue reina por su matrimonio con Pedro I de Aragón. Su presencia en la corte se registra por primera vez el 28 de abril de 1095. Falleció entre el 17 de noviembre de 1099, fecha en que confirma un diploma real por última vez, y el 15 de enero de 1100 cuando el rey aparece solo en una donación a la catedral de Santiago de Compostela. No hubo descendencia de este matrimonio.

Su penúltimo matrimonio fue a principios de 1100 con Isabel y «la polémica ha radicado durante siglos en si esta última era la mora Zaida o un personaje distinto». Ambos aparecen juntos por primera vez el 14 de mayo de 1100 aunque el diploma es considerado sospechoso, y la segunda vez en ese mismo año pero sin fecha. Las últimas menciones de Isabel en diplomas reales fueron el 8 y el 14 de mayo de 1107 y probablemente murió a mediados de ese año. Esta es, según Salazar y Acha, Zaida que después de su bautismo se llamó Isabel. Su origen es incierto. El obispo Pelayo no se refiere a su origen. Lucas de Tuy en el siglo XIII, basándose en el epitafio de Isabel, la hace hija del rey Luis de Francia, quien por esas fechas tendría que ser Luis VI aunque no consta que tuviera una hija llamada Isabel y, además, de ser así, Isabel hubiera tenido unos cinco o seis años de edad al casar. Reilly considera que su origen probablemente fue borgoñón, aunque no consta en la documentación.

Nacieron dos hijas de este matrimonio: 

El rey Alfonso contrajo un quinto matrimonio, posiblemente en los primeros meses de 1108, con Beatriz. Ambos aparecen juntos por primera vez el 28 de mayo de 1108 en la catedral de Astorga y después en otros dos diplomas reales; el 1 de enero de 1109 en la catedral de León y por última vez el 25 de abril del mismo año en la catedral de Oviedo, unos tres meses antes de la muerte del rey. 
Según el obispo Pelayo de Oviedo, una vez viuda del rey, Beatriz regresó a su patria. Jaime de Salazar y Acha sugiere que fue hija de Guillermo de Poitiers, duque de Aquitania y , y de Hildegarda de Borgoña y que después de enviudar volvió a contraer matrimonio con Elías de la Fleche, conde de Maine. No hubo descendencia de este matrimonio.

Alfonso, ya anciano, tuvo que ocuparse del problema sucesorio. Berta había muerto sin darle un heredero a finales de 1099; poco después Alfonso se casó con una francesa que le dio dos hijas, pero ningún varón. Para complicar aún más la situación, en marzo del 1105 nació Alfonso Raimúndez, hijo de Urraca y Raimundo y nieto, por tanto de Alfonso. A este posible pretendiente a la corona se oponía el hijo del rey con Zaida, Sancho. Montenegro, opina que el rey legitimó a Sancho probablemente coincidiendo con la reunión de un concilio en Carrión de los Condes en enero de 1103 debido a que desde esa fecha, Sancho comienza a encabezar la lista de los confirmantes de diplomas reales, antes que sus cuñados Enrique y Raimundo de Borgoña. En mayo del 1107 Alfonso impuso el reconocimiento de Sancho como heredero, a pesar del probable disgusto de sus hijas y yernos, en el transcurso de una curia regia celebrada en León. La situación mejoró para el rey con la muerte de Raimundo en septiembre y el acuerdo con Urraca para que esta quedase como señora vitalicia de Galicia, salvo en caso de que se casase, ya que entonces pasaría a su hijo. 

Para asegurar la sucesión, Alfonso eligió entonces a Urraca, pero decidió casarla con su rival y famoso guerrero Alfonso I de Aragón en el otoño del 1108. Aunque el matrimonio se celebró a finales del año siguiente, no condujo a la esperada estabilidad, sino a una larga guerra civil que se prolongó ocho años.

Alfonso VI falleció en la ciudad de Toledo el día 1 de julio de 1109 El rey había acudido a la ciudad a tratar de defenderla de un inminente asalto almorávide. Su cadáver fue conducido a la localidad leonesa de Sahagún, siendo sepultado en el Monasterio de San Benito de Sahagún, cumpliéndose así la voluntad del monarca. Los restos mortales del rey fueron depositados en un sepulcro de piedra, que fue colocado a los pies de la iglesia del monasterio de San Benito, hasta que, durante el reinado de Sancho IV, pareciéndole indecoroso a este rey que su predecesor estuviese sepultado a los pies del templo, ordenó trasladar el sepulcro al interior del templo, y colocarlo en el crucero de la iglesia, donde se hallaba el sepulcro que contenía los restos de Beatriz Fadrique, hija del infante Fadrique de Castilla, quien había sido ejecutado por orden de su hermano, Alfonso X el Sabio, en 1277.
El sepulcro que contuvo los restos del rey, desaparecido en la actualidad, se sustentaba sobre leones de alabastro, y era un arca grande de mármol blanco, de ocho pies de largo y cuatro de ancho y alto, siendo la tapa que lo cubría lisa y de pizarra negra, y estando cubierto el sepulcro de ordinario por un tapiz de seda, tejido en Flandes, en el que aparecía el rey coronado y armado, hallándose en los lados la representación de las armas de Castilla y León, y en la parte de la cabecera del sepulcro un crucifijo.

El sepulcro que contenía los restos de Alfonso VI fue destruido en 1810, durante el incendio que sufrió el Monasterio de San Benito. Los restos mortales del rey y los de varias de sus esposas, fueron recogidos y conservados en la cámara abacial hasta el año 1821, en que fueron expulsados los religiosos del monasterio, siendo entonces depositados por el abad Ramón Alegrías en una caja, que fue colocada en el muro meridional de la capilla del Crucifijo, hasta que, en enero de 1835, los restos fueron recogidos de nuevo e introducidos en otra caja, siendo llevados al archivo, donde se hallaban en esos momentos los despojos de las esposas del soberano. El propósito era colocar todos los restos reales en un nuevo santuario que se estaba construyendo entonces. No obstante, cuando el monasterio de San Benito fue desamortizado en 1835, los religiosos entregaron las dos cajas con los restos reales a un pariente de un religioso, que las ocultó, hasta que en el año 1902 fueron halladas por el catedrático del Instituto de Zamora Rodrigo Fernández Núñez.

En la actualidad, los restos mortales de Alfonso VI el Bravo reposan en el Monasterio de las monjas benedictinas de Sahagún, a los pies del templo, en un arca de piedra lisa y con cubierta de mármol moderna, y en un sepulcro cercano, igualmente liso, yacen los restos de varias de las esposas del rey.

En el terreno cultural, Alfonso VI fomentó la seguridad del Camino de Santiago e impulsó la introducción de la reforma cluniacense en los monasterios de Galicia, León y Castilla. En la primavera del 1073, realizó la primera concesión de un monasterio leonés a los cluniacenses. 

El monarca sustituyó la liturgia mozárabe o toledana por la romana. A este respecto cuenta la tradición popular que Alfonso tomó un breviario mozárabe y uno romano y los arrojó al fuego. Al arder solo el breviario romano, el rey volvió a arrojar al fuego el mozárabe, imponiendo así el rito romano. Es posible que esta leyenda sea el origen del refrán que afirma «Allá van las leyes, do quieran los reyes».

Alfonso VI, el conquistador de Toledo, el gran monarca europeizador, ve, en los últimos años de su reinado, cómo la gran obra política realizada se resquebraja ante el empuje almorávide y las debilidades internas. Alfonso VI había asumido plenamente la idea imperial leonesa y su apertura a la influencia europea le había hecho conocer las prácticas políticas feudales que, en la Francia de su tiempo, alcanzaban su expresión más acabada. En la conjunción de estos dos elementos, ve Claudio Sánchez-Albornoz la explicación de la concesión "iure hereditario" (reparto entre las dos hijas y el hijo del reino en lugar de legar todo el reino al único hijo varón) –más propio de la tradición navarroaragonesa– de los gobiernos de los condados de Galicia y Portugal a sus dos yernos borgoñones, Raimundo, primer marido de Urraca, y Enrique, casado con Teresa. De esa decisión, arrancó, a la vuelta de unos años, la independencia portuguesa y la perspectiva de una Galicia independiente bajo Alfonso Raimúndez, que luego no se hizo realidad al convertirse este en Alfonso VII de León.



</doc>
<doc id="6656" url="https://es.wikipedia.org/wiki?curid=6656" title="Antesis">
Antesis

La antesis (del griego ἄνθησις "floración") es el periodo de florescencia o floración de las plantas con flores; estrictamente, es el tiempo de expansión de una flor hasta que está completamente desarrollada y en estado funcional, durante el cual ocurre el proceso de polinización, si bien se usa frecuentemente para designar el período de floración en sí; el acto de florecer. La etapa previa a la floración suele llamarse preantesis. 

El desarrollo de la antesis es espectacular en algunas especies con flores múltiples:

En las especies de "Banksia", por ejemplo, la antesis expone los extremos de los estilos desde las partes superiores de los periantos de cada flor individual, primero en la parte baja de la inflorescencia hasta que la floración completa alcanza a las flores de la parte superior. 

La antesis de las flores de la "Banksia" es secuencial en la inflorescencia, así que cuando los estilos y los periantos se desarrollan secuencialmente con colores diferentes, el resultado es un cambio de color impactante que se desplaza gradualmente de abajo arriba a lo largo de la inflorescencia.


</doc>
<doc id="6657" url="https://es.wikipedia.org/wiki?curid=6657" title="Anemofilia">
Anemofilia

En Botánica se denomina anemofilia a la adaptación de muchas plantas espermatofitas que aseguran su polinización por medio del viento. El término se aplica también a cualquier dispersión de esporas realizado por el viento, como ocurre en muchos hongos o en los helechos.

Las Especies Anemófila tiene que ver con la polinización,ya que es conocida como la Polinización Anemófila.
La anemofilia es propia de especies que constituyen poblaciones densas en formaciones vegetales monoespecíficas o pauci específicas (con una o pocas especies), en donde son las especies dominantes del ecosistema. Sólo así puede ser eficaz la polinización por el viento. Las plantas que crecen dispersas, perdidas entre pies de otras especies, suelen ser polinizadas por insectos u otros animales es decir zoófilas, porque necesitan vectores especializados capaces de encontrar sucesivamente a los pocos individuos existentes en un área determinada. Las especies anemófilas tienen que producir en cambio cantidades muy grandes de polen, lo que es a veces muy perceptible en bosques de pinos, cuando el suelo se tiñe de amarillo por la gran cantidad de polen que estas especies producen durante la floración. Sólo cuando se dan estas dos condiciones puede asegurarse la polinización.

Entre los árboles la condición anemófila es propia de las coníferas, que tienden a formar bosques monoespecíficos en las latitudes frías y las montañas, o de los árboles dominantes en los bosques de las latitudes templadas, como robles o hayas. La anemofilia no se encuentra en los árboles de los bosques ricos en especies propios de las regiones tropicales. La anemofilia es también característica de las gramíneas y otras plantas próximas (como las ciperáceas) que constituyen formaciones herbáceas abiertas, donde no encuentra obstáculos el viento.

Otro grupo de polinización anemófila es la familia Juglandaceae, constituido por los Géneros "Juglans", que agrupa las especies de Nogal y los "Carya" y "Pterocarya", entre los que se encuentra la llamada Nuez Alada.
En América del sur la familia "Nothofagaceae" que forman densos bosques son polinizados por el viento. Ejemplos son la Lenga, el Coigüe y el Ñire.
Uno de los ejemplos más conocidos es el de la planta "Cannabis sativa", esta planta posee pequeñas flores las cuales son polinizadas por brisas de viento.

La anemófilia involucra una gran cantidad de adaptaciones a nivel de inflorescencias, flores y gametofitos. Así, las especies anemófilas suelen presentar flores poco vistosas, sin pétalos atractivos o, directamente, sin pétalo alguno, como en los fresnos. Esta desaparición del perianto también se aplica a otras estructuras bastante típicas tales como nectario, osmóforos y guías de néctar, los cuales se tornan innecesarios en estas especies. Además, las flores se disponen en inflorescencia frecuentemente péndulas (como por ejemplo en los amentos masculinos péndulos de "Corylus, Alnus" y "Quercus").

Asimismo, las especies anemófilas caducifolias suelen florecer temprano, antes que el follaje aparezca, de modo tal que el mismo no obstaculice la circulación del polen (ejemplo, en los robles, alisos, fresnos, sauces, álamos y olmos).

Este particular modo de polinización requiere que el polen sea pequeño o con una relación superficie/volumen muy grande, lo que reduce su velocidad de sedimentación y facilita que llegue más lejos cuando es arrastrado por el viento. Así, por ejemplo, las compuestas del género "Artemisia" presentan un polen pequeño, liso y seco, mientras que la mayoría de los miembros de la familia lo presentan más grande, ornado y cubierto de aceites, lo que facilita la aglutinación y la adherencia a vectores animales. En los pinos la ligereza se consigue por medio de dos sacos aéreos huecos, lo que reduce su densidad y aumenta la superficie de rozamiento. Asimismo, los granos de polen de las especies anemófilas carecen de cemento polínico, son secos y frecuentemente de exina lisa, por ello se separan fácilmente unos de otros.

La producción de gran cantidad de polen en estas especies se logra a través del incremento en el número de flores masculinas o de estambres (por ejemplo, en "Corylus", se producen 2 millones y medio de granos de polen por cada estambre). La expulsión del polen es facilitada por la movilidad de los filamentos de los estambres (poáceas) o por mecanismos de tensión de las anteras dentro del pimpollo floral que determinan literalmente la "explosión" de la flor y la liberación concomitante de una nube de polen en el momento de la antesis "(Urtica, Pilea").
Los estilos y estigmas de las especies anemófilas están muy agrandados con el objeto de facilitar la captura del polen que es transportado en el aire. El número de óvulos en los carpelos, por el contrario, suele ser muy reducido en relación al hecho de que la polinización usualmente la llevan a cabo granos de polen aislados.




</doc>
<doc id="6660" url="https://es.wikipedia.org/wiki?curid=6660" title="Caducifolio">
Caducifolio

Caducifolio, del latín "cadūcus" («caduco, caído», participio de "cadĕre" «caer») y "folĭum" («hoja»), hace referencia a los árboles o arbustos que pierden su follaje durante una parte del año, la cual coincide en la mayoría de los casos con la llegada de la época desfavorable, la estación más fría (invierno) en los climas templados. Sin embargo, algunos pierden el follaje durante la época seca del año en los climas cálidos y áridos.

También son llamados de hoja caduca, por oposición a los árboles llamados de hoja perenne. En Puerto Rico, por la influencia que ejerce la cultura estadounidense, es también conocido como deciduo, calco del inglés "deciduous". A su vez, la raíz de esta palabra remite al latín "dēciduus", derivada de "dēcidō", «caer, morir».

Muchos árboles y arbustos caducifolios florecen durante el período en que no tienen hojas, ya que esto aumenta la efectividad de la polinización. La ausencia de hojas beneficia la dispersión del polen por el viento o, en el caso de las plantas polinizadas por insectos, el que las flores sean más visibles por estos. Sin embargo, esta estrategia no carece de riesgos, ya que las flores pueden resultar dañadas por el hielo o, en las zonas con estaciones secas, las plantas pueden agotarse más con este esfuerzo.



</doc>
<doc id="6662" url="https://es.wikipedia.org/wiki?curid=6662" title="Polinización">
Polinización

La polinización es el proceso de transferencia del polen desde los estambres hasta el estigma o parte receptiva de las flores en las angiospermas, donde germina y fecunda los óvulos de la flor, haciendo posible la producción de semillas y frutos.

El transporte del polen lo pueden realizar diferentes agentes que son llamados vectores de polinización. Los vectores de polinización pueden ser tanto bióticos, como aves, insectos (principalmente abejas), murciélagos, etc.; como abióticos, por ejemplo agua o viento.

Existe una gran variedad de vectores bióticos, entre ellos los himenópteros (abejorros, abejas y avispas), lepidópteros (mariposas y polillas) y dípteros (moscas), así como colibríes, algunos murciélagos y en casos raros algunos ratones o monos. 

Algunas flores pueden ser polinizadas por muchos vectores, en cuyo caso se dice que son flores generalistas en cuanto a polinizadores; o, por el contrario, sólo pueden ser polinizadas por un género o especie debido a que la morfología tanto de la flor como del polinizador se han acoplado a lo largo de la evolución, en cuyo caso se dice que las flores son especialistas. La especialización de la polinización genera un beneficio tanto para la planta como para el polinizador por lo cual ésta se vuelve muy eficiente pues el insecto volará con seguridad a otra flor de la misma especie y depositará el polen en el estigma de esta flor. Entre las orquídeas es común encontrar una gran especialización en la interacción con los polinizadores (Véase "Xanthopan morganii praedicta").

El transporte del polen lo pueden realizar agentes físicos como el viento (plantas anemófilas), el agua (plantas hidrófilas), o un polinizador animal (plantas zoófilas). Las características físicas y fenológicas de las flores anemófilas, hidrófilas y zoófilas, así como las de su polen, suelen ser marcadamente diferentes. Las plantas zoófilas deben llamar la atención de sus vectores con colores y olores atrayentes, así como recompensarlos con alimento o refugio. Diferentes tipos de polinizadores requieren diferentes tipos de atractivos, así las flores zoófilas han evolucionado y se han diversificado en una gran variedad de tipos los cuales pueden agruparse en síndromes florales. La belleza visual característicamente asociada a las flores es el efecto de su coevolución con insectos u otros animales polinizadores.

En los casos en que la polinización se produce como resultado de relaciones planta-animal estas relaciones son predominantemente de tipo mutualista, es decir relaciones en que ambos participantes se benefician. A diferencia de las relaciones obligatorias (propiamente simbióticas) que existen en la naturaleza, la mayoría de las relaciones de polinización son facultativas u opcionales y muy flexibles: la desaparición de un polinizador o planta no acarrea necesariamente la extinción del otro participante en la interacción, ya que cada uno de ellos posee alternativas (otras fuentes de alimento en el caso del animal, u otras especies de polinizadores en el caso de la planta). Sin embargo, existen algunos casos sumamente interesantes de relaciones simbióticas entre un polinizador y una especie de plantas, tales como la avispa de los higos y la polilla de la yuca.

Los fósiles más antiguos que muestran polinización son plantas espermatofitas del Carbonífero tardío (se trata de polinización abiótica). Algunas gimnospermas del período Triásico ya presentan señales de polinización biótica, o sea por animales, en que los granos fosilizados tienen algunas de las características de granos de polen que son llevados por agentes polinizadores en el presente. Además el contenido intestinal, las piezas bucales y estructura de las alas de ciertos escarabajos y moscas sugieren que deben haber actuado como polinizadores. Los primeros síndromes florales de polinización surgieron entonces.

La asociación entre escarabajos y angiospermas en el Cretácico temprano llevó a radiaciones evolutivas tanto de unos como de otros en el Cretácico tardío. La evolución de los nectarios u órganos productores de néctar señalan el comienzo de un mutualismo entre insectos himenópteros y angiospermas.

En años recientes ha habido una pérdida de polinizadores. Tales pérdidas pueden tener un gran impacto económico. Entre las posibles causas están pérdida de hábitat, plaguicidas, parasitismo, cambios climáticos y otras. Algunos investigadores piensan que es un sinergismo de todos estos factores. Ver: David Ward Roubik. Ups and Downs in Pollinator Populations: When is there a Decline?

Los polinizadores silvestres, a menudo visitan un gran número de especies de plantas; éstas, a su vez, son visitadas por muchas especies de polinizadores. Todas estas relaciones forman una red de interacciones entre plantas y polinizadores. La estructura de estas redes presenta sorprendentes similitudes en diferentes ecosistemas y continentes.

La estructura de las redes planta-polinizador puede tener un gran impacto en la forma en que estas comunidades responden a los estreses ecológicos. Hay modelos matemáticos que analizan las consecuencias de la estructura de las redes en la estabilidad de las comunidades de polinizadores. Tales modelos sugieren que la forma específica en que las redes están organizadas reduce la competencia entre polinizadores y aumenta la biodiversidad. Esto incluso puede llevar a una fuerte facilitación entre polinizadores cuando las condiciones son seriamente desventajosas. Así es posible que el conjunto de especies de polinizadores pueda sobrevivir condiciones severas. Pero también significa que las especies de polinizadores puedan llegar a un colapso simultáneo si las condiciones ambientales llegan a un punto crítico. La recuperación de la comunidad de polinizadores y plantas después de tal colapso puede ser sumamente difícil.

En agricultura, la mayoría de los cultivos, por ejemplo los cereales, son anemófilos, es decir polinizados por el viento o son autógamos (autopolinizados). Aproximadamente el 30% de los cultivos agrícolas del mundo (por ejemplo, muchos frutos y hortalizas) dependen de la polinización realizada por insectos y otros animales.

Es un error creer que la polinización es un «servicio ecológico gratuito» de la naturaleza. Una polinización efectiva necesita algunos recursos, por ejemplo refugios de vegetación natural prístina y hábitats adecuados para los polinizadores. Cuando éstos se reducen o se pierden, se limita la actividad de los polinizadores y se necesitan prácticas de gestión adaptable para mantener los medios de subsistencia.

En efecto, en todo el mundo la diversidad agrícola y de los agroecosistemas afronta el peligro de que las poblaciones de polinizadores están disminuyendo. Los principales causantes de este problema son la fragmentación de los hábitats, las sustancias químicas agrícolas e industriales, los parásitos y las enfermedades, así como la introducción de especies exóticas. En California, los productores de almendras habitualmente importan abejas melíferas de otros estados de los Estados Unidos para asegurar la polinización de sus cultivos. Este transporte puede contribuir a las epidemias.




</doc>
<doc id="6665" url="https://es.wikipedia.org/wiki?curid=6665" title="Iris (mitología)">
Iris (mitología)

En la mitología griega, Iris (en griego Ἶρις, ‘arco iris’) es hija de Taumante y de la oceánide Electra y hermana de las Harpías y de Arce. En la "Ilíada", se la describe como mensajera de los dioses; sin embargo, en la "Odisea" este papel está reservado a Hermes. También aparece en la "Eneida" como mensajera de Hera. Eurípides la incluye en su tragedia "Heracles", Iris aparece como, otra vez, la mensajera de Hera.

Iris es la diosa del arco iris que anuncia el pacto de los humanos y los dioses y el fin de la tormenta; al igual que Hermes, es la encargada de hacer llegar los mensajes de los dioses a los seres humanos. También es conocida como una de las diosas del mar y del cielo.

Durante la Titanomaquia, Iris fue elegida para ser la mensajera de los Olímpicos mientras que su hermana gemela, Arce, se convirtió en mensajera de los Titanes.

En el himno homérico a Apolo, los dioses presentes en el nacimiento en Delos, enviaron a Iris para que trajera a Ilitia y ayudara a Leto a dar a luz a Apolo y Artemisa.

En la Ilíada, Iris fue quien avisó a Menelao del secuestro de Helena en Esparta. Impidió a Hera y Atenea entrar en combate para ayudar a los aqueos. Ayudó a Afrodita cuando fue herida por Diomedes, donde la llevó al Olimpo conduciendo las riendas del los caballos de Ares. Además, fue la encargada de avisar a Aquiles para que liberara el cadáver de Patroclo, que estaba en poder de los troyanos. A continuación, Iris acude a la morada de los vientos para informarles de las súplicas de Aquiles; que avivaran la hoguera en la que yacía el cadáver de su amigo.

En la Eneida, por orden de Hera, Iris corta el cabello rubio de la reina Dido que une a las personas a la vida. En otro pasaje, Iris toma la forma de la anciana Beroe, para que suscite en las mujeres troyanas el deseo de no viajar más y quemar las naves de Eneas. Por otra parte, Iris acompaña a Turno a la batalla, en donde informa al rey que los troyanos están sin su caudillo.

En la tragedia de Eurípides, "Heracles", Iris es la responsable de la locura de Heracles.

En una ocasión, Iris le pide a los Argonautas alados, Calais y Zetes que no maten a sus hermanas las Harpías y promete que Fineo no será molestado por ellas nunca más.

Se representa a Iris como una hermosa joven virgen con alas doradas y con una túnica múlticolor, apresurándose a la velocidad del viento de un extremo a otro del mundo, a las profundidades del mar y del inframundo en donde tenía acceso libre. Es la mensajera especialmente de Hera, y está relacionada con Hermes, cuyo caduceo lleva a menudo. Por orden de Zeus, lleva un jarrón con agua del río Estigia, con la que hace dormir a todos los que perjuran. Sus atributos son el caduceo y un jarrón. También es representada suministrando a las nubes el agua que necesitan para inundar el mundo. 

Puesto que la función de Iris es transmitir los mensajes de los dioses, Platón relacionaba su etimología con "eireín", cuyo significado es «hablar». Así, Iris personificaría la dialéctica y la filosofía. Su origen sería el asombro, puesto que su padre, Taumante, está relacionado etimológicamente con la palabra "thoûma" (asombro).


</doc>
<doc id="6671" url="https://es.wikipedia.org/wiki?curid=6671" title="Alejandro Magno">
Alejandro Magno

Alejandro III de Macedonia (Pela, Grecia, 20 o 21 de julio de 356 a. C.-Babilonia, 10 o 13 de junio de 323 a. C.), más conocido como o Alejandro el Grande, fue el rey de Macedonia desde 336 a. C. hasta su muerte en junio de 323 a. C. Hijo y sucesor de Olimpia de Epiro y Filipo II de Macedonia, su padre, quien lo preparó para reinar, proporcionándole una experiencia militar y encomendando a Aristóteles su formación intelectual. Alejandro Magno dedicó los primeros años de su reinado a imponer su autoridad sobre los pueblos sometidos a Macedonia, que habían aprovechado la muerte de Filipo para rebelarse. Como hegemón de toda Grecia en concepto de sucesor de su padre (asesinado), continuó el plan de su padre y el que habían aprobado las polis griegas: conquistar el vasto imperio de Persia, para vengar todos los daños que les habían causado a los griegos por siglos, incluyendo la captura de todas las ciudades costeras de Asia Menor y varias islas del mar Egeo. Preparó un ejército de aliados griegos (mayormente macedonios) y en el año 334 a. C. se lanzó con su pequeño ejército, de apenas 40000 hombres, contra el poderoso Imperio persa: una guerra de venganza de los griegos —bajo el liderazgo de Macedonia— contra los persas.

En su reinado de trece años, cambió por completo la estructura política y cultural de la zona al conquistar el Imperio aqueménida y dar inicio a una época de extraordinario intercambio cultural, en la que los griegos se expandieron por los ámbitos mediterráneo y próximoriental. Es el llamado Período helenístico (323 a. C.-30 a. C.) Tanto es así, que sus hazañas lo han convertido en un mito y, en algunos momentos, en casi una figura divina, posiblemente por la profunda religiosidad que manifestó a lo largo de su vida.

Tras consolidar la frontera de los Balcanes y la hegemonía macedonia sobre las ciudades-estado de la antigua Grecia, poniendo fin a la rebelión que se produjo tras la muerte de su padre, Alejandro cruzó el Helesponto hacia Asia Menor (334 a. C.) y comenzó la conquista del Imperio persa, regido por Darío III. Victorioso en las batallas del Gránico (334 a. C.), Issos (333 a. C.), Gaugamela (331 a. C.) y de la Puerta Persa (330 a. C.), se hizo con un dominio que se extendía por la Hélade, Egipto, Anatolia, Oriente Próximo y Asia Central, hasta los ríos Indo y Oxus. Habiendo avanzado hasta la India, donde derrotó al rey Poro en la batalla del Hidaspes (326 a. C.), la negativa de sus tropas a continuar hacia Oriente le obligó a retornar a Babilonia, donde falleció sin completar sus planes de conquista de la península arábiga. Con la llamada «política de fusión», Alejandro promovió la integración de los pueblos sometidos a la dominación macedonia promoviendo su incorporación al ejército y favoreciendo los matrimonios mixtos. Él mismo se casó con dos mujeres persas de noble cuna.

En sus 32 años de vida, su Imperio se extendió desde Grecia, hasta el valle del Indo por el Este y hasta Egipto por el Oeste, donde fundó la ciudad de Alejandría (hoy Al-ʼIskandariya, الاسكندرية). Fundador prolífico de ciudades, esta ciudad egipcia habría de ser con mucho la más famosa de todas las Alejandrías fundadas por el también faraón Alejandro. De las 70 ciudades que fundó, 50 de ellas llevaban su nombre.

El conquistador macedonio falleció en circunstancias oscuras -los escritos más antiguos dejan clara evidencia de una muerte lenta producto de un envenenamiento- dejando un imperio sin consolidar. El control sobre diversas regiones era débil en el mejor de los casos, y había regiones del norte de Asia Menor que jamás se hallaron bajo dominio macedonio. Al morir sin nombrar claramente un heredero, le sucedió su medio hermano Filipo III Arrideo (323-317 a. C.), que era una persona con discapacidad intelectual, y su hijo póstumo Alejandro IV (323-309 a. C.). El verdadero poder estuvo en manos de sus generales, los llamados diádocos (sucesores), que iniciaron una lucha despiadada por la supremacía que conduciría al reparto del imperio de Alejandro y su fraccionamiento en una serie de reinos, entre los cuales acabarían imponiéndose el Egipto Ptolemaico, el Imperio seléucida y la Macedonia antigónida.

Alejandro es el mayor de los iconos culturales de la Antigüedad, ensalzado como el más heroico de los grandes conquistadores, un segundo Aquiles («soldado y semidiós»), para los griegos su héroe nacional y libertador, o vilipendiado como un tirano megalómano que destruyó la estabilidad creada por los persas. Su figura y legado han estado presentes en la historia y la cultura, tanto de Occidente como de Oriente, a lo largo de más de dos milenios, y ha inspirado a los grandes conquistadores de todos los tiempos, desde Julio César hasta Napoleón Bonaparte.

Hijo de Filipo II, rey de Macedonia (dinastía de los Argéadas), y de Olimpia, hija de Neoptólemo I de Epiro, según Plutarco, el día de su nacimiento se tuvo noticia en la capital de tres triunfos: el del general Parmenión frente a los Ilirios, la victoria del sitio a una ciudad portuaria por su padre y la victoria del carro del rey en competición, que fueron considerados increíbles augurios en aquel tiempo, aunque quizá fueran meras invenciones posteriores creadas bajo la aureola de grandeza de este personaje.

Existen dudas sobre la paternidad de Filipo sobre Alejandro, ya que hay otras dos versiones, y posiblemente haya existido una infidelidad por parte de su madre. Plutarco refiere que su madre Olimpia quedó encinta luego que un rayo cayera sobre su vientre. Filipo notó que en su abdomen apareció una figura del rostro de un león y la acusó de adulterio. Es una de las razones por la que el rey macedonio deseaba contraer matrimonio con otra mujer. En tanto, Pseudo Calístenes narra que la vinculación de Alejandro con el dios Amón y la posterior visita al oráculo está relacionada con su verdadero padre, el faraón egipcio Nectanebo I, que huyó a Grecia al ser invadido su país nuevamente por los persas. Según el relato histórico Nectanebo fue recibido en la corte de Filipo como «mago».

Personificado bajo la serpiente de Amon, convenció a Olimpia de engendrar un hijo que ponga a salvo a las dos naciones, a lo cual ella accedió. Se ha mantenido varios años en la corte, hasta que murió en una caminata nocturna junto a «su hijo». Alejandro, según Calístenes, supo que su verdadero padre era Nectanebo esa misma noche, razón por la que, descreyendo de él lo empujó a un pozo y murió. Hay vertientes que pueden indicar una cosa como la otra. Lo cierto es que los autores más antiguos que escribieron sobre él dan por válido un adulterio de Olimpia. Y no es para nada desechable la relación de Amon dios que «engendró» a Alejandro, ya que según relatos en el oráculo de Amon, lo trataron como hijo de Zeus («el rayo»).

Alejandro Magno tenía el hábito de inclinar ligeramente la cabeza sobre el hombro derecho, era físicamente de hermosa presencia, de baja estatura 1.65-1.70 cm, con cutis blanco, la nariz algo curva inclinada a la izquierda, cabello semi-ondulado de color castaño claro, con un estilo de cabello denominado anastole («dentro del espíritu») y ojos heterócromos (el izquierdo marrón, y el derecho gris), se desconoce si eran así de nacimiento o como consecuencia de un traumatismo craneal. Plutarco y Calístenes citan que poseía un aroma físico agradable naturalmente, a lo que ellos llamaban «humor». Por descripciones de Plutarco, normalmente antes de dar batalla, Alejandro lanzaba un dardo hacia el cielo (Zeus) con la mano izquierda, como también se aprecian en algunas de sus esculturas, se lo ve portando objetos con el mismo brazo, por lo que sería aceptable afirmar que era zurdo.

Su educación fue inicialmente dirigida por Leónidas, un austero y estricto maestro macedonio que daba clases a los hijos de la más alta nobleza, que lo inició en el ejercicio corporal pero también se encargó de su educación. Lisímaco, un profesor de letras bastante más amable, se ganó el cariño del Magno llamándole Aquiles, y a su padre, Peleo. Sabía de memoria los poemas homéricos y todas las noches colocaba la "Ilíada" debajo de su cama. También leyó con avidez a Heródoto y a Píndaro.

Se cuentan numerosas anécdotas de su niñez, siendo la más referida aquella que narra Plutarco: Filipo II había comprado un gran caballo al que nadie conseguía montar ni domar. Alejandro, aun siendo un niño, se dio cuenta de que el caballo se asustaba de su propia sombra y lo montó dirigiendo su vista hacia el Sol. Tras domar a Bucéfalo, su caballo, su padre le dijo: «Búscate otro reino, hijo, pues Macedonia no es lo suficientemente grande para ti».Según coinciden algunos historiadores antiguos, especialmente Calístenes, quien narra la participación de Alejandro en su adolescencia de los Juegos Olímpicos (a pedir de Filipo), en la cual obtuvo victorias en competencias de carros.

A los trece años fue puesto bajo la tutela de Aristóteles, el filósofo que más influyó en la filosofía y las ciencias. Durante cinco años sería su maestro, en un retiro de la ciudad macedonia de Mieza. Aristóteles le daría una amplia formación intelectual y científica en las ramas que este abordó, como filosofía, lógica, retórica, metafísica, estética, ética, política, biología, y otras tantas áreas.

Muy pronto (340 a. C.) su padre lo asoció a tareas del gobierno nombrándolo regente, a pesar de su juventud. Recibía personalmente a los enviados persas, deseosos que Macedonia pagase los altos tributos exigidos por Darío. Les conversaba amablemente, y así obtenía información, acerca de las travesías de rutas tierra-mar, la preparación del ejército persa, valioso para las acciones que desarrolló en el futuro. En el 338 a. C. dirigió la caballería macedónica en la batalla de Queronea, siendo nombrado gobernador de Tracia ese mismo año. Desde pequeño, Alejandro demostró las características más destacadas de su personalidad: activo, enérgico, sensible y ambicioso. Es por eso que, a pesar de tener apenas 16 años, se vio obligado a repeler una insurrección armada. Se afirma que Aristóteles le aconsejó esperar para participar en batallas, pero Alejandro le respondió: «Si espero, perderé la audacia de la juventud».

Su culto estaba dedicado enteramente a los Dioses Olímpicos.principalmente Zeus (Dios Supremo del Panteón olímpico, del Cielo y del Trueno). Zeus es el padre de los dioses, llamados «menores» o "theos", a quienes también rindió culto, especialmente Ares (dios de la Guerra), Apolo (dios de la Música y la Medicina), Atenea (diosa de la Estrategia y la Victoria), y como semi-dios Aquiles, su héroe, a quién honorificó en Troya. Existen reiterados pasajes donde en los preparativos a las batallas le rendía sacrificios (de animales) a Ares. En la mitología griega, este "theos" estaba enemistado con Atenea, a quien también recurrió para elaborar sus planes, por lo que realizó prácticas conciliadoras entre ambas divinidades,y los constantes diálogos con Zeus .

Con respecto a otros credos, su idiosincrasia fue la tolerancia religiosa para con otras culturas y civilizaciones, incluso venerándolas, tal como es el caso de Egipto (especialmente Amón, su "padre"), Persia, India. Respecto a su creencia y culto religioso, entonces son todos coincidentes los relatos de Calístenes (sobrino de Aristóteles, y su biógrafo en vida), Ptolomeo (su amigo y general, quien a su muerte heredó Egipto), Plutarco (curador del Oráculo de Delfos), y el filósofo-militar Flavio Arriano

Un nuevo matrimonio de su padre, que podría llegar a poner en peligro su derecho al trono (no conviene olvidar que el mismo Filipo fue regente de su sobrino Amintas IV —hijo de Pérdicas III—, hasta la mayoría de edad, pero se adueñó del trono), hizo que Alejandro se enemistara con Filipo. Es famosa la anécdota de cómo, en la celebración de la boda, el nuevo suegro de Filipo (un poderoso noble macedonio llamado Átalo) rogó porque el matrimonio diera un heredero legítimo al rey, en alusión a que la madre de Alejandro era una princesa de Epiro y que la nueva esposa de Filipo, siendo macedonia, daría a luz a un heredero totalmente macedonio y no mitad macedonio y mitad epirota como Alejandro, con lo cual sería posible que se relegara a este último de la sucesión. Alejandro se enfureció y le lanzó una copa, espetándole: «Y yo ¿qué soy? ¿un bastardo?». En ese momento Filipo se acercó a poner orden, pero debido a su estado de embriaguez, se tropezó y cayó al suelo, lo que le granjeó una burla de Alejandro: «Quiere cruzar Asia, pero ni siquiera es capaz de pasar de un lecho a otro sin caerse.» La historia le valió la ira de su padre, por lo que Alejandro tuvo que exiliarse a Epiro junto con su madre, Olimpia. Para evitar un complot, Filipo también ordenó el exilio de todos sus amigos, siendo Frigio uno de los más cercanos. Mas tarde, Filipo terminaría por perdonarle.

Filipo muere asesinado en el año 336 a. C. a manos de Pausanias, un capitán de su guardia, como resultado de una conspiración que es generalmente atribuida a Olimpia. Luego de este hecho, Alejandro se aseguró que no quedara vivo ningún heredero que pudiese reclamar el trono, de esta forma tomaría las riendas de Macedonia a la edad de 20 años.

Tras suceder a su padre, Alejandro se encontró con que debía gobernar un país radicalmente distinto de aquel que heredó Filipo II 23 años antes, ya que Macedonia había pasado de ser un reino fronterizo, pobre y desdeñado por los griegos, a un territorio que tras el reinado de Filipo se consideraba como parte de la Hélade y un poderoso Estado militar de fronteras consolidadas con un ejército experimentado que dominaba indirectamente a Grecia a través de la Liga de Corinto. En un discurso, puesto en boca de Alejandro por el filósofo e historiador griego Flavio Arriano, se describía la transformación del pueblo macedonio en los siguientes términos:

La muerte del gran Filipo supuso que algunas polis griegas sometidas por él se alzasen en armas contra Alejandro ante la aparente debilidad de la monarquía macedonia. Alejandro debía resolver dos puntos importantes: mantener el control de las ciudades y reclutar mercenarios de las polis para su campaña contra Persia.

En la primavera del 335 a.C. lanza una exitosa campaña al norte, Iliria (hoy Albania y FYROM) y Tracia (hasta las inmediaciones del río Danubio, hoy Rumania), donde es avisado que Tebas se había sublevado, tomando una guarnición macedonia.

Alejandro, con una reacción relámpago, demostró rápidamente su destreza estratégica y militar: viajó casi 600 kilómetros hasta Tesalia para reafirmar el dominio en la región (ya había sido conquistada por Filipo), y emprendió el camino hacia el Ática, reprimiendo la sublevación de Tebas, que opuso una feroz resistencia, reduciendo la ciudad a escombros. Luego de ajusticiar a los sublevados, entrevistó a una parte de la población, ordenando más tarde la reconstrucción de la ciudad. Uno de los perjudicados era un deportista tebano de los Juegos Olímpicos, a quien Alejandro felicitó durante el desarrollo de estos, y otro relato cuenta que una mujer que mató a un general tracio durante la contienda, fue liberada luego de haber hecho una «defensa sincera».

Camino al sur del Ática, visitó el gran oráculo de Delfos, donde un general ateniense había depuesto a la pitonisa del templo, y que luego Alejandro restableció a la misma en su puesto. Allí tuvo en dos ocasiones sus oráculos. La primera visita fue bastante errática, teniendo los sacerdotes que irrumpir en varias ocasiones. «Alejandro, no puedes entrar con espadas aquí. Y tampoco puedes llevarte las cosas». En la segunda, fue a pedir el oráculo, pero en la residencia la pitia (sacerdotisa), que forcejeando le dijo «hijo mío, eres invencible».Su paso por Atenas fue por demás totalmente atípica. Los atenienses cerraron sus puertas, no por sublevación, sino por temor por lo ocurrido en Tebas. Alejandro, que sentía un gran respeto por los filósofos, el arte y la cultura de la ciudad, envió entonces una primera carta (era su estilo), a lo que respondieron: «estamos debatiendo si presentarte batalla o dejarte entrar». Por lo que Alejandro, a través de otra carta propuso dejar a su ejército fuera y entrar solo. Dejó que solamente lo acompañaran algunos de sus amigos, los "hetaroi." Una vez allí, Atenas reconoció su supremacía por el gesto, nombrándolo de esta manera "Hegemon", título que ya había ostentado su padre y que lo situaba como gobernante de toda Grecia, consolidando así la hegemonía macedónica, tras lo cual Alejandro se dispuso a cumplir su siguiente proyecto: conquistar el Imperio persa.
Una conocida historia fue, de visita en Corinto durante los Juegos Ístmicos, se encontró con el filósofo Diógenes de Sinope, que se encontraba sentado en un gran barril reflexionando, preguntándole «Diógenes, dime que puedo hacer por ti?». A lo que este le respondió con una ironía: «si, te corres de ahí, que me tapas el sol». La elocuente respuesta le valió las bromas de sus "compañeros" allí presentes. Asombrado por la elocuencia, Alejandro exclamó «si no fuera yo Alejandro, me gustaría ser Diógenes!». Esto trascendió en los manuscritos de los filósofos y sofistas de toda Grecia. En otra ocasión, encontró a Diógenes revolviendo basura, al preguntarle qué era lo que estaba buscando, Diógenes respondió «estoy buscando huesos de esclavos, pero no hallo la diferencia entre éstos y los de tu padre». Era claro que Diógenes despreciaba a Alejandro, quien nunca tomó represalia alguna.

Alejandro, luego de asegurar el orden en toda la región de la hélade y sureste de Europa, dejó a Antípatro al mando de todos los dominios. Preparó 160 embarcaciones, abastecimiento suficiente y armamento (y ya no contaba con tanto dinero para pagar a sus hombres), con su pequeño ejército de unos 40 000 soldados que contaba con miles de aliados griegos y mercenarios. Cruzó el Helesponto hacia Asia Menor, para iniciar la conquista del Imperio Persa, pretendiendo seguir los planes de su padre de liberar a todas las ciudades "polis" griegas de la zona de Jonia (Misia, Lidia, Licia) que se encontraban bajo dominio persa en Asia Menor (hoy Turquía). Hizo una breve parada en Troya, donde honró la tumba de su héroe Aquiles (héroe griego de la Guerra de Troya, relatada por Homero en "La Ilíada"). 

En la primera contienda que se libró en territorio asiático, la batalla del Gránico, a orillas del riachuelo Gránico, los sátrapas persas le hicieron frente con un ejército de igual número que los helenos, unos 40 000 hombres, comandado por Memnón de Rodas, compuesto en su mayor parte por persas en la vanguardia, y mercenarios griegos en la retaguardia, pero el ejército persa ofreció una débil resistencia y fue vencido. En este combate Alejandro estuvo cerca de la muerte, pues un persa trató de matarlo por la espalda. Finalmente salvó su vida gracias a Clito, uno de los hombres de confianza de Filipo, que mató al enemigo.Memnón era un general mercenario griego al servicio de Persia, y poseía amplios dominios en el emplazamiento de Troya, donde se desarrolló la batalla del Gránico. En otros tiempos Filipo II (padre de Alejandro) le dio hospedaje junto a su familia en Macedonia durante la invasión persa, donde conoció a Alejandro y al filósofo Aristóteles, por lo que conocía muy bien al oponente. Con una inmensa flota bajo su mando, su objetivo fue recuperar las tierras que los persas le obsequiaron, atacando las líneas de suministros a Alejandro a través del Helesponto e islas del Egeo, y recibiendo una gran cantidad de barcos desde Chipre, Fenicia y Egipto. Memnón puso en aprietos a Alejandro en varias ocasiones con sus movimientos tácticos. Desafortunadamente para los persas, Memnon muere durante el asedio a Mitilene. Las ciudades griegas de las costas, Éfeso, Halicarnaso, Pérgamo, Mileto, y otras tantas más, lo recibieron como libertador, y otras se sometieron por temor.

Con la muerte de Memnon, la amenaza marítima estaba ya descartada, y teniendo ya el control del Mar Egeo, Alejandro dispuso hacer una pausa en Jonia, nuevamente restablecida a los griegos, ya sin la amenaza persa. Allí conoció al célebre pintor, Apeles.

Alejandro fue un gran amante de las artes. Era consciente del poder de propaganda que puede tener el arte y supo muy bien controlar la reproducción de su efigie, cuya realización solo autorizó a tres artistas: el célebre escultor Lisipo, un orfebre. Y un pintor, el jonio Apeles. Los biógrafos de Alejandro cuentan que este tenía en gran aprecio al pintor y que visitaba con frecuencia su taller y que incluso se sometía a sus exigencias. Son innumerables las representaciones de Apeles pintando sin atuendos a Alejandro, y a Campaspe, concubina del macedonio, y de aparentemente una gran belleza. Campaspe fue también modelo para representar a la diosa Afrodita (Venus) Una vez concluida esta primera etapa de conquistas, se celebraron bodas masivas de soldados griegos y mujeres de la polis liberadas. Por lo que en el otoño de 334 a. C., estando Alejandro en Caria, envió a aquellos soldados recién casados a Macedonia para que pasaran el invierno junto a sus esposas. Coeno, uno de los comandantes más capaces de Alejandro, los condujo de vuelta a Grecia. A finales de 334 a. C., Alejandro decidió pasar el invierno en Gordión, antigua capital de Frigia (al centro de Turquía), a la espera de refuerzos. Allí se encontraba un famoso carro real, sujeto a un nudo muy complicado de deshacer. Según el oráculo de Gordión, "quien supiera deshacerlo conquistaría Asia". Algunas fuentes indican que Alejandro desató el nudo pacientemente, mientras que otras afirman que lo cortó con su espada. En cualquier caso, la tormenta que siguió al hecho se interpretó como un claro signo de que Zeus daba su aprobación.

Coeno regresó de Grecia a encontrarse con Alejandro en Gordion, ya con refuerzos: los soldados macedonios recién casados y nuevos reclutas.

Alejandro se dirigió desde Gordion hacia la región de Cilicia, y emprendió su marcha hacia el sur, donde es avisado que desde Siria los persas, al mando del rey Darío, destruyeron un campamento macedonio, aniquilando sus guarniciones (que eran casi todos soldados heridos en batalla), por lo que tuvo que retomar el camino norte, donde los persas le hicieron frente del otro lado del río Issos, con un ejército superior a los 500 000 hombres, cuando los aliados griegos no superaban los 50 000 hombres. Aun así, prevaleció la estrategia por sobre el número. Los persas perdieron casi la mitad de sus tropas, y tal es como describen las narraciones de esta batalla, una verdadera masacre. Esta es conocida como la batalla de Isos —pequeña llanura situada entre las montañas y el mar cerca de Siria— en el 333 a. C., en la cual, el rey Darío, ante tal debacle, huyó amparado en la oscuridad de la noche dejando en el campo de batalla, abandonando sus tesoros, armas y su manto púrpura. El rey tomó conciencia de la amenaza y envió propuestas de negociación, que fueron rechazadas.

La familia de Darío III fue capturada en el interior de una lujosa tienda. Alejandro trató a todos con gran cortesía y les manifestó que no tenía "ninguna cuestión personal contra Darío, sino que luchaba contra él para conquistar Asia". Al tiempo le propondría matrimonio a una de sus hijas, Barsine-Estatira.

Luego de Issos, y asegurarse que no había amenazas por tierra y por mar, retomó el rumbo sur, conquistando fácilmente Fenicia, siendo bien recibido en Judea (considerado un "libertador", puesto que los liberó de los persas), y con la excepción de la isla de Tiro, donde quiso de manera pacífica honrar a los dioses en sus templos, enviando emisarios diplomáticos, que fueron asesinados a traición, por lo que decidió asediar esta ciudad, con una duración de enero a agosto de 332 a. C. Este asedio es conocido como el sitio de Tiro, una isla fortificada, en la que tuvo que construir muelles y vado sobre el mar, emplear torres de asedio y catapultas mas modernas (símil a grandes ballestas lanza-cohetes). Una vez destruidos los muros, Tiro fue arrasada. Otro sitio importante fue el de Gaza durante otro arduo enfrentamiento. Una vez conquistada, Alejandro se dirigió a Egipto.

Alejandro fue bien recibido por los egipcios, quienes le apoyaron en su lucha contra los persas, cuyos reyes habían dominado Egipto en dos ocasiones: de 523 a 404 a. C. (Dinastía XXVII) y de 343 a 332 a. C. (Dinastía XXXI). Recibido como su "salvador y libertador", e "hijo de Amón", por decisión popular se concedió a Alejandro la corona de los dos reinos, siendo nombrado faraón en noviembre de 332 a. C. en Menfis.

En enero del 331 a. C. Alejandro fundó la ciudad de Alejandría en una zona costera muy fértil al oeste del delta del Nilo. Los motivos de la fundación eran tanto económicos (la apertura de una ruta comercial en el mar Egeo) como culturales (la creación de una ciudad al estilo griego en Egipto, cuya planificación se dejó en manos del arquitecto Dinócrates). La escritora inglesa Mary Renault, en su biografía de Alejandro, comenta:

Posteriormente, tras un dificultoso viaje por el desierto, llegó al oasis de Siwa, donde el profeta del dios Amón le anunció que le saludaba tanto de parte del dios como de su padre. Alejandro preguntó si había quedado sin castigo alguno de los asesinos de su padre Filipo, y si se le concedería dominar a todos los hombres. Habiéndole dado el dios favorable respuesta y asegurándole que Filipo estaba vengado, Alejandro le hizo magníficas ofrendas, y entregó ricos presentes a los hombres allí destinados. También se dice que Alejandro, en una carta enviada a su madre, le comunicó haberle sido hechos ciertos vaticinios arcanos, que solo a ella revelaría. Algunos han escrito que queriendo el profeta saludarle en idioma griego con cierto cariño le dijo «hijo mío», equivocándose en una letra; y que a Alejandro le agradó este error, por dar motivo a que pareciera le había llamado hijo de Zeus.

La cultura del antiguo Egipto impresionó a Alejandro desde los primeros días de su estancia en este país. Los egipcios nos han dejado testimonio, grabado en piedra, de estos hechos y apetencias. En Karnak existe un bajorrelieve donde se representa a Alejandro haciendo ofrendas al dios Amón en calidad de converso. En él, viste la indumentaria de faraón:


En los jeroglíficos del muro se distinguen además los títulos de Alejandro-faraón que se representan dentro de un serej y un cartucho egipcio:

En esa época controló la situación de rebeldía en Anatolia y el Egeo, de tal modo que en la primavera del 331 a. C., desde Tiro, organizó los territorios conquistados. Darío, con un ejército más numeroso, decidió hacerle frente en Gaugamela a orillas del Tigris, pero apenas logró salvar su vida, ya que pese a la superioridad numérica se vio derrotado por el genio militar del joven rey macedonio. Así Alejandro con su ejército logró entrar en Babilonia quedando a las puertas del propio territorio persa.

En el año 331 a. C., el ejército macedonio invadió Persia entrando fácilmente a Susa, la vieja capital de Darío I, mientras que el derrotado Darío III huía hacia el interior del territorio persa en busca de fuerzas leales para enfrentar nuevamente a Alejandro.

Alejandro procedió cuidadosamente ocupando las ciudades, apoderándose de los caudales persas y asegurando las líneas de abastecimiento. Desde Susa pasó a Persépolis, capital ceremonial del Imperio aqueménida, donde quemó el palacio de la ciudad durante una fiesta. Después se dirigieron hacia Ecbatana para perseguir a Darío. Lo encontraron asesinado por sus nobles, que ahora obedecían a Bessos. Alejandro honró a su otrora rival y enemigo, cubriéndolo con el "manto púrpura" que Darío abandonó en la batalla de Isos, y que Alejandro recogió. Le rindió un funeral real y prometió a la familia de este perseguir a sus asesinos.Bessos escapó a la zona lindera del Hindú Kush (hoy Afganistán), en las inmediaciones de Sogdiana (al este de Asia), acompañado por una resistencia formada por nobles y arqueros a caballo, autoproclamándose rey de Persia, cosa que Alejandro no toleraba, motivo también por el cual lo perseguiría.
Los extranjeros que vivían en Persia se sintieron identificados con Alejandro y se comprometieron con él para venerarle como nuevo gobernante. En su idea de conquista también estaba la de querer globalizar su Imperio mezclando distintas razas y culturas. Los sátrapas persas en su mayoría conservaron sus puestos, aunque supervisados por un oficial macedonio que controlaba las fuerzas armadas.

En su intento de mezclar la cultura persa y la griega entrenó a un regimiento de muchachos persas para combatir a la manera macedonia. La mayoría de los historiadores creen que Alejandro adoptó el título real persa de "Shahanshah" ("Rey de Reyes").

En el 330 a. C. Filotas, hijo de Parmenión, fue acusado de conspirar contra Alejandro y asesinado junto con su padre (por temor a que este se rebelara al enterarse de la noticia). Asimismo, el primo de Alejandro, Amintas (hijo de Pérdicas III), fue ejecutado por intentar pactar con los persas para convertirse en el nuevo rey (de hecho, era el legítimo sucesor al trono macedonio). Tiempo después hubo una nueva conjura contra Alejandro, ideada por sus pajes, la cual tampoco logró su objetivo. Tras esto, Calístenes (quien hasta ese momento había sido el encargado de redactar la historia de las travesías de Alejandro) fue considerado como impulsor de este complot, por lo que fue condenado a muerte. Sin embargo, él se quitó antes la vida.Uno de sus generales más queridos del último ejército legado por su padre fue Clito, apodado «El Negro», al que Alejandro nombraría antes de este incidente sátrapa de Bactriana. Alejandro, adoptando la costumbre persa de la "proskynesis", pretendió ser adorado como un dios. En un banquete, su amigo Clito, cansado de tantas lisonjas y de oír cómo Alejandro se proclamaba mejor que su padre Filipo, le dijo indignado: «Toda la gloria que posees es gracias a tu padre»; incorporándose volvió a gritarle: «Sin mí, habrías perecido en el Gránico.»

Alejandro, que estaba ebrio, le arrojó una manzana a la cabeza, a lo que siguió una discusión en tonos líricos y cantados (aparentemente la «discusión teatral» era común en Grecia), hasta que finalmente Alejandro buscó su espada, pero uno de los guardias la ocultó. Clito fue sacado del lugar por varios amigos, pero regresó por otra puerta, y mirando fijamente al conquistador, le cantó un verso de Eurípides: «Qué perversa costumbre han introducido los griegos.» Alejandro arrebató una lanza a uno de los guardias y mató a Clito, que se desplomó en medio del estupor de los presentes. Arrepentido del crimen, pasó 3 días encerrado en su tienda y algunos afirman que hasta trató de suicidarse a consecuencia de la muerte de su amigo.

Tras muchos preparativos, y luego de establecer un nuevo orden en Babilonia, Alejandro partió en la persecución de Bessos, el asesino del rey Darío, y conquistar las satrapías persas de Asia Central. La mayoría de los sátrapas persas continuaron en sus cargos, dejando Alejandro en ellas pequeñas guarniciones de aliados griegos. Contaba con una expedición mediana de soldados griegos, llevando consigo soldados persas (entrenados al estilo de combate macedonio), que conocían bien los territorios y los dialectos de las zonas a ocupar.

Los escritos antiguos dejaron testamento que este viaje fue tan exótico como penoso. Una extensa travesía, con falta de provisiones, y fundamentalmente agua. Detalladamente se pueden encontrar en las cartas que le envió Alejandro a Aristóteles (recopiladas por Pseudo Calístenes), donde cuenta que en la expedición fueron atacados por «hombres gigantes sin inteligencia humana, que nos ocasionaron varias bajas».
En las inmediaciones del mar Caspio, tuvo un encuentro con las Amazonas (pueblo de mujeres guerreras), que lo recibieron con ofrendas, aunque inicialmente, con cautela. Según estas cartas, Alejandro recalcó «la belleza de esas mujeres y su gran fortaleza física». También narra que en un oasis en medio de la expedición, Alejandro avistó piedras preciosas en las cristalinas aguas. Luego de que unos soldados se metiesen al agua y fueran «devorados al instante por bestias acuáticas», planificó una de jaula anfibia, con tubos hechas con tripas de animales para respirar, para sumergirse él mismo y rescatarlas. Luego de todas estas exóticas experiencias, siguió la ruta trazada para perseguir a Bessos, internándose en zonas que oscilaban entre desiertos y montañas. Hasta que llegó a Sogdiana y Bactriana, donde entabló una relación de confianza con el sátrapa persa Artabazo II, cuya hija, la princesa Roxana, con quien Alejandro se casó, sería su compañía a partir de ahí en las campañas sucesivas.

Finalmente, Bessos es arrestado por sus propios cortesanos, y entregado vivo a Ptolomeo, general y amigo de Alejandro (y futuro regente de Egipto). Es ejecutado, dando supuestamente por terminada la persecución. Alejandro dio aviso inmediatamente a la familia de Darío, que su asesino estaba vengado.

Pero ocurrió algo impensado: Espitamenes, cortesano de Bessos y principal mentor de su entrega, a cambio había pedido la independencia de Sogdiana y otras satrapías. Al tener la negativa provocó importantes revueltas en las ciudades, aniquilando guarniciones griegas y generando un gran caos al imperio establecido por Magno.

Espitamenes se desenvolvía en la región de "Aria", logró aliados de tribus nómades, jinetes arqueros de estepas y desiertos, y tomó las ciudades del este asiático controladas por los griegos (atacó la capital Maracanda, y Bactriana, pero Artabazo II repelió los ataques).

Alejandro ordenó fortificar todas las ciudades y satrapías, ya ahora en pasos montañosos defendibles. Pero el factor decisivo fue "fortificar todos los oasis", dejando a Espitamenes sin recursos para sus soldados y caballería. 
En diciembre de 328a.C., el comandante macedonio Coeno lo derrotó, y cuando los sogdianos y las tribus nómadas se enteraron de que el ejército principal de Alejandro se acercaba, los masagetas asesinaron a su líder y enviaron su cabeza al conquistador.

Espitamenes tenía una hija, Apama, quien se casó con uno de los generales más importantes de Alejandro, Seleuco (febrero de 324 a. C.). La pareja tuvo un hijo, Antíoco. Tras la muerte de Alejandro, Seleuco fundó la Dinastía Seleucida (todos los territorios persas desde Media, Asia Central y este), siendo Apama reconocida como la madre de la Dinastía Seléucida. Varias ciudades fueron llamadas Apamea en su honor.

Pronto llevaría a su ejército a atravesar el Hindu Kush y a dominar el valle del Indo, con la única resistencia del rey indio Poros en el río Hidaspes.

Con sus acciones militares extendió ampliamente la influencia de la civilización griega y preparó el camino para los reinos del período helenístico.

Tras la muerte de Espitámenes y su boda con Roxana ("Roshanak" en bactriano) para consolidar sus relaciones con las nuevas satrapías de Asia Central, en el 326 a. C. Alejandro puso toda su atención en el subcontinente indio e invitó a todos los jefes tribales de la anterior satrapía de Gandhara, al norte de lo que ahora es Pakistán para que vinieran a él y se sometieran a su autoridad. Āmbhi, rey de Taxila, cuyo reino se extendía desde el Indo hasta el Hidaspes, aceptó someterse pero los rajás de algunos clanes de las montañas, incluyendo los "aspasioi" y los "assakenoi" de la tribu de los "kambojas", conocidos en los textos indios como "ashvayanas" y "ashvakayanas" (nombres que se refieren a la naturaleza ecuestre de su sociedad, de la raíz sánscrita "ashva", que significa ‘caballo’), se negaron a ello.

Alejandro tomó personalmente el mando de los portadores de escudo, los compañeros de a pie, los arqueros, los agrianos y los lanzadores de jabalina a caballo y los condujo a luchar contra la tribu de los "kamboja" de la que un historiador moderno escribe que «eran gentes valientes y le fue difícil a Alejandro aguantar sus acometidas, especialmente en Masaga y Aornos».

Alejandro se enzarzó en una feroz contienda contra los "aspasioi" en la que le hirieron en el hombro con un dardo, pero en la que los "aspasioi" perdieron la batalla y 40 000 de sus hombres cayeron prisioneros. Los "assakenoi" fueron al encuentro de Alejandro con un ejército de 30 000 soldados de caballería, 38 000 de infantería y 30 elefantes, lucharon valientemente y opusieron una tenaz resistencia al invasor en las batallas de las ciudades de Ora, Bazira y Masaga, ciudad esta última cuyo fuerte fue reducido solo tras varios días de una sangrienta lucha en la que hirieron a Alejandro de gravedad en el tobillo.

Cuando el rajá de Masaga murió durante la batalla, el comandante supremo del ejército acudió a la vieja madre de este, Cleofis, la cual también parecía dispuesta a defender su tierra hasta el final y asumió el control total del ejército, lo que empujó también a otras mujeres del lugar a luchar por lo que Alejandro solo pudo controlar Masaga recurriendo a estratagemas políticas y actos de traición. Según Quinto Curcio Rufo, «Alejandro no solo mató a toda la población de Masaga, sino que redujo sus edificios a escombros». Una matanza similar ocurrió en Ora, otro bastión de los "assakenoi".

Mientras todas estas matanzas ocurrían en Masaga y Ora, varios "assakenoi" huyeron a una alta fortaleza llamada Aornos donde Alejandro los siguió de cerca y capturó la roca tras cuatro días de sangrienta lucha. La historia de Masaga se repitió en Aornos, y la tribu de los "assakenoi" fue masacrada.

En sus escritos acerca de la campaña de Alejandro contra los "assakenoi", Victor Hanson comenta: «Después de prometer a los "assakenoi", quienes estaban rodeados, que salvarían sus vidas si capitulaban, ejecutó a todos los soldados que aceptaron rendirse. Las contiendas de Ora y Aornos se saldaron de forma similar. Probablemente todas sus guarniciones fueron aniquiladas».
Sisikottos, que había ayudado a Alejandro en esta campaña, fue nombrado gobernador de Aornos. Tras reducir Aornos, Alejandro cruzó el Indo y luchó y ganó una batalla épica contra el gobernante local Poros, que controlaba la región de Panjab, en la batalla del Hidaspes del 326 a. C.

Tras la batalla, Alejandro quedó tan impresionado por la valentía de Poros que hizo una alianza con él y le nombró sátrapa de su propio reino al que añadió incluso algunas tierras que este no poseía antes. Alejandro llamó Bucéfala a una de las dos ciudades que había fundado, en honor al caballo que le había traído a la India, y que habría muerto durante la contienda del Hidaspes. Alejandro siguió conquistando todos los afluyentes del río Indo.

Al este del reino de Poros, cerca del río Ganges, estaba el poderoso Imperio de Magadha gobernado por la dinastía Nanda. Temiendo la perspectiva de tener que enfrentarse con otro gran ejército indio y cansados por una larga campaña, el ejército macedonio se amotinó en el río Hífasis (actual río Beas), negándose a seguir hacia el Este:

Alejandro, tras reunirse con su oficial Coeno, uno de sus hombres de confianza, se convenció de que era mejor regresar. Alejandro no tuvo más remedio que dirigirse al sur. Por el camino su ejército se encontró con los malios. Los malios eran las tribus más aguerridas del sur de Asia por aquellos tiempos. El ejército de Alejandro desafió a los malios, y la batalla los condujo hasta la ciudadela malia. Durante el asalto, el propio Alejandro fue herido gravemente por una flecha malia en el pulmón. Sus soldados, creyendo que el rey estaba muerto, tomaron la ciudadela y descargaron su furia contra los malios que se habían refugiado en ella, llevando a cabo una masacre, y no perdonaron la vida a ningún hombre, mujer o niño. A pesar de ello y gracias al esfuerzo de su cirujano, Critodemo de Cos, Alejandro sobrevivió a esa herida. Después de esto, los malios supervivientes se rindieron ante las fuerzas macedónicas, y estas pudieron continuar su marcha. Alejandro envió a la mayor parte de sus efectivos a Carmania (al sur del actual Irán) bajo el mando del general Crátero, y ordenó montar una flota para explorar el golfo Pérsico bajo el mando de su almirante Nearco, mientras que él conduciría al resto del ejército de vuelta a Persia por la ruta del sur a través del desierto de Gedrosia (ahora parte del sur de Irán y de Makrán, en Pakistán). En su regreso a Babilonia, Alejandro sufre una importante pérdida: su oficial Coeno muere (326a.C.), producto de una enfermedad que había cotraído. Siendo Coeno uno de sus oficiales de infantería más destacados, Magno le rindió un funeral con todos los honores.

Alejandro dejó, no obstante, refuerzos en la India. Nombró a su oficial Peitón sátrapa del territorio del Indo, cargo que este ocuparía durante los próximos 10 años hasta el 316 a. C., y en Panyab dejó a cargo del ejército a Eudemos, junto con Poros y Āmbhi. Eudemos se convirtió en gobernador de una parte de Panyab después de que estos murieran. Él y Peitón volvieron a Occidente en el 316 a. C. con sus ejércitos. En el 321 a. C., Chandragupta Mauria fundó el Imperio mauria en la India y expulsó a los sátrapas griegos.

Tras enterarse de que muchos de sus sátrapas y delegados militares habían abusado de sus poderes en su ausencia, Alejandro ejecutó a varios de ellos como ejemplo mientras se dirigía a Susa. Como gesto de agradecimiento, Alejandro pagó las deudas de sus soldados, y anunció que enviaría a los veteranos mayores a Macedonia bajo el mando de Crátero, pero sus tropas malinterpretaron sus intenciones y se amotinaron en la ciudad de Opis, negándose a partir y criticando con amargura su adopción de las costumbres y forma de vestir de los persas, así como la introducción de oficiales y soldados persas en las unidades macedonias. Alejandro ejecutó a los cabecillas del motín, pero perdonó a las tropas. En un intento de crear una atmósfera de armonía entre sus súbditos persas y macedonios, casó en una ceremonia masiva a sus oficiales más importantes con persas y otras nobles de Susa, pero pocas de esas parejas duraron más de un año. Mientras tanto, en su regreso, Alejandro descubrió que algunos hombres habían saqueado la tumba de Ciro II el Grande, y los ejecutó sin dilación, ya que se trataba de los hombres que debían vigilar la tumba que Alejandro honraba.

Tras viajar a Ecbatana para recuperar lo que quedaba del tesoro persa, su amigo más íntimo, Hefestión, murió a causa de una enfermedad o envenenado, lo que afectó mucho a Alejandro.

El 13 de junio del 323 a. C. (10, según algunos autores), Alejandro murió en el palacio de Nabucodonosor II de Babilonia. Le faltaba poco más de un mes para cumplir los 33 años de edad. Existen varias teorías sobre la causa de su muerte, que incluyen envenenamiento por parte de los hijos de Antípatro (Casandro y Yolas, siendo este último, copero de Alejandro) u otros sospechosos; enfermedad (se sugiere que pudo ser la fiebre del Nilo), o una recaída de la malaria que contrajo en el 336 a. C. Se sabe que el 2 de junio Alejandro participó en un banquete organizado por su amigo Medio de Larisa. Tras beber copiosamente, inmediatamente antes o después de su baño, le metieron en la cama por encontrarse gravemente enfermo. Los rumores de su enfermedad circulaban entre las tropas, que se pusieron cada vez más nerviosas. El 12 de junio, los generales decidieron dejar pasar a los soldados para que vieran a su rey vivo por última vez, de uno en uno.

Plutarco hace referencia respecto a su última semana con vida, en la que "se internaba en extensos baños de inmersión para curarse" y sacrificar a los dioses (lo que sugiere la práctica de la "hidroterapia", muy común entre los griegos).

La teoría del envenenamiento deriva de la historia que sostenían en la antigüedad Justino y Curcio. Según ellos, Casandro, hijo de Antípatro, regente de Grecia, transportó el veneno a Babilonia con una mula, y el copero real de Alejandro, Yolas, hermano de Casandro y amante de Medio de Larisa, se lo administró. Muchos tenían razones de peso para deshacerse de Alejandro. Las sustancias mortales que podrían haber matado a Alejandro en una o más dosis incluyen el heléboro y la estricnina. Según el historiador Robin Lane Fox, el argumento más fuerte contra la teoría del envenenamiento es el hecho de que pasaron doce días entre el comienzo de la enfermedad y su muerte y en el mundo antiguo no había, con casi toda probabilidad, venenos que tuvieran efectos de tan larga duración.

Una de la hipótesis posibles es que sufrió una pancreatitis aguda, ya que los síntomas que sufrió, según explican los autores clásicos, encajan con los propios de esa enfermedad.

Alejandro no tenía ningún heredero legítimo. Su medio hermano Filipo Arrideo era deficiente, su hijo Alejandro nacería tras su muerte, y su otro hijo Heracles, cuya paternidad está cuestionada, era de una concubina. Debido a ello la cuestión sucesoria era de vital importancia.

En su lecho de muerte, sus generales le preguntaron a quién legaría su reino. Se debate mucho lo que Alejandro respondió: algunos creen que dijo "Krat'eroi" (‘al más fuerte’) y otros que dijo "Krater'oi" (‘a Crátero’). Esto es posible porque la pronunciación griega de ‘el más fuerte’ y ‘Crátero’ difieren solo por la posición de la sílaba acentuada. La mayoría de los historiadores creen que si Alejandro hubiera tenido la intención de elegir a uno de sus generales obviamente habría elegido a Crátero porque era el comandante de la parte más grande del ejército, la infantería, porque había demostrado ser un excelente estratega, y porque tenía las cualidades del macedonio ideal. Pero Crátero no estaba presente, y los otros pudieron haber elegido oír "Krat'eroi", ‘el más fuerte’. Fuera cual fuese su respuesta, Crátero no parecía ansiar el cargo. Entonces, el imperio se dividió entre sus sucesores (los diádocos).

Todos sus familiares y herederos, tanto su madre Olimpia, su esposa Roxana, su hijo Alejandro, su amante Barsine y su hijo Heracles, fueron mandados a asesinar por Casandro, lo que llevó a la extinción de la dinastía Argéada.

A pesar de los intentos de mantener unificado el Imperio macedónico, este acabaría por dividirse en varios reinos independientes que fundaron sus dinastías.


Algunos autores clásicos, como Diodoro, relatan que Alejandro dio detalladas instrucciones por escrito a Crátero poco antes de su muerte. Aunque Crátero ya había empezado a cumplir órdenes de Alejandro, como la construcción de una flota en Cilicia para realizar una expedición contra Cartago, los sucesores de Alejandro decidieron no llevarlas a cabo, basándose en que eran poco prácticas y extravagantes. El testamento, descrito en el libro XVIII de Diodoro, pedía expandir el imperio por el sur y el oeste del Mediterráneo, hacer construcciones monumentales y mezclar las razas occidentales y orientales. Sus puntos más interesantes fueron:


El cuerpo de Alejandro se colocó en un sarcófago antropomorfo de oro, que se puso a su vez en otro ataúd de oro y se cubrió con una capa púrpura. Pusieron este ataúd junto con su armadura en un carruaje dorado que tenía un techo abovedado soportado por peristilos jónicos. La decoración del carruaje era muy lujosa y fue descrita por Diodoro con gran detalle. Mary Renault resume sus palabras:

Según una leyenda, se conservó el cadáver de Alejandro en un recipiente de arcilla lleno de miel (que puede actuar como conservante) e introducido en un ataúd de cristal. Claudio Eliano cuenta que Ptolomeo robó el cuerpo mientras lo llevaban a Macedonia y lo trajo a Alejandría, donde se mostró hasta la Antigüedad Tardía. Ptolomeo IX, uno de los últimos sucesores de Ptolomeo I, reemplazó el sarcófago de Alejandro por uno de cristal, y fundió el oro del original para acuñar monedas y saldar deudas que surgieron durante su reinado. Los ciudadanos de Alejandría se mostraron horrorizados por esto y poco después Ptolomeo IX fue asesinado.

Luego que Roma ocupara Egipto definitivamente en el año 29a.C., la tumba de Alejandro ha sido saqueada, y el propio cuerpo de Magno flagelado por los mismos emperadores romanos. El emperador Octavio Augusto rompió la nariz de Alejandro. Luego Pompeyo el Grande robó su capa. Se dice que el emperador romano Calígula saqueó la tumba, robando la coraza de Alejandro para ponérsela. Alrededor del 200 d. C., el emperador Septimio Severo cerró la tumba de Alejandro al público. Su hijo y sucesor, Caracalla, admiraba mucho a Alejandro y visitó la tumba durante su reinado. Tras esto, los detalles sobre el destino de la tumba son confusos.

Ahora se piensa que el llamado «Sarcófago de Alejandro», descubierto cerca de Sidón y ahora situado en el Museo Arqueológico de Estambul, pertenecía en realidad a , a quien Hefestión nombró rey de Sidón por orden de Alejandro. El sarcófago muestra a Alejandro y a sus compañeros cazando y luchando contra los persas.

Al comienzo de la campaña, su ejército era de 40000 hombres. Luego ese número se incrementó hasta 50000 al recibir refuerzos de aliados griegos. Por lo que, inicialmente, su ejército estaba compuesto de 35000 soldados de infantería, y 5000 de caballería. Es un número bastante bajo en comparación de los grandes volúmenes de ejércitos que utilizaba Darío (600000) y las ciudades de los sátrapas persas.

El ejército macedonio bajo Filipo II y Alejandro Magno consistía de diferentes cuerpos que se complementaban entre sí: caballería pesada y caballería ligera; infantería pesada e infantería ligera, armas de asedio (catapultas).

Caballería pesada formaba por izquierda (aliados griegos) y por derecha con Alejandro, que la constituían los "hetairoi" o compañeros. 

A lo largo de su vida, Alejandro se casó con varias princesas de los anteriores territorios persas:

Alejandro fue padre de al menos dos niños: Heracles de Macedonia, nacido en el 327 a. C. de la princesa Barsine, hija del sátrapa Artabazo de Frigia, y Alejandro IV de Macedonia, nacido en el 323 a. C. de la princesa Roxana, seis meses después de la muerte de Alejandro.

Una de sus concubinas más célebres, fue la tesalia Campaspe, de aparentemente gran belleza, que a pedido de Alejandro ha sido retratada por Apeles (su pintor preferido), y que ha modelado en el retrato de "Venus saliendo del mar," entre otras obras. Alejandro sentía simpatía y respeto por este pintor, e incluso se sometía a sus exigencias.

Generalmente se considera que el objeto principal de los afectos de Alejandro fue su amigo, estratega de campo de batalla y comandante de caballería, Hefestión, al que probablemente se hallaba unido desde la niñez, dado que ambos se educaron en la corte de Pella. Hefestión hace su aparición en la Historia en el momento en que el conquistador alcanza Troya. Allí ambos realizaron sacrificios en los altares de los héroes de la "Ilíada", Alejandro honrando a Aquiles y Hefestión a Patroclo.

A continuación, citaremos algunos fragmentos cuestionados, enmarcados dentro de la saga de escritores denominado «vulgata», llamados así literalmente en el Anábasis de Magno (Flavio Arriano, prólogo), por la falta de rigor histórico, basado en «habladurías». Algunos de ellos son Justino, Diodoro y Curcio.

Ejemplo, la carta 24 atribuida al sofista y cínico Diógenes —de muy dudosa fiabilidad, ya que vivió en el siglo IVa.C., y esta fue escrita en el siglo I o II d.C.— expresa que amonestó a Alejandro diciendo «Si quieres ser hermoso y bueno ("kalos kai agathos"), arroja ese trapo que tienes sobre tu cabeza y ven con nosotros. Pero no serás capaz de hacerlo, dado que estás dominado por los muslos de Hefestión». Como se sabe, Diógenes despreciaba a todos por igual, y Alejandro ha sido su principal centro de ironías y burlas.

El escritor romano Curcio (siglo Id.C.) fue uno de los impulsores de introducir la idea de su ambivalencia sexual. Curcio relata que «Alejandro despreciaba los placeres sensuales a tal grado que su madre estaba ansiosa por temor de que este no le dejase descendencia». Para agudizar su apetito por las mujeres, el rey Filipo (que ya había reprochado a su hijo por cantar con voz demasiado aguda cuando Alejandro era aun pequeño) junto a su madre Olimpia, trajo a una costosa cortesana llamada Calixina, esta narración se sitúa en la época adolescente de Alejandro, etapa en la que el macedonio estaba deslumbrado por las enseñanzas de Aristóteles, cuando sus padres tenían buena relación. 

Curcio mantiene que Alejandro también tomó como amante a Bagoas, un eunuco persa que Alejandro designó como uno de sus trierarcas, hombres de capacidad administrativa y carácter que supervisaban y financiaban la construcción de barcos. Además de Bagoas, Curcio menciona otro amante de Alejandro, Euxenipo. 

Los debates sobre la identidad sexual de Alejandro son considerados anacronismos por los eruditos en ese período, quienes señalan que el concepto de homosexualidad no existía en la Antigüedad: la atracción sexual entre hombres era vista como normal y parte universal de la naturaleza humana, ya que el hombre era atraído hacia la belleza, que era un atributo de la juventud, independientemente del sexo. Si la vida amorosa de Alejandro fue transgresora lo fue no por su amor hacia jóvenes bellos, sino por su relación con hombres de su propia edad en un tiempo en el que el modelo estándar del amor masculino era el que relacionaba hombres mayores con otros mucho más jóvenes.

Principalmente en Asia, Alejandro Magno es adjetivado "Dhul-Qarnayn" (‘el de dos cuernos’), porque se hacía representar como el dios Zeus-Amón, llevando una diadema con dos cuernos de carnero (el animal que representa a Amón), y por los dos largos penachos blancos que salían de su yelmo.

La figura del rey macedonio se prestó desde la Antigüedad a todo tipo de fantasías legendarias. Así, una leyenda neogriega recogida por Nikolaos Politis presenta a Alejandro obsesionado por la inmortalidad (como Gilgamesh) y emprendiendo en vano la búsqueda del agua sagrada que podría proporcionársela.

Los zoroastristas lo recuerdan en el Arda Viraf como el «maldito Alejandro», responsable de la destrucción del Imperio persa y el incendio de su fastuosa capital, Persépolis.

Entre las culturas orientales se le conoce como Eskandar-e Maqduni (‘Alejandro de Macedonia’) en persa, "Dhul-Qarnayn" (‘el de los dos cuernos’) en las tradiciones del Medio Oriente, "Al-Iskandar al-Akbar الإسكندر الأكبر" en árabe, "Sikandar-e-azam" en urdu e hindi, "Skandar" en pashto, "Alexander Mokdon" en hebreo, y "Tre-Qarnayia" (‘el de los dos cuernos’) en arameo, debido a una imagen empleada en monedas acuñadas durante su reinado en las que aparece con los cuernos de carnero del dios egipcio Amón. Sikandar, su nombre en urdu e hindi, también se utiliza como sinónimo de ‘experto’ o ‘extremadamente hábil’.

El cráter lunar "Alexander" lleva este nombre en su honor.

Al final de la República y a principios del Imperio, los ciudadanos romanos cultos usaban el latín solo para asuntos legales, políticos y ceremoniales, empleando el griego para hablar sobre filosofía o sobre cualquier otro debate intelectual. A ningún romano le gustaba oír que su dominio de la lengua griega era pobre. En el mundo romano, la única lengua que se hablaba en todas partes era la "koiné", variante de griego que hablaba Alejandro.

Muchos romanos admiraban a Alejandro Magno y sus conquistas y querían igualar sus hazañas, aunque poco se sabe acerca de las relaciones diplomáticas que mantenían Roma y Macedonia en aquellos tiempos. Julio César lloró en Hispania con la sola presencia de una estatua de Alejandro, "lamentándose de que a su edad no había conseguido realizar tantas cosas". También Julio César honró la tumba de Alejandro Magno en su estadía en Alejandría (Egipto), siendo Cleopatra su aliada y anfitriona, y que posteriormente le daría un hijo, Cesarión (fue la última reina de la dinastía helenística de Ptolomeo). En el año 29a.C. el Egipto de Ptolomeo cae en manos de Roma definitivamente, Alejandría era el último bastión helénico en pie. Luego de la caída de Alejandría, la tumba y el cuerpo de Magno fue saqueado y arruinado poco a poco por los propios emperadores romanos. El emperador Augusto (más conocido como Octavio), luego de someter a Egipto y sus ciudades más importantes, fue a visitar su tumba en Alejandría, le preguntaron si quería ver el lugar de descanso de los faraones ptolemaicos, a lo que respondió que "Alejandro era el único líder que merecía su visita".Acto seguido, Augusto, en su empeño de honrar a Alejandro, rompió accidentalmente la nariz del cuerpo momificado mientras dejaba una guirnalda en el altar del rey. Pompeyo el Grande robó la capa de Alejandro, de 260 años de antigüedad, y se la puso como símbolo de grandeza. Calígula, el emperador desequilibrado, robó la coraza de Alejandro de su tumba para su uso personal. Los Macriani, una familia romana que ascendió al trono imperial en el siglo III d. C., llevaban siempre consigo la imagen de Alejandro, ya fuera estampada en brazaletes y anillos o cosida en sus ropas. Hasta en su vajilla estaba representada la cara de Alejandro, y la vida del rey se podía ver descrita con dibujos a lo largo de los bordes de los platos.

Se han escrito innumerables obras de Alejandro





















Como soberano
Historia de Grecia


</doc>
<doc id="6672" url="https://es.wikipedia.org/wiki?curid=6672" title="Enciclopedia Libre Universal en Español">
Enciclopedia Libre Universal en Español

La Enciclopedia Libre Universal en Español (EL) es un proyecto escindido de la Wikipedia en español. Fue creado el 26 de febrero de 2002.

El 27 de enero de 2018 la Enciclopedia Libre contaba con 50 240 artículos y 15 578 imágenes.

Durante un poco más de un año después de su creación en 2002, contó con un grupo de colaboradores y de visitas más numeroso que la Wikipedia en Español (antes de la actualización del software en Wikipedia en octubre de 2002, considerablemente superior). A lo largo de su trayectoria, EL ha ido incorporando materiales que, en el caso de la Wikipedia en español, pertenecen a proyectos paralelos, como entradas de diccionario (cubiertos en Wikipedia en español por el Wikcionario) o documentos históricos (gestionados por Wikisource).

Según algunos de sus colaboradores, al ser desconectada de la enciclopedia madre, la de lengua inglesa, la Enciclopedia Libre no tendría tendencia a ser una mera traducción de la Wikipedia en inglés, y produciría más artículos "locales". Sin embargo, los mismos colaboradores piensan que el no poder enlazar con artículos de otras enciclopedias en otras lenguas es privarse de cierta riqueza cultural . También argumentan que la existencia de varias enciclopedias colaborativas en español constituirían un enriquecimiento del enciclopedismo libre en red, ya que permitirían a quien consulta tener varios puntos de vista sobre un mismo tema.

Debido a ciertas acciones tomadas en Wikipedia, consideradas lesivas, algunos cooperadores decidieron crear una web aislando el fin lucrativo ideado en la empresa Bomis Inc. El primer desarrollo fue en la Universidad de Sevilla decidido por Juan Antonio Ruiz Rivas, uno de sus empleados.

El intercambio entre la Enciclopedia Libre y la Wikipedia en español se hace fundamentalmente mediante la copia integral de artículos de una a otra (también existen wikipedistas que colaboran en EL, produciendo los mismos contenidos para ambas enciclopedias) . Aunque en principio pudiera pensarse que este intercambio haría que ambas se parecieran cada día más, de hecho tanto los contenidos como, fundamentalmente, la estructura de la información van divergiendo .

El Taller es un proyecto hermano de la Enciclopedia Libre, y su principal diferencia con otros proyectos wiki es que se orienta a la iniciativa y creatividad individual. Se destina a albergar contenidos no enciclopédicos tales como imágenes, poemas, cuentos, entre otras cosas que son creaciones enteramente de cada usuario (a diferencia de Commons).



</doc>
<doc id="6674" url="https://es.wikipedia.org/wiki?curid=6674" title="Melanoma">
Melanoma

Melanoma es el nombre genérico de los tumores melánicos o pigmentados ("mélas" (μελας gr.) "negro" + -o-ma 1 (-ομα gr.) "tumor") y el melanoma maligno es una grave variedad de cáncer de piel, causante de la mayoría de las muertes relacionadas con el cáncer de piel. Se trata de un tumor generalmente cutáneo, pero también del intestino y el ojo (melanoma uveal) y altamente invasivo por su capacidad de generar metástasis. Actualmente el único tratamiento efectivo es la resección quirúrgica del tumor primario antes de que logre un grosor mayor de 1 mm. 

Cerca de 160.000 casos nuevos de melanoma se diagnostican cada año mundialmente, y resulta más frecuente en hombres y personas de raza blanca que habitan regiones con climas soleados. Según un informe de la Organización Mundial de la Salud, ocurren cada año cerca de 48.000 muertes relacionadas con el melanoma. Se estima que el melanoma maligno produce un 75% de las muertes asociadas al cáncer de piel.

Por lo general, el riesgo de un individuo de contraer un melanoma depende de dos tipos de factores: intrínsecos y ambientales. Los factores intrínsecos incluyen la historia familiar y el genotipo heredado; mientras que el factor ambiental o extrínseco más relevante es la exposición a la luz solar.

Los estudios epidemiológicos sugieren que la exposición a la radiación proveniente de la luz ultravioleta (UVA y UVB) es una de las causas principales en la aparición del melanoma. 

El melanoma es más frecuente en la espalda de los hombres y en las piernas de las mujeres. El riesgo parece estar fuertemente influido por las condiciones socioeconómicas de la persona, no tanto por el hecho de que su ocupación se desarrolle en el interior o en el exterior de un edificio. De modo que es más común ver melanomas en profesionales y personal administrativo que en trabajadores o graduados.
El uso de camas de bronceado con rayos ultravioleta penetrantes se ha asociado con la aparición del cáncer de piel, incluyendo el melanoma.

La radiación causa daño en el ADN de las células, típicamente una dimerización de la timina que, al no ser reparado por la maquinaria intracelular, crea mutación en los genes celulares. La secuenciación masiva del genoma de muestras de melanomas metastásicos de pacientes ha permitido detectar distintas mutaciones, no solo mutaciones puntuales (transiciones C->T principalmente), sino también reordenamientos cromosómicos (deleciones, amplificaciones, translocaciones), incluyendo el fenómeno de la cromotripsis, que provocan una alta inestabilidad genómica. Cuando la célula se divide, estas mutaciones se propagan a nuevas generaciones de células. Si la mutación ocurre justo sobre un protooncogén (dará lugar a un oncogén) o si se produce en genes supresores tumorales, la velocidad de la mitosis o división celular en las células se vuelve descontrolada con las mutaciones, conllevando a la formación de un tumor. La mayoría de los estudios sobre quemaduras sugieren una relación positiva o directa entre las quemaduras a edades tempranas y el consiguiente riesgo de padecer melanoma. Los pacientes que presentan un historial de alta exposición a la luz ultravioleta, suelen tener un porcentaje de mutaciones en genes como NRAS o BRAF (oncogenes) superior al que poseen los pacientes con una exposición normal o baja.

Según su localización se denominan:

Tiene formas compuestas:

Para prevenirse del melanoma, ante la llegada del verano, es preciso adoptar una serie de medidas de protección, como la utilización de gorras o sombreros que no dejen pasar los rayos solares, de cremas con factor protección solar mayor de 50, así como tomar el sol de una forma gradual y evitarlo en las horas de irradiación más intensa (entre las 12:00 y 16:00). Incluso debajo de las sombrillas el sol es dañino, ya que el efecto espejo de la arena puede inducir los rayos solares con mayor intensidad.

El prototipo humano con mayores posibilidades de contraer dicha enfermedad es una mujer entre 40 y 45 años, de piel y ojos claros que realice exposiciones solares intensas e intermitentes desde la infancia, con quemaduras en la etapa infantil, con un número importante de nevus congénitos o atípicos, y con antecedentes familiares de melanoma.

Algunos consejos para prevenir la aparición de melanomas son:

Para saber cuándo la apariencia es sospechosa existe una regla denominada A, B, C y D. Así, cuando un nevus es Asimétrico, tiene unos Bordes irregulares, toma una Coloración muy oscura o irregular y su Diámetro aumenta, son indicios de melanoma, por lo que se debe acudir al médico.

Se debe prestar especial atención, por ser marcadores de melanoma, a los nevus pigmentocelulares adquiridos que, a lo largo de la vida, modifican su morfología. También a los nevus atípicos o (urielosis) y congénitos, siendo los principales signos de alarma los nevus asimétricos, con bordes imprecisos, color cambiante y sangrado. Este cuadro puede darse conjuntamente.

Los melanomas se clasifican con fines pronósticos según sus características. Hay varios sistemas:

Una detección precoz permite la extirpación quirúrgica de la práctica totalidad de los melanomas. Actualmente se utilizan técnicas de diagnosis no cruentas tales como la dermatoscopia (también denominada epiluminiscencia) que permiten detectar cualquier alteración precoz de los nevus y su posible malignidad. Tras la cirugía solo los pacientes de alto riesgo necesitan inmunoterapia adicional. Si en un período de 3 a 5 años no se ha reproducido el melanoma, las posibilidades de recaída son mínimas.

En 2012 se aprobó el uso del vemurafenib, comercializado como Zelboraf, para tratar un tipo de melanoma asociado a una mutación del gen BRAF, una de las formas más malignas de esta enfermedad. Además, y a raíz de los resultados del último estudio denominado coBRIM, en el que se ha combinado Vemurafenib con un nuevo medicamento en fase de ensayo (Cobimetinib), se ha confirmado que el uso conjunto de ambas medicinas permite bloquear diferentes dianas de la célula tumoral impidiendo durante más tiempo la progresión del tumor.




</doc>
<doc id="6678" url="https://es.wikipedia.org/wiki?curid=6678" title="Código de barras">
Código de barras

El código de barras es un código basado en la representación de un conjunto de líneas paralelas de distinto grosor y espaciado que en su conjunto contienen una determinada información, es decir, las barras y espacios del código representan pequeñas cadenas de caracteres. De este modo, el código de barras permite reconocer rápidamente un artículo de forma única, global y no ambigua en un punto de la cadena logística y así poder realizar inventario o consultar sus características asociadas.

La correspondencia o mapeo entre la información y el código que la representa se denomina "simbología". Estas simbologías pueden ser clasificadas en grupos atendiendo a dos criterios diferentes:

La primera patente de código de barras fue registrada en octubre de 1952 (US Patent #2,612,994) por los inventores Joseph Woodland, Jordin Johanson y Bernard Silver en Estados Unidos. La implementación fue posible gracias al trabajo de los ingenieros Raymond Alexander y Frank Stietz. El resultado de su trabajo fue un método para identificar los vagones del ferrocarril utilizando un sistema automático. Sin embargo, no fue hasta 1966 cuando el código de barras comenzó a utilizarse comercialmente y recién tuvo éxito comercial en 1980.


Ejemplo de datos contenidos en un código de barras GTIN 13:


Los códigos de barras se imprimen en los envases, embalajes o etiquetas de los productos. Entre sus requisitos básicos se encuentran la visibilidad y fácil legibilidad por lo que es imprescindible un adecuado contraste de colores. En este sentido, el negro sobre fondo blanco es el más habitual encontrando también azul sobre blanco o negro sobre marrón en las cajas de cartón ondulado. El código de barras lo imprimen los fabricantes (o, más habitualmente, los fabricantes de envases y etiquetas por encargo de los primeros) y, en algunas ocasiones, los distribuidores.

Para no entorpecer la imagen del producto y sus mensajes promocionales, se recomienda imprimir el código de barras en lugares discretos tales como los laterales o la parte trasera del envase. Sin embargo, en casos de productos pequeños que se distribuye individualmente no se puede evitar que ocupe buena parte de su superficie: rotuladores, barras de pegamento, entre otros.

Los códigos de barras se dividen en dos grandes grupos: los códigos de barras lineales y los códigos de barras de dos dimensiones.


Es un código multifilas, continuo, de longitud variable, que tiene alta capacidad de almacenamiento de datos. El código consiste en un patrón de marcas (17,4), los subjuegos están definidos en términos de valores particulares de una función discriminadora, cada subjuego incluye 929 "codewords" (925 para datos, 1 para los descriptores de longitud y por lo menos 2 para la corrección de error) disponibles y tiene un método de dos pasos para decodificar los datos escaneados. Es un archivo portátil de datos ("Portable Data File"), tiene una capacidad de hasta 1800 caracteres numéricos, alfanuméricos y especiales. El código contiene toda la información, no se requiere consultar a un archivo. Cuenta con mecanismos de detección y corrección de errores: 9 niveles de seguridad lo que permite la lectura y decodificación exitosa aun cuando el daño del código llegue hasta un 40%.

Se usa en:

Cada símbolo tiene regiones de datos, que contienen un juego de módulos cuadrados nominales en un arreglo regular. En grandes símbolos ECC 200, las regiones de datos están separadas por patrones de alineamiento. Puede codificar hasta 2335 caracteres en una superficie muy pequeña. Desarrollado en 1989 por International Data Matrix Inc. La versión de dominio público es la ECC 200. Si bien existen y se han publicado desde su creación la ECC 000,0010,0040,0050,0060,0070,0080,0090,0100,0120,0130,0140, la última Ecc 200 también de dominio público y libre implementación, fue desarrollada por International Data Matrix en 1995.


Es un código bidimensional con una matriz de propósito general diseñada para un escaneo rápido de información. QR es eficiente para codificar caracteres Kanji (fue diseñado por la compañía Denso Wave y lo desarrolló en Japón), es una simbología muy popular en Japón. El código QR es de forma cuadrada y puede ser fácilmente identificado por su patrón de cuadros oscuros y claros en tres de las esquinas del símbolo.

Existe la posibilidad de que los particulares y los comercios y hostelería pongan el código QR de los locales y establecimientos.

Entre todas las primeras justificaciones de la implantación del código de barras se encontraron la necesidad de agilizar la lectura de los artículos en las cajas y la de evitar errores de digitación. Otras ventajas que se pueden destacar de este sistema son:

La información se procesa y almacena con base en un sistema digital binario donde todo se resume a sucesiones de unos y ceros. La memoria y central de decisiones lógicas es un computador electrónico del tipo universal, disponible ya en muchas empresas comerciales y generalmente compatible con las distintas marcas y modelos de preferencia en cada país. Estos equipos permiten también interconectar entre sí distintas sucursales o distribuidores centralizando toda la información.
Ahora el distribuidor puede conocer mejor los parámetros dinámicos de sus circuitos comerciales, permitiéndole mejorar el rendimiento y las tomas de decisiones, ya que conocerá con exactitud y al instante toda la información proveniente de las bocas de venta estén o no en su casa central. Conoce los tiempos de permanencia de depósito de cada producto y los días y horas en que los consumidores realizan sus rutinas de compras, pudiendo entonces decidir en qué momento debe presentar ofertas, de qué productos y a qué precios.

Entre las pocas desventajas que se le atribuyen se encuentra la imposibilidad de recordar el precio del producto una vez apartado del lineal. También hay que aclarar que el código QR no es un código de barras propiamente, sus métodos de lectura se diferencian y claramente los QR no son barras. Se incluyen aquí por ser utilizados para el mismo fin que los códigos de barras.

Para facilitar la lectura del código de barras, se aplica un contraste alto entre los componentes oscuros y claros del código.







</doc>
<doc id="6682" url="https://es.wikipedia.org/wiki?curid=6682" title="IEEE 1394">
IEEE 1394

IEEE 1394 es un tipo de conexión para diversas plataformas, destinado a la entrada y salida de datos en serie a gran velocidad. Suele utilizarse para la interconexión de dispositivos digitales como cámaras digitales y videocámaras a computadoras. Existen cuatro versiones de 4, 6, 9 y 12 pines. En el mercado doméstico su popularidad ha disminuido entre los fabricantes de hardware, y se ha sustituido por la interfaz USB en sus versiones 2.0 y 3.0, o la interfaz Thunderbolt, aunque es ampliamente utilizado en automatización industrial, industria militar y para el entorno profesional.


Publicado en 2000. Duplica aproximadamente la velocidad del FireWire 400, hasta 786,5 Mbit/s con tecnología semi-duplex, cubriendo distancias de hasta 100 metros por cable.
Firewire 800 reduce los retrasos en la negociación, utilizando para ello 8b/10b (código que codifica 8 bits en 10 bits, que fue desarrollado por IBM y permite suficientes transiciones de reloj, la codificación de señales de control y detección de errores. El código 8b/10b es similar a 4B/5B de FDDI (que no fue adoptado debido al pobre equilibrio de corriente continua), que reduce la distorsión de señal y aumenta la velocidad de transferencia. Así, para usos que requieran la transferencia de grandes volúmenes de información, resulta muy superior al USB 2.0. Posee compatibilidad retroactiva con Firewire 400 utilizando cables híbridos que permiten la conexión en los conectores de Firewire400 de 6 pines y los conectores de Firewire800, dotados de 9 pines. En el 2003 lanzó Apple el primer dispositivo de uso comercial de Firewire800.
Anunciados en diciembre de 2007, permiten un ancho de banda de 1,6 y 3,2 Gbit/s, cuadruplicando la velocidad del Firewire 800, a la vez que utilizan el mismo conector de 9 pines. Es ideal para su utilización en aplicaciones multimedia y almacenamiento, como videocámaras, discos duros y dispositivos ópticos. Su popularidad es escasa.

Anunciado en junio de 2007. Aporta mejoras técnicas que permiten el uso de la interfaz con puertos RJ-45 sobre cable CAT 5, combinando así las ventajas de Ethernet con Firewire800.






La edición de vídeo digital con IEEE 1394 ha permitido la incorporación de FireWire en cámaras de vídeo de bajo costo y elevada calidad lo cual permite la creación de vídeo profesional en las plataformas Macintosh y PC. La interfaz IEEE 1394 permite la captura de vídeo directamente de cámaras de vídeo digital con puertos FireWire incorporados y de sistemas analógicos mediante conversores de audio y vídeo.



</doc>
<doc id="6686" url="https://es.wikipedia.org/wiki?curid=6686" title="European Article Number">
European Article Number

European Article Number (EAN) o International Article Number (IAN) es un sistema de códigos de barras adoptado por más de 100 países y cerca de un millón de empresas (2003). En el año 2005, la asociación EAN se ha fusionado con la UCC ("Uniform Code Council") para formar una nueva y única organización mundial identificada como GS1, con sede en Bélgica.

El código EAN más usual es EAN13, constituido por 13 dígitos y con una estructura dividida en cuatro partes: 


Por ejemplo, para 123456789041 el dígito de control será:


El código quedará así: 1234567890418.




</doc>
<doc id="6688" url="https://es.wikipedia.org/wiki?curid=6688" title="Cifra (matemática)">
Cifra (matemática)

Una cifra es un símbolo o carácter gráfico que sirve para representar un número. Por ejemplo, los caracteres '0', '1', '2', '3', '4', '5', '6', '7', '8' y '9' son cifras del sistema de numeración arábigo, mientras que los caracteres ", ", ", ", ", " y " son cifras del sistema de numeración romano.

Las cifras se usan también como identificadores en: números de teléfono, numeración de carreteras; como indicadores de orden en: números de serie; como códigos (ISBN), etc.

Un numeral es una cadena de cifras utilizada para denotar un número (no un código identificativo). A modo de ejemplo, los numerales "21", "2", "3", "4" y "500" representan en el sistema arábigo los mismos números que los respectivos numerales "", "", "", "" y "" en el sistema romano.

Un número dígito es un número que puede expresarse empleando un numeral de una sola cifra. Por extensión se puede decir que un dígito es cada símbolo o guarismo de los usados para expresar un numeral o un número.

En el sistema decimal son: 0, 1, 2, 3, 4, 5, 6, 7, 8 y 9. Así, 157 se compone de los dígitos 1, 5 y 7. El nombre dígito proviene del latín dígitus dedo, porque los 10 dedos corresponden a los 10 dígitos en el sistema numérico común en base 10, esto es, un dígito decimal.

En matemáticas y ciencia de la computación, un dígito numérico es un símbolo, v.gr. '3', que usado en combinaciones, v.gr. "37", representa números (enteros o reales) en sistemas de numeración posicionales. 

Por tradición, al menos desde la época del Antiguo Egipto, se usa el sistema decimal, debido al arcaico uso de los diez dedos para ayudarse a contar, aunque no hay ninguna razón especial para que un sistema de numeración deba utilizar la base diez.

En el sistema decimal se necesitan 10 dígitos, aunque tienen diferente valor en función de su posición en el numeral, pues su valor varía de diez en diez, esto es unidades, decenas (10), centenas (10), millares (10), y así sucesivamente, de modo que un dígito a la izquierda tiene diez veces el valor de la posición dada y a la derecha la décima parte del valor de la misma. Para separar valores menores a la unidad se usa el punto decimal (en Europa la coma). Este método de notación posicional, proviene de la India y fue transmitido a Occidente por los matemáticos musulmanes durante la Edad Media.

El más simple es el sistema binario, que sólo precisa dos dígitos, generalmente representados por 0 y 1; en el sistema binario varían dos en dos: unidades, parejas (2), cuartetas (2), y así sucesivamente. Es un sistema profusamente empleado en informática. 

Ejemplos de dígitos incluyen cualquiera de los caracteres decimales desde "0" hasta "9", o de los caracteres del sistema binario "0" o "1", y los dígitos "0"..."9", "A"...,"F" usados en el sistema hexadecimal. En un sistema de numeración dado, si la base (radical, en inglés ) es un entero, el número de dígitos necesarios, para la parte entera, es igual al siguiente entero del logaritmo del número a representar dividido entre el logaritmo de la base. Para la parte fraccionaria el número de dígitos dependerá de la precisión necesaria a manejar.

En los sistemas de numeración, los dígitos se combinan para representar distintos números. Si el valor viene determinado por la posición del dígito, se habla de notación posicional. Si los dígitos tienen un valor fijo, que no depende de su posición, se habla de notación aditiva, como, por ejemplo, la numeración romana. 

Cuando los árabes del siglo X adoptaron la numeración de la India, tradujeron la palabra «"sunya"», que significaba ‘vacío’ o ‘en blanco’, por «"sifr"», ‘vacío’ en árabe. Después, el sistema de numeración indo-arábigo fue introducido en Italia y la palabra «"sifr"» se latinizó como «"zephirum"». El proceso comenzó a principios del siglo XIII y con el correr del tiempo una sucesión de cambios culminó con la palabra italiana «"zero"».

Casi paralelamente se desarrolló un proceso similar en Alemania. Jordanus Nemorarius cambió la palabra «"sifr"» por «cifra». Durante un tiempo en Europa ambas palabras denotaban el cero. Como uno de los testimonios de esta etapa, la palabra inglesa «"cipher"» tiene actualmente dos significados: ‘cifra’, en el sentido moderno, y ‘cero’ en su forma arcaica, de acuerdo a su etimología.

Las palabras «cifra», «"chiffre"», «"cipher"», «"ziffer"» y «"zero"» representaban el cero para los doctos.

La historia no contempla los títulos y honores de los doctos. Los procesos sociales cambian irremediablemente algunos de los conceptos originales. Cuando la masa adopta un uso, es inútil todo esfuerzo en sentido contrario.

En la Edad Antigua y en la Edad Media los cálculos eran realizados por expertos. Hasta la adopción definitiva del sistema de posición y el cero, la multiplicación y la división se realizaban por duplicaciones y mediaciones, respectivamente. Por ejemplo, para multiplicar un número por 13 se descomponía al multiplicador en potencias de 2, en este caso, 8 + 4 + 1. El multiplicando se duplicaba dos y tres veces. Luego se sumaban la triple duplicación, la doble duplicación y la cantidad original. La división seguía un proceso análogo pero inverso. Los cálculos demandaban mucho tiempo de trabajo y el costo era elevado. Puede observarse un residuo de esto en la forma en que se subdividen las medidas antiguas, como la pulgada inglesa: medios, cuartos, octavos, dieciseisavos, treintaidosavos.

Los comerciantes de aquellos tiempos debían solventar esos gastos para tener control e información de sus negocios. Cuando llegó a ellos la noticia del nuevo sistema de numeración, vieron muy prontamente la ventaja que les daría. Los cálculos eran fáciles de realizar y ya no hacía falta una formación superior para dominar las operaciones aritméticas. No tendrían que pagar por el servicio de un experto.

Es realmente notable que estas personas se dieran cuenta del papel fundamental del cero en el nuevo sistema. La masa identificó todo el sistema con su rasgo más característico, la cifra, usando, entonces, cifra con el sentido de signo numérico que tiene hoy en nuestra civilización. Este uso era totalmente opuesto al significado de la cifra de los doctos.

Los comerciantes consideraron que era prudente reservar ese uso para ellos, como una ventaja. El sistema se utilizó en secreto. De esta forma, la palabra «cifra» era usada como un signo secreto. De esa etapa sobreviven las palabras «descifrar» y «cifrado». Un código cifrado es un texto de significado inaccesible si no se dispone de la clave. Cuando se obtiene la clave el secreto queda revelado, el código secreto se descifra, «se le quita el cero» o el secreto.

Por motivos egoístas los comerciantes guardaron para sí el sistema. Por otro lado, hubo una reacción de parte de los partidarios de las tradiciones y defensores de antiguas filosofías, a la que se sumaron quienes vivían de los cálculos difíciles de antaño. Por estas razones, el sistema tardó mucho en imponerse. La lucha duró desde el siglo XI hasta el siglo XV. En algunos lugares hasta fue prohibido. Pero hacia principios del siglo XVI ya estaba decididamente establecido y no sufrió ningún retraso en su desarrollo.

Los partidarios del sistema de posición se denominaban «algoristas» y los defensores del viejo sistema, «abacistas», porque en sus cálculos utilizaban el ábaco. En esos tiempos también «"abaci"» era sinónimo de aritmética.

Una vez que quedó completamente adoptado el nuevo sistema, el uso de la palabra «cifra» en el sentido de un signo numérico estaba tan fuertemente arraigado que fue inútil el esfuerzo de los doctos por volver al significado original de ‘cero’. No tuvieron más remedio que dejar «cifra» con ese sentido y tomar «"zero"» para designar al espacio vacío hasta llegar al uso que tiene ahora.

En astronomía un dígito astronómico es cada una de las partes iguales en que se divide el diámetro de los discos lunar y solar para expresar la importancia de un eclipse. Así, un eclipse de Luna de 8 "dígitos" afecta a los dos tercios del diámetro de nuestro planeta (ver magnitud de un eclipse).



</doc>
<doc id="6692" url="https://es.wikipedia.org/wiki?curid=6692" title="Cantidad">
Cantidad

Una cantidad es la asignación, usualmente numérica, de una magnitud matemática a una propiedad medible que admite grados de comparación y representa o bien un contaje del número de elementos de un conjunto, o bien el resultado de una medición física de una magnitud. Así las cantidades pueden ser comparadas en términos de "más", "menos" o "igual" (o no ser comparables), y generalmente pueden ser representadas por diferentes sistemas de unidades (la masa se puede medir en kilogramos o en onzas).

Un gran número de propiedades pueden ser representadas por cantidades escalares, aunque algunas magnitudes físicas requieren el uso de cantidades vectoriales más complejas. Una cantidad escalar es el valor numérico que resulta de una medición (de una magnitud) que se expresa con números acompañado por unidades, de la forma siguiente:


Por ejemplo: 20 kg, 1 m, 60 s, son resultado de medir las magnitudes masa, longitud y tiempo. Igualmente ciertas magnitudes físicas como la cantidad de movimiento, o la velocidad requieren ser representadas por objetos matemáticos como vectores que no son simplemente valores numéricos.

Tradicionalmente , se han dividido las cantidades en dos grupos las referidas al contaje (magnitudes discretas) y las referidas a la comparación con una escala continua (magnitudes continuas). Las cantidades son representaciones formales de propiedades físicas que deben satisfacer algunas condiciones generales como:

La escalabilidad frecuentemente es una consecuencia de la aditividad: dada una magnitud extensiva y una propiedad medible, al dividir un sistema en dos subsistemas se obtienen dos cantidades asociadas a la propiedad medible, de tal manera que su suma es la cantidad asociada al sistema original sin dividir.

Las cantidades en general admiten siempre escalabilidad: dada una magnitud siempre es posible imaginar una cantidad mayor que ella. Esto diferencia a los resultados de una medición o contaje de otros índices numéricos como los porcentajes o las probabilidades (que por definición deben ser inferiores al 100%).

Las magnitudes de tipo contaje y las magnitudes continuas escalares tiene estructura de conjunto totalmente ordenado, así dadas dos longitudes siempre será posible decir cual de las dos longitudes es mayor que la otra. Otras magnitudes como la velocidad vectorial admiten sólo orden parcial, por ejemplo está claro que una velocidad de formula_1 representa una velocidad menor que una velocidad formula_2. Pero si ahora se considera la velocidad 
formula_3 no puede decirse si esta velocidad es mayor o menor que formula_4 (lo único que puede decirse es que son velocidades vectoriales diferentes). Esto sucede porque en un espacio vectorial no puede definirse una relación de orden total compatible con la suma.

Todas las magnitudes parecen escalables, aun así puede establecerse una diferencia sobre la posibilidad de construir o no una escala continua entre dos valores cualesquiera asociados a una misma propiedad medible. Las cantidades de tipo contaje son representadas por un número natural positivo y representa el número de elementos en un cierto conjunto. Por el contrario las magnitudes de tipo continuo, se refieren a la comparación de propiedades físicas con patrones y entre dos resultados R < R siempre puede encontrarse una medición R3 tal que R < R < R.

Usualmente las cantidades discretas se represetan por valores numéricos que son números enteros de formula_5 o formula_6. Mientras que las cantidades continuas se representan por números no necesariamente enteros de formula_7 o formula_8. En la práctica una medida directa con una precisión finita siempre será un contaje, que convenientemente reescalado dará como resultado un número racional. Sin embargo, para medidas indirectas calculadas a partir de mediciones directas frecuentemente se lleva a cabo una operación matemática sobre las medidas directas, y entonces el resultado no siempre es un número racional, por esa razón resulta conveniente usar representaciones sobre los números reales formula_8. Pero en ciertos casos, como en electrotecnia puede sacarse provecho de representar algunas magnitudes no como números reales, sino como números complejos formula_10. E incluso operacionalmente pueden considerarse conjuntos numéricos más complejos, en algunos ámbitos.

Las cantiades asociadas a un simple contaje de elementos frecuentemente no usan un tipo de unidades (aunque a veces se pueden usar unidades para clarificar su valor). En cambio las cantidades continuas obtenidas inicialmente comparando experimentalemente con una escala necesitan especificar la escala. Una escala definirá siempre implícitamente un sistema de unidades, de ahí que las magnitudes continuas para poder ser comparadas o interpretadas requieran especificar el valor de la unidad asociada a la escala de medición/comparación.

Las cantidades continuas posee una estructura particular que fue caracterizada explícitamente por primera vez por O. Hölder (1901) mediante un conjunto de axiomas que definen las características como "identidades" y "relaciones" entre magnitudes.

En ciencias naturales, la estructura cuantitativa está sujeta a las condiciones de la investigación empírica y no puede asumirse que existan a priori para una cierta propiedad. Algunas características fundamentales señaladas por Hölder para las cantidades asociadas a propiedades medibles son:

La unidad en cantidades continuas, es la cantidad que sirve de comparación, y en cantidades discretas es cada uno de los objetos que se cuentan. La unidad puede ser de dos tipos: libre o necesaria.


</doc>
<doc id="6693" url="https://es.wikipedia.org/wiki?curid=6693" title="Cero">
Cero

El cero (0) es el signo numérico de valor nulo, que en notación posicional ocupa los lugares donde no hay una cifra significativa. Si está situado a la derecha de un número entero se multiplica por 10 su valor; colocado a la izquierda, no lo modifica.

Utilizándolo como número, se pueden realizar con él operaciones algebraicas como: sumas, restas, multiplicaciones, entre otras. Pero, por ser la expresión del valor nulo (nada, nadie, ninguno...), puede dar lugar a expresiones indeterminadas o que carecen de sentido.

Es el elemento del conjunto ordenado de los números enteros (, ≤) que sigue al –1 y precede al 1. Algunos matemáticos lo consideran perteneciente al conjunto de los naturales () ya que estos también se pueden definir como el conjunto que nos permite contar el número de elementos que contienen los demás conjuntos, y el conjunto vacío tiene ningún elemento. El número cero se puede representar como cualquier número más su opuesto (o, equivalentemente, menos él mismo): () 0.

Antiguas y grandes civilizaciones —como las del Antiguo Egipto, Babilonia, la Antigua Grecia y la civilización maya— poseen documentos de carácter matemático o astronómico mostrando símbolos indicativos del valor cero; pero por diversas peculiaridades de sus sistemas numéricos, no supieron obtener el verdadero beneficio de este capital descubrimiento.

El cero apareció por primera vez en Babilonia en el siglo III a. C., aunque su escritura en tablillas de arcilla se remonta al 2000 a. C. Los babilonios escribían en arcilla sin cocer, sobre superficies planas o tablillas. Su notación era cuneiforme. En tablillas datadas en el año 1700 a. C. se ven anotaciones numéricas en su particular forma. Los babilonios utilizaban un sistema de base 60. Con su sistema de notación no era posible distinguir el número 23 del 203 o el 2003, aunque esta ambigüedad no pareció preocuparles.

Alrededor del 400 a. C., los babilonios comenzaron a colocar el signo de «dos cuñas» en los lugares donde en nuestro sistema escribiríamos un cero, que se leía «varios». Las dos cuñas no fueron la única forma de mostrar las posiciones del cero; en una tablilla datada en el 700 a. C. encontrada en Kish, antigua ciudad de Mesopotamia al este de Babilonia, utilizaron un signo de «tres ganchos». En otras tablillas usaron un solo «gancho» y, en algunos casos, la deformación de este se asemeja a la forma del cero.
El cero también surgió en Mesoamérica e ideado por las civilizaciones mesoamericanas antes de la era cristiana, por la cultura maya. Posiblemente fue utilizado antes por la cultura olmeca.

El primer uso documentado mostrando el número cero corresponde al año 36 a. C., haciendo uso de la numeración Maya. A causa de la anomalía introducida en el tercer lugar de su notación posicional, les privó de posibilidades operativas.

Claudio Ptolomeo en el "Almagesto", escrito en , usaba el valor de «vacío» o «0». Ptolomeo solía utilizar el símbolo entre dígitos o al final del número. Podría pensarse que el cero habría arraigado entonces, pero lo cierto es que Ptolomeo no usaba el símbolo como «número» sino que lo consideraba un signo de anotación. Este uso no se difundió, pues muy pocos lo adoptaron.

Los romanos no utilizaron el cero. Sus números eran letras de su alfabeto; para representar cifras usaban: I, V, X, L, C, D, M, agrupándolas. Para números con valores iguales o superiores a 4000, dibujaban una línea horizontal sobre el «número», para indicar que el valor se multiplicaba por 1000.

La civilización india es la cuna de la notación posicional, de uso casi universal en el siglo XXI. La palabra «cero» proviene de la traducción de su nombre en sánscrito "shunya" (vacío) al árabe "sifr" (صفر), a través del italiano. La voz española «cifra» también tiene su origen en "sifr".

Es posible que el matemático indio Brahmagupta (siglo VI) fuera el primero en teorizar sobre el concepto de "cero" no sólo como definición de una cantidad nula, sino como posible sumando para números negativos y positivos. El primer testimonio del uso del «cero indio» está datado en el año 683: una inscripción camboyana de Angkor Wat, tallada en piedra, que incluye el número "605". Otras pruebas de uso se datan hacia el año 810. Las inscripciones de Gwalior están datados en 875-876. Abu Ja'far Mujammad ibn Musa (Al-Juarismi), en su obra titulada «Tratado de la adición y la sustracción mediante el cálculo de los indios» explica el principio de numeración posicional decimal, señalando el origen indio de las cifras. La décima figura, que tiene forma redondeada, es el «cero».

Los árabes lo transmitieron por el Magreb y Al-Ándalus, pasando posteriormente al resto de Europa. Los primeros manuscritos que muestran las cifras indias (llamadas entonces «árabes») provienen del norte de España y son del siglo X: el "Codex Vigilanus" y el "Codex Aemilianensis". El cero no figura en los textos, pues los cálculos se realizaban con ábaco, y su uso aparentemente no era necesario.

Aunque se atribuyen los primeros usos del "cero" en Francia, o al controvertido papa Silvestre II, alrededor del año 1000, la mayor parte de las referencias indican que el cero (llamado "zefhirum") fue introducido en Europa por el matemático italiano Fibonacci en el siglo XII, mostrando el álgebra árabe en su "Liber abaci" ("El libro del ábaco"), aunque por la facilidad del nuevo sistema, las autoridades eclesiásticas lo tildaron de mágico o demoniaco.

La iglesia y la casta de los calculadores profesionales —clérigos en su mayoría, que utilizaban el ábaco— se opusieron frontalmente, vetando la nueva álgebra, en algunos lugares hasta el siglo XV.

El cero se representa en textos occidentales con la cifra «0». Desde el siglo XX, y especialmente con el desarrollo de la informática, es frecuente que este signo aparezca cortado por una barra diagonal (/), nueva notación que evitaba la confusión con la grafía de la letra «o». Hasta hace poco, la conjunción disyuntiva "o" debía llevar tilde: «ó», cuando iba escrita entre cifras para no ser confundida con el signo numérico 0. Actualmente, dicha regla no está en vigor.

En coordenadas cartesianas el origen de coordenadas se asocia al valor 0 (cero).

El cero, por ser un concepto numérico especial, no se incluía en el conjunto de los números naturales , por convenio. Y se representaba como , al conjunto de los números naturales cuando incluye al cero, por ello es posible encontrar muchos libros donde los autores no consideran al cero como número natural. De hecho, aún no hay consenso al respecto.

A algunos matemáticos les resulta conveniente tratarlo como a los otros números naturales, por eso la discrepancia. Desde un punto de vista histórico el cero aparece tan tarde que algunos no creen que sea justo llamarlo natural.

En la suma, el cero es el elemento neutro; es decir, cualquier número sumado con vuelve a dar .
Ejemplo:

En la resta, el cero es el elemento neutro; es decir, cualquier número restado con vuelve a dar , excepto cuando el cero es el minuendo, en cuyo caso resulta .
Ejemplos:

En el producto, el cero es el elemento absorbente; cualquier número operado con da .
Ejemplo:

El cero puede ser dividido por otros números, en cuyo caso es el elemento absorbente (ejemplo: ). El cero no puede dividir a ningún número.

En los números reales (incluso en los complejos) la división entre cero es una indeterminación; así, las expresiones:

carecen de sentido.

Intuitivamente, significa que no tiene 'sentido' «repartir» 8 manzanas entre niños de un aula vacía. Tampoco tiene 'sentido', distribuir 0 billetes entre cero personas: nada entre nadie.

Matemáticamente, el cero es el único número real por el cual no se puede dividir. Por eso 0 es el único real que no tiene inverso multiplicativo.

Ejemplo:

En el análisis matemático existen definiciones de distintos tipos de límites. Por ejemplo:

Sin embargo, si se analiza cada numerador y denominador por separado, el límite de todo ellos es cero. Por eso se dice que es indeterminado, pues pueden obtenerse resultados tan diferentes como infinito, uno o cero.


El valor formula_6 no está definido como potencia, pero según el contexto o por comodidad se puede elegir uno de los resultados mediante una definición. Algunas calculadoras científicas dan 1 como resultado.

En el contexto de los límites, formula_6 es una indeterminación pues los límites de potencias tales que los límites de base y exponente por separado son cero, pueden terminar dando cualquier cosa.

En el conjunto de los enteros, el es un número par; satisface la definición de paridad, así como también todas las características de los números pares.

El cero, junto con los números 1, , "", "" están relacionados en la célebre Identidad de Euler:

En otra ramas de la matemática, especialmente en el álgebra, se llama «cero» y se simboliza también con «0» a elementos de otros conjuntos muy diferentes de los reales. Es el caso del vector nulo en el conjunto de los vectores del plano o del espacio. En general se le dice cero al elemento neutro de un grupo abeliano.

El 0 se asocia con la posición de "apagado" en lógica positiva (el 1 se asocia con la posición de "encendido") y es uno de los dos dígitos (0 y 1) del sistema binario.

El cero absoluto es, en el campo de la física, la temperatura más baja que teóricamente puede alcanzar la materia. Esta temperatura da lugar a la escala Kelvin, que establece como 0 K dicha temperatura. Su equivalencia en grados celsius es de –273,15 °C.





</doc>
<doc id="6698" url="https://es.wikipedia.org/wiki?curid=6698" title="Corán">
Corán

El Corán (del árabe , ', ‘la recitación’, [qurˈan], persa: [ɢoɾˈɒn]), también transliterado como Alcorán, Qurán o Korán"', es el libro sagrado del islam, que según los musulmanes contiene la palabra de Dios (o "Allāh", ), revelada a Mahoma (Muhammad, ), quien se considera que recibió estas "revelaciones" por medio del arcángel Gabriel (Ğibrīl ). 

Durante la vida del profeta Mahoma, las revelaciones eran transmitidas oralmente o escritas en hojas de palmeras, trozos de cuero o huesos, etc. A la muerte del profeta, en 632, sus seguidores comenzaron a reunir estas revelaciones, que durante el Califato de Utman ibn Affan () tomaron la forma que hoy conocemos, 114 capítulos ("azoras", ), cada uno dividido en versículos ("aleyas", ). 

El Corán menciona muchos personajes que aparecen en los libros sagrados del judaísmo y el cristianismo (Tanaj y Biblia) y en la literatura devota (por ejemplo, los libros apócrifos), con muchas diferencias en detalle. Personajes del mundo hebreo y cristiano muy conocidos como Adán, Noé, Abraham, Moisés, Jesús de Nazaret y Juan Bautista aparecen mencionados como profetas islámicos.

Los musulmanes creen que el Corán es la palabra «eterna e increada» de Dios. Por ello, su transmisión debería realizarse sin el menor cambio en la lengua originaria, el árabe clásico. El Corán ha sido traducido a muchos idiomas, principalmente pensando en aquellos creyentes cuyas lenguas no son el árabe. Aun así, en la liturgia se utiliza exclusivamente el árabe, ya que la traducción únicamente tiene valor didáctico, como glosa o instrumento para ayudar a entender el texto original. De hecho, una traducción del Corán ni siquiera se considera un Corán auténtico sino una interpretación del mismo.

El origen del Corán ha generado mucha controversia porque los especialistas islámicos parten de la presunción de que el Corán es un texto incorrupto y divino, mientras que los especialistas laicos lo ven como un texto humano semejante a cualquier otro. Tales divergencias hacen que sea necesario conocer ambas versiones de la historia. El Corán reta a los lectores a que encuentren alguna contradicción o divergencias en él y les enfatiza que no la encontrarán, puesto que al suponerse de origen divino no debería haber contradicciones en él. 

Las variedades más extendidas de la teología musulmana consideran que el Corán es eterno y que no fue creado. Tomando en cuenta que los musulmanes creen que figuras bíblicas tales como Moisés y Jesús predicaron el Islam, la doctrina de la revelación inmutable y no creada del Corán implica que los textos más antiguos - como el Tanaj o la Biblia - se debieron a la "degeneración humana".

No obstante, algunos islamistas de carácter liberalizador, particularmente las escuelas mutazilí e ismailí, implícita o explícitamente cuestionan la doctrina de un "Corán" no creado cuando realizan ciertas preguntas relacionadas a la aplicación de la "Sharia" o ley islámica. Algunos pensadores contemporáneos, como Reza Aslan o Nasr Hamid Abu Zayd, han argüido que tales leyes fueron creadas por Alá para solucionar las necesidades particulares de la comunidad de Mahoma (la "ummah"). Otros rebaten que tales leyes no difieren en nada de la ley mosaica.

Entre las razones ofrecidas por la crítica de la doctrina del "Corán eterno" se encuentra su implicación en el "tawhid", «la unicidad de Dios». El pensamiento de que el Corán es la palabra eterna y no creada de Alá y que siempre ha existido junto a Él podría llevar a pensar en un concepto plural de la naturaleza de dicha deidad. Preocupados de que esta interpretación parezca hacerse eco del concepto cristiano de la «palabra eterna de Yahvé» "(Logos)", algunos musulmanes, y particularmente los mutazilíes rechazaron la noción de la eternidad del Corán. Sin embargo, buena parte de los musulmanes actuales opinan que esta visión de los mutazilíes es producto de la no comprensión profunda de la naturaleza misma del Corán y de su relación con el "tawhid".

Según la tradición, Mahoma no podía leer ni escribir sino que, simplemente, recitó lo que le era revelado para que sus compañeros lo escribieran y memorizaran. Algunos exégetas creen que esta tradición de que Mahoma no podía leer ni escribir está en contradicción con el texto coránico mismo por doble partida: primero el Corán anuncia que el profeta «no solía leer ni escribir» es decir no era dado a la lectura o la escritura, esto, según ellos, no quiere decir que no supiera hacerlo, pero existe otra aleya susceptible de ser interpretada como indicio de que sabía leer, la número dos de la azora «La Congregación»: «Fue Dios quien levantó de entre los iletrados un Apóstol de entre ellos mismos, recitando Sus Señales, purificándoles y enseñándoles el Libro y la sabiduría..." Los simpatizantes del Islam tienen por verdad que la redacción del texto coránico existente hoy corresponde exactamente a lo que fue revelado al profeta Mahoma, es decir, las palabras textuales de Dios entregadas a Mahoma por medio del arcángel Gabriel.

Los acompañantes de Mahoma, según las tradiciones musulmanas, empezaron a registrar las azoras de forma escrita antes de que su líder muriera en el año 632. Esta práctica de escribir las «revelaciones» a medida que le llegaban al profeta era una libertad que todos los testigos de los momentos en que ocurrían las revelaciones podían tomarse, aunque se trataba de una reabundancia literaria ya que el Corán fue compilado bajo los auspicios del profeta mismo. Basta decir que entre todos los coranes que existen hoy y han existido no hay ninguna diferencia. Existe solo una versión del Sagrado Corán, las copias de varias azoras escritas durante su vida se citan con frecuencia en las tradiciones. Por ejemplo, en la historia de la conversión de Umar ibn al-Jattab (momento en que Mahoma todavía estaba en La Meca), se dice que su hermana estaba leyendo un texto de la azora Ta-Ha. En Medina, se dice que alrededor de sesenta y cinco acompañantes actuaron como escribas para él en algún momento o en otro. El profeta los llamaba para que escribieran las «revelaciones» justo después de tenerlas.

Una tradición documenta que la primera recopilación completa del Corán fue hecha durante el mandato del primer califa, Abu Bakr as-Siddiq. Zayd ibn Thabit, que había sido uno de los secretarios de Mahoma, «reuniendo el Corán a partir de varias piezas de hueso y de los pechos (es decir, ‘los recuerdos’) de los hombres». Esta recopilación fue conservada por Hafsa bint Umar, hija del segundo califa Umar y una de las viudas de Mahoma.

Durante el califato de Utman ibn Affan, hubo disputas relativas a la recitación del Corán. En respuesta, Utman decidió codificar, estandarizar y transcribir el texto. Se dice que Utman comisionó a un comité (que incluía a Zayd y varios miembros prominentes de Quraysh) para poder producir una copia estándar del texto.

Según algunas fuentes, esta recopilación se basó en el texto conservado por Hafsa. Otras versiones indican que Utman hizo esta recopilación de manera independiente y que el texto de Hafsa habría sido llevado adelante y que, al final, se encontró que los dos textos coincidían perfectamente. Sin embargo, otros documentos omiten por completo referencias a Hafsa.

Los eruditos musulmanes afirman que si el califa hubiera ordenado la recopilación del Corán, este nunca habría sido relegado al cuidado de una de las viudas del profeta. 

Cuando terminó el proceso de recopilación, entre los años 650 y 656, Utman envió copias del texto final a todos los rincones del imperio islámico y ordenó la destrucción de todas las copias que difirieran de la nueva versión.

Varios de los manuscritos, incluyendo el manuscrito de Samarcanda, son reivindicados como copias originales de las enviadas por Utman; no obstante, muchos especialistas, occidentales e islámicos, dudan que sobreviva algún manuscrito utmánico original.

En lo que respecta a las copias que fueron destruidas, las tradiciones islámicas aseguran que Abdallah Ibn Masud, Ubay Ibn Ka'b y Alí, primo y yerno de Mahoma, habían preservado algunas versiones que diferían en algunos aspectos del texto utmánico que es considerado ahora por todos los musulmanes. Los especialistas musulmanes registran determinadas diferencias entre las versiones, las cuales consisten casi totalmente en variantes léxicas y ortográficas o diferentes conteos de versos. Se ha registrado que los tres (Ibn Masud, Ubay Ibn Ka'b y Alí) aceptaron el texto utmánico como la autoridad definitiva.

La versión de Utman se compuso según un viejo estilo de escritura árabe, que no incluía vocales, razón por la cual se puede interpretar y leer de varias formas. Este escrito utmánico básico se ha llamado rasma y, con algunas diferencias menores, es la base de varias tradiciones orales de recitación. Para fijar estas recitaciones y prevenir cualquier error, los escribanos y eruditos comenzaron a anotar las rasmas utmánicas con varias marcas diacríticas —puntos y demás— para indicar la forma en que las palabras debían ser pronunciadas. Se cree que este proceso de anotación comenzó alrededor del año 700, poco tiempo después de la compilación de Utman, y que terminó aproximadamente en el año 900. El texto del Corán más usado en la actualidad está basado en la tradición de recitación de los Hafs, tal y como fue aprobado por la Universidad Al-Azhar de El Cairo, en 1922, (para más información relacionada con las tradiciones de recitación, refiérase a Recitación coránica, más adelante en este mismo artículo).

Aunque algunos eruditos concuerdan con varios de los aspectos señalados por las tradiciones islámicas relativas al Corán y sus orígenes, especialistas escépticos aseguran que Mahoma mismo compuso los versos y las leyes que integran el texto y que se las atribuyó a Alá para darles legitimidad; agregan que sus seguidores memorizaron y escribieron sus revelaciones y que numerosas versiones de estas revelaciones circularon después de su muerte en el año 632; aseguran asimismo que Utman ordenó la recopilación y el ordenamiento de esta masa de material entre 650 y 656, lo cual también es descrito por los eruditos islámicos. Los eruditos occidentales señalan muchas características del Corán (sus repeticiones, su ordenamiento, la mezcla de estilos y géneros) como signos de un muy humano proceso de recopilación que nada tiene que ver con supuestos «métodos divinos».

Estos eruditos explican las numerosas similitudes entre el "Corán" y las escrituras hebreas argumentando que Mahoma enseñaba a sus seguidores lo que él pensaba que era historia universal, tal y como lo había escuchado de las bocas de judíos y cristianos que había encontrado en Arabia y durante sus viajes. Ciertos eruditos seglares también debaten la creencia islámica de que todo el "Corán" fue enviado por Dios a la humanidad. En este sentido, notan que en numerosos pasajes se alude a Dios directamente en tercera persona o bien cuando la voz narrativa jura por varios entes, incluyendo a Dios. Otros especialistas tienden a no atribuir el Corán entero a Mahoma, arguyendo que no hay una verdadera prueba de que el texto haya sido compilado bajo el mandato de Utman, puesto que las más viejas copias conservadas del "Corán" completo datan de varios siglos después de Utman (la más vieja copia existente del texto completo es del siglo IX). Alegan que el Islam se formó lentamente, durante los siglos transcurridos tras las conquistas musulmanes y en la medida en que los conquistadores islámicos iban elaborando sus propias creencias en respuesta de los desafíos judíos y cristianos. Una propuesta influyente en este punto de vista fue la del Dr. John Wansbrough, un académico inglés. Sin embargo, los escritos de Wansbrough estaban redactados en un estilo denso, complejo y casi hermético y han tenido una gran influencia en los estudios islámicos a través de sus estudiantes, Michael Cook y Patricia Crone y no tanto por sí mismos. En 1977, Crone y Cook publicaron un libro llamado "Hagarism", en el que se sostiene que:

Este libro fue extremadamente controvertido en su tiempo, pues desafiaba no solo la ortodoxia musulmana, sino las actitudes prevalecientes entre los mismos islamistas seglares. Wansbrough fue criticado por su interpretación del "Corán" y por la "mala" interpretación de las palabras originales en árabe. Crone y Cook se han desdicho de algunos de sus argumentos en el sentido de que el "Corán" evolucionó a lo largo de varios siglos, pero todavía sostienen que la tradición de lectura sunita es muy poco fiable, pues proyecta su ortodoxia contemporánea en el pasado —del mismo modo que si los exégetas del Nuevo Testamento quisieran comprobar que Jesús era católico o metodista.

Fred Donner ha argüido contra Crone y Cook, en lo relativo a la temprana fecha de la recopilación del "Corán", basado en sus lecturas del propio texto. Él argumenta que si el "Corán" hubiera sido recopilado a lo largo de los tumultuosos siglos iniciales del Islam (con sus vastas conquistas, expansión y los sangrientos incidentes entre los rivales del califato), habría habido evidencia de esta historia en el texto. No obstante, según él, no hay nada en el "Corán" que no refleje las cosas conocidas de la temprana comunidad musulmana.

Algunos aseguran que los hallazgos arqueológicos de 1972 pueden arrojar luz acerca de los orígenes del "Corán". En ese año, durante la restauración de la Gran Mezquita de San'a, en Yemen, los obreros hallaron un «cementerio de papeles» que contenía decenas de millares de papeles en donde se leían fragmentos del "Corán" (los ejemplares del "Corán" todavía se desechan de esta manera, pues se considera impiedad tratar el «texto sagrado» como si fuera basura ordinaria). Se creyó que algunos de esos fragmentos eran los textos coránicos más antiguos que se han encontrado. El especialista europeo Gerd R. Puin ha estudiado estos fragmentos y ha publicado no solamente un corpus de textos, sino también algunos descubrimientos preliminares. Las variantes de los textos descubiertos parecen coincidir con ciertas variantes menores reportadas por algunos eruditos islámicos en sus descripciones de las variantes del "Corán", que una vez estuvieron en posesión de Abdallah Ibn Masud, Ubay Ibn Ka'b y Alí, y que fueron suprimidas por órdenes de Utman.

El Corán ha producido un gran corpus de comentarios y explicaciones. Los musulmanes tardíos no siempre comprendían la lengua del Corán, no entendieron ciertas alusiones que parecían claras a los primeros musulmanes y estaban extremadamente preocupados en reconciliar las contradicciones y los conflictos en el Corán. Los comentadores glosaron el árabe, explicaron las alusiones y, lo que quizá sea más importante, decidieron qué versos coránicos habían sido revelados primero en la carrera profética de Mahoma (lo cual era apropiado para la naciente comunidad musulmana) y cuáles habían sido revelados después, cancelando o abrogando el texto original. Los recuerdos de las "ocasiones de revelación", es decir, las circunstancias en que Mahoma había hecho públicas las revelaciones, también fueron recopiladas, pues se pensaba que podrían explicar algunas oscuridades.

Por todas estas razones, fue extremadamente importante para los comentadores explicar cómo fue revelado el Corán —cuando y bajo qué circunstancias. Muchos comentarios o tafsir, concernían a la historia. Los primeros tafsir son unas de las mejores fuentes de la historia islámica. Algunos comentadores famosos son al-Tabari, az-Zamakhshari, at-Tirmidhi y Ibn Kathir. Generalmente estos comentarios clásicos incluían todas las interpretaciones comunes y aceptadas, mientras que los comentarios de los fundamentalistas modernos, como el escrito por Sayyed Qutb tienden a dar solo una de las interpretaciones posibles.

Los comentadores se sienten muy seguros de las exactas circunstancias que motivaron algunos versos, como la azora Iqra o las aleyas 190-194, de la azora al-Baqara. Pero en algunos casos (como la azora al-Asr), lo más que se puede decir es en qué ciudad estaba viviendo Mahoma en ese momento. En otros casos, como con la azora al-Kawthar, los detalles de las circunstancias están en disputa, pues diversas tradiciones entregan versiones diferentes.

Las más importantes «ayudas exteriores» que se han usado para interpretar los significados del Corán son las hadith — la colección de tradiciones en las que algunos eruditos musulmanes (los ulemas) basaron la historia y las leyes islámicas. Los especialistas han inspeccionado las miles de páginas de las hadices, intentando descubrir cuáles eran ciertas y cuáles eran fabricaciones. Un método muy utilizado era el estudio de la cadena de narradores, el isnad, a través de los cuales fue transmitida la tradición.

Obsérvese que aunque se dice que ciertos hadiz —los hadiz qudsí— registran las palabras no canónicas que según la tradición Dios le dirigió a Mahoma, o el sumario de estas, los musulmanes no consideran que esos textos sean parte del Corán.

El Corán retoma las historias de muchos de los personajes y eventos que aparecen en los libros sagrados de los judíos y los cristianos (El Tanaj, La Biblia) y la literatura devocional (Los libros apócrifos y el Midrásh), aunque difiere en muchos detalles. Ciertos personajes bíblicos muy bien conocidos, como Adán, Noé, Abraham, Isaac, Jacob, Moisés, Juan el Bautista y Jesús son mencionados en el Corán como Profetas del Islam. Sin olvidar a María (Maryam en árabe), madre de Jesús.

Los episodios son los mismos con diferencias de detalle, unas menos trascendentes que otras, y los fragmentos se encuentran dispersos entre las aleyas de las suras. Narra detalles de los episodios sobre la creación del hombre al que se da el nombre de Adán en el Jardín, la desobediencia del ángel Iblis ante el mandato de Dios Alláh de postrarse ante Adán, y cómo Dios llama Demonio (Shaytan) a Iblis; la expulsión del Jardín; una mención, indirecta, a Caín y Abel; Noé (Nuh), el arca (la nave) y el diluvio con la destrucción del pueblo de Noé y la muerte y de uno de sus hijos, así como la condenación de su mujer por traición; el arca se posa en el Chudi (los montes de Ararat según el Génesis de la Tanaj); la fecundidad de la mujer de Abraham, el nacimiento de Isaac y la prueba de Dios a Abraham (Ibrahim) pidiéndole sacrificar a Isaac; la destrucción del pueblo de Lot (Sodoma) y la condenación de su mujer por traición; la estancia de los israelitas en Egipto, el nacimiento de Moisés, su competencia con los magos del Faraón, los nueve signos (las diez plagas según la Tanaj), el paso del mar, el encuentro de Moisés con Dios al ver un fuego (en ángel del Señor en una llama de fuego en medio de una zarza, según la Tanaj), las tablas de la ley, el ternero (el becerro de oro); David, que mata a Goliat, etc.

Ya en comparación con los evangelios del Nuevo Testamento de la Biblia, el Corán narra el nacimiento de María como hija de "la mujer de Imran" y su tutela por el sacerdote Zacarías, esposo de Isabel; el anuncio a Zacarías del nacimiento de su hijo Juan (el Bautista); la anunciación a María del nacimiento de su hijo Jesús (Isa) y su embarazo por el Espíritu de Dios en forma mortal acabado para regalarle un muchacho puro; su retiro, los dolores del parto y el nacimiento de Isa/Jesús, quien habla en la cuna declarándose mortal, siervo, profeta y enviado por Dios a los hijos de Israel, e incluso anuncia la próxima llegada de Ahmad (Muhammad, Mahoma) a quien el Corán denomina el "Sello de los profetas".

Otra similitud es respecto a la ascendencia de María o Maryam, pues en el Corán se menciona que es hija de Imran y también hermana de Aarón, y aunque los evangelios no la mencionan como tal sí lo hacen indirectamente, como el de Lucas, que describe a María como parienta de Isabel (Lc 1:36) y a ésta como descendiente de Aarón (Lc 1:15), y así también al esposo de Isabel, Zacarías, como sacerdote del grupo de Abías (Lc 1:5), descendiente de Aarón, siendo Aarón hijo de Amran (Imran) y sumo sacerdote de los levitas, los descendientes de Leví, la tribu consagrada por Yahvé (en la Tanaj) para ser sus sacerdotes.

La palabra "Corán", generalmente, es traducida como "recitación", en indicación de que no puede existir como un simple texto. Siempre ha sido transmitido oralmente al mismo tiempo que gráficamente.
Para al menos ser capaz de realizar una salat (oración), una obligación indispensable en el Islam, un musulmán tiene que aprender al menos algunas azoras del Corán (generalmente, empezando con la primera azora, al-Fatiha, conocida como "los siete versos repetidos", y luego avanzando hasta las más cortas que están al final del libro).

Una persona que pueda recitar todo el Corán se llama qāri' () o hāfiz (términos que se traducen como "recitador" o "memorizador," respectivamente). Mahoma es recordado como el primer hāfiz. El canto ("tilawa" ) del Corán es una de las bellas artes del mundo musulmán.

Existen diversas escuelas de recitación coránica y todas constituyen pronunciaciones permitidas del rasm utmánico. Hoy existen diez recitaciones canónicas y cuatro no canónicas del Corán. Para que una recitación sea canónica tiene que cumplir con tres condiciones:

Ibn Mujahid documentó siete recitaciones de este tipo e Ibn Al-Jazri agregó tres. Se trata de:

Estas recitaciones difieren en la vocalización ("tashkil" تشكيل) de unas cuantas palabras, las cuales a su vez le dan a la palabra un significado diferente, según las reglas de la gramática árabe. Por ejemplo, la vocalización de un verbo puede cambiar su voz activa y pasiva. También puede cambiar su formación, lo que implica la intensidad, por ejemplo. La vocales se pueden cambiar en su cantidad (es decir, se pueden alargar o acortar) y las pausas glotales (hamzas) pueden agregarse o elidirse, según las reglas respectivas de la recitación en particular. Por ejemplo, el nombre del Arcángel Gabriel se puede pronunciar de manera diferente en distintas recitaciones: Jibrīl, Jabrīl, Jibra'īl, y Jibra'il. El nombre "Qur'ān" se pronuncia sin la pausa glotal (como en "Qurān") en una recitación y el nombre del profeta Ibrāhīm se puede pronunciar Ibrāhām en otra.

Las narraciones más usadas son las de Hafs (), Warsh (), Qaloon () y Al-Duri a través de Abu `Amr (). Los musulmanes creen firmemente que todas las recitaciones canónicas fueron hechas por el Profeta mismo, citando la respectiva cadena de narración isnad canónica y las aceptan como válidas para la adoración como una referencia para las leyes de Sharia. Las recitaciones no canónicas son llamadas «explicativas» por su papel de darle diferentes perspectivas a un verso o aleya dada. Hoy varias personas poseen el título de «Memorizador de las diez recitaciones», lo cual se considera el máximo honor en las ciencias del Corán.

El Corán consiste en 114 azoras (capítulos) compuestas a su vez por un total de 6 236 aleyas (versos) dejando por fuera 112 de los 113 bizmillas o basmalas con que empiezan las azoras pues son idénticos («En el nombre de Dios, el Compasivo y Misericordioso») y, por lo general, se dejan sin enumerar. De manera alternativa, se pueden incluir los bizmillas en el recuento de los versos, lo cual arroja un número de 6348 aleyas. El número exacto de aleyas ha sido discutido, no por una disputa relativa al contenido del Corán sino debido a los métodos de conteo. Varios «musulmanes de El Corán original» han rechazado dos versos del Corán por considerarlos espurios y trabajan con la suma de 6346. Por lo general, los musulmanes no se refieren a las azoras por sus números sino por un nombre derivado del texto de cada azora. Las azoras no están dispuestas en orden cronológico (en el orden en el que los estudiosos islámicos suponen que fueron reveladas) sino que están ordenadas según el tamaño, aunque no de manera exacta; también se cree que este método es de inspiración divina. Luego de una breve introducción, aparece en el Corán la azora más larga y el texto concluye con las más cortas. Se dice que hay aproximadamente 77 639 letras en él.

Según algunos lingüistas no musulmanes, el orden decreciente de las azoras del Corán está inspirado probablemente en el tipo de ordenación de los "divanes" poéticos. El resultado final responde, más o menos, a un orden cronólogico invertido: las más largas, del periodo medinés, al principio del libro; las más cortas (correspondientes al inicio de la Revelación), del periodo mecano, al final.

Además de la división en azoras —y muy independientes de esta—, existen varias formas de dividir el Corán en secciones de similar tamaño que facilitan la lectura, la recitación y la memoria. Las siete manzil (estaciones) y las treinta juz' (partes) se pueden usar para trabajar con todo el Corán durante una semana o un mes (un mandil o un juz' por día). Un juz' se puede dividir en dos ahzab (grupos), y cada ahzab se puede subdividir en cuatro cuartos. Una estructura diferente ofrece el ruku'at, en la cual aparecen unidades semánticas que se asemejan a párrafos y que se componen aproximadamente de diez aleyas.

Un hafiz es un hombre que ha memorizado todo el texto del Corán. Se cree que hay millones de ellos, desde niños hasta ancianos; muchos niños y adultos incluidos muchos que no pueden leer árabe, memorizan el Corán parcialmente o en su totalidad. Para realizar la salat (oración) se necesita memorizar el texto al menos de forma parcial.

Todos los capítulos, con excepción de uno, empiezan con las palabras "Bismillah ir-Rahman ir-Rahim", «En el nombre de Dios, el más Misericordioso, el Compasivo». Veintinueve azoras empiezan con letras tomadas de un subconjunto restringido del alfabeto árabe; así, por ejemplo, la azora Maryam empieza «Kaf. Ha. Ya. 'Ain. Sad. (Esta es) una mención de la Misericordia de tu Señor a Su siervo Zacarías».

Aunque ha habido alguna especulación sobre el significado de estas letras, el consenso de los eruditos musulmanes es que su sentido último está más allá de la capacidad de entendimiento humano. Sin embargo, se ha observado que, en cuatro de los 29 casos, estas letras aparecen seguidas casi inmediatamente por la mención misma de la revelación coránica. Los esfuerzos de los académicos occidentales han sido provisionales; una propuesta, por ejemplo, fue que se trataba de las iniciales o los monogramas de los escribas que originalmente escribieron las azoras.

La creencia en el origen divino, directo e incorrupto del Corán es considerado fundamental por la mayoría de los musulmanes. Esto trae como consecuencia directa la creencia de que el texto no tiene errores ni inconsistencias.

A pesar de esto, a veces ocurre que unos versos prohíben una práctica determinada mientras que otros la permiten. Esto es interpretado por los musulmanes a la luz de la cronología relativa de los versos: debido a que el Corán fue revelado durante el curso de 23 años, muchos de los versos fueron clarificados o relacionados ("mansūkh") con otros versos. Los comentadores musulmanes explican esto afirmando que Mahoma fue dirigido de manera tal que pudiera liderar a un pequeño grupo de creyentes por el camino recto, en vez de revelarles de una sola vez el rigor total de la ley. Por ejemplo, la prohibición del alcohol fue llevada a cabo de forma gradual, no de inmediato. El verso más antiguo les dice a los creyentes “No se aproximen a las oraciones con una mente nublada, a menos que puedan entender todo lo que dicen” (4:43), se trata entonces de una prohibición de la ebriedad, pero no del consumo de alcohol: «Si piden consejo sobre el vino y el juego, diles: ‘Hay algún provecho en ellos para los hombres, pero el pecado es más grande que el provecho’» (2:219).

Finalmente, en algunos casos la mayoría de los académicos musulmanes aceptan la doctrina de la “abrogación” ("naskh"), según el cual los versos revelados más tarde a veces están por encima de los versos entregados anteriormente. Qué versos abrogan a otros es una cuestión generadora de controversia.

El Corán fue uno de los primeros textos redactados en árabe. Se halla escrito en una forma temprana del árabe clásico, que se conoce en español como árabe “coránico”. No hay muchos otros ejemplos de la lengua árabe de aquella época (algunos especialistas consideran que las Mu'allaqat u Odas suspendidas son ejemplos de árabe preislámico; otros consideran que fueron escritas antes de Mahoma; de cualquier manera, solo sobreviven cinco inscripciones en árabe preislámico).

Poco tiempo después de la muerte de Mahoma, en 632, el Islam se expandió más allá de Arabia y conquistó mucho de lo que era entonces el mundo «civilizado». Había millones de musulmanes en el extranjero con quienes los gobernadores árabes tenían que comunicarse. Por consiguiente, la lengua cambió rápidamente en respuesta a la nueva situación, perdiendo los casos y el vocabulario oscuro. Unas cuantas generaciones después de la muerte del profeta, muchas palabras usadas en el Corán ya se habían vuelto arcaísmos. Debido a que el lenguaje beduino había cambiado a un ritmo mucho más lento, los primeros lexicógrafos árabes recurrieron al beduino para explicar palabras o dilucidar cuestiones gramaticales. En buena medida debido a las necesidades religiosas de explicar el Corán al pueblo, la gramática y la lexicografía árabes se convirtieron en ciencias importantes, y el modelo para el lenguaje literario sigue siendo hasta el día de hoy el árabe usado en los tiempo coránicos, y no las variantes habladas en la actualidad.

Los musulmanes aseguran que el Corán destaca por su poesía y por su belleza y que su perfección literaria es una evidencia de su origen divino. Debido al hecho de que esta perfección solo es perceptible para los que hablan árabe, se considera que el texto original en árabe es el «verdadero Corán». En general, las traducciones a otras lenguas, aunque realizadas siempre por eminentes arabistas, son tenidas como simples glosas, en tanto interpretaciones, de las palabras directas de Dios. La lectura en otro idioma, sin la cadencia de la recitación en árabe, puede resultar confusa, tediosa y reiterativa:

Las tradiciones imperantes en la traducción y la publicación del Corán sostienen que cuando el libro es publicado simplemente debería titularse El Corán y, asimismo, debería incluir siempre un adjetivo calificativo (que evite cualquier confusión con otras «recitaciones»), este es el motivo por el cual la mayoría de las ediciones disponibles del Corán se llaman «El glorioso Corán», «El noble Corán» y otros títulos similares.

Existen numerosas traducciones del Corán a lenguas occidentales, llevadas a cabo por conocidos estudiosos islámicos. Cada traducción es un poco diferente de las otras y muestra la habilidad del traductor para verter el texto de una forma que sea al mismo tiempo fácil de entender y que mantenga el sentido original.

Prácticamente, todos los eruditos islámicos son capaces de leer y comprender el Corán en su forma original y, de hecho, la mayoría se lo sabe de memoria íntegramente.

El Corán mezcla la narrativa, la exhortación y la prescripción legal. Por lo general, las azoras combinan estos tres tipos de secuencias textuales y no siempre de maneras que resultan obvias para el lector, sino algunas veces de formas inexplicables. Los musulmanes señalan que el estilo único del Corán es un indicio más de su origen divino.

Existen muchos elementos que se repiten en el Corán: epítetos («Señor de los cielos y la tierra»), oraciones («Y cuando dijimos a los ángeles: 'Postraos ante Adán', todos se postraron»), e incluso historias, como la historia de Adán. Los especialistas musulmanes explican estas repeticiones como una forma de enfatizar y explicar diferentes aspectos de temas importantes. Asimismo, los académicos señalan que las traducciones a las lenguas occidentales demandan grandes cambios en la redacción y en el orden para poder mantener la explicación y el significado específicos.

El Corán oscila entre la rima y la prosa. Tradicionalmente, los gramáticos árabes consideran que el Corán es un género único en sí mismo. No es ni poesía (definida como palabras con métrica y rima) ni tampoco prosa (definida esta como una conversación normal, pero sin métrica ni rima, saj').

El Corán en ocasiones utiliza rima asonante entre los versos sucesivos; por ejemplo, en el inicio de la azora Al-Faǧr:

o, para dar un ejemplo menos asonante, la azora “al-Fîl”:

Obsérvese que las vocales finales de verso se dejan sin pronunciar cuando estos se pronuncian de manera aislada, se trata del fenómeno regular de las pausas en el árabe clásico. En estos casos, «î» y «û» riman a menudo y hay una cierta búsqueda de variación en las consonantes en posición final de sílaba).

Algunas azoras también incluyen un refrán que se repite varias veces, por ejemplo «"ar-Rahman"» («¿Entonces cuál de los favores de vuestro Señor negaréis?”) y «"al-Mursalat"» («¡Reproches ese día a los que repudien!»).

Los estudiosos islámicos del Corán dividen los versículos del libro en dos partes: los revelados en La Meca y los revelados en Medina después de la Hégira. En general, las azoras más viejas, de la Meca, tienden a contar con versículos más cortos, mientras que las de Medina, que lidian con cuestiones legales, son más largas. Contrástense las azoras de La Meca transcritas antes y unos versículos como los de "Al-Baqara", 229:

Del mismo modo, las azoras de Medina tienden a ser más largas; entre estas se encuentra la más larga del Corán: "Al-Baqara".

Antes de poder tocar una copia del Corán o mushaf, un musulmán debe realizar un wudu (la ablución o ritual de limpieza con agua). Esto se basa en una interpretación literal de la sura «Pues Este es en verdad el Honorable Corán, el Libro bien conservado, que nadie podrá tocar salvo quienes son limpios».

La execración del Corán significa insultar el Corán sacándolo de su contexto o desmembrándolo. Los musulmanes siempre tratan el libro con reverencia y, por consiguiente, está prohibido reciclar, reimprimir o simplemente descartar las copias viejas del texto (en este último caso, los volúmenes deben ser quemados respetuosamente, o bien, enterrados).

El respeto hacia el texto escrito del Corán es un elemento importante de la fe religiosa de muchos musulmanes. Ellos creen que insultar el Corán intencionalmente es una forma de blasfemia. De acuerdo con las leyes de algunos países musulmanes, la blasfemia se puede penar con una prisión de muchos años o incluso con la pena de muerte.

La mayoría de los musulmanes de hoy usan versiones impresas del Corán. Existen ediciones coránicas para todos los gustos, libros de bolsillo, muchos de ellos en ediciones bilingües, con el texto árabe por un lado y una glosa en lengua vernácula del otro. El primer Corán impreso se publicó en 1801 en Kazán.

Antes de que la impresión fuera común, el Corán se transmitía a través de copistas y calígrafos. Debido al hecho de que la tradición musulmana sentía que retratar directamente a los personajes sagrados podría conducir a la idolatría, se prohibió decorar el Corán con imágenes (como sí se hace con frecuencia en los textos cristianos, por ejemplo). En vez de esto, los musulmanes desarrollaron un amor y un cariño especiales por el texto en sí. Una de las consecuencias de esto es que la Caligrafía árabe es un arte que posee un honor muy alto en el mundo musulmán. Los musulmanes también decoraron sus ejemplares del Corán con figuras abstractas conocidas como arabescos, con tintas de colores y doradas. Algunas páginas de algunos de estos Coranes antiguos se han usado a lo largo de este artículo con fines ilustrativos.

El Corán ha sido traducido a muchos idiomas, pero las traducciones no son consideradas por los musulmanes como copias auténticas del Corán, sino simplemente como «glosas interpretativas» del libro; por lo tanto no se les da mucho peso en los debates relativos al significado del Corán. Además de esto, como simples interpretaciones del texto, se les trata como libros corrientes, en vez de darles todos los cuidados especiales que sí se les dan generalmente a los libros en lengua árabe. A pesar de esto, como es un Mensaje dirigido a toda la humanidad, se debe traducir el significado general de sus frases, estudiadas siglo tras siglo por multitud de sabios.

Robert de Ketton fue el primero en traducir el Corán y lo hizo al latín, en 1143 y, posiblemente, la más reciente con su correspondiente Tafsir o exégesis sea la de Ali Ünal.






</doc>
<doc id="6700" url="https://es.wikipedia.org/wiki?curid=6700" title="11 de agosto">
11 de agosto

El 11 de agosto es el 223.º (ducentésimo vigesimotercer) día del año en el calendario gregoriano y el 224.º en los años bisiestos. Quedan 142 días para finalizar el año.








</doc>
<doc id="6701" url="https://es.wikipedia.org/wiki?curid=6701" title="Música electrónica">
Música electrónica

La música electrónica es aquel tipo de música que emplea instrumentos musicales electrónicos y tecnología musical electrónica para su producción e interpretación. En general, se puede distinguir entre el sonido producido mediante la utilización de medios electromecánicos de aquel producido mediante tecnología electrónica, la cual también puede ser mezclada. Algunos ejemplos de dispositivos que producen sonido electro-mecánicamente son el telarmonio, el órgano Hammond y la guitarra eléctrica. La producción de sonidos puramente electrónica puede lograrse mediante aparatos como el theremin, el sintetizador de sonido y el ordenador.

La música electrónica se asoció en su día exclusivamente a una forma de música culta occidental, pero desde finales del año 1960, la disponibilidad de tecnología musical a precios accesibles propició que la música producida por medios electrónicos se hiciera cada vez más popular. En la actualidad, la música electrónica presenta una gran variedad técnica y compositiva, abarcando desde formas de música culta experimental hasta formas populares como la música electrónica de baile.

La habilidad de grabar sonidos suele relacionarse con la producción de música electrónica, aunque esta no sea absolutamente necesaria para ello. El primer dispositivo conocido capaz de grabar sonido fue el fonoautógrafo, patentado en 1857 por Édouard-Léon Scott de Martinville. Podía grabar sonidos visualmente, pero no reproducirlos de nuevo. 

En 1878, Thomas A. Edison patentó el fonógrafo, que utilizaba cilindros similares al aparato de Scott. Aunque se siguieron utilizando los cilindros durante algún tiempo, Emile Berliner desarrolló el fonógrafo de disco en 1887. Otro invento significativo que posteriormente tendría una gran importancia en la música electrónica fue la válvula audión, del tipo triodo, diseñada por Lee DeForest. Se trata de la primera válvula termoiónica, inventada en 1906, que permitiría la generación y amplificación de señales eléctricas, la emisión de radio, la computación electrónica y otras variadas aplicaciones.

Con anterioridad a la música electrónica, ya existía un creciente deseo entre los compositores de utilizar las tecnologías emergentes en el terreno musical. Se crearon multitud de instrumentos que empleaban diseños electromecánicos, los cuales facilitaron el camino para la aparición de instrumentos electrónicos. Un instrumento electromecánico llamado Telharmonium (en ocasiones Teleharmonium o Dynamophone) fue desarrollado por Thaddeus Cahill entre los años 1898 y 1912. Sin embargo, como consecuencia de su inmenso tamaño, nunca se llegó a adoptar. Se suele considerar como primer instrumento electrónico el Theremin, inventado por el profesor Léon Theremin alrededor de 1919–1920. Otro primitivo instrumento electrónico fue el Ondes Martenot, que se hizo conocido al ser utilizado en la obra "Sinfonía Turangalila", por Olivier Messiaen. También fue utilizado por otros compositores, especialmente franceses, como Andre Jolivet.

En 1907, justo un año después de la invención del triodo audión, Alexander Iskenderian publicó "Esbozo de una Nueva Estética de la Música", que trataba sobre el uso tanto de fuentes electrónicas como de otras en la música del futuro. Escribió sobre el futuro de las escalas microtonales en la música, posibles gracias al Dynamophone de Cahill: «Solo mediante una larga y cuidadosa serie de experimentos, y un continuo entrenamiento del oído, puede hacerse de este material desconocido uno accesible y plástico para la generación venidera y para el arte».

Como consecuencia de este escrito, así como a través de su contacto personal, Busoni tuvo un profundo efecto en multitud de músicos y compositores, especialmente en su discípulo Edgard Varèse.

En Italia, el futurismo se acercó a la estética musical en transformación desde un ángulo diferente. Una idea de la filosofía futurista era la de valorar el "ruido", así como dotar de valor artístico y expresivo a ciertos sonidos que anteriormente no habían sido considerados como musicales. El "Manifiesto Técnico de la Música Futurista" de Balilla Pratella, publicado en 1911, establece que su credo es: «Presentar el alma musical de las masas, de las grandes fábricas, de los trenes, de los cruceros transatlánticos, de los acorazados, de los automóviles y aeroplanos. Añadir a los grandes temas centrales del poema musical el dominio de la máquina y el victorioso reinado de la electricidad».

El 11 de marzo de 1913, el futurista Luigi Russolo publicó su manifiesto "El arte de los ruidos" (original en italiano, "L'arte dei Rumori"). En 1914, organizó el primer concierto del "arte de los ruidos" en Milán. Para ello utilizó su Intonarumori, descrito por Russolo como "instrumentos acústico ruidistas, cuyos sonidos (aullidos, bramidos, arrastramientos, gorgoteos, etc.) eran manualmente activados y proyectados mediante vientos y megáfonos". En junio se organizaron conciertos similares en París.

Esta década trajo una gran riqueza de instrumentos electrónicos primitivos, así como las primeras composiciones para instrumentación electrónica. El primer instrumento, el Theremin, fue creado por Léon Theremin (nacido como Lev Termen) entre 1919 y 1920 en Leningrado. Gracias a él se realizaron las primeras composiciones para instrumento electrónico, opuestas a aquellas realizadas por los que se dedicaban a crear sinfonías de ruidos. En 1929, Joseph Schillinger compuso su "Primera Suite Aerofónica para Theremin y Orquesta", interpretada por primera vez por la Orquesta de Cleveland y Leon Theremin como solista.

Además del Theremin, el "Ondes Martenot" fue inventado en 1928 por Maurice Martenot, quien debutó en París. El año siguiente, Antheil compuso por primera vez para dispositivos mecánicos, aparatos productores de ruidos, motores y amplificadores en su ópera inacabada "Mr. Bloom".

La grabación de sonidos dio un salto cualitativo en 1927, cuando el inventor estadounidense J. A. O'Neill desarrolló un dispositivo para la grabación que utilizaba un tipo de cinta recubierta magnéticamente. No obstante, fue un desastre comercial. Dos años más tarde, Laurens Hammond abrió una empresa dedicada a la fabricación de instrumentos electrónicos. Comenzó a producir el Órgano Hammond, basado en los principios del Telharmonium, junto a otros desarrollos como las primeras unidades de reverberación.

El método foto-óptico de grabación de sonido utilizado en el cine hizo posible obtener una imagen visible de la onda de sonido, así como sintetizar un sonido a partir de una onda de el mismo.

En la misma época, comenzó la experimentación del arte sonoro, cuyos primeros exponentes incluyen a Tristan Tzara, Kurt Schwitters y Filippo Tommaso Marinetti entre otros.

Desde aproximadamente el año 1900 se utilizaba el magnetófono de alambre magnético de baja fidelidad, y a comienzos de 1930 la industria cinematográfica comenzó a convertirse a los nuevos sistemas de grabación de sonido ópticos basados en células fotoeléctricas. En esta época la empresa alemana de electrónica AEG desarrolla el primer magnetófono de cinta práctico, el Magnetophon K-1, revelado en el Berlin Radio Show en agosto de 1935.

Durante la Segunda Guerra Mundial, Walter Weber redescubrió y aplicó la técnica AC Bias, que incrementó drásticamente la fidelidad de las grabaciones magnéticas al añadir una alta frecuencia inaudible. Extendió en 1941 la curva de frecuencia del Magnetophon K4 hasta 10 kHz y mejoró la relación señal/ruido hasta 60 dB, sobrepasando cualquier sistema de grabación conocido en aquel tiempo.

En 1942, AEG ya estaba realizando pruebas de grabación en estéreo. No obstante, estos dispositivos y técnicas fueron un secreto fuera de Alemania hasta el final de la guerra, cuando varios de estos aparatos fueron requisados y llevados a Estados Unidos por Jack Mullin. Estos sirvieron de base para los primeros grabadores de cinta profesionales que fueron comercializados en Estados Unidos, entre ellos el Model 200 producido por la empresa Ampex.

La cinta de audio magnética abrió un gran campo de posibilidades sonoras para músicos, compositores, productores e ingenieros. Esta era relativamente barata y su fidelidad en la reproducción era mejor que cualquier otro medio de audio conocido hasta la fecha. Cabe señalar que, a diferencia de los discos, ofrecía la misma plasticidad que la película: puede ser ralentizada, acelerada o incluso reproducirse al revés. Puede editarse también físicamente e incluso pueden unirse diferentes trozos de cinta en loops infinitos, que reproducen continuamente determinados patrones de material pregrabado. La amplificación de audio y el equipo de mezcla expandieron aún más allá las posibilidades de la cinta como medio de producción, permitiendo que múltiples grabaciones fueran grabadas a la vez en otra cinta diferente. Otra posibilidad de la cinta era su capacidad de ser modificada fácilmente para convertirse en máquinas de eco, produciendo así de modo complejo, controlable y con gran calidad, efectos de eco y reverberación, lo que es prácticamente imposible de conseguir por medios mecánicos.

Pronto los músicos comenzaron a utilizar el grabador de cinta o magnetófono para desarrollar una nueva técnica de composición llamada música concreta. Esta técnica consiste en la edición de fragmentos de sonidos de la naturaleza o de procesos industriales grabados conjuntamente. Las primeras piezas de esta música fueron creadas por Pierre Schaeffer, con la colaboración de Pierre Henry. En 1950, Schaeffer dio el primer concierto de música concreta en la École Normale de Musique de Paris. Posteriormente, Pierre Henry colaboró con Schaeffer en la "Symphonie pour un homme seul" (1950), la primera obra importante de música concreta. Un año más tarde, RTF creó el primer estudio para la producción de música electrónica, lo que se convertiría en una tendencia global. También en 1951, Schaeffer y Henry produjeron una ópera, "Orpheus", para sonidos y voces concretos.

Karlheinz Stockhausen trabajó brevemente en el estudio de Schaeffer en 1952 y posteriormente, durante muchos años, en el Estudio de Música Electrónica de la WDR de Colonia, en Alemania.

En Colonia, el que se convertiría en el estudio de música electrónica más famoso del mundo, inició actividades en la radio de la NWDR en 1951, después de que el físico Werner Meyer-Eppler, el técnico de sonido Robert Beyer y el compositor Herbert Eimert convencieran al director de la NWDR, Hanns Hartmann, de la necesidad de dicho espacio. En el mismo año de su creación fueron transmitidos los primeros estudios de música electrónica en un programa de la propia radio y presentados en los‭ Cursos de Verano de Darmstadt.‭ ‬En‭ ‬1953‭ ‬hubo una demostración pública en la sala de conciertos de la Radio de Colonia, donde se dejaron escuchar siete piezas electrónicas.‭ ‬Los compositores de los estudios electrónicos eran Herbert ‬Eimert,‭ ‬Karel ‬Goeyvaerts,‭ ‬Paul‭ ‬Gredinger,‭ Henry ‬Pousseur y Karlheinz Stockhausen.

El programa comprendía las siguientes piezas:

En su tesis de 1949, "Elektronische Klangerzeugung: Elektronische Musik und Synthetische Sprache", Meyer-Eppler concibió la idea de sintetizar música desde señales producidas electrónicamente. De esta manera, la E"lektronische Musik" se diferenciaba a gran escala respecto a la M"usique concrète" francesa, que utilizaba sonidos grabados a partir de fuentes acústicas.

Con Stockhausen y Mauricio Kagel como residentes, el estudio de música electrónica de Colonia se convirtió en un emblema del "avant-garde o" vaguardismo, cuando se empezó ya a combinar sonidos generados electrónicamente con los de instrumentos tradicionales. Ejemplos significativos son "Mixtur" (1964) y "Hymnen, dritte Region mit Orchester" (1967). Stockhausen afirmó que sus oyentes decían que su música electrónica les daba una experiencia de "espacio exterior", sensaciones de volar, o de estar en "un mundo de ensueño fantástico".

Aunque los primeros instrumentos electrónicos como el Ondes Martenot, el Theremin o el Trautonium eran poco conocidos en Japón con anterioridad a la Segunda Guerra Mundial, algunos compositores habían tenido conocimiento de ellos en su momento, como Minao Shibata. Años después, diferentes músicos en Japón comenzaron a experimentar con música electrónica, a lo que contribuyó el apoyo institucional, lo que permitió a los compositores experimentar con el último equipamiento de grabación y procesamiento de audio. Estos esfuerzos dieron lugar a una forma musical que fusionaba la música asiática con un nuevo género y sembraría las bases del dominio japonés en el desarrollo de tecnología musical durante las siguientes décadas.

Tras la creación de la compañía Sony (conocida entonces como Tokyo Tsushin Kogyo K.K.) en 1946, dos compositores japoneses, Toru Takemitsu y Minao Shibata, de modo independiente, escribieron sobre la posibilidad de utilizar la tecnología electrónica para producir música hacia finales de los años 1940. En 1948, Takemitsu concibió una tecnología que pudiese traer "ruido" dentro de tonos musicales atemperados dentro de un pequeño y complejo tubo, una idea similar a la m"usique concrète" que Pierre Schaeffer había aventurado en el mismo año. En 1949, Shibata escribió sobre su concepto de "un instrumento musical con grandes posibilidades de actuación" que pudiera "sintetizar cualquier tipo de onda de sonido" y que sea "manejado muy fácilmente," prediciendo que con un instrumento tal, "la escena musical sería cambiada drásticamente". Ese mismo año, Sony desarrolló el magnetófono magnético G-Type.

En 1950, el estudio de música electrónica Jikken Kobo sería fundado por un grupo de músicos que querían producir música electrónica experimental utilizando magnetófonos Sony. Entre sus miembros se encontraban Toru Takemitsu, Kuniharu Akiyama y Joji Yuasa, y estaba apoyado por Sony, empresa que ofrecía acceso a la última tecnología de audio. La compañía contrató a Takemitsu para componer música electroacústica electrónica para mostrar sus magnetófonos. Más allá del Jikken Kobo, muchos otros compositores como Yasushi Akutagawa, Saburo Tominaga y Shiro Fukai también estaban experimentando con música electroacústica entre 1952 y 1953. La música electrónica se asoció en su día exclusivamente con una forma de música culta occidental, pero desde finales de los años 90 la disponibilidad de tecnología musical a precios accesibles permitió que la música por medios electrónicos se hiciera cada vez más popular.

En Estados Unidos, se utilizaban sonidos creados electrónicamente para diferentes composiciones, como ejemplifica la pieza "Marginal Intersection" de Morton Feldman. Esta pieza está pensada para vientos, metales, percusión, cuerdas, dos osciladores y efectos de sonido.

El "Music for Magnetic Tape Project" fue formado por miembros de la Escuela de Nueva York (John Cage, Earle Brown, Christian Wolff, David Tudor y Morton Feldman) y duró tres años hasta 1954. Durante esta época, Cage completó su "Williams Mix", en 1953.

Un importante desarrollo lo constituyó la aparición de ordenadores utilizados para componer música, en contraposición de la manipulación o creación de sonidos. Iannis Xenakis comenzó lo que se conoce como "musique stochastique" o música estocástica, un método compositivo que emplea sistemas matemáticos de probabilidad estocásticos. Se utilizaban diferentes algoritmos de probabilidad para crear piezas bajo un set de parámetros. Xenakis utilizó papel gráfico y una regla para ayudarse a calcular la velocidad de las trayectorias de los glissandos para su composición orquestal "Metástasis" (1953-1954), utilizando posteriormente ordenadores para componer piezas como "ST/4" para cuarteto de cuerda y "ST/48" para orquesta.

En 1954, Stockhausen compuso "Elektronische Studie II", la primera pieza electrónica en ser publicada como banda sonora.

En 1955 aparecieron más estudios electrónicos y experimentales. Fueron notables la creación del Estudio de Fonología, un estudio en el NHK de Tokio fundado por Toshiro Mayuzumi y el estudio de Phillips en Eindhoven, Holanda, que se trasladó a la Universidad de Utrecht como Instituto de Sonología en 1960.

La banda sonora de "Forbidden Planet", producida por Louis y Bebe Barron, fue compuesta únicamente mediante circuitos caseros y magnetófonos en 1956.

El primer computador del mundo en reproducir música fue el CSIRAC, diseñado y construido por Trevor Pearcey y Maston Beard. El matemático Geoff Hill programó el CSIRAC para tocar melodías de música popular. No obstante, este computador reproducía un repertorio estándar y no pudo utilizarse para ampliar el pensamiento musical o para tocar composiciones más elaboradas.

El impacto de los computadores continuó durante 1956. Lejaren Hiller y Leonard Isaacson compusieron "Iliac Suite" para un cuarteto de cuerda, siendo la primera obra completa en ser compuesta con la asistencia de un computador utilizando un algoritmo en la composición. Posteriores desarrollos incluyeron el trabajo de Max Mathews en Bell Laboratories, quien desarrolló el influyente programa MUSIC I. La tecnología del vocoder fue otro importante desarrollo de esta época.

En 1956, Stockhausen compuso "Gesang der Jünglinge", la primera gran obra del estudio de Colonia, basada en un texto del "Libro de Daniel". Un importante desarrollo tecnológico fue la invención del sintetizador Clavivox por Raymond Scott, con ensamblaje de Robert Moog.

El sintetizador RCA Mark II Sound Synthesizer apareció en 1957. A diferencia de los primeros Theremin y Ondes Martenot, era difícil de usar, pues requería una extensa programación y no podía tocarse en tiempo real. En ocasiones denominado el primer sintetizador electrónico, el RCA Mark II Sound Synthesizer utilizaba osciladores de válvula termoiónica e incorporaba el primer secuenciador. Fue diseñado por RCA e instalado en el Columbia-Princeton Electronic Music Center, donde sigue en la actualidad. Posteriormente, Milton Babbitt, influenciado en sus años de estudiante por la "revolución en el pensamiento musical" de Schoenberg, comenzó a aplicar técnicas seriales a la música electrónica.

Estos fueron años fértiles para la música electrónica, no solo para la académica, sino también para algunos artistas independientes a medida que la tecnología del sintetizador se volvía más accesible. En esta época, una poderosa comunidad de compositores y músicos que trabajaba con nuevos sonidos e instrumentos se había establecido y estaba creciendo. Durante estos años aparecen composiciones como "Gargoyle" de Luening para violín y cinta, así como la premiere de "Kontakte" de Stockhausen para sonidos electrónicos, piano y percusión. En esta última, Stockhausen abandonó la forma musical tradicional basada en un desarrollo lineal y en un clímax dramático. En este nuevo acercamiento, que él denominó como "forma momento", se recuerdan las técnicas de "cinematic splice" del cine de principios del siglo XX.

El primero de estos sintetizadores en aparecer fue el Buchla, en 1963, siendo producto del esfuerzo del compositor de música concreta Morton Subotnick.

El Theremin había sido utilizado desde los años 20, manteniendo una cierta popularidad gracias a su utilización en numerosas bandas sonoras de películas de ciencia ficción de los años 50 (por ejemplo: "The Day the Earth Stood Still" de Bernard Herrmann). Durante los años 60, el Theremin hizo apariciones ocasionales en la música popular.

En el Reino Unido, durante este período, el BBC Radiophonic Workshop (creado en 1958) emergió como uno de los estudios más productivos y renombrados del mundo, gracias a su labor en la serie de ciencia ficción "Doctor Who". Uno de los artistas electrónicos británicos más influyentes de este período fue Delia Derbyshire. Es famosa por su icónica ejecución en 1963 del tema central de "Doctor Who", compuesto por Ron Grainer y reconocida por algunos como la pieza de música electrónica más conocida en el mundo. Derbyshire y sus colegas, entre los que se encuentran Dick Mills, Brian Hodgson (creador del efecto de sonido TARDIS), David Cain, John Baker, Paddy Kingsland y Peter Howell, desarrollaron un amplio cuerpo de trabajo que incluye bandas sonoras, atmósferas, sinfonías de programas y efectos de sonido para BBC TV y sus emisoras de radio.
En 1961, Josef Tal creó el "Centre for Electronic Music in Israel" en la Universidad Hebrea, y en 1962 Hugh Le Caine llegó a Jerusalén para instalar su "Creative Tape Recorder" en el centro.

Milton Babbitt compuso su primer trabajo electrónico utilizando el sintetizador, que fue creado mediante el RCA en el CPEMC. Las colaboraciones se realizaban superando las barreras de los océanos y continentes. En 1961, Ussachevsky invitó a Varèse al Columbia-Princeton Studio (CPEMC), siendo asistido por Mario Davidovsky y Bülent Arel. La intensa actividad del CPEMC, entre otros, inspiró a la creación en San Francisco del Tape Music Center en 1963, por Morton Subotnick junto a otros miembros adicionales, como Pauline Oliveros, Ramón Sender, Terry Riley y Anthony Martin. Un año después tuvo lugar el Primer Seminario de Música Electrónica en Checoslovaquia, organizado en el Radio Broadcast Station de Plzen.

Se siguieron desarrollando nuevos instrumentos, y uno de los más importantes avances tuvo lugar en 1964, cuando Robert Moog introdujo el sintetizador Moog, el primer sintetizador analógico controlado por un sistema integrado modular de control de voltaje. Moog Music introdujo posteriormente un sintetizador más pequeño con un teclado, llamado Minimoog, que fue utilizado por multitud de compositores y universidades, haciéndose así muy popular. Un ejemplo clásico del uso del Moog de gran tamaño es el álbum "Switched-On Bach," de Wendy Carlos.

En 1966, Pierre Schaeffer fundó el "Groupe de Recherches Musicales" (Grupo de Investigación Musical) para el estudio y la investigación de la música electrónica. Su programación está estructurada a partir de un compromiso en los procesos de difusión, investigación y creación de la música contemporánea y las tendencias en videoarte e imagen más actuales. Sus exhibiciones y conciertos son reproducidas en tiempo real mediante dispositivos electrónicos, interfaces de audio-vídeo y una plantilla de músicos y videoartistas nacionales e internacionales abiertos al uso de tecnologías punta.

CSIRAC, el primer computador en reproducir música, realizó este acto públicamente en agosto de 1951. Una de las primeras demostraciones a gran escala de lo que se conoció como "computer music" fue una emisión nacional pre-grabada en la red NBC para el programa Monitor el 10 de febrero de 1962. Un año antes, LaFarr Stuart programó el ordenador CYCLONE de la Universidad del Estado de Iowa para reproducir canciones sencillas y reconocibles a través de un altavoz amplificado adherido a un sistema originalmente utilizado para temas administrativos y de diagnóstico.

Los años 50, 60 y la década de los 70 presenciaron también el desarrollo de grandes marcos operativos para síntesis informática. En 1957, Max Mathews, de Bell Labs, desarrolló el programa MUSIC, culminando un lenguaje de síntesis directa digital.

En París, IRCAM se convirtió en el principal centro de investigación de música creada por computador, desarrollando el sistema informático Sogitec 4X, que incluía un revolucionario sistema de procesamiento de señal digital en tiempo real. "Répons" (1981), obra para 24 músicos y 6 solistas de Pierre Boulez, utilizó el sistema 4X para transformar y dirigir a los solistas hacia un sistema de altavoces.

En Estados Unidos, la electrónica en directo fue llevada a cabo por primera vez en los años 60 por miembros del Milton Cohen's Space Theater en Ann Arbor, Míchigan, entre los que estaban Gordon Mumma, Robert Ashley, David Tudor y The Sonic Arts Union, fundada en 1966 por los nombrados anteriormente, incluyendo también a Alvin Lucier y David Behrman. Los festivales ONCE, que mostraban música multimedia para teatro, fueron organizados por Robert Ashley y Gordon Mumma en Ann Arbor entre 1958 y 1969. En 1960, John Cage compuso "Cartridge Music", una de las primeras obras de electrónica en vivo.

Los compositores y músicos de jazz, Paul Bley y Annette Peacock, fueron de los primeros en tocar en concierto utilizando sintetizadores Moog hacia finales de 1960. Peacock hacía un uso regular de un sintetizador Moog adaptado para procesar su voz tanto en el escenario como en grabaciones de estudio.

Robert Moog (también conocido como Bob Moog), a finales de 1963, conoció al compositor experimental Herbert Deutsch, quien, en su búsqueda por sonidos electrónicos nuevos, inspiró a Moog a crear su primer sintetizador, el Sintetizador Modular Moog.

El Moog, aunque era conocido con anterioridad por la comunidad educativa y musical, fue presentado a la sociedad en el otoño de 1964, cuando Bob hizo una demostración durante la Convención de la Sociedad de Ingeniería de Audio, celebrada en Los Ángeles. En esta convención, Moog ya recibió sus primeros pedidos y el negocio despegó.

La compañía Moog Music creció de forma espectacular durante los primeros años, haciéndose aún más conocida cuando Wendy Carlos editó el álbum "Switched on Bach". Bob diseñó y comercializó nuevos modelos, como el Minimoog (la primera versión portátil del Moog Modular), el Moog Taurus (teclado de pedales de una octava de extensión, con transposición para bajos y agudos), el PolyMoog (primer modelo 100% polifónico), el MemoryMoog (polifónico, equivalía a seis MiniMoog's en uno), el MinitMoog, el Moog Sanctuary, etc.

Moog no supo gestionar bien su empresa y esta pasó de tener listas de espera de nueve meses a no recibir ni un solo pedido. Agobiado por las deudas perdió el control de la empresa, la cual fue adquirida por un inversionista. Aun así continuó diseñando instrumentos musicales hasta 1977, cuando abandonó Moog Music y se mudó a un pequeño poblado en las montañas Apalaches. Sin él, la Moog Music fue a pique poco después.

En 1967, Kato se acercó al ingeniero Fumio Mieda, quien deseaba iniciarse en la construcción de teclados musicales. Impulsado por el entusiasmo de Mieda, Kato le pidió construir un prototipo de teclado y, 18 meses después, Mieda le presentó un órgano programable. La compañía Keio vendió este órgano bajo la marca Korg, hecha de la combinación de su nombre con la palabra órgano, en inglés "Organ".

Los órganos producidos por Keio fueron exitosos a finales de los años 60 y principios de los 70 pero, consciente de la competencia con los grandes fabricantes de órganos ya establecidos, Kato decidió usar la tecnología del órgano electrónico para construir teclados dirigidos al mercado de los sintetizadores. De hecho, el primer sintetizador de Keio (MiniKorg) fue presentado en 1973. Después del éxito de este instrumento, Keio presentó diversos sintetizadores de bajo costo durante los años 70 y 80 bajo la marca Korg.

En 1970, Charles Wuorinen compuso "Time's Encomium", convirtiéndose así en el primer ganador del Premio Pulitzer por ser una composición completamente electrónica. Los años 70 también vieron cómo se generalizaba el uso de sintetizadores en la música rock, con ejemplos como: Pink Floyd, Tangerine Dream, Yes y Emerson, Lake & Palmer, etc.

A lo largo de los años 70, bandas como The Residents y Can abanderaron un movimiento de música experimental que incorporaba elementos de música electrónica. Can fue uno de los primeros grupos en utilizar loops de cinta para la sección de ritmo, mientras The Residents crearon sus propias cajas de ritmos. También en esta época diferentes bandas de rock, desde Genesis hasta The Cars, comenzaron a incorporar sintetizadores en sus arreglos de rock.

En 1979, el músico Gary Numan contribuyó a llevar la música electrónica a un público más amplio con su hit pop "Cars", del álbum "The Pleasure Principle". Otros grupos y artistas que contribuyeron significativamente a popularizar la música creada exclusiva o fundamentalmente de modo electrónico fueron Kraftwerk, Depeche Mode, Jean Michel Jarre, Mike Oldfield o Vangelis.

En 1980, un grupo de músicos y fabricantes se pusieron de acuerdo para estandarizar una interfaz a través de la que diferentes instrumentos pudieran comunicarse entre ellos y el ordenador principal. El estándar se denominó MIDI (Musical Instrument Digital Interface). En agosto de 1983, la especificación 1.0 de MIDI fue finalizada.

La llegada de la tecnología MIDI permitió que con el simple acto de presionar una tecla, controlar una rueda, mover un pedal o dar una orden en un micro ordenador, se pudieran activar todos y cada uno de los dispositivos del estudio remotamente y de forma sincronizada, respondiendo cada dispositivo de acuerdo a las condiciones prefijadas por el compositor.

Miller Puckette desarrolló un software para el procesamiento gráfico de señal de 4X llamado Max, que posteriormente sería incorporado a Macintosh para el control de MIDI en tiempo real, haciendo que la composición algorítmica estuviera disponible para cualquier compositor que tuviera un mínimo conocimiento de programación informática.

En 1979, la empresa australiana Fairlight lanzó el Fairlight CMI (Computer Musical Instrument), el primer sistema práctico de sampler polifónico digital. En 1983, Yamaha introdujo el primer sintetizador digital autónomo, el DX-7, el cual utilizaba síntetis de modulación de frecuencia (síntesis FM), probada por primera vez por John Chowning en Stanford a finales de los años 60.

Hacia finales de los años 80, los discos de música de baile que utilizaban instrumentación exclusivamente electrónica se hicieron cada vez más populares. Esta tendencia ha continuado hasta el presente, siendo habitual escuchar música electrónica en los "clubs" de todo el mundo.

En los años 90, comenzó a ser posible llevar a cabo actuaciones con la asistencia de ordenadores interactivos. Otro avance reciente es la composición "Begin Again Again" de Tod Machover (MIT y IRCAM) para hyper chelo, un sistema interactivo de sensores que miden los movimientos físicos del chelista. Max Methews desarrolló el programa Conductor para control en tiempo real del tempo, la dinámica y el timbre de un tema electrónico.

A medida que la tecnología informática se hace más accesible y el "software" musical avanza, la producción musical se hace posible utilizando medios que no guardan ninguna relación con las prácticas tradicionales. Lo mismo ocurre con los conciertos, extendiéndose su práctica utilizando ordenadores portátiles y "live coding". Se populariza el término "Live PA" para describir cualquier tipo de actuación en vivo de música electrónica, ya sea utilizando ordenador, sintetizador u otros dispositivos.

En las décadas de 1990 y 2000 surgieron diferentes entornos virtuales de estudio construidos sobre "software", entre los que destacan productos como Reason, de Propellerhead, y Ableton Live, que se hacen cada vez más populares. Estas herramientas proveen alternativas útiles y baratas para los estudios de producción basados en "hardware". Gracias a los avances en la tecnología de microprocesadores, se hace posible crear música de elevada calidad utilizando poco más que un solo ordenador. Estos avances han democratizado la creación musical, incrementándose así masivamente y estando disponible al público en internet.

El avance del "software" y de los entornos de producción virtuales ha llevado a que toda una serie de dispositivos, antiguamente solo existentes como "hardware," estén ahora disponibles como piezas virtuales, herramientas o "plugins" de los "software". Algunos de los "softwares" más populares son Max/Msp y Reaktor, así como paquetes de código abierto tales como Pure Data, SuperCollider y ChucK.

Los avances en la miniaturización de los componentes electrónicos, que en su época facilitaron el acceso a instrumentos y tecnologías usadas solo por músicos con grandes recursos económicos, han dado lugar a una nueva revolución en las herramientas electrónicas usadas para la creación musical. Por ejemplo, durante los años 90, los teléfonos móviles incorporaban generadores de tonos monofónicos que algunas fábricas usaron no solo para generar los "Ringtones" de sus equipos, sino que permitieron a sus usuarios algunas herramientas de creación musical. Posteriormente, los cada vez más pequeños y potentes computadores portátiles, computadores de bolsillo y PDA´s, abrieron camino a las actuales "Tablets y" teléfonos inteligentes que permiten no solo el uso de generadores de tono, sino el de otras herramientas como "samplers", sintetizadores monofónicos y polifónicos, grabación multipista, etc. que permiten la creación musical en casi cualquier lugar.





</doc>
<doc id="6702" url="https://es.wikipedia.org/wiki?curid=6702" title="Islam">
Islam

El Islam (en árabe: الإسلام, "") es una religión monoteísta abrahámica cuyo dogma se basa en el Corán, el cual establece como premisa fundamental para sus creyentes que «No hay más Dios que Alá y que Mahoma es el último mensajero de Alá». La palabra árabe "Allah", hispanizada como Alá, significa Dios y su etimología es la misma de la palabra semítica "El", con la que se nombra a Dios en la Biblia. Los eruditos islámicos definen al islam como: «La sumisión a Dios el Altísimo a través del monoteísmo, la obediencia y el abandono de la idolatría». Los seguidores del islam se denominan musulmanes (del árabe "muslim" مسلم, 'que se somete'). Creen que Mahoma es el último de los profetas enviados por Dios y sello de la Profecía. El libro sagrado del islam es el Corán, que según los musulmanes fue dictado por Alá a Mahoma a través de Yibril (el arcángel Gabriel).

Se aceptan como profetas principalmente (pero no limitándose) a Adán, Noé, Abraham, Moisés, Salomón y Jesús (llamado Isa). Además del Corán, los musulmanes de tradición sunita siguen asimismo los hadices y la sunna del profeta Mahoma, que conforman el "Registro histórico de las acciones y las enseñanzas del Profeta". Se aceptan también como libros sagrados la Torá (el Antiguo Testamento de los cristianos), los Libros de Salomón y los Evangelios (el Nuevo Testamento).

El Islam es una religión abrahámica monoteísta que adora exclusivamente a Alá sin copartícipes. Es la segunda religión más grande del mundo, tras el cristianismo y la que tiene mayor crecimiento en términos de seguidores, quienes se estima alcanzan a 1.8 miles de millones o el 24.1% de la población, quienes se conoce como musulmanes. Los musulmanes son la mayoría de la población en 50 países. 

El islam se inició con la predicación de Mahoma en el año 622 en La Meca (en la actual Arabia Saudita). Bajo el liderazgo de Mahoma y sus sucesores, el islam se extendió rápidamente. Existe discrepancia entre los musulmanes y no musulmanes de si se extendió por imposición religiosa o militar, o por conversión de los pueblos al islam.

La palabra "Islām", de la raíz trilítera s-l-m, deriva del verbo árabe "aslama", que significa literalmente ‘aceptar, rendirse o someterse’. Así, el islam representa la aceptación y sometimiento ante Dios. Los fieles deben demostrar su sumisión venerándolo, siguiendo estrictamente sus órdenes y aboliendo el politeísmo. En palabras del arabista Pedro Martínez Montávez:
La palabra está dada por numerosos significados en el Corán. En algunos versos ("ayat", en español aleyas), la calidad del islam como una convicción interna es acentuada: «A quien quiera que Dios se desee dirigir, él ampliará su pecho al islam». Otros versos conectan la palabra "islām" y "dīn" (traducido usualmente como ‘religión’ o ‘fe’): «Hoy, he perfeccionado su religión "(dīn)" para usted; he completado mi bendición sobre usted; he aprobado el islam para su religión». Todavía, algunas facciones describen el islam como una acción de devolver a Dios, más que solamente una afirmación verbal de fe.

Con frecuencia se confunden los significados de palabras o términos como árabe, musulmán, islámico e islamista, que tienen significados distintos; para aclarar los significados de estas palabras pueden consultarse diversas referencias bibliográficas.

La teología de chiitas contiene cinco principios de la religión conocida como Principios de la religión y además de los tres de sunitas creen en dos otros, es decir: "Tawhid" (Monoteísmo), "Nubuwwah" (Profecía), "Maad" (El Día de la Resurrección), "Imamah" (Liderazgo), "Adl" (Justicia).

Según la opinión de sunní la doctrina islámica tiene cinco pilares en su fe que forman parte de las acciones interiores de los musulmanes. Los pilares principales son:


A los cinco pilares de la concepción sunní añaden algunos el sexto pilar del "yihad" o "esfuerzo" en defensa de la fe. En términos estrictamente religiosos, se entiende fundamentalmente como un esfuerzo espiritual interior de cada creyente por vivificar su fe y vivir de acuerdo con ella. A esto se le llama "yihad mayor", mientras que existe un "yihad menor" que consiste en predicar el islam o defenderlo de los ataques. De este último concepto nace la idea de yihad como lucha o guerra que se ha popularizado en todo el mundo. 

Además, conforme al Corán todos los musulmanes tienen que creer en Dios, sus ángeles, sus libros, sus profetas, la predestinación y en la próxima vida.

La creencia en la existencia de Dios es un principio común entre todas las doctrinas divinas, y básicamente, la diferencia substancial y fundamental entre una persona religiosa —cualquiera sea la doctrina que practique— y un individuo materialista, radica en esta cuestión.

Dios en el Corán se nombra a sí mismo como "Allah", nombre derivado de la raíz semítica El. Aunque el término es conocido en Occidente como referencia al Dios musulmán, para los hablantes en árabe (de cualquier religión, incluidos cristianos y judíos) se emplea como referencia a "Dios".
La creencia en Dios dentro del islam consiste en cuatro aspectos:

Dado que se trata del mismo Dios de cristianos y judíos, las cualidades que los musulmanes le atribuyen son básicamente las mismas que le atribuyen aquellos, pero hay diferencias considerables. Es reseñable, sin embargo, que el islam, a semejanza del judaísmo pero alejándose del cristianismo, insiste en su radical unidad "(tawhid)", es decir, que es uno y no tiene diversas personas (como afirma en cambio la mayoría de las corrientes cristianas con el dogma de la Trinidad) en su carácter incomparable e irrepresentable.

El islam se refiere a Dios también con otros noventa y nueve nombres, que son otros tantos epítetos referidos a cualidades de Dios, tales como El Clemente (Al-Rahmān), El Apreciadísimo (Al-'Azīz), El Creador (Al-Jāliq). El conjunto de los 99 Nombres de Dios recibe en árabe el nombre de "al-asmā' al-husnà" o ‘los más bellos nombres’, algunos de los cuales han sido utilizados asimismo por cristianos y judíos o han designado a dioses de la Arabia preislámica. Algunas tradiciones afirman que existe un centésimo nombre que permanece incognoscible, que es objeto de especulaciones místicas, y que se define en ocasiones como el Nombre Inmenso "(ism al-'Azam)", o como el Nombre de la Esencia, figura que existe igualmente en el judaísmo, y que ha tenido una gran importancia en el sufismo. Otras veces, se utiliza simplemente la palabra "rabb" (señor).

Mahoma dijo que Dios tenía 99 nombres; en este versículo del Corán se mencionan algunos:
La palabra Allāh está en el origen de algunas palabras españolas como "" ("law šá lláh", si Dios quiere). En cuanto a la etimología de las palabras "olé" y "hala", el etimólogo Joan Corominas les atribuye un origen de creación expresiva por su similitud con otras expresiones francesas, inglesas y alemanas, mientras para el arabista Emilio García Gómez "olé" derivaría de la exclamación musulmana "Wa-llâh", 'por Alá' o '¡por Dios!'.


Todos los eruditos islámicos dicen que la orden más importante que Dios da al hombre es que este reconozca su absoluta unicidad (en árabe: توحيد Tawhid) y esto significa que lo adore únicamente a Él, y esta adoración no es válida excepto del monoteísta, por lo tanto Mahoma divulgó su mensaje entre hombres que tenían diferentes tipos de adoración: algunos adoraban ángeles, otros adoraban profetas y hombres piadosos, otros adoraban árboles, piedras, y entre ellos había quien adoraba al sol y a la luna. A todos ellos el Profeta les reprendió sus actos invitándolos al islam sin hacer distinción alguna.

La mayoría de los versículos del Corán sobre esta materia enfatizan la Unidad de Dios con respecto a la Creación, las órdenes (la dirección del mundo) y el culto. Primero llaman la atención del hombre al hecho de que solamente Dios es el Creador del mundo. Solamente Él tiene la autoridad soberana sobre el mismo. Luego extraen la conclusión de que solamente El merece ser adorado.


El Corán refuta la teoría de la pluralidad de dioses de la siguiente manera:

Si el mundo fuese a tener más de un creador, las relaciones subsecuentes estarían limitadas a asumir una de las siguientes formas:


Son criaturas que constituyen un intermedio entre Dios y este mundo visible y Dios les encargó los asuntos del mundo de la existencia y la legislación. Los ángeles son siervos honorables que nunca desobedecen a Dios en lo que les ordena. Todo lo que les manda lo llevan a cabo.
La fe en los ángeles dentro del islam consiste en:


La fe en los libros revelados dentro del islam comprende:


El Corán significa en idioma árabe la recitación por excelencia. Es el libro revelado al Profeta Muhammad por el Arcángel Gabriel de parte de Dios Altísimo. 

El Sagrado Corán es el milagro por excelencia de Mahoma, ya que los árabes, especialmente en la época del Profeta, el año 600 de la era cristiana.

Existen numerosas tradiciones y diferentes puntos de vista en cuanto al proceso de compilación del Corán. La mayoría de los musulmanes aceptan lo que indican diversos hadices: el primer califa, Abu Bakr, ordenó a Zaid ibn Zabit compilar todos los auténticos versos del Corán, tal como se preservaban en forma escrita o a través de la tradición oral. La compilación realizada por Zaid, conservada por la viuda de Mahoma, Hafsa bint Umar, y que fue utilizada por 'Uthmān, es la base del Corán actual.

La versión de 'Uthmān organiza las azoras (capítulos) según su extensión, de forma que las más largas se encuentran al comienzo del Corán y las más cortas al final. Hay teorías que indican que este orden no cronológico de las azoras fue establecido por Dios.
El Corán fue escrito originalmente en escritura hijazi, masq, ma'il y cúfica. En un principio, sin vocales, solo con consonantes, siguiendo la técnica de escritura vigente hasta entonces en árabe y en otras lenguas semíticas de la península arábiga. Para evitar posibles desacuerdos en cuanto al contenido de los versos del Corán, se crearon marcas diacríticas que indicaran las vocales o la ausencia de estas, el fonema hamza y la prolongación o geminación de consonantes. En cambio, no tiene signos de puntuación, interrogación o exclamación, pues el idioma árabe contaba con partículas (palabras breves) de interrogación y de énfasis.

La forma del Corán más utilizada actualmente es el texto de Al-Azhar de 1123, preparado por un grupo de prestigiosos eruditos de la Universidad Islámica de Al-Azhar de El Cairo.

La mayor parte de los musulmanes veneran el libro del Corán. Lo envuelven en paños limpios y se lavan las manos antes de los rezos o para leerlo. Los ejemplares coránicos en desuso no se destruyen como papel viejo, sino que se queman o se depositan en "tumbas" para el Corán.

Muchos musulmanes memorizan al menos parte del Corán en su idioma original. Aquellos que memorizan totalmente el Corán son conocidos como "hāfiz". En la actualidad existen millones de hāfiz en el mundo.

Desde el comienzo del islam, la mayoría de los musulmanes consideran que el Corán es perfecto únicamente en la versión árabe en la que fue revelado. Las traducciones son interpretaciones no infalibles del texto original. Muchas versiones actuales del Corán indican la versión original en árabe en una página y la traducción vernácula en otra.

El Corán afirma que Dios mandó un mensajero (profeta) a cada comunidad, llamando adorar únicamente a Dios, y a descreer en todo lo que es adorado fuera de Él.
Cada uno de ellos era veraz, guiado y recto, y obedecieron a Dios en lo que les fue encomendado, ninguno de ellos cambió o alteró su mensaje. Todos ellos eran seres humanos, creaciones de Dios, sin cualidades de divinidad o Señorío, y no pueden responder si se les pide ayuda. El Corán menciona más de veinte profetas, desde Adán hasta Mahoma y llama a Mahoma, «sello de la profecía», creen que Su misión era devolver el mensaje divino a su pureza inicial, como en su momento hizo Jesús de Nazaret o Issah ibn Maryam en árabe ("Issah:" Jesús, "Ibn:" ‘hijo’, "Mariam:" María), a quien Alá en el Corán lo considera como un profeta y no su hijo.

Según lo que sostiene la escuela chiita del islam, el número de profetas es de ciento veinticuatro mil. Entre los que fueron mencionados en el Sagrado Corán, se encuentran: Adán, Noé, Abraham, Ismael, Isaac, Lot, Jacob, José, Job, Moisés, Aarón, Ezequiel, David, Salomón, Jonás, Zacarías, Juan el Bautista, Jesús y Muhammad.

Entre ellos, Noé, Abraham, Moisés, Jesús y Muhammad tuvieron una misión universal y trajeron nuevos códigos de ley y una "Sharîat" (Ley Divina). Ellos son llamados "Ûlûl ‘azm" significando «los poseedores de determinación».

Muhammad Bin Abdullah Bin Abdul-Muttalib Bin Hashem (Año del Elefante/, La Mecca-, Medina. Según los musulmanes, Profeta del islam, Muhammad es uno de los Archiprofetas (Ulul ‘Azm), y el último Profeta divino cuyo principal milagro fuese el Corán. Su invitación fue al monoteísmo y a la moral. La opinión de los musulmanes no es la del creador de una nueva religión, sino como el restaurador de la original, la fe monoteísta de Adán, Abraham y de otros que se había corrompido. En la tradición musulmana, Mahoma se ve como el último y el más grande de una serie de profetas, como un hombre muy cercano a la perfección, poseedor de virtudes en todos los campos de la vida, espirituales, políticos, militares y sociales. Por 23 años de su vida, comenzando a la edad de 40, Mahoma divulgó la recepción de revelaciones de Dios. El contenido de estas revelaciones, conocido como el Corán, era memorizado y registrado por sus compañeros. Durante este tiempo, Mahoma predicó a la gente de La Meca, implorándola para abandonar el politeísmo. Aunque algunos se convirtieron al islam, Mahoma y sus seguidores fueron perseguidos por las autoridades principales de Meca. Después de 13 años de predicación, Mahoma y los musulmanes realizaron la Hégira ("emigración") a la ciudad de Medina (conocida antes como Yathrib) en 622. Allí, con los convertidos de Medina (Ansar) y los emigrantes de La Meca (Muhayirun), Mahoma estableció su autoridad política y religiosa.

La Sunna, libros que contienen la compilación de la vida de Mahoma, es de gran valor para muchos musulmanes, y la creen indispensable para la interpretación del Corán. Esto es debido a que se tiene registrado dentro de ella que el mismo Mahoma les ordenó a sus compañeros que escribieran todo lo que él decía, y conforme al Corán, toman sus palabras como revelación.

De acuerdo con la tradición, Mahoma era una persona de carácter excelente, bien parecido, iletrado y un profeta para toda la humanidad. Es frecuente entre los devotos la creencia en que el hecho de que Mahoma fuera analfabeto es una señal más de que solo pudo recibir el Corán por revelación divina, dada la complejidad del libro.

Según el Corán, Jesús (llamado Isa) fue uno de los profetas más queridos por Dios y, a diferencia de lo que ocurre en el cristianismo, para los musulmanes no tiene carácter divino.

El Corán confirma su nacimiento virginal. Dios purificó a su madre María. Existe un capítulo entero en el Corán llamado "Maryam" (María). El Corán describe el nacimiento de Jesús como sigue:

Jesús nació milagrosamente [sin padre] por orden de Dios quien creó a Adán sin padre ni madre. Dios dijo:

Durante su misión profética, Jesús hizo varios milagros. Dios nos dice que Jesús dijo:

Los musulmanes creen que Jesús no fue crucificado (y mucho menos que murió en la cruz). Era el plan de los enemigos de Jesús el crucificarlo (y matarlo), pero Dios lo salvó y lo elevó hacia Sí. La apariencia de Jesús fue colocada sobre otra persona, y los enemigos de Jesús aprehendieron a este hombre y lo crucificaron, pensando que era Jesús. Dios dijo:

La predestinación forma parte de las creencias islámicas categóricas que nos llegan en el Libro de Dios y la tradición profética, y que son confirmadas por las pruebas e indicios lógicos contundentes.

Los pilares de la creencia de la predestinación en el islam son cuatro:


Creen que todos los acontecimientos sean buenos o malos, beneficiosos o dañinos, ocurren por la predestinación y el designio de Alá, pero que a la vez el ser humano tiene una facultad de elección, mas esta no es total.

Todas las religiones celestiales están de acuerdo en la necesidad de la fe en el Más Allá y la exigencia de la creencia en la Resurrección. 
La muerte no es el final de la vida y la extinción, sino una transición de un estado de existencia a otro. En realidad, es el traslado hacia una vida eterna que damos en llamar «La Resurrección», solo que entre esos dos estados de existencia hay un tercero intermedio que es denominado "Barzaj", y al morir el ser humano es trasladado a ese estado hasta que acontezca la Hora de la Resurrección.

Creen en una vida dentro de la tumba después de la muerte y en su tribulación.
Ellos creen que el tiempo de "Qiyāmah" es predestinado por Dios, pero no fue revelado a los hombres. El juicio y las pruebas precedentes y durante el "Qiyāmah" son descritas en el Corán y el Hadiz, y también en los comentarios de eruditos islámicos, en la retribución y rendición de cuentas ante Dios, que cada individuo recibirá un libro escrito por los ángeles que incluirá una mención completa de todas las obras que realizó el ser humano en la vida terrena, quien lo reciba en la diestra será de los exitosos y quien lo reciba en la mano izquierda será de los perdedores, en el Paraíso y el Infierno, así como en las Señales que indican la llegada de la Última Hora, afirman que la primera era la llegada del profeta Mahoma y entre las últimas es el retorno del profeta Jesús que romperá las cruces y legislará con el islam.

La creencia en "El día de Resurrección", "yawm al-Qiyāmah" (también conocido como "yawm ad-dīn", "El día del juicio final" y "as-sā`a", "La última hora") es asimismo crucial para los musulmanes.
El Corán acentúa la resurrección corporal, una rotura del entendimiento preislámico de muerte. Esto declara que la resurrección será seguida de la reunión de toda la humanidad, culminando en su juicio por Dios.

El Corán hace referencia a varios pecados que pueden condenar a una persona al Jahanam (como la incredulidad, la usura y la falta de honradez). Los musulmanes ven el paraíso, Janah, como un lugar de alegría y dicha, con referencias del Corán que describen sus rasgos y los placeres físicos de dicho lugar. Hay también referencias a una aceptación de mayor júbilo por Dios. Tradiciones místicas en el islam colocan estos placeres divinos en el contexto de una conciencia extática de Dios.

En el Sagrado Corán se ha llamado al «Retorno» de muchas formas, como: «El Día de la Resurrección», «El Día del Cómputo», «El Último Día», «El Día del Resurgimiento» y de otras maneras.

Los pilares del islam según la opinión de Sunita son cinco:


El modo de vida islámico se encuentra basado en una relación personal entre Alá y el creyente, siguiendo la Sharia, en donde la "intención" será el rasgo fundamental que rija todas las acciones del mismo.


La "yihad" (en árabe, ﺟﻬﺎﺩ "yihād": significa ‘esfuerzo’ y ‘lucha en el camino de Dios’, inglés o al francés, "jihad") es considerada «el sexto pilar de islam» por una minoría de autoridades musulmanas. Yihad en su sentido más amplio, es definido clásicamente como «el poder extremo de alguien, esfuerzos, habilidades, o la capacidad en contienda con un objeto de desaprobación». Dependiendo del objeto, que suele ser un enemigo visible y los aspectos cotidianos de uno mismo, las diferentes categorías de la Yihad son definidas:

Cuando es usada sin justificación alguna es entendida en su aspecto militar. También se refiere a los esfuerzos de un fiel por lograr la perfección religiosa y moral.

Hay dos clases de yihad:

La defensa del islam, de los musulmanes o de sus países frente al enemigo externo puede efectivamente adquirir el carácter de lucha militar o "guerra santa", y así se halla en el Corán, donde se anima a combatir contra los infieles si el islam resulta atacado:

La "Sharia" (literalmente: ‘el camino que conduce al abrevadero’), es la Ley Divina, en el sentido de que es la encarnación concreta de la Voluntad Divina que el hombre debería seguir tanto en su vida privada como en sociedad. En cada religión, la Voluntad Divina se manifiesta de un modo u otro y los mandamientos morales y espirituales de cada religión son de origen divino. Pero en el islam, la encarnación de la Voluntad Divina no solo es un conjunto de enseñanzas generales sino concretas. Al hombre se le dice no solo que sea caritativo, sino cómo serlo en circunstancias particulares de la vida. La Sharia contiene los mandamientos de la Voluntad Divina en su aplicación a cada situación de la vida. Es la Ley que Dios quiere que siga el musulmán en su vida. Por lo tanto, es la guía de la acción humana y abarca todas las facetas de la vida humana. Al vivir según la Sharia, el hombre coloca toda su existencia en "manos" de Dios. La Sharia, al tener en cuenta todos los aspectos de la acción humana, santifica la vida entera y le da un significado religioso a las actividades que podrían parecer más mundanales. En el islam, sharia es la expresión del divino destino «y constituye un sistema de deberes que son encargados a un musulmán en virtud de su creencia religiosa».

Los sabios musulmanes la interpretan como: «Los juicios que Dios determina para que el hombre sea feliz en esta vida y en la próxima».

Y los musulmanes la prefieren sobre cualquier sistema por lo siguiente:


Por consecuencia, creen que la diferencia entre la sharîah y los otros sistemas o leyes de los hombres es una diferencia como el Creador y Su Creación.

La ley islámica cubre todos los aspectos de la vida del musulmán. Aquellas leyes islámicas que están expresamente descritas en el Corán se denominan "hudud". Incluyen la prohibición del homicidio, relaciones sexuales extramaritales, consumo de alcohol y juegos de azar. El Corán también detalla leyes relacionadas con la herencia, el matrimonio, la compensación en los casos de homicidio o daños físicos, así como reglas para el ayuno, el azaque y la oración. Los preceptos y prohibiciones son interpretados en la práctica por los eruditos en religión o ulemas.

Según la opinión de chiita Los principios de la Ley contenidos en el Corán fueron explicados y amplificados en el Hadiz y la Sunna del Profeta, que constituyen la segunda fuente básica de la Ley. Éstos, a su vez, fueron entendidos con la ayuda del consenso de la comunidad islámica (iŷmâ‘). Finalmente, estas fuentes de la Ley fueron completadas por el razonamiento humano analógico (qiyâs) donde fue necesario. Según el punto de vista islámico tradicional, por lo tanto, las fuentes de la Sharî‘a son el Corán, el Hadiz, el iŷmâ‘ y el qiyâs, los dos primeros de los cuales son los más importantes y son aceptados por todas las escuelas jurídicas, mientras que los otros dos son considerados de menor importancia o bien son rechazados por alguna de las escuelas.

Otros aspectos legales son dirimidos por los "takzir" o jueces. Se les da el poder de dictar sentencia siempre que se atengan a los principios del Corán y la Sunna («tradición»). La ley islámica es directamente aplicable cuando la constitución del país involucrado así lo establece, como es el caso de Arabia Saudita o Irán. De otro modo, se aplica la legislación sancionada por el Estado, que, según el caso, puede coincidir en mayor o menor medida con la Shariah.

La principal fuente del islam es el Corán. Existe consenso entre todos los musulmanes sobre su autenticidad. En orden de importancia, sigue la "Sunna" o tradición: el conjunto de los hadices, que son dichos y hechos de Mahoma narrados por sus contemporáneos. Estos hadices son transmitidos por fuentes reconocidas y recopilados en distintas colecciones. En ellas se menciona la cadena de personas consideradas dignas de fe que transmitieron cada uno de los dichos o hechos expuestos. La tercera fuente es el consenso de la comunidad (ár. iyma' إجماع).

A diferencia del texto coránico, las colecciones de hadices no son unívocas. Se clasifican según su grado de verosimilitud. Unos son considerados exactos y genuinos; otros, "débiles" y apócrifos. Las distintas escuelas y vertientes a menudo no coinciden sobre la autenticidad de uno u otro hadiz. Hay colecciones que gozan de consenso muy generalizado, al menos dentro de la vertiente suní mayoritaria. Destacan los dos "Sahih", que significa "verdadero": el de Muslim y el de Al-Bujari.

Las colecciones más importantes de la tradición sunita son:


Alrededor del tiempo de estos recopiladores, surgen cuatro escuelas sunitas de interpretación, llamadas "madhhab". Se reconocen mutuamente entre sí. Se denominan "hanafí", por Abu Hanifah, "malikí", por Malik Ibn Anas, "shafi'í", por Al-Shafi', y "hanbalí", por Ahmad bin Hanbal. Estas escuelas tienen diferencias menores en la liturgia y a veces en la jurisprudencia, pero no difieren en lo que podría denominarse el "dogma" o doctrina.

Los libros generales de hadîz compilados que hoy se consideran el eje de referencia de la doctrina y las normas del Shiísmo son:


Éste conforma el segundo conjunto de los compendios del "hadîz" que elaboró y ordenó la "Shî‘ah" a lo largo de la historia mediante sus raudos esfuerzos hasta los siglos cuarto y quinto de la hégira. Como ya hemos mencionado, fueron elaborados compendios de hadices durante la época de los imames en los siglos segundo y tercero, que se denominan «las primeras compilaciones», eso sumado a los "usûl al-arba‘mî’ah" (los cuatrocientos documentos elaborados directamente por los compañeros de los Imames Inmaculados) cuyo contenido fue trasladado al segundo conjunto de los compendios del hadîz.
Desde que la Ciencia del Hadîz fue siempre objeto de atención por parte de la Shî‘ah, de¬bido a ello, en los siglos XI y XII fueron elaboradas otras compilaciones del hadîz que no mencionamos para no extendernos. Las más famosas de estas compilacio¬nes son "Bihâr Al-Anwâr " (Los Mares de Luces) del ‘Al•lâmah Muham¬mad Bâqir Al-Maÿlisî, y "Wasâ’il Ash-Shî‘ah" (Los Medios de la Shî‘ah) de Muhammad Ibn Al-Hasan Al-Hurr Al-‘Âmilî.

Algunos eruditos musulmanes dicen que una nación islámica se basa en cuatro pilares:


En segundo lugar se sitúan los emires o príncipes, y a continuación le siguen el jeque, el alcalde y el imán. El islam no tiene sacerdotes, sino guías religiosos llamados imanes (ár. "imam" —religión—), que generalmente son nombrados por la propia comunidad. Existe de todos modos una serie de sabios, los "ulama", y alfaquíes, que tienen el mismo tipo de autoridad social y religiosa que el clero en otras religiones.

El islam está abierto a todos sin importar la raza, edad, creencias previas o sexo. Es suficiente ser creyente en los principios fundamentales del islam. Esto se realiza atestiguando la unicidad de Dios y la aceptación de Mahoma como profeta de Dios, recitando la "shahada" (testificación), lo cual debe hacerse sin coacción y sinceramente estando presentes otros musulmanes.

Todos los actos que el islam nos ha ordenado hacer, poseen indudables ventajas, y todos los actos que nos veda, tienen claras desventajas. Ninguno de los mandatos islámicos deja de tener una razón válida que los sustente. Por ejemplo, las cosas comestibles y bebibles (lícitas e ilícitas según la ley islámica), las relaciones legales, etc., tienen alguna ventaja o desventaja inherente, ya sea que exista alguna ley que les concierna o no. Las órdenes divinas se basan sobre esas ventajas o desventajas innatas.

Por ejemplo, las bebidas alcohólicas y las substancias narcóticas son dañinas, independientemente de lo que la ley islámica diga sobre ella. De igual manera, la usura es una gran trampa usada para la explotación económica. La adoración a Dios es purificante y vigorizadora. Si los intoxicantes y la usura están prohibidos es porque son malignos. Si el rezo u oración ha sido ordenado se debe a sus efectos benéficos sobre el ser humano.

Ali ibn Abi Talib, el primer imán de los chiitas, escribió una carta a Mâlik Al-Ashtar Al-Naja’í al designarlo gobernador de Egipto y sus provincias cuando el gobierno de Muhammad Ibn Abu Bakr estaba en peligro. Es el más largo compendio de instrucciones (en el "Nahyul Balâgha", "La cumbre de la elocuencia").

Muchos de los gobernadores creen que los consejos son los mejores consejos para los gobernadores.

Entre todas sus cartas ésta es la que contiene el mayor número de buenos consejos. 

Unas partes de sus consejos a su compañero, Mâlik:
¡Mâlik! Debes ser amable, compasivo y amar a tus súbditos. No te comportes (con ellos) como una bestia voraz y rapaz, considerándolos como una presa fácil, pues ellos una de dos: o son tus hermanos en religión, o se equiparan a ti en su creación (como seres humanos). Hombres de una y otra clase padecen de las mismas debilidades e incapacidades que se heredan en la carne, pecan y dan rienda suelta a sus vicios, ya sean intencional o involuntariamente, sin darse cuenta de la enormidad de sus actos. Deja que tu misericordia y compasión los rescate y los ayude de la misma manera que tú esperas que Dios te demuestre su misericordia y su perdón.

¡Mâlik! no debes olvidar jamás, que tú gobiernas sobre ellos, el Califa gobierna sobre ti y Dios es el Señor Supremo sobre el Califa. Y la realidad es que el Señor te ha elegido gobernador y te ha probado dándote la responsabilidad de gobernar. No pienses jamás en elevarte a un prestigio tan vano que te atrevas a declararle la guerra a Dios, porque no podrás evitar Su castigo y Su venganza. No podrás jamás liberarte de la necesidad de Su misericordia y compasión.

No sientas vergüenza de perdonar y olvidar. No te apresures a castigar y, no te enorgullezcas de tu poder de castigo. No te enfades ni pierdas la calma por los errores y fallas de aquellos a los que gobiernas; por el contrario, sé paciente y compasivo con ellos. El enojo y deseo de venganza no te ayudarán en tu administración.

Nunca digas: «Yo detento la autoridad, doy órdenes, y debo ser sumisa y humildemente obedecido». Porque tal pensamiento te trastornará y te hará vanidoso y arrogante, debilitará tu fe en la religión y te hará buscar el apoyo de cualquier otro poder distinto que el de Dios (tal vez, el de tu partido o el de tu gobierno).

Economistas islámicos presentan las siguientes particularidades de su sistema económico:


Si un hombre es rico puede ser el mejor musulmán al igual que el pobre, lo único que los distingue es su obediencia a Dios.

Se puede argumentar que la economía que existe en el islam no llega a ser una teoría económica, siendo solamente un sistema moral que se entiende que lo ofrece a sus creyentes, a quienes le pide que lo sigan.

En cambio, así como el islam exhorta a la gente a ser veraz, honesta, paciente y cordial, refrenándola de la falsedad y las peleas, de la misma manera la exhorta a ayudar al pobre, a no cometer injusticias, a no usurpar los derechos de otros y a no obtener dinero por medios ilegales. Así como ha ordenado el ayuno, la oración y la peregrinación, también ha prescripto el zakat como un acto meritorio compulsivo, para implementar su política de ayuda al pobre.

Todas estas leyes representan los requerimientos morales del islam y apuntan a la elevación moral de los musulmanes. No tienen el sentido de la invención de una teoría económica con vistas a organizar la sociedad.

El islam ha definido los confines de la justicia y ha establecido leyes generales para la vida en sociedad en los distintos campos de la producción, distribución de los bienes y relaciones mutuas. Ha descrito cualquier violación o rechazo de estas leyes y mandatos como injusticia y transgresión.

En el islam cada miembro de la sociedad tiene un conjunto de derechos y deberes. A todo ser humano que acepta esta religión se le exige que oriente su vida de acuerdo con estas reglas.

Es un libro moral se llama "Risalatul Huquq" (Tratado Sobre los Derechos), Ali ibn al-Husayn el cuarto imán de los chiitas en este libro expresó las tareas de los humanos, hay que hace el humano para Dios y la gente y él mismo. 

Algunos de los derechos enumerados en este tratado son:

De una manera general, la ley del islam impone cuatro clases de derechos y deberes en el hombre:




Sostener los vínculos de parentesco es uno de los mayores principios del islam y uno de los rasgos característicos del Derecho islámico.

En numerosas aleyas del Corán la orden de complacer a los padres está ligado después de la complacencia a Dios, Mahoma encomendó ser bondadoso con ellos aunque profesen una religión diferente, y la madre debe ser la primera persona en grado de importancia para el musulmán, debe de tratar bien a los amigos de sus padres y pedir por ellos ya después de su fallecimiento. Desobedecerlos es uno de los pecados mayores. Inclusive antes de partir al yihad tiene que gozar de su autorización.

En el Corán se describe que la vida matrimonial debe ser de la siguiente manera:



Muchas prácticas comprenden la categoría de "adab" islámico o de etiqueta. Esto incluye entre otros el saludo "salamu` alaykum" («la paz sea con vosotros»), diciendo "bismilah" («en el nombre de Alá»), antes de las comidas, y usan solo la mano derecha para comer y beber, respecto al aseo la mano izquierda, como sonarse la nariz. Las prácticas de higiene islámicas principalmente en la categoría de aseo personal y de la salud, como la circuncisión de los varones descendientes. Los rituales islámicos de entierro incluyen el "salat al-Janazah" («la oración fúnebre»), ya que bañan y envuelven el cadáver en un manto blanco y posteriormente lo colocan en la tumba. Los musulmánes, como los judíos, están restringidos en su dieta, y los alimentos prohibidos incluyen productos de cerdo, sangre, carroña y el alcohol. Toda la carne debe proceder de animales herbívoros sacrificados en el nombre de Dios por un musulmán, judío o cristiano, con la excepción del juego que uno tiene de caza o de pesca para uno mismo. La alimentación permisible para los musulmanes se conoce como alimentos halal.

El profeta del islam dijo: 

El creyente come teniendo presente el apetito de su familia, y la familia del hipócrita come teniendo presente la voracidad de él.
Imam Ali ibn Abi Talib, el primer imán de los chiitas dijo:

Empezad por la sal al inicio de vuestras comidas, puesto que si la gente supiera el beneficio de la sal la preferirían como medicamento probado.
Hay una larga lista de recomendaciones sobre el beber y el comer, provenientes de la Sunna o Conducta del Profeta del islam, Muhammad y retransmitidas por los sabios del Islam, como las siguientes escritas por el teólogo iraní Allamah Muhammad Baqir Ibn Muhammad at-Taqi al-Maylisi (1628-1699):


El velo es un valor de acuerdo con la naturaleza del ser humano. El exhibicionismo y el aumento de las relaciones íntimas entre los sexos es una acción en contra de la naturaleza, ya que contradice las peticiones humanas.
Dios, el Supremo en pro de la vida de la pareja formada por el hombre y la mujer y para que ellos puedan administrar sus asuntos en esta vida, ha establecido una orden correspondiente a su naturaleza.

Para los seguidores del islam, el puritanismo en la indumentaria es considerado como una orden de Alá, según establece su libro sagrado, el Corán, en el cual, Mahoma estableció lo que está permitido usar o no para los musulmanes, y aquello que es recomendable y lo que no lo es. Tanto el hombre como la mujer no deben vestir ropas demasiado justas ni provocativas a la vista de los demás, cuando se está frente a personas ajenas a su familia, a excepción de sus parejas.

Está plenamente prohibido que el hombre vista como mujer y viceversa.

Una de las consecuencias más polémicas de indumentaria islámica es uso prescriptivo de prendas femeninas que cubren totalmente el rostro de la mujer y que a veces son rechazadas en los territorios no islámicos por los no musulmanes, como es el caso del velo o el burka.

Algunos defensores del islam responden a esta acusación argumentando que el islam "mira" a las mujeres como si fueran joyas. Afirman buscar su protección de los ojos lujuriosos y de los corazones perversos como es el caso de los violadores, ya que el islam evita los medios que lleven a un perjuicio grave para la sociedad, reduciendo con esto el número de adulterios, la fornicación y las violaciones. Estos argumentos pueden resultar ofensivos para los habitantes de países donde hay minorías islámicas, ya que consideran que van en contra de los derechos de la mujer. Basándose en su moral religiosa, establecen taxativamente que si cualquier hombre desea a una mujer, no tiene otro recurso sino el matrimonio; por ello es el único lazo que hace lícita la unión del hombre con la mujer y permite todo aquello que antes era prohibido, puesto que para el islam el matrimonio es la única vía para que la mujer y el hombre puedan gozar uno del otro.

La palabra "imam" o imán (en árabe, إمام) denota una persona que se mantiene o camina «delante». Para el islam sunita, la palabra se utiliza comúnmente para referirse a una persona que dirige el curso de la oración en la mezquita. También significa que el jefe de una escuela de jurisprudencia ("escuela de pensamiento"). Sin embargo, desde el punto de vista de los chiitas esto es simplemente el básico entendimiento de la palabra en la lengua árabe y, para su uso religioso propio, la palabra «imam» se aplica solo a los miembros de la Casa del Profeta designados como infalible por el imán anterior.

Los chiitas creen que el Profeta del islam fue ordenado por Dios a nombrar a su sucesor, al que la gente debe obedecer. Este sucesor fue el imán Ali Ibn Abi Talib y después de él el imán Hasan y Husayn.

Los chiitas duodecimanos (Izna-´ashari) creen que después del Husayn, nueve de sus descendientes fueron nombrados como sus sucesores. El doceavo de ellos, el imán Mahdi se encuentra oculto a las miradas y un día se levantará para establecer la Justicia en el mundo.
El imán es una personalidad conductora única, y su función entre Dios y sus siervos consiste en preservar los asuntos devocionales de la religión divina, así como los éticos y sociales. Él es por cierto un modelo perfecto para la humanidad y una merced universal (general para todos). Es quien con su capacidad puede conducir el desarrollo humano hacia la perfección; y por esto es obligatorio para el siervo creyente obedecer sus órdenes e imitarlo en todo asunto. Es el ejemplo racional y perfecto para la educación tanto del individuo como de la sociedad en general; y en particular es además alguien cuyos antecedentes biográficos son un ejemplo para una formación excelente del individuo islámico.

La mayoría de los sabios de la escuela sunni sostienen que la Imamah y la jilafah no difieren en nada en cuanto a su significado, pues son términos sinónimos que tienen un significado único, a saber la responsabilidad suprema, social y religiosa, que se le confiere a alguien de parte de la ummah (comunidad islámica universal). Y él (el califa o imán) consigue ese cargo sin importar si se lo selecciona o elige para ello. El sentido es que el califa responde por la licitud de las cuestiones religiosas, y con el poder militar custodia las fronteras del territorio y la seguridad general. Según esto entonces el imán es un jefe ordinario, un simple gobernante dedicado a aplicar la justicia social y la preservación de la seguridad del territorio y nada más; tarea para lo cual desde luego es elegido.

Lo que más distingue a la escuela de Ahlul Bayt de las otras escuelas islámicas es el concepto del Imamah, o la inmediata sucesión del Profeta Muhammad. La escuela de Ahlul Bayt cree que el oficio del imán es un oficio divino —significa que, el imán o califa (líder) tiene que ser designado directamente por Dios, este oficio es tan importante como el de la profecía—. La gente ha sido comandada de este modo por Dios a seguir sucesores específicos (Imames) después del fallecimiento del Profeta. 
La sucesión —Califato o Imamah— es designada solamente por Al.lah, en repetidas ocasiones esto es mencionado en el sagrado Corán. En la escuela de Ahlul Bayt, el califato se refiere no solo al poder temporal y a la autoridad política sobre el pueblo, mucho más, se refiere a la autoridad para ejercerlo. Esta autoridad debe provenir de Dios ya que Él se atribuye el Gobierno y el Juicio a sí mismo.

Entre los chiíes, el término imán, aparte de referirse al guía de una comunidad, es el título que ostentaban los jefes supremos de toda la comunidad chií (el equivalente al califa suní), cargo hereditario cuyo último representante, Muhammad al Mahdi, según la tradición, «desapareció» en el año y vive desde entonces oculto (el mahdi o imán oculto), rigiendo desde la sombra los destinos de la comunidad (creencia sostenida por la mayor parte de los chiíes, denominados imamíes).
Lista de los más renombrados imanes chiíes:


Hay diferentes puntos de vista de acuerdo a la enseñanza del Corán respecto a otras religiones. Existen grupos no musulmanes que enfatizan la siguiente azora que indica:

En cambio, los musulmanes consideran que juzgar al islam en partes es como un lector que, al leer, se tapa un ojo y no quiere leer con el otro, ya que hay textos que reprenden este acto. Además en el Corán, en la vida de Mahoma y en la historia del islam, también hay ejemplos para la misericordia con los no musulmanes.

El islam afirma que todos los profetas han sido musulmanes y que ninguno de ellos afirmó que su religión haya sido el judaísmo o el cristianismo, por lo tanto creen que Abraham no era judío ni cristiano. Asimismo aseguran que Moisés y Jesús predicaron el islam.

De la misma manera el Corán indica en la azora:

Los musulmanes han respetado a los judíos y a los cristianos como «gente del libro», pero aseguran que han abandonado el monoteísmo y corrompido las sagradas escrituras. El islam tolera a judíos y cristianos, pues les está permitido vivir y practicar su religión en territorios musulmanes, aunque tienen que pagar un impuesto especial, la "yizia", sustitutiva del azaque. Está prohibido el uso de la fuerza para convertir al incrédulo al islam.

La apostasía está penada (con la muerte) bajo la ley islámica según se indica en la Sura XVI, 106:
Sin embargo, los no musulmanes sufren persecución en ciertos países islámicos, y así lo muestran determinados informes del Human Rights Watch. Por ejemplo, los Ahmadis en Arabia Saudí o en Indonesia; cristianos y bahaíes en Irán; cristianos en Egipto, cristianos y animistas en Sudán, etc.

La Península Arábiga en las centurias previas a la llegada de Mahoma estaba escasamente poblada por habitantes de habla árabe, la mayoría eran beduinos, pastores nómadas organizados por tribus. Aunque hasta el siglo VII, amplias zonas desérticas en la actualidad, como el territorio de Banu Hanifa en Arabia Central, producían suficiente cereal como para generar asentamientos de agricultores. Tradicionalmente la zona más fértil se situaba en el sur de Arabia, la actual Yemen, conocida como la Arabia feliz. Este potencial agrario permitió trascender la organización tribal y dio lugar a la aparición de auténticos reinos, que desarrollaron toda una infraestructura hidráulica, que permitió cierto desarrollo demográfico en la región. En el norte el comercio también permitió el desarrollo de ciertos centros urbanos, la futura Medina cobró importancia como enclave estratégico entre las rutas caravaneras del norte. La Meca hizo lo propio, pero como centro de peregrinación en torno a la Kaaba. Más al norte, como puentes de contacto entre los grandes imperios en guerra permanente, el Imperio bizantino y el Imperio persa, fueron surgiendo estructuras estatales, los reinos de gassaní y lajmí.

El aumento de la conflictividad bélica entre el norte y sur de Arabia en los años previos a Mahoma, no se debió como sugiere la historiografía tradicional a un enfrentamiento entre quaysíes y yemeníes, árabes del norte y sur respectivamente, sino a enfrentamientos entre las diferentes tribus del norte y del sur, fruto de la presión demográfica. En este contexto, bajo la durísima vida que impone el desierto, la "asabiyya" o solidaridad tribal fue fundamental en la supervivencia y desarrollo posterior del islam. Esto favoreció que la mayor parte de la Península Arábiga se organizase en tribus y clanes, al margen de una estructura estatal (excepto lo mostrado en el párrafo anterior), aunque a través del "Kisrá", un texto generado por la embajada persa en Arabia, muestra cómo va surgiendo, progresivamente, una identidad árabe que se irá superponiendo a la tribal.

En ese tiempo, la mayoría de los árabes eran seguidores de las religiones politeístas, aunque unas pocas tribus seguían el judaísmo, el cristianismo (incluido el nestorianismo) o zoroastrianismo. La ciudad de La Meca era un centro religioso para algunos politeístas árabes norteños, ya que contenía el muro sagrado del Zamzam y un pequeño templo, la Kaaba.

 
La historia del islam comienza en la Arabia en el siglo VII con la predicación del profeta Mahoma, seguida de la violenta conquista de los mayores Estados de la época: el imperio persa sasánida, buena parte del Imperio romano y el reino visigodo.

Omar fue sucedido por Uthman ibn Affan, otro de los primeros seguidores de Mahoma. Bajo Uthman, el Nuevo califato se vio sumido en una guerra civil a la que se le llamó la Fitna, o desorden. Muchos de los familiares y primeros seguidores de Mahoma estaban descontentos con Uthman, porque sentían que estaba favoreciendo indebidamente a sus parientes y actuando menos como un líder religioso y más parecido a un rey. Soldados rebeldes mataron a Uthman y ofrecieron el liderazgo a Ali ibn Abi Talib, el primo y yerno de Mahoma. El período del califato de Ali ibn Abi Talib fue aquel en que asumió directamente la carga y dirección de la comunidad. Fue una etapa extremadamente importante para el Estado Islámico pues intentó, en un lapso de tiempo muy corto, poner en práctica su concepción de gobierno, de la sociedad civil, de los lazos que unían a sus miembros así como de su concepción de la vida doctrinaria, intelectual y espiritual de la comunidad.
Ali murió a manos de un asesino jariyí, y los omeyas reclamaron el califato. Ellos lograron retener el liderazgo de la mayoría de los musulmanes por varias generaciones, pero salvo por un breve período, nunca volvieron a gobernar sobre un imperio islámico no dividido. La fe islámica divergió también, separándose en las principales de la actualidad: los suníes y los chiíes.

En la historia del islam existen diversas dinastías que se disputaron los califatos o el liderazgo del islam y muchos Estados islámicos que ofrecían una mínima o ninguna obediencia al califa.

No obstante, el imperio de los califas abasíes y el de los turcos selyúcidas se contaban entre los más poderosos de su época. Después de la desastrosa derrota de los bizantinos en la batalla de Manzikert en 1071, la Europa cristiana llevó a cabo diversas cruzadas. Tras la Primera Cruzada, los occidentales lograron capturar y gobernar por algún tiempo Jerusalén. Saladino, sin embargo, restableció la unidad islámica en el Oriente Próximo y derrotó a los chiíes fatimíes.

Entre los siglos XIV y el XVII, uno de los más poderosos imperios fue el Imperio de Malí, cuya capital era Tombuctú. Sin embargo, esta cultura estuvo profundamente pautada por la árabe (incluso en el idioma), no siendo realmente original.
En el siglo XVIII, hubo tres grandes imperios musulmanes: el otomano en torno a Turquía, Oriente Próximo, el Mediterráneo y los Balcanes; el safaví en Irán, Iraq, la Armenia histórica, el Cáucaso y Afganistán; y el mogol en el Indostán. En el siglo XIX, estos imperios habían caído bajo la dominación del poder político y económico de Europa. Después de la Primera Guerra Mundial, el remanente del Imperio otomano fue dividido en protectorados o esferas de influencia europeas. El islam y el poder político del islam han experimentado un resurgimiento en el siglo XX, en buena medida gracias al petróleo. Sin embargo, las relaciones entre Occidente y cierto número de Estados de mayoría musulmana siguen siendo precarias cuando no tensas.

Luego de las pérdidas posteriores a la primera guerra mundial, los restos del Imperio otomano son esparcidos con los protectorados europeos. Desde entonces la mayoría de las sociedades musulmanas se han convertido en naciones independientes, y han adquirido prominencia nuevos temas, como la riqueza petrolera y las relaciones con el Estado de Israel.

El "dinar" es la unidad monetaria de diversos Estados del mundo, la mayoría de los cuales de lengua árabe o que antiguamente habían formado parte del Imperio otomano, ya que históricamente fue usado en tierras musulmanas. La palabra "dinar" (دينار en árabe y en persa) tiene el mismo origen que dinero, puesto que deriva del denario romano.

Era una antigua moneda musulmana de oro que se empezó a acuñar a finales del siglo VII en Al-Andalus y que tenía un peso que, según las épocas, oscilaba entre los 3,85 y 4,25g. En sus inicios imitaba los modelos bizantinos, pero pronto adquirió carácter propio y definido, hasta el punto de que fue imitado fuera de los territorios califales.

Estados que usan actualmente el dinar como moneda:


El "dirham" o "dirhem" (en árabe: درهم) era una antigua moneda de plata utilizada en varios puntos del mundo islámico que valía la décima parte del dinar de oro. El nombre "dirham" procede del griego "dracma" (δραχμή). Las monedas actualmente en circulación con este nombre son el dírham marroquí y el dírham de los Emiratos Árabes Unidos.

Si bien el más famoso movimiento del islam en tiempos recientes ha sido el fundamentalismo islámico, existen diversas corrientes liberales que ven como alternativa el alinear al islam con los tiempos contemporáneos.

Este movimiento no está dirigido a cuestionar los fundamentos del islam, sino que trata de aclarar malas interpretaciones o abrir paso a la renovación del islam como un centro moderno de pensamiento y libertad.

Según el World Factbook de la CIA, en el año 2005 el islam era la segunda religión con más seguidores en el mundo, un 19,9% de la población mundial. Es asimismo la religión que está creciendo más rápidamente, hecho atribuible principalmente al mayor crecimiento demográfico en los países musulmanes, así como a las conversiones al islam como religión monoteísta.

La población musulmana se estima que excede los 1200 millones de personas. Solamente el 18% de los musulmanes son étnicamente árabes; otro 20% se encuentra en la región del sur del Sahara en África, y el 30% en el subcontinente indio (sumando los fieles de Pakistán, Bangladés y la India). El país con la población de musulmanes más grande del mundo es Indonesia, con casi 200 millones de fieles. También hay importantes grupos musulmanes en China, Europa, Asia Central y Rusia.

En Europa, Austria fue el primer país en reconocer el islam como una de sus religiones oficiales, mientras que Francia es el país europeo con mayor población de musulmanes: seis millones, que representan un 10% de su población total.

Se dice que esclavos que llegaron a América con los conquistadores españoles introdujeron el islam en esta región, se establecieron en países como Brasil, Venezuela, Panamá y Colombia.

En España hay alrededor de un millón de musulmanes, mientras que la comunidad más grande de musulmanes en Latinoamérica se encuentra en Brasil. En Argentina está localizado el Centro Islámico Rey Fahd que es el más grande de Latinoamérica. En Colombia la mezquita de Omar Ibn Al-Jattab en Maicao, La Guajira; en Caracas existe la mezquita Ibrahim, en México la mezquita Dar as Salam, cerca de la ciudad de México y en el caso de Chile, la mezquita As-Salam en Santiago, la mezquita Mohhamed VI en Coquimbo y la mezquita Bilal en Iquique.

Los lugares santos del islam son tres: las ciudades de La Meca y Medina, así como la Mezquita de Al-Aqsa en Jerusalén.

La Meca es la ciudad a donde los musulmanes por lo menos tienen que peregrinar una vez en su vida si tienen la capacidad de hacerlo, en la Biblia es mencionada como "Padan-aram" (Parán=Mecca), en ella nació Mahoma y se halla Masjid al-Haram, donde rezar en ella se considera como tener la recompensa de 100000 oraciones.
En esta mezquita está localizada la Kaaba, templo construido por el profeta Abraham e Ismael, el Pozo de Zamzam, considerado por milagroso por los musulmanes desde el tiempo en que le fue revelado a Agar, ya que provee a miles de personas en todo el país y cada peregrino bebe de él, En los alrededores se encuentra Mina y el Monte Arafat, donde Mahoma pronunció su sermón de despedida frente a más de 100000 personas y el permanecer ahí está considerado como un pilar en la peregrinación.

Medina es un lugar muy querido por los musulmanes, ya que recibió al profeta Mahoma cuando emigró de La Meca, le dio refugio, recibió y aceptó su mensaje, sus habitantes fueron conocidos como los "Ansar" por haberlo acogido y hacer vencer al islam, temas sobre los que todos los musulmanes están de acuerdo. Mahoma transmitió que en ella se duplica la recompensa de las buenas acciones, una oración en la Mezquita del profeta tiene la recompensa de 1000 oraciones. También dijo que a su entrada hay ángeles que la protegen de las epidemias y que le prohibirán la entrada al Falso Mesías (con el nombre árabe de Al-Dayal) al igual que La Meca.

Mahoma la declaró como sagrada y dijo que expulsa a la mala gente como el fuelle de fragua expele a las impurezas del hierro, y debido a la elevada posición que fue concedida a esta ciudad y a sus habitantes, informó que Dios los defiende y maldice a todo aquel que los amenace injustamente. Aconsejó vivir y morir en ella, dijo que la fe en esta ciudad vuelve como una serpiente vuelve a su cueva. En Medina es donde Mahoma murió y fue enterrado.

Se encuentra en Jerusalén, la tradición musulmana relata que es el lugar donde Mahoma ascendió a los cielos. En el cielo le fueron presentados los profetas y conoció a Abraham, Moisés y Jesús entre otros. Posteriormente se comunicó con Dios interponiéndose una gran luz entre ellos y le fue establecida la oración.

A este acontecimiento se le llama "Al-Israh wa Al-Miray" (‘viaje nocturno y ascensión’), el capítulo 17 del Corán habla de ello y el rezar en la Mezquita de Al-Aqsa equivale a la recompensa de 500 oraciones.

Tanto los chiitas como los suníes comparten una cierta veneración y obligaciones religiosas hacia ciertos santuarios y lugares sagrados, como La Meca, Medina y Mezquita de Al-Aqsa pero la mezquita del Imán Alí y la mezquita del Imán Hussein también son veneradas. Después de La Meca y Medina, Nayaf y Kerbala son las ciudades más sagradas para los chiitas.

Erróneamente se piensa que el verde es el color del islam, pero esto no es cierto; más adelante se explicará el origen de esta confusión. Creen que la adoración a símbolos u objetos materiales va en contra del monoteísmo. Mucha gente piensa que la estrella y la luna creciente simbolizan el islam, pero esto tampoco es cierto. Eran, simplemente, el símbolo del Imperio otomano y no del islam. El color verde también se asocia frecuentemente con el islam por costumbre, sin que tenga significado religioso alguno. Sin embargo, los musulmanes a menudo usan azoras caligrafiadas para decorar las mezquitas o sus casas propias.

El panarabismo tradicionalmente ha utilizado el rojo, el blanco, el verde y el negro en las banderas de diversos países de población mayoritariamente musulmana, por lo que dichos colores a veces se confunden con los colores del islam. Estos colores pueden observarse en las banderas de Yemen, Egipto, Sudán, Irak, Siria, Sahara Occidental y Palestina. El color rojo simboliza la sangre de los mártires y también fue el color de la dinastía Hachemí. El color blanco fue empleado por la dinastía de los omeyas y el verde por el califato fatimí. El negro fue el color del califato abasí. Su único símbolo, usado en guerras, es la media luna.

El calendario islámico comienza con la Hégira, es decir, la emigración de Mahoma de La Meca a Medina. Ese año equivale al 622 del calendario gregoriano. Los años del calendario lunisolar pueden tener 354 o 355 días. Por eso, para establecer un año islámico, no basta con restar 622 años al calendario gregoriano.

Los días festivos islámicos, basados en el calendario lunisolar, se celebrarían en distintas fechas cada año si los lleváramos al calendario gregoriano.

Los musulmanes tienen dos festividades: Eid al-Fitr (en árabe: عيد الفطر, ‘banquete de caridad’) y Eid al-Adha (en árabe: عيد الأضحى, ‘celebración del sacrificio’), otros agregan el día viernes.





Estas dos festividades las celebran los creyentes visitando los hogares y comiendo los platos especiales cocinados para esta ocasión. Todos se sientan juntos. Por tradición los niños reciben regalos, las gratificaciones y los dulces entregados por sus seres queridos como símbolo de amor. La forma de desear una feliz fiesta es pronunciando las palabras: ¡Eid Mubarak!

La Arquitectura islámica es un término amplio que agrupa los estilos religiosos propios de la cultura islámica desde los tiempos de Mahoma hasta nuestros días, influenciando en el diseño y construcción de edificios y estructuras por todo el mundo. Asimismo, la arquitectura islámica manifiesta la adaptación del estilo arábigo a las culturas y técnicas con las que toma contacto, desde el vasto aporte helenístico y bizantino en Oriente Próximo, África y Anatolia, hasta el visigótico en al-Andalus, o el hindú al Oriente.

La arquitectura islámica de todos los períodos y de todas las regiones tiene un rasgo distintivo: la armonía y la belleza. El dicho del Mensajero de Dios, Muhammad: «Dios es Bello y Ama la Belleza», habla de la fuerte ligazón que existe en el islam entre arte y espiritualidad.

Los tipos principales de construcciones de la arquitectura islámica son la mezquita, la tumba, el palacio y el fuerte; aunque también destacaron edificaciones de menor importancia como los baños públicos, las fuentes y la arquitectura doméstica.

Se dice que la columna, el arco y la cúpula son la «Santísima Trinidad» de la arquitectura islámica, ya que las tres juntas son características que le dan belleza y originalidad.

En 630 el ejército de Mahoma reconquistó la ciudad de La Meca para la tribu de Quraish. El santuario santo de Kaaba fue reconstruido y dedicado al islam; la reconstrucción fue llevada a cabo antes de la muerte de Mahoma en 632 por un náufrago carpintero abisinio en su estilo nativo. Este santuario estuvo entre los primeros trabajos de gran envergadura del islam. Las paredes fueron decoradas con pinturas de Jesús, María, Abraham, profetas, ángeles y árboles. Después las doctrinas del islam a partir del siglo VIII, basadas en el Hadiz, prohibieron el uso de ese tipo de imágenes en su arquitectura, especialmente seres humanos y animales.

En el siglo VII las fuerzas musulmanas conquistaron extensos territorios. Una vez que se establecían en la región, primero necesitaban un lugar donde construir una mezquita. El diseño simple, basado en la casa del profeta Mahoma, proveyó de elementos que fueron incorporados a las nuevas mezquitas y otras construcciones por los primeros musulmanes, o lo adaptaron a edificios ya existentes como iglesias para su propio uso.

La caligrafía árabe está asociada con el arte geométrico islámico del arabesco en las paredes y también en los techos de las mezquitas, así como en los textos escritos. Muchos artistas contemporáneos en el mundo islámico dibujan basándose en la herencia de la caligrafía árabe para utilizar inscripciones y abstracciones caligráficas en su trabajo.

El árabe engloba en un solo término ("jatt") las nociones de escritura y las de caligrafía, hecho que se explica por el carácter sagrado de una lengua que es la del Sagrado Corán. Pocas civilizaciones han llevado el arte de la caligrafía a un rango tan elevado como lo han hecho los musulmanes. 

La caligrafía ha comenzado a ser la más venerada forma de arte islámico porque constituye un enlace entre la lengua de los musulmanes y su religión. El libro sagrado del islam, el Corán, ha jugado un rol muy importante en el desarrollo y evolución de la lengua árabe, y por extensión, en la forma de escribir el alfabeto árabe, es decir, en su caligrafía. Proverbios y amplios pasajes del Corán siguen siendo fuentes activas para la caligrafía islámica.

En el islam hay diferentes denominaciones religiosas que son esencialmente similares en la creencia, pero tienen diferencias teológicas y legales importantes. Las mayores ramas del islam son los suníes (o sunitas) y los chiíes. El sufismo no es una rama, sino una derivación esotérica del islam. Distintas cofradías y órdenes practican esta versión del islam. El sufismo, si bien está asociado al islam como mística, es una corriente considerada anterior al islam, y que de algún modo entroncó con este.

Cerca del 90% de los musulmanes son suníes (solo son minoría frente a los chiíes duodecimanos en Irán, Irak y Líbano). Creen que Mahoma fue un profeta, un ser humano ejemplar y que deben imitar sus palabras y actos en la forma más exacta posible, pues el Corán indica que el profeta Mahoma es un buen ejemplo a seguir. Los hadices describen sus palabras y actos, constituyendo el principal pilar de la doctrina suní.

Los musulmanes chiíes, la segunda rama mayor del islam, difieren de los suníes en que rechazan la legitimidad de los tres primeros califas. Siguen los preceptos de hadices diferentes a los de los suníes y tienen sus propias tradiciones legales. Los eruditos chiíes tienen mayor autoridad que los suníes y mayor amplitud para la interpretación del Corán y de los hadices. Los imanes desempeñan un papel fundamental en la doctrina chií. La principal vertiente chií es la escuela ya`farí (llamada así en honor de su fundador, Ya`far as-Sadiq) o escuela chií duodecimana, cuyo nombre deriva de los doce imames o líderes infalibles que reconocen después del fallecimiento de Mahoma. Aun así, todos tiene en común la creencia en Ali ibn Abi Talib, o Imam Ali como el sucesor del Profeta, y la sucesión de este último por sus dos hijos: primero su hijo el Imam Hasan y luego su hijo Husayn.
Los chiitas existen en la mayoría de los países islámicos, pero la mayoría de ellos se encuentran en Irán, Irak, Azerbaiyán, India, Paquistán y el Líbano.

En sentido no estricto, se denomina también chiíes a sectas tales como las del grupo ismailí, entre ellas los seguidores del Aga Jan, localizados principalmente en el Subcontinente Indio, los alawitas de Siria, los zaídes del Yemen, etc.

El sufismo es una práctica que tiene seguidores entre los suníes y los chiíes. Según la mayoría de los autores suníes, es el camino de la práctica del tercer aspecto del islam, el "ihsan" o perfección espiritual. Por otro lado, puede decirse que su objetivo es el esfuerzo por adquirir las características del siervo o ser humano perfecto ("insan al-kamil" o "abd al-kulli"). Enfatizan varios aspectos espirituales, como el perfeccionamiento de la fe, el estado de rememoración divina continuo ("dhikr"), la purificación del ego ("nafs") a través de determinadas prácticas espirituales. La mayoría de sus seguidores se organizan en cofradías (tariqa en árabe) sufíes. No obstante, hay algunas de ellas que no pueden incluirse dentro de esas dos ramas, como es la bektashi u otras, como las de aparición en Europa y América, que pertenecen a movimientos "new age".

El sufismo está presente en el mundo islámico desde su Occidente, en países como Senegal, hasta su Oriente, como por ejemplo Indonesia, así como en países europeos o americanos.

Los jariyíes o jariyitas (en árabe خارجي plural خوارج, "jāriyī", plural "jawāriy") son una de las tres ramas principales del islam, junto a la de los chiíes y los suníes.

La palabra "jariyí" significa «el que se sale», en referencia a la deserción que protagonizaron en el año 657 cuando abandonaron el bando de Ali Ibn Abi Talib al aceptar este en el campo de batalla de Siffín un arbitraje entre él y su adversario, el omeya Muawiya.

A diferencia de los suníes, que consideraban que el califa debía ser un árabe miembro de la tribu de Quraish, y de los chiíes, que consideraban que debía ser Ali o un descendiente directo suyo, los jariyíes pensaban que la dignidad califal emana de la comunidad, que debe elegir libremente al más digno «aunque sea un esclavo negro».

Hoy en día, continuada tan solo por los ibadíes de Omán, donde son mayoría, y prácticamente extinta en el resto del mundo islámico.

Lista de musulmanes por países.

Nota importante: La población contada por afiliación religiosa, como la mayoría de las características demográficas de la población, están basadas en ciencias estadísticas, sujetas a error observacional y están técnicamente referidas como estimaciones. El porcentaje de población musulmana en cada país fue extraída del estudio del Departamento de Estado de los Estados Unidos titulado "International Religious Freedom Report 2004" . Otras fuentes usadas fueron el CIA World Factbook y Adherents.com . En algunos casos de estimaciones conflictivas, se ha calculado el promedio entre las estimaciones más altas y más bajas. La población total de cada país fue tomada de census.gov (estimaciones del 2005).


Estos procentajes fueron calculados usando las cifras siguientes. El primer procentaje, cuarta columna, es el porcentaje de la población que es musulmana en una región (Musulmanes en la región * Población total de la región). La última columna muestra la población musulmana comparada a la población musulmana total del mundo (Musulmanes en la región * Población musulmana total del mundo). 

"(Nota: Egipto, Sudán y los países del Magreb se cuentan como parte de África del Norte, no de Oriente Medio.)"

Top sesenta por población en la izquierda y por porcentaje en la derecha.

El islam ha sido criticado desde sus etapas formativas. Las primeras críticas escritas procedían de los cristianos, antes del siglo IX, que veían al islam como una herejía del cristianismo. Los objetivos de la crítica incluyen:

Desde noviembre de 2013, en Angola el islam está catalogado como «secta peligrosa» junto a otras 200 cultos religiosos y está prohibido. Desde entonces han sido cerradas o derrumbadas 87 mezquitas.

Sobre el islam:

Sobre las instituciones islámicas:

Sobre la práctica del islam en distintos países:


Estudios empíricos sobre opiniones de los musulmanes en el mundo:


</doc>
<doc id="6707" url="https://es.wikipedia.org/wiki?curid=6707" title="Carica papaya">
Carica papaya

Carica papaya, es una especie de planta arbustiva del género "Carica" en la familia Caricaceae. Su fruto se conoce comúnmente como papaya, papayón, olocotón, papayo en Canarias, lechosa en República Dominicana y Venezuela, fruta bomba en Cuba, melón papaya, melón de árbol, mamón en Paraguay.

"Carica" del griego “karike”, nombre de una higuera. Papaya, deriva del maya “páapay-ya” que significa zapote jaspeado. Pertenece a la familia de las Caricaceae. La planta de papaya es una especie arborescente perennifolia.

Se trata de un arbusto herbáceo de tronco generalmente no ramificado (sólo se ramifica si dicho tronco es herido), cultivado presenta una altura entre 1,8 y 2,5 m coronado por un follaje de hojas largamente pecioladas. El mismo conserva aún en los individuos maduros una textura suculenta y turgente, escasamente leñosa, y presenta numerosas cicatrices características, producto del crecimiento y caída consecutivas de las hojas. La savia es de consistencia lechosa (de aquí su nombre de «lechosa»), y tóxica en estado natural para el ser humano, pudiendo producir irritaciones alérgicas con el contacto con la piel. Esta savia lechosa contiene una enzima muy útil, la papaína, empleada como ablandador de carnes: en las parrillas o barbacoas se emplea el jugo que fluye al cortar la corteza de la papaya verde para rociarlo sobre la carne a la cual deja sumamente tierna y jugosa.

Las hojas son alternas, aglomeradas en el ápice del tronco y ramas, patentes, de 25-75 cm de diámetro, lisas, más o menos profundamente palmeadas con venas medias robustas, irradiantes; la base es profundamente cordada con lóbulos sobrepuestos; hay de 7-11 lóbulos grandes, cada uno con la base ancha o un tanto constreñido y acuminado, ápice agudo, pinatinervado e irregularmente pinnatilobada. El haz es de color verde oscuro o verde amarillo, brillante, marcado en forma visible por las nervaduras hundidas de color blanco amarillento y las venas reticuladas; por debajo es de color verde amarillento pálido y opaco con nervaduras y venas prominentes y visibles; el pecíolo es redondeado de color verde amarillento, teñido con morado claro o violeta, fistular, frágil, de 25-100 cm de largo y 0,5-1.5 cm de grueso.

Los arbustos de papayo tienen tres clases de pies diferentes; unos con flores femeninas, otros con flores hermafroditas y otros con flores masculinas. Las flores femeninas tienen un cáliz formado por una corona o estrella de cinco puntas muy pronunciada y fácil de distinguir. Encima de éste se encuentra el ovario, cubierto por los sépalos; éstos son cinco, de color blanco amarillo, y cuando muy tiernos, ligeramente tocados de violeta en la punta; no están soldados. Los estigmas son cinco, de color amarillo, y tienen forma de abanico. Los frutos de este pie son grandes y globosos. Las flores hermafroditas tienen los dos sexos y el árbol que las posee tiene a su vez tres clases de flores diferentes. Una llamada pentandria, parecida a la flor femenina, pero al separar los pétalos se aprecian cinco estambres y el ovario es lobulado. Los frutos de esta flor son globosos y lobulados. Otro tipo de flor es la llamada elongata y tiene diez estambres, colocados en dos tandas; la flor es alargada y de forma cilíndrica, al igual que el ovario, dando frutos alargados. El último tipo de flor es la intermedia o irregular, no es una flor bien constituida, formando frutos deformes.

Las flores masculinas crecen en largos pedúnculos de más de medio metro de longitud y en cuyos extremos se encuentran racimos constituidos por 15 - 20 florecillas. Las flores están formadas por un largo tubo constituido por los pétalos soldados, en cuyo interior se encuentran 10 estambres, colocados en dos tandas de a cinco cada una. La flor tiene un pequeño pistilo rudimentario y carece de estigmas. Estas flores no dan frutos, pero si lo hacen son alargados y de poca calidad. Los frutos y las flores se desarrollan en racimos justo debajo de la inserción de los tallos de las hojas palmeadas.

Los frutos poseen una textura suave y una forma oblonga, y pueden ser de color verde, amarillo, naranja o rosa. Pudiendo pesar hasta 9 kg, en la mayoría de los casos no suelen pesar más de 500 o 600 g, especialmente en una variedad de cultivo de plantas enanas, muy productivas y destinadas generalmente a la exportación, por su mayor duración después de la cosecha y antes de su consumo. La talla de los frutos disminuye en función de la edad de la planta. Baya ovoide-oblonga, piriforme o casi cilíndrica, grande, carnosa, jugosa, ranurada longitudinalmente en su parte superior, de color verde amarillento, amarillo o anaranjado amarillo cuando madura, de una celda, de color anaranjado o rojizo por dentro con numerosas semillas parietales y de 10 - 25 cm o más de largo y 7-15 cm o más de diámetro. Las semillas son de color negro, redondeadas u ovoides y encerradas en un arilo transparente, subácido; los cotiledones son ovoide-oblongos, aplanados y de color blanco.

No es planta exigente en cuanto a suelos, pudiendo desarrollarse en cualquier terreno abandonado o incluso en alguna maceta grande. Es una de las plantas más productivas con relación a su tamaño ya que siempre tiene flores y frutos al mismo tiempo. El desarrollo de los frutos produce la caída de las hojas inferiores, por lo que quedan siempre al descubierto por debajo de las hojas.

Es una especie originaria de Mesoamérica. En México se distribuye por el Golfo desde Tamaulipas hasta la Península de Yucatán, por el Pacífico se le encuentra desde Baja California a Chiapas. En la actualidad la encontramos cultivada en todas las regiones tropicales de América, desde México a Argentina y Brasil, naturalizada en los trópicos del Viejo Mundo.

Es conocido desde muy antiguo que las plantas, al igual que cualquier otro ser vivo, no se distribuyen uniformemente en la superficie terrestre y que cada una de ellas ocupa unos territorios determinados. "Carica papaya" es una planta de origen centroamericano, conocida y empleada en casi toda América desde hace varios siglos, aunque hoy día se cultiva en muchos países de otros continentes, principalmente, en Asia y África. Antes de la llegada de los europeos, en México se le daba el nombre "chichihualtzapotl", que en náhuatl significa «zapote nodriza», y era un fruto especialmente relacionado con la fertilidad.

Originaria de los bosques de México, Centroamérica y del norte de América del Sur, la planta de la papaya se cultiva en la actualidad en la mayoría de los países de la zona intertropical del orbe, siendo los primeros países productores: India, Brasil, Indonesia, Nigeria y México (datos de FAO, 2013).

La especie presenta dioecia naturalmente, pero la selección artificial ha producido especímenes hermafrodita América Central (Sur de México). Actualmente se cultiva en Florida, Hawái, Costa subtropical de Granada (España), Islas Canarias, África Oriental, Sudáfrica, Ceilán, India, Argentina, Colombia, Ecuador,Paraguay Perú, Chile, Venezuela, Archipiélago Malayo y Australia.

Puede crecer en lomeríos y cañadas, prospera en toda la tierra caliente en un clima tropical o subtropical, desde el cálido más seco de los subhúmedos hasta la variante húmeda del clima subhúmedo. La humedad y el calor son condiciones esenciales para su buen desarrollo y fructificación. Crece y se desarrolla desde el nivel del mar hasta los 1500 msnm. La precipitación media es de 1,500 a 2500 mm anuales y la temperatura media anual de 20 a 25º C. Desarrolla en diferentes clases de suelo siempre que sean fértiles, blandos, profundos y permeables con un pH de 5.5 a 6.
Esta excelente fruta es originaria de las zonas tropicales de México y Centroamérica. Se cultiva en terrenos de muy distinta naturaleza, pero es fundamental que éstos sean ricos en materia orgánica y que contengan una humedad abundante. El papayo es una planta tropical, puede cultivarse desde el nivel del mar hasta los 1000 metros sobre el nivel del mar, pero los frutos de mejor calidad y los rendimientos más altos se obtienen en altitudes por debajo de los 800 metros. A continuación se analizan los factores climáticos más importantes que influyen de manera decisiva en el desarrollo de este cultivo, así como las características principales que debe tener un suelo para que el cultivo produzca de manera exitosa.Temperatura
La humedad y el calor son las condiciones esenciales para el buen desarrollo del papayo. Requiere zonas de una pluviometría media de 1800 mm anuales y una temperatura media anual de 20-22 ºC; aunque puede resistir fríos ligeros, si no tiene la cantidad suficiente de calor, se desarrolla mal y los frutos no llegan a madurar. No se debe cultivar en áreas propensas a heladas o a temperaturas por debajo de la de congelación ya que éstas provocarían la muerte del vegetal. Las noches frescas y húmedas ocasionan que la fruta madure lentamente y resulte de mala calidad. En cuanto al viento, lo soporta bien ya que su tallo es muy flexible y a él se le unen los pecíolos de las hojas y los pedúnculos de las flores, siendo difícil que se desprendan. Los fuertes vientos pueden dañar algunas hojas pero no flores ni frutos.
Papaya_Maradol_06La temperatura es el factor climático limitante, que permite que este frutal se desarrolle, o no. El rango de temperatura es entre 22° y 30°C, pero su óptima es entre 23° y 26°C. Las temperaturas bajas inhiben su crecimiento y las temperaturas altas le provocan abscisión floral y bajas en la producción. Canículas y sequías especialmente en la floración ocasionan su caída y la planta llega a suspender su crecimiento.
Humedad
El agua es el contribuyente principal de la planta; alrededor del 85% está compuesta por agua. La papaya, tanto en el proceso de germinación, vivero y primeros meses después de plantada, necesita para su crecimiento y desarrollo una gran cantidad de agua, por lo cual en esta fase se deben realizar riegos semanales. En la época seca y cuando la lluvia no es adecuada, se debe recurrir al riego para mantener las plantas con un buen desarrollo.
Luz
La papaya necesita abundante luz debido a su gran actividad fotosintética. Es imposible desarrollar plantaciones con restricciones de luz, pues las plantas serían alargadas y amarillas, sintomatología esta de desnutrición, lo que trae como consecuencia un inadecuado desarrollo de las plantas.
Viento
Por ser una planta herbácea de pecíolos largos, tiende a ser sensible a la acción de los vientos. Por tanto si se cultiva en zonas donde se presenten vientos fuertes, son necesarias las barreras rompevientos, usándose de preferencia arboles nativos (guasima o caulote) y vegetación con propiedades que tienen la función de hacer una barrera de retención de plagas como lo es el paraíso y el nim.
Suelos
Las principales características que debe reunir un suelo para este cultivo son las siguientes:
Suelto y húmedo.
Con buen drenaje.
Alto contenido de materia orgánica.
Un pH que fluctúe entre seis y siete.
Suelos fértiles y profundos.
El suelo también puede ser mejorado, por lo cual no es de los factores más preocupantes cuando se planifica una plantación.
El papayo se desarrolla en cualquier tipo de suelo siempre que sean suelos ligeros, fértiles (ricos en humus), blandos, profundos y permeables. Al tener sus tallos y raíces blandos y esponjosos, debe cultivarse en terrenos con perfecto drenaje, ya que en suelos demasiado húmedos y compactos, se pudrirán las raíces.
Altitud
Las mejores producciones se presentan entre los 0 y 800 msnm. Y la mayor parte de la tierra agrícola en el municipio de Buenavista se encuentra entre los 300 msmn y los 800 msnm, por lo que nuestra producción está en la altura optima para el desarrollo de nuestra producción de papaya.

Nativa del sur de México y América Central, extendida en gran parte del mundo en las regiones tropicales y subtropicales. En Mesoamérica se encuentra de forma silvestre y cultivada, en el resto de otros países solo está presente de forma cultivada. No es una especie que se ubique en alguna categoría de la norma 059 de la SEMARNAT de México. 

La papaya es conocida como fruta de consumo, tanto en forma directa como en jugos y dulces (elaborados con la fruta verde cocinada con azúcar), y tiene unas magníficas propiedades para facilitar la digestión de alimentos de difícil asimilación, debido a su alto contenido de papaína. De esta enzima llamada papaína se producen más de 1000 toneladas anuales en el mundo entero. La utilidad de dicho producto derivado está en la fabricación de cerveza, cosméticos e industria alimenticia.

Es eupéptico-digestivo, coadyuvante de la cicatrización; antiinflamatorio, antihelmíntico.
Las semillas son vermífugo, emenagogo. Especialmente interesantes contra ancylostomas, áscaris, trichuris y strongyloides.

Indicado para dispepsias hiposecretoras. Prevención de la arteriosclerosis y tromboembolismos. Parasitosis intestinales. Tópicamente es usado para heridas y ulceraciones tróficas con restos inflamatorios o necróticos, forúnculos.

Al manipular la papaína en polvo se deben proteger los ojos, por la posibilidad de producción de ulceraciones corneales, debidas a su acción queratolítica.

Se usa el látex, obtenido por incisión de los frutos.

Es uno de los frutos más importantes y de mayor consumo. Muy apreciada por sus propiedades nutritivas y su delicado sabor. Ideal para regímenes, por contener vitaminas B1, B2 y Niacina o B3, todas del Complejo B, que regulan el sistema nervioso y el aparato digestivo; fortifican el músculo cardíaco; protegen la piel y el cabello y son esenciales para el crecimiento. Contiene también vitaminas A y C, es rica en minerales como calcio, fósforo, magnesio, hierro, azufre, silicio, sodio y potasio. Por otra parte tiene bajo valor calórico, cerca de 40 calorías por cada 100 gramos de fruta. El contenido de fibra mejora la digestión. Tiene propiedades astringentes.

Asimismo, su cáscara contiene una sustancia, la papaína, que tiene múltiples usos. La papaya también es un fuente de licopeno, conteniendo unas 1800 μg cada 100 g.

El fruto es usualmente consumido crudo, sin su cáscara o sus semillas. El fruto verde inmaduro de la papaya puede ser consumido en ensaladas y estofados. Posee una cantidad relativamente alta de pectina, la cual puede ser usada para preparar mermeladas.

La papaya verde es usada en la cocina Thai ya sea cruda o cocinada. En la cocina criolla de Venezuela se usa la lechosa verde para hacer dulce de lechosa secando las tajadas primero y luego cocinándolas en un melado de papelón.

Las semillas negras tienen un sabor fuerte pero son comestibles. Algunas veces son molidas y usadas como sustituto de la pimienta negra. En algunas partes de Asia las hojas jóvenes de la papaya son hervidas y consumidas como espinaca. En algunas partes del mundo las hojas son preparadas como té para ser consumidas como prevención de la malaria, aunque no existe evidencia científica real de la efectividad de este tratamiento.

En Cuba es costumbre consumirla madura (muchos le agregan azúcar) pero como también se elaboran dulces con ella, se emplean las maduras y las pintonas (ni verdes ni maduras).

La zona de la Región de Coquimbo del centro norte de Chile (Ovalle, La Serena), es famosa por su producción de papayas chilenas o "Chilean carica", de la variedad "Carica candamarcensis" o "Pubescens", que se caracteriza por ser muy aromática, de color amarillo, tamaño pequeño, piel delgada y de alto valor nutritivo-funcional: enzima papaína que complementa la digestión y asimilación de proteínas, eliminación de toxinas del sistema digestivo, fibra para la eliminación de azúcar, vitamina C como antioxidante.

En el noreste argentino y sur de Paraguay es muy común consumir la papaya o "Mamón" (como se denomina en la zona) cruda, con un poco de azúcar o preparada en almíbar, en un proceso de hervido con azúcar y bicarbonato por varias horas. El producto final es exquisito y se acompaña con algún queso en el postre. Cabe destacar que dicho producto también se consume en Venezuela (donde se le da el nombre de "dulce de lechosa") mayormente en época de Navidad. Se sustituye el azúcar por panela (papelón) y se añaden clavos de olor.

Se realiza mediante esquejes obtenidos de las ramificaciones del arbolito de forma artificial, ya que el papayo no se ramifica hasta que tiene tres o cuatros años. Los árboles viejos sufrirán la operación de desmoche o eliminación de la cabeza o cogollo del árbol, provocando así la producción de ramas o cogollos laterales.
Los esquejes serán los brotes de 25-30 cm que se cortan y se cauterizan con agua caliente a unos 50 °C. Estos esquejes se plantan en macetas que se colocan en lugares protegidos de los rayos solares y con humedad hasta la emisión de raíces.
Este método de propagación es muy laborioso y costoso ya que implica el mantenimiento de plantaciones de más de tres años para la obtención de plantas madre.

Propagación por semilla.
Es la forma más económica y fácil de propagar el papayo. Se obtendrán distintos resultados, según se empleen semillas procedentes de árboles femeninos fecundados con papayos masculinos o semillas procedentes de árboles femeninos y hermafroditas.
El poder germinativo de las semillas del papayo suele ser corto, por lo que se hará una siembra lo más cerca posible a la época de recolección. Esta siembra puede ser directa sobre el terreno o previa en semillero. La siembra en semillero se hará empleando macetas de turba y plástico negro de 10 cm de diámetro y 15 cm de profundidad.
La tierra del semillero deberá mantenerse húmeda, cuando las plantitas tengan unos 10-15 cm (unos dos meses después de la siembra) de altura se trasplantarán al terreno de cultivo

"Carica papaya" fue descrita por Carlos Linneo y publicado en "Species Plantarum" 2: 1036. 1753.

Recibe este fruto distintos nombres en América así:




</doc>
<doc id="6710" url="https://es.wikipedia.org/wiki?curid=6710" title="Johann Wolfgang von Goethe">
Johann Wolfgang von Goethe

Johann Wolfgang von Goethe (; Fráncfort del Meno, 28 de agosto de 1749-Weimar, 22 de marzo de 1832) fue un poeta, novelista, dramaturgo y científico alemán, contribuyente fundamental del Romanticismo, movimiento al que influyó profundamente.

En palabras de George Eliot (1819-1880) fue «el más grande hombre de letras alemán... y el último verdadero hombre universal que caminó sobre la tierra». Su obra, que abarca géneros como la novela, la poesía lírica, el drama e incluso controvertidos tratados científicos, dejó una profunda huella en importantes escritores, compositores, pensadores y artistas posteriores, siendo incalculable en la filosofía alemana posterior y constante fuente de inspiración para todo tipo de obras. Su novela "Wilhelm Meister" fue citada por Arthur Schopenhauer como una de las cuatro mejores novelas jamás escritas, junto con "Tristram Shandy", "La Nouvelle Heloïse" y "Don Quijote". Su apellido da nombre al Goethe-Institut, organismo encargado de difundir la cultura alemana en todo el mundo.

El propio Goethe narró su vida en un libro autobiográfico, "Poesía y verdad" (1811 y ss.), que llega hasta el año 1775, cuando se pone al servicio del príncipe heredero Carlos Augusto en Weimar.

Nació en Fráncfort del Meno ("Frankfurt am Main"), hijo de Johann Caspar Goethe, un abogado y consejero imperial que se retiró de la vida pública y educó a sus hijos él mismo, bajo la máxima de no perder el tiempo en lo más mínimo, y de Katharina Elisabeth Textor, hija de un antiguo burgomaestre de Fráncfort. Estas vinculaciones familiares le pusieron en contacto desde el principio con el patriciado urbano y la vida política.

De inteligencia superdotada y provisto de una enorme y enfermiza curiosidad, hizo prácticamente de todo y llegó a acumular una omnímoda o completa cultura. En primer lugar estudió lenguas, aunque sus inclinaciones iban por el arte y nunca, a lo largo de toda su vida, dejó de cultivar el dibujo; al tiempo que escribía sus primeros poemas, se interesó por otras ramas del conocimiento como la geología, la química y la medicina.

Goethe estudió Derecho en Leipzig (1765); allí conoció los escritos de Winckelmann sobre arte y cultura griegas, pero una grave enfermedad le obligó a dejar los estudios en 1768 y volver a Fráncfort. Katharina von Klettenberg, amiga de su madre, le cuidó y le introdujo en el misticismo pietista, que ponía su énfasis en el sentimiento dentro de la confesión protestante; por entonces compuso sus primeros poemas. Retomó los estudios en 1770 en Estrasburgo y los concluyó al año siguiente. Esos dos años allí fueron muy importantes para él: conoció a Friederike Brion, que le inspiró la mayoría de sus personajes femeninos, y trabó amistad con el teólogo y teórico del arte y la literatura Johann Gottfried von Herder. Herder le introdujo en la poesía popular alemana, le descubrió el universo de Shakespeare y le liberó definitivamente del Neoclasicismo francés y de la confianza en la razón de la "Aufklärung" alemana.

Empezó a hacer prácticas de abogacía en Wetzlar y colaboró con Herder en la redacción del manifiesto fundador del movimiento "Sturm und Drang" («Tempestad e ímpetu»), considerado el preludio del Romanticismo en Alemania: "Sobre el estilo y el arte alemán" (1772). En esta obra se reivindica la poesía de James MacPherson (Ossian) y de Shakespeare. Otra vez de vuelta en Fráncfort, escribió la tragedia "Götz von Berlichingen" (1773) y al año siguiente su novela "Las cuitas del joven Werther" (1774). La inspiración del "Werther" la había encontrado a mediados de 1772, cuando hacía prácticas de abogacía en el tribunal de Wetzlar: se había enamorado de Charlotte Buff, novia y prometida de su colega, también abogado en prácticas, Johann Christian Kestner, y Karl Willhelm Jerusalem, otro abogado atormentado por un amor no correspondido, se suicidó utilizando una pistola prestada por Kestner. Goethe unió ambas historias para su novela "Werther", en parte epistolar, y alcanzó un éxito tan grande al representar en la figura del protagonista el desencanto de las jóvenes generaciones, que suscitó una epidemia de suicidios de adolescentes en el país.

El mismo año que el "Werther" (1774) Goethe publica su drama "Clavijo" mientras intentaba abrir con poca fortuna un bufete de abogado en Fráncfort, y en la primavera de 1775 se comprometió con la hija de un banquero de la ciudad, Lili Schönemann. Sin embargo, las diferencias sociales y de estilo de vida entre ambas familias terminaron por desbaratar este compromiso, que no llegó a formalizarse en matrimonio. El noviazgo terminó en el otoño de ese mismo año y, ansioso de escapar de este ambiente, no dudó en aceptar la invitación a la Corte de Weimar de Carlos Augusto de Sajonia-Weimar-Eisenach, heredero del ducado de Sajonia-Weimar. Tras publicar su "Stella" (1775), marchó inmediatamente hacia Weimar, huyendo prácticamente de dos cosas: el compromiso sentimental con Lili Schönemann y el ejercicio de la abogacía.

Al servicio del príncipe heredero Carlos Augusto fijará su residencia en Weimar ya hasta su muerte. No obstante, las numerosas tareas que éste le encomendaba le hicieron abandonar la literatura durante casi diez años, a pesar de que Ana Amalia de Brunswick-Wolfenbüttel, madre de Carlos Augusto, había empezado a crear un círculo de intelectuales con el preceptor de su hijo, Wieland, y lo amplió al incluir en él a Goethe y posteriormente a intelectuales tan destacados como Herder y Friedrich von Schiller; fugazmente pasaron también por allí Jakob Michael Reinhold Lenz y Friedrich Maximilian Klinger. Goethe destacó enseguida y pasó de ser consejero secreto de legación (1776) a consejero secreto (1779), y finalmente se convirtió en una especie de ministro supremo. Otra de sus funciones fue la supervisión de la Biblioteca ducal, que bajo su dirección llegó a ser una de las más importantes de toda Alemania.

Inicia en esa época sus investigaciones científicas. Interesado por la óptica, concibió una teoría distinta a la de Isaac Newton sobre los colores y también investigó en geología, química y osteología, disciplina esta última en que descubrió el hueso intermaxilar en marzo de 1784, que pone una de las primeras piedras en la teoría de la evolución del hombre, aunque en esto se le adelantó por muy poco el anatomista francés Vicq d'Azyr, lo que le supuso una gran frustración. Las cartas a Charlotte von Stein dan fe de esta época de su vida, envuelta en todo tipo de encargos y gestiones para reformar el muy pequeño y humilde Estado de Weimar.

Desde un puesto tan importante tuvo la oportunidad de relacionarse con la alta aristocracia y conoció a personajes notables, como Napoleón Bonaparte, Ludwig van Beethoven, Friedrich von Schiller y Arthur Schopenhauer. En 1782 fue añadida la partícula "von" a su apellido por el mismo Duque Carlos Augusto pese a las protestas de la nobleza, para formar parte de la Corte con un cargo equiparable al de los restantes ministros, pertenecientes todos a ella.

Ingresó en la Masonería el 11 de febrero de 1783, aunque según el escritor masónico Lorenzo Frau Abrines, la fecha de su ingreso es anterior, el 23 de junio de 1780, dentro de la efímera logia "Amalia", que abatió columnas dos años después. En 1830, dos años antes de su muerte, Goethe compuso un poema titulado "Para la fiesta de San Juan de 1830 en ocasión de celebrarse su cincuentenario como miembro de la masonería. "A su condición de masón y a su paso por la Masonería, así como a otras aficiones que al parecer cultivó, se atribuye influencia en su obra, especialmente en "Fausto."

Por otra parte, seguía profundizando en el estudio del teatro de William Shakespeare y de Pedro Calderón de la Barca, algunas de cuyas obras (por ejemplo, "El príncipe constante" de Calderón) hace representar con éxito como encargado del teatro en la Corte de Weimar; en estas funciones empezó a cartearse con Schiller. Las lecturas teatrales de estos autores amplían notablemente los horizontes de su espíritu. Le domina además el entusiasmo ante la falsa poesía céltica de Ossian y escribe un famoso monólogo del gran dios del Romanticismo, "Prometeo", que personificaba el genio rebelde de los creadores y del cual se sintió justamente orgulloso: 

Así fue en efecto, en lo referido al movimiento conocido como titanismo, uno de cuyos más preclaros representantes fue Giacomo Leopardi. Merced a Goethe, Weimar se convirtió en el auténtico centro cultural de Alemania; allí compuso poemas inspirados por Charlotte von Stein y empezó la redacción de sus obras más ambiciosas, como sus dramas "Ifigenia en Táuride" (1787) "Egmont" y "Fausto", que luego revisaría a fondo tras la profunda impresión que recibió en su trascendental viaje a Italia (1786-1788), que cambió su desequilibrada estética romántica por el equilibrio clásico. Empezó en Venecia, donde compuso sus "Epigramas venecianos", y terminó en Roma, donde estudió la cultura grecolatina a fondo; de esta época son sus "Elegías romanas". El viaje a Italia supone el comienzo de su periodo clásico.

Sin embargo, a su regreso a Weimar en 1788 se encuentra una gran oposición a su nueva estética, el llamado Clasicismo de Weimar; es más, se forma un cierto escándalo cuando llega a divulgarse que desde ese mismo año vive amancebado con una jovencita, Christiane Vulpius (1765-1816), que le dio al año siguiente un hijo, Julius August Walther von Goethe (1789-1830); cuatro abortos sucesivos posteriores inducen a creer que entre ambos había incompatibilidad de grupos sanguíneos, en aquella época desconocida. Goethe legitimó a su único hijo en 1800.

No abandonó completamente su pretensión de labrarse una carrera científica. En "Zur Farbenlehre", 1810, intentó refutar con poca fortuna la teoría de los colores de Newton. En el primer volumen de esta obra se halla la que es sin duda la primera historia comprensiva de la ciencia.

Dirigió el Teatro ducal entre 1791 y 1813 y con motivo de este cargo conoció en 1794 al dramaturgo Friedrich von Schiller, con el que sostuvo una luenga amistad y cierta correspondencia epistolar hasta la muerte de éste en 1805. Schiller publicó las hasta entonces inéditas "Elegías romanas" de Goethe en su periódico, "Las Horas", en 1795. También imprimió la novela "Los años de aprendizaje de Wilhelm Meister" (1796) y la novela en verso "Hermann y Dorothea" (1798). Schiller incitó a Goethe a que prosiguiera en la gran obra de su vida, el "Fausto", poema que no paraba de corregir y ampliar y cuya primera versión apareció en 1808. Desde dos años antes se hallaba ya casado con Christiane Vulpius, quizá para acallar a quienes criticaban su estilo de vida. El hecho más importante quizá de esta época de su vida es su entrevista en Erfurt con Napoleón I en 1808, cuando el ejército francés ocupaba parte del territorio prusiano en el marco de las guerras Napoleónicas.

La Revolución francesa supuso para Goethe un gran trastorno. Algunos de sus epigramas venecianos ya tratan este tema, pero como su pensamiento se hallaba por completo imbuido del equilibrio y armonía del clasicismo y veía el ser como una totalidad orgánica a partir de la filosofía de Kant, el desarrollo de la revolución y el cambio provocado por la violencia le parecían una atrocidad. Eso se plasmó en algunas obras de entonces, como la colección de novelitas breves "Conversaciones de emigrados alemanes" (1795), la obra épica "Germán y Dorotea" (1797) y la tragedia "La hija natural" (1799 y ss.). Algo después aparecen las novelas de madurez: "Las afinidades electivas" (1809) y "Los años de peregrinaje de Wilhelm Meister" (1821, revisado en 1829), así como un diario de su viaje por Italia, "Viajes italianos" (1816), su autobiografía "Poesía y verdad" en varias entregas (1811-1833) y un poemario, "Diván de Oriente y Occidente" (1819), donde se deja sentir algo el influjo de la poesía oriental. Goethe murió en Weimar el 22 de marzo de 1832. La versión final de su gran poema coral "Fausto" apareció póstuma ese mismo año.

En cuanto a su carrera literaria, Goethe la inició en el seno de un exasperado Romanticismo deudor del "Sturm und Drang", cuya obra más representativa se encargó de escribir él mismo: "Las cuitas del joven Werther". El viaje a Roma supuso para él ir arrinconando esa estética en una evolución que le hizo al cabo renegar del Romanticismo e identificarse con el equilibrio clásico grecolatino, lo que puso fin a su tormentosa vida interior. Fue esa la revelación del Clasicismo, verdadera raíz con la que podía identificarse la cultura alemana. «"Ahora comprendo el sentido del mármol"», escribirá en una de sus "Elegías romanas".

De ese viaje por Italia son fruto también los "Epigramas venecianos", entre los cuales hay algunas meditaciones profundas sobre la contemporánea Revolución francesa o el significado de la vida y de la cultura. La postura política de Goethe es, sin embargo, conservadora: «"prefiero la injusticia al desorden"», escribirá. Eso le supuso algunos recelos por parte de otros artistas a los que no les importaba en lo más mínimo no acordarse con su contexto social, como por ejemplo Beethoven. En las dos versiones de su complejo y grandioso "Fausto" se encuentra el último mito que fue capaz de engendrar la cultura europea, el de cómo la grandeza intelectual y la sed omnímoda de saber pueden, sin embargo, engendrar la miseria moral y espiritual. Por otra parte, en la lectura y estudio de Spinoza encuentra también un consuelo al desequilibrio romántico que le embargaba, como cuenta en "Poesía y verdad", donde se extiende en comentar especialmente su frase de que «"quien bien ama a Dios, no debe exigir que Dios le ame a él"».

Goethe disfrutó ya en vida de fama, respeto, prestigio y admiración. Delacroix le retrató en una litografía en 1827, aparte de ilustrar "Fausto" y "Götz von Berlichingen". Por ello, fueron muchos los jóvenes de su época que quisieron conocerle en persona o, como suele decirse pedantemente: "vera effigies". Por otra parte, su secretario, Eckermann, anotaba cuidadosamente sus conversaciones con el maestro a lo largo de los años y escribió unas "Conversaciones con Goethe", donde aparecen reflejadas las opiniones que en sus últimos años sostuvo sobre esas visitas y también sobre todo lo divino y lo humano. El filósofo George Santayana escribió sobre él:

La mejor obra dramática de Goethe es sin duda el "Fausto", que ha pasado a ser una obra clásica de la Literatura Universal. La primera versión, el "Urfaust" o "Fausto original", estaba acabada en 1773. Pero el autor la siguió retocando hasta 1790, año en que publicó un fragmento; ya en abril de 1806 estaba completo, pero las guerras napoleónicas demoraron dos años la publicación hasta 1808; la segunda versión o segunda parte sólo sería publicada en 1833, un año después del fallecimiento del autor. La tragedia "Fausto" original se articula en torno a dos centros fundamentales; el primero es la historia de cómo Fausto, fatigado de la vida y decepcionado de la ciencia, hace un pacto con el diablo que le devuelve la juventud a cambio de su alma; el segundo es la historia de amor entre Fausto y Gretchen, también llamada Margarita, que Mefistófeles manipula de forma que Fausto llegue al homicidio —mata al hermano de su amada— y Gretchen tenga un embarazo indeseado, que le conduce primero al infanticidio y luego a ser ejecutada por asesinar a su hijo. 

La historia empieza en el cielo, donde Mefistófeles hace un pacto con Dios: dice que puede desviar al ser humano favorito de Dios (Fausto), que está esforzándose en aprender todo lo que puede ser conocido, lejos de propósitos morales. La siguiente escena tiene lugar en el estudio de Fausto donde el protagonista, desesperado por la insuficiencia del conocimiento religioso, humano y científico, se vuelve hacia la magia para alcanzar el conocimiento infinito. Sospecha, sin embargo, que su intento no está obteniendo resultados. Frustrado, considera el suicidio, pero lo rechaza cuando escucha el eco del comienzo de la cercana Pascua. Va a dar un paseo con su ayudante Wagner y es seguido a casa por un caniche vulgar. En el estudio de Fausto el caniche se transforma en el diablo. Fausto hace un trato con él: el demonio hará todo lo que Fausto quiera mientras esté en la tierra, y a cambio Fausto servirá al demonio en la otra vida. El trato incluye que, si durante el tiempo que Mefistófeles esté sirviendo a Fausto éste queda complacido tanto con algo que aquel le dé, al punto de querer prolongar ese momento eternamente, Fausto morirá en ese instante. Tras este marco, Goethe desarrolla las dos historias: la relación entre Mefistófeles y Fausto y la de Fausto y Gretchen/Margarita.

El tema general es cómo la riqueza de conocimiento material acarrea sin embargo la miseria moral y espiritual. La historia de Fausto se inspira, como muchas leyendas, en hechos ciertos. Existió un tal Johann Faust que nació hacia 1490 en el sur de Alemania y se doctoró en la Universidad de Heidelberg en 1509. Tras dejar la universidad, emprendió una vida de aventuras marcada por una huida constante a causa de las múltiples acusaciones de brujería que se le hicieron. Dejó una biblioteca que incluía libros de medicina, matemáticas y magia negra. Esta pintoresca vida dio origen a la leyenda popular, aprovechada por autores de piezas de títeres y marionetas, y había servido además para inspirar leyendas populares. El primer libro sobre este mito se editó en 1587 por parte de Johannes Spiess, quien, en su prólogo, advirtió que había omitido referir fórmulas mágicas para evitar que quienes tuvieran el libro fueran acusados de brujería. Otros libros y libretos teatrales trataron el tema del pacto con el diablo para lograr el dominio sobre la naturaleza: en el teatro de títeres de los siglos XVI y XVII, la historia se cerraba siempre con los demonios llevándose a Fausto, pero Goethe alteró este argumento haciendo que se salvara Gretchen al final de la primera parte, anticipando la salvación de Fausto al término de la segunda, cuando los demonios que quieren llevarse su alma tienen que retirarse ante la llegada de una legión de ángeles. Además Goethe cambia el impulso que mueve a Fausto: el deseo que lo acercaba a la brujería no es codicia, maldad o vagancia, sino el ansia de saber, el deseo de grandeza, de plenitud, de totalidad. La moraleja que acaso tenga la obra será que ese deseo de conocimiento conlleva la miseria moral.

La obra ha sido interpretada modernamente por Walter Benjamin, por Thomas Mann en su novela "Doktor Faustus" y por el hijo de éste, Klaus Mann en "Mephisto", en la que concibe el pacto con el diablo como una metáfora del pacto de Alemania con Hitler.


El pensamiento científico de Goethe, como el literario, es también muy original. Aunque a menudo ha sido considerado como uno de los representantes más destacados de la "Naturphilosophie", en realidad su producción científica se sitúa entre el romanticismo y el clasicismo, desmarcándose, por ejemplo, de los excesos especulativos de Schelling.
La morfología de Goethe se construye en torno a dos conceptos nucleares: el tipo y la metamorfosis: 

En "La metamorfosis de las plantas" ("Versuch die Metamorphose der Pflanzen zu erklären"), publicada en 1790, Goethe presenta todas las estructuras vegetales como variaciones de la hoja, entendida como una estructura ideal. Goethe comienza con los cotiledones, a los que considera hojas imperfectas. Estos últimos, bajo la influencia generativa y cada vez más refinada de la savia, se metamorfosean en los sépalos, los pétalos, los estambres y los pistilos. De este modo, todos los órganos vegetales se conciben como apéndices idénticos, variedades de un apéndice vegetal abstracto, que difieren entre sí por su forma y grado de expansión.

Sus ideas acerca de las plantas y la morfología y homología animal fueron desarrolladas por diversos naturalistas decimonónicos, entre ellos Charles Darwin.







</doc>
<doc id="6714" url="https://es.wikipedia.org/wiki?curid=6714" title="Cable submarino">
Cable submarino

Un cable submarino o Interoceánico es aquel cable de cobre o fibra óptica instalado sobre el lecho marino y destinado fundamentalmente a servicios de telecomunicación.

No obstante, también existen cables submarinos destinados al transporte de energía eléctrica, aunque en este caso las distancias cubiertas suelen ser relativamente pequeñas y además van insertados dentro de una tubería especial para evitar riesgos al contacto con el agua ya que son cables de alto voltaje.
En lo relativo al servicio de telecomunicación los primeros cables, destinados al servicio telegráfico, estaban formados por hilos de cobre recubiertos de un material aislante denominado gutapercha, sistema desarrollado en 1847 por el alemán Werner von Siemens. Con este sistema se logró tender, en 1852, el primer cable submarino que unía el Reino Unido y Francia a través del Canal de la Mancha.
En 1855 se aprobó el proyecto para tender el primer cable trasatlántico que quedó fuera de servicio en poco tiempo. En 1865 se puso en marcha el segundo proyecto, empleándose para ello el mayor barco existente en ese entonces, el Great Eastern. Este cable no llegaría a funcionar hasta el año 1866 y unía Irlanda y Terranova. Algunos años más tarde, en 1958, se instaló finalmente un cable que atravesaba el océano atlántico y conectaba Irlanda con Canadá, optimizando enormemente la comunicación entre Estados Unidos y Gran Bretaña, reduciendo drásticamente el tiempo en que los mensajes podían llegar a su destino. De días (tiempo en que los barcos tardaban en entregar el mensaje en la otra costa) a únicamente horas.

Fueron el especialista estadounidense en telégrafos Cyrus West Field y el físico y matemático irlandés William Thomson, más tarde conocido como Lord Kelvin, quienes se aventuraron a instalar este cable, en un contexto donde la idea de poder comunicarse a grandes distancias en poco tiempo, era aún más importante que la luz eléctrica.

El procedimiento consistió en encontrar dos barcos a medio camino y luego transportar cada extremo de cable a cada una de las costas. Cuya medida eran 3.000 kilómetros. Hasta ese entonces, la idea de un cable submarino no era posible debido a que no se contaba con un material lo suficientemente resistente. Con el implemento de la gutapercha, material obtenido de la savia de algunos árboles, pudo cubrirse el cable y permitir conexiones subacuáticas. Si bien el primer intento fue un fracaso, en Europa pronto tomó fama y fue instalado en diferentes naciones, logrando conexiones importantes en el mar Mediterráneo y en el mar Negro. Se estima que en el año 1855 ya había instalados por lo menos veinticinco cables submarinos. Esto fue lo que permitió a Field y Thomson a intentar conectar sus dos naciones, que en aquel momento su contexto político exigía una mejor manera de mantenerse comunicados. 

Las dificultades de tendido fueron considerables, así como las de explotación, debido a las elevadas atenuaciones que sufrían las señales como consecuencia de la capacitancia entre el conductor activo y la toma de tierra, así como por los problemas de aislamiento. Muchos de estos problemas eran ocasionados por el sabotaje de los accionistas de las compañías marítimas, introduciendo clavos y perforando así la capa aislante del cable. Se tuvo que emplear muchos hombres y un trabajo minucioso y a conciencia para poder repararlos. El progreso de éste, era perjudicial económicamente para las compañías navieras.

Tras el evidente fracaso de esta conexión, varios inversionistas se retiraron del proyecto y no fue sino hasta seis años después que se volvió a realizar un intento por conectar ambas naciones. 

Ideológicamente, podría decirse que el cable sirvió para la consolidación de la sincronización del mundo occidental entre dos potencias importantes. Así como sirvió para instaurar la primera gran noción de un mundo completamente conectado. E incluso abrió la puerta para que se siguieran dando pasos en relación al desarrollo de las conexiones de comunicación. 

El descubrimiento de aislantes plásticos posibilitó la construcción de cables submarinos para telefonía, dotados de repetidores amplificadores sumergidos, con suministro de energía a través de los mismos conductores a través de los cuales se transmitía la conversación.

Posteriormente, en la década de los 60, se instalaron cables submarinos formados por pares coaxiales, que permitían un elevado número de canales telefónicos analógicos, del orden de 120 a 1800, lo que para la época era mucho, utilizando multiplexación por división de frecuencia. ya en los ´80, los cables submarinos de fibra óptica que trabajan en base al mismo principio de multiplexación (pero empleando diferentes longitudes de onda), han posibilitado la transmisión de señales digitales portadoras de voz, datos, televisión, Internet, etc. con velocidades de transmisión de hasta 1000 Tbit/s.

Actualmente los cables submarinos de fibra óptica son la base de la red mundial de telecomunicaciones. La comunicación por satélite es minoritaria.

El cable se muestra como una solución más interesante, tiene que ver con con la resistencia ante inclemencias meteorológicas, se posiciona como una infraestructura más fiable y rápida.

A pesar de que el cable era parte de un desarrollo de comunicación, también puede pensarse en todo lo que significó. En un mundo donde la noción de “"conectividad"” apenas empezaba a esclarecerse y donde la idea de entender a la sociedad a través de una metáfora de red era prácticamente inexistente, hubo quienes se aventuraron a intentar esta gran hazaña.

Con un elemento que es como una azada, esto crea un surco donde se posará el cable, lo cubre la corriente marina.

Lo más difícil es la profundidad del océano, el océano es similar a la tierra consta de subidas y bajadas además de montañas y profundidades, esto hace que haya zonas muy conflictivas y en condiciones muy diversas para pasar los cables. Por eso se estudia en profundidad la ruta por donde pasará los cables y las zonas mas aptas,para ello se hace uso de las últimas tecnologías que ayudan en esta tarea.

La zona marítima donde hay un cable es la que pasa por la Fosa de Japón, donde se posa una red a una profundidad de 8.000 metros tiene la altura del Monte Everest, pero bajo el océano.

En realidad el cable debe ser bastante más largo que esa profundidad en el momento de su colocación, ya que el barco está en movimiento soltando cable, y el ángulo que forma con el mayor punto de profundidad hace que el cable llegue a medir 16 kilómetros.

Desplegar un cable de Japón a Estados Unidos toma varios meses, si tenemos en cuenta todo el proceso de exploración nos vamos por encima de los dos años.

Los cables en la actualidad no son muy gruesos , pueden alcanzar el tamaño de un brazo humano. No ocupa mucho espacio la protección que recubre los cables.

3.840 giga-bits por segundo, es decir 102 discos DVD en esos segundos, esto es posible por la fibra óptica. Un cable consta de 16 hilos de fibra,por lo tanto se llenan 1.700 discos DVD por segundo, por encima de los 60 terabits_por_segundo.

La reparación de un cable a tanta profundidad es una tarea laboriosa, en la actualidad se arreglan en la superficie. Se realiza con un aparato parecido a un garfio, y en zona seca se repara, une o soluciona el daño, al ser tan elaborado la jornada se alarga bastante aproximadamente una semana y de es de coste elevado, no hay muchos barcos que se dediquen a esta labor

Hay varias razones por la que los cables sufren daños, el clima, anclas de barcos, peces. La fauna marina se puede comer el recubrimiento de los cables - tiburones -, atraídos por los campos magnéticos. Afortunadamente los nuevos diseños de cables contemplan este problema.

Otros casos que se contemplan, son los terremotos. En 2006 uno con magnitud 7.0 rompió ocho cables en la zona de Taiwán, afectando gravemente a las comunicaciones en China. Un total de once barcos estuvieron 49 días trabajando en la reparación.

En la actualidad se están incorporando sistemas de reconocimiento de movimientos en el fondo oceánico para los posibles terremotos, que puedan derivar en algo más complicado. Se sabe que ahí se originan, podemos contar con la velocidad con la que el cable puede avisar de lo que acontece, esto nos da un margen de maniobra muy grande para prevenir situaciones en las ciudades.

En la actualidad los cables no están protegidos por los países.

"Cada año se producen entre 100 y 150 cortes de cables submarinos"



</doc>
<doc id="6716" url="https://es.wikipedia.org/wiki?curid=6716" title="Consonancia">
Consonancia

En música, la consonancia (eufonía) es una noción subjetiva según la cual se consideran ciertos intervalos musicales menos tensos que otros. En oposición a este concepto, está el de disonancia, que se usa para referirse a intervalos que se consideran más tensos que otros. Según la Real Academia Española, consonancia es la "cualidad de aquellos sonidos que, oídos simultáneamente, producen efecto agradable."

Hoy en día se aceptan como consonancias los intervalos de octava justa, quinta justa, cuarta justa, tercera mayor, tercera menor, sexta mayor y sexta menor. También se consideran consonantes los intervalos compuestos que derivan de estos. Algunas clasificaciones dividen las consonancias en perfectas (octavas, quintas y cuartas justas) e imperfectas (terceras y sextas).




</doc>
<doc id="6717" url="https://es.wikipedia.org/wiki?curid=6717" title="Astor Piazzolla">
Astor Piazzolla

Astor Pantaleón Piazzolla (Mar del Plata, 11 de marzo de 1921 - Buenos Aires, 4 de julio de 1992) fue un bandoneonista y compositor argentino considerado uno de los músicos más importantes del Siglo XX,y el compositor más importante de tango en todo el mundo.

Estudió armonía, música clásica y contemporánea con la compositora y directora de orquesta francesa Nadia Boulanger. En su juventud tocó y realizó arreglos orquestales para el bandoneonista, compositor y director Aníbal Troilo. Cuando comenzó a hacer innovaciones en el tango en lo que respecta a ritmo, timbre y armonía, fue muy criticado por los "tangueros" de la «guardia vieja», ortodoxos en cuanto a ritmo, melodía y orquestación.

Cuando en los años 1950 y 1960 los tangueros tradicionales —que lo consideraban «el asesino del tango»— decretaron que sus composiciones no eran tango, Piazzolla respondió con una nueva definición: «Es música contemporánea de Buenos Aires». A pesar de esto, en Argentina las estaciones radiodifusoras no difundían sus obras y los comentaristas seguían atacando su arte. Durante años, tangueros y críticos musicales lo consideraron un esnob irrespetuoso que componía música "híbrida", con exabruptos de armonía disonante.

En sus últimos años de vida fue reivindicado por intelectuales, jazzistas y músicos de rock de todo el mundo, al igual que por nuevos referentes del tango, y actualmente se lo considera como uno de los músicos argentinos más importantes en la historia de su país. Compuso también música para cerca de 40 películas.

Piazzolla nació en Mar del Plata, Argentina en 1921, hijo de Vicente Piazzolla y Asunta Manetti (ambos nacidos en Mar del Plata, hijos de padres italianos). El nombre Astor no existía en ese entonces, su padre se le puso en homenaje a su amigo Astore Bolognini, corredor de moto y primer violonchelista de la Orquesta Sinfónica de Chicago. Piazzolla vivió gran parte de su niñez con su familia en Nueva York, donde desde muy joven entró en contacto tanto con el "jazz" como con la música barroca de Bach.

Mientras vivió allí, aprendió a hablar fluidamente cuatro idiomas: castellano, inglés, francés e italiano. Comenzó a tocar el bandoneón en 1927 cuando su padre, nostálgico de su Argentina natal, le compró uno usado en una casa de empeños por 18 dólares. En 1932 compuso su primer tango, "La Catinga", que nunca difundió. En 1933 tomó clases con Bela Wilda, un pianista húngaro discípulo de Serguéi Rachmáninov, de quien señaló Piazzolla: “Con él aprendí a amar a Bach.”

Piazzolla conoció a Carlos Gardel en Manhattan en 1934, al llevarle un presente realizado por su padre. A Gardel le cayó muy bien el joven, y le resultó muy útil para realizar sus compras en la ciudad, ya que el joven conocía muy bien la ciudad, además que dominaba el inglés, idioma que Gardel desconocía totalmente. Al año siguiente el cantor lo invitó a participar en la película que rodaba en esos días, "El día que me quieras", como un joven vendedor de diarios. Piazzolla cuenta que después de escucharlo tocar el bandoneón, Gardel le dijo: «Vas a ser grande pibe te lo digo yo... el fuelle lo tocás bárbaro, pero al tango lo tocás como un gallego».

Gardel invitó a Piazzolla a unirse en su gira por América, pero su padre decidió que éste era aún muy joven, su lugar fue reemplazado por el boxeador argentino José Corpas Moreno. Esta temprana desilusión probó ser una suerte en la desgracia, ya que fue en esta gira en la que Gardel y toda su banda perdieron la vida en un accidente aéreo. En 1978, en una carta imaginaria a Gardel, Astor bromearía al respecto sobre ese hecho:

Volvió a Argentina en 1937, donde el tango estrictamente tradicional aún reinaba. Mientras tanto, Astor tocaba en clubes nocturnos con una serie de grupos, incluyendo la orquesta de Aníbal Troilo, considerado en ese momento el mejor bandoneonista y líder en Buenos Aires. Se le aconsejó estudiar con el compositor Alberto Ginastera y posteriormente con Raúl Spivak. Introducido en grabaciones de Stravinsky, Bartók, Ravel, entre otros, iba cada mañana a oír la orquesta del Teatro Colón, mientras continuaba tocando tango de noche.

En 1942 se casó con Dedé Wolf y del matrimonio nacieron sus hijos Diana en 1943 y Daniel en 1944.

En 1950 compuso la banda de sonido de la película "Bólidos de Acero".

En 1952 compuso "La Epopeya Argentina", un movimiento sinfónico para narrador, coro y orquesta con texto de Mario Nuñez, que sobrevive en una transcripción para piano del compositor, publicada en 1952 por Editorial Saraceno. Es un panegírico al gobierno peronista de esos años, donde la rítmica es cuadrada, predominan los acordes por cuartas y las figuras modales. La voz del narrador no lleva notación. El coro alterna entre la vocalización.

También en esa década continuó con la composición de obras de música tales como "Rapsodia porteña", "Sinfonietta" y "Buenos Aires (tres movimientos sinfónicos)". Por esta última ganaría el premio Fabien Sevitzky, por lo que el gobierno francés le otorgó una beca para estudiar con Nadia Boulanger en París, en 1953.

Nadia Boulanger fue una pieza muy importante en su carrera, ya que hasta su encuentro con ella, Piazzolla se debatía entre ser un músico de tango o un compositor de música clásica. Boulanger lo animó a seguir con el tango, pero si hasta ese momento todo era o tango o música clásica, a partir de entonces sería tango y música clásica. Varios aspectos de la relación de Piazzolla con Boulanger no son muy claros, entre ellos como el músico argentino tomó contacto con Boulanger, ni quién se la recomendó. Según Simon Collier y María Susana Azzi, el propio Alberto Ginastera le recomendó a Piazzolla contactarse con alguién, pero no especificaron concretamente con quién. Natalio Gorin afirmó que el bandoneonista le confesó que: "Casi tomo clases con Olivier Messiaen". Por otro lado, su hija Diana dijo que su padre recién arribado a París se enteró que Boulanger estaba dando clases muy cerca de donde se alojaba él. Para ser aceptado como alumno le tuvo que mostrar algo de lo que había compuesto, tampoco esta claro cual fue la pieza que le enseñó, ya que por un lado según Diana fue la "Sinfonietta", mientras que Collier y Azzi aseguran que fueron los "Tres movimientos sinfónicos de Buenos Aires". Los biografos de Piazzolla están de acuerdo en que Boulanger notó la carencia de algo en su música, lo que Piazzolla años más tarde denomino "sentimineto". Un día Boulanger indagando sobre que música hacía en Buenos Aires, él con algo de timidez le confesó que componía tangos y que tocaba el bandoneón, entonces fue allí cuando le pidió que tocase al piano uno de sus tangos, Piazzolla tocó "Triunfal" y antes de terminarlo su profesora le tomo las manos para decirle: "No abandone jamás esto. Ésta es su música. Aquí está Piazzolla".

También en París, tuvo la oportunidad de escuchar al octeto del saxofonista Gerry Mulligan y quedó impresionado por su improvisación y por el distendimiento con el que tocaban los músicos.

Estudió 11 meses con Boulanger, pero al mismo tiempo formó una orquesta de cuerdas con músicos de la Ópera de París, con Lalo Schifrin y Martial Solal alternándose en el piano, y grabó el álbum "Two Argentineans in Paris" (1955) con temas como «Picasso», «Luz y sombra» y «Bandó».

Años más tarde Piazzolla recordaría a Boulanger diciendo:
En 1955 volvió a Buenos Aires, donde formó una orquesta de cuerdas con músicos argentinos, en la que cantó Jorge Sobral (para esta formación compone "Tres minutos con la realidad", obra síntesis entre el tango y la música de Stravinsky y Bartók), y el famoso Octeto Buenos Aires, conjunto considerado como el iniciador del tango moderno, tanto por su instrumentación (incluía por primera vez una guitarra eléctrica en un conjunto de tango), como por sus novedades armónicas y contrapuntísticas (acordes con 13as. aumentadas, seisillos y fugas).

En 1958 disuelve ambas formaciones y se marcha a los Estados Unidos, donde graba los dos únicos discos de lo que él llamó el "jazz"-tango (los cuales actualmente son muy difíciles de encontrar).

En 1959, durante una actuación en Puerto Rico, junto a Juan Carlos Copes y María Nieves, recibe la noticia de la muerte de su padre, Vicente "Nonino" Piazzolla. Astor vuelve a Nueva York, donde vivía con su familia, y allí compuso «Adiós Nonino», su obra más célebre, que conservaría la sección rítmica del anterior tango «Nonino», más una sentida elegía de despedida, que se convertiría en un sinónimo de Piazzolla a lo largo de los años.

En 1990, durante una entrevista declaró que: «El tango número uno es 'Adiós Nonino'. Me propuse mil veces hacer uno superior y no pude». Se registran más de 170 versiones de «Adiós Nonino» de distintos músicos.
Frustrado por el intento del "jazz"-tango, vuelve a Buenos Aires en 1960 y forma la agrupación que definiría su estilo musical definitivamente, que sería la base de agrupaciones posteriores y a la que volvería cada vez que se sentía frustrado por otros proyectos: el Quinteto Nuevo Tango, formado en su primera versión, por Piazzolla en el bandoneón, Jaime Gosis en piano, Simón Bajour en violín, "Kicho" Díaz en contrabajo y Horacio Malvicino en guitarra eléctrica.

Con esta agrupación daría a conocer "Adiós Nonino" y todas las composiciones que dieron forma a su estilo y que serían las más recordadas: Las Estaciones ("Verano Porteño", "Otoño Porteño", "Invierno Porteño" y "Primavera Porteña"), La Serie del Ángel ("Introducción al ángel", "Milonga del ángel", "Muerte del ángel" y "Resurrección del ángel"), La Serie del Diablo ("Tango diablo", "Vayamos al diablo" y "Romance del diablo"), "Revirado", "Fracanapa", "Calambre", "Buenos Aires Hora Cero", "Decarísimo", "Michelangelo ´70" y "Fugata", entre otros. Esa última pieza está basada en la obra del compositor alemán Johann Sebastian Bach.

En 1963, forma el Nuevo Octeto, para el cual compuso "Introducción a «Héroes y tumbas»", con letra de Ernesto Sabato.
En ese año también gana el Premio Hirsch por su «Serie de tangos sinfónicos», estrenados bajo la dirección de Paul Klecky.

En 1965, junto al Quinteto, una orquesta formada "ad hoc", y con las voces de Luis Medina Castro como recitante y Edmundo Rivero como cantante, graba el disco "El tango", que contiene temas con letras de Jorge Luis Borges, incluido "Hombre de la esquina rosada", suite para canto, recitado y doce instrumentos. Precisamente en el citado año, Piazzolla cobró por 754.000 pesos (una suma alta para la época) de regalías por parte de la SADAIC, los álbumes del quinteto se vendían razonablemente bien, lo que le permitió negociar con los sello condiciones que no fueran abusivas para él.

En 1966 se separa de Dedé Wolff y en 1967 empieza su colaboración con el poeta Horacio Ferrer, con quien compuso la operita "María de Buenos Aires", que se estrenaría al año siguiente, con la cantante Amelita Baltar. Por otra parte, Piazzolla inicia con Baltar una relación sentimental que durará cinco años.

En 1969, Piazzolla y Ferrer componen la exitosa "Balada para un loco", que supondría una popularidad súbita para Piazzolla.

En 1970 retornó a París donde nuevamente junto a Ferrer, creó el oratorio "El pueblo joven", estrenado poco después en 1971 en Saarbrücken, Alemania. Al año siguiente fue invitado por primera vez a presentarse en el Teatro Colón en Buenos Aires, junto con otras importantes orquestas de tango. También en 1972, Piazzolla compone, para su Conjunto 9 el «Concierto de Nácar, para nueve tanguistas y orquesta filarmónica», primer antecedente de sus obras sinfónicas para bandoneón posteriores.

En 1973 sufre un infarto que le obliga a reducir su actividad, por lo que se instala en Italia, en donde permaneció grabando durante cinco años. Durante esos años formó el Conjunto Electrónico, un octeto integrado por bandoneón, piano eléctrico o acústico, órgano, guitarra, bajo eléctrico, batería, sintetizador y violín (el cual posteriormente fue reemplazado por una flauta traversa o saxo). La formación fue integrada por reconocidos músicos italianos como Giuseppe Prestipino (Pino Presti), bajo eléctrico, Tullio De Piscopo, batería. Tiempo más tarde, Astor incorporaría al octeto al cantante José Ángel Trelles.

En 1974 se separó de Amelita Baltar, y ese mismo año graba, junto a una orquesta de músicos italianos, los álbumes "Summit", con Gerry Mulligan, y "Libertango", cuyo éxito lo hace conocido en Europa. Al año siguiente, el Ensemble Buenos Aires graba su obra "Tangazo" para orquesta sinfónica.

En 1975, después del fallecimiento de Aníbal Troilo, Astor compone en su memoria una obra en cuatro movimientos a la que llamó "Suite Troileana", la cual grabó junto al Conjunto Electrónico.
Al año siguiente, en 1976 conoce a Laura Escalada, quien sería su esposa definitiva. En diciembre de ese año presenta junto al Conjunto Electrónico en el teatro Gran Rex en Buenos Aires su obra "500 motivaciones". Meses después ofrecería otro concierto en el Olympia de París junto a una formación similar a la que tocó en Buenos Aires, la cual sería su última presentación junto a una formación de carácter eléctrico.

A partir de 1978 volvió a trabajar junto al quinteto Nuevo Tango y retomó la composición de obras sinfónicas y piezas de cámara.

En 1982 escribe "Le Grand Tango", para chelo y piano, el cual estuvo dedicado al chelista ruso Mstislav Rostropóvich. En 1985 fue nombrado Ciudadano ilustre de Buenos Aires, obtuvo el Premio Konex de Platino como el mejor músico de tango de vanguardia de la historia en Argentina y estrenó en Bélgica su "Concierto para Bandoneón y Guitarra: Homenaje a Lieja". En 1985 regaló al grupo «Nuevos aires» su partitura «500 Motivaciones» que fue interpretada en la Sala A y B del Centro Cultural General San Martín en su homenaje al ser nombrado «Ciudadano ilustre de la Ciudad de Buenos Aires».

En 1987 viaja a Estados Unidos, donde graba en vivo en el Central Park junto a la Orquesta de St. Luke's, dirigida por Lalo Schifrin, sus obras "Concierto para Bandoneón" y "Tres Tangos para Bandoneón y Orquesta". Durante esta etapa en los Estados Unidos también tuvo la oportunidad de grabar "Tango Zero Hour", "Tango apasionado", "La Camorra", "Five Tango Sensations" (junto al Kronos Quartet) y "Piazzolla con Gary Burton" entre otros.

En 1988 fue operado del corazón en un cuádruple baipás y a principios del año siguiente formaría su último conjunto, el Sexteto Nuevo Tango formado por dos bandoneones, piano, guitarra eléctrica, contrabajo y violonchelo.

El 4 de agosto de 1990 en París, sufrió una trombosis cerebral cayendose en el baño de un apart-hotel parisino. Fue internado con un infarto cerebral del que no se recuperó. Lo transladaron a Buenos Aires el 12 de agosto, de la que finalmente fallecería dos años después en Buenos Aires el 4 de julio de 1992, a los 71 años. Sus restos están inhumados en el cementerio Jardín de Paz, en la localidad de Pilar.

En sus últimos diez años, escribió más de 300 tangos, unas cincuenta bandas musicales de películas, entre las cuales se encuentran: "Henry IV" de Marco Bellocchio, "Lumière" de Jeanne Moreau, "Armaguedon" de Alain Delon, "Sur", "El exilio de Gardel" de Fernando Solanas. En febrero de 1993, Piazzolla fue nominado de manera póstuma para los Premios Grammy 1992 en Los Ángeles por "Oblivion" en la categoría "Mejor Composición Instrumental".

Entre los músicos contemporáneos de quienes Piazzolla tenía como fuente de inspiración y admiraba se encuentran Alfredo Gobbi y, fundamentalmente, Osvaldo Pugliese. Este último con sus composiciones "Negracha", "Malandraca" y "La yumba" se adelantó a lo que Astor luego realizaría. Básicamente en la música de Piazzolla la marcación rítmica está basada en el tango "Negracha" compuesto por Pugliese en 1943 y grabado en 1948. Siempre hubo entre ellos una relación de respeto y admiración mutua. Pugliese hizo versiones de tangos de Piazzolla como "El cielo en las manos" en 1951, "Marrón y azul" en 1956, "Nonino" entre 1961 y 1962, "Verano porteño" en 1965, "Balada para un loco" en 1970 y "Zum" en 1976. Piazzolla a su vez grabó de Pugliese: "Recuerdo" en 1966 y "Negracha" en 1956. Compartieron un recital juntos en el teatro Carré de Ámsterdam, el 29 de junio de 1989. Cerraron el recital tocando juntos sus éxitos más populares: "La yumba" y "Adiós Nonino". Ambos en una entrevista previa manifestaron su admiración y respeto mutuos y lamentaron el hecho que ese recital no se realizase en Argentina.

Entre las influencias de la música europea, podemos también citar a Johann Sebastian Bach (del periodo barroco) de quien queda notoriamente marcado su influjo en lo tocante al desarrollo de patrones armónicos, fugas y el uso del contrapunto, como así también a Bela Bartok (contemporáneo). Según el baterista José Luis Properzi, su música también tiene puntos en común con la obra de los estadounidenses George Gershwin y Brian Wilson. Por otro lado, tuvo una gran admiración y conoció personalmente a Igor Stravinsky.

En 1995 la Fundación Konex le confirió el Premio Konex de Honor por su incalculable aporte a la música en la Argentina.

En 1996, los días 13, 14 y 15 de junio en el Teatro Ópera de Buenos Aires se realizó un homenaje ideado por Eliseo Álvarez con el nombre de «Astortango». En dicho espectáculo actuaron destacados músicos argentinos y de todo el mundo interpretando las obras del maestro Piazzolla, entre ellos se encontraban Gary Burton, Chick Corea, Hermeto Pascoal, Jairo, Gerardo Gandini, Fernando Suárez Paz, Horacio Malvicino, Juan Carlos Cirigliano, Rodolfo Mederos, Julio Pane, Néstor Marconi, Raúl Luzzi, Arturo Schneider, Daniel Binelli, su hijo Daniel y su nieto Daniel «Pipi» Piazzolla.

En 1993, la Asociación de Música de Pesaro, por la voluntad del Maestro Hugo Aisemberg y otras personalidades de la cultura de Pesaro, fundó el Centro Astor Piazzolla.

En 2008, el aeropuerto internacional de Mar del Plata, su ciudad natal, recibió el nombre de «Aeropuerto Internacional Astor Piazzolla», en su memoria.

A partir del año 2007 se formó el quinteto radicado en Londres Fugata Quintet, con músicos graduados en la Royal Academy of Music de Londres, unidos por su común pasión por Piazzolla y su Nuevo Tango. Su nombre mismo se deriva «del segundo movimiento, «Fugata», de la Tangada-suite 'Silfo y Ondina' de Astor Piazzolla. Es un reconocimiento del quinteto, cuyo carácter y origen son principalmente clásicos, queriendo reflejar los propios orígenes clásicos del compositor, y su frecuente uso de las formas y las técnicas de composición clásicas en sus muchas y notables obras para quinteto». Formado por los músicos Antonis Hatzinikolaou (guitarra), Anastasios Mavroudis (violín), Zivorad Nikolic (acordeón), James Opstad (contrabajo) y Anahit Chaushyan (piano), están llevando a cabo en los últimos años en Londres una importante labor de difusión de la obra de Piazzolla, con exitosas actuaciones en directo y grabaciones dedicadas monográficamente a su obra en salas tan prestigiosas como el Royal Albert Hall, el Southbank Centre's Purcell Room, o The Forge, así como emisiones a través de la Radio 3 de la BBC, habiendo lanzado recientemente un aclamado álbum doble con música del compositor argentino.







</doc>
<doc id="6718" url="https://es.wikipedia.org/wiki?curid=6718" title="Carlos Gardel">
Carlos Gardel

Carlos Gardel fue un cantante, compositor y actor de cine. Es el más conocido representante del género en la historia del tango. Iniciador y máximo exponente del tango canción, fue uno de los intérpretes más importantes de la música popular mundial en la primera mitad del siglo, por la calidad de su voz, por la cantidad de discos vendidos (como cantante y como compositor), por sus numerosas películas relacionadas con el tango y por su repercusión mundial.

No hay unanimidad sobre el lugar y la fecha de su nacimiento. La hipótesis uruguayista sostiene que nació en Tacuarembó (Uruguay), un 11 de diciembre entre 1883 y 1887. La hipótesis francesista sostiene que nació en Toulouse (Francia) el 11 de diciembre de 1890.

Hay unanimidad en el hecho de que vivió desde su infancia en Buenos Aires y se nacionalizó argentino en 1923. Falleció el 24 de junio de 1935 en Medellín, Colombia, en un accidente aéreo.

La persona y la imagen de Gardel ha sido objeto de idolatría popular, especialmente en Argentina y Uruguay, colocándolo en un lugar de mito y símbolo cultural que aún mantiene su vigencia.

En 2003 la voz de Gardel fue registrada por la Unesco en el programa Memoria del Mundo, dedicado a la preservación de documentos pertenecientes al patrimonio histórico de los pueblos del mundo.

La fecha y el país de nacimiento de Gardel está sujeto a controversias históricas (ver sección Controversias sobre su lugar de nacimiento). Para la hipótesis uruguayista nació en Tacuarembó (Uruguay) entre 1883 y 1887, mientras que para la hipótesis francesista nació en Toulouse (Francia) en 1890. Como consecuencia de dichas discrepancias, cada una de las hipótesis sostiene relatos diferentes sobre los hechos de su infancia y adolescencia.

Para la hipótesis francesista Marie Berthe Gardes, cuyo nombre castellanizado fue Berta Gardés, fue la madre biológica de Charles Romuald Gardes, cuyo nombre fue castellanizado en Buenos Aires como Carlos Gardés y luego transformado en Carlos Romualdo Gardel por el propio cantante. En esta versión, Gardel habría estudiado en el Colegio Salesiano Pio IX de Buenos Aires, donde permaneció pupilo en 1901 y 1902 y fue compañero de coro de Ceferino Namuncurá, futuro beato argentino.

La hipótesis uruguayista sostiene que Marie Berthe Gardes obró como madre adoptiva de Carlos Gardel y que Charles Romuald Gardes fue un hijo biológico de Berthe, menor que Carlos. Algunos integrantes de esta corriente, sostienen que Gardel pudo haber cursado al menos el primer grado en la Escuela de 2.º Grado de Varones del barrio Sur de Montevideo.

Ambas hipótesis coinciden en el hecho de que Gardel fue abandonado por su padre y que vivió en Buenos Aires al menos desde 1893, en habitaciones de conventillos que compartía con su madre, aunque con intermitencias que varían según el historiador. Recién en 1927 Gardel comprará una casa en el barrio del Abasto, donde se mudará con su madre.

Ambas hipótesis coinciden también en que el joven Gardel, durante la primera década del siglo xx, pudo haber tenido conductas y frecuentar ámbitos ubicados en los márgenes de la legalidad, de lo que dan cuenta prontuarios policiales de 1904 y 1915 que lo mencionan y cuyas huellas digitales coinciden con las del cantante, como probó una investigación realizada por el criminólogo Raúl Torre y el médico forense Juan José Fenoglio.

Durante su infancia y adolescencia, Gardel vivió en paupérrimas casas de inquilinato o conventillos, ubicados en el barrio de San Nicolás: primero en Uruguay 162 y luego en Corrientes 1553. Con sus primeros ingresos como músico profesional en 1914, se mudó, siempre con su madre, a un departamento modesto en Corrientes 1714. La pobreza extrema y las condiciones de vida degradantes de los conventillos porteños en la época de la gran inmigración, han sido estudiadas en obras sociológicas, y representadas en obras artísticas, como "El conventillo de la Paloma". Su amigo y chofer Antonio Sumaje ha contado que, cuando Gardel ya era una estrella, solía pedirle que lo llevara a los conventillos en los que había vivido de niño en especial al de Uruguay 162, donde se bajaba y se quedaba mirando la fachada:

El barrio en el que Gardel se crio es la zona de los teatros porteños que tiene su eje en la calle Corrientes, luego transformada en avenida. Eso le permitió desde muy chico estar en contacto con el mundo teatral. Su madre trabajaba planchando ropa, a veces para algunos de esos teatros, y él mismo fue reclutado por un personaje conocido como «Patasanta», que organizaba claques de aplaudidores en los teatros, cobrando dinero por prestar ese servicio. Con la «"troupe" de animadores» de Patasanta, Gardel fue claque, utilero y comparsista (extra), a cambio de poder asistir a los espectáculos y recibir entradas. De esta manera logró estar en contacto con actores y cantantes, de quienes imitaría los ejercicios de vocalización y otras conductas que serían de importancia para su futura formación artística:

Así, entre muchos otros empleos informales, se desempeñó como tramoyista en el Teatro de la Victoria, donde escuchó al zarzuelista español Sagi Barba, con quien incluso llegó a tomar sus primeras lecciones informales de canto, y en 1902 pasó al Teatro Ópera, donde conoció al barítono italiano Titta Ruffo.

En esa época, siendo ya un adolescente, comenzó a frecuentar el barrio del Abasto, un barrio popular recién organizado alrededor del nuevo Mercado de Abasto, abierto en 1893. Gardel fue invitado por un grupo de jóvenes (José «El Tanito» Oriente, Domingo «Daguita» Vito) a integrarse a la «barra» del café O'Rondeman, que estaba en Agüero y Humahuaca. El café era propiedad de los hermanos Traverso (Alberto o «Giggio», Constancio, Félix y José o «Cielito»). Estaba administrado por el primero de ellos, el «Gordo» Giggio o Yiyo, que establecería con Gardel una relación de gran afecto mutuo, con características paterno-filiales, al punto que cuando aquel falleció en 1923, Carlos fue uno de los que sostuvo el féretro. Los hermanos Traverso, liderados por Constancio, dominaban políticamente el barrio del Abasto, en nombre del Partido Autonomista Nacional, el partido conservador fundado por Julio Argentino Roca que gobernó sin alternancia el país, sobre la base del fraude electoral, entre 1874 y 1916. El joven Gardel fue un protegido de los hermanos Traverso, que valoraron desde un inicio la calidad de su canto -de gran importancia para organizar la sociabilidad popular-, y promovieron su actuación tanto en el Bar O'Rondeman, como en los comités conservadores del barrio, de otras zonas de la ciudad, e incluso de Avellaneda, donde se relacionó con el hombre fuerte del conservadurismo bonaerense, Alberto Barceló y su famoso matón Ruggierito.

Gardel comenzó a cantar semiprofesionalmente en el café de los Traverso y en el comité conservador de Anchorena 666. Años después, en 1927, se mudó con su madre a una casa que compró exactamente a la vuelta del comité, actual Casa Museo Carlos Gardel. En esa época el canto popular estaba dominado por el arte de la payada, cuya figura máxima era Gabino Ezeiza. Gardel no tenía habilidad para inventar sus propios versos a medida que cantaba, que era la característica decisiva para el éxito de los payadores, pero la calidad de su voz le fue abriendo camino poco a poco. «Gardel nunca fue payador; él era cantor», dice el historiador Pablo Taboada.

De esa época viene la relación de Gardel con la payada, en especial con José Betinotti, a quien se le atribuye haberle puesto el apodo del «Zorzalito» o «Zorzal Criollo», es decir el nombre de uno de los pájaros característicos de las pampas, como es el zorzal, destacado por la belleza de su canto. Uno de los primeros temas que grabó Gardel fue «Pobre mi madre querida», la canción más famosa de Betinotti. También de esta época viene la relación con el payador Arturo de Nava. En 1922, el dúo Gardel-Razzano, grabaría la obra más famosa de De Nava, «El carretero», que se convirtió en el principal éxito de Gardel en su primera gira a Francia (1928/1929) y que luego fue incluida entre los famosos cortos cinematográficos musicales realizados en 1930, donde Gardel aparece hablando con el payador, ya en el momento de decadencia de su carrera, quien le agradece el hecho de cantar su tema.

Durante toda esa primera década como cantor, Gardel nunca cantó un tango, aunque lo bailaba. Construyó su estilo de canto a partir de la payada y las canciones camperas, pero también de la "canzonetta" napolitana y la ópera.

En 1910, siendo todavía desconocido para el público, cantó una noche para una tertulia habitual de yoqueis y cuidadores de caballos de pura sangre en la confitería La Frazenda, en el Bajo Belgrano, con motivo de haberle apostado a una yegua que ganó la carrera, obteniendo una importante ganancia. En 1936 un tal Laureano Gómez, que estuvo presente aquella noche, publicó un relato acerca de la presentación de Gardel:

Para la segunda década del siglo, Gardel era habitualmente referido como "El Morocho del Abasto".

A comienzos de la segunda década del siglo se encuentran Carlos Gardel y el uruguayo José Razzano, "El Oriental". En sus "Memorias" Razzano ubica ese encuentro en 1911, en la casa de un amigo ubicada en la calle Guardia Vieja, a pocos metros del Mercado de Abasto. Años después esa parte de la calle, entre Jean Jaurés y Anchorena, será renombrada como pasaje Carlos Gardel.

Gardel ya había empezado a cantar a dúo con Francisco Martino, sumándose Razzano y poco después el cuyano Saúl Salinas. Lo cierto es que los cuatro se mantuvieron vinculados, cantando alternativamente en dúos, trío y cuarteto, en diversos barrios y ciudades de Argentina de manera semiprofesional, hasta que poco a poco fue decantando el dúo Gardel-Razzano, estableciendo su barra de amigos y su base artística en el Café de los Angelitos, un punto intermedio entre el Abasto -donde "paraba" Gardel- y Balvanera Sur, donde estaba ubicado el "Café del Pelado" de Moreno y Entre Ríos (aún en pie), en el que "paraba" Razzano.

En ese período la Casa Tagini, que tenía la representación de Columbia Records y se había convertido en la principal empresa discográfica de Argentina, contrató a Gardel para grabar en 1912 siete discos dobles con canciones de su elección, que son lanzados al mercado en 1913, cuando todavía era un desconocido. Esos discos son la primera constancia de la presentación del joven cantor con el nombre de Carlos Gardel. Los siete discos fueron los siguientes:

La expresión «estilo» es la que se utilizaba en la época para referirse a los ritmos camperos y rurales. Por esa razón los cantores como Gardel eran llamados «estilistas». Algunas décadas después, en Argentina comenzó a usarse la expresión para denominar a esos géneros musicales. En el repertorio elegido por Gardel se destaca «Mi madre querida» ―canción emblemática del payador José Betinotti― y seis poemas musicalizados de Andrés Cepeda ―un poeta asesinado dos años antes y acosado por la policía debido a sus ideas anarquistas y su condición homosexual, que conmovía la sensibilidad del joven Gardel―. El resultado no fue el esperado y Gardel debería esperar hasta 1917 para volver a grabar nuevamente.

En 1914 Gardel y Razzano fueron contratados para cantar en el suntuoso cabaré Armenonville de Buenos Aires, por un caché de 70 pesos la noche, una suma inesperada que Gardel confundió con la retribución quincenal. Gobello considera que esa fue la primera actuación profesional de Gardel. El éxito de sus actuaciones en el Armenonville le abrieron al dúo las puertas hacia los grandes escenarios del espectáculo porteño. Pocos días después el célebre Pablo Podestá, los contrataba para cantar durante dos semanas en el espectáculo que estaba por estrenar en el Teatro Nacional, su primera actuación en la calle Corrientes.

Años después en una carta a Razzano escrita desde París, Gardel recordaría aquel debut en el Armenonville del siguiente modo:

1915 fue un año complejo para Gardel, en el que las dificultades del pasado y los éxitos del futuro parecieron confundirse. A mitad de año fueron contactados por el empresario uruguayo Manuel Barca que había ido a Buenos Aires para contratarlos para actuar en Montevideo. Los jóvenes recibieron la oferta incrédulos e inseguros. El destacado historiador montevideano Julio César Puppo cuenta del siguiente modo aquel encuentro:

Montevideo los recibió como si fueran celebridades, con la ciudad empapelada con sus retratos y un programa de actividades que incluía ser recibidos en el puerto, llevados a desayunar, entrevistas con la prensa y una actuación reservada para personas influyentes. El 18 de junio debutaron en el Teatro Royal, con lleno completo, y por primera vez el público les pidió que repitieran los temas al grito de «tocate otra, Carlitos». Dice Puppo que al terminar la función Gardel se puso a llorar de emoción en el camerino.
Desde entonces Gardel se sentiría en Montevideo como en su propia casa, con su propia barra de amigos, volvería a cantar una y otra vez y al final de sus días mandó a construir una vivienda en la que no llegó a vivir debido al accidente que le costó la vida.

Pocos días después, su condición de indocumentado lo llevó a proporcionar datos falsos para obtener documentos que le permitieran viajar a Brasil, en una gira de la Compañía Dramática Rioplatense encabezada por Elías Alippi, en la que al dúo le correspondía realizar el fin de fiesta.
En el barco conoció al cantante de ópera napolitano Enrico Caruso, quien elogió la voz de Gardel, pero la presentación de la compañía en São Paulo y Río de Janeiro no pudo superar la barrera del idioma, aunque la actuación del dúo recibió elogios de la prensa brasileña. Para mal de males Gardel fue detenido por la policía brasileña al haber sido encontrado en compañía de delincuentes argentinos que se habían establecido allí.
Del expediente formado para tramitar la documentación, surgió también que Gardel tenía antecedentes como estafador de poca monta por realizar «cuentos del tío».
Estos datos bloquearían años después el proyecto de nombrar a la Avenida Corrientes con su nombre.

En el barco de regreso de Brasil Alippi le ofrece al dúo participar de una nueva producción de "Juan Moreira", famosa obra fundacional del teatro argentino, estrenada exitosamente el 12 de noviembre en el teatro San Martín. "Los Gardel Razzano" cantaban en una siempre celebrada escena musical en una pulpería en la que baila Moreira, estrenando en esa oportunidad la cueca «Corazones partidos» de su excompañero Saúl Salinas.
En esa ocasión el dúo fue acompañado por 20 guitarristas encabezados por José Ricardo y Horacio Pettorossi. La diferencia de calidad los llevó a contratar desde entonces a Ricardo como guitarrista permanente del dúo, en tanto que Pettorossi integraría el grupo de guitarristas de Gardel en los años 30.

Antes de finalizar el año, en la noche del 10 al 11 de diciembre de 1915, recibió un balazo en un confuso episodio. El hecho sucedió durante un altercado en la calle luego de celebrar su cumpleaños en el Palais de Glace (salón de baile de la época en el barrio de la Recoleta), cuando estaba acompañado por los actores Elías Alippi y Carlos Morganti. Para entonces Gardel ya era conocido y el hecho apareció en la crónica policial de los diarios "La Prensa" y "La Razón" («Agresión a Gardel»), donde se señaló que los agresores fueron un tal Roberto Guevara -el autor del disparo- y Moreno Gallegos Serna, probables matones del bajo mundo, este último mencionado por Eduardo Arolas al dedicarle su tango «Suipacha».
Las causas y sucesos posteriores a la agresión permanecen confusos.
Su amigo Edmundo Guibourg relata que luego del ataque, Gardel fue a Tacuarembó para recuperarse, donde se encontró con el hermano menor del caudillo Traverso, «Cielito Traverso», escondido allí por haber asesinado a un hombre en el cabaré Armenonville.
También se ha difundido la información falsa de que el matón Roberto Guevara era en realidad Roberto Guevara Lynch, tío del todavía no nacido Che Guevara y miembro de una rica familia porteña.
Finalmente, al morir Gardel la bala aparecería en su autopsia, dando pie también a hipótesis sobre un enfrentamiento armado en el avión que habría causado el accidente que le costó la vida.

En la segunda década del siglo el mundo del espectáculo porteño se caracterizó por una enorme difusión del "varieté", una modalidad surgida en Francia y tomada de España, que consistía en una sucesión de actuaciones cortas, de los más diversos tipos (musicales, dramáticas, humorísticas, circenses, de magia, etc.). Luego de iniciarse en 1916, durante la temporada veraniega de Mar del Plata, a mitad del año son contratados para presentarse en el Teatro Esmeralda (luego Teatro Maipo), ubicado a metros de la famosa esquina tanguera de Corrientes y Esmeralda, y a principios del año siguiente debutan también en el vecino Empire Theatre, de la calle Corrientes y Maipú, dirigidas a un público de mayor poder adquisitivo. El éxito fue consagratorio y sus actuaciones se extenderían en ambos teatros por seis años.

En esa primera época la actuación del dúo Gardel-Razzano alternaba temas cantados como solistas y temas a dúo. El famoso folklorista Osvaldo Sosa Cordero recuerda haberlos visto cuando era un adolescente y contó que la presentación la abrieron a dúo interpretando «Brisas de la tarde», la primera canción del dúo sobre un poema de José Mármol, tras lo cual Razzano cantaba la cifra «Entre colores», una de las canciones con la que se lo identificaba. Luego cantaron a dúo «Cantar eterno» de Villoldo y el gato «El sol del 25». Los favoritos de Gardel para cerrar eran dos canciones sobre caballos, la gran pasión de Gardel: «El moro» (sobre un poema Juan María Gutiérrez) y «El pangaré».
Precisamente, en el suceso en el que fue baleado, el pistolero había gritado «¡Ya no vas a cantar más "El moro"!».

Simultáneamente Gardel volvería a grabar y ya no dejaría de hacerlo en adelante. Vencido el contrato leonino con la discográfica Tagini-Columbia, llega a un acuerdo con la empresa de Max Glücksmann, bajo sellos como Disco Nacional y Odeon. El contrato establecía un monto de cuatro centavos por disco vendido (simples doble faz). En esos discos de 1917 el dúo grabó su repertorio, entre ellos «Mi noche triste», el primer tango que grababa Gardel.
Los discos se vendieron masivamente, en cantidades que superaban las 50 000 unidades de cada uno, con ganancias en el orden de los 8000 pesos por cada uno.

Finalmente Gardel coronaría ese año excepcional, protagonizando la película muda "Flor de durazno", basada en una exitosa novela de Hugo Wast, que fue dirigida por Francisco Defilippis Novoa y en la que interpretó al protagonista Fabián. Se trata de una de los primeros largometrajes del cine latinoamericano, cuando todavía era mudo, lo que indica la visión integral del espectáculo que Gardel estaba desarrollando. Gardel estuvo a punto de abandonar la filmación, descontento con su desempeño actoral, pero fue convencido de quedarse por el director, con el argumento de incluir varias secuencias suyas cantando, argumento incierto si se tiene en cuenta que se trataba de una película muda, aunque existe información de que en 1940 fue proyectada una versión sonora de la película, que quizás incluyera esos fragmentos.
"Flor de durazno" fue estrenada el 28 de septiembre de 1917, con excelente respuesta del público, manteniéndose varios años en cartel y superando las 800 representaciones.

La creciente preocupación de Gardel por su imagen, que tenía sus antecedentes en su famosa sonrisa y la simpatía que lo caracterizó desde un principio, se evidenciará también en las primeras fotos de estudio que comienza a encargar, sobre todo de quien se volvería su fotógrafo preferido, el hispano-uruguayo José María Silva y en el trabajo que iniciaría para estilizar su cuerpo, teniendo en cuenta que Gardel era un hombre bajo (menos de 1,70 de altura) y que en ese momento pesaba alrededor de 120 kilos.
Los biográfos Julián y Osvaldo Barsky dicen que estas conductas de Gardel indicaban «su esfuerzo por construir el galán-cantor, figura que lo proyectará internacionalmente».

El éxito masivo del dúo y de Gardel en particular, así como su ingreso al tango, coinciden con un momento de gran importancia en la vida político-social de la Argentina: la conquista de la democracia. Las presiones políticas y sindicales habían obligado al gobierno conservador a aprobar una ley de voto secreto y obligatorio (solo para varones), que le dio el triunfo en 1916 al partido radical, un movimiento ampliamente popular que llevó a la presidencia a Hipólito Yrigoyen.

En 1917 Gardel cantó y grabó un tango por primera vez. Se trató del tango "Mi noche triste", un tema musical compuesto por Samuel Castriota titulado "Lita" al que Pascual Contursi le había puesto letra. La interpretación de "Mi noche triste" por Gardel está considerada como la fecha de nacimiento del tango canción: luego de décadas de evolución, el tango había empezado a encontrar cantores y letristas capaces de interpretar la misma cadencia emocional que ya expresaba la música y el baile de tango.

El éxito del novedoso estilo del tango canción no fue inmediato. "Mi noche triste", con su letra lunfarda y su temática sobre el hombre de pueblo abandonado por su mujer («Percanta que me amuraste»), fue recibido por el público sin ningún entusiasmo desbordante. Por otra parte, los cantores "puros" veían con malos ojos ese lenguaje de calle y esa sensualidad prosaica y de mal gusto, que se apartaba del "verdadero arte criollo".
El gran Gabino Ezeiza establecía con claridad el rechazo al tango al aconsejarle a Carlos Marambio Catán:

Al año siguiente (1918) el sainete "Los dientes del perro", puesto en escena por la compañía de Muiño-Alippi, incluyó una escena en la que la jovencísima actriz Manolita Poli cantaba "Mi noche triste". El número causó sensación y fue decisivo para que tanto la obra como la versión de Gardel, lanzada en disco ese año, fueran un gran éxito.
Desde entonces el sainete y el tango establecerán un vínculo estrecho, promoviéndose mutuamente.

Ese año Gardel grabó otro tango, "A fuego lento", también de Contursi, y poco a poco fue construyendo un repertorio integrado mayoritariamente por tangos.
La voz y la manera de cantar de Gardel también fueron evolucionando a medida que se iba convirtiendo en un cantor de tangos. Gardel aprovecha sus orígenes en el ámbito de la payada y su gusto por la canzonetta napolitana y la ópera, en una ciudad considerada como "la más italiana fuera de Italia", en la que las personas de origen italiano, sobre todo los jóvenes, se habían vuelto el grupo étnico más numeroso, para desarrollar un canto más lento, grave, melancólico y menos ansioso, caracterizado por una interpretación emocional que lo ligaba a los sentimientos del oyente.

En 1919 solo una de las trece canciones que grabó Gardel ese año, fue un tango. En 1920 ya eran seis sobre veinticuatro (un 25%) y en 1921, ocho sobre veintidós (un 30%). Hasta que en 1922 los tangos superaron la mitad: doce sobre veintiún canciones grabadas.
En ese período el dúo suma un segundo guitarrista, Guillermo Barbieri y en 1923 Gardel estrena el tango "Mano a mano" («Rechiflado en mi tristeza»), con letra de Celedonio Flores, un notable poeta descubierto por Gardel en 1920, del que ya había grabado "Margot". "Mano a mano" se constituyó en uno de los máximos éxitos de Gardel, marcando el momento en que el tango canción terminaba de imponerse y, junto a las transformaciones instrumentales de músicos como Julio de Caro, se abría una era de plenitud para el género: la Guardia Nueva. En total Gardel grabaría 21 tangos de "El Negro Cele", entre ellos "El bulín de la calle Ayacucho", "Malevito", "Viejo smoking", "Mala entraña", "Canchero" y "Pan".
Flores también es autor de la letra del famoso tango "Corrientes y Esmeralda" (1933) que dice "en tu esquina rea, cualquier cacatúa sueña con la pinta de Carlos Gardel". Por humildad, Gardel se abstuvo de cantar ese célebre tango que lo idolatraba en vida.

Gardel sin embargo nunca dejaría de cantar los ritmos populares más variados. A través de músicos como el cordobés Cristino Tapia y el santiagueño Andrés Chazarreta incluyó nuevas canciones del folklore argentino norteño, a la vez que incorporaba cuecas chilenas, bambucos colombianos, foxtrots, shimmys, valses, tangos españoles, canciones en italiano, francés e inglés, y hasta una balada rusa como "Sonia" compuesta por un húngaro y un judío austríaco que años después sería asesinado por los nazis en Auschwitz o un tango con expresiones en guaraní como "Los indios" de Canaro y Caruso.
El investigador Félix Scolatti, quien acompañó al dúo en su única gira por Chile en 1917, contó que Gardel estaba todo el tiempo buscando nuevos ritmos populares y que oía con atención lo que cantaba la gente común en las calles y las plazas, memorizándolas y tomando notas, para después identificarlas.

En 1920 gestionó ante el consulado uruguayo en Buenos Aires, una certificación de nacionalidad y una cédula de identidad, donde declara haber nacido en Tacuarembó, Uruguay, en 1887. En 1923, con los documentos uruguayos obtenidos, solicitó la nacionalidad argentina, que le fue concedida inmediatamente, expidiéndose el pasaporte argentino que utilizaría para viajar.

Consolidado en su dominio del tango canción y con su dúo con Razzano en el punto más alto de celebridad en la Argentina, Gardel ya estaba en condiciones de apuntar a Europa y al mercado musical internacional creado por el disco, mercado que en el futuro inmediato se verá amplificado por el cine y la radio.

El tango venía difundiéndose como baile de moda en Europa desde la primera década del siglo, estallando la "tangomanía" poco antes de la Primera Guerra Mundial (1914-1918). En 1921 el italiano Rodolfo Valentino, adoptando la identidad de un bailarín argentino de tango vestido como gaucho, causaba sensación mundial con la película "Los cuatro jinetes del Apocalipsis". España en particular, tenía una historia tanguera previa incluso al tango argentino (el tango flamenco) y desarrollaría una importante vertiente autónoma del género, con epicentro en Barcelona y con canciones paradigmáticas, como «Fumando espero» del catalán Josep Viladomat y revistas especializadas como "Tango Moda".

En ese contexto Gardel estaba a punto de mostrarle al mundo un modo de cantar el tango que lo ubicaría entre los cantantes célebres de la historia de la música popular.

En 1923 el dúo Gardel-Razzano tiene la oportunidad de realizar su primera gira a Europa, puntualmente a España, acompañando a la compañía teatral encabezada por la actriz Matilde Rivera y su esposo el actor Enrique de Rosas. Como una estrategia escénica derivada del estereotipo internacional de la Argentina, los empresarios teatrales insistieron en que los músicos se presentaran vestidos de gauchos, aunque en Buenos Aires actuaran vestidos de esmoquin. Por esa razón, antes de partir se sacaron en Montevideo una nueva serie de fotos con José María Silva, disfrazados de gauchos. Debutaron el 10 de diciembre en el Teatro Apolo de Madrid, actuando con sus dos guitarristas como «fin de fiesta», luego de la representación dramática que la compañía realizaba cada noche. Las críticas sobre el dúo fueron buenas y luego de 40 presentaciones y con la misión de haber desembarcado en Europa cumplida, dejaron la compañía para ir a Francia, donde conocieron París y Gardel visitó a la familia Gardes en Toulouse.

En septiembre de 1925, luego de 12 años de cantar juntos y debido a una lesión de laringe de Razzano, el dúo decide separarse pasando Razzano a ejercer las funciones empresariales.
Años después, luego de un estricto trabajo foniátrico, Razzano intentaría volver al canto, pero sin mayores resultados. De ese intento final quedarían las dos últimas grabaciones del dúo, «Claveles mendocinos» de A. Pelaia y «Serrana impía», de José del Valle, grabadas el último día de 1929.

Mientras tanto, ya como solista, Gardel volvió a realizar giras por Europa, actuando nuevamente en España (1925/1926 y 1927) y luego en Francia (1928/1929). La gira de 1925/1926, con la misma compañía teatral que la realizada dos años antes, incluyó esta vez también a Barcelona, una ciudad que establecería un vínculo muy especial con Gardel.
El éxito obtenido allí lo llevó a extender sus actuaciones de diez días iniciales a dos meses. En Barcelona Gardel grabó también veintiún temas, incluyendo un tango que le ofreció en ese momento el pianista madrileño Teodoro Diez Cepeda, «Dolor», el primero de varios tangos y canciones españolas que Gardel irá grabando en el futuro, expresión de su vocación constante de conectarse con la canción popular de cada lugar. En la gira de 1925/1926, Gardel también se presentó con un éxito moderado en Madrid y en Vitoria, en el País Vasco, donde hasta ese momento era completamente desconocido.

Estando en Barcelona Gardel grabó varios temas para el sello Odeón utilizando por primera vez la grabación eléctrica con micrófono (antes se usaba bocina captora), el cambio tecnológico más significativo hasta el descubrimiento del elepé a fines de la década de 1940.
En Argentina, Gardel comenzará a grabar mediante este sistema desde el noviembre de 1926. La calidad de sus grabaciones mejorará notablemente desde ese entonces.
Los años 1926 y 1927 fueron los años en que más discos grabó, superando en ambos las 100 canciones. De los éxitos de esa época se destacan «Bajo Belgrano» y «Siga el corso» («Esa Colombina puso en sus ojeras humo de la hoguera de su corazón»), ambos de Aieta y García Jiménez, la primera versión de «Caminito», «A media luz» («Corrientes 348, segundo piso ascensor»), «Tiempos viejos» («¿Te acordás, hermano? ¡Qué tiempos aquellos!») de Canaro y Romero, así como las primeras canciones de letristas jóvenes que se volverían clásicos, como Enrique Cadícamo y Enrique Santos Discépolo («Que vachaché»).

En 1927 Gardel compra para él y su madre, su primera y única casa propia, ubicada en la calle Bermejo (luego Jean Jaurés) 735, una casa sencilla en el corazón de su barrio espiritual, el Abasto, exactamente a la vuelta del comité conservador en el que comenzó a cantar.

A fines de 1927 Gardel inició una nueva gira por España, la tercera, actuando en Barcelona, Madrid, Bilbao y Santander, con un éxito arrasador. «Vino Gardel y supimos lo que eran los tangos argentinos», sintetizó por entonces el periodista y músico catalán Brauli Solsona.
Antes de volver a Buenos Aires, Gardel pasó por París donde cerró un contrato para presentarse en París, en el segundo semestre de 1928.

Su estadía en Buenos Aires fue breve y luego de grabar varios discos (entre ellos el primer éxito de Discépolo «Esta noche me emborracho»), de cantar por radio Prieto, presentarse en el Teatro Solís de Montevideo y contratar un tercer guitarrista, el «Indio» José Aguilar, zarpó nuevamente para Europa, esta vez con destino a París.

Cuando Gardel debutó en París, el tango en Francia ya tenía más de dos décadas de historia y junto al jazz, era protagonista de la noche parisina.
El centro de la vida nocturna en París era Montmartre y la aledaña plaza Pigalle y el centro de la vida tanguera era el restaurante «El Garrón», donde durante casi una década había brillado el músico Manuel Pizarro, que jugó un papel crucial promoviendo la contratación de Gardel. «Sin embargo -dicen los biógrafos Barsky-, a todo ese despliegue tanguero de casi tres décadas (en París) le faltaba una gran voz».

Gardel y sus tres guitarristas (Ricardo, Barbieri y Aguilar) debutaron en París el 30 de septiembre de 1928, en una función de beneficencia en el teatro Fémina en Les Champs-Élysées y luego el 2 de octubre en el cabaré Florida, en Montmartre. Sus presentaciones se extendieron hasta abril de 1929, actuando también en los teatros Empire y Paramount, así como en las ciudades de Cannes y Montecarlo, siendo el punto más alto la invitación a participar del distinguido evento benéfico Bal des Petit Lits Blancs en la Ópera de París. Interpretó un repertorio variado, que incluía canciones en francés, que fueron muy bien recibidas. De aquel repertorio se destacaron los tangos «Adiós muchachos» y «Siga el corso», pero sobre todo la canción campera de Arturo de Nava, «El carretero», que Gardel interpretaba con silbidos, como si él mismo estuviera arreando los bueyes de la carreta.
El éxito fue rotundo, la venta de discos superó todas las previsiones y los parisinos silbaban «El carretero» por las calles.
En la Navidad de ese año, su foto fue tapa de "La Rampe", la principal revista de espectáculos, mientras que el mayor periódico francés, "Le Figaro", ya había descripto su presentación en la Ciudad Luz como un «éxito triunfal» y explicaba del siguiente modo la sensación que el cantor argentino generaba sobre el público:

Luego de actuar seis meses en París, Gardel actuó un mes más en Barcelona y Madrid, antes de retornar a Buenos Aires, donde llegó el 16 de junio de 1929, nueve meses después de su partida. En Madrid, tras catorce años con Gardel, Ricardo decidió dejar el grupo de guitarristas, al parecer disgustado con el protagonismo instrumental que pretendía Aguilar.

Permanecerá en Buenos Aires durante un año y medio. En ese plazo se presentó en Buenos Aires y Montevideo, realizó una extensa gira por las provincias argentinas, cantó por Radio Nacional, se sometió a una pequeña operación en sus cuerdas vocales y contrató a Domingo Riverol para reemplazar a Ricardo.

El 6 de septiembre de 1930 se produjo un golpe de estado cívico-militar en Argentina que derrocó al presidente democrático Hipólito Yrigoyen. Fue el primero de una serie interrupciones de la institucionalidad democrática que se extenderá hasta 1983. En esa ocasión Gardel asumió una posición de apoyo al golpe grabando el tango «¡Viva la patria!» de Aieta y García Jiménez, que lo enfrentó con los sectores yrigoyenistas de la Unión Cívica Radical, que al menos en un par de ocasiones boicotearon sus actuaciones.
Simultáneamente Gardel comienza a tener dificultades económicas y diferencias con Razzano, quien se desempeñaba como su representante, que le granjean enemistades en la barra de amigos que compartían y en el medio artístico y periodístico.

Finalmente, en 1930 Gardel iniciaría una nueva modalidad para difundir su canto, que redefinirá radicalmente su carrera y la masividad de su arte: el cine.

A fines de los años 20 la industria cinematográfica argentina mostraba una enorme vitalidad que la convertiría en una de las tres más importantes de América Latina, con México y Brasil durante el siglo.
Carlos Gardel, por su parte, ya había tenido una importante incursión cuando el cine era mudo, demostrativa de su sensibilidad para detectar los mecanismos modernos de construcción de la popularidad masiva, más allá incluso de las fronteras nacionales. Su amigo Enrique Cadícamo diría que:

En muchos aspectos Gardel se anticiparía en décadas a fenómenos culturales de masas en los que se unen la pasión, la identificación personal y la música, como la beatlemanía de los años 60, o fenómenos latinos equivalentes como Sandro, Soda Stereo y Luis Miguel, en los años 70, 80 y 90.
El crítico Claudio Iván Remeseira ha utilizado incluso la palabra «gardelmanía» para referirse a esta última etapa de la vida de Gardel.

En 1930 Gardel protagoniza quince cortometrajes musicales sonoros, cada uno sobre una canción, con dirección de Eduardo Morera y producción de Federico Valle, uno de los pioneros del cine latinoamericano. Valle había nacido en Italia en 1880, y luego de trabajar con los Hermanos Lumière y tomar clases con Georges Méliès, emigró a la Argentina en 1911 y desde entonces produjo decenas de obras cinematográficas de gran valor, incluyendo los primeros noticieros y los largometrajes animados de Quirino Cristiani, los primeros en la historia del cine mundial en su género.

De los quince cortos, cinco resultaron arruinados en el laboratorio, entre ellos uno titulado "Leguisamo solo" en el que aparecía el yóquey Irineo Leguisamo. En 1995 fue hallado otro de los cortos no lanzados, "El quinielero", de Luis Cluzeau Mortet y Roberto Aubriot Barboza.

Los diez cortos lanzados fueron: "El carretero", "Añoranzas", "Rosas de otoño", "Mano a mano", "Yira, yira", "Tengo miedo", "Padrino pelao", "Enfundá la mandolina", "Canchero" y "Viejo smoking".

El más elaborado de todos es "Viejo smoking", un tango con letra de Celedonio Flores y música de su guitarrista Guillermo Barbieri, en el que Gardel antes de cantar, protagoniza un sketch dramático con César Fiaschi e Inés Murray sobre el desempleo, la pobreza y el desalojo, en el contexto de la Gran Depresión. También se destacan "Yira, yira", "El carretero", "Mano a mano" y "Rosas de otoño" en los que Gardel sostiene diálogos muy significativos con sus autores, Enrique Santos Discépolo, el payador Arturo de Nava, Celedonio Flores y Francisco Canaro, respectivamente.

Los cortometrajes fueron filmados en Buenos Aires entre el 23 de octubre y el 3 de noviembre de 1930 y estrenados el 3 de mayo de 1931 en el cine Astral de la calle Corrientes. En algunos casos fueron presentados como «tangos teatralizados».

Se ha afirmado que los cortometrajes de Gardel fueron los primeros videoclips de la historia del cine y la primera producción de cine sonoro de Argentina y eventualmente América Latina.
La afirmación es parcialmente cierta. Los cortometrajes de Gardel constituyeron la primera producción de cine sonoro con banda de sonido incorporada en la película (sistema Movietone) realizada en América Latina, pero desde 1927 en el mundo ("El cantor de jazz") y 1929 en América Latina ("Mosaico criollo"), se habían realizado películas sonoras musicales, utilizando el sistema de discos sincronizados (sistema Vitaphone), y también con anterioridad se había utilizado el sistema Movietone (banda de sonido) para filmar clips musicales de Sofía Bozán con dirección de José Bohr, aunque realizados en Nueva York.
Más allá del debate sobre la prioridad cronológica, los diez cortometrajes de Gardel, dirigidos por Morera y producidos por Valle, constituyen un esfuerzo pionero tanto de la industria cinematográfica latinoamericana, como del videoclip musical.

En enero de 1931 Gardel emprendió una nueva gira por Francia, iniciada con dos meses de presentaciones exitosas en Niza, donde despide a su guitarrista Aguilar por un comentario homofóbico en su contra.
A fines de abril llegó a París con la firme determinación de filmar una película en los estudios que la empresa estadounidense Paramount tenía en la localidad de Joinville-le-Pont, a 40 kilómetros al sudoeste de la capital francesa, dedicada a producir películas para los mercados no estadounidenses. Pocos días después, el 1 de mayo, lograba su objetivo y firmaba un contrato que lo incluía en un largometraje musical protagonizado por las figuras de la compañía de revistas del Teatro Sarmiento de Buenos Aires, propiedad de Augusto Álvarez, encabezada por Manuel Romero y Luis Bayón Herrera. La película estuvo finalizada a fin de mes y se llamó "Las luces de Buenos Aires".

El director fue el chileno Adelqui Millar y los guionistas Manuel Romero y Luis Bayón Herrera. Los actores principales eran Gardel, en el papel protagónico del estanciero Anselmo Torres, y Sofía Bozán, su novia que está a punto de ser corrompida por un empresario de Buenos Aires al igual que su amiga interpretada por Gloria Guzmán y que termina siendo rescatada por los gauchos, para devolvérsela al estanciero (Gardel). El elenco se completaba con Vicente Padula, Pedro Quartucci y Carlos Baeza, entre otros.

La película incluye escenas de canto y baile (tango, malambo y otras danzas folklóricas). Para componer la música se contrató a Gerardo Matos Rodríguez, el autor de «La cumparsita» y para ejecutarla a Julio de Caro (violín), su hermano Francisco de Caro (piano) y Pedro Laurenz (bandoneón). Gardel canta dos canciones, un tango propio titulado «Tomo y obligo», con letra de Romero que canta en una cantina de La Boca en una famosa escena que generaba el delirio popular, y el vals «El rosal» de Matos Rodríguez y Romero, que es el tema romántico con el que cierra el filme mientras Gardel besa a su novia (Bozán). Bozán también canta dos canciones, en tanto que Guzmán canta una.

"Las luces de Buenos Aires" fue estrenada en Buenos Aires el 23 de septiembre de 1931. Primero en los cines más importantes de Buenos Aires y luego en los cines de barrio y del resto del país, con enorme éxito.
Pero lo más importante fue la recepción de la película en los países de habla hispana que nunca habían podido ver a Gardel. En Guatemala se exhibió durante tres años, en Madrid todos los días durante tres meses, en Barcelona y Nueva York el público obligaba a los operadores a rebobinar la película una y otra vez para oír nuevamente «Tomo y obligo». En Ecuador el escritor Ricardo Descalzi recordaba aquel momento:

Gardel comenzaba a concretar la imagen de galán-cantor que había constituido la médula de su proyecto artístico desde un inicio. Su figura se había estilizado y continuaría estilizándose hasta pesar 76 kilos, luego de haber llegado a los 120 kilos.

En agosto de 1931 volvió a Buenos Aires donde permanecerá nueve semanas. Paradójicamente, al mismo tiempo que su popularidad en Argentina, Uruguay y todo el mundo hispanohablante comenzaba a alcanzar proporciones nunca antes vista, en el mundo artístico y los medios de comunicación de Buenos Aires crecía un sentimiento de rechazo a Gardel, por lo que se percibía como una pérdida de contacto con sus raíces «criollas». El 15 de septiembre de 1931, su amigo, el escritor Carlos de la Púa, publicó en el diario "Crítica", una dura carta abierta titulada «Che Carlitos, largá la canzoneta»:

Al volver a París Gardel encontró a la Paramount francesa en plena crisis, en el marco de la depresión mundial y de un clima político que se enrarecía, pocos meses antes de que Hitler tomara el poder en Alemania. Transcurrido el primer semestre de 1932 sin novedades y cuando Gardel ya había decidido volver a Buenos Aires, la empresa decidió realizar nuevas películas con el cantor argentino. Para ello designó al experimentado director francés Louis Gasnier, definió un guion adaptado de otro proveniente de Estados Unidos y fijó la fecha de filmación en septiembre de 1932 de una película que llevaría el título de "Espérame", con el subtítulo de "Andanzas de un criollo en España".

Es en ese momento que toma importancia la presencia de Alfredo Le Pera, con quien Gardel había empezado a congeniar en diciembre del año anterior. Le Pera asumió la carga de reescribir en un brevísimo tiempo un guion lleno de incongruencias culturales y geográficas, producto de los estereotipos estadounidenses sobre el mundo hispanoamericano y de luchar contra el director para aportar humanidad cotidiana a las escenas, rol que reiteraría en las películas siguientes, pero que nunca lo dejaría satisfecho.
Pero además el encuentro de Gardel y Le Pera, dará vida a gran parte de las canciones con las que quedará asociada la fama mundial del cantante.

El equipo musical organizado por Gardel estaba integrado por el brillante pianista Juan Cruz Mateo, con quien ya venía actuando y grabando discos, José Sentis (alias Teruel), uno de los argentinos que había instalado el tango en París, el compositor francés Marcel Lattès que sería asesinado por los nazis en Auschwitz, el director de orquesta cubano Don Aspiazú, el guitarrista Héctor Pettorossi quien lo acompañara años atrás en "Juan Moreira" y el compositor Mario Battistella, que se volvería un autor crucial para Gardel.

Pese a los esfuerzos de Le Pera, la película finalmente no pudo superar la endeblez del guion original, con actores mediocres que desmerecen el resultado final. Gardel interpreta a Carlos Acuña, un cantor enamorado de una joven (Goyita Herrero) que a su vez se ve extorsionada por un rico estanciero. Finalmente el estanciero es expuesto públicamente y el cantante se queda con la joven. Gardel canta cuatro canciones, entre ellos el tema principal de la película, la rumba «Por tus ojos negros», que compusieron Aspiazú, Le Pera y Carlos Lenzi, el autor de «A media luz». Los otros tres temas son «Estudiante», «Me da pena confesarlo» y «Criollita de mis ensueños» de Gardel, Le Pera y Battistella.

La Paramount tomó conciencia de que era necesario acompañar a Gardel con un elenco de mayor nivel artístico y que había que otorgarle más libertad a Le Pera como guionista. Sin embargo -como sucedería con todas las películas de Gardel para la Paramount-, las decisiones del director y de los encargados del montaje, limitarían los esfuerzos de Le Pera para construir films con hondura humana y restablecerían los estereotipos.
Nada de eso, de todos modos, impediría que las películas de Gardel alcanzaran una inserción profunda en el alma de los pueblos hispano-hablantes.

En cuanto terminó la filmación de "Espérame" la Paramount convocó a la famosa actriz argentino-española Imperio Argentina para encarar la siguiente película. En octubre de 1932 Gardel e Imperio Argentina realizaron un cortometraje picaresco, con algunos diálogos osados para la época, titulado "La casa es seria", dirigido por Jaquelux (Lucien Jaquelux).
Gardel canta en el corto dos canciones que compuso con Le Pera, el tango «Recuerdo malevo» y «Quiéreme». En 1940 cuando París fue ocupada, la cinta fue destruida por las tropas alemanas. Sobrevivieron sin embargo algunos discos Vitaphone que se habían grabado, registrando la totalidad de los diálogos y canciones del film.

Inmediatamente después comenzó la filmación de "Melodía de arrabal", otra vez con dirección de Gasnier y guion de Le Pera, que incluirá canciones fundamentales del cancionero de Gardel, como el tango «Melodía de arrabal» («Barrio plateado por la luna, rumores de milonga»), de Gardel, Le Pera y Battistella y «Silencio», de Gardel, Le Pera y Pettorossi. La película es un policial diseñado por Le Pera, en el que Gardel interpreta a un cantor de tangos (Ramírez) en una cantina de La Boca, que con otro nombre también es jugador profesional de cartas. Su novia es Imperio Argentina. En una pelea Ramírez mata a un matón y se deshace del cuerpo con un ingenioso ardid utilizando un fósforo. El comisario a cargo de la investigación descubre que Ramírez es el mismo que le había salvado la vida años atrás y en la última escena le devuelve el fósforo. Gardel canta también «Cuando tú no estás» y «Mañanita de sol», esta última a dúo con Imperio Argentina, la única mujer con la que cantó.

Gardel volvió a Buenos Aires el 30 de diciembre antes que las películas fueran estrenadas. El 5 de abril de 1933 se estrenó "Melodía de arrabal", el 19 de mayo "La casa es seria", y el 5 de octubre se estrenó "Espérame".
Como era esperable "Melodía de arrabal" tuvo mejor recepción que "Espérame", y a los tres meses ya había superado el éxito de taquilla que había alcanzado "Las luces de Buenos Aires".

En 1933 se terminó de romper su relación con Razzano por diferencias económicas, agravadas por el desorden en sus cuentas y las deudas. Designó entonces como representante a Armando Defino, uno de sus amigos provenientes de la barra del Café de los Angelitos, antes de ser famoso. Ese año concentró sus presentaciones en el interior de Argentina (Rosario, Santa Fe, Paraná, Arrecifes, San Pedro, Azul, Olavarría, Mendoza, San Juan, Córdoba, Villa María, entre otras ciudades), cantó por radio con una audiencia inaudita, participó en Buenos Aires de la revista "De Gabino a Gardel", y grabó varios discos difundidos bajo el título de "Gardel canta a Gardel", en los que, utilizando una novedosa tecnología, canta a dúo consigo mismo.

En septiembre conoció en la radio a Hugo Mariani, un uruguayo que hacía años que vivía en Nueva York, donde había creado y dirigía la Orquesta Sinfónica de la National Broadcasting Corporation (NBC), que tenía incluso un programa llamado "El tango romántico". Ambos simpatizaron inmediatamente y Mariani le propuso ir a Nueva York para cantar por la NBC. Pocos días después le llegaba el contrato.

En la noche del 7 de noviembre de 1933 Gardel partió hacia Europa en el barco Conte Biancamano, antes de ir a Estados Unidos. A la mañana Defino, su representante, le había pedido que redactara un testamento ológrafo, es decir escrito de su puño y letra. Ese fue el último día de Gardel en Buenos Aires.

El 7 de noviembre salió nuevamente de gira: fue la última vez que estuvo en Argentina. Primero fue a Barcelona y París, y luego viajó acompañado por el talentoso pianista Alberto Castellanos a los Estados Unidos, donde debutó en la radio de la NBC de Nueva York, el 30 de diciembre. En sus actuaciones radiales en Nueva York, Gardel prescindió de sus guitarristas, con excepción de la audición del 5 de mayo de 1934, en la que cantó desde Nueva York, siendo acompañado de las guitarras de Barbieri, Vivas y Riverol, que se encontraban en Buenos Aires, por medio de un enlace entre la NBC y LS5 Radio Rivadavia de Buenos Aires, inédito en la historia de la radiofonía.

En la NBC, Gardel cantó acompañado por la destacada orquesta de la radio neoyorquina, dirigida por Hugo Mariani, desempeñándose como arreglador el músico argentino Terig Tucci, radicado hacía años en Estados Unidos, que le aportó un nuevo sonido basado en armonías novedosas. Tucci y Castellanos congeniaron de inmediato y fueron quienes le sugirieron a Gardel un cambio mayor en su canto: que extendiera el registro de su voz hacia los tonos graves, para llegar a un barítono alto.
Es con esa voz que Gardel registrará sus últimas y más célebres canciones. Gardel pasó por un período de adaptación a la orquesta y las nuevas armonías de Tucci, pero cuando llegó el momento de formar el equipo de músicos que lo acompañaría en sus películas estadounidenses, lo llevó a Tucci como arreglador musical y director.

Las audiciones de Gardel tuvieron una excelente recepción en la importante comunidad latina de Nueva York, que en aquel entonces era de unas 500.000 personas, en una época en la que la población de origen latino en Estados Unidos era muy pequeña.

Gardel convocó a Le Pera a Nueva York para que actuara como su representante en las negociaciones con la Paramount, en un momento en la que Estados Unidos padecía la gran depresión de la década de 1930. El contrato se firmó el 20 de marzo de 1934, acordando crear una empresa productora subsidiaria del gigante del cine estadounidense con el nombre de Éxito Corporation, cuyo único accionista fue el cantante argentino.
Inicialmente se realizarían dos películas ese mismo año: "Cuesta abajo" y "El tango en Broadway".

Cuesta abajo debió filmarse en dos semanas, con guion escrito por Le Pera, a partir de su propia vivencia autobiográfica. Gardel y Le Pera tenían en claro que no iban a repetir la imposición europea de que los personajes debían vestir como gauchos. El director fue una vez más el francés Louis Gasnier, impuesto por la Paramount. Encontrar actores que hablaran español fue uno de los grandes problemas de producción. Para los papeles principales contrataron a Mona Maris, una argentina que vivió desde muy niña en Europa, Vicente Padula, Manuel Peluffo y Anita del Campillo. Para los papeles secundarios Gardel convenció para que actuaran en su película a los diplomáticos de Argentina, Chile, Colombia, Venezuela y Perú.

El argumento trata de un cantante de tango (Gardel como Carlos Acosta), que deja a su novia (Anita del Campillo), al enamorarse de una prostituta (Mona Maris). En el momento culminante de la película Gardel se enfrenta a duelo con el "cafisho" (Manuel Peluffo) e intenta matarla a ella, en una célebre escena en la que canta "Cuesta abajo" ("si arrastré por este mundo, la vergüenza de haber sido y el dolor de ya no ser..."), de Gardel y Le Pera.

En las canciones principales de la película ("Cuesta abajo", "Mi Buenos Aires querido", "Amores de estudiante" y "Criollita decí que sí"), Gardel está acompañado por un notable quinteto dirigido por Alberto Castellanos en el piano, junto a Remo Bolognini y Hugo Mariani, como primer y segundo violín, Washington Castro en violonchelo y Humberto Di Tata en contrabajo.

Gardel, Le Pera y Castellanos no quedaron del todo satisfechos con la película, sobre todo porque en el proceso de edición, Gasnier y los técnicos de la Paramount quitaron muchas de las escenas más divertidas y chispeantes. Pese a ello el film tuvo un éxito apoteósico, tanto en Estados Unidos como en América Latina. En Nueva York miles de personas desbordaron el cine, ocupando las calles, al punto de que la empresa exhibidora colocó parlantes en la calle para que el público que no pudo entrar a la sala, pudiera oír las canciones.

Paradójicamente, solo en los medios periodísticos de Buenos Aires se elevaron críticas a la película, especialmente en la clase alta. Mientras que el público desbordó los cines y exigía que se repitieran las canciones, los diarios tradicionales "La Nación" y "La Prensa", cuestionaron la película por la mala imagen que daba de la Argentina. También el letrista Homero Manzi, cuestionaría duramente el film, en un artículo publicado en la revista "Antena", cuestionándola por carecer de "valor nacionalista".

El tango en Broadway fue filmada entre fines de junio y la tercera semana de julio de 1934 y el director volvió a ser Louis Gasnier. Para esta ocasión Gardel y Le Pera buscaron hacer una comedia de enredos que cambiara el tono de las películas anteriores y que a su vez reflejara la situación de los artistas latinos en Estados Unidos.

Nuevamente tuvieron problemas para conseguir actores y actrices que hablaran español. Los papeles principales fueron interpretados por el español Jaime Devesa (el tío), un hallazgo, nuevamente el argentino Vicente Padula, la guatemalteca Blanca Visher (Laurita) y la mexicana Trini Ramos (Celia). Gardel hace de Carlos Bazán, un representante de artistas latinos.

El argumento trata de un grupo de artistas latinos en Nueva York reunidos alrededor de Gardel, cuando llega el tío, frente a quien intenta aparentar que es un hombre de negocios. Laurita entonces se hace pasar por la novia de Carlos, y su novia, Celia, se hace pasar por su secretaria, generándose sucesivos enredos. Carlos se da cuenta que en realidad ama a Laurita en una de las escenas culminantes de la película, cuando canta el tango "Soledad" ("En la doliente sombra de mi cuarto al esperar, sus pasos que quizás no volverán...").

Las canciones que interpreta Gardel en la película son "Soledad", el foxtrot "Rubias de New York", "Golondrinas" y "Caminito soleado". En las tres primeras fue acompañado por la orquesta de Terig Tucci y en la última por el piano de Castellano con las guitarras de Cáceres, Ayala y Cornejo.

Una vez más, Gardel y Le Pera no quedaron plenamente satisfechos con la película y tomaron la decisión de prescindir de Gasnier en las siguientes. Ello no obstó a que el film volviese a tener un enorme éxito, consolidando la "gardelmanía" que se estaba gestando en la comunidad latina estadounidense y en los países hispanoamericanos.

Luego de ir a Francia por poco tiempo, a fines de 1934 volvió Nueva York, actuando en la NBC y filmando el musical Cazadores de estrellas (su título original es "The Big Broadcast of 1936"), un catálogo de las estrellas musicales de la Paramount, entre las que se encontraban Bing Crosby, Richard Tauber, La Orquesta de Ray Noble con Glenn Miller en el trombón, Los Niños Cantores de Viena, entre otras grandes figuras. En este film Gardel trabajó junto a Celia Villa, hija del famoso revolucionario mexicano Pancho Villa a quien Gardel admiraba.
Cantó dos canciones suyas con Le Pera: un tema campero "Apure delantero buey" y "Amargura" en una versión bilingüe español-inglés con el título de "Cheating muchachita". La película se estrenó poco después de la muerte de Gardel y por esa razón la Paramount decidió sacar la parte del cantor argentino. Las secciones cortadas en las que aparece el "Rey del Tango" son muy difíciles de hallar, aunque se sabe que alguna copia de ellas se encuentran en poder de coleccionistas privados.

Entusiasmada con el éxito de las películas de Gardel, la Paramount decidió filmar dos películas más en 1935, que serían sus últimas películas. Gardel y Le Pera ya habían decidido que su etapa de trabajo con Gasnier estaba agotada y eligieron a John Reinhardt, un joven director de origen austríaco, que había dirigido varias películas con actores latinos y que se mostraba mucho más receptivo a las sugerencias de Gardel y Le Pera.
Dos características centrales resaltan en estas últimas dos películas: la primera es la decisión de registrar el canto de Gardel en vivo, eliminando el doblaje tradicional en posproducción; la segunda es la decisión de dirigir el tono de las películas hacia el público de los países habla española, alejándose tanto del tono porteñista, como de los estereotipos europeo-norteamericanos.

El día que me quieras se filmó en enero de 1935, con Reinhardt como director y Le Pera como libretista, ante el fracaso de los libretos que la Paramount encargó a autores argentinos e hispanoamericanos, buscando mejorar el nivel.

También mejoró la calidad de los actores seleccionados. En los papeles principales que acompañan a Gardel (Julio Argüelles) figuran la actriz española Rosita Moreno (que interpreta a Margarita y su hija Marga), Tito Lusiardo (Rocamora), Fernando Adelantado (Carlos Argüelles) y Mario Peluffo (Saturnino). También actúa en una breve escena, por pedido de Gardel, el niño Ástor Piazzolla.

En el libreto, Gardel es hijo de un multimillonario rechazado por su padre por dedicarse a cantar y haber formado pareja con una artista (Rosita Moreno como Margarita), a quien le declara su amor con la canción "El día que me quieras". Reducidos a la pobreza, Margarita cae enferma gravemente y Gardel se ve forzado a robarle a su padre para sobrevivir. Pese a ello Margarita muere en una escena histórica en la que Gardel canta "Sus ojos se cerraron", que por el grado de emotividad que alcanzó su interpretación, dejó al estudio en silencio durante varios minutos antes de estallar en aplausos. Gardel queda entonces solo al cuidado de su hija, Marga. El relato se reinicia años después, cuando Gardel y su hija se han convertido en artistas exitosos. Repitiendo la historia de su madre, Marga se enamora de un joven argentino, cuyo millonario padre que se opone a la relación ("yo la veo vulgar y demasiado libre"). Todos coinciden en el viaje de vuelta a Buenos Aires donde, en otra escena famosa, apoyado en la baranda del barco, Gardel canta el tango "Volver" ("Volver con la frente marchita, las nieves del tiempo platearon mi sien"). Finalmente, Gardel revela que es el heredero de la empresa en la que trabaja, lo que lleva al padre del joven a cambiar cínicamente de parecer y autorizar el casamiento. En la última escena Gardel, abrazado a su hija y su futuro yerno, de noche y mirando al mar, canta la segunda parte de "El día que me quieras".

Le Pera nuevamente se quejó de los cortes de posproducción, pero el film claramente había ganado en calidad frente a los anteriores. La película fue estrenada pocos días después de la muerte de Gardel. En una carta escrita a su representante Defino cuatro días antes de morir, Gardel le dice:

Tango Bar, la última película de Gardel, se filmó en febrero de 1935. Rainhardt volvió a ser el director y Le Pera, el libretista. La producción continuó mejorando el nivel de actuación incorporando al excelente actor argentino Enrique de Rosas, quien se sumó a Rosita Moreno, Tito Luisardo, Fernando Adelantado y Manuel Peluffo. El argumento es un policial romántico, que gira alrededor de un apostador a las carreras de caballos (Gardel) que pone un tango bar en Barcelona y una ladrona de joyas (Rosita Moreno), que resulta chantajeada. Entre canciones la dupla Gardel-Le Pera compone para el film los éxitos de "Por una cabeza" y "Arrabal amargo".

Terminada la filmación y en medio del éxito con que era reconocido su arte y su ángel, Gardel comenzó a preparar una gira por varios países latinoamericanos.

Las películas estadounidenses de Gardel terminaron por consolidar un fenómeno de idolatría popular en el público de habla hispana, completamente inusual para la época.

La gardelmanía produjo también el extraño encuentro entre Gardel y Ástor Piazzola, los dos máximos referentes de la historia del tango, cuando este último tenía apenas doce años. El padre de Piazzolla era un inmigrante argentino en Estados Unidos, que trabajaba de peluquero en Manhattan. Cuando se enteró que Gardel se había instalado en Nueva York, Piazzolla padre talló una figura especialmente para Gardel y lo envió a su hijo a que se la entregara. La picardía del niño, que trataba a Gardel de "Charlie" y sabía tocar el bandoneón, generó la simpatía inmediata de Gardel, y se estableció entre los dos una inusual amistad, en la que Ástor obró muchas veces como traductor. Como resultado de la misma Gardel invitó al niño Piazzolla a aparecer como "canillita" en "El día que me quieras" y luego fue más allá invitándolo a formar parte de la comitiva que lo acompañaría en su gira latinoamericana, pero su padre pensó que era todavía muy joven para ello. Como si se tratara de un guion de película, esa negativa del padre hizo que Ástor no estuviera en el accidente aéreo y décadas después se transformara en el gran renovador del tango argentino.

En 1978 Ástor Piazzolla le escribió a Gardel una carta imaginaria:

A medianoche del 28 de marzo de 1935 Gardel salió de Nueva York en el yate "Coamo" para iniciar su gira latinoamericana por Puerto Rico, Venezuela, Aruba, Curazao, Colombia, Panamá, Cuba y México. Lo acompañaban Le Pera, sus guitarristas Barbieri, Aguilar y Riverol, el boxeador argentino José Corpas Moreno como su secretario y el español José Plaja, su profesor de inglés. En Puerto Rico se sumaría al grupo el puertorriqueño Alfonso Azzaf, como masajista y encargado de la iluminación.

Gardel llegó a Puerto Rico el 1 de abril a las 5 de la mañana. Más de cuarenta mil personas lo estaban esperaban en el puerto, sorprendiendo a una comitiva que recién comenzaba a tomar dimensión de la idoloatría popular que había desencadenado el cantante argentino en el público latino. Había sido contratado por una semana, pero la demanda popular fue de tal magnitud que postergó su llegada a Venezuela para actuar durante dos semanas más. Cantó en los teatros Paramount de la capital, Yaguez de Mayagüez, Broadway de Ponce, así como en las ciudades de Yauco, Manatí, Río Piedras, Cayey, Guayama, Cataño y Arecibo.

El 25 de abril llegó a Venezuela, en la motonave "Lara". Otra multitud los esperaba en los muelles de La Guaira y de allí en tren hasta la estación de Caño Amarillo en Caracas. La presión popular sobre el ídolo fue de tal magnitud que demoró enormemente su llegada. Miles de mujeres, especialmente las adolescentes, intentaban abrazarlo y pellizcarlo e incluso la capota del automóvil en el que iba fue destrozada por la gente para poder verlo, desencadenando una represión policial en la que Le Pera recibió un sablazo del la cara. Actualmente en ese lugar, una estatua de Gardel y dos guitarristas, de la artista Marisol Escobar, conmemora la recepción del pueblo venezolano al cantor argentino.
Permaneció en Venezuela doce días, actuando en los teatros Principal y el Rialto, así como en el Hotel Majestic y en la Radio de Caracas. También cantó en Valencia, en la población petrolera de Cabimas, donde el público destrozó el circo en el que actuó exigiendo que cantara más canciones, ante el presidente Juan Vicente Gómez en su residencia de Maracay y finalmente en Maracaibo.

El 23 de mayo Gardel llega a Curazao, donde actúo cinco noches. Allí dona al grupo de exiliados venezolanos la suma de diez mil bolívares que le había regalado el presidente Gómez.

El 28 de mayo arribó por barco a Aruba.
En Aruba, el público lo sacó del palco y lo llevó en andas por toda la ciudad. En esa situación Gardel logró convencer a la gente para que lo llevaran hasta el espigón del puerto donde abordó el avión que lo llevaría de nuevo a Curazao.

El 2 de junio Gardel llegó a Colombia, desembarcando en Puerto Colombia, que por entonces era la terminal marítima de Barranquilla. El diario "El Tiempo" de Bogotá, al anunciar el arribo del "Jilguero de las Pampas" decía que "la llegada del cantante argentino saturó a Barranquilla, que está viviendo a ritmo de tango".
Nuevamente el afecto popular se expresaba en multitudes que lo seguían a todos lados, para abrazarlo y besarlo, con escenas de delirio colectivo que obligó una vez más a que intervinieran las fuerzas policiales. En Bogotá más de diez personas invadieron la pista en el momento del aterrizaje.

Desde Bogotá, Barbieri le escribió a su esposa contándole lo que estaba pasando:

En Medellín, uno de los empresarios había intentado disculparse por las molestias de la efusividad popular:

Actuó en las ciudades colombianas de Barranquilla, Cartagena, Medellín y Bogotá. El domingo 23 de junio cantó por la radio "La voz de la Victor", ante un inmenso público que colmó los estudios y la plaza Bolívar, donde la emisora colocó altoparlantes. Entre otros temas cantó los tangos "Cuesta Abajo", "Insomnio", "El Carretero", "No te engañes corazón" para cerrar con "Tomo y obligo". Fue su última actuación.
Antes de cerrar se había despedido con estas palabras:

Al día siguiente, 24 de junio, Gardel y sus acompañantes debían continuar la gira en Cali. Para ello tomaron un avión piloteado por Stanley Harvey, que se dirigió primero a Medellín, para que asumiera la conducción del vuelo el célebre aviador Ernesto Samper Mendoza, propietario de la empresa Saco. Al momento de despegar del aeropuerto de Medellín, el avión sufrió el accidente que le costó la vida a Gardel y a sus acompañantes con excepción de Aguilar y Plaja.

El 24 de junio de 1935 Carlos Gardel, junto con Alfredo Le Pera, su guitarrista Guillermo Barbieri y su secretario Corpas Moreno, falleció en el choque de dos aviones en el momento de despegar, sobre la pista del aeropuerto Olaya Herrera que se conocía entonces como Aeródromo "Las Playas" de la ciudad de Medellín (Colombia). Días después también morirían Alfonso Azzaf y el guitarrista Ángel Domingo Riverol. En el accidente murieron también el as de la aviación colombiana y dueño de la SACO, Ernesto Samper Mendoza, el radiooperador Willis Foster, el empresario chileno Celedonio Palacios, el promotor de espectáculos Henry Swartz, así como los siete ocupantes del otro avión. En total 17 muertos. Solo hubo tres supervivientes: el guitarrista José María Aguilar, José Plaja y Grant Flynt, funcionario de SACO.

El accidente se produjo cuando el avión en que viajaba Gardel, un trimotor Ford de la empresa SACO, se desvió en pleno carreteo de despegue y embistió a otro avión similar de la empresa de origen alemán SCADTA, que esperaba su turno para despegar, incendiándose ambos.

Las causas del accidente nunca fueron establecidas con claridad. Ambas empresas aeronáuticas mantenían una dura competencia, detrás de la cual se encontraban los intereses estratégico-militares de los Estados Unidos y Alemania. Ni bien sucedió el accidente, cada una de las empresas se apresuró a atribuirle a la otra la responsabilidad. El propio presidente de Colombia culpó con dureza a la empresa alemana. La justicia por su parte decidió que las causas del accidente se debieron a las características de la pista y a un fuerte viento proveniente del sudeste.

Gardel fue enterrado primero en Medellín, pero luego Armando Defino ―su albacea― logró la repatriación del cuerpo. Para dicho fin, el féretro que contenía los restos mortales de Carlos Gardel debió realizar un largo recorrido que incluyó viajes en lomo de burro, carreta, tren y barco. El cuerpo del malogrado cantor pasó por las poblaciones interiores de Colombia, luego fue a Panamá, se lo veló en Estados Unidos, y llegó finalmente a la Argentina en barco hacia 1936.
Actualmente sus restos se hallan enterrados en el Cementerio de la Chacarita de Buenos Aires.

Gardel y Razzano primero y luego Gardel como solista, se caracterizaron por el acompañamiento exclusivo de guitarras.
El acompañamiento con guitarras fue inusual en el tango, ya que si bien la guitarra nunca estuvo del todo excluida en la orquestación tanguera, el peso de la misma cayó sobre el bandoneón centralmente, y secundariamente en el piano y el violín, instrumentos que formaron la llamada «orquesta típica de tango». Las guitarras de Gardel no vienen del mundo tanguero, sino del mundo de la música campera y la payada, del que provenía el dúo.
La utilización del acompañamiento de guitarras y la calidad de los guitarristas elegidos, ha dado lugar a frecuentes debates estéticos.

Inicialmente Gardel y Razzano se acompañaban con guitarras prestadas, pero a medida que se fueron consolidando profesionalmente, recurrieron a la contratación de guitarristas expertos. El primero fue José Ricardo, "El Negro José", un músico afroargentino nacido en el barrio de Balvanera de Buenos Aires, que fue contratado en 1916 y permaneció hasta mayo de 1929. En 1921 el dúo contrató como guitarrista al argentino Guillermo Barbieri, que asumió el rol de segunda guitarra hasta el retiro de Ricardo. En 1928, Gardel contrató al uruguayo José María Aguilar, "El Indio", que se desempeñó hasta diciembre de 1930, cuando fue despedido por Gardel por un comentario homofóbico en su contra, volviéndose a integrar en 1935. En marzo de 1930 Gardel contrató a Ángel Domingo Riverol, quien lo acompañaría desde entonces. Entre septiembre de 1931 y noviembre de 1933 Julio Vivas integró el grupo de guitarristas y durante este último año se sumó también Horacio Pettorossi, formando un brillante cuarteto con Barbieri, Riverol y Vivas.

Los guitarristas de Gardel eran músicos consumados y Gardel grabó muchas canciones compuestas por ellos, algunas convertidas en clásicos, además de introducir solos de guitarra para su lucimiento, gestos que fueron muy valorados por los propios músicos y los críticos, por el significado artístico y económico que tuvo esa conducta.

Ricardo compuso 11 canciones grabadas por Gardel, entre ellas «Margot» (música junto a Gardel), «Pobre gallo bataraz» y «Resignate hermano» (con Barbieri).

Barbieri compuso 32 tangos grabados por Gardel, entre ellos: "Anclao en París", «Viejo smocking», «Incurable», «Mar bravío», «Quién tuviera 18 años», «El que atrasó el reloj», «Pordioseros», «Idilio campero», «Cruz de palo», «Pobre amigo», «La novia ausente», «Preparate p’al domingo», «Resignate hermano» (con Ricardo), «Besos que matan», «Barrio viejo», «Cariñito», «Viejo curda» y «Olvidao».

Aguilar compuso 11 tangos grabados por Gardel, entre los que se destacan «Al mundo le falta un tornillo» (con letra de Cadícamo), «Tengo miedo», «Lloró como una mujer» y la letra de «Milonguera».

Entre las canciones propias o en coautoría de Vivas grabadas por Gardel se destacan «El Olivo», «Salto Mortal», «Quejas del Alma» y «Amante, Corazón».

Pettorossi fue compositor de tangos célebres de Gardel, como «Silencio» (en coautoría con Gardel y Le Pera), «Angustia», «Esclavas blancas» y «Lo han visto con otra».

Riverol compuso 3 tangos grabados por Gardel, entre ellos «Falsas promesas» y «Trovas».

Barbieri, Riverol y Aguilar acompañaban a Gardel en el vuelo que le causó la muerte. Barbieri murió en el acto, Riverol murió dos días después y Aguilar fue el único sobreviviente del accidente, sufriendo gravísimas heridas.

La principal relación personal de la vida de Gardel fue con su madre, la inmigrante francesa Marie Berthe Gardes o Berta Gardés, según la versión castellanizada, que para la hipótesis francesista fue su madre biológica y para la hipótesis uruguayista, obró como su madre adoptiva. Gardel vivió toda su vida con su madre, aunque durante algún tiempo cuando era adolescente parece ser que huyó de su hogar. Por otra parte Gardel siempre expresó devoción por su madre, aunque algunos aspectos ambivalentes de la relación señalan aristas complejas no del todo reveladas.
Cuando Gardel se volvió un artista bien remunerado, proveyó a Berta del dinero necesario para visitar todos los años a su madre, hermano y demás familiares franceses de Toulouse, a quien él también visitaba, aunque notablemente nunca viajaron juntos.
Berta se hallaba justamente en Toulouse al momento de la muerte de su hijo.
Al morir Carlos, fue Berta la que heredó todos los bienes de su hijo, incluyendo la casa que éste compró para ambos en el barrio del Abasto.

Gardel era una persona extrovertida y simpática, que tendía a establecer fuertes relaciones de amistad, aunque como suele suceder con las personas de fama y fortuna, no siempre eran realmente correspondidas. Entre sus amigos más importantes se destaca la relación con José Razzano mantenida según éste, desde 1911 hasta su muerte, aunque en los últimos años se debilitó por razones económicas.
Otros buenos amigos fueron Edmundo Guibourg, con quien se conocían de chicos, pero comenzaron a ser amigos en 1915; el jockey Irineo Leguizamo, a quien Gardel le dedicaría el tango «Leguisamo solo», compuesto por Modesto Papavero;
Francisco Maschio, cuidador de caballos de carrera con el que Gardel compartió su pasión por los «burros», como decía en lunfardo; y los actores Elías Alippi y César Ratti.

Su representante hasta 1932 fue su amigo y compañero José Razzano. En este último año se distanciaron y Gardel designó a Armando Defino. Al morir Gardel, Defino y su esposa Adela Blasco constituyeron el soporte espiritual de Berta, incluso viviendo juntos en la casa del Abasto. Berta, que murió en 1943, legó todos sus bienes a Defino, quien a su vez hizo lo mismo con su esposa al morir en 1958. Adela vivió hasta 1984 y falleció sin herederos forzosos, legando todos sus bienes a Nuria de Fortuny.

Con respecto a sus relaciones de pareja, Gardel fue extremadamente reservado con las mismas, no dando a conocer públicamente ninguna relación.
La reserva de Gardel sobre su vida íntima ha dado lugar a diversos y contradictorios rumores y estudios sobre la naturaleza de sus relaciones afectivas y sexuales.
En su correspondencia privada existen amplias constancias sobre Isabel del Valle, una niña de 13 años con la que Gardel se relacionó en 1920 y con la que mantuvo un vínculo ambiguo hasta 1933.
Por otra parte existe unanimidad en el hecho de que Gardel no tuvo hijos.

Apostar en las carreras de caballos fue la gran pasión de Gardel y quedó reflejada contundentemente en su cancionero en lo que se ha dado en llamar «tangos burreros», tomando la expresión lunfarda de «burros» para referirse al "turf".

Muchos de sus amigos pertenecían al mundo de las carreras de caballos, como el célebre yóquey Irineo Leguisamo, los hermanos Tortercio, el cuidador Francisco Maschio y el yóquey Alfredo Peluffo.
Gardel fue propietario de ocho caballos de carrera: Lunático (su preferido, montado por Leguisamo y cuidado por Maschio), La Pastora, Cancionero, Amargura, Theresa, Guitarrista, Explotó y Mocoroa, estos dos últimos compartidos en propiedad con Leguisamo y Mascchio.

Entre los «tangos burreros» del repertorio gardeliano se destacan «Por una cabeza», de su autoría con letra de Alfredo Le Pera en 1935, el único compuesto por él en esta serie temática y «Leguisamo solo» de Modesto Papavero, en homenaje a su amigo y en el que Gardel al interpretarlo menciona a sus amigos Francisco Maschio y "El Pulpo" Leguisamo, y a su caballo Lunático:

Otros tangos «burreros» del cancionero de Gardel son «Palermo» de Enrique Pedro Delfino y Juan Villalba y Hermido Braga; «La catedrática» (Soy una fiera), de su excompañero Francisco Martino; «Uno y uno», de Traverso y Pollero; «Paquetín, paquetón» de Carlos Dedico, Germán Ziclis y Salvador Merico; «Bajo Belgrano» de Francisco García Jiménez y Anselmo Aieta; «Preparate pa´l domingo», de su guitarrista Guillermo Barbieri y José Rial (h).
Guillermo Barbieri y Eugenio Cárdenas compusieron el tango «Lunático», que llegó a ser grabado por Gardel para el sello Records, y en 1935 se estrenó la película del mismo nombre.

Gardel era también aficionado a otros deportes, como el fútbol -fue socio de Racing Club y uno de sus tangos "futboleros" es "Patadura"- y el boxeo. Además de su afición por estos deportes, jugaba habitualmente a la pelota vasca y a las bochas, salía a trotar y asistía a clases de gimnasia.

En 1915, el tenor italiano Enrico Caruso vino a la Argentina a cantar al Teatro Colón y al volverse en barco al Brasil se dio la coincidencia de que en él se encontrase Carlos Gardel, que era amigo de muchos de los profesores de la Orquesta Estable. Algunos de ellos lo convencieron para que se encontrara con el famoso italiano. Así lo hizo y una vez que Caruso lo escuchó cantar un tango, una zamba y una cueca, el italiano, le comentó: “"Si usted hubiera estudiado seriamente, sería el mejor barítono del mundo"”. Con el tiempo, efectivamente, Carlos Gardel eligió como maestro al prestigioso profesor Alberto Castellanos, quien le cambió el registro de tenor a barítono. Por eso, en los en los primeros discos de Gardel, se percibe su canto en un tono más agudo; mientras que en los últimos se lo escucha más cómodo en el registro apropiado.

Su voz fue evolucionando, ajustando su dicción a los cambios de los sistemas de grabaciones acústicas. El maestro Eduardo Bonessi, quien fue profesor de canto de Gardel dijo hacia 1963:

En su libro "Carlos Gardel: a la luz de la Historia", de la Fundación BankBoston, Montevideo, 2000,
el arquitecto Nelson Bayardo, que durante más de treinta años investigó la vida y los orígenes de Carlos Gardel, describe la voz de del cantante resaltando cinco aspectos:


Con respecto a la «N» que Gardel pronunciaba como una «R», el cantante argentino Edmundo Rivero, en un libro dedicado exclusivamente al análisis técnico de su canto, dio la siguiente explicación:

En 1985 la Fundación Konex lo homenajeó con el Premio Konex de Honor por su incalculable aporte a la historia de la música popular argentina.

El 24 de junio de 2005, por decisión conjunta de las autoridades municipales de las ciudades de Buenos Aires, Montevideo, Tacuarembó y Medellín (donde falleció), se recordaron los 70 años de la muerte de Carlos Gardel. Por primera vez, se obvió la conmemoración del llamado Día de Carlos Gardel en la ciudad francesa de Toulouse.

En Argentina se celebra cada 11 de diciembre el Día del Tango, debido a que ese día nacieron Julio de Caro y Carlos Gardel.

Existen dos versiones sobre el nacimiento de Carlos Gardel: una sostiene que nació en Francia y la otra, que nació en Uruguay. La primera es conocida como la «hipótesis francesista» y la segunda como la «hipótesis uruguayista». Ambas difieren también en el año de nacimiento y en gran cantidad de hechos de su vida personal, relacionados con su identidad, principalmente durante su infancia y adolescencia. Pero ambas versiones coinciden en el hecho de que el cantor se nacionalizó argentino en 1923.

Gardel por su parte, en sus declaraciones públicas, fue en general reticente a contestar cuál era su nacionalidad y cuando lo hacía solía expresarse de manera ambigua y sin proporcionar detalles.

La «hipótesis francesista» sostiene que Carlos Gardel nació en Toulouse, Francia, el 11 de diciembre de 1890, bajo el nombre de Charles Romuald Gardes, siendo su madre biológica Marie Berthe Gardes y su padre Paul Jean Lasserre, quien no lo reconoció. En 1893 Berthe emigró a la Argentina con su hijo, radicándose en Buenos Aires. Sostienen también que su nombre fue castellanizado en Argentina como Carlos Romualdo Gardés y que adoptó como nombre artístico, el de Carlos Gardel, correspondiendo por lo tanto los tres nombres a una misma persona.

Las pruebas documentales principales de la «hipótesis francesista» es la partida de nacimiento de Charles Romuald Gardes en Toulouse, cuya autenticidad no está discutida, y el testamento ológrafo de Carlos Gardel, donde este declara ser Charles Romuald Gardes, hijo de Berthe Gardes, nacido en Toulouse.

La «hipótesis francesista» sostiene que los documentos de Gardel en los cuales figura que nació en Tacuarembó, Uruguay, como el registro de nacionalidad gestionado en 1920, su cédula y sus pasaportes, contienen datos falsos aportados por el propio Gardel por no haberse enlistado en las fuerzas francesas para hacer el servicio militar, ni tampoco para luchar por Francia durante la Primera Guerra Mundial. Si Gardel hubiese sido identificado como un ciudadano francés por nacimiento, habría corrido el riesgo de ser llevado a la cárcel por desertor en caso de viajar a Francia o a cualquier país que tuviese un tratado de extradición con Francia.

Esta corriente constituye la posición biográfica tradicional, presentada en la primera biografía de Gardel, escrita en 1946 por su amigo Francisco García Jiménez, sobre las memorias de José Razzano.
Luego de la presentación de la «hipótesis uruguayista» en 1967, surgieron autores dedicados a confirmar la «hipótesis francesista» como el argentino Augusto Fernández, el argentino Carlos Esteban y los franceses Monique Ruffié y Georges Galopa, la francesa Christiane Bricheteau, Raúl Torre y Juan Fenoglio, Enrique Espina Rawson, y Norberto Regueira.
Entre los biógrafos de Gardel que aceptan la hipótesis del nacimiento en Toulouse se encuentran el británico Simon Collier, y los argentinos Julián y Osvaldo Barsky.

La «hipótesis uruguayista» sostiene que Carlos Gardel nació en Tacuarembó, Uruguay, probablemente un 11 de diciembre, siendo su madre María Lelia Oliva, embarazada por el esposo de su hermana Blanca, Carlos Félix Escayola Medina, jefe militar y político de Tacuarembó. El año de nacimiento varía, según los autores, entre 1883, cuando María Lelia tenía 13 años y su hermana Blanca estaba viva, y 1887, cuando María Lelia tenía 17 años y su hermana Blanca ya había muerto.

La «hipótesis uruguayista» sostiene que al nacer el niño, no fue reconocido por ninguno de sus padres y quedó sin inscripción. Berthe Gardes habría estado en ese entonces en Tacuarembó, haciéndose cargo inicialmente del niño, pero luego viajó nuevamente a Toulouse donde en 1890 tuvo un hijo no reconocido por el padre, al que llamó Charles Romuald Gardes, con quien emigró a la Argentina, donde volvió a hacerse cargo de Carlos, el niño uruguayo.

Pocos años después, los padres biológicos de Gardel según la versión uruguayista, Carlos Escayola y María Lelia Oliva, formalizarían su relación, casándose y teniendo seis hijos, hermanos de Gardel.
Gardel, por su parte, siempre habría sabido su verdadera filiación e incluso se habría encontrado algunas veces con sus hermanos.

La base de la «hipótesis uruguayista» es que Charles Romuald Gardes, el hijo de Berthe nacido en 1890 en Toulouse, y Carlos Gardel, criado parcialmente por Berthe y nacido varios años antes en Tacuarembó, fueron personas distintas y que finalmente Gardel terminó tomando la identidad de Charles, el hijo biológico de Berthe.

Las pruebas documentales principales de la «hipótesis uruguayista» son los documentos uruguayos y argentinos, tramitados por Gardel en 1920 y 1923, en los que el propio Carlos Gardel declara haber nacido en Tacuarembó el 11 de diciembre de 1887, identificando a sus padres como Carlos Gardel y María Gardel.

La «hipótesis uruguayista» sostiene que el contenido del testamento de Gardel es falso y que formó parte de una maniobra legal para que Berthe Gardes pudiese heredarlo de acuerdo con las leyes argentinas de la época. También han peticionado judicialmente con resultado negativo, la exhumación de los restos de Gardel y su madre, a fin de realizar estudios que establezcan su identidad genética.

Esta corriente fue iniciada por el periodista uruguayo Erasmo Silva Cabrera, alias Avlis, el primero en sostener la hipótesis a partir de 1967.
Otros estudiosos pertenecientes a la corriente uruguayista son el argentino Blas Matamoro, el uruguayo Nelson Bayardo, el uruguayo Eduardo Payssé González, la argentina Martina Iñíguez, el argentino Ricardo Ostuni, el uruguayo Nelson Sica dell’Isola, y el uruguayo Gonzalo Vázquez Gabor.

En 1997 Susana Cabrera publicó el libro "Los secretos del Coronel", que narra y documenta esta hipótesis, sobre el que se realizó el documental "El padre de Gardel" (Ricardo Antonio Casas, 2013). Allí se trata el supuesto del incesto y el ocultamiento de la verdadera paternidad de Gardel.

Gran parte de la celebridad y la pasión despertadas por Gardel en vida y luego de muerto se debe a su preocupación por cuidar y difundir su imagen. El poeta Celedonio Flores escribe en el famoso tango "Corrientes y Esmeralda", compuesto en 1933, cuando Gardel todavía vivía, que "cualquier cacatúa sueña con la pinta de Carlos Gardel".
En esa imagen juegan un papel de gran importancia las fotografías que le tomara el fotógrafo hispano-uruguayo José María Silva, en especial los famosos retratos de 1933.

El músico de rock argentino Luis Alberto Spinetta refleja la idolatría popular por Gardel, en una de sus más conocidas canciones, "El anillo del Capitán Beto", en la que describe la cabina de un colectivero, donde conviven la famosa foto de Gardel, con la pasión "futbolera" y las creencias religiosas.

Silva conoció a Gardel casi por casualidad, cuando en 1917, estando el dúo Gardel-Razzano en Montevideo, deciden sacarse fotos para difundir su imagen. De ese modo, caminando por la Avenida 18 de Julio, la principal de la ciudad, ingresan a un negocio fotográfico en el que Silva trabajaba como empleado, siendo todavía un joven de 20 años. Gardel quedó sorprendido por la calidad inusual para la época de las fotos de Silva, y desde entonces recurrió a él, ya independizado, para sacarse las fotos con las que difundiría mundialmente su imagen.

Luego de 1917, Gardel se saca otras dos grandes tandas de fotos con Silva: la primera en 1923, antes de partir en su primera gira a Europa, en la que se toma la conocida foto vestido de gaucho -porque en el "viejo continente" se asociaba estereotipadamente el tango con el gaucho-; y la última en 1933 donde se toma una secuencia de retratos del rostro, que constituyen los famosos retratos con los que se asocia universalmente la imagen de Gardel. Silva jamás cobró un centavo por los derechos de reproducción.

Silva diría sobre Gardel:

Gardel realizó 957 grabaciones, cubriendo 792 temas diferentes.

No solo grabó tangos; también música folclórica, milongas, zambas, rancheras, tonadas, estilos, etc. (treinta géneros en total). Grabó algunos foxtrots, un tango en español e inglés, y también algunas canciones tradicionales en francés e italiano y hasta un tango en guaraní.

















</doc>
<doc id="6719" url="https://es.wikipedia.org/wiki?curid=6719" title="Karlheinz Stockhausen">
Karlheinz Stockhausen

Karlheinz Stockhausen (Mödrath, 22 de agosto de 1928-Kürten-Kettenberg, 5 de diciembre de 2007) fue un compositor alemán ampliamente reconocido, tanto por la crítica como por la opinión musical más ilustrada, como uno de los compositores más destacados y polémicos de la música culta del siglo XX. Para muchos, no sólo es una figura importante, sino que se trata de uno de los mayores visionarios de la música del siglo XX. 

Es conocido por sus trabajos de música contemporánea y sus innovaciones en música electroacústica, música aleatoria y composición serial. 

Fue educado primeramente en la Hochschule für Musik Köln para después acudir a la Universidad de Colonia. Posteriormente, Stockhausen estudiaría con Olivier Messiaen en París y Werner Meyer-Eppler en la Universidad de Bonn. 

Como una de las principales figuras de la Escuela de Darmstadt que fue, sus teorías y composiciones siguen siendo, aún hoy en día, de gran influencia para compositores de todos los tipos y estilos (también en la música popular y en el "jazz"). 

Sus obras, compuestas a lo largo de casi sesenta años, se abstienen de ser partícipes de las formas musicales más tradicionales. Aparte de las obras de música electrónica, sus trabajos van desde realizar miniaturas para cajas de música hasta trabajos para instrumentos solistas, canciones, música de cámara, músicas coral y orquestal e incluso un ciclo de siete óperas de larga duración. 

Sus escritos, tanto teóricos como de diversos temas, ocupan más de diez imponentes volúmenes. Stockhausen recibió numerosos premios y distinciones por sus composiciones, grabaciones y partituras publicadas por su propia editorial. 

Entre sus composiciones más notables encontramos la serie de diecinueve "Klavierstücke" (piezas para piano), "Kontra-Punkte" para diez instrumentos, su electrónica/música concreta "Gesang der Jünglinge", "Gruppen" para tres orquestas, la obra para percusión solista "Zyklus", "Kontakte", la cantata "Momente", su obra de electrónica en vivo "Mikrophonie I", "Hymnen", "Stimmung" para seis vocalistas, "Aus den sieben Tagen", "Mantra" para dos pianos y electrónica, "Tierkreis", "Inori" para solistas y orquesta, y su gigantesco círculo de operas "Licht". 

Stockhausen murió de improviso, debido a un ataque cardiaco, el 5 de diciembre de 2007, a la edad de 79 años, en su residencia de Kürten, Alemania. 

Karlheinz Stockhausen nació en el castillo de la ciudad de Mödrath, que servía en esa época de maternidad del distrito de Rhein-Erft-Kreis, al oeste del estado de Renania del Norte-Westfalia, estado alemán fronterizo con Bélgica. (La ciudad de Mödrath, localizada cerca de Kerpen y Colonia, fue desplazada en 1956 para explotar una mina a cielo abierto de lignito, pero el castillo sigue existiendo).

Su padre era profesor de escuela y su madre era hija de una próspera familia de granjeros de Neurath, cerca de Colonia. Ella tocaba el piano y cantaba, pero después de quedar embarazada tres veces consecutivas, sufrió una crisis nerviosa y fue internada en un hospital psiquiátrico en diciembre de 1932. Pocos meses más tarde falleció su hermano menor Hermann.

Stockhausen se fue a vivir a Altenberg a los 7 años, donde recibió clases de piano del organista de la Catedral de Altenberg, Franz-Josef Kloth. Su padre, Simon Stockhausen, se volvió a casar en 1938 con otra mujer, Luzia, con la que tuvo dos hijas.

Como muchos, el vivió una tragedia familiar al desencadenarse la II Guerra Mundial cuando sólo tenía 11 años.

En 1941 o 1942 se enteró de que su madre había muerto, aparentemente de leucemia, como todos los internados en ese hospital, que habían muerto supuestamente de esa misma enfermedad. Se da por hecho que ella fue una víctima de la política nazi de «eutanasia para los individuos no productivos». 
Posteriormente, Stockhausen representará la ejecución de su madre en el hospital mediante una inyección letal, en el Acto 1 escena 2 («Mondeva») de la ópera Donnerstag aus Licht.

En parte debido a su mala relación con su madrastra, en enero de 1942, Karlheinz ingresó en un internado de Xanten donde continuó aprendiendo piano y también estudió oboe y violín e hizo trabajos en una granja.

En el otoño de 1944, fue alistado para servir como camillero trasladando heridos en Bedburg. En febrero de 1945 se reunió por última vez con su padre en Altenberg, el cual se despidió de Stockhausen premonitoriamente, antes de ser enviado a luchar al frente este, del que no volvería.

De 1947 a 1951 estudió piano y pedagogía musical en el Conservatorio Superior de Colonia (la prestigiosa «Hochschule für Musik Köln»). También estudió musicología, filosofía y lengua germánica en la Universidad de Colonia. Completó sus estudios de armonía y contrapunto con el compositor Hermann Schroeder.

En 1950 se interesó por la composición y fue admitido a final del año en la clase del compositor suizo Frank Martin, que empezaba un contrato de 7 años como profesor en Colonia. Simultaneó sus estudios con varios trabajos, de obrero en una fábrica, de guardia en un aparcamiento y de vigilante de viviendas de las tropas de ocupación.

En 1951 se matriculó en los cursos de verano de Darmstadt, centro difusor del serialismo y de corrientes vanguardistas afines, donde tomó contacto con la música de Anton Webern y con la nueva generación de compositores serialistas. 
Allí conoció al compositor belga Karel Goeyvaerts, que había estudiado análisis musical con Olivier Messiaen y composición con Darius Milhaud en París, y que influyó en la decisión de Stockhausen de realizar esos mismos estudios.

En Darmstadt tomó contacto con los compositores que integraban la vanguardia musical alemana fuera del dodecafonismo —Paul Hindemith, Edgar Varèse...— y dentro del dodecafonismo —Arnold Schönberg, Ernst Krenek...— y también la estética de Theodor W. Adorno y René Leibowitz.
Junto con Bruno Maderna, György Ligeti y Luigi Nono, Stockhausen asistió en Darmstadt a ciclos de conciertos que cambiarían su concepción de la música, como el famoso estudio de piano Modo de valores e intensidades de Messiaen, que le decidió a trasladarse a París, donde llegó, el 8 de enero de 1952, para matricularse en el Conservatorio y asistir a las clases de composición de Milhaud y al curso de análisis y estética de Messiaen.

Con Messiaen se familiarizó con la técnica del serialismo, junto con otros compositores importantes y también alumnos de Messiaen, como Iannis Xenakis o Pierre Boulez, que en ese momento trabajaba en las Structures I para dos pianos y con quien entablaría una gran amistad, que inicia una larga correspondencia entre ambos compositores.

Previamente, en 1951, Stockhausen se había casado con una compañera de estudios, Doris Andreä, con la que tuvo cuatro hijos, Suja (1953), Christel (1956), Markus (1957) y Majella (1961)

Desde 1953 compuso obras de música electroacústica, como Gesang der Jünglinge [«El canto de los adolescentes»], sirviendo como demostración práctica de la viabilidad de componer usando métodos nunca probados en música clásica, como dispositivos electrónicos o algoritmos matemáticos.

Al regresar a Alemania, en marzo de 1953, inició su colaboración con el Estudio de Música Electroacústica de la Radio Oeste de Colonia —llamada NWDR y WDR a partir del 1 de enero de 1955—, con el puesto de asistente del director Herbert Eimert.
El Estudio de Música Electrónica de Colonia fue una institución clave para otros compositores de música electroacústica, como John Cage. En 1962 Stockhausen sucedió a Eimert como director del estudio.

También comenzó a divulgar sus teorías en los cursos de Darmstadt, una actividad que mantuvo hasta mediados de la década de los años 1970. 

De 1954 a 1956 Stockhausen estudió fonética, acústica y teoría de la información con Werner Meyer-Eppler en la Universidad de Bonn.

Junto con Eimert editó la influyente revista «Die Reihe» desde 1955 a 1962.

Stockhausen dio conferencias y conciertos en Europa, Norte América y Asia. Fue profesor invitado de composición en la Universidad de Pensilvania en 1965, y en la Universidad Davis de California en 1966-67. Fundó y dirigió los Cursos de Nuevas Músicas de Colonia desde 1963 a 1968. En 1971 fue nombrado profesor de composición del Conservatorio Nacional de Música, permaneciendo en el puesto hasta 1977.

En 1961 compró un terreno en la vecindad de Kürten, un pueblo al este de Colonia cerca de Bergisch Gladbach en la Bergisches Land, donde se hizo construir una casa diseñada por el arquitecto Erich Schneider-Wessling, donde fijó su residencia una vez finalizada en 1965.

En 1967 se casó con la pintora y escultora alemana Mary Bauermeister —nacida en 1934 y fundadora hacia 1960 de un movimiento artístico de vanguardia en Colonia que más tarde daría lugar al movimiento Fluxus— y con la que tuvo dos hijos: Julika (1966) y Simon (1967).

En 1998 creó los Cursos Stockhausen, impartidos anualmente en Kürten.

A finales de 2001, Stockhausen fue protagonista de una agria polémica debido a unas declaraciones que hizo en relación al atentado del 11 de septiembre de 2001 de Nueva York. Varios medios de comunicación publicaron que el había calificado de «obra de arte» el atentado terrorista. Stockhausen se quejó de que algunas palabras suyas habían sido sacadas de contexto y malinterpretadas.

A continuación se incluye una descripción pormenorizada de la polémica: 

En una conferencia de prensa en Hamburgo, el 16 de septiembre de 2001, Stockhausen fue preguntado por un periodista sobre si los personajes de la obra "Licht" eran para él «figuras fuera de una historia cultural común» o si poseían «representación material». El compositor replicó: «Rezo diariamente a Miguel, no a Lucifer. He renunciado a él. Pero está demasiado presente, como en Nueva York recientemente». Otro periodista le preguntó sobre como le afectaban los recientes atentados terroristas del 11 de septiembre de 2001, y que visión tenía de esos sucesos en relación a la armonía de la humanidad representada en la obra "Hymnen".

En un posterior mensaje, afirmó que la prensa había publicado un «falso y difamatorio reportaje» sobre sus comentarios y aclaró lo que sigue:

Como resultado de la reacción a los comentarios de él, un festival de cuatro días sobre su obra en Hamburgo fue cancelado. Además su hija pianista anunció a la prensa que no volvería a actuar con el apellido Stockhausen.

De acuerdo con el anuncio efectuado por la Fundación Stockhausen el 7 de diciembre de 2007, el falleció en la mañana del 5 de diciembre de 2007, debido a un repentino fallo cardíaco, en la ciudad de Kürten-Kettenberg cercana a Colonia en el estado alemán de Renania del Norte-Westfalia. 
Acababa de terminar dos trabajos encargados para espectáculos en Bolonia y el Festival de Holanda previsto para junio de 2008 en Ámsterdam.

Stockhausen compuso 363 obras, todas ellas grabadas y recogidas en 139 CD. Frecuentemente se apartó radicalmente de la tradición de la música clásica influenciado por Messiaen, Edgard Varèse, y Anton Webern. También se dejó influir por otras disciplinas artísticas como el cine (Stockhausen 1996b) o pintores como Piet Mondrian (Stockhausen 1996a, 94; "Texte" 3, 92–93; Toop 1998) y Paul Klee.
Junto a su labor como compositor destacó también su tarea de director de orquesta.

Stockhausen empezó a componer durante su tercer año de conservatorio, pero sólo ha publicado cuatro de sus primeras composiciones de estudiante: Chöre für Doris, Drei Lieder, para voz de alto y orquesta de cámara, Choral, para coro a capella (las tres de 1950), y una Sonatina para violín y piano (1951).

En agosto de 1951, justo después de su primera visita a Darmstadt, Stockhausen empezó a trabajar con una forma de música atemática compuesta serialmente que rechazaba la técnica de 12 tonos de Schoenberg.

Calificó muchas de esas tempranas composiciones como «música puntual», aunque algún crítico concluye después de analizar en profundidad esas partituras que Stockhausen «nunca compuso puntualmente».
Las composiciones de esta época incluyen obras como Kreuzspiel (1951), el Klavierstücke I–IV (1952) —la cuarta pieza es especialmente citada por Stockhausen como un ejemplo de «música puntual»), y la primera versión (no publicada) de Punkte y Kontra-Punkte (1952).

De lo que no hay duda es que algunas obras de esos años muestran a Stockhausen formulando sus primeras contribuciones, rompedoras y revolucionarias, a la teoría y práctica de la composición, como la «composición grupal», una técnica usada en sus composiciones desde 1952 y durante toda su obra. Este principio fue descrito públicamente por primera vez por el en una locución de radio de diciembre de 1955 titulada «"Gruppenkomposition: Klavierstücke I"».

En diciembre de 1952 compuso un Konkrete Etüde, en el estudio de música concreta de Pierre Schaeffer en París. En marzo de 1953 se trasladó al estudio de la radio NWDR de Colonia y cambió a la música electrónica con dos Estudios Electrónicos (1953 y 1954).

En la obra Gesang der Jünglinge (1955-56) introdujo, por vez primera, el emplazamiento espacial de fuentes de sonido, con su mezcla de música concreta y música electrónica.

La experiencia ganada con los Estudios le convencieron de que era una simplificación inaceptable tratar los timbres como entidades estables.

Reforzado por sus estudios con Meyer-Eppler, a principios de 1955 Stockhausen formuló nuevos criterios «estadísticos» de composición, enfocando su atención hacia la música aleatoria, tendencia en la que se movía el movimiento sonoro, «el cambio de un estado a otro, con o sin movimiento de retorno, como opuesto al estado estático».

Posteriormente Stockhausen escribió, describiendo ese periodo en su trabajo de composición: «La primera revolución musical que ocurrió desde 1952/53, denominada "música concreta", "música electrónica con cinta magnetofónica", y "música espacial", requería componer con transformadores, generadores, moduladores, magnetófonos, etc, integrar "todas" las posibilidades sonoras concretas y abstractas (sintéticas) incluyendo todos los ruidos, y lograr la proyección controlada del sonido en el espacio».

Su posición como «compositor alemán líder de su generación» fue conquistada con Gesang der Jünglinge y tres piezas compuestas concurrentemente con diferentes medios: Zeitmasze para cuatro instrumentos de viento de madera, Gruppen para tres orquestas, y Klavierstücke XI. Los principios subyacentes a estas tres composiciones fueron presentados en el artículo teórico más conocido de Stockhausen: «...wie die Zeit vergeht...» [«...Como pasa el tiempo...»], publicado por primera vez en 1957 en el vol. 3 de la revista «Die Reihe».

En 1960 Stockhausen volvió a la composición de música vocal (por primera vez desde Gesang der Jünglinge) con Carré para cuatro coros y cuatro orquestas. Dos años después comienza una cantata expandible titulada Momente, para soprano solista, cuatro grupos de coro y trece instrumentistas (1962-64/69). 

Fue pionero en la interpretación electrónica en vivo, con obras como Mixtur, para electrónica y orquesta (1964/67/2003), Mikrophonie I, para tam-tam, dos micrófonos y dos filtros con potenciómetros (6 intérpretes) (1964), Mikrophonie II, para coro, órgano Hammond y cuatro moduladores en anillo (1965) y Solo para instrumento melódico con realimentación (1966).

También compuso dos obras electrónicas para cinta, Telemusik (1966) e Hymnen (1966-67), de la que hay también una versión con solistas y la 3ª de estas cuatro «regiones» en una versión con orquesta.

En esa época, Stockhausen empezó a incorporar música tradicional de todo el mundo en sus composiciones, siendo Telemusik el primer ejemplo conocido de esta tendencia.

Durante la década de 1960, Stockhausen exploró las posibilidades de la «música procesada» en trabajos para espectáculos en vivo, como Prozession (1967), Kurzwellen, y Spiral (ambas de 1968), culminando en las composiciones de música instintiva descritas verbalmente de Aus den Sieben Tagen (1968), Für kommende Zeiten (1968-70), Ylem (1972) y las primeras tres partes de Herbstmusik (1974). 

En 1968 Stockhausen compuso el sexteto vocal Stimmung, para el «Collegium Vocale Köln», una obra de una hora basada enteramente en los armónicos de un Si bemol grave. Seis vocalistas se sientan alrededor de una esfera luminosa y durante una hora y cuarto mantienen una misma nota en la que, con un complejo sistema de variaciones, alternan por turnos siglas mágicas y fragmentos poéticos, determinando una lenta, casi imperceptible, mutación de color vocálico y un progresivo desplazamiento del centro tonal del acorde formado por las notas fijas 

A partir de Mantra (1970), Stockhausen experimentó con la composición matemática, una técnica que recurre a la proyección y multiplicación de una melodía simple, doble o triple mediante una fórmula matemática lineal (Kohl 1983; Kohl 1990; Kohl 2004). Esta fórmula es para Stockhausen, una semilla o célula-madre que mantiene la cualidad principal de la serie de la duración o de la escala elegida en los valores de tempo. Mantra en el contexto de la música de Stockhausen es herramienta para desencadenar una especie de meditación que permite comprender la naturaleza y técnica de la música, pretendiendo que a la larga se produzca una implantación que pueda lograr una transformación en nuestras vidas. En todo su trabajo, como en Mantra, por más increíble que parezca, hay un intento de preparar a una humanidad para esta transformación (psicológica o espiritual).

Algunas veces, (como en Mantra e Inori), la fórmula es colocada al principio como una introducción. 
Continúa usando esta técnica para completar su ciclo de ópera Licht en 2003.

Otras obras de esta época no están creadas con la técnica de fórmulas matemáticas y comparten un estilo simple orientado a la melodía como:
El dueto vocal «Am Himmel wandre ich» [«En el cielo andando estoy»], uno de los 13 componentes de la obra multimedia Alphabet für Liège, 1972), "Laub und Regen" ("Hojas y lluvia", para la pieza teatral Herbstmusik (1974), y la ópera coral Atmen gibt das Leben ("Respiración da vida", 1974/77),

Las piezas Tierkreis («Zodiaco», 1974-75) y In Freundschaft («En camaradería», 1977) han sido las composiciones de Stockhausen más ampliamente interpretadas y grabadas.

Esta dramática simplificación de estilo se convirtió en el modelo a seguir por una nueva generación de compositores alemanes, comúnmente conocidos con la etiqueta de «Neue Einfachheit» o «Nueva Simplicidad». El más conocido de estos compositores es Wolfgang Rihm, que estudió con Stockhausen en 1972-73, y cuya composición orquestal Sub-Kontur (1974-75) reutiliza la fórmula de Stockhausen de Inori (1973–74).

Entre 1977 y 2003 compuso un ciclo de siete óperas llamadas "Licht: Die sieben Tage der Woche," «Luz: Los siete días de la semana»]), que trata sobre las costumbres asociadas a cada día de la semana en varias tradiciones históricas —lunes, nacimiento y fertilidad; martes, conflicto y guerra; miércoles, reconciliación y cooperación; jueves, viajes y aprendizaje; etc.— junto a la relación e interacción entre tres caracteres arquetípicos: Lucifer, Miguel, y Eva.

La concepción de la ópera de Stockhausen está basada en la ceremonia y en el ritual, con influencias del estilo de teatro japonés Noh (Stockhausen, Conen, y Hennlich 1989, 282), además de las tradiciones judeo-cristianas y de los Vedas.

Junto a ello, el trato que da a las voces y a los textos algunas veces se aleja del uso tradicional: los personajes son esquematizados para ser representados por instrumentos, bailarines o cantantes, y en algunas partes de Licht (como por ejemplo, "Luzifers Traum" del Samstag, las "escenas reales" durante Freitag o "Welt-Parlament" y "Michaelion" del Mittwoch) usó textos escritos o improvisados en lenguajes simulados o inventados.

Stockhausen tuvo su época dorada en la década de los años 1970, pero posteriormente siguió produciendo obras significativas como Stimmung (1986), en la que seis voces exploran durante setenta minutos las diversas posibilidades de un solo acorde.

Después de completar "Licht", Stockhausen se embarcó en un nuevo ciclo de composiciones basado en las horas del día, titulado Klang [«Sonidos»], del que completó veintiuna piezas antes de su fallecimiento. 

Las obras realizadas de este ciclo son "Primera hora": Himmelfahrt [«Ascensión»], para órgano o sintetizador, soprano y tenor (2004-05); "Segunda hora": Freude [«Felicidad»] para dos arpas (2005); "Tercera hora": Natürliche Dauern [«Duraciones naturales»] para piano (2005-06); y "Cuarta hora": Himmels-Tür [«Puerta del cielo»] para percusionista y niña pequeña (2005).

La "Quinta hora", "Harmonien" {«Armonías»], es un solo en tres versiones para flauta, clarinete bajo, y trompeta (2006); las versiones de clarinete bajo y flauta fueron estrenadas en Kürten el 11 de julio y 13 de julio de 2007, respectivamente.
La "Sexta hora" y las siguientes hasta la "Duodécima hora" son obras de música de cámara basadas en la partitura de la Quinta Hora. 

Los estrenos de la "Sexta" (Schönheit, para flauta, trompeta, y clarinete bajo), "Séptima" ("Balance", para flauta, corno inglés, y clarinete bajo), "Novena" (Hoffnung, para trío de cuerdas), y "Décima" (Glanz, para nueve instrumentos, encargado por el Asko Ensemble y el Festival de Holanda) han sido anunciados para 2008.

La "Decimotercera hora" Cosmic Pulses (una obra electrónica creada mediante la superposición de 24 capas de sonido, cada una con su propio movimiento espacial, usando ocho altavoces situados alrededor de la sala de concierto) fue estrenada en Roma el 7 de mayo de 2007 en el Auditorio Parco della Musica, (Sala Sinopoli). Las "Horas" 14 a 21 son piezas solistas para voz de bajo, barítono, corno di bassetto, tenor, soprano, saxofón soprano y flauta, respectivamente, con el acompañamiento electrónico de algunas de las capas de sonido de Cosmic Pulses.

A principios de 1990, Stockhausen adquirió las licencias de muchas de las grabaciones de su música que había realizado hasta entonces, y fundó su propia compañía discográfica para permitir que toda su música estuviese permanentemente disponible en compact disc.

También diseñó e imprimió sus partituras musicales, que a menudo incorporan notación no convencional. La partitura de su pieza Refrain, por ejemplo, incluye un estribillo escrito en una cinta sin fin de plástico transparente que se debe rotar para la lectura, y el volumen en Weltparlament (la primera escena de Mittwoch aus Licht) está codificado en color.

Stockhausen fue uno de los pocos grandes compositores del siglo XX que ha escrito gran cantidad de música para trompeta, inspirado por su hijo Markus Stockhausen, trompetista.

Una de las obras más espectaculares y anticonvencionales de Stockhausen es el Helikopter-Streichquartett [«Cuarteto para cuerdas y helicóptero»] (la tercera escena de Mittwoch aus Licht), completada en 1993. En ella, los cuatro miembros de un cuarteto de cuerdas tocan en cuatro helicópteros que vuelan independientemente sobre las cercanías de la sala de conciertos. La música de los intérpretes es mezclada con el sonido de los helicópteros y reproducida mediante altavoces a la audiencia de la sala. Vídeos de la performance son también transmitidos a la sala de conciertos. Los intérpretes están sincronizados con la ayuda de un metrónomo electrónico. 

La primera representación de la pieza se realizó en Ámsterdam el 26 de junio de 1995, como parte del «Holland Festival».
Debido a su naturaleza extremadamente inusual, la pieza ha sido interpretada en pocas ocasiones, incluyendo una el 22 de agosto de 2003 como parte del Festival de Salzburgo para inaugurar las instalaciones de Hangar-7, y su estreno en Alemania fue el 17 de junio de 2007 en Braunschweig dentro del Festival Stadt der Wissenschaft 2007. La pieza ha sido grabada por el «Arditti Quartet».

Stockhausen y su música han sido controvertidos e influyentes. Los dos tempranos Estudios Electrónicos (especialmente el segundo) tuvieron una poderosa influencia en el posterior desarrollo de la música electrónica en las décadas de 1950 y 1960, particularmente en el trabajo del italiano Franco Evangelisti y de los polacos Andrzej Dobrowolski y Włodzimierz Kotoński.

La influencia de su Kontrapunkte, Zeitmasse y Gruppen puede observarse en el trabajo de muchos compositores, incluyendo algunas obras de Igor Stravinsky, como Threni (1957-58), Movements for Piano and Orchestra (1958-59) y Variaciones: Homenaje a Aldous Huxley (1963-64), cuyos ritmos «probablemente han sido inspiradas, al menos en parte, por ciertos pasajes de "Gruppen" de Stockhausen».

En todo caso músicos de la generación de Stockhausen pueden considerarle una influencia improbable. En una conversación de 1957 Stravinsky dijo:

Músicos de jazz como Miles Davis, Cecil Taylor, Charles Mingus, Herbie Hancock, Yusef Lateef, y Anthony Braxton citan a Stockhausen como una influencia.

El también fue influyente en la música pop y rock. Frank Zappa hace un reconocimiento de Stockhausen en la portada de "Freak Out!", disco de 1966 con el que debutó con los Mothers of Invention. Rick Wright y Roger Waters de Pink Floyd también lo reconocen a el como una influencia (Macon 1997, 141; Bayles 1996, 222).

Las bandas psicodélicas de San Francisco Jefferson Airplane y Grateful Dead han afirmado vagamente que seguían el mismo camino de experimentación musical. Stockhausen se merece considerarse fundador de la banda Grateful Dead, algunos de cuyos integrantes estudiaron con Luciano Berio y haberlos «orientado correctamente a través de la nueva música».

Los miembros fundadores de la banda experimental de Colonia Can, Irmin Schmidt y Holger Czukay, han estudiado recientemente con Stockhausen, como también hicieron los pioneros de la música electrónica alemana Kraftwerk. Los experimentadores del sonido de guitarra de New York Sonic Youth también reconocen la influencia de Stockhausen , igual que la vocal Islandesa Björk (Guðmundsdóttir 1996; Ross 2004, 53 & 55), y el grupo británico de música industrial Coil. Chris Cutler del grupo experimental Británico Henry Cow nombraron la obra "Carré" de Stockhausen como una de sus cuatro grabaciones favoritas (en "Melody Maker" , febrero de 1974).

Stockhausen, junto con John Cage, es uno de los pocos compositores de vanguardia que han conseguido llegar a la conciencia popular y mucha más gente conoce su nombre de la que ha escuchado su música. 

Paul McCartney, decidió incluirlo entre los personajes que aparecen en la portada del álbum "Sgt. Pepper's Lonely Hearts Club Band", como menciona Hunter Davies en la biografía autorizada de The Beatles. Esto refleja su influencia en los experimentos de vanguardia de la banda, pero también la fama y notoriedad general que obtuvo a partir de 1967.

El nombre de Stockhausen, y la percepción de su música como extraña e inescuchable, han sido objeto de chistes en dibujos y caricaturas, como se documenta en una página de la web oficial de Stockhausen.
Probablemente la más cáustica frase sobre Stockhausen es la atribuida a "sir" Thomas Beecham. A la pregunta: «¿Ha oído algo de Stockhausen?», respondió diciendo: «No, pero creo que alguna vez lo he pisado».

Su fama también se refleja en menciones en obras literarias. Por ejemplo, es mencionado en una novela de Philip K. Dick de 1974 Flow My Tears, The Policeman Said, y en la novela de Thomas Pynchon de 1966 The Crying of Lot 49. La novela de Pynchon transcurre en The Scope, un bar con «una estricta política de música electrónica». La protagonista Oedipa Maas pregunta a un habitual del bar por un «frenético coro de gritos y quejidos» que procede de «algún tipo de aparato musical». Él replica: «Esto es de Stockhausen... la primera tendencia popular en entusiasmar de tu sonido de Radio Colonia. Más tarde logramos comprender su ritmo».

Robin Maconie, uno de los compositores y críticos especialistas en Stockhausen, escribió que, «Comparado con el trabajo de sus contemporáneos, la música de Stockhausen tiene una profundidad e integridad racional muy destacada... Sus investigaciones, inicialmente guiadas por Meyer-Eppler, tienen una coherencia mayor que cualquier otro compositor contemporáneo o posterior».
Maconie comparó a Stockhausen con Beethoven: «Si un genio es alguien cuyas ideas resisten todos los intentos de explicación, entonces por definición Stockhausen es lo más cercano a Beethoven que este siglo ha producido. ¿Razón? Su continua producción musical». (Maconie 1988), y «Como dijo Stravinsky, uno nunca piensa en Beethoven como un excepcional compositor porque la calidad de su inventiva transciende la mera competencia profesional. Pasa lo mismo con Stockhausen: la intensidad de su imaginación eleva las impresiones musicales de una elemental y perceptible belleza inescrutable, superando los límites del diseño consciente».

Igor Stravinsky expresó un gran entusiasmo, no falto de crítica, por la música de Stockhausen en los libros sobre sus conversaciones con Robert Craft y durante años organizó audiciones privadas con amigos en su casa donde escuchaban cintas con los últimos trabajos de él. 
En una entrevista publicada en marzo de 1968 dijo:
El siguiente octubre, un reportaje en "Sovetskaia Muzyka" (Anon. 1968) tradujo esta frase —y alguna más del mismo artículo— al ruso, sustituyendo la conjunción «pero» por la frase «Ia imeiu v vidu Karlkheintsa Shtokkhauzena» («Me estoy refiriendo a Karlheinz Stockhausen»). 
Cuando esta traducción fue anotada en la biografía de Stravinsky escrita por Druskin, la interpretación fue ampliada a todas las composiciones de Stockhausen y añade para la buena comprensión, «de hecho, obras que él llama innecesarias, inútiles y poco interesantes», citando de nuevo a partir del mismo artículo de "Sovetskaia Muzyka", a pesar de que ha quedado claro que la referencia era a «compositores universitarios» americanos.

A principios de 1995, la BBC Radio 3 envió a Stockhausen grabaciones de los artistas contemporáneos —Aphex Twin, Plastikman, Scanner y Daniel Pemberton— y le preguntó por su opinión acerca de esa música. En agosto de ese año, el periodista de Radio 3, Dick Witts, entrevistó a Stockhausen acerca de esas piezas para un programa emitido en octubre —y posteriormente publicada en el número de noviembre de la publicación británica The Wire— preguntándole qué consejos le daría a esos jóvenes músicos. Stockhausen hizo sugerencias individuales a cada uno de ellos, que fueron entonces invitados a responder. Todos menos Plastikman lo agradecieron.

En una entrevista realizada por una periodista que le cuestionaba «¿Qué es la música?», el maestro Stockhausen le respondió de la siguiente manera: «el sonido emitido por un ama de casa mientras cocina no es música, pero si yo la grabo, eso ya es música».

Algunos de los numerosos premios y distinciones otorgadas a Stockhausen son:






</doc>
<doc id="6721" url="https://es.wikipedia.org/wiki?curid=6721" title="Marismas del Ampurdán">
Marismas del Ampurdán

Las marismas del Ampurdán (en catalán y oficialmente "Aiguamolls de l’Empordà") son un conjunto de humedales generados por el curso final del río Muga y del Fluviá (Alto Ampurdán) y del Ter y del Daró (Bajo Ampurdán), constituyendo la segunda zona húmeda más importante de Cataluña (España), después del delta del Ebro.

Esta área húmeda del Alto Ampurdán fue declarada "Paraje de Interés Natural" según una ley aprobada por el Parlamento de Cataluña en 1983.

Tiene una extensión de 4824 hectáreas y abarca las tierras del curso bajo del río Muga y del Fluviá (lago de Vilaüt, la Massona, la Rogera, la Llarga...), junto con una amplia zona periférica.

Su principal interés se encuentra en la riqueza de la fauna ornitológica, especialmente migratoria.

La declaración de tierras protegidas afecta a nueve municipios: Pau, Rosas, Pedret y Marsá, Perelada, Palau Sabardera, La Armentera, La Escala, San Pedro Pescador y Castellón de Ampurias.

El año 1995 el Parque Natural de las Marismas del Ampurdán empezó la reintroducción de nutrias en los ríos Fluviá y Muga, dejando en libertad un primer ejemplar traído de Extremadura. Esta experiencia, al igual que otras similares como la impulsada en Aigüestortes, permiten que este mamífero vuelva a un hábitat natural del que prácticamente había desaparecido.

Parque Natural de las Marismas del Ampurdán


</doc>
<doc id="6722" url="https://es.wikipedia.org/wiki?curid=6722" title="Batalla de Puebla">
Batalla de Puebla

La batalla de Puebla fue un combate librado el 5 de mayo de 1862 en las cercanías de la ciudad de Puebla, entre los ejércitos de la República Mexicana, bajo el mando de Ignacio Zaragoza, y del Segundo Imperio francés, dirigido por Charles Ferdinand Latrille, conde de Lorencez, durante la Segunda Intervención Francesa en México, cuyo resultado fue una victoria importante para los mexicanos ya que con unas fuerzas consideradas como inferiores lograron vencer a uno de los ejércitos más experimentados y respetados de su época. Pese a su éxito, la batalla no impidió la invasión del país, aunque sí que sería la primera batalla de una guerra que finalmente México ganaría. Los franceses regresarían al siguiente año, con lo que se libró una segunda batalla en Puebla en la que se enfrentaron 35 000 franceses contra 29 000 mexicanos (defensa que duró 62 días) y lograrían avanzar hasta la Ciudad de México, lo que permitió establecer el Segundo Imperio Mexicano.
Finalmente, ante la incapacidad de consolidar un imperio, y después de perder 11 000 hombres debido a la actividad guerrillera que nunca dejó de subsistir, los franceses se retiraron incondicionalmente del país en el año 1867.

Después de que el Presidente Benito Juárez anunciara que no pagaría la deuda externa, en octubre de 1861, Francia, Inglaterra y España suscribieron la Convención de Londres, en la cual se comprometieron a enviar contingentes militares a México para reclamar sus derechos como acreedores por una deuda que ascendía a alrededor de 80 millones de pesos, de los que aproximadamente 69 millones corresponderían a Inglaterra, 9 millones a España y 2 millones a Francia.

El contingente europeo estaba conformado como sigue:

Poco después de reunirse, los representantes de los tres países enviaron un ultimátum al gobierno mexicano en el que pedían el pago de sus deudas; de lo contrario, invadiría el país. Juárez, quien gobernaba a un país que apenas empezaba a levantarse de la postración económica, respondió con un exhorto a lograr un arreglo amistoso, y los invitó a una conferencia. Acompañó ese mensaje con la derogación del decreto que suspendió los pagos. Al mismo tiempo, en vista de la posibilidad real de una invasión militar que buscara llegar hasta la Ciudad de México, ordenó el traslado de pertrechos y la fortificación de Puebla, así como crear una unidad, a la que se designó como Ejército de Oriente, que fue puesta bajo el mando del general [José López Uraga. En vista del desempeño deficiente de este mando, fue destituido y en su lugar se designó a Zaragoza, quien dejó el Ministerio de Guerra y se dirigió a Puebla para organizar la oposición al avance francés con cerca de 10 000 hombres; cantidad mínima si se toma en cuenta el vasto territorio que debía cubrirse.

Los representantes aceptaron el llamado y en febrero de 1862 se reunieron con los ministros juaristas del Exterior, Manuel Doblado, y de Guerra, Ignacio Zaragoza, en la hacienda de La Soledad, cerca de Veracruz. Gracias a la habilidad como negociador de Doblado se firmaron los Tratados preliminares de La Soledad, en los que se obtuvo el reconocimiento como interlocutor para el gobierno de Juárez y se garantizó el respeto a la integridad e independencia del país. Además, se convino que las negociaciones sobre la deuda se realizaran el Orizaba, donde se establecerían las fuerzas aliadas, además de Córdoba y Tehuacán, para evitar el rigor del clima tropical del puerto; si no se llegaba a un acuerdo, se retirarían a la costa para así comenzar las hostilidades.

El 5 de marzo, cuando aún se realizaban las negociaciones en Orizaba, llegó a Veracruz un contingente militar francés bajo el mando de Charles Ferdinand Latrille, conde de Lorencez, quien relevó en el mando a Jurien de la Gravière y se dirigió a Tehuacán. También llegó el general conservador
Juan Nepomuceno Almonte, quien de inmediato se proclamó "jefe supremo de la nación" y empezó a reunir a las tropas conservadoras, remanentes de la Guerra de Reforma, para apoyar a los franceses.

En abril de 1862 la alianza tripartita se rompió debido a que España e Inglaterra se dieron cuenta de que Francia tenía un interés soterrado, de tipo geopolítico, bajo el reclamo económico: derrocar al gobierno republicano de México para establecer una monarquía favorable a su política colonial, con miras a contrarrestar el creciente poderío de Estados Unidos. De las instrucciones de Napoleón III dadas al jefe militar de la expedición, se sabe que el objetivo imperialista francés consistía en ampliar sus dominios estableciendo un protectorado, cuya administración serviría para ampliar los mercados, sostener las colonias en las Antillas y del sur de América y, de ese modo, garantizar el abasto de las materias primas en Francia. Los representantes español (Prim) e inglés (Charles Wyke) negociaron con el gobierno juarista por separado y en última instancia aceptaron las propuestas de moratoria del gobierno mexicano, y reembarcaron a sus tropas. La posición de Francia, en contraste, presentada por el diplomático Dubois de Saligny, fue la de exigir el pago inmediato de la deuda, que incluía un cobro exagerado por parte de la Casa Jecker por los destrozos causados durante la Guerra de Reforma, y tener control total y absoluto de las aduanas, así como intervención directa en la política económica del país.

A finales de abril, Lorencez desconoció los Tratados de Soledad y se puso en marcha, junto con sus efectivos, hacia Puebla, con el fin último de conquistar la Ciudad de México. A los militares franceses los rodeaba un aura de invencibilidad en combate dado que no habían sido derrotados desde Waterloo, casi 50 años antes, con sonadas victorias en las batallas de Solferino, Magenta y Sebastopol. Esta actitud quedó de manifiesto en el siguiente mensaje, que Lorencez envió al conde Jacques Louis César Alexandre Randon, ministro de Guerra francés, poco después de la batalla de Las Cumbres: "Somos tan superiores a los mexicanos en organización, disciplina, raza, moral y refinamiento de sensibilidades, que le ruego anunciarle a Su Majestad Imperial, Napoleón III, que a partir de este momento y al mando de nuestros 6,000 valientes soldados, ya soy dueño de México”. La confianza del alto mando francés no se debía sólo a un palmarés militar impecable, sino a la fragilidad general de México y sus instituciones. Con una economía destruida por casi 50 años de guerras civiles, con un Estado débil y una población dividida por las pugnas entre facciones, la conquista del país parecía una empresa factible con un contingente reducido.

Al conocer sobre el avance, el general Alejandro Constante Jiménez al mando de 2000 soldados se unió al general Zaragoza, que partió de Puebla con 4000 soldados para salir al encuentro de los franceses, quienes ya sostenían escaramuzas con guerrilleros. El comandante mexicano había enfrentado diversos problemas para conformar su ejército. Ante la falta de voluntarios y a que aún se mantenían hostilidades con grupos conservadores remanentes de la Guerra de Reforma, se había recurrido a la leva. Aunque se contaba con un cuerpo de oficiales joven pero experimentado, la mayor parte de la tropa carecía de la disciplina mínima, y estaba mal equipada y alimentada. En los días anteriores a la batalla, Zaragoza solicitó una y otra vez al alto mando en la Ciudad de México, el envío urgente de recursos económicos, ya que no podía costear ni siquiera los alimentos para las tropas. Para colmo, la explosión de un polvorín en la colecturía de los diezmos del poblado de San Andrés Chalchicomula (hoy Ciudad Serdán), ocurrida el 6 de marzo, había matado a 1,322 soldados de la Brigada de Oaxaca enviados por el general Ignacio Mejía para incorporarse al Ejército de Oriente.

El 28 de abril, el Ejército de Oriente se topó con la columna de Lorencez en un paso de montaña en las Cumbres de Acultzingo, en el límite entre Veracruz y Puebla, lo que representó el primer encuentro bélico formal. Zaragoza no pretendía cortarle el paso a los invasores, sino más bien foguear a sus soldados, muchos de ellos faltos de experiencia, y al mismo tiempo causarle el máximo de pérdidas posible al enemigo. En la llamada Batalla de Las Cumbres murieron 500 franceses, mientras las bajas mexicanas ascendieron sólo a 50. Pese a este saldo favorable, Zaragoza aún tenía desconfianza sobre el desempeño real de sus tropas en un combate en campo abierto. Luego de la retirada de los mexicanos, los franceses tomaron control del paso, con lo que aislaron al centro del país del principal puerto en el Golfo, y tuvieron la vía franca hacia Puebla.

Asegurado el paso de Acultzingo, el 2 de mayo de 1862 la columna principal del ejército expedicionario francés salió de San Agustín del Palmar, en Veracruz, para cruzar la Sierra Madre Oriental y dirigirse hacia Puebla, paso obligado para llegar a la capital del país y que era además uno de los bastiones del Partido Conservador, donde esperaban ser recibidos "con una lluvia de rosas", como le aseguró Saligny a Napoleón III en una carta. El 3 de mayo por la noche, el general Zaragoza arribó a Puebla, dejando en su retaguardia una brigada de caballería para hostigar a los invasores. Los efectivos del Ejército de Oriente se organizaron por las calles desiertas de la ciudad, ya que la mayoría de la población era partidaria de la invasión.

Zaragoza estableció su cuartel a unos cuantos metros de la línea de batalla, donde estableció el plan para la defensa de la plaza (ver tabla superior), que consistió en concentrar los pertrechos en el sur y oriente de la ciudad, esperando evitar que los franceses alcanzaran al área urbana de Puebla.

El 4 de mayo, los exploradores mexicanos volvieron con noticias de que una columna de conservadores a caballo, al mando de Leonardo Márquez y José María Cobos, marchaba por la zona de Atlixco para unirse con las fuerzas de Lorencez en el ataque a Puebla. Zaragoza envió una brigada de 2000 hombres bajo el mando de Tomás O'Horán y Antonio Carbajal, con el fin de detenerlo, lo cual lograron. Aunque sus fuerzas habían disminuido, los mexicanos se prepararon para la defensa de Puebla. Contaban con dos baterías de artillería de batalla y dos de montaña, cubriendo los fuertes con 1200 hombres y formando a otros 3500 en cuatro columnas de infantería con una batería de batalla y una brigada de caballería por el lado del camino a Amozoc.

El ala derecha mexicana la cubrían las tropas de Oaxaca dirigidas por Porfirio Díaz. El centro de la línea lo ocuparon Felipe Berriozábal y Francisco Lamadrid con las tropas del Estado de México y San Luis Potosí. La izquierda se apoyó en el cerro de Acueyametepec ubicado en el norte de la ciudad y en cuya cumbre se ubicaban los Fuertes de Loreto y Guadalupe, con el general Miguel Negrete a la cabeza de la Segunda División de Infantería. La artillería sobrante la colocaron en los fortines y reductos dentro de Puebla, quedando al mando del general Santiago Tapia.

A las 9:15 de la mañana del 5 de mayo, los franceses aparecieron en el horizonte, avanzando desde la cercana Hacienda de Rementería, cruzando fuego con las guerrillas de caballería que se batían en retirada y que no se replegaron hasta que las líneas francesas estuvieron formadas y listas para avanzar. La batalla se inició en forma a las 11:15 de la mañana, anunciándose con un cañonazo desde el Fuerte de Guadalupe y acompañado por los repiques de las campanas de la ciudad. En ese momento se dio una maniobra sorpresiva: la columna francesa, que venía avanzando en orden de oriente a poniente, se dividió en dos: la primera, compuesta por aproximadamente 4000 hombres y protegida con su artillería, dio un violento viraje hacia la derecha y se dirigió hacia los fuertes; mientras que la segunda columna, compuesta por el resto de la infantería, quedó como reserva.

Los conservadores Almonte y Antonio de Haro y Tamariz, que acompañaban a los franceses, habían sugerido que el ataque se dirigiera a las inmediaciones del ex Convento del Carmen, en el sur de la ciudad, tomando como antecedente lo que sucedió en el sitio durante la Guerra con Estados Unidos. Lorencez, confiado en la superioridad de sus tropas, así como en el auxilio que esperaba del contingente de Márquez, desoyó el consejo y decidió concentrar el ataque en los fuertes, donde los mexicanos contaban con la ventaja. Zaragoza advirtió la maniobra y rápidamente replanteó su plan de batalla, movilizando las tropas hacia las faldas del cerro. El 6o. Batallón de la Guardia Nacional del Estado de Puebla, bajo el mando del entonces coronel Juan Nepomuceno Méndez, fue el primer cuerpo del Ejército de Oriente en hacer frente a los franceses, al ubicarse en la línea comprendida entre los fuertes, y rechazar su ataque. Zaragoza hizo avanzar a las fuerzas de Berriozábal a paso veloz entre las rocas, situándolas entre la hondonada que separa a Loreto y Guadalupe. Mientras, el general Antonio Álvarez con su brigada protegió el flanco izquierdo de los reductos.

La línea de batalla mexicana formó un ángulo que se extendió desde Guadalupe hasta un sitio conocido como Plaza de Román, frente a las posiciones enemigas. Zaragoza dispuso que el general Lamadrid defendiera con las tropas potosinas y dos piezas de artillería el camino que conectaba a la ciudad con la garita de Amozoc. La derecha de la línea de batalla mexicana la cerró Porfirio Díaz con la División de Oaxaca, auxiliado por los escuadrones de Lanceros de Toluca y Oaxaca.

Los franceses continuaron su avance, colocando sus baterías frente a Guadalupe, al tiempo que devolvían el fuego mexicano proveniente de esa posición.

En ese momento los zuavos, el regimiento de élite de la infantería francesa, iniciaron su ascenso por el cerro hacia Guadalupe, perdiéndose de la vista de los fusileros mexicanos. De repente, aparecieron disparando frente a la fortificación. Sin embargo, el fuego lanzado por los mexicanos los detuvo en seco. En ese instante, los soldados de Berriozábal los recibieron con sus bayonetas, por lo que tuvieron que retirarse en buen orden hasta ponerse fuera de tiro. Se repusieron rápidamente y se lanzaron de nuevo intentando tomar el fuerte.

Los franceses, apoyados por el 1.. y 2o. Regimientos de Infantería de Marina, se abalanzaron sobre el resto de la línea mexicana, siendo recibidos con la bayoneta. La columna francesa fue rechazada en Guadalupe y Loreto, siendo igualmente repelidos los ataques de otras columnas francesas desplegadas. En ese momento, el coronel mexicano José Rojo avisó a Antonio Álvarez que era tiempo de que la caballería mexicana entrara en acción para alcanzar una victoria completa. Ordenó a los Carabineros de Pachuca cargar sobre los restos de la columna, disparando sus carabinas y lanzando mandobles de sable sobre los franceses, siendo totalmente rechazados.

A las dos y media de la tarde, cuando se empezaba a perfilar una victoria para los mexicanos, Lorencez se dispuso a lanzar el último asalto, dirigiendo a los Cazadores de Vincennes y el Regimiento de Zuavos hacia Guadalupe, mientras ponía en marcha una segunda columna de ataque compuesta de los restos de los cuerpos de batalla —excepto el 99 de Línea, el cuál quedó de reserva en el campamento francés—, para atacar por la derecha de la línea de batalla mexicana.

Ante esta situación, salieron a su encuentro los Zapadores de San Luis Potosí, al mando del general Lamadrid, librándose un terrible combate a la bayoneta. Una casa situada en la falda del cerro fue el objetivo. Los franceses la tomaron y se guarecieron en ella, siendo desalojados por los zapadores; la recobraron y de nuevo fueron expulsados por las tropas de Lamadrid. Un cabo mexicano de apellido Palomino se mezcló entre los zuavos y se batió con ellos cuerpo a cuerpo, posesionándose de su estandarte como botín de guerra al caer muerto el portador del mismo. Este momento significó un golpe anímico a favor de los defensores.

Ya entrada la tarde cayó un aguacero sobre el campo, lo cual dificultó el avance a las tropas francesas. Zaragoza dispuso que el Batallón Reforma de San Luis Potosí saliera en auxilio de los fuertes. En Loreto había un cañón de 68 libras que causaba enormes estragos en las filas francesas. Los zuavos hicieron una carga de infantería desesperada para apoderarse de esa pieza. El artillero mexicano, sorprendido por la rapidez de los franceses, tenía en sus manos la bala de cañón que no alcanzó a colocar en la boca de fuego. Un zuavo apareció frente a él y tras éste el resto del cuerpo que, una vez apoderados de ese fortín, levantarían la moral francesa y podría perderse la victoria conseguida. El artillero arrojó la bala al soldado francés, que herido mortalmente por el golpe en la cabeza rodó al foso del parapeto. Luego de que este asalto fue rechazado, los franceses retrocedieron siendo perseguidos por el Batallón Reforma.

Mientras, cuando la segunda columna llegó al Fuerte de Guadalupe protegida por una línea de tiradores, Porfirio Díaz acudió en auxilio de los Rifleros de San Luis Potosí, que estaban a punto de ser rodeados. Movió en columna al Batallón Guerrero, a las órdenes del coronel Jiménez, y le ganó el terreno a los franceses. Para apoyar envió al resto de las tropas de Oaxaca, con los coroneles Espinoza y Loaeza a la cabeza, con lo que se logró expulsar al enemigo de las cercanías. El éxito alentó a Díaz, que destacó al Batallón Morelos con dos piezas de artillería a la izquierda, mientras por la derecha los Rifleros de San Luis Potosí se reponían de la pelea, antecedidos por una carga de los Lanceros de Oaxaca, trabándose un combate cuerpo a cuerpo que hizo retroceder a los atacantes.

En aquel momento, luego de ser repelidos por última vez, las efectivos franceses empezaron a huir, completamente dispersados. Se replegaron a la hacienda Los Álamos, para finalmente retirarse hacia Amozoc.

Mientras se libraba la batalla, en el Palacio Nacional y en la Ciudad de México en general se vivía un ambiente de tensa espera. Lo último que se sabía de Puebla era el telegrama enviado por Zaragoza hacia las 12:30 del día, en el que avisaba que el fuego de artillería de ambos lados había iniciado. Luego, silencio. Ante la incertidumbre, el gobierno había hecho salir precipitadamente al general Florencio Antillón al mando de los Batallones de Guanajuato, quedando como guardianes de la capital sólo 2,000 hombres del Regimiento de Coraceros Capitalinos y algunos centenares de milicianos pobremente armados. Si las tropas guanajuatenses se perdían, la capital quedaría desprotegida.

A las 4:15 de la tarde finalmente se recibieron noticias:

Zaragoza envió más tarde otro telegrama en el que dijo que los franceses habían iniciado la retirada hacia Amozoc, pero sin mencionar el resultado final de la batalla. Finalmente, a las 5:49 de la tarde se recibió otro parte, dirigido al ministro de Guerra, que causó júbilo (y un gran alivio) en Palacio Nacional:

Dos horas después de haber sido remitido el parte anterior a la Secretaría de Guerra, el presidente de la República recibía el siguiente:

El saldo final de la batalla fue de 476 soldados perdidos y 345 heridos o enfermos del lado francés, así como 83 muertos, cerca de 132 heridos y 12 desaparecidos para el Ejército de Oriente. A las 7 de la noche del día 6 de mayo arribaron a Puebla el general Antillón y sus tropas; Zaragoza esperaba un nuevo ataque de Lorencez, pero éste, el día 8 de mayo, dispuso la retirada hasta San Agustín del Palmar, siendo "saludado" por la artillería republicana y la Banda de Guerra de los Carabineros, quienes tocaron "Escape".

El 5 de septiembre de 1862, todavía acuartelado en Puebla, el general Zaragoza contrajo tifo y falleció tres días después.Lo sustituyó en el mando del Ejército de Oriente el general Jesús González Ortega, quien se encargaría de la defensa de la ciudad ya que se esperaba el regreso de los franceses, reagrupados y con refuerzos, lo cual sucedió en marzo del siguiente año. Los historiadores concuerdan en señalar el talento de Zaragoza como organizador y motivador de sus tropas. Antes de la batalla, las arengó diciéndoles que si bien los franceses eran considerados "los primeros soldados del mundo", ellos eran "los primeros hijos de México", lo cual tuvo tal efecto en la moral de sus soldados que su determinación por defender la plaza ante los invasores compensó sus carencias materiales y de disciplina. Además, no temió tomar decisiones arriesgadas, como prescindir de los 2000 efectivos que O'Horan se llevó para batir a Leonardo Márquez, y en el curso de la batalla actuó con serenidad y efectividad. Se le considera héroe nacional y en su honor, tiempo después, Juárez renombró a la ciudad como Heroica Puebla de Zaragoza.

Cabe atribuir parte de la responsabilidad en el resultado de la batalla a Lorencez, por decidir lanzarse en primer lugar contra Loreto y Guadalupe en lugar de ir sobre la ciudad. Esta acción no carece de sentido si se toma en cuenta que el general francés se encontraba confiado en la victoria por lo que había sucedido en las Cumbres, además de que bajo la lógica militar de su tiempo, primero había que atacar al enemigo en sus posiciones más fuertes. En todo caso, ensoberbecido por la superioridad "per se" de los franceses, no contó con la férrea resistencia mexicana y cometió yerros garrafales: así, por ejemplo, fue famosa su orden de colocar sus cañones en batería a dos kilómetros y medio de las fortificaciones poblanas, lo cual fue calificado por el propio Napoleón III como un disparate ya que las balas llegaban a sus blancos, pero sin fuerza. El conde fue repatriado y lo sustituyó Frédéric Forey en el mando de las tropas expedicionarias.
Cuando en Francia se supo la derrota del ejército francés, originó dolor, histeria y llanto, más aún cuando llegaron las historias de que los indígenas zacapoaxtlas (que en realidad se trataba del sexto Batallón de Guardia Nacional del Estado de Puebla) habían atacado con machetes, arma desconocida en Europa, y se comían los cadáveres. El resto de Europa, con incredulidad, sorpresa y asombro, comentaban como el ejército francés, invicto desde la Batalla de Waterloo en 1815, había sido derrotado en México, un ejército considerado el mejor del mundo, el vencedor en la conquista de Argelia y de la Indochina francesa (hoy Vietnam), había sido derrotado por un país tropical, utilizando tácticas de guerra poco utilizada en Europa, como era la "Guerra de Guerrillas".

La guerra de guerrillas efectivamente fue utilizada en México, pero ya antes se conocía tal táctica en Europa, y más en concreto en España, lugar donde se dio por vez primera este tipo de guerra 400 años antes de Cristo, e incluso contra la invasión de las tropas de Napoleón a principios del siglo XIX, y que por tal motivo en este país se le dio tal nombre, guerra de guerrillas.

El 21 de mayo de 1862 el presidente Juárez publicó el decreto de condecoración a los vencedores de las batallas del 28 de abril en las Cumbres de Acultzingo y del 5 de mayo en Puebla, y ambas se consideraron victorias ante el ejército expedicionario francés.

El 30 de mayo se entregaron a los miembros del Ejército de Oriente los "diplomas de Concurrencia" a las mismas batallas, según lo estipulado en el artículo cuarto del mencionado decreto.

El 29 de noviembre Juárez viajó, acompañado por sus ministros de Guerra, Miguel Blanco Múzquiz, y de Relaciones Exteriores y Gobernación, Juan Antonio de la Fuente, a Puebla para una serie de ceremonias y reconocimientos a los defensores de la ciudad. Se reunió con González Ortega, y finalmente, el 4 de diciembre, en medio de una gran ceremonia en el Fuerte de Guadalupe, hizo entrega formal de las medallas a los vencedores de las batallas del 28 de abril y del 5 de mayo de ese año, y partió al día siguiente a la Ciudad de México. Asimismo, el 2 de marzo de 1863, en vísperas del inicio del Sitio de Puebla, se llevó a cabo una segunda ceremonia en Guadalupe, en la que entregó más medallas.

Con excepción del Grito de Dolores, la conmemoración de la Batalla de Puebla es la fecha más significativa del calendario cívico mexicano, al tratarse de una de las escasas victorias ante un ejército extranjero invasor. Simbólicamente, representa la consecución de una gran empresa por parte de los mexicanos, que puede conseguirse si se olvidan las divisiones y se sobreponen éstas a las carencias, como lo demuestra el hecho de que se consiguió la victoria, con valor y dedicación, pese a que se tenía todo en contra: inferioridad numérica y material, la moral disminuida por la tragedia de Chalchicomula, y la simpatía de algunos sectores de las élites y de la clase política hacia los invasores. A cambio, los republicanos respondieron con celeridad a las situaciones que la batalla iba planteando (movilizaron el grueso de sus efectivos del casco urbano de Puebla hacia los fuertes) y supieron sacar ventaja de los errores de los franceses. Semanas antes de la batalla, Juárez había declarado pena de muerte para los mexicanos que se unieran a los invasores, pero también una amnistía a los enemigos de la república en la guerra de Reforma si se unían a él para defender al país de la invasión. El caso más célebre es el del general conservador Miguel Negrete, quien abandonó al partido conservador y se puso a disposición de Zaragoza con la siguiente frase: "Yo tengo patria antes que partido."

El 5 de mayo es una fecha entrañable para los mexicanos; se celebra en las principales ciudades del país con desfiles y verbenas. Ese día se le toma protesta en todo el país a los jóvenes que cumplen el Servicio Militar Nacional.

Sin embargo, el recuerdo de la batalla no se agota en el protocolo. En algunos lugares del país se realizan fiestas populares en las que se recrea la batalla misma o algunos de sus aspectos, como en el caso del Peñón de los Baños, en la Ciudad de México, o en Huejotzingo, en Puebla; sitios en donde se da una peculiar fusión de elementos de carnaval con la fiesta cívica. Incluso en una celebración plenamente religiosa como son las Morismas de Bracho, en Zacatecas, que se hacen tradicionalmente el último fin de semana del mes de agosto, donde miles de personas representan combates entre moros y cristianos, aparecen participantes con uniformes tomados de la batalla de Puebla; por ejemplo, el contingente de los moros adoptó el uniforme de los zuavos franceses; asimismo, el ejército cristiano adoptó el uniforme del regimiento de zapadores, y las bandas de guerra cristianas llevan el uniforme mexicano utilizado el 5 de mayo. Ambas tropas simulan combates al son de marchas francesas.

En los Estados Unidos, el 5 de mayo es el "Día de la Herencia Latina", en la que se celebra la inmigración procedente de México. Ello ha dado pie a que se piense, erróneamente, que el aniversario de la batalla es el día de la Independencia de México.




</doc>
<doc id="6724" url="https://es.wikipedia.org/wiki?curid=6724" title="República Checa">
República Checa

La República Checa ( ), también denominada abreviadamente Chequia ("Česko" ), es un país soberano de Europa Central sin litoral. Limita con Alemania al oeste, con Austria al sur, con Eslovaquia al este y con Polonia al norte. Su capital y mayor ciudad es Praga. La República Checa tiene territorios de lo que antaño fueron Moravia y Bohemia y una pequeña parte de Silesia.

El Estado checo, antes conocido como Bohemia, se formó en el siglo IX d. C. como un pequeño ducado en torno a Praga en el seno del entonces poderoso Imperio de Gran Moravia. Tras la disolución de este imperio en 907, el centro de poder pasó de Moravia a Bohemia bajo la dinastía Premislidas y desde 1002 el ducado fue formalmente reconocido como parte del Sacro Imperio Romano Germánico. En 1212 el ducado alcanzó la categoría de reino y durante el gobierno de los reyes y duques Premislidas y sus sucesores, los Luxemburgo, el país alcanzó su mayor extensión territorial en los siglos XIII y XIV. Durante las guerras husitas el reino tuvo que sufrir embargos económicos y la llegada de caballeros cruzados de toda Europa.

Tras la batalla de Mohács en 1526, el Reino de Bohemia pasó a integrarse gradualmente a los dominios de los Habsburgo como uno de sus tres dominios principales, junto al Archiducado de Austria y el Reino de Hungría. La derrota de los bohemios en la batalla de la Montaña Blanca, que significó el fracaso de la revuelta de 1618-1620, llevó a la guerra de los Treinta Años y a una mayor centralización de la monarquía, además de a la imposición de la fe católica y la germanización. Con la disolución del Sacro Imperio Romano Germánico en 1806, el reino de Bohemia se integró en el Imperio austríaco. Durante el siglo XIX las tierras checas se alzaron como centro industrial de la monarquía y después como núcleo de la República de Checoslovaquia que se creó en 1918, resultado del colapso del Imperio austrohúngaro en la Primera Guerra Mundial. Después de 1933, Checoslovaquia era la única democracia de toda la Europa central y del este.

Tras los Acuerdos de Múnich en 1938, la anexión polaca del área de Zaolzie y la ocupación alemana de Checoslovaquia y la consecuente desilusión de los checos con la pobre respuesta de Occidente, los comunistas se hicieron con su favor liberando el país del yugo nazi durante la Segunda Guerra Mundial. El Partido Comunista de Checoslovaquia ganó las elecciones de 1946 y en el Golpe de Praga de 1948 el país pasó a estar gobernado por el comunismo. Sin embargo, la creciente insatisfacción del pueblo llevó a intentar la reforma del régimen, que culminó en la conocida como Primavera de Praga de 1968 y provocó la invasión de las fuerzas armadas del Pacto de Varsovia, unas tropas que permanecieron en el país hasta la Revolución de Terciopelo de 1989, cuando colapsó el régimen comunista. El 1 de enero de 1993, Checoslovaquia se dividió pacíficamente en sus dos Estados constituyentes, la República Checa y la República Eslovaca.

En 2006 la República Checa se convirtió en el primer exmiembro del Comecon en alcanzar el estatus pleno de país desarrollado según el Banco Mundial. Además, el país tiene el mayor índice de desarrollo humano de toda Europa Central y del Este y por ello está considerado un Estado con «Desarrollo humano muy alto». Es el noveno país más pacífico de Europa, el más democrático y el que registra menor de su región. La República Checa es una democracia representativa parlamentaria, miembro de la Unión Europea, la OTAN, la OCDE, la OSCE, el Consejo de Europa y el Grupo Visegrád.

La República Checa o Chequia se llama en checo "Česká republika" o "Česko", denominación que deriva del nombre de la mayor región del país, "Čechy" (Bohemia en español) o del etnónimo "checos", nombre de una de las tribus eslavas que habitaron el actual territorio del país después de la época de las migraciones y que dominó la zona hacia el 530. El origen del nombre de la tribu es desconocido. De acuerdo con una leyenda, proviene del líder Praotec Čech ("padre Checo").

El territorio checo fue unificado a finales del siglo IX por la dinastía de los "přemyslitas" (checo "Přemyslovci"; este nombre significa "los que piensan mucho"). El Reino de Bohemia fue un poder regional significante, con el rey de Bohemia como uno de los siete electores del emperador del Sacro Imperio Romano Germánico. Las minas de oro convirtieron el reino en un poder que no tenía impuestos, y podía reclutar mercenarios casi sin límite, pues la riqueza de las minas mantuvo el poder del reino hasta su agotamiento.

Durante los quinientos años siguientes fue un reino estable, centro de cultura y educación en la Europa Central. Durante el reinado de Carlos IV de Luxemburgo (1344-1378), Bohemia vivió su época de oro (siempre auspiciado por las minas del mismo metal). Carlos IV convirtió a esta monarquía en la capital del Sacro Imperio Romano Germánico. En el año 1348 fundó la Universidad Carolina de Praga, el centro de estudios superiores más antiguo de la Europa Central.

A la muerte de Carlos IV, comienza un período de decadencia del reino e inestabilidad política. Uno de los factores fueron los conflictos religiosos, como las guerras husitas provocadas por la quema en la hoguera del reformista Jan Hus en 1415 en el Concilio de Constanza.

Después de la dinastía de los reyes polacos Jagellón, fue elegido en 1526 al trono checo el español Fernando I de Habsburgo, nieto de los Reyes Católicos e hijo de Juana I de Castilla. Con este acto, y por casi cuatrocientos años, los Habsburgo ocuparon la corona checa y, por ende, pasó a formar parte del Imperio austriaco, posteriormente Austrohúngaro.

A partir de entonces y durante los siglos XVI, XVII y parte del XVIII las relaciones checo-españolas recibieron un fuerte impulso, principalmente potenciado por la población católica, que veía a España como potencia protectora y garante de esta religión. La nobleza católica checa se orientó hacia España a través de enlaces matrimoniales con miembros de la corte española que había llegado a Praga con Fernando I, formada por consejeros, secretarios, embajadores y también artistas. Estas familias checo-españolas fueron el origen de lo que posteriormente se ha denominado «facción española». Esta facción española estaba formada por checos católicos procedentes de la nobleza -como las familias Pernstejn, Rozmberk, Lobkowicz y Dietrichstein, entre otras- y del clero, en el que sobresalían los jesuitas del Clementinum. La facción española era reducida en número pero muy influyente en términos políticos, económicos y culturales, y actuaron como transmisores de cultura y costumbres españolas, adquiridas como símbolos de prestigio y que traspasaron a sus descendientes. Así, la influencia española en Bohemia se mantuvo tras la muerte de Fernando I. La moda española dominó el ambiente checo durante los siglos XVI y XVII, especialmente bajo el reinado de Rodolfo II (1576-1612), aferrado al protocolo y vestimenta española y aficionado al coleccionismo de objetos exóticos provenientes de la América española. En la sociedad checa lo español tendió a identificarse con el lujo y la religión católica. Los no católicos denominaban a los católicos checos «spanihelé» (españoles). Otra vía muy importante de expansión de la influencia española en Bohemia fue la actuación de las órdenes religiosas, principalmente de los jesuitas, llegados desde España en la década de 1520, antes del comienzo del reinado de Fernando I. Los jesuitas reformaron las universidades checas y mejoraron su nivel. En algunos lugares del país fue habitual que los rectores de las universidades fueran personajes españoles. Una de las figuras españolas más importantes en el ámbito académico checo fue Rodrigo de Arriaga, el filósofo jesuita más importante de su época, estudiado y citado frecuentemente por Descartes y Leibniz y por el que nació el dicho "Videre Pragam et audire Arriagam" (ver Praga y escuchar a Arriaga). Otras órdenes religiosas también influyeron decisivamente en el ambiente cultural checo, como la orden benedictina con la figura de Juan Caramuel, prior del monasterio de Emaús.

Bohemia sufrió guerras devastadoras en los siglos XVII y XVIII, como la Guerra de los Treinta Años y la Guerra de los Siete Años durante el reinado de María Teresa en 1756-1763, pero también se benefició del impulso económico y social que vivió la monarquía durante los siglos XVIII y XIX, que convirtieron a Bohemia en el corazón industrial de la Monarquía.

Después del colapso del Imperio austrohúngaro tras la Primera Guerra Mundial, los checos junto con sus vecinos los eslovacos y los rutenos se unieron para formar la república independiente de Checoslovaquia en 1918. Este nuevo país contenía a una gran minoría alemana, lo cual llevó a la disolución de Checoslovaquia cuando Alemania anexó a esta minoría en virtud de los Acuerdos de Múnich en 1938 y Eslovaquia declaró su independencia. El Estado checo restante fue ocupado por los alemanes en 1939.

Al finalizar la Segunda Guerra Mundial, Checoslovaquia se convirtió en un Estado socialista alineado con la Unión Soviética. En 1968, una intervención armada de las fuerzas del Pacto de Varsovia terminó con una serie de reformas impulsadas por el entonces primer ministro Alexander Dubček, conocidas como la "Primavera de Praga", tendentes según sus partidarios a crear un ""socialismo con rostro humano"". En 1989, Checoslovaquia adoptó el multipartidismo y empezó a abandonar progresivamente la economía socialista, lo que se conoce como "Revolución de Terciopelo". El 1 de enero de 1993, Checoslovaquia se dividió en dos por decisión parlamentaria. Desde entonces, la República Checa (o Chequia), por un lado, y la República Eslovaca (o Eslovaquia), por otro, son dos países independientes.

La República Checa se adhirió a la OTAN en 1999 y a la Unión Europea en 2004.

La República Checa es una democracia parlamentaria, cuya Constitución y la Carta de los Derechos y Libertades Fundamentales (parte integrante de la Carta Magna de la República Checa) fueron ratificadas el 16 de diciembre de 1992 y entraron en vigencia el primero de enero de 1993.

El derecho civil está basado en el sistema legal austrohúngaro. El sistema legal se encuentra actualmente en la etapa final de armonización con la legislación de la Unión Europea.

El presidente de la República Checa es quien ejerce las funciones de jefe de Estado. El presidente de la República era elegido en elecciones indirectas por las dos cámaras del Parlamento en sesión conjunta, hasta que en el año 2013 fue elegido por primera vez un presidente por votación directa; el mandato presidencial es de cinco años.

Su pertenencia a la Unión Europea es el eje central de la política exterior de la República Checa. La República Checa tomó la Presidencia de la Unión Europea durante la primera mitad de 2009.

El Ministerio de Relaciones Exteriores de la República Checa recomienda la denominación «Chequia» (en checo "Česko") para cualquier situación excepto para documentos oficiales y desea que se siga el mismo patrón que con otros estados, por ejemplo, la República Francesa o el Reino de España, usualmente conocidos como "Francia" y "España", respectivamente. Aun así, aunque en castellano el nombre corto comienza a utilizarse, el término no ha sido reconocido en forma cartográfica. El Diccionario de la lengua española que no incluye entre sus definiciones los topónimos de ningún tipo (países, regiones, ciudades, etc.) muestra la siguiente acepción del gentilicio checa: «natural de la República Checa», sin hacer mención también a "Chequia". No obstante, sí es reconocido por las Academias de la lengua a través del Diccionario panhispánico de dudas, en donde recoge que «no hay razones para censurar, en textos de carácter no oficial, el uso de la forma "Chequia", surgida por analogía con Eslovaquia».

Las fuerzas armadas checas se componen del ejército de tierra, la fuerza aérea y de unidades de soporte especializadas. En 2004, el servicio militar dejó de ser obligatorio y las fuerzas armadas se transformaron en un cuerpo totalmente voluntario tanto en cuanto a tierra como en cuanto a aire. El país es miembro de la OTAN desde el 12 de marzo de 1999. El gasto en defensa ronda el 1,8 % del PIB (2006).

La República Checa consta de trece regiones ("kraje" en checo) y una ciudad capital "(hlavní město)", marcada con un *:

Ubicada en Europa Central, a unos 50 grados de latitud Norte y 15 de longitud Este. Con una superficie de 78 867 km², comparable con la de Portugal, Austria o Irlanda, este estado tiene una densidad demográfica de 131 habitantes por kilómetro cuadrado.

En el interior del país aparecen planicies y mesetas ligeramente onduladas, mientras que a lo largo de la frontera, con excepción del sureste del país, se alzan cadenas montañosas que formaban históricamente la frontera natural de los llamados "Países Checos". El punto más bajo es la salida del río Elba del territorio checo, mientras que el más alto es el monte Sněžka, con 1 602 metros de altitud.

El bioma dominante es el bosque templado de frondosas, aunque también está presente el bosque templado de coníferas en los Cárpatos. WWF divide el territorio de la República Checa entre cuatro ecorregiones:

La República Checa posee una economía altamente desarrollada, con un PIB per cápita de 82 % del promedio de la Unión Europea. La República Checa es además una de las más estables y prósperas economías dentro de los países del antiguo bloque soviético, con un crecimiento económico sobre el 6% anual durante los años previos a las crisis del 2008, principalmente impulsado por las exportaciones al resto de países de la Unión Europea, en especial a Alemania, y por la demanda interna. Recientemente se ha acordado la venta de un 7% de las acciones del productor de energía, grupo CEZ (Skupina ČEZ). Un estudio realizado en 2009 encontró que la mayoría de los economistas están a favor de continuar con la liberalización en la mayoría de los sectores de la economía.

En el año 2009, la República Checa tenía una población de 10 501 197 de habitantes. La esperanza de vida es de 78,7 años. El 99% de la población está alfabetizada. El promedio de hijos por mujer es de tan sólo 1,22 lo cual está provocando que su población se reduzca un 0,07% cada año. El crecimiento de la población se ha producido principalmente, desde 2003, gracias a la inmigración.

La mayoría de sus habitantes (80%) son oriundos del país y hablan el checo, idioma perteneciente a las lenguas eslavas, en concreto a las lenguas eslavas occidentales. Otros grupos étnicos presentes son alemanes, gitanos checos, húngaros, ucranianos, vietnamitas y polacos. Después de la división de 1993, algunos eslovacos permanecieron en territorio checo y forman el 2% de la población actual. La frontera entre la República Checa y Eslovaquia se cerró completamente para los ciudadanos de la antigua Checoslovaquia. En 2004, la República Checa se adhirió al Acuerdo de Schengen.

En cuanto a las creencias religiosas y al escepticismo, el 62,2% de la población es agnóstica o atea, el 34% católica, el 2% protestante, el 1% husita checoslovaca, y también existe una pequeña comunidad judía (0,8 %).

La composición étnica actual es la siguiente:


La República Checa tiene una de las poblaciones más arreligiosas en el mundo, siendo la tercera población más arreligiosa por detrás de China y Japón. Históricamente, los checos se han caracterizado como «tolerantes e incluso indiferentes hacia la religión». Según el censo de 2011, el 34% de la población declaró no tener religión, el 10,3% era católico, el 0,8% era protestante (0,5% Hermanos Checos y el 0,4% husita) y el 9% seguía otras formas de religión, tanto confesionales o no (de los cuales 863 personas respondieron que son paganas). El 45% de la población no respondió a la pregunta sobre religión. De 1991 a 2001, y aún más para 2011, la adhesión al catolicismo disminuyó del 39% al 27% y luego al 10%; el protestantismo bajó igualmente del 3,7% al 2% y luego al 0,8%.

Según una encuesta del Eurobarómetro de 2010, el 16% de los ciudadanos checos respondió que «creen que hay un Dios» (la tasa más baja entre los países de la Unión Europea), mientras que el 44% contestó que «creen que hay algún tipo de espíritu o fuerza de la vida» y el 37% dijo que «no creen que haya ningún tipo de espíritu, Dios o fuerza vital».

Praga es una ciudad donde la cultura y las artes brillan con especial intensidad. El cartel de actividades culturales es rico y variado. Los amantes de las artes pueden encontrar en esta ciudad un paraíso cultural. Las localidades para los diversos eventos se suelen agotar rápidamente por lo que conviene reservarlas con bastante antelación (a través de agencias de viajes y en las propias taquillas del lugar donde se celebre el acontecimiento).

La cultura de esta república es rica y variada. Doce de constan inscritos en la lista del Patrimonio de la Humanidad de la Unesco.

En la República Checa el es de 132 litros por habitante al año, el mayor del mundo.

Entre sus principales bebidas figuran Fernet, Becherovka, Sekt (vino espumoso) y, por supuesto, la cerveza de Pilsen conocida mundialmente como Pils o Pilsner.

Los deportes más populares son el hockey sobre hielo (en el que los checos se han proclamado campeones mundiales y olímpicos en diversas ocasiones) y el fútbol (con dos subtítulos mundiales y un subcampeonato de Europa en 1996). La ya desaparecida Checoslovaquia obtuvo un título olímpico (Moscú 1980) y un campeonato de Europa en 1976.

En tenis el país ha dado muchos jugadores de primer nivel mundial, como Ivan Lendl (nacionalizado estadounidense durante la recta final de su carrera), Karel Novacek, Petr Korda, Radek Stepanek o Tomas Berdych, quien ganó la Copa Davis tanto como Checoslovaquia como ya República Checa. Aparte, tenistas nacidos en otros países como Richard Krajicek o Vasek Pospisil tienen orígenes checos. Martina Navratilova, nacionalizada estadounidense pero que empezó su carrera compitiendo como checoslovaca, Hana Mandlikova, Helena Sukova, Jana Novotna, Petra Kvitova y Karolína Plíšková son las tenistas más destacadas que ha dado el país.

La República Checa casi ha monopolizado el decatlón en las últimas olimpiadas, con Roman Šebrle como poseedor de la plusmarca mundial.

Otros deportes donde destaca son el baloncesto femenino o el balonmano, entre otros.




</doc>
<doc id="6727" url="https://es.wikipedia.org/wiki?curid=6727" title="6 de julio">
6 de julio

El 6 de julio es el 187.º (centésimo octogésimo séptimo) día del año en el calendario gregoriano y el 188.º en los años bisiestos. Quedan 178 días para finalizar el año.





El 6 de julio es el Día del Beso Robado, que se festeja en el Reino Unido y es una celebración por aparte aunque similar a la del Día Internacional del Beso, el 13 de abril.




</doc>
<doc id="6733" url="https://es.wikipedia.org/wiki?curid=6733" title="Cerio">
Cerio

El cerio es un elemento químico de símbolo Ce y número atómico 58. Es uno de los 14 elementos químicos que siguen al lantano en la tabla periódica, denominados por ello lantánidos. El cerio está situado entre el lantano y el praseodimio.

Es un metal blando, de color gris metálico similar al hierro, dúctil, que se oxida fácilmente al contacto con el aire y se torna pardo rojizo. El cerio es el más abundante de los elementos de las tierras raras, su abundancia representa solo el 0,0046% en peso de la corteza terrestre, donde aparece disperso en diversos minerales, como la cerita, bastnasita y monacita. Existen numerosas aplicaciones comerciales del cerio. Entre estos usos se incluyen catalizadores, aditivos para el combustible para reducir la contaminación ambiental y a los vidrios y esmaltes para cambiar sus colores. El óxido de cerio es un componente importante de los polvos utilizados para pulir vidrios y fósforos utilizados en pantallas y lámparas fluorescentes. Es utilizado también en la "piedra" o "yesca" de los encendedores (aleación ferrocerio).

Fue descubierto en 1803 por Martin Heinrich Klaproth y Jöns Jacob Berzelius y de manera independiente también por Wilhelm von Hisinger. El elemento fue nombrado en honor al planeta enano Ceres, descubierto dos años antes. El planeta lleva el nombre de la diosa romana Ceres (mitología).

Fue descubierto en 1803 por Martin Heinrich Klaproth y Jöns Jacob Berzelius y de manera independiente también por Wilhelm von Hisinger. Tomó su nombre de Ceres, el planeta enano/asteroide que se había encontrado años antes, concretamente en 1801 (que a su vez fue denominado así en honor a la diosa romana de la agricultura).

Es el lantánido más abundante y económico. El metal es duro y de color gris acerado, tornándose pardo rojizo. Es buen conductor del calor y la electricidad. Reacciona con los ácidos diluidos y con el agua (produciendo hidrógeno). Es inestable en el aire seco, cubriéndose de una capa de óxido en el aire húmedo.

En la Tierra el cerio es casi tan abundante como el cobre; especialmente en forma de óxido de cerio, que habitualmente se utiliza como polvos abrasivos para pulir vidrio. El metal cerio es pirofórico, lo que significa que se inflama fácilmente. El cerio no realiza ninguna función biológica conocida.

El elemento natural está constituido de los isótopos Ce, Ce, Ce y Ce. El Ce es prácticamente estable con una vida media de 5 x 10 años. El cerio se encuentra mezclado con otras tierras raras en muchos minerales, en particular en monacita y bastnasita y también se halla entre los productos de la fisión de uranio, torio y plutonio.


Las principales menas de cerio son la cerita, la bastnasita y la monacita. 

El óxido de cerio es un abrasivo que puede encontrarse en algunos ambientes de trabajo, donde constituye un riesgo al ser inhalado en forma de partículas finas. La exposición prolongada puede provocar embolias pulmonares. El cerio, al igual que otros lantánidos, puede sustituir al calcio en algunos procesos metabólicos. Sin embargo, su absorción por vía oral es muy baja y no constituye un peligro inmediato. El cloruro de cerio administrado por vía intravenosa puede inducir fallo cardiovascular y hepático.

El cerio es vertido al medio ambiente en muchos lugares diferentes, principalmente por industrias productoras de petróleo. También puede entrar en el medio ambiente cuando se tiran los equipos domésticos. El cerio se acumula gradualmente en los suelos y en el agua de los suelos y esto lleva finalmente a incrementar su concentración en humanos, animales y partículas del suelo. 
En los animales acuáticos provoca daños a las membranas celulares, lo que tiene varias influencias negativas en la reproducción y en las funciones del sistema nervioso.

En "El Sistema Periódico", el escritor y químico de profesión Primo Levi dedica el capítulo "Cerio" a su estadía en el campo de concentración de Auschwitz, donde sobrevivió gracias al intercambio de pequeñas varillas de ferrocerio por comida. 


</doc>
<doc id="6734" url="https://es.wikipedia.org/wiki?curid=6734" title="Lenguas de Argentina">
Lenguas de Argentina

El uso del idioma español es predominante, entendido y hablado como primera o segunda lengua por casi toda la población de la República Argentina, que según las últimas estimaciones supera los 40 millones.

El inglés es la segunda lengua más conocida en el país, y su enseñanza es obligatoria desde la escuela primaria en varias provincias. Argentina es el único país latinoamericano calificado como país de "alta aptitud" en el inglés, ubicándose en el puesto 15 a nivel mundial, según un informe del Índice de Aptitud en Inglés (EF EPI). El inglés es, además, la lengua que se usa por la mayoría de los habitantes de las islas bajo control del Reino Unido que para Argentina constituyen el departamento Islas del Atlántico Sur de la provincia de Tierra del Fuego, Antártida e Islas del Atlántico Sur. 

El guaraní y el quechua son otras lenguas importantes con más de un millón de hablantes cada uno (en 2015).

Existen unas 25 lenguas indoamericanas vivas y existieron varias (hoy extintas) en diversas regiones. Las lenguas indoamericanas vernáculas ("nativas del territorio argentino") vivas son habladas por muy pocas personas. Por otra parte, el lunfardo que es una jerga y argot tipo "pidgin" con predominio de palabras originadas en lenguas italianas como el piamontés, ligur o «zeneise», calabrés, siciliano, lombardo, etc., hablado en la Zona Núcleo de Argentina desde al menos 1880. Por otra parte el portuñol es un pidgin de portugués brasileño y español hablado desde aproximadamente 1960 en las zonas de Argentina fronterizas con Brasil.

Otra lengua nativa es la lengua de señas argentina (LSA), lengua señalada por las comunidades sordas que surge claramente a partir de 1885 e influencia a muchas otras lenguas de señas de países limítrofes.

Entre los hablantes de "lenguas no vernáculas" se encuentran (aparte de los más de 40 millones de argentinos que hablan el español (principalmente el español rioplatense como lengua materna, lengua coloquial y lengua vehicular); los del italiano (alrededor de 1.500.000 [un millón quinientasmil] personas). Por otra parte los desconocedores de antropología y lingüística suelen muy erróneamente incluir como «lenguas vernáculas» al guaraní (impuesto por la invasión de los avá procedentes de la Amazonia hacia el siglo XV DEC y propagado por los misioneros europeos jesuitas en lo que hoy es el NEA como lengua vehicular entre diversas etnias que nada tenían de linajes "avá" o «guaraníes»); algo similar ocurrió en el NOA con el quechua y el aimara impuestos tras la invasión inca procedente del sureste peruano y lo que hoy es el norte del Collao o Altiplano boliviano en el NOA y el extremo noroeste del Cuyo en el siglo XV DEC y luego aún más difundido por los misioneros jesuitas y franciscanos como lengua vehicular para la catequesis a partir de la Conquista española en el siglo XVI y que, actualmente tras la migración ocurrida desde Bolivia y de Perú a partir de la segunda mitad de la década de los 1990. 

Luego de las antecitadas lenguas e idiomas le siguen el alemán (cerca de 400 000, incluyendo un número significativo de hablantes del dialecto alemán del Volga y del plautdietsch). Los idiomas árabe, francés, portugués, ruso, euskera, gallego, catalán (o valenciano), asturiano, yidish oriental, chino (unos 100 000 hablantes, principalmente de los dialectos de Fujian y de Taiwán), coreano, japonés (alrededor de 50 000, en su mayoría hablantes de okinawense), rumano, occitano, lituano, letón, estonio, ucraniano, bielorruso, croata, esloveno, checo, eslovaco, finés, sueco, danés, noruego, islandés, irlandés, neerlandés, polaco, húngaro, serbio, bosnio, albanés, griego, macedonio, búlgaro, hebreo, turco, armenio, romaní vlax (lengua de los roma, llamados vulgarmente «gitanos») también son importantes.

La República Argentina no ha establecido por norma legal ningún idioma oficial; con todo, el idioma castellano es el utilizado (desde la fundación del estado argentino) por la administración de la república y en el que se imparte la educación en todos los establecimientos públicos, hasta tal punto que en los niveles básico y secundario existe como asignatura obligatoria la de la lengua castellana (asignatura llamada «Lengua»). Tal obligatoriedad es una imposición que ha resultado en un factor de cohesión social entre los millones de habitantes de Argentina. Existe una Academia Argentina de Letras, fundada en 1931, que desde 1952 colabora regularmente con la Real Academia Española para el registro de las variantes locales.

Si bien la Constitución Nacional establece como función del Congreso Nacional «reconocer la preexistencia étnica y cultural de los pueblos indígenas argentinos», a éstos aún no se les ha reconocido la oficialidad de sus lenguas nativas, excepto en las provincias de Chaco y Corrientes.

El idioma castellano en Argentina se presenta principalmente a través del dialecto rioplatense. También existen un dialecto en la región de Cuyo y el acento cordobés. En el norte se habla un español andino y el noreste tiene grandes influencias del español paraguayo.

Las lenguas indoeuropeas de Argentina habladas por comunidades estables pertenecen a cinco ramas: romance (español y portugués), germánico occidental ("plautdietsch" y alemán estándar), celta britónico (galés) y indoario central (romaní).

Las lenguas indígenas de Argentina por otra parte son más diversas y pertenecen a diferentes familias lingüísticas entre ellas:
(†): extintos

Además del español, se registran en Argentina las siguientes lenguas vivas con desarrollo local propio:


La lengua de señas argentina, entendida por alrededor de dos millones de personas sordas de la Argentina, sus instructores, descendientes y otros. Se diferencian variantes regionales, tales como la de Córdoba. 

El quechua sureño: de la familia de lenguas quechuas. Presenta 7 variaciones que se enmarcan en su origen geográfico. Aquí se detallan el sudboliviano y la lengua quichua santiagueña:


En las provincias de Corrientes, Misiones, Chaco, Formosa, Entre Ríos y Buenos Aires donde los dialectos del idioma guaraní argentinos son hablados o conocidos por cerca de un millón de personas, incluyendo inmigrantes paraguayos que hablan el guaraní paraguayo o el jopará (2005). En Corrientes, en donde se habla el dialecto guaraní argentino se decretó en 2004 la cooficialidad de la lengua guaraní y su uso obligatorio en la enseñanza y gobierno.


El mapudungun, araucano, mapuchedungun, chedungun, mapuche o mapudungu, dialectos: pehuenche, nguluche, huilliche, ranquelche: una lengua aislada con aproximadamente 40 000 a 100 000 hablantes en las provincias de Neuquén, Río Negro, Chubut y Santa Cruz en el año 2000.

Aimara central: lengua del grupo aimara, hablada por 30 000 habitantes de Jujuy, del norte de Salta, además de inmigrantes de la puna boliviana y de Perú.

Del grupo mataco o mataguayo:

Del grupo guaicurú:


Además de las lenguas indígenas sobrevivientes, antes del contacto con los europeos y durante algún tiempo durante la conquista de América en Argentina se hablaron además las siguientes lenguas, que la actualidad están extintas:
















</doc>
<doc id="6748" url="https://es.wikipedia.org/wiki?curid=6748" title="Símbolos de Almería">
Símbolos de Almería


Su origen está en el año 1147, cuando el ejército genovés aliado de Alfonso VII desembarcó en la playa de los Genoveses de cabo de Gata para participar en la conquista de Almería. La ciudad adoptó como símbolo propio la enseña genovesa, que se corresponde con la cruz de San Jorge (cruz griega en gules sobre campo de plata).

La bandera oficial quedó descrita de la siguiente manera por decreto de la Consejería de Gobernación y Justicia de la Junta de Andalucía en 1997:


El escudo de la ciudad de Almería fue inscrito en el Registro Andaluz de Entidades Locales por resolución de la Consejería de Gobernación y Justicia de la Junta de Andalucía el 25 de enero de 2005. En el texto oficial referido se formula una descripción que no se corresponde con las reglas del blasón y se atribuye una correspondencia dudosa a alguna de las armerías representadas:
La inclusión del emblema del águila y su atribución son controvertidas y hay versiones diferentes sobre si se corresponde o no con el sello de Sancho VII el Fuerte, de origen navarro. atribuye en 1860 el siguiente blasón: «Las armas de la provincia y de la ciudad de Almería están cuarteladas por una cruz llana de gules y bordura de castillos, leones y granadas alternadas.» Durante el franquismo, fueron editados por Correos una serie de sellos en los que aparecen las cadenas, emblema moderno del reino de Navarra.


Oficialmente, el himno de Almería es el aprobado por el ayuntamiento de la ciudad el 21 de agosto de 1916, con letra del poeta Antonio Ledesma y música de Manuel García Martínez, maestro de capilla de la catedral. Sin embargo, el himno que hoy día suele interpretarse es un poema de José María Álvarez de Sotomayor llamado "Almería", musicado por el compositor José Padilla y adoptado por el ayuntamiento de Almería en 1946.El poema estaba dedicado a toda la provincia, aunque oficialmente el himno es oficial sólo para su capital. Asimismo, puede considerarse himno oficioso de la capital el popular "Fandanguillo de Almería", obra del compositor Gaspar Vivas, que puede escucharse en el carrillón del ayuntamiento de la ciudad.


El símbolo por excelencia de Almería y su provincia es el indalo, originalmente una pintura rupestre neolítica descubierta en la cueva de los Letreros (en el actual Parque Natural de Sierra de María-Los Vélez) en 1868 que representa a un hombre cazando con arco. 

En los años 1940, el Movimiento Indaliano idealizó el símbolo, queriendo ver en él un hombre que sostenía al arco iris. Su nombre se le habría dado en honor a San Indalecio y provendría en última instancia del ibero "indal eccius", «mensajero de los Dioses».

La tradición popular, no obstante, lo veneraba como símbolo benefactor desde siglos atrás. Solía verse pintado sobre las paredes de las casas para protegerse del rayo y el mal de ojo en poblaciones como Mojácar, cuyos artesanos estilizaron la figura durante el "boom" turístico de los años 60, convirtiéndolo en símbolo primero de dicha localidad y después de toda la provincia. 

Se trata de un símbolo muy arraigado en la sociedad almeriense y es frecuente verlo por doquier. Encontramos uno, por ejemplo, en el escudo de la Unión Deportiva Almería.


Otro símbolo muy unido a la ciudad es el erróneamente llamado sol de Portocarrero, altorrelieve que representa un sol antropomorfo rodeado de guirnaldas y que aparece esculpido en el testero de la capilla funeraria del obispo Diego Fernández de Villalán, en la catedral de Almería, levantada durante en siglo XVI. Por confusión histórica, se ha relacionado tradicionalmente este símbolo con otro obispo de la diócesis almeriense, Juan del Castillo y Portocarrero, del que toma nombre a pesar de ser el mandato de éste muy posterior.
El escudo de la Universidad de Almería, por ejemplo, está basado en este símbolo.

La patrona de la ciudad es la Virgen del Mar, que celebra su festividad el sábado anterior al último domingo de agosto. Su imagen fue encontrada en la playa de Torregarcía, cercana a la capital, por el vigía de la torre homónima, Andrés de Jaén, el 21 de diciembre de 1502. La leyenda dice que la imagen, gótica, pudo formar parte de los enseres de un barco naufragado, o ser un mascarón de proa que los fieles habrían vestido como Virgen. Muy cerca de la torre se construyó a principios del siglo XX la ermita de la Virgen del Mar, en torno a la cual se celebra una romería el segundo fin de semana de cada enero.

El patrón de la ciudad y diócesis de Almería es San Indalecio, fundador de la diócesis de la antigua Urci en el siglo I y uno de los siete Varones Apostólicos, primeros evangelizadores de la península Ibérica por encomienda de San Pablo. Su festividad se celebra el 15 de mayo.


</doc>
<doc id="6757" url="https://es.wikipedia.org/wiki?curid=6757" title="Programación extrema">
Programación extrema

La programación extrema o "eXtreme Programming" (de ahora en adelante, XP) es una metodología de desarrollo de la ingeniería de software formulada por Kent Beck, autor del primer libro sobre la materia, "Extreme Programming Explained: Embrace Change" (1999). Es el más destacado de los procesos ágiles de desarrollo de software. Al igual que éstos, la programación extrema se diferencia de las metodologías tradicionales principalmente en que pone más énfasis en la adaptabilidad que en la previsibilidad. Los defensores de la XP consideran que los cambios de requisitos sobre la marcha son un aspecto natural, inevitable e incluso deseable del desarrollo de proyectos. Creen que ser capaz de adaptarse a los cambios de requisitos en cualquier punto de la vida del proyecto es una aproximación mejor y más realista que intentar definir todos los requisitos al comienzo del proyecto e invertir esfuerzos después en controlar los cambios en los requisitos.

Se puede considerar la programación extrema como la adopción de las mejores metodologías de desarrollo de acuerdo a lo que se pretende llevar a cabo con el proyecto, y aplicarlo de manera dinámica durante el ciclo de vida del software.

Los valores originales de la programación extrema son: simplicidad, comunicación, retroalimentación ("feedback") y coraje. Un quinto valor, respeto, fue añadido en la segunda edición de "Extreme Programming Explained". Los cinco valores se detallan a continuación:

La simplicidad es la base de la programación extrema. Se simplifica el diseño para agilizar el desarrollo y facilitar el mantenimiento. Un diseño complejo del código junto a sucesivas modificaciones por parte de diferentes desarrolladores hacen que la complejidad aumente exponencialmente.

Para mantener la simplicidad es necesaria la refactorización del código, ésta es la manera de mantener el código simple a medida que crece.

También se aplica la simplicidad en la documentación, de esta manera el código debe comentarse en su justa medida, intentando eso sí que el código esté autodocumentado. Para ello se deben elegir adecuadamente los nombres de las variables, métodos y clases. Los nombres largos no decrementan la eficiencia del código ni el tiempo de desarrollo gracias a las herramientas de autocompletado y refactorización que existen actualmente.

Aplicando la simplicidad junto con la autoría colectiva del código y la programación por parejas se asegura que cuanto más grande se haga el proyecto, todo el equipo conocerá más y mejor el sistema completo.

La comunicación se realiza de diferentes formas. Para los programadores el código comunica mejor cuanto más simple sea. Si el código es complejo hay que esforzarse para hacerlo inteligible. El código autodocumentado es más fiable que los comentarios ya que éstos últimos pronto quedan desfasados con el código a medida que es modificado. Debe comentarse sólo aquello que no va a variar, por ejemplo el objetivo de una clase o la funcionalidad de un método.

Las pruebas unitarias son otra forma de comunicación ya que describen el diseño de las clases y los métodos al mostrar ejemplos concretos de como utilizar su funcionalidad.
Los programadores se comunican constantemente gracias a la programación por parejas.
La comunicación con el cliente es fluida ya que el cliente forma parte del equipo de desarrollo. El cliente decide qué características tienen prioridad y siempre debe estar disponible para solucionar dudas.

Al estar el cliente integrado en el proyecto, su opinión sobre el estado del proyecto se conoce en tiempo real.

Al realizarse ciclos muy cortos tras los cuales se muestran resultados, se minimiza el tener que rehacer partes que no cumplen con los requisitos y ayuda a los programadores a centrarse en lo que es más importante.

Considérense los problemas que derivan de tener ciclos muy largos. Meses de trabajo pueden tirarse por la borda debido a cambios en los criterios del cliente o malentendidos por parte del equipo de desarrollo.
El código también es una fuente de retroalimentación gracias a las herramientas de desarrollo. Por ejemplo, las pruebas unitarias informan sobre el estado de salud del código. Ejecutar las pruebas unitarias frecuentemente permite descubrir fallos debidos a cambios recientes en el código.

Muchas de las prácticas implican valentía. Una de ellas es siempre diseñar y programar para hoy y no para mañana. Esto es un esfuerzo para evitar empantanarse en el diseño y requerir demasiado tiempo y trabajo para implementar el resto del proyecto. La valentía le permite a los desarrolladores que se sientan cómodos con reconstruir su código cuando sea necesario. Esto significa revisar el sistema existente y modificarlo si con ello los cambios futuros se implementarán más fácilmente. Otro ejemplo de valentía es saber cuando desechar un código: valentía para quitar código fuente obsoleto, sin importar cuanto esfuerzo y tiempo se invirtió en crear ese código. Además, valentía significa persistencia: un programador puede permanecer sin avanzar en un problema complejo por un día entero, y luego lo resolverá rápidamente al día siguiente, sólo si es persistente.

El respeto se manifiesta de varias formas. Los miembros del equipo se respetan los unos a otros, porque los programadores no pueden realizar cambios que hacen que las pruebas existentes fallen o que demore el trabajo de sus compañeros. Los miembros respetan su trabajo porque siempre están luchando por la alta calidad en el producto y buscando el diseño óptimo o más eficiente para la solución a través de la refactorización del código. Los miembros del equipo respetan el trabajo del resto no haciendo menos a otros, una mejor autoestima en el equipo eleva su ritmo de producción.

Las características fundamentales del método son:









La simplicidad y la comunicación son extraordinariamente complementarias. Con más comunicación resulta más fácil identificar qué se debe y qué no se debe hacer. Cuanto más simple es el sistema, menos tendrá que comunicar sobre éste, lo que lleva a una comunicación más completa, especialmente si se puede reducir el equipo de programadores.

Escribe las pruebas unitarias y produce el código del sistema. Es la esencia del equipo.

Escribe las historias de usuario y las pruebas funcionales para validar su implementación. Asigna la prioridad a las historias de usuario y decide cuáles se implementan en cada iteración centrándose en aportar el mayor valor de negocio.

Ayuda al cliente a escribir las pruebas funcionales. Ejecuta pruebas regularmente, difunde los resultados en el equipo y es responsable de las herramientas de soporte para pruebas.

Es el encargado de seguimiento. Proporciona realimentación al equipo. Debe verificar el grado de acierto entre las estimaciones realizadas y el tiempo real dedicado, comunicando los resultados para mejorar futuras estimaciones.

Responsable del proceso global. Guía a los miembros del equipo para seguir el proceso correctamente.

Es un miembro externo del equipo con un conocimiento específico en algún tema necesario para el proyecto. Ayuda al equipo a resolver un problema específico.
Además este tiene que investigar según los requerimientos.

Es el dueño de la tienda y el vínculo entre clientes y programadores. Su labor esencial es la coordinación.




</doc>
<doc id="6761" url="https://es.wikipedia.org/wiki?curid=6761" title="Harry Markowitz">
Harry Markowitz

Harry Max Markowitz, (Chicago, 1927 - ) recibió el premio Nobel de Economía en 1990. Markowitz es uno de los numerosos economistas laureados con el Nobel de Economía durante el siglo XX producidos por la prestigiosa Escuela de Economía de Chicago de la Universidad de Chicago.

Harry Markowitz nació en el seno de una familia judía, hijo de Morris y Mildred Markowitz. Durante la escuela secundaria, Markowitz desarrolló interés por la física y la filosofía, en particular las ideas de David Hume, un interés que siguió durante sus años de estudiante en la Universidad de Chicago. Después de recibir su B.A., Markowitz decidió continuar sus estudios en la Universidad de Chicago, eligiendo especializarse en economía. Allí tuvo la oportunidad de estudiar con importantes economistas, entre ellos Milton Friedman, Tjalling Koopmans, Jacob Marschak y Leonard Savage. Cuando aún era estudiante, fue invitado a convertirse en miembro de la Comisión Cowles de Investigación en Economía, que se encontraba en Chicago en ese momento.

Markowitz eligió aplicar las matemáticas al análisis del mercado bursátil como tema de su disertación. Jacob Marschak, que era el asesor de tesis, lo alentó a seguir con el tema, señalando que también había sido un interés favorito de Alfred Cowles, el fundador de la Comisión Cowles. Mientras investigaba la comprensión actual de los precios de las acciones, que en ese momento consistía en el modelo de valor presente de John Burr Williams, Markowitz se dio cuenta de que la teoría carece de un análisis del impacto del riesgo. Esta idea condujo al desarrollo de su teoría fundamental de la asignación de carteras bajo incertidumbre, publicada en 1952 por el Journal of Finance.

En 1952, Harry Markowitz fue a trabajar para la Corporación RAND, donde conoció a George Dantzig. Con la ayuda de Dantzig, Markowitz continuó investigando técnicas de optimización, desarrollando aún más el algoritmo de línea crítica para la identificación de las carteras óptimas de varianza media, confiando en lo que más tarde se denominó la frontera de Markowitz. En 1955, recibió un doctorado de la Universidad de Chicago con una tesis sobre la teoría de la cartera. El tema era tan novedoso que, mientras Markowitz defendía su disertación, Milton Friedman argumentó que su contribución no era económica. Durante 1955-1956 Markowitz pasó un año en la Fundación Cowles, que se había mudado a la Universidad de Yale, por invitación de James Tobin. Publicó el algoritmo de línea crítica en un documento de 1956 y usó su estancia en la fundación para escribir un libro sobre asignación de cartera que se publicó en 1959.

En el año 1989 recibió el Premio de Teoría John von Neumann de la Sociedad de Investigación de Operaciones de América (ahora Instituto de Investigación de Operaciones y Ciencias de la Gestión, INFORMS) por sus contribuciones en la teoría de tres campos: teoría de la cartera; métodos de matriz dispersa; y programación del lenguaje de simulación (SIMSCRIPT). Los métodos de matriz dispersa se usan ahora ampliamente para resolver sistemas muy grandes de ecuaciones simultáneas cuyos coeficientes son en su mayoría cero. SIMSCRIPT se ha utilizado ampliamente para programar simulaciones por computadora de fabricación, transporte y sistemas informáticos, así como juegos de guerra. SIMSCRIPT (I) incluyó el método de asignación de memoria Buddy, que también fue desarrollado por Markowitz.

Markowitz ganó el Premio Nobel Memorial en Ciencias Económicas en 1990 mientras era profesor de finanzas en el Baruch College de la City University de Nueva York. Aunque el tema de las finanzas empresariales ya había sido tratado por otros dos laureados, James Tobin (1981) y Franco Modigliani (1985), el Nobel de 1990 sorprendió a los economistas estudiosos de la teoría económica "pura".
Aunque Markowitz, Miller y Sharpe —los tres premiados— eran ampliamente respetados en los ambientes académicos, no se esperaba que su trabajo —notoriamente puntual, dada la amplitud de la ciencia económica— fuese premiado; esto resulta evidente si se analizan los trabajos de los economistas previamente galardonados.

Habría una explicación concreta para que el premio haya sido concedido a la economía financiera (son expresidentes de la Sociedad Estadounidense de Finanzas): el hecho de que los servicios financieros sean actualmente muy importantes en los países desarrollados y que los mercados bursátiles cubran amplios segmentos de ahorristas, lo que es indudablemente un fenómeno nuevo en la década de los años 1980.

La compañía que se convertiría en CACI International fue fundada por Herb Karr y Harry Markowitz el 17 de julio de 1962 como California Analysis Center, Inc. Ellos ayudaron a desarrollar SIMSCRIPT, el primer lenguaje de programación de simulación, en RAND y después de su lanzamiento al dominio público, fue fundada CACI para proporcionar soporte y capacitación para SIMSCRIPT.

En 1968, Markowitz se unió a la compañía Arbitrage Management fundada por Michael Goodkin. Al trabajar con Paul Samuelson y Robert Merton, creó un fondo de cobertura que representa el primer intento conocido de negociación de arbitraje computarizado. Asumió el cargo de director ejecutivo en 1970. Después de una exitosa carrera como fondo privado de cobertura, AMC fue vendida a Stuart & Co. en 1971. Un año después, Markowitz dejó la compañía. 

Markowitz ahora divide su tiempo entre la enseñanza (es profesor adjunto en la Rady School of Management de la Universidad de California en San Diego, UCSD); da conferencias de vídeo; y hace consultoría (desde sus oficinas de Harry Markowitz Company). Actualmente es miembro del Consejo Asesor de BPV Capital Management (anteriormente SkyView Investment Advisors), una firma de asesoría de inversiones alternativa y un fondo de fondos de cobertura. Markowitz también es miembro del Comité de Inversiones de LWI Financial Inc. ("Loring Ward"), un asesor de inversiones con sede en San José, California; en el panel asesor de la firma de gestión de inversiones de Robert D. Arnott en Newport Beach, California, Research Affiliates; en el Consejo Asesor de Mark Hebner's Irvine, California y en la firma consultora de inversiones basada en Internet, Index Fund Advisors; y como asesor del Comité de Inversiones de 1st Global, una firma de asesoría de inversiones y gestión de patrimonio con sede en Dallas, Texas. Markowitz asesora y forma parte de la junta directiva de ProbabilityManagement.org, una organización sin fines de lucro 501 (c) fundada por el Dr. Sam L. Savage para remodelar la comunicación y el cálculo de la incertidumbre.


Harry Markowitz presentó este modelo en 1952. Consiste en la selección de la cartera de inversiones más eficiente al analizar varias carteras posibles de valores dados. Al elegir valores que no se "mueven" exactamente juntos, el modelo HM muestra a los inversores cómo reducir su riesgo. El modelo HM también se denomina modelo de media varianza debido a que se basa en los rendimientos esperados (media) y la desviación estándar (varianza) de las diversas carteras. Harry Markowitz hizo las siguientes suposiciones al desarrollar el modelo HM:

1. El riesgo de una cartera se basa en la variabilidad de los rendimientos de dicha cartera.

2. Un inversor es reacio al riesgo.

3. Un inversor prefiere aumentar el consumo.

4. La función de utilidad del inversor es convexa y creciente, debido a su aversión al riesgo y preferencia de consumo.

5. El análisis se basa en un modelo de inversión de un solo período.

6. Un inversor maximiza el rendimiento de su cartera para un nivel de riesgo dado o maximiza su rendimiento por el riesgo mínimo.

7. Un inversor es de naturaleza racional.

Para elegir el mejor portafolio entre una cantidad de portafolios posibles, cada uno con diferente rentabilidad y riesgo, se deben tomar dos decisiones por separado:

1. Determinación de un conjunto de carteras eficientes.

2. Selección de la mejor cartera del conjunto eficiente.

Una cartera que ofrece el máximo rendimiento para un riesgo dado, o el riesgo mínimo para un rendimiento dado es una cartera eficiente. Por lo tanto, las carteras se seleccionan de la siguiente manera:

(a) De las carteras que tienen la misma rentabilidad, el inversor preferirá la cartera con menor riesgo, y

(b) A partir de las carteras que tienen el mismo nivel de riesgo, un inversor preferirá la cartera con mayor tasa de rendimiento.
Como el inversor es racional, les gustaría obtener un mayor rendimiento. Y como es reacio al riesgo, quiere tener un riesgo menor. En la Figura 1, el área sombreada PVWP incluye todos los valores posibles en los que un inversor puede invertir. Los portafolios eficientes son los que se encuentran en el límite de PQVW. Por ejemplo, en el nivel de riesgo x2, hay tres carteras S, T, U. Pero la cartera S se denomina cartera eficiente, ya que tiene el rendimiento más alto, y2, en comparación con T y U. Todas las carteras que se encuentran en el límite de PQVW son portafolios eficientes para un nivel de riesgo dado.

El límite PQVW se llama frontera eficiente. Todas las carteras que se encuentran debajo de la Frontera Eficiente no son lo suficientemente buenas porque la rentabilidad sería menor para el riesgo dado. Las carteras que se encuentran a la derecha de Efficient Frontier no serían lo suficientemente buenas, ya que existe un mayor riesgo para una tasa de rendimiento dada. Todas las carteras que se encuentran en el límite de PQVW se denominan Carteras Eficientes. La frontera eficiente es la misma para todos los inversores, ya que todos los inversores quieren el máximo rendimiento con el menor riesgo posible y son reacios al riesgo.

Para la selección de la cartera óptima o la mejor cartera, se analizan las preferencias de riesgo-rendimiento. Un inversor que es muy reacio al riesgo mantendrá una cartera en la esquina inferior izquierda de la frontera, y un inversor que no sea demasiado reacio al riesgo elegirá una cartera en la parte superior de la frontera.
La Figura 2 muestra la curva de indiferencia de riesgo-rendimiento para los inversores. Se muestran las curvas de indiferencia C1, C2 y C3. Cada uno de los diferentes puntos en una curva de indiferencia particular muestra una combinación diferente de riesgo y rendimiento, que proporcionan la misma satisfacción a los inversores. Cada curva a la izquierda representa una mayor utilidad o satisfacción. El objetivo del inversor sería maximizar su satisfacción moviéndose a una curva que sea más alta. Un inversionista puede tener satisfacción representada por C2, pero si su satisfacción / utilidad aumenta, entonces él / ella pasa a la curva C3. Por lo tanto, en cualquier momento, un inversor será indiferente entre las combinaciones S1 y S2, o S5 y S6.
La cartera óptima del inversor se encuentra en el punto de tangencia de la frontera eficiente con la curva de indiferencia. Este punto marca el nivel más alto de satisfacción que el inversor puede obtener. Esto se muestra en la Figura 3. R es el punto donde la frontera eficiente es tangente a la curva de indiferencia C3, y también es una cartera eficiente. Con esta cartera, el inversor obtendrá la mayor satisfacción, así como la mejor combinación de riesgo-rendimiento (una cartera que ofrece el mayor rendimiento posible para una cantidad determinada de riesgo). Cualquier otra cartera, por ejemplo, X, no es la cartera óptima, aunque se encuentra en la misma curva de indiferencia que está fuera de la cartera factible disponible en el mercado. La cartera Y tampoco es óptima, ya que no se encuentra en la mejor curva de indiferencia factible, a pesar de que es una cartera de mercado factible. Otro inversor que tenga otros conjuntos de curvas de indiferencia podría tener una cartera diferente como su cartera mejor / óptima.

Todas las carteras hasta ahora se han evaluado en términos de valores de riesgo solamente, y es posible incluir también valores sin riesgo en una cartera. Una cartera con valores libres de riesgo permitirá a los inversores alcanzar un mayor nivel de satisfacción. Esto se explica en la Figura 4.
R1 es el rendimiento libre de riesgo, o el rendimiento de los valores del gobierno, ya que se considera que esos valores no tienen ningún riesgo a efectos de modelado. R1PX se dibuja de modo que sea tangente a la frontera eficiente. Cualquier punto en la línea R1PX muestra una combinación de diferentes proporciones de valores libres de riesgo y carteras eficientes. La satisfacción que obtiene un inversionista de las carteras en la línea R1PX es más que la satisfacción obtenida de la cartera P. Todas las combinaciones de carteras a la izquierda de P muestran combinaciones de activos riesgosos y libres de riesgo, y todas las que están a la derecha de P representan compras de activos de riesgo hechos con fondos prestados a la tasa libre de riesgo.

En el caso de que un inversor haya invertido todos sus fondos, se pueden tomar prestados fondos adicionales a una tasa libre de riesgo y se puede obtener una combinación de cartera que se encuentra en R1PX. R1PX se conoce como Capital Market Line (CML). esta línea representa la transacción de riesgo-rendimiento en el mercado de capitales. La CML es una curva de pendiente ascendente, lo que significa que el inversor asumirá un mayor riesgo si el rendimiento de la cartera también es mayor. La cartera P es la cartera más eficiente, ya que depende tanto de CML como de Efficient Frontier, y cada inversor preferiría obtener esta cartera, P. La cartera de P se conoce como la Cartera de Mercado y también es la cartera más diversificada. Consiste en todas las acciones y otros valores en el mercado de capitales.

En el mercado de carteras que consiste en valores con riesgo y libres de riesgo, la CML representa la condición de equilibrio. La línea Capital Market dice que el rendimiento de una cartera es la tasa libre de riesgo más la prima de riesgo. La prima de riesgo es el producto del precio de mercado del riesgo y la cantidad de riesgo, y el riesgo es la desviación estándar de la cartera.


__FORZAR_TDC__

</doc>
<doc id="6762" url="https://es.wikipedia.org/wiki?curid=6762" title="Cuadro de mando">
Cuadro de mando

El concepto de cuadro de mando proviene del concepto denominado "tableau de bord" en Francia, que traducido de manera literal,significa tablero de mandos, o cuadro de instrumentos, como los que se encuentran en el salpicadero de un coche.

La gestión de las empresas requiere un sistema de indicadores (en inglés KPIs o "Key Performance Indicators") que nos faciliten la toma de decisiones y el control. Se requiere un sistema completo de análisis.

Existe infinidad de posibles indicadores que podemos utilizar. Algunos ratios o indicadores son de uso muy general. Los más habituales son, por ejemplo:

Otros indicadores deberán ser elaborados expresamente para analizar una empresa concreta.

El sistema de indicadores debe organizarse en un cuadro de mando. El cuadro de mando recoge los principales indicadores y los presenta de un modo claro y útil. El cuadro de mando es un sistema que nos informa de la evolución de los parámetros fundamentales del negocio.

Los cuadros de mando han de presentar sólo aquella información que sean imprescindible, de una forma sencilla y por supuesto, sinóptica y resumida.



</doc>
<doc id="6764" url="https://es.wikipedia.org/wiki?curid=6764" title="Rotación del inventario">
Rotación del inventario

La rotación del inventario o rotación de existencias es uno de los parámetros utilizados para el control de gestión de la función logística o del departamento comercial de una empresa. La rotación, en este contexto, expresa el número de veces que se han renovado las existencias (de un artículo, de una materia prima...) durante un período, normalmente un año.

Este valor constituye un buen indicador sobre la calidad de la gestión de los abastecimientos, de la gestión del stock y de las prácticas de compra de una empresa. No puede establecerse una cifra ya que varía de un sector a otro: las empresas fabricantes suelen tener índices de rotación entre 4 y 5; los grandes almacenes procuran llegar a 8; y los hipermercados pueden llegar a 25 en algunos artículos del surtido de alimentación. 

La rotación del inventario corresponde a la frecuencia media de renovación de las existencias consideradas, durante un tiempo dado.
Se obtiene al dividir el consumo (venta, expediciones...), durante un período, entre el valor del inventario medio, de ese mismo período. 

Por ejemplo, si un vendedor de coches mantiene de media 10 coches en exposición en su tienda y al año vende un total de 150 vehículos, su stock tiene una rotación de 15. La rotación se calcula dividiendo las ventas totales, en este caso 150, entre el inventario medio, en este caso 10.

La rotación del inventario, en realidad, está informando del número de veces que se recupera la inversión en existencias, durante un periodo. En el ejemplo anterior, el vendedor de coches ha recuperado 15 veces la inversión en coches que realizó durante el año, al vender 150 vehículos, manteniendo unas existencias medias de 10.

La rotación, o índice de rotación, IR, se calcula con la expresión:



Las dos cifras deben expresarse en la misma unidad.

Fórmula para determinar la rotación de inventarios
La rotación de inventarios se determina dividiendo el costo de las mercancías vendidas en el periodo entre el promedio de inventarios durante el periodo. (Coste mercancías vendidas/Promedio inventarios) = N veces.

La rotación es una parte importante de la rentabilidad. De forma abreviada:

En muchos casos, cuando el margen es ajustado, la mejor opción para aumentar la rentabilidad es incrementar la rotación.

El mantener inventarios produce un costo de oportunidad, pues para tenerlos se debe hacer una inversión de capital, por ello la importancia de determinar adecuadamente su tamaño.




</doc>
<doc id="6766" url="https://es.wikipedia.org/wiki?curid=6766" title="Margen">
Margen

Margen o El margen puede hacer referencia a:






</doc>
<doc id="6767" url="https://es.wikipedia.org/wiki?curid=6767" title="Indicador de ventas">
Indicador de ventas

Los indicadores de ventas del cuadro de mando suelen ser las ventas por metro cuadrado, las ventas por empleado y ventas por establecimiento.

Las "ventas por metro cuadrado". Un ratio muy empleado para realizar comparaciones entre distintas secciones de una tienda, entre tiendas y entre distintas empresas. Las ventas por metro cuadrado facilitan analizar la evolución de las tiendas a lo largo del tiempo. Es un dato que suele estar fácilmente disponible para comparar las empresas unas con otras.

Las "ventas por empleado" es igualmente un indicador fácil de conseguir. Permite realizar comparaciones entre secciones, tiendas y empresas. Y también es un ratio que nos facilita seguir la evolución a lo largo del tiempo.

"Ventas por establecimiento". Si dividimos las ventas totales entre el número de tiendas de la "cadena", obtenemos la media de ventas por establecimiento. Permite comparar unas tiendas con otras y 


</doc>
<doc id="6768" url="https://es.wikipedia.org/wiki?curid=6768" title="Rentabilidad financiera">
Rentabilidad financiera

En economía, la rentabilidad financiera o «ROE» (por sus iniciales en inglés, "Return on equity") relaciona el beneficio económico con los recursos necesarios para obtener ese lucro. Dentro de una empresa, muestra el retorno para los accionistas de la misma, que son los únicos proveedores de capital que no tienen ingresos fijos.

La rentabilidad puede verse como una medida de cómo una compañía invierte fondos para generar ingresos. Se suele expresar como porcentaje.
La rentabilidad financiera, ROE, se calcula:

formula_1 

Por ejemplo si se coloca en una cuenta un millón y los intereses generados son cien mil, la rentabilidad es del 10%. La rentabilidad de la cuenta se calcula dividiendo la cantidad generada y la cantidad que se ha necesitado para generarla.

Sumando al numerador del anterior ratio la cuota del impuesto que grava la renta de la sociedad, se obtiene la rentabilidad financiera antes de los impuestos. Cuando la rentabilidad económica es superior al coste del endeudamiento (expresado ahora en tanto por ciento, para poder comparar, y no en valor absoluto como anteriormente), cuanto mayor sea el grado de endeudamiento mayor será el valor de la rentabilidad financiera o rentabilidad de los accionistas, en virtud del juego del denominado efecto palanca. Por el contrario, cuando la rentabilidad económica es inferior al coste de las deudas (el capital ajeno rinde menos en la empresa de lo que cuesta) se produce el efecto contrario: el endeudamiento erosiona o aminora la rentabilidad del capital propio.

A efectos de poder realizar un análisis más detallado de las causas que generan rentabilidad, en la empresa DuPont desarrolló a principios del siglo XX la fórmula de DuPont que desagrega la fórmula anterior en tres términos:

Permite a la empresa dividir su retorno en los componentes de utilidad sobre ventas y eficiencia sobre uso de los activos.
Se pueden introducir en la expresión otras variables que afectan a la rentabilidad financiera: ventas y activos.

formula_2

Por la descomposición y ampliación de la expresión se obtiene:

formula_3

Los dos primeros componentes se muestran en el apartado anterior, el margen y la rotación. El tercer componente es el apalancamiento que se define como la relación entre las inversiones (el activo) y los recursos propios de la empresa. Los dos primeros componentes son derivados de la operatoria del negocio, mientras que el tercero es el agregado financiero. Un ROE que aumenta por crecimiento de Margen o Rotación es un ROE que crece por razones de negocio, mientras que un ROE que crece por un aumento del apalancamiento muestra una empresa que tiene un aumento en su riesgo financiero. 

Una de las grandes ventajas de la descomposición del ROE mediante Dupont es que nos permite identificar los "drivers" de la rentabilidad y seguir su evolución en el tiempo. De esta manera es posible seguir la evolución del Margen, la Rotación y el apalancamiento a lo largo del tiempo y poder identificar cuales son las causas de una caída de la rentabilidad y tomar medidas correctivas a tiempo. Es útil crear un indicador de "Margen x Rotación" que ayuda a ver la contribución combinada de los componentes del negocio de la rentabilidad.




</doc>
<doc id="6769" url="https://es.wikipedia.org/wiki?curid=6769" title="Cuadro de mando integral">
Cuadro de mando integral

El concepto de Cuadro de mando Integral – CMI (Balanced Scorecard – BSC) Se presentó en el número de enero/febrero de 1992 de la revista "Harvard Business Review", con base en un trabajo realizado para una empresa de semiconductores. Sus autores, Robert Kaplan y David Norton, plantean el CMI como un sistema de administración o sistema administrativo ("management system"), que va más allá de la perspectiva financiera con la que los gerentes acostumbran a evaluar la marcha de una empresa. Según estos dos consultores, gestionar una empresa teniendo en cuenta solamente los indicadores financieros tradicionales (existencias, inmovilizado, ingresos, gastos...) olvida la creciente importancia de los activos intangibles de una empresa (relaciones con los clientes, habilidades y motivaciones de los empleados...) como fuente principal de ventaja competitiva. 

De ahí surge la necesidad de crear una nueva metodología para medir las actividades de una compañía en términos de su visión y estrategia, proporcionando a los gerentes una mirada global del desempeño del negocio. El CMI es una herramienta de administración de empresas que muestra continuamente cuándo una compañía y sus empleados alcanzan los resultados definidos por el plan estratégico. Adicionalmente, un sistema como el CMI permite detectar las desviaciones del plan estratégico y expresar los objetivos e iniciativas necesarios para reconducir la situación.

Según el libro "The Balanced ScoreCard: Translating Strategy into Action", Harvard Business School Press, Boston, 1996: 

El CMI sugiere que veamos a la organización desde cuatro perspectivas, cada una de las cuales debe responder a una pregunta determinada:
El CMI es por lo tanto un sistema de gestión estratégica de la empresa, que consiste en:

En general, los indicadores financieros están basados en la contabilidad de la compañía, y muestran el "pasado" de la misma. El motivo se debe a que la contabilidad no es inmediata (al emitir un proveedor una factura, la misma no se contabiliza automáticamente), sino que deben efectuarse "cierres" que aseguren la compilación y consistencia de la información. Debido a estas demoras, algunos autores sostienen que dirigir una compañía prestando atención solamente a indicadores financieros es como "conducir a 100 km/h mirando por el espejo retrovisor".

"Este comentario es exagerado pues existe una herramienta llamada presupuesto que se realiza anualmente y se ajusta cada tres meses, presentando estados financieros proyectados con margen de error de 5 a 10%.

Lo que es posible, es utilizar el CMI como una herramienta adicional para ver de mejor forma las estrategias utilizadas en el presupuesto proyectado."

Esta perspectiva abarca el área de las necesidades de los accionistas. Esta parte del BSC se enfoca a los requerimientos de crear valor para el accionista como: las ganancias, rendimiento económico, desarrollo de la compañía y rentabilidad de la misma.

Valor Económico Agregado (EVA), Retorno sobre capital empleado (ROCE), Margen de Operación, Ingresos, Rotación de Activos son algunos indicadores de esta perspectiva.

Algunos indicadores frecuentemente utilizados son: 

Para lograr el desempeño financiero que una empresa desea, es fundamental que posea clientes leales y satisfechos. Con ese objetivo en esta perspectiva se miden las relaciones con los clientes y las expectativas que los mismos tienen sobre los negocios. Además, en esta perspectiva se toman en cuenta los principales elementos que generan valor para los clientes integrándolos en una propuesta de valor, para poder así centrarse en los procesos que para ellos son más importantes y que más los satisfacen.

La Perspectiva de clientes, como su nombre indica, está enfocada a la parte más importante de una empresa: sus clientes, puesto que sin consumidores no existe ningún tipo de mercado. Por consiguiente, se deberán cubrir las necesidades de los compradores entre las que se encuentran los precios, la calidad del producto o servicio, tiempo, función, imagen y relación. Cabe mencionar que todas las perspectivas están unidas entre sí. Esto significa que para cubrir las expectativas de los accionistas también se deben cubrir las de los consumidores para que compren y se genere una ganancia. Algunos indicadores de esta perspectiva son: satisfacción de clientes, desviaciones en acuerdos de servicio, reclamos resueltos sobre el total de reclamos, e incorporación y retención de clientes.

El conocimiento de los clientes y de los procesos que más valor generan es muy importante para lograr que el panorama financiero sea próspero. Sin el estudio de las peculiaridades del mercado al que está enfocada la empresa no podrá existir un desarrollo sostenible en la perspectiva financiera, ya que en gran medida el éxito financiero proviene del aumento de las ventas, situación que es el efecto de clientes que repiten sus compras porque prefieren los productos que la empresa desarrolla teniendo en cuenta sus preferencias.

Una buena manera de medir o saber la perspectiva del cliente es diseñando protocolos básicos de atención y utilizar la metodología de cliente incógnito para la relación del personal en contacto con el cliente (PEC).

Usualmente se consideran cuatro categorías, a saber:

Los instrumentos que usualmente se utilizan para obtener el valor de tales indicadores son entrevistas y encuestas:

Analiza la adecuación de los procesos internos de la empresa de cara a la obtención de la satisfacción del cliente y logro de altos niveles de rendimiento financiero. Para alcanzar este objetivo se propone un análisis de los procesos internos desde una perspectiva de negocio y una predeterminación de los procesos clave a través de la cadena de valor. 

Se distinguen cuatro tipos de procesos:





Indicadores: bases de datos estratégicos, software propio, las patentes y copyright (marcas registradas), entre otras.

En la actualidad -debido a las turbulencias del entorno empresarial, influenciado en la mayoría de los casos por una gran presión competitiva, así como por un auge de la tecnología- es cuando comienza a tener una amplia trascendencia.

El concepto de cuadro de mando deriva del concepto denominado "tableau de bord" en Francia, que traducido de manera literal, vendría a significar algo como tablero de mandos o cuadro de instrumentos.

A partir de los años 80, es cuando el Cuadro de Mando pasa a ser, además de un concepto práctico, una idea académica, ya que hasta entonces el entorno empresarial no sufría grandes variaciones, la tendencia del mismo era estable, las decisiones que se tomaban carecían de un alto nivel de riesgo.

Para entonces, los principios básicos sobre los que se sostenía el Cuadro de Mando ya estaban estructurados, es decir, se fijaban unos fines en la entidad, cada uno de éstos eran llevados a cabo mediante la definición de unas variables clave, y el control era realizado a través de indicadores.

Básicamente, y de manera resumida, podemos destacar tres características fundamentales de los cuadros de mando:


En definitiva, lo importante es establecer un sistema de señales en forma de Cuadro de Mando que nos indique la variación de las magnitudes verdaderamente importantes que debemos vigilar para someter a control la gestión.

A la hora de elaborar los cuadros de mando, muchos son los criterios que se pueden entremezclar, siendo los que a continuación se describen, algunos de los más importantes, para clasificar tales herramientas de apoyo a la toma de decisiones:


Otras clasificaciones:

En la actualidad, no todos los cuadros de mando integral están basados en los principios de Kaplan y Norton, aunque sí influenciados en alguna medida por ellos. Por este motivo, se suele emplear con cierta frecuencia el término dashboard, que refleja algunas características teóricas del cuadro de mando. De forma genérica, un "dashboard" engloba a varias herramientas que muestran información relevante para la empresa a través de una serie de indicadores de rendimiento, también denominados KPIs ("key performance indicators"). Cabe señalar que un "dashboard" puede no ser balanceado, término que evoca al "Balanced" Scorecard, es decir un balance entre indicadores que visualicen en forma transversal la organización o empresa y que quizás para un "dashboard" solo puede buscar y dirigir su mirada a un conjunto focalizado y parcial de indicadores.

Los Cuadros de Mando (CM) son herramientas de control empresarial orientadas a la monitorización de los objetivos de la empresa o de las diferentes áreas de negocio a través de indicadores. En función de la naturaleza de los indicadores estaríamos hablando de Cuadro de Mandos Estratégico (CME) si se trata de indicadores estratégicos u Cuadro de mandos Operativo (CMO) si los indicadores son operativos, es decir, indicadores rutinarios ligados a áreas o departamentos específicos de la empresa (las áreas suelen ser procesos).

La periodicidad de los CMO puede ser diaria, semanal o mensual, y además está focalizado en indicadores que generalmente representan procesos, por lo que su puesta en funcionamiento es más barata y sencilla y suele ser un buen punto de partida para aquellas compañías que intentan evaluar la implantación de un cuadro de mando integral.

El CMO en línea es fundamental en momentos críticos.

Seis serán las etapas propuestas:


En una primera etapa, la empresa debe conocer en qué situación se encuentra, valorar dicha situación y reconocer la información con la que va a poder contar en cada momento o escenario, tanto la del entorno como la que maneja habitualmente.

Esta etapa se encuentra muy ligada con la segunda, en la cual la empresa habrá de definir claramente las funciones que la componen de manera que puedan ser estudiadas las necesidades según los niveles de responsabilidad en cada caso y poder concluir cuáles son las prioridades informativas que se han de cubrir, cometido que se llevará a cabo en la tercera de las etapas.

Por otro lado, en una cuarta etapa se han de señalizar las variables críticas necesarias para controlar cada área funcional. Estas variables son ciertamente distintas en cada caso, ya sea por los valores culturales y humanos que impregnan la filosofía de la empresa en cuestión, o ya sea por el tipo de área que se esté analizando. Lo importante en todo caso, es determinar cuáles son las más importantes en cada caso para que se pueda llevar a cabo un correcto control y un adecuado proceso de toma de decisiones.

Posteriormente, y en la penúltima de nuestras etapas, se ha de encontrar una correspondencia lógica entre el tipo de variable crítica determinada en cada caso, y el ratio, valor, medida, etc., que nos informe de su estado cuando así se estime necesario. De este modo podremos atribuir un correcto control en caos"'. Con base en las relaciones de causa-efecto, se elabora un Mapa estratégico (Si bien la traducción literal de "Strategy Map" es "Mapa de la estrategia") que permite ver ágilmente la evolución de los indicadores y tomar acciones tendientes a modificarlos.

En último lugar, se debe configurar el cuadro de mando en cada área funcional, y en cada nivel de responsabilidad de manera que albergue siempre la información mínima, necesaria y suficiente para poder extraer conclusiones y tomar decisiones acertadas.

Los responsables de cada uno de los cuadros de mando de los diferentes departamentos han de tener en cuenta una serie de aspectos comunes en cuanto a su elaboración. Entre dichos aspectos cabría destacar los siguientes:






De alguna manera, lo que se incorpore en esta herramienta, será aquello con lo que se podrá medir la gestión realizada y, por este motivo, es muy importante establecer en cada caso qué es lo que hay que controlar y cómo hacerlo. En general, el Cuadro de Mando debe tener cuatro partes bien diferenciadas:





No se deben perder de vista los objetivos elementales que se pretenden alcanzar mediante el Cuadro de Mando, ya que, sin fines a alcanzar, difícilmente se puede entender la creación de ciertos informes. Entre dichos objetivos podemos considerar que:








Los principales elementos que pueden hacer que el Cuadro de Mando muestre notables diferencias con respecto a otras herramientas contables y de gestión son:


En relación con el tipo de información utilizada, el Cuadro de Mando, aparte de reunir información de similares características que la empleada en las distintas disciplinas de naturaleza contable, es decir, financiera, debe contener información de carácter no financiero. Ya desde su presentación como una herramienta útil de gestión, el Cuadro de Mando se destacaba por su total flexibilidad para recoger tal información.

Otro aspecto a destacar, es la relación mutua que ha de existir entre el Cuadro de Mando y el perfil de la persona a quien va destinado. Precisamente, las necesidades de cada directivo, han de marcar la pauta que caracterice y haga idónea a esta herramienta en cada caso y situación, sobre todo con respecto al nivel de mayor responsabilidad de la jerarquía actual de la empresa, debido a que se precisa un esfuerzo mucho mayor de generalidad y síntesis.

Un rasgo más del Cuadro de Mando es la solución de problemas mediante acciones rápidas. Cuando se incorporan indicadores de carácter cualitativo al Cuadro de Mando, en cierto modo, éstos están más cerca de la acción que los propios indicadores o resultados financieros. Asimismo, estos indicadores nominales nos dan un avance en cuanto a qué resultados están por alcanzarse.

El último de los rasgos que diferenciarían al Cuadro de Mando es el hecho de utilizar informaciones sencillas y poco voluminosas. Las disciplinas y herramientas contables habituales precisan una mayor dedicación de tiempo de análisis y de realización y, al momento de la toma de decisiones siempre necesita de otros aspectos que en un principio no formaban parte de su marco de acción.

El Cuadro de Mando se orienta hacia la reducción y síntesis de conceptos, es una herramienta que, junto con el apoyo de las nuevas tecnologías de la información y comunicación, puede y debe ofrecer una información sencilla, resumida y eficaz para la toma de decisiones.

CMI vs ISO. El Cuadro de mando integral es compatible con otros modelos de gestión de la Calidad, como la ISO9001, ISO TS 16949, etc. Si bien, estas normas de calidad, se centran más bien en el control de proceso interno y no establecen las relaciones de causa efecto entre las distintas perspectivas desde la perspectiva financiera hasta la perspectiva de aprendizaje y desarrollo. El CMI es perfectamente compatible con estos modelos de calidad y en los sectores muy desarrollados, es común compaginar distintos modelos: Lean, ISO y CMI.

En relación a las principales variables a tener en cuenta en la Dirección General, Direcciones Funcionales y Subdirecciones Funcionales, se concluye que no existe una única fórmula para todas las empresas, sino que para cada tipo de organización habrá que tomar unas variables determinadas con las que llevar a cabo la medición de la gestión.

Es importante tener en cuenta que el contenido de cualquier Cuadro de Mando, no se reduce tan sólo a cifras o números, ha de ser un contenido muy concreto para cada departamento o para cada responsable. De igual manera, se ha de tener presente que la información que se maneja en un Cuadro de Mando determinado puede ser válida para otro.

Con respecto a los indicadores, éstos son elementos objetivos que describen situaciones específicas, y que tratan de medir de alguna manera las variables propuestas en cada caso. Al analizar los indicadores necesarios, se establece una distinción básica entre los financieros y no financieros.

El Cuadro de Mando se nutre de todo este tipo de indicadores, tiene en cuenta los aspectos prospectivo y retrospectivo, configurando un punto de vista global mucho más completo y eficaz. Su función es conjugar una serie de elementos para suministrar una visión de conjunto y ofrecer soluciones en cada caso.

La mayoría de las técnicas tienen como elemento común, el mostrar las relaciones que existen entre las categorías de las variables más que entre las propias variables. El Cuadro de Mando, no debe profundizar tanto en estas técnicas, sino en la obtención de la información mínima necesaria, para que, junto a las variables de carácter monetario, pueda llevar a cabo la ya mencionada gestión globalizada.


De modo previo, al abordar la presentación del Cuadro de Mando, se debe resaltar una cuestión que es de gran importancia en relación a su contenido. Se trata del aspecto cualitativo de esta herramienta, ya que hasta el momento no se le ha prestado la importancia que se merece y, sobre todo, porque existen numerosos aspectos como el factor humano, cuyo rendimiento queda determinado por el entorno que le rodea en la propia organización, y estas cuestiones rara vez se toman en cuenta.

La empresa -desde una perspectiva meramente global- constituye un conjunto de vínculos más o menos establecidos y de recursos compartidos con un fin común. Asimismo, se puede señalar que la empresa en sí representa un conjunto de subsistemas de información, claramente definidos y normalizados.

Información y Fluidez. La información que puede obtener y utilizar la empresa, según cuál sea su naturaleza, puede ser válida para unos u otros Cuadros de Mando. La información que contienen los CMI pueden dividirse en dos grandes áreas: externa e interna.

Cabe señalar que con más frecuencia la preocupación de las empresas por contar con sistemas organizados, ágiles y fluidos de comunicación entre todos los niveles de responsabilidad va en aumento debido a las crecientes exigencias del mercado en materia de actualización y tecnología que les permitan posicionarse y perpetuar una posición en dicho mercado. Dicha comunicación se da a través de los canales que se establezcan y hagan posible que el personal, por medio del conocimiento claro de los temas que les afectan, pueda sentirse más involucrados en sus tareas diarias.

Relación causa-efecto. Entre los diversos objetivos de una Compañía, pueden establecerse relaciones de causa-efecto. Esto es, hallar una relación entre la variación de las métricas de un objetivo y las de otro a lo largo del tiempo. Esto permite predecir cómo se comportarán algunas métricas en el futuro a partir del análisis de otras en el momento actual; y tomar alguna decisión que permita cambiar el rumbo de los acontecimientos.





</doc>
<doc id="6771" url="https://es.wikipedia.org/wiki?curid=6771" title="Control de gestión">
Control de gestión

El control de gestión es el proceso administrativo que sirve para evaluar el grado de cumplimiento de los objetivos organizacionales previstos por la dirección o gobierno corporativo.

Existen diferencias importantes entre las concepciones clásica y moderna de control de gestión. La primera es aquella que incluye únicamente al control operativo y que lo desarrolla a través de un sistema de información relacionado con la contabilidad de costos, mientras que la segunda integra muchos más elementos y contempla una continua interacción entre todos ellos. El nuevo concepto de control de gestión centra su atención por igual en la planificación y en el control, y precisa de una orientación estratégica que dote de sentido sus aspectos más operativos.

El SCG cuenta con el diagnóstico o análisis para entender las causas raíces que condicionan el comportamiento de los sistemas físicos, permite establecer los vínculos funcionales que ligan las variables técnicas-organizativas-sociales con el resultado económico de la empresa y es el punto de partida para la mejora de los estándares; mediante la planificación orienta las acciones en correspondencia con las estrategias trazadas, hacia mejores resultados; y, finalmente, cuenta con el control para saber si los resultados satisfacen los objetivos trazados.
El control de gestión, considera como elementos a evaluar, no solo aspectos cuantitativos sino también cualitativos, además, utiliza la visión interna y externa de la organización, para lo cual asigna un valor agregado a la cultura organizacional .
Se puede señalar como diferencia principal entre Control de Gestión y Control Interno que el primero evalúa los tres niveles de las organizaciones: nivel estratégico, nivel táctico y nivel operativo, y el segundo, se limita solo a los dos últimos niveles.

Taylor (1895) fue uno de los iniciadores del CG industrial, introdujo la contabilidad analítica, el cronometraje de los tiempos de mano de obra directa, los estándares, la asignación de los costos indirectos, la remuneración por rendimientos. Brown (1907) estableció la fórmula de la rentabilidad del capital. Todavía hoy se observan muchos ejemplos en las empresas el CG gira en torno al control de la eficiencia interna de la empresa, centrando su atención en los recursos que consume, en el beneficio inmediato y en la información financiera exterior.

En la segunda mitad del siglo XX han ocurrido cambios sustanciales del entorno, el cual ha pasado de estable con reglas de juego fijas, a turbulento y muy competitivo. Estos cambios de entorno han desencadenado en las empresas un gran número de cambios internos, en variables tales como la orientación hacia el cliente, el desarrollo tecnológico y la innovación, el papel rector de la dirección estratégica, los enfoques de calidad, el rol de los recursos humanos en la organización, la gestión de la información y otros. El éxito empresarial por lo tanto, exige una continua adaptación de la empresa a su entorno y la competitividad se convierte en el criterio económico por excelencia para orientar y evaluar el desempeño dentro y fuera de la empresa.

Según García (1975), el control de gestión (CG) es ante todo un método, un medio para conducir con orden el pensamiento y la acción, lo primero es prever, establecer un pronóstico sobre el cual fijar objetivos y definir un programa de acción. Lo segundo es controlar, comparando las realizaciones con las previsiones al mismo tiempo que se ponen todos los medios para compensar las diferencias constatadas.

Blanco (1984) plantea que la moderna filosofía del CG presenta la función de control como el proceso mediante el cual los directivos se aseguran de la obtención de recursos y del empleo eficaz y eficiente de los mismos en el cumplimiento de los objetivos de la empresa.

La gestión es una mezcla de decisiones locales con objetivos globales de la compañía, según lo ve Goldratt (1990), desde su teoría sobre gestión de las limitaciones (TOC), precisando que el control es una parte del sistema de información que responde a una de las preguntas gerenciales más perturbadoras: ¿cómo medir objetiva y constructivamente el desempeño local pasado?

Según Huge Jordan (1995), el CG es un instrumento de la gestión que aporta una ayuda a la decisión y sus útiles de dirección van a permitir a los directores alcanzar los objetivos; es una función descentralizada y coordinada para la planificación de objetivos, acompañada de un plan de acción y la verificación de que los objetivos han sido alcanzados.

A partir de 1990, aparece el término "controlling" (4) en Alemania, España y Estados Unidos. El salto cualitativo no está en la definición misma de control de gestión, sino en lo que enfatiza ahora la literatura con este término: las nuevas características que debe presentar el control de gestión ante el cambio radical que está operándose en los modelos de perfeccionamiento empresarial. Kupper (1992) lo ve como un medio de coordinación de las numerosas partes del sistema de management. Pacher-Theinburg (1992) subraya la significación del controlling por la integración alcanzada entre las funciones de planificación y control. García Echevarría (1994) resalta tanto su dimensión estratégica y global de la empresa como su dimensión específica en la función que se dirige. El controlling, como el control de gestión, orientado más hacia el futuro que al pasado y donde se ve fundamentalmente a la empresa desde afuera de sí misma, integrada con el cliente y la competencia.

Si continuáramos citando autores, se comprobaría que la definición de CG no es única, varía con cada autor y con el transcurso de los años, ya que el constante cambio del entorno empresarial conduce a una evolución en la forma de pensar y actuar, así como en los métodos y herramientas empleadas para dirigir una organización. 

Repasando diferentes definiciones sobre control de gestión se observa que:





Aquí se considera que el CG debe ofrecer información homogénea en la medida que asciende en la pirámide de información para ofrecer información agregada sobre estados o resultados pero cuando se avanza en sentido contrario, lo que se maneja es información sobre decisiones, algunas tan directas y heterogéneas como lo son las relativas a los procesos sobre los que actúan. En estas condiciones, la problemática a resolver por el CG es servir de puente entre los resultados económicos y las decisiones que se toman sobre los procesos físicos de la empresa poniendo de manifiesto sus vínculos funcionales.






La Editorial Profit www.profiteditorial.com cuenta con una serie de libros que enumeran varios componentes del Sistema de Control de Gestión. Considero que es importante mencionar algunos de ellos para comprender cómo es que en conjunto representan un tablero de indicadores que muestra cual es la situación de una empresa con respecto a su estrategia y recursos:
Por mencionar un ejemplo, Rieckhof menciona la importancia de medir y mejorar los recursos naturales. El impacto ambiental de las empresas es uno más de los componentes del Control de Gestión;

""Challenged by decreasing natural resources, corporations need to significantly improve their resource efficiency. For the purpose of a more efficient and sustainable use of natural resources, the internationally standardized approach of MFCA is a promising tool. However, goals such as resource efficiency can only be achieved if corporations commit themselves to these targets on a strategic level and transfer them to all corporate levels by using MCS. Thus, MFCA requires an increased interrelation with MCS, which can drive corporate strategy toward resource efficiency"."


Los sistemas de control se basan en una serie de principios básicos, los cuales permiten alcanzar los objetivos propuestos por todo sistema de control. A saber son:




</doc>
<doc id="6782" url="https://es.wikipedia.org/wiki?curid=6782" title="Buenas prácticas">
Buenas prácticas

Por buenas o mejores prácticas se entiende un conjunto coherente de acciones que han rendido buen o incluso excelente servicio en un determinado contexto y que se espera que, en contextos similares, rindan similares resultados. Estas dependen de las épocas, de las modas y hasta de la empresa consultora o del autor que las preconiza. No es de extrañar que algunas sean incluso contradictorias entre ellas.

Las expresiones "buenas"/"mejores prácticas" son traducciones demasiado literales de la expresión inglesa best practices. En respuesta a una consulta a la Real Academia de la Lengua, esta recomienda "el empleo de otros sintagmas alternativos, dependiendo del contexto, como mejores soluciones, mejores métodos, procedimientos más adecuados, prácticas recomendables, o similares."

Algunos consideran las mejores prácticas como un conjunto heterogéneo de términos o teorías, unas nuevas e innovadoras, y otras que simplemente renombran prácticas administrativas que ya se utilizaban en la práctica profesional pero que nadie había presentado como propias. Entre estas teorías podemos mencionar: calidad total, justo a tiempo, estudio de referencia, reingeniería, externalización, redimensionamiento, gestión basada en actividades, gestión basada en el valor, gestión por objetivos, destrucción creativa, etc.

Otros, en cambio, reconocen que las mejores prácticas son sólo un buen comienzo, mejor que una hoja en blanco, pero que no reemplazan al sentido común y a la reflexión y que, mientras se usen de manera racional y coherente, pueden acelerar la puesta en servicio de mejoras en los procesos de las organizaciones.

Sus detractores dicen que la mayoría de estos términos son empleados por empresas consultoras como Accenture, McKinsey, Boston Consulting Group, Price Watherhouse, Deloitte & Touche, Stern Stewart. Ellas los comercializan y ellas mismas se encargan de convencer a las empresas para que los pongan en práctica. Muchas de estas teorías, afirman los detractores, resultan ser una moda pasajera que, impulsada por las grandes empresas consultoras, toma fuerza pero después de cierto tiempo cae en desuso tras quedar en evidencia sus limitaciones, o bien ante la aparición de una nueva moda.

Algunas de esas aseveraciones son en cierto grado ciertas. Sin embargo, esos mismos detractores reconocen que no quiere decir esto que todas sean un mero producto de la comercialización de las consultoras. Aplicadas con sentido común, pueden aportar soluciones a problemas reales.

Existe la Agencia Estatal de Evaluación de las Políticas Públicas y la Calidad de los Servicios , que es el órgano encargado, en este ámbito, de elaborar libros blancos conteniendo las mejores prácticas.



</doc>
<doc id="6783" url="https://es.wikipedia.org/wiki?curid=6783" title="Castúo">
Castúo

Castúo o dialecto extremeño es el nombre de las variedades del idioma español habladas en Extremadura.

La denominación "castúo" fue acuñada por el poeta extremeño Luis Chamizo Trigueros, natural de Guareña, provincia de Badajoz, cuando en 1921 publicó su libro de poemas "El Miajón de los Castúos" en el que intentaba reflejar el habla rural que definió como "castizo, mantenedor de la casta de labradores que cultivaron sus propias tierras". El nombre de "castúo", aunque además de ser un término creado modernamente por el poeta Luis Chamizo en la década de 1920, dicho término puede llevar a confusión, pues también se denomina así a las hablas castellanas meridionales de tránsito con el leonés habladas en el resto de Extremadura y es precisamente para una obra escrita en un habla de esta índole para lo que fue creado el término originalmente, aunque más tarde se popularizara para referirse a todas las hablas de Extremadura en general. 

Hoy se diferencia el castú como el dialecto castellano hablado en Extremadura y el estremeñu para la variedad idiomática perteneciente al diasistema lingüístico astur-leonés, por lo que no se debe confundir, aunque castúo es una parabra que ha llegado a significar a tenor de sus acepciones:


Con el tiempo la denominación de castúo se ha hecho popular para denominar las hablas de Extremadura, tanto las que conforman el altoextremeño, como las que siempre fueron español meridional de ligera influencia leonesa, el medioextremeño y el bajoextremeño (en el centro, oriente o sur de la Comunidad extremeña), como es el caso de la propia habla en que escribió Luis Chamizo.

Las hablas bajoextremeñas y medioextremeñas comprenden la mayor parte de la provincia de Badajoz y el centro, sur y este de la provincia de Cáceres. Como habla castellana meridional de tránsito, se caracterizaría por compartir con altoextremeño algunas lexicalizaciones de conservación del grupo "mb" latino, algunas expresiones de genitivo partitivo del estilo "unos pocos de", el diminutivo en -ino (que no -inu) y sobre todo los rasgos comunes con el andaluz occidental distinguidor (ni ceceante ni seseante), incluyendo la aspiración de efe inicial latina y ciertos cambios de género en algunas palabras ("la caló"). La neutralización -l/-r en posición implosiva, si existe, siempre tiende a la "r" en bajoextremeño ("arto" en lugar de "alto"). La erre y la ele finales habitualmente se omiten (como en andaluz occidental o como en el habla altoextremeña de buena parte de las Hurdes: "superió", "comé", "fatá", "papé"). Existe cierto léxico particular característico, que tiende a perderse en algunos casos entre las nuevas generaciones. Por ejemplo sigue siendo frecuente el uso de la palabra "guarro" o "gorrino" referida al cerdo en tanto que animal o la expresión "una mijina" (un poquito), que puede reforzarse convirtiéndose en "una mijirrinina". En algunas ocasiones este léxico es coincidente con el altoextremeño ("barruntar" en el sentido de "percibir un ruido", "privá" por "boñiga", "afechar" en lugar de "cerrar con llave"...etc). El seseo sólo aparece en algunas localidades cercanas a Portugal y en Fuente del Maestre. Existen pequeños islotes de uso esporádico del artículo con el posesivo ("la mi cama") también en la provincia de Badajoz, incluso en lugares del extremo sur de ésta, aunque ya en franca decadencia.

Desde las obras de José María Gabriel y Galán y Luis Chamizo, el castúo o extremeño hablado tradicionalmente se ha plasmado en textos escritos, configurando una literatura vernácula; donde cada vez son más los nuevos poetas y escritores que utilizan esta lengua como vehículo de cultura.

Entre los nuevos libros escritos en este habla destacan:



 


</doc>
<doc id="6784" url="https://es.wikipedia.org/wiki?curid=6784" title="Almacén de datos">
Almacén de datos

En el contexto de la informática, un almacén de datos (del inglés data warehouse) es una colección de datos orientada a un determinado ámbito (empresa, organización, etc.), integrado, no volátil y variable en el tiempo, que ayuda a la toma de decisiones en la entidad en la que se utiliza. Se lo usa por reportajes y análisis de datos y se considera un componente meollo de la inteligencia empresarial. Se trata, sobre todo, de un expediente completo de una organización, más allá de la información transaccional y operacional, almacenado en una base de datos diseñada para favorecer el análisis y la divulgación eficiente de datos (especialmente OLAP, "procesamiento analítico en línea"). El almacenamiento de los datos no debe usarse con datos de uso actual. Los almacenes de datos contienen a menudo grandes cantidades de información que se subdividen a veces en unidades lógicas más pequeñas dependiendo del subsistema de la entidad del que procedan o para el que sean necesario.

Bill Inmon fue uno de los primeros autores en escribir sobre el tema de los almacenes de datos, define un data warehouse (almacén de datos) en términos de las características del repositorio de datos:

Inmon defiende una metodología descendente (top-down) a la hora de diseñar un almacén de datos, ya que de esta forma se considerarán mejor todos los datos corporativos. En esta metodología los Data marts se crearán después de haber terminado el data warehouse completo de la organización.

Ralph Kimball es otro conocido autor en el tema de los data warehouse, define un almacén de datos como: "Es una almacén de datos que extrae, limpia, conforma y entrega una fuente de datos dimensional para la consulta y el análisis". También fue Kimball quien determinó que un data warehouse no era más que: "la unión de todos los Data marts de una entidad". Defiende por tanto una metodología ascendente (bottom-up) a la hora de diseñar un almacén de datos.

Las definiciones anteriores se centran en los datos en sí mismos. Sin embargo, los medios para obtener esos datos, para extraerlos, transformarlos y cargarlos, las técnicas para analizarlos y generar información, así como las diferentes formas para realizar la gestión de datos son componentes esenciales de un almacén de datos. Muchas referencias a un almacén de datos utilizan esta definición más amplia. Por lo tanto, en esta definición se incluyen herramientas para extraer, transformar y cargar datos, herramientas para el análisis (inteligencia empresarial) y herramientas para gestionar y recuperar los metadatos.

En un almacén de datos lo que se quiere es contener datos que son necesarios o útiles para una organización, es decir, que se utiliza como un repositorio de datos para posteriormente transformarlos en información útil para el usuario. Un almacén de datos debe entregar la información correcta a la gente indicada en el momento óptimo y en el formato adecuado. El almacén de datos da respuesta a las necesidades de usuarios expertos, utilizando Sistemas de Soporte a Decisiones (DSS), Sistemas de información ejecutiva (EIS) o herramientas para hacer consultas o informes. Los usuarios finales pueden hacer fácilmente consultas sobre sus almacenes de datos sin tocar o afectar la operación del sistema.

En el funcionamiento de un almacén de datos son muy importantes las siguientes ideas:


Periódicamente, se importan datos al almacén de datos de los distintos sistemas de planeamiento de recursos de la entidad (ERP) y de otros sistemas de software relacionados con el negocio para la transformación posterior. Es práctica común normalizar los datos antes de combinarlos en el almacén de datos mediante herramientas de extracción, transformación y carga (ETL). Estas herramientas leen los datos primarios (a menudo bases de datos OLTP de un negocio), realizan el proceso de transformación al almacén de datos (filtración, adaptación, cambios de formato, etc.) y escriben en el almacén.

Los Data marts son subconjuntos de datos de un data warehouse para áreas específicas.

Entre las características de un data mart destacan:

Los cubos de información o cubos OLAP funcionan como los cubos de rompecabezas en los juegos, en el juego se trata de armar los colores y en el data warehouse se trata de organizar los datos por tablas o relaciones; los primeros (el juego) tienen 3 dimensiones, los cubos OLAP tienen un número indefinido de dimensiones, razón por la cual también reciben el nombre de hipercubos. Un cubo OLAP contendrá datos de una determinada variable que se desea analizar, proporcionando una vista lógica de los datos provistos por el sistema de información hacia el data warehouse, esta vista estará dispuesta según unas dimensiones y podrá contener información calculada. El análisis de los datos está basado en las dimensiones del hipercubo, por lo tanto, se trata de un análisis multidimensional.

A la información de un cubo puede acceder el ejecutivo mediante "tablas dinámicas" en una hoja de cálculo o a través de programas personalizados. Las tablas dinámicas le permiten manipular las vistas (cruces, filtrados, organización, totales) de la información con mucha facilidad. Las diferentes operaciones que se pueden realizar con cubos de información se producen con mucha rapidez. Llevando estos conceptos a un data warehouse, éste es una colección de datos que está formada por «dimensiones» y «variables», entendiendo como dimensiones a aquellos elementos que participan en el análisis y variables a los valores que se desean analizar.

Las dimensiones de un cubo son atributos relativos a las variables, son las perspectivas de análisis de las variables (forman parte de la tabla de dimensiones). Son catálogos de información complementaria necesaria para la presentación de los datos a los usuarios, como por ejemplo: descripciones, nombres, zonas, rangos de tiempo, etc. Es decir, la información general complementaria a cada uno de los registros de la tabla de hechos.

También llamadas “indicadores de gestión”, son los datos que están siendo analizados. Forman parte de la tabla de hechos. Más formalmente, las variables representan algún aspecto cuantificable o medible de los objetos o eventos a analizar. Normalmente, las variables son representadas por valores detallados y numéricos para cada instancia del objeto o evento medido. En forma contraria, las dimensiones son atributos relativos a las variables, y son utilizadas para indexar, ordenar, agrupar o abreviar los valores de las mismas. Las dimensiones poseen una granularidad menor, tomando como valores un conjunto de elementos menor que el de las variables; ejemplos de dimensiones podrían ser: “productos”, “localidades” (o zonas), “el tiempo” (medido en días, horas, semanas, etc.), ...

Ejemplos de variables podrían ser:
Ejemplos de dimensiones podrían ser:
Según lo anterior, podríamos construir un cubo de información sobre el índice de ventas (variable a estudiar) en función del producto vendido, la provincia, el mes del año y si el cliente está casado o soltero (dimensiones). Tendríamos un cubo de 4 dimensiones.

Uno de los componentes más importantes de la arquitectura de un almacén de datos son los metadatos. Se define comúnmente como "datos acerca de los datos", en el sentido de que se trata de datos que describen cuál es la estructura de los datos que se van a almacenar y cómo se relacionan.

El metadato documenta, entre otras cosas, qué tablas existen en una base de datos, qué columnas posee cada una de las tablas y qué tipo de datos se pueden almacenar. Los datos son de interés para el usuario final, el metadato es de interés para los programas que tienen que manejar estos datos. Sin embargo, el rol que cumple el metadato en un entorno de almacén de datos es muy diferente al rol que cumple en los ambientes operacionales. En el ámbito de los data warehouse el metadato juega un papel fundamental, su función consiste en recoger todas las definiciones de la organización y el concepto de los datos en el almacén de datos, debe contener toda la información concerniente a:

Los procesos de Extract, transform and load (ETL) son importantes ya que son la forma en que los datos se guardan en un almacén de datos (o en cualquier base de datos). Implican las siguientes operaciones:


Middleware es un término genérico que se utiliza para referirse a todo tipo de software de conectividad que ofrece servicios u operaciones que hacen posible el funcionamiento de aplicaciones distribuidas sobre plataformas heterogéneas. Estos servicios funcionan como una capa de abstracción de software distribuida, que se sitúa entre las capas de aplicaciones y las capas inferiores (sistema operativo y red). El "middleware" puede verse como una capa API, que sirve como base a los programadores para que puedan desarrollar aplicaciones que trabajen en diferentes entornos sin preocuparse de los protocolos de red y comunicaciones en que se ejecutarán. De esta manera se ofrece una mejor relación costo/rendimiento que pasa por el desarrollo de aplicaciones más complejas, en
menos tiempo.

La función del middleware en el contexto de los data warehouse es la de asegurar la conectividad entre todos los componentes de la arquitectura de un almacén de datos.

Para construir un Data Warehouse se necesitan herramientas para ayudar a la migración y a la transformación de los datos hacia el almacén. Una vez construido, se requieren medios para manejar grandes volúmenes de información. Se diseña su arquitectura dependiendo de la estructura interna de los datos del almacén y especialmente del tipo de consultas a realizar. Con este criterio los datos deben ser repartidos entre numerosos data marts. Para abordar un proyecto de data warehouse es necesario hacer un estudio de algunos temas generales de la organización o empresa, los cuales se describen a continuación:









Almacén de datos espacial es una colección de datos orientados al tema, integrados, no volátiles, variantes en el tiempo y que añaden la geografía de los datos, para la toma de decisiones. Sin embargo la componente geográfica no es un dato agregado, sino que es una dimensión o variable en la tecnología de la información, de tal manera que permita modelar todo el negocio como un ente holístico, y que a través de herramientas de procesamiento analítico en línea (OLAP), no solamente se posea un alto desempeño en consultas multidimensionales sino que adicionalmente se puedan visualizar espacialmente los resultados.

El almacén de datos espacial forma parte de un extensivo "Sistema de Información Geográfica para la toma de decisiones", éste al igual que los SIG, permiten que un gran número de usuarios accedan a información integrada, a diferencia de un simple almacén de datos que está orientado al tema, el "Data warehouse" espacial adicionalmente es Geo-Relacional, es decir que en estructuras relacionales combina e integra los datos espaciales con los datos descriptivos. Actualmente es geo-objetos, esto es que los elementos geográficos se manifiestan como objetos con todas sus propiedades y comportamientos, y que adicionalmente están almacenados en una única base de datos Objeto-Relacional.

Los Data Warehouse Espaciales son aplicaciones basadas en un alto desempeño de las bases de datos, que utilizan arquitecturas Cliente-Servidor para integrar diversos datos en tiempo real. Mientras los almacenes de datos trabajan con muchos tipos y dimensiones de datos, muchos de los cuales no referencian ubicación espacial, a pesar de poseerla intrínsecamente, y sabiendo que un 80% de los datos poseen representación y ubicación en el espacio, en los "Data warehouse" espaciales, la variable geográfica desempeña un papel importante en la base de información para la construcción del análisis, y de igual manera que para un "Data warehouse", la variable tiempo es imprescindible en los análisis, para los Data warehouse espaciales la variable geográfica debe ser almacenada directamente en ella.

Hay muchas ventajas por las que es recomendable usar un almacén de datos. Algunas de ellas son:


Utilizar almacenes de datos también plantea algunos inconvenientes, algunos de ellos son:





</doc>
<doc id="6786" url="https://es.wikipedia.org/wiki?curid=6786" title="Chlorobia">
Chlorobia

Las bacterias verdes del azufre o Chlorobia constituyen un pequeño grupo de bacterias del filo Chlorobi que realizan la fotosíntesis anoxigénica. Son fotolitoautótrofas obligadas que usan sulfuro de hidrógeno (HS) o azufre (S) como donantes de electrones (por comparación, las plantas durante la fotosíntesis usan agua como donante de electrones y producen oxígeno). Las estructuras donde se almacenan los pigmentos fotosintéticos están unidas a la membrana y se conocen como clorosomas o vesículas clorobiales. Estos clorosomas contienen bacterioclorofila "c", "d" y "e".

Estas bacterias se encuentran en las zonas ricas en azufre y anaerobias de los lagos. Algunas de estas bacterias contienen vesículas que les permiten ajustar la profundidad para conseguir una cantidad óptima de luz y HS ya que estas bacterias son generalmente inmóviles (se conoce una especie que tiene un flagelo). Otras especies no tienen vesículas y se las encuentra en el fango rico en azufre en el fondo de los lagos y lagunas. Estas bacterias son bien diversas morfológicamente y se pueden presentar como bacilos, cocos y vibrios. Algunas crecen solas, otras en cadenas y pueden ser de color verde grama o marrón chocolate.

Una especie de bacteria verde del azufre ha sido encontrada viviendo en una fumarola de la costa de México a una profundidad de 2.500 metros bajo la superficie del Océano Pacífico. A esta profundidad, las bacterias, denominadas GSB1, viven del débil resplandor del respiradero termal puesto que ninguna luz del sol puede penetrar a tal profundidad. 

Las bacterias verdes del azufre se clasifican en la familia Chlorobiaceae que, al no estar estrechamente emparentada con ninguna otra, se incluye en su propio filo, Chlorobi. El grupo más próximo es Bacteroidetes.



</doc>
<doc id="6787" url="https://es.wikipedia.org/wiki?curid=6787" title="Metafísica">
Metafísica

La metafísica (del latín "metaphysica", y este del griego μετὰ [τὰ] φυσικά, «más allá de [la] naturaleza») es la rama de la filosofía que estudia la naturaleza, estructura, componentes y principios fundamentales de la realidad. Esto incluye la clarificación e investigación de algunas de las nociones fundamentales con las que entendemos el mundo, como ser, entidad, existencia, objeto, propiedad, relación, causalidad, tiempo y espacio.

Antes del advenimiento de la ciencia moderna, muchos de los problemas que hoy pertenecen a las ciencias naturales eran estudiados por la metafísica bajo el título de filosofía natural. Hoy la metafísica estudia aspectos de la realidad que son inaccesibles a la investigación empírica. Según Immanuel Kant, las afirmaciones metafísicas son juicios sintéticos a priori, que por principio escapan a toda experiencia sensible.

Aristóteles designó la metafísica como «primera filosofía». En la química se asume la existencia de la materia y en la biología la existencia de la vida, pero ninguna de las dos ciencias define la materia o la vida; solo la metafísica suministra estas definiciones básicas.

La ontología es la parte de la metafísica que se ocupa de investigar qué entidades existen y cuáles no, más allá de las apariencias. La metafísica tiene dos temas principales: el primero es la ontología, que en palabras de Aristóteles viene a ser la ciencia que estudia el ser en cuanto tal. El segundo es la teleología, que estudia los fines como causa última de la realidad. Existe, sin embargo, un debate que sigue aún hoy sobre la definición del objeto de estudio de la metafísica, y sobre si sus enunciados tienen propiedades cognitivas.

A lo largo de los siglos, muchos filósofos han sostenido de alguna manera u otra, que la metafísica es imposible. Esta tesis tiene una versión fuerte y una versión débil. La versión fuerte es que todas las afirmaciones metafísicas carecen de sentido o significado. Esto depende por supuesto de una teoría del significado. Ludwig Wittgenstein y los positivistas lógicos fueron defensores explícitos de esta posición. Por otra parte, la versión débil es que si bien las afirmaciones metafísicas poseen significado, es imposible saber cuáles son verdaderas y cuáles falsas, pues esto va más allá de las capacidades cognitivas del hombre. Esta posición es la que sostuvieron, por ejemplo, David Hume e Immanuel Kant. Por otra parte, algunos filósofos han sostenido que el ser humano tiene una predisposición natural hacia la metafísica. Kant la calificó de «necesidad inevitable», y Arthur Schopenhauer incluso definió al ser humano como «animal metafísico».

La palabra «metafísica» deriva del griego "μετὰ φύσις", que significa «más allá de la naturaleza o más allá de la física», proviene del título puesto por Andrónico de Rodas (Siglo I a. C.) a una colección de escritos de Aristóteles. Esto no implica que la metafísica haya nacido con Aristóteles, sino que es de hecho más antigua, dado que hay casos de pensamiento metafísico en los filósofos presocráticos. Platón estudió en diversos diálogos lo que es el ser, con lo que preparó el terreno a Aristóteles de Estagira, que elaboró lo que él llamaba una «filosofía primera», cuyo principal objetivo era el estudio del ser en cuanto tal, de sus atributos y sus causas.

El término «metafísica» proviene de una obra de Aristóteles compuesta por catorce volúmenes (rollos de papiro), independientes entre sí, que se ocupan de diversos temas generales de la filosofía. Estos libros son de carácter esotérico, es decir, Aristóteles nunca los concibió para la publicación. Por el contrario, son un conjunto de apuntes o notas personales sobre temas que pudo haber tratado en clases o en otros libros sistemáticos.

El peripatético Andrónico de Rodas al sacar la primera edición de las obras de Aristóteles ordenó estos libros detrás de los ocho libros sobre física ("μετὰ [τὰ] φυσικά"). De allí surgió el concepto de «metafísica», que en realidad significa: «aquello que en el estante está después de la física», pero que también de manera didáctica significa: «aquello que sigue a las explicaciones sobre la naturaleza» o «lo que viene después de la física», entendiendo «física» en su acepción antigua que se refería al estudio de la "φύσις", es decir, de la naturaleza y sus fenómenos, no limitados al plano material necesariamente.

En la Antigüedad la palabra «metafísica» no denotaba una disciplina particular concerniente al interior de la filosofía, sino el compendio de rollos de Aristóteles ya mencionado. Sólo es a partir del siglo XIII que la metafísica pasa a ser una disciplina filosófica especial que tiene como objeto el ente en cuanto ente. Es hacia ese siglo cuando el conocimiento de las teorías aristotélicas se comienza a conocer en el Occidente latino gracias al influjo de pensadores musulmanes como el persa Avicena y el andalusí Averroes.

A partir de entonces la metafísica pasa a ser la más alta disciplina filosófica, llegando así hasta la Edad Moderna. Con el tiempo la palabra «metafísica» adquirió el significado de «difícil» o «sutil» y en algunas circunstancias se utiliza con un carácter peyorativo, pasando a significar «especulativo, dudoso o no científico». En este sentido, también la metafísica es considerada como un modo de reflexionar con demasiada sutileza en cualquier materia que discurriese entre lo oscuro y difícil de comprender.

En la Metafísica de Aristóteles se encuentran diversas definiciones de la metafísica como ciencia. La metafísica considerada como «aiteología» es la ciencia de las causas supremas (A, 1). Como ontología es la ciencia del ente en cuanto ente (G, 1). Como teología es la ciencia de las cosas divinas (E, 1) y como «useología» es la ciencia de la sustancia (Z, 1). A través de la historia las posiciones en cuanto a estas definiciones han sido diversas. De hecho, algunos consideran que en la Metafísica de Aristóteles se encuentran cuatro metafísicas distintas; mientras que otros piensan que las cuatro definiciones se integran para formar una sola metafísica. La metafísica encuentra su unidad de la siguiente manera: la ontología y la useología poseen universalidad de predicación, mientras que la ontología y la useología son universales por causalidad. De esta forma, el "subiectum" de la metafísica sería en el ente en cuanto ente, ahora bien el ente se dice primariamente de la sustancia, por ello el "subiectum" integra las ciencias universales por predicación. Los principios de la metafísica provienen de las ciencias universales por causalidad.

Para Immanuel Kant, «La metafísica es un conocimiento especulativo de la razón, enteramente aislado, que se alza por encima de las enseñanzas de la experiencia mediante meros conceptos (no como la matemática, mediante aplicación de los mismos a la intuición), y en donde, por lo tanto, la razón debe ser su propio discípulo.»

La Real Academia Española define a la metafísica como la «parte de la filosofía que trata del ser en cuanto tal, y de sus propiedades, principios y causas primeras.»

La metafísica pregunta por los últimos fundamentos del mundo y de todo lo existente. Su objetivo es lograr una comprensión teórica del mundo y de los principios últimos generales más elementales de lo que hay, porque tiene como fin conocer la verdad más profunda de las cosas, por qué son lo que son; y, aún más, por qué son.

Tres de las preguntas fundamentales de la metafísica son:

No sólo se pregunta entonces por lo que hay, sino también por qué hay algo. Además aspira a encontrar las características más elementales de todo lo que existe: la cuestión planteada es si hay características tales que se le puedan atribuir a todo lo que es y si con ello pueden establecerse ciertas propiedades del ser.

Algunos de los conceptos principales de la metafísica son: ser, nada, existencia, esencia, mundo, espacio, tiempo, mente, Dios, libertad, cambio, causalidad y fin.

Algunos de los problemas más importantes y tradicionales de la metafísica son: el problema de los universales, el problema de la estructura categorial del mundo, y los problemas ligados al espacio y el tiempo.

Lo que es decisivo para distinguir los diferentes tipos de metafísica es el concepto de ser. La tradición distingue dos tipos de enfoques esencialmente distintos:

Según este enfoque, «ser» es la característica más general de diferentes cosas (llamadas entes o entidades), aquello que sigue siendo igual a todos los entes, después de que se han eliminado todas las características individuales a los entes particulares, esto es: el hecho de que «sean», esto es, el hecho de que a todas ellas les corresponda «ser» (cfr. diferencia ontológica).

Este concepto de «ser» es la base de la «metafísica de las esencias». Lo opuesto al «ser» viene a ser en este caso la «esencia», a la cual simplemente se le agrega la existencia. En cierto sentido no se diferencia ya mucho del concepto de la nada. Un ejemplo de ello lo dan ciertos textos de la filosofía temprana de Tomás de Aquino ("De ente et essentia").

Según este enfoque, el «ser» viene a ser aquello que se le puede atribuir a «todo», aunque de distintas maneras (analogía "entis"). El ser es aquello, en lo que los diferentes objetos coinciden y en lo que, a su vez, se distinguen.

Este enfoque del ser es la base de una metafísica (dialéctica) del ser. El concepto opuesto a ser, es aquí la nada, ya que nada puede estar fuera del ser. La filosofía tardía de Tomás de Aquino nos brinda un ejemplo de esta comprensión de «ser» ("Summa theologica")

Tradicionalmente la metafísica se divide en dos ramas:


La metafísica puede proceder de distintas maneras:


Ya desde los inicios de la filosofía en Grecia, con los llamados filósofos presocráticos, se aprecian los intentos de entender el universo todo a partir de un principio (originario) único y universal, el "αρχη" (arjé). 

Parménides de Elea (siglo VI-V a. C.) es considerado el fundador de la ontología. Es él quien utiliza por primera vez el concepto de ser/ente en forma abstracta. Este saber, metafísico, comenzó cuando el espíritu humano se hizo consciente de que lo real sin más no es lo que nos ofrecen los sentidos, sino lo que se capta con el pensamiento. («Lo mismo es pensar y ser») Es lo que él llama «ser», y que caracteriza a través de una serie de determinaciones conceptuales que están al margen de los datos de los sentidos, como ingénito, incorruptible, inmutable, indivisible, uno, homogéneo, etc.

Parménides expone su teoría con tres principios: «el ser (o el ente) es y el no-ser no es», «nada puede pasar del ser al no-ser y viceversa» y «lo mismo es el pensar que el ser» (esto último se refiere a que no puede existir lo que no puede ser pensado).

A partir de su afirmación básica («el ser es, el no-ser no es») Parménides deduce que el ser es ilimitado, ya que lo único que podría limitarlo es el no-ser; pero como el no-ser no es, no puede establecer limitación alguna.

Por lo tanto, según deducirá Meliso de Samos, el ser es infinito (ilimitado en el espacio) y eterno (ilimitado en el tiempo).
La influencia de Parménides es decisiva en la historia de la filosofía y del pensamiento mismo. Hasta Parménides, la pregunta fundamental de la filosofía era: ¿de qué está hecho el mundo? (a lo que algunos filósofos habían respondido que el elemento fundamental era el aire, otros que era el agua, otros un misterioso elemento indeterminado, etc.) Parménides instaló al «ser» ("esse") en la escena como objeto principal del discurrir filosófico. El próximo paso decisivo lo dará Sócrates.

La filosofía de Sócrates (470-399 a. C.) se centra en la moral. Su pregunta fundamental es: ¿qué es el bien? Sócrates creía que si se lograba extraer el concepto del bien se podía enseñar a la gente a ser buena (como se enseña la matemáticas, por ejemplo) y se acabaría así con el mal. Estaba convencido de que la maldad es una forma de ignorancia, doctrina llamada intelectualismo moral. Desarrolló la primera técnica filosófica que se conoce: la mayéutica. Consistía en preguntar y volver a preguntar sobre las respuestas obtenidas una y otra vez, profundizando cada vez más. Con ello pretendía llegar al «logos» o la razón final que hacía que una cosa fuera esa cosa y no otra. Este «logos» es el embrión de la «idea» de Platón, su discípulo.

El punto central de la filosofía de Platón (427-347 a. C.), lo constituye la teoría de las Ideas. Platón observó que el "logos" de Sócrates era una serie de características que percibimos en los objetos (físicos o no) y están asociadas a él. Si a ese "logos" lo separamos del objeto físico y le damos existencia formal, entonces se llama «idea» (la palabra «idea» la introdujo Platón). En los diálogos platónicos aparece Sócrates preguntando por lo que es justo, valeroso, bueno, etc. La respuesta a estas preguntas presupone la existencia de ideas universales cognoscibles por todos los seres humanos que se expresan en estos conceptos. Es a través de ellas que podemos captar el mundo en constante transformación. 

Las ideas son el paradigma de las cosas. Su lugar está entre el ser y el no-ser. Son anteriores a las cosas, que participan (methexis) de ellas. En sentido estricto sólo ellas son. Las cosas particulares que vemos sólo representan copias más o menos exactas de las ideas. La determinación o definición de las ideas se obtiene a través del ejercicio dialógico riguroso, enmarcado en determinado contexto histórico y coyuntural, delimitando aquello en lo que se ha centrado la investigación (la idea). 

Con la teoría de las Ideas Platón pretende probar la posibilidad del conocimiento científico y del juicio imparcial. El hecho de que todos los seres humanos tengan la posibilidad de acceder a un mismo conocimiento, tanto en el campo de las matemáticas, como en el de la ética, lo explica a través de la teoría del «recuerdo» ("ἀνάμνησις"), según la cual recordamos las ideas eternas que conocimos antes de nuestro nacimiento. Con ello Platón explica la universalidad de la capacidad racional de todos los seres humanos, enfrentándose a algunos de sus contemporáneos que sostenían la incapacidad de acceder al conocimiento por parte de esclavos o pueblos no-helénicos, entre otros.

La tradición postplatónica muchas veces entendió la teoría de las Ideas de Platón, en el sentido de que habría supuesto una existencia de las ideas separada de la existencia de las cosas. Esta teoría de la duplicación de los mundos, en la Edad Media condujo a la polémica sobre los universales.

Aristóteles (384-322 a. C.) nunca usó la palabra «metafísica» en su obra conocida como "Metafísica". Dicho título se atribuye al primer editor sistemático de la obra del estagirita, Andrónico de Rodas, que supuso que, por su contenido, los catorce libros que agrupó debían ubicarse después de la «física» y por esa razón usó el prefijo «"μετὰ"» (más allá de... o después de...) En su análisis del ente, Aristóteles va más allá de la materia, al estudiar las cualidades y potencialidades de lo existente para acabar hablando del «ser primero», el «motor inmóvil» y generador no movido de todo movimiento, que más tarde sería identificado con Dios.

Para Aristóteles la metafísica es la ciencia de la esencia de los entes y de los primeros principios del ser. El ser se dice de muchas maneras y éstas reflejan la esencia del ser. En ese sentido elabora ser, independientemente de las características momentáneas, futuras y casuales. La "ousía" (generalmente traducido como sustancia) es aquello que es independiente de las características (accidentes), mientras que las características son dependientes de la "ousía". La ousía es lo que existe en sí, en contraposición al accidente, que existe en otro. Gramaticalmente o categorialmente, se dice que la sustancia es aquello a lo que se adscribe características, es decir, es aquello sobre lo cuál se puede afirmar (predicar) algo. Aquello que se afirma sobre las sustancias son los predicados. 

A la pregunta de qué sería finalmente la esencia que permanece inmutable, la respuesta de Aristóteles viene a ser que la "ousía" es una forma determinante –el "eidos"- es el origen de todo ser, es decir, que por ejemplo en el "eidos" de Sócrates, lo que en su forma humana, determina su humanidad. Y también la que determina que siendo el hombre por naturaleza libre y no siendo el esclavo libre, determina que el esclavo sea parte constitutiva de su amo, es decir, que no sea sólo esclavo de su amo en determinada coyuntura y desde determinada perspectiva, sino que sea esclavo por naturaleza.

En la Edad Media, se dio el debate sobre la distinción y orden de jerarquías entre la metafísica y la teología, en especial en la escolástica. La cuestión de la distinción entre metafísica y teología es también omnipresente en la filosofía moderna.

La llegada de la filosofía griega al campo de influencia del islam no fue directa, sino que tiene que ver con los cenobios cristianos en la península arábiga y los pertenecientes a ideologías consideradas heréticas y que utilizaban la filosofía griega no como un fin, sino como un instrumento que les servía para sus especulaciones teológicas (como los monofisistas o los nestorianos), pero es por el interés utilitarista en la medicina griega cuando empiezan a hacerse traducciones al persa que después pasarían tardíamente al árabe. 

Cabe mencionar que en árabe no existe el verbo «ser» y más difícilmente una construcción como «ser», que es un verbo convertido en sustantivo. Es reseñable que la metafísica del mundo islámico quedó influenciada en gran medida por la metafísica de Aristóteles.

En la Edad Media la metafísica es considerada la «reina de las ciencias» (Tomás de Aquino). Se proponen la tarea de conciliar la tradición de la filosofía antigua con la doctrina religiosa (musulmana, cristiana o judía). Con base en el neoplatonismo tardío la metafísica medieval se propone reconocer el «verdadero ser» y a Dios a partir de la razón pura.

Los temas centrales de la metafísica medieval son la diferencia entre el ser terrenal y el ser celestial (analogía "entis"), la doctrina de los trascendentales y las pruebas de la existencia de Dios. Dios es el fundamento absoluto del mundo, del cual no se puede dudar. Se discute si Dios ha creado el mundo de la nada (creación ex nihilo) y si es posible acceder a su conocimiento a través de la razón o sólo a través de la fe. Inspirados en la teoría de la duplicación de los mundos atribuida a Platón su metafísica se manifiesta como una suerte de «dualismo» del «acá» y del «más allá», de la «mera percepción sensible» y del «pensar puro como conocimiento racional», de una «inmanencia» de la vida interior y una «trascendencia» del mundo exterior.

La tradición moderna ha dividido a la metafísica en: metafísica general u ontología, ciencia del ente en tanto ente, y metafísica especial, que se divide en tres ramas:

Esta clasificación, que fue propuesta entre otros por Christian Wolff, ha sido posteriormente discutida, pero sigue siendo considerada canónica.

La filosofía trascendental de Kant significó un «giro copernicano» para la metafísica. Su posición frente a la metafísica es paradigmática. Le atribuye ser un discurso de «palabras huecas» sin contenido real, la acusa de representar «las alucinaciones de un vidente», pero por otra parte recoge de ella la exigencia de universalidad. Kant se propuso fundamentar una metafísica «que pueda presentarse como ciencia». Para ello examinó primero la posibilidad misma de la metafísica. Para Kant las cuestiones últimas y las estructuras generales de la realidad están ligadas a la pregunta por el sujeto. A partir de este presupuesto dedujo que hay que estudiar y juzgar aquello que puede ser conocido por nosotros. A través de su criticismo se diferenció explícitamente de las posiciones filosóficas que tienen como objeto la pregunta sobre qué es el conocimiento. Se alejó así de las tendencias filosóficas imperantes, tales como el empirismo, el racionalismo y el escepticismo. También a través del criticismo marcó distancia del dogmatismo de la metafísica que -según Kant- se había convertido en una serie de afirmaciones sobre temas que van más allá de la experiencia humana. Intentó entonces llevar a cabo un análisis detallado de la facultad humana de conocer, es decir, un examen crítico de la razón pura, de la razón desvinculada de lo sensible ("Crítica de la razón pura", 1781-1787). Para ello es decisivo el presupuesto epistemológico de Kant de que al ser humano la realidad no se le presenta tal como es realmente (en sí), sino tal como se le aparece debido a la estructura específica de su facultad de conocimiento.

Como el conocimiento científico también depende siempre de la experiencia, el hombre no puede emitir juicios sobre cosas que no están dadas por las sensaciones (tales como «Dios», «alma», «universo», «todo», etc.) Por ello Kant dedujo que la metafísica tradicional no es posible, porque el ser humano no dispone de la facultad de formar un concepto basándose en la experiencia sensible de lo espiritual, que es la única que permitiría la verificación de las hipótesis metafísicas. Como el pensar no dispone de ningún conocimiento de la realidad en este aspecto, estos asuntos siempre permanecerán en el ámbito de lo especulativo-constructivo. Entonces, por principio, no es posible según Kant decidir racionalmente sobre preguntas centrales tales como si Dios existe, si la voluntad es libre o si el alma es inmortal. Las matemáticas y la física pueden formular juicios sintéticos a priori y, por ello, alcanzar un conocimiento universal y necesario, un conocimiento científico.

Desde la crítica kantiana surge el idealismo alemán, representada sobre todo por Fichte, Schelling y Hegel, y que considera a la realidad como un acontecimiento espiritual en el que el ser real es superado, siendo integrado en el ser ideal.

El idealismo alemán recoge el giro trascendental de Kant, es decir que, en vez de entender la metafísica como la búsqueda de la obtención del conocimiento objetivo, se ocupa de las condiciones subjetivas de posibilidad de tal conocimiento. Así, se plantea hasta qué punto el ser humano puede llegar a reconocer estas evidencias. Sin embargo, rechaza que el conocimiento se limite a la experiencia posible y a los meros fenómenos, y propone una superación de esta posición, volviendo a postulados metafísicos que puedan reclamar validez universal: «conocimiento absoluto» como se decía desde Fichte hasta Hegel. Si aceptamos que los contenidos del conocimiento sólo valen en relación con el sujeto -como suponía Kant- y consideramos que esta perspectiva es absoluta, es decir, es la perspectiva de un sujeto absoluto, entonces el conocimiento válido para este sujeto absoluto también tiene validez absoluta. A partir de este planteamiento el idealismo alemán considera que puede superar la contradicción empírica entre sujeto y objeto, para poder captar lo absoluto.

Hegel sostiene que de una identidad pura y absoluta no puede surgir o entenderse una diferencia (esa identidad sería como «la noche, en la que todas las vacas son negras»): no explicaría la realidad en toda su diversidad. Por eso «la identidad de lo absoluto» debe entenderse como que está desde su origen ya que contiene en sí la posibilidad y la necesidad de una diferenciación. Esto implica que lo absoluto se realiza en su identidad por el plasmado y la superación de momentos no idénticos, esto es, la identidad dialéctica. A partir de este planteamiento Hegel desarrolla la "Ciencia de la lógica" considerado, tal vez, como el último gran sistema de la metafísica occidental.

Friedrich Nietzsche considera que Platón es el iniciador del pensamiento metafísico y le hace responsable de la escisión en el ser que tendrá luego formas variadas pero constantes. La división entre mundo sensible y mundo inteligible, con su correlato cuerpo-alma, y la preeminencia del segundo asegurada por la teoría de las Ideas sitúa el mundo verdadero más allá de los sentidos. Esto deja fuera del pensar el devenir, aquello no apresable en la división sensible-inteligible por su carácter informe, y que también dejan escapar las subsiguientes divisiones aristotélicas, como sustancia-accidente y acto-potencia.

Martin Heidegger dijo que nuestra época es la del «cumplimiento de la metafísica», pues desde los inicios del pensamiento occidental se han producido unos determinados resultados que configuran un panorama del que el pensamiento metafísico no puede ya dar cuenta. El propio éxito de la metafísica ha conducido fuera de ella. Ante esto, la potencia del pensamiento consiste precisamente en conocer e intervenir sobre lo conocido. Pero el pensamiento metafísico carece ya de potencia ya que ha rendido sus últimos frutos.

Heidegger afirmó que la metafísica es «el pensamiento occidental en la totalidad de su esencia». La utilización del término «esencia» en esta definición, implica que la técnica para estudiar la metafísica como forma de pensamiento, es o debe ser la metafísica en el primer sentido antes indicado. Esto quiere decir que los críticos de la metafísica como esencia del pensamiento occidental, son conscientes de que no existe una «tierra de nadie» en que situarse, más allá de esa forma de pensamiento; sólo el estudio atento y la modificación consciente y rigurosa de las herramientas proporcionadas por la tradición filosófica, pueden ajustar la potencia del pensamiento a las transformaciones operadas en aquello que la metafísica estudiaba: el ser, el tiempo, el mundo, el hombre y su conocer. Pero esa modificación supone a su vez un «salto» que toda la tradición del pensamiento ha escenificado, ha fingido o soñado dar a lo largo de su desarrollo. El salto fuera de la metafísica y por tanto, quizá la revocación de sus consecuencias.
Heidegger caracterizó el discurso metafísico por su impotencia para pensar la diferencia óntico-ontológica, es decir, la diferencia entre los entes y el ser. La metafísica refiere al ser el modelo de los entes (las cosas), pero aquél sería irreductible a éstos: los entes son, pero el ser de los entes no puede caracterizarse simplemente como estos. El ser es pensado como ente supremo, lo que le identifica con Dios; la pulsión ontoteológica es una constante en el pensamiento occidental. Para Heidegger la metafísica es el «olvido del ser», y la conciencia de este olvido debe abrir una época nueva, enfrentada a la posibilidad de expresar lo dejado al margen del pensamiento.

La filosofía analítica fue desde su nacimiento con autores como Russell y Moore muy escéptica respecto a la posibilidad de una metafísica sistemática tal y como se había defendido tradicionalmente. Esto se debe a que el nacimiento de la filosofía analítica se debiera principalmente a un intento de rebelión contra el idealismo neohegeliano entonces hegemónico en la Universidad británica. Sería a partir de los años veinte cuando el Círculo de Viena, ofrecería una crítica total a la metafísica como un conjunto de proposiciones carentes de significado por no cumplir con los criterios verificacionistas del significado. No obstante esta posición es hoy minoritaria en el panorama analítico, donde se ha recuperado el interés por ciertos problemas clásicos de la metafísica como el de los universales, la existencia de Dios y otros de tipo ontológico.

El postestructuralismo (Gilles Deleuze, Michel Foucault, Jacques Derrida) retoma la crítica de Nietzsche, y argumenta que lo no pensable en la metafísica es precisamente la «diferencia» en tanto tal. La diferencia, en el pensar metafísico, queda subordinada a los entes, entre los que se da como una «relación». La pretensión de «inscribir la diferencia en el concepto» transformando éste y violentando para ello los límites del pensamiento occidental aparece ya como una pretensión que lleva a la filosofía más allá de la metafísica.





</doc>
<doc id="6788" url="https://es.wikipedia.org/wiki?curid=6788" title="Sonar">
Sonar

El sonar (del inglés "SONAR", acrónimo de "Sound Navigation And Ranging", ‘navegación por sonido’) es una técnica que usa la propagación del sonido bajo el agua (principalmente) para navegar, comunicarse o detectar objetos sumergidos.

El sonar puede usarse como medio de localización acústica, funcionando de forma similar al radar, con la diferencia de que en lugar de emitir ondas electromagnéticas emplea impulsos sonoros. De hecho, la localización acústica se usó en aire antes que el GPS, siendo aún de aplicación el SODAR (la exploración vertical aérea con sonar) para la investigación atmosférica. La señal acústica puede ser generada por piezoelectricidad o por magnetostricción.

El término «sonar» se usa también para aludir al equipo empleado para generar y recibir el sonido de carácter infrasonoro. Las frecuencias usadas en los sistemas de sonar van desde las intrasónicas a las extrasónicas (entre 20 Hz y 20 000 Hz), la capacidad del oído humano. Sin embargo, en este caso habría que referirse a un hidrófono y no a un sonar. El sonar tiene ambas capacidades: puede ser utilizado como hidrófono o como sonar.

Existen otros sonares que no abarcan el espectro del oído humano, (cazaminas); pueden comprender varias gamas de alta frecuencia, (80 kHz o 350 kHz), por ejemplo. Ganan en precisión a la hora de determinar el objeto, pero pierden en alcance.

Aunque algunos animales (como delfines y murciélagos) han usado probablemente el sonido para la detección de objetos durante millones de años, el uso por parte de humanos fue registrado por vez primera por Leonardo Da Vinci en 1490. Se decía que se usaba un tubo metido en el agua para detectar barcos, poniendo un oído en su extremo. En el siglo XIX se usaron campanas subacuáticas como complemento a los faros para avisar del peligro a los marineros.

El uso de sonido para la «ecolocalización» submarina parece haber sido impulsado por el desastre del "Titanic" en 1912. La primera patente del mundo sobre un dispositivo de este tipo fue concedida por la Oficina Británica de Patentes al meteorólogo inglés Lewis Richardson un mes después del hundimiento del "Titanic" , y el físico alemán Alexander Behm obtuvo otra por un resonador en 1913. El ingeniero canadiense Reginald Fessenden construyó un sistema experimental en 1914 que podía detectar un iceberg a dos millas de distancia, si bien era incapaz de determinar en qué dirección se hallaba.

Durante la 1ª Guerra Mundial, y debido a la necesidad de detectar submarinos, se realizaron más investigaciones sobre el uso del sonido. Los británicos emplearon pronto micrófonos subacuáticos, mientras el físico francés Paul Langevin, junto con el ingeniero eléctrico ruso emigrado Constantin Chilowski, trabajó en el desarrollo de dispositivos activos de sonido para detectar submarinos en 1915. Aunque los transductores piezoeléctricos y magnetostrictivos superaron más tarde a los electrostáticos que usaron, este trabajo influyó sobre el futuro de los diseños detectores. Si bien los transductores modernos suelen usar un material compuesto como parte activa entre la cabeza ligera y la cola pesada, se han desarrollado muchos otros diseños. Por ejemplo, se han usado películas plásticas ligeras sensibles al sonido y fibra óptica en hidrófonos (transductores acústico-eléctricos para uso acuático), mientras se han desarrollado el Terfenol-D y el PMN para los proyectores. Los materiales compuestos piezoeléctricos son fabricados por varias empresas, incluyendo Morgan Electro Ceramics.

En 1916, bajo el patrocinio del Consejo Británico de Invenciones e Investigaciones, el físico canadiense Robert Boyle se encargó del proyecto del sonar activo, construyendo un prototipo para pruebas a mediados de 1917. Este trabajo, para la División Antisubmarina, fue realizado en el más absoluto secreto, y usaba cristales de cuarzo piezoeléctricos para producir el primer aparato de detección subacuática de sonido activo factible del mundo. Mientras tanto, en el mismo laboratorio se encargaba Albert Beaumont Wood del desarrollo de sistemas de escucha pasiva.

Para 1918 tanto Francia como Gran Bretaña habían construido sistemas activos. Los británicos probaron su ASDIC (así eran conocidos los equipos de detección activa) en el HMS "Antrim" en 1920 y empezaron la producción de unidades en 1922. La 6ª Flotilla Destructora tuvo buques equipados con ASDIC en 1923. Un buque-escuela antisubmarino, el HMS "Osprey", y una flotilla de entrenamiento compuesta por cuatro buques se estableció en Isla de Portland en 1924. El "Sonar QB" estadounidense no llegó hasta 1931.

Con el inicio de la Segunda Guerra Mundial, la Marina Real Británica tenía cinco equipos para diferentes clases de buques de superficie y otros para submarinos, incorporados en un sistema de ataque antisubmarino completo. La efectividad de los primeros ASDIC estaba limitada por el uso de las cargas de profundidad como arma antisubmarina. Esto exigía que el buque atacante pasase sobre el contacto sumergido antes de lanzar las cargas, lo que hacía perder el contacto sonar en los momentos previos al ataque. El ataque exigía, pues, disparar a ciegas, periodo en el que el comandante del submarino podía adoptar con éxito medidas evasivas. Esta situación se remediaba usando varios buques cooperando juntos y con la adopción de «armas de lanzamiento delantero», como el "Hedgehog" y más tarde el "Squid", que lanzaban las cargas a un blanco situado delante del atacante y por tanto aún en contacto ASDIC. Los desarrollos durante la guerra desembocaron en unos equipos ASDIC que usaban diferentes formas de onda, lo que permitía que los puntos ciegos fueran cubiertos continuamente. Más tarde se emplearon torpedos acústicos.

Al inicio de la Segunda Guerra Mundial la tecnología británica de sonar fue transferida a los Estados Unidos. La investigación sobre el sonar y el sonido submarino se amplió enormemente, particularmente en este país. Se desarrollaron muchos nuevos tipos de sonar militar, entre ellos las sonoboyas, el sonar sumergible y el de detección de minas. Este trabajo formó la base de los desarrollos de posguerra destinados a contrarrestar los submarinos nucleares. El sonar siguió desarrollándose en muchos países para usos tanto militares como civiles. En los últimos años la mayoría de los desarrollos militares han estado centrados en los sistemas activos de baja frecuencia.

En la Segunda Guerra Mundial Estados Unidos usó el término "SONAR" para sus sistemas, acrónimo acuñado como equivalente de "RADAR". En 1948, con la formación de la OTAN, la estandarización de señales llevó al abandono del término "ASDIC" en favor de "SONAR". 

El rendimiento de la detección, clasificación y localización de un sonar depende del entorno y del equipo receptor, además del equipo emisor en un sonar activo o del ruido radiado por el blanco en un sonar pasivo.

El funcionamiento del sonar se ve afectado por las variaciones en la velocidad del sonido, especialmente en el plano vertical. El sonido viaja más lentamente en el agua dulce que en el agua salada, variando en función del módulo de elasticidad y la densidad de masa. El módulo de elasticidad es sensible a la temperatura, a la concentración de impurezas disuelta (normalmente la salinidad) y a la presión, siendo menor el efecto de la densidad. Según Mackenzie, la velocidad del sonido "c" (en m/s) en el agua del mar es aproximadamente igual a:

Donde "T" es la temperatura (en grados Celsius , para valores entre 2 y 30 °C), "S" la salinidad (en partes por mil, para valores de 25 a 40) y "D" la profundidad en m (para valores entre 0 y 8000 m). Esta ecuación empírica es razonablemente precisa para los rangos indicados. La temperatura del océano cambia con la profundidad, pero entre 30 y 100 m hay un cambio a menudo notable, llamado termoclina, que divide el agua superficial más cálida de las profundas más frías que constituyen el grueso del océano. Esto puede dificultar la acción del sonar, pues un sonido que se origine en un lado del termoclino tiende a curvarse o refractarse al cruzarlo. La termoclina puede estar presente en aguas costeras menos profundas, donde sin embargo la acción de las olas mezcla a menudo la columna de agua, eliminándolo. La presión del agua también afecta la propagación del sonido en el vacío, aumentando su viscosidad a presiones mayores, lo que hacen que las ondas sonoras se retracten alejándose desde la zona de mayor viscosidad. El modelo matemático de refracción se denomina Ley de Snell.

Las ondas sonoras que se radian hacia el fondo del océano se curvan de vuelta a la superficie en grandes arcos senoidales debido a la presión creciente (y por tanto mayor velocidad del sonido) con la profundidad. El océano debe tener al menos 1850 m de profundidad para que las ondas sonoras devuelvan el eco del fondo en lugar de refractarse de vuelta a la superficie, reduciendo la pérdida del fondo el rendimiento. En las condiciones adecuadas estas ondas sonoras se concentrarán cerca de la superficie y serán reflejadas de vuelta al fondo repitiendo otro arco atx. Cada foco en la superficie se denomina zona de convergencia, formando un anillo en el sonar. La distancia y anchura de la zona de convergencia depende de la temperatura y salinidad del agua. Por ejemplo, en el Atlántico Norte las zonas de convergencia se encuentran aproximadamente cada 33 millas náuticas (61 km), dependiendo de la época del año. Los sonidos que pueden oírse desde sólo unas pocas millas en línea directa pueden ser también detectados cientos de millas más lejos. Con sonares potentes la primera, segunda y tercera zonas de convergencia son bastante útiles; más allá de ellas la señal es demasiado débil y las condiciones térmicas demasiado inestables, reduciendo la fiabilidad de las señales. La señal se atenúa naturalmente con la distancia, pero los sistemas de sonar modernos son muy sensibles, pudiendo detectar blancos a pesar de las bajas relaciones señal-ruido.

Si la fuente de sonido es profunda y las condiciones adecuadas, la propagación puede ocurrir en el «canal de sonido profundo». Este proporciona una pérdida de propagación extremadamente baja para un receptor en el canal, lo que se debe a que el sonido atrapado en el canal no tiene pérdidas en los límites. Propagaciones parecidas pueden ocurrir en la «cinta de superficie» en condiciones buenas. Sin embargo en este caso hay pérdidas por reflejo en la superficie.

En aguas poco profundas la propagación es generalmente por repetidos sonidos en la superficie y el fondo, pudiéndose producir pérdidas considerables.

La propagación del sonido también se ve afectada por la absorción del agua así como de la superficie y el fondo. Esta absorción cambia con la frecuencia, debiéndose a diferentes mecanismos en el agua marina. Por esto el sonar que necesita funcionar en distancias largas tiende a usar frecuencias bajas, de forma que se minimicen los efectos de la absorción.

El mar contiene muchas fuentes de ruido que interfieren con la señal deseada. Las principales fuentes de ruido se deben a las olas y la navegación. El movimiento del receptor por el agua también puede producir ruido de baja propagación, en función de sus decibelios.

Cuando se usa un sonar activo, se produce dispersión por los pequeños objetos del mar así como por el fondo y la superficie. Esto puede ser una fuente importante de interferencia activa que no ocurre en el sonar pasivo. Este efecto de dispersión es diferente del que sucede en la reverberación de una habitación, que es un fenómeno reflexivo. Una analogía es la dispersión de las luces de un coche en la niebla: un rayo de luz de una linterna potente puede penetrar la niebla, pero los faros son menos direccionales y producen un «borrón» en el que la reverberación devuelta domina. De forma similar, para superar la reverberación en el agua, un sonar activo necesita emitir una onda estrecha.

El blanco de un sonar, como un submarino, tiene dos características principales que influyen sobre el rendimiento del equipo. Para el sonar activo son sus características reflectoras, conocidas como «fuerza» del blanco. Para el sonar pasivo, la naturaleza del ruido radiado por el blanco. En general el espectro radiado consistirá en un ruido continuo con líneas espectrales, usadas para clasificarlo.

También se obtienen ecos de otros objetos marinos tales como ballenas, estelas, bancos de peces y rocas.

Los submarinos atacados pueden lanzar contramedidas activas para aumentar el nivel de ruido y crear un gran blanco falso. Las contramedidas pasivas incluyen el aislamiento de los dispositivos ruidosos y el recubrimiento del casco de los submarinos.

El sonar activo usa un emisor de sonido y un receptor. Cuando los dos están en el mismo lugar se habla de funcionamiento monoestático. Cuando el emisor y el receptor están separados, de funcionamiento biestático. Cuando se usan más emisores o receptores espacialmente separados, de funcionamiento multiestático. La mayoría de los equipos de sonar son monoestático, usándose la misma matriz para emisión y recepción, aunque cuando la plataforma está en movimiento puede ser necesario considerar que esta disposición funciona biestáticamente. Los campos de sonoboyas activas pueden funcionar multiestáticamente.

El sonar activo crea un pulso electromagnético de sonido, llamado a menudo un «ping», y entonces oye la reflexión (eco) del mismo. Este pulso de sonido suele crearse electrónicamente usando un proyecto sonar formado por un generador de señal, un amplificador de potencia y un transductor o matriz electroacústica, posiblemente un conformador de haces. Sin embargo, puede crearse por otros medios, como por ejemplo químicamente, usando explosivos, o térmicamente mediante fuentes de calor. También puede crearse mediante el infrasonido.

Para calcular la distancia a un objeto se mide el tiempo desde la emisión del pulso a la recepción de su eco y se convierte a una longitud conociendo la velocidad del sonido. Para medir el rumbo se usan varios hidrófonos, midiendo el conjunto el tiempo de llegada relativo a cada uno, o bien una matriz de hidrófonos, midiendo la amplitud relativa de los haces formados mediante un proceso llamado conformación de haz. El uso de una matriz reduce la respuesta espacial de forma que para lograr una amplia cobertura se emplean sistemas multihaces. La señal del blanco (si existe) junto con el ruido se somete entonces a un procesado de señal, que para los equipos simples puede ser sólo una medida de la potencia. Se presenta entonces el resultado a algún tipo de dispositivo de decisión que califica la salida como señal o ruido. Este dispositivo puede ser un operador con auriculares o una pantalla, en los equipos más sofisticado un software específico. Pueden realizarse operaciones adicionales para clasificar el blanco y localizarlo, así como para medir su velocidad.

El pulso puede ser de amplitud constante o un pulso de frecuencia modulada ("chirp") para permitir la compresión de pulso en la recepción. Los equipos simples suelen usar el primero con un filtro lo suficientemente ancho como para cubrir posibles cambios Doppler debidos al movimiento del blanco, mientras los más complejos suelen usar la segunda técnica. Actualmente la compresión de pulso suele lograrse usando técnicas de correlación digital. Los equipos militares suelen tener múltiples haces para lograr una cobertura completa mientras los más simples sólo cubren un arco estrecho. Originalmente se usaba un único haz realizando el escaneo perimetral mecánicamente, pero esto era un proceso lento.

Especialmente cuando se usan transmisiones de una sola frecuencia, el efecto Doppler puede usarse para medir la velocidad radial del blanco. La diferencia de frecuencia entre la señal emitida y la recibida se mide y se traduce a una velocidad. Dado que los desplazamientos Doppler pueden deberse al movimiento del receptor o del blanco, debe tenerse la primera en cuenta para lograr un valor preciso.

El sonar activo se usa también para medir la distancia en el agua entre dos transductores (radioemisores) de sonar o una combinación de hidrófono y proyector. Cuando un equipo recibe una señal de interrogación, emite a su vez una señal de respuesta. Para medir la distancia, un equipo emite una señal de interrogación y mide el tiempo entre esta transmisión y la recepción de la respuesta. La diferencia de tiempo permite calcular la distancia entre dos equipos. Esta técnica, usada con múltiples equipos, puede calcular las posiciones relativas de objetos estáticos o en movimiento.

En época de guerra, la emisión de un pulso activo era tan comprometida para el camuflaje de un submarino que se consideraba una brecha severa de las operaciones.

Los emisores de sonar de alta potencia pueden afectar a la fauna marina, si bien no se sabe exactamente cómo. Algunos animales marinos como ballenas y delfines usan sistemas de ecolocalización parecidos a los del sonar activos para detectar a predadores y presas. Se teme que los emisores de sonar puedan confundir a estos animales.

Se ha sugerido que el sonar militar infunde pánico a las ballenas, haciéndoles emerger tan rápidamente como para sufrir algún tipo de síndrome de descompresión. Esta hipótesis fue planteada por vez primera en un ensayo publicado en la revista "Nature" en 2003, que informaba de lesiones agudas por burbujas de gas (indicativas de síndrome de descompresión) en ballenas encalladas poco después del inicio de maniobras militares junto a las Islas Canarias en septiembre de 2002.

En 2000 en la Bahamas un ensayo de la Armada de Estados Unidos de transmisiones sonar provocó el encallamiento de diecisiete ballenas, siete de las cuales fueron halladas muertas. La Armada asumió su responsabilidad en un informe que halló que las ballenas muertas habían sufrido hemorragias inducidas acústicamente en los oídos. La desorientación resultante probablemente llevó al encallamiento.

Un tipo de sonar de media frecuencia ha sido relacionado con muertes masivas de cetáceos en todo el mundo, y culpado por los ecologistas de dichas muertes. El 20 de octubre de 2005 se presentó una demanda en Santa Mónica (California) contra la Armada de Estados Unidos por violar en las prácticas de sonar varias leyes medioambientales, incluyendo la "National Environmental Policy Act", la "Marine Mammal Protection Act" y la "Endangered Species Act".

El sonar pasivo detecta sin emitir. Se usa a menudo en instalaciones militares, así mismo tiene aplicaciones científicas, como detectar la ausencia o presencia de peces en diversos entornos acuáticos.

El sonar pasivo cuenta con una amplia variedad de técnicas para identificar la fuente de un sonido detectado. Por ejemplos, los buques estadounidenses suelen contar con motores de corriente alterna de 60 Hz. Si los transformadores o generador se montan sin el debido aislamiento de la vibración respecto al casco o se inundan, el sonido de 60 Hz del motor puede ser emitido por el buque, lo que puede ayudar a identificar su nacionalidad, pues la mayoría de submarinos europeos cuentan con sistemas a 50 Hz. Las fuentes de sonido intermitentes (como la caída de una llave inglesa) también pueden detectarse con equipos de sonar pasivo. Recientemente, la identificación de una señal era realizada por un operador según su experiencia y entrenamiento, pero actualmente se usan ordenadores para este cometido.

Los sistemas de sonar pasivo pueden contar con una gran base de datos sónica, si bien la clasificación final suele ser realizada manualmente por el operador de sonar. Un sistema informático usa a menudo esta base de datos para identificar clases de barcos, acciones (por ejemplo, la velocidad de un buque, o el tipo de arma disparada), e incluso barcos particulares. La Oficina de Inteligencia Naval estadounidense publica y actualiza constantemente clasificaciones de sonidos.

El sonar pasivo suele tener severas limitaciones por culpa del ruido generado por los motores y la hélice. Por este motivo muchos submarinos son impulsados por reactores nucleares que pueden refrigerarse sin bombas, usando sistemas de convección silenciosos, o por células de combustible o baterías, que también son silenciosas. Los propulsores de los submarinos también se diseñan y construyen de forma que emitan el menor ruido posible. La propulsión a alta velocidad suele crear diminutas burbujas de aire, fenómeno que se conoce como cavitación y tiene un sonido característico.

Los hidrófonos del sonar pueden remolcarse detrás del barco o submarino para reducir el efecto del ruido generado por el propio agua. Las unidades remolcadas también combaten la termoclina, ya que puede ajustarse su altura para evitar quedar en esta zona.

La mayoría de los sonares pasivos usaban una representación bidimensional. La dirección horizontal de la misma era la marcación y la vertical la frecuencia, o a veces el tiempo. Otra técnica de representación era codificar con colores la información frecuencia-tiempo de la marcación. Las pantallas más recientes son generadas por ordenadores e imitan las típicas pantallas indicadoras de posición de los radares.

La guerra naval hace un uso extensivo del sonar. Se usan los dos tipos descritos anteriormente, desde varias plataformas: buques de superficie, aeronaves e instalaciones fijas. La utilidad de los sonares activos y pasivos depende de las características del ruido radiado por el blanco, generalmente un submarino. Aunque en la Segunda Guerra Mundial se usó principalmente el sonar activo, excepto por parte de los submarinos, con la llegada de los ruidosos submarinos nucleares se prefirió el sonar pasivo para la detección inicial. A medida que los submarinos se hacían más silenciosos se fue usando más el sonar activo.

El sonar activo es extremadamente útil dado que proporciona la posición exacta de un objeto. Su uso es sin embargo algo peligroso, dado que no permite identificar el blanco y cualquier buque cercano a la señal emitida la detectará. Eso permite identificar fácilmente el tipo de sonar (normalmente por su frecuencia) y su posición (por la potencia de la onda sonora). Más aún, el sonar activo permite al usuario detectar objetos dentro en un determinado alcance, pero también permite que otras plataformas detecten el sonar activo desde una distancia mucho mayor.

Debido a que el sonar activo no permite una identificación exacta y es muy ruidoso, este tipo de detección se usa desde plataformas rápidas (aviones y helicópteros) o ruidosas (la mayoría de buques de superficies), pero rara vez desde submarinos. Cuando un sonar activo se usa en superficie, suele activarse muy brevemente en periodos intermitentes, para reducir el riesgo de detección por el sonar pasivo de un enemigo. Así, el sonar activo suele considerarse un apoyo del pasivo. En las aeronaves el sonar activo se usa en sonoboyas desechables que se lanzan sobre la zona a patrullar o cerca de los contactos de un posible enemigo.

El sonar pasivo escucha los ruidos por lo que tiene ventajas evidentes sobre el activo. Generalmente tiene un alcance mucho mayor que el activo y permite la identificación del blanco. Dado que cualquier vehículo de motor hace algo de ruido, terminará siendo detectado, dependiendo sólo de la cantidad de ruido emitido y del presente en la zona, así como la tecnología usada. En un submarino, el sonar pasivo montado a proa detecta en unos 270º respecto al centro del buque, la matriz montada en el casco, unos 160º a cada lado, y la matriz de la torreta en los 360º. Las zonas ciegas se deben a la propia interferencia del buque. Cuando se detecta una señal en cierta dirección (lo que significa que algo hace ruido en dicha dirección, a lo que se llama detección de banda ancha) es posible enfocar y analizar la señal recibir (análisis de banda estrecha). Esto se suele hacer usando una transformada de Fourier para mostrar las diferentes frecuencias que forman el sonido. Dado que cada motor hace un ruido específico, es fácil identificar el objeto.

Otro uso del sonar pasivo es determinar la trayectoria del blanco. Este proceso se llama Análisis del Movimiento del Blanco (TMA, "Target Motion Analysis"), y permite calcular el alcance, curso y velocidad del blanco. El TMA se realiza marcando desde qué dirección procede el sonido en momentos diferentes, y comparando el movimiento con el del buque del propio operador. Los cambios en el movimiento relativo se analizan usando técnicas geométricas estándar junto con algunas asunciones respecto a los casos límite.

El sonar pasivo es furtivo y muy útil, pero requiere componentes muy sofisticados y caros (filtros de paso de banda, receptores, ordenadores, software de análisis, etcétera). Suele equiparse en barcos caros para mejorar la detección. Los buques de superficie lo usan eficazmente, pero es incluso mejor usado en submarinos y también se emplea en aviones y helicópteros.

Hasta hace poco, los sonares en barcos de superficie solían montarse sobre el casco, a los lados o en la proa. Pronto se determinó tras sus primeros usos que se necesitaba un medio de reducir el ruido de la navegación. Primero se usó lienzo montado en un marco, y luego protecciones de acero. Actualmente los domos suelen hacerse de plástico reforzado o goma presurizada. Estos sonares son principalmente activos, como por ejemplo el SQS-56.

Algunas características de los sonares de buques de superficie más modernos son las siguientes:
Un ejemplo es el más moderno sonar de la Armada Española, el LWHP53SN desarrollado por Indra Sistemas y Lockheed Martin instalado en la fragata Cristóbal Colón (F-105), que incorpora todas estas características.
Debido a los problemas del ruido de los barcos también se emplean sonares remolcados. Estos también tienen la ventaja de poder situarse a mayor profundidad. Sin embargo, existen limitaciones a su uso en aguas poco profundas. Un problema es que los cabrestantes necesarios para lanzar y recuperar estos sonares son grandes y caros. Un ejemplo de este tipo de sonares es el Sonar 2087 fabricado por Thales Underwater Systems.

Los torpedos modernos suelen incluir un sonar activo/pasivo, que puede usarse para localizar directamente el blanco, pero también para seguir estelas. Un ejemplo pionero de este tipo de torpedos es el Mark 37.

Las minas pueden incorporar un sonar para detectar, localizar y reconocer su blanco. Un ejemplo es la mina CAPTOR.

El sonar antiminas (MCM, "Mine Countermeasure") es un tipo especializado de sonar usado para detectar objetos pequeños. La mayoría de ellos se montan en el casco, siendo un ejemplo el Tipo 2093.

Los submarinos confían en el sonar mucho más que los barcos de superficie, que no pueden usarlo a gran profundidad. Estos equipos pueden montarse en el casco o ser remolcados. Además, son muy útiles en cuestiones oceanográficas.

Los helicópteros pueden usarse para la lucha antisubmarina desplegando campos de sonoboyas activas/pasivas o empleado un sonar sumergible, como el AQS-13. Los aviones convencionales también pueden lanzar sonoboyas, teniendo más autonomía y capacidad para ello. El proceso de los datos recogidos por estos equipos puede realizarse en la aeronave o en un barco. Los helicópteros también se han usado en misiones de contramedidas frente a las minas, usando sonares remolcados como el AQS-20A.

Pueden ser remolcadas o independientes. Un ejemplo pionero fue el Sieglinde alemán.

Los barcos y submarinos van equipados con sonares especiales para la comunicación submarina. Un estándar OTAN permite que los diferentes tipos interactúen. Un ejemplo de estos equipos es el Sonar 2008.Este es uno de los más importantes

Durante muchos años los Estados Unidos operó un gran conjunto de matrices de sonar pasivo en varios puntos de los océanos del mundo, llamado colectivamente SOSUS ("Sound Surveillance System", ‘sistema de vigilancia sonora’) y más tarde IUSS ("Integrated Undersea Surveillance System", ‘sistema integrado de vigilancia submarina’). Se cree que un sistema parecido fue operado por la Unión Soviética. Al ser utilizadas matrices montadas permanentemente en el fondo del océano, se situaban en lugares muy silenciosos para lograr grandes alcances. El procesamiento de señales se realizaba utilizando grandes computadores en tierra. Con el final de la Guerra Fría una matriz SOSUS ha sido destinada a uso científico.

El sonar puede usarse para detectar hombres-rana y otros buceadores. Esto puede ser necesario alrededor de barcos o en las entradas de los puertos. El sonar activo también puede usarse como mecanismo disuasorio. Un ejemplo de estos equipos es el "Cerberus".

Este sonar se diseña para detectar y localizar las transmisiones de sonares hostiles. Un ejemplo es el Tipo 2082 equipado en los submarinos de clase Vanguard.

La pesca es una importante industria sujeta a una demanda creciente, pero el volumen de capturas mundial cae como resultado de una mayor escasez de recursos. La industria se enfrenta a un futuro de consolidación mundial continua hasta que puede alcanzarse un punto de sostenibilidad. Sin embargo, la consolidación de las flotas pesqueras ha acarreado una creciente demanda de sofisticados equipos electrónicos de localización pesquera tales como sensores, emisores y sonares. Históricamente, los pescadores han usado muchas técnicas diferentes para localizar bancos de peces. Sin embargo, la tecnología acústica ha sido una de las fuerzas más importantes tras el desarrollo de los pesqueros comerciales modernos.

Las ondas sonoras viajan de forma diferente a través de los peces que por aguas limpias debido a que la vejiga natatoria rellena de aire de éstos tiene una densidad diferente a la del agua marina. Esta diferencia de densidad permite la detección de bancos de peces usando el sonido reflejado. Actualmente, los pesqueros comerciales dependen casi completamente de los equipos acústicos para detectar peces.

Compañías como Marport Canada, Wesmar, Furuno, Krupp y Simrad fabrican sonares e instrumentos acústicos para la industria pesquera. Por ejemplo, los sensores de redes toman varias medidas bajo el agua y transmiten la información hasta un receptor a bordo. Cada sensor va equipado con uno o más transductores acústicos dependiendo de su función concreta. Los datos se transmiten usando telemetría acústica y se reciben en un hidrófono montado en el casco. Las señales se procesan y muestran en un monitor de alta resolución.

Emitiendo ondas sonoras directamente hacia el fondo y registrando el eco de retorno es posible calcular la profundidad, dado que la velocidad del sonido en el agua es más o menos estable en un rango de profundidades pequeño. 

Se emplean equipos acústicos montados sobre las redes, que transmiten la información registrada por cable o telemetría acústica al buque pesquero. Así se sabe con exactitud la distancia de la red al fondo y la superficie, la cantidad de pescado dentro de la misma, y otros datos relevantes.

Se han desarrollado sonares para medir la velocidad del barco relativa al agua y al fondo marino.

Se han equipado pequeños sonares en ROVs y UUVs para permitir su funcionamiento en condiciones de baja visibilidad. Estos sonares se usan para explorar por delante del vehículo.

Las aeronaves se equipan con sonares que funcionan como boyas para permitir su localización en caso de un accidente en el mar.

Pueden usarse sonares para estimar la biomasa presente en una región acuática, en función del reflejo sonoro devuelto por ésta. La principal diferencia con los equipos de localización pesquera es que el análisis hidroacústico cuantitativo requiere que las medidas se realicen con un equipo lo suficientemente sensible y bien calibrado como para obtener medidas fiables.

Los equipos hidroacústicos proveen un método repetible y no invasivo de recoger datos continuos y de alta resolución (por debajo del metro) en secciones tridimensionales, lo que permite medir la abundancia y distribución de los recursos pesqueros.

Para seguir los movimientos de peces, ballenas, etcétera puede acoplarse a un animal un dispositivo acústico que emita pulsos a ciertos intervalos, posiblemente codificando, por ejemplo, la profundidad.

Un transductor acústico vertical montado en el fondo marino o sobre una plataforma puede usarse para realizar medidas del tono y moléculas de las olas. De esto pueden derivarse estadísticas de las condiciones en la superficie de una ubicación dada.

Se han desarrollado sonares de corto alcance especiales para permitir la medida de la velocidad del agua, al vacío.

Se han desarrollado sonares que pueden usarse para caracterizar el fondo marino: fango, arena, grava, limos, etcétera. Esto suele lograrse comparando los retornos directos y reflejados por el fondo.

Los sonares de barrido lateral pueden usarse para confeccionar datos de la topografía de una zona. Sonares de baja frecuencia como GLORIA han sido usados en la exploración de la plataforma continental mientras los de mayor frecuencia se emplean para exploraciones detalladas de zonas más pequeñas.

Se han desarrollado potentes sonares de baja frecuencia para permitir la caracterización de las capas superficiales del fondo marino.

Diversos sonares de apertura sintética han sido construidos en laboratorio y algunos han llegado a usarse en sistemas de búsqueda y eliminación de minas de grafito.

Detección de pecios y yacimientos subacuáticos y su localización en el fondo marino.



</doc>
<doc id="6790" url="https://es.wikipedia.org/wiki?curid=6790" title="Radar">
Radar

' 
El radar (término derivado del acrónimo inglés "radio detection and ranging", “detección y medición de distancias por radio”) es un sistema que usa ondas electromagnéticas para medir distancias, altitudes, direcciones y velocidades de objetos estáticos o móviles como aeronaves, barcos, vehículos motorizados, formaciones meteorológicas y el propio terreno. Su funcionamiento se basa en emitir un impulso de radio, que se refleja en el objetivo y se recibe típicamente en la misma posición del emisor. A partir de este "eco" se puede extraer gran cantidad de información. El uso de ondas electromagnética con diversas longitudes de onda permite detectar objetos más allá del rango de otro tipo de emisiones (luz visible, sonido, etc.)

Entre sus ámbitos de aplicación se incluyen la meteorología, el control del tráfico aéreo y terrestre y gran variedad de usos militares.


En 1934 el GEMA ("La sociedad de aparatos electro-acústico y mecánico"), uno de cuyos fundadores fue Hans Hollmann, construye un magnetrón capaz de trabajar a 650 MHz. Ése fue el paso tecnológico que permitió el desarrollo del Freya, un radar de vigilancia aérea que trabajaba a 125 MHz con un alcance entre 80 y 150 millas. Era un radar para trabajar en superficie por sus dimensiones, por ello, una versión posterior fue el Seetakt que trabajaba a 375 MHz y tenía un alcance de 10 millas adaptado para ser montado en buques. Este radar fue utilizado en el verano de 1938 en la Guerra Civil Española. 

La competencia en la industria alemana de la época hizo que, en el año 1935, la empresa alemana Telefunken lanzara un radar de antena parabólica giratoria, antecesor del radar de alerta aérea Würzburg, radar de tiro de 560 MHz de trabajo y con deflector de 3m de diámetro.

El Freya y el Würzburg fueron la base de la defensa terrestre de los alemanes durante la Segunda Guerra Mundial, y el Steetakt pieza fundamental para la de detección a bordo de los buques de la Armada Alemana.
Al inicio de la Segunda Guerra Mundial Alemania decidió alistar a científicos e ingenieros en el frente, pensando que la guerra sería corta y satisfactoria, lo que hizo que no avanzara significativamente en esos años. En consecuencia quedó retrasada con respecto a sus adversarios, que siguieron avanzando.

El modelo de radar actual fue creado en 1935 y desarrollado principalmente en Inglaterra durante la Segunda Guerra Mundial por el físico Robert Watson-Watt. Supuso una notable ventaja táctica para la Royal Air Force en la Batalla de Inglaterra, cuando aún era denominado RDF ("Radio Direction Finding"). Aunque fue desarrollado con fines bélicos, en la actualidad cuenta con multitud de usos civiles, siendo la mejor herramienta para el control de tráfico aéreo.

En los momentos anteriores a la II Guerra Mundial, Robert Watson-Watt, físico y director del Laboratorio de Investigación de Radio y su ayudante, el físico Arnold Wilkins, estuvieron a cargo de la invención de un "rayo de la muerte" que sería utilizado en esa guerra. La idea de Watson-Watt era elevar la temperatura del piloto atacante a 41 °C aproximadamente para que, al provocarle fiebre, quedara incapacitado. 

Como lo escribió el propio Wilkins: 

Esta observación, hecha en enero de 1935, dio lugar una serie de hechos que culminaron con la invención del radar. Los hechos a los que Wilkins se refirió habían sido observados en muchos lugares y en todos se consideró esta perturbación como un estorbo que mucha gente había tratado de eliminar. De hecho, en 1932, la Oficina Postal Británica publicó un informe en el que sus científicos documentaron fenómenos naturales que afectaban la intensidad de la señal electromagnética recibida: tormentas eléctricas, vientos, lluvia y el paso de un aeroplano en la vecindad del laboratorio. Wilkins conoció este informe de manera accidental, conversando con la gente de la Oficina Postal, que se quejaba por la interferencia.

Cuando Wilkins sugirió la posibilidad de utilizar el fenómeno de interferencia de ondas de radio para detectar aviones enemigos, Watson-Watt lo comisionó inmediatamente para trabajar en el cálculo de los aspectos cuantitativos. 

Al terminar sus cálculos, a Wilkins le pareció increíble que el efecto deseado pudiera detectarse; revisó sus cálculos, no encontró ningún error y se los dio a Watson-Watt, quien los vio fantásticos y verificó los cálculos matemáticos. Al no encontrar error, envió los resultados. El hecho de que un rayo de la muerte no fuera factible no sorprendió, sin embargo atrajo la idea de poder detectar un avión.

Dos científicos del Naval Research Laboratory (NRL) Hoyt Taylor y L. Young dieron forma a las especulaciones de Marconi y las plasmaron en un experimento en el que transmitieron una señal de radio de onda continua a través del río Potomac detectando que al pasar los buques se producían alteraciones en la calidad de la señal recibida. Lograron perturbaciones con distancias de hasta tres millas. Observando esto, concluyeron con que se podría diseñar un elemento que detectara buques en el mar. 

Al mismo tiempo, la Armada de los EE. UU. se encontraba muy ocupada dotando a los buques de comunicaciones sin hilos. A pesar de esto, se continuó con su investigación a nivel científico en muchos campos. Es así que el NRL, en cooperación con el Carnegie Institute , durante el año 1925 investigó la reflexión de ondas en la ionosfera y la modulación por pulsos de la onda, de tal manera que conociendo el instante de salida de un pulso y midiendo su retardo se podría calcular la distancia del rebote. A partir de estas investigaciones se diseñó a principio de los años 30 el primer radar de impulsos, obteniéndose los primeros pulsos reflejados por aviones en diciembre de 1934. Aunque no fue hasta julio de 1936 cuando consiguieron que funcionara correctamente, debido a un error en el diseño del ancho de banda del receptor (demasiado estrecho). El radar trabajaba a 200 MHz con una anchura de pulso de 10µs. Este radar utilizaba una única antena en emisión y recepción pues incluía el primer duplexor, una novedad tecnológica que supuso una gran diferencia entre países durante varios años.

Las ondas electromagnéticas se dispersan cuando hay cambios significativos en las constantes dieléctricas o diamagnéticas. Esto significa que un objeto sólido en el aire o en el vacío (es decir, un cambio en la densidad atómica entre el objeto y su entorno) producirá dispersión de las ondas de radio, como las del radar. Esto ocurre particularmente en el caso de los materiales conductores como el metal y la fibra de carbono, lo que hace que el radar sea especialmente indicado para la detección de aeronaves. En ocasiones los aviones militares utilizan materiales con sustancias resistivas y magnéticas que absorben las ondas del radar, reduciendo así el nivel de reflexión. Estableciendo una analogía entre las ondas del radar y el espectro visible, estos materiales equivaldrían a pintar algo con un color oscuro.

La reflexión de las ondas del radar varía en función de su longitud de onda y de la forma del blanco. Si la longitud de onda es mucho menor que el tamaño del blanco, la onda rebotará del mismo modo que la luz contra un espejo. Si por el contrario es mucho más grande que el tamaño del blanco, lo que ocurre es que este se polariza (separación física de las cargas positivas y negativas) como en un dipolo (véase: Dispersión de Rayleigh). Cuando las dos escalas son similares pueden darse efectos de resonancia. Los primeros radares utilizaban longitudes de onda muy elevadas, mayores que los objetivos; las señales que recibían eran tenues. Los radares actuales emplean longitudes de onda más pequeñas (de pocos centímetros o inferiores) que permiten detectar objetos del tamaño de una barra de pan.

Las señales de radio de onda corta (3 kHz-30 MHz) se reflejan en las curvas y aristas, del mismo modo que la luz produce destellos en un trozo de cristal curvo. Para estas longitudes de onda los objetos que más reflejan son aquellos con ángulos de 90º entre las superficies reflectivas. Una estructura que conste de tres superficies que se juntan en una esquina (como la de una caja) siempre reflejará hacia el emisor aquellas ondas que entren por su abertura.

Este tipo de reflectores, denominados reflectores de esquina ("corner reflectors", ver imagen a la derecha), se suelen usar para hacer "visibles" al radar objetos que en otras circunstancias no lo serían (se suelen instalar en barcos para mejorar su detectabilidad y evitar choques). Siguiendo el mismo razonamiento, si se desea que una nave no sea detectada, en su diseño se procurará eliminar estas esquinas interiores, así como superficies y bordes perpendiculares a las posibles direcciones de detección. De ahí el aspecto extraño de los aviones "stealth" (avión furtivo). Todas estas medidas no eliminan por completo la reflexión debido a la difracción, especialmente para longitudes de onda grandes. Otra contramedida habitual es arrojar cables y tiras metálicas cuyo largo es media longitud de onda ("chaffs") con la idea de cegar al radar; son efectivas, si bien la dirección hacia la que se reflejan las ondas es aleatoria cuando lo óptimo sería dirigir la reflexión hacia el radar que se quiere evitar. El factor que da la medida de cuánto refleja un objeto las ondas de radio se llama "sección transversal de radar" ("σ"), traducción del inglés RCS ("Radar Cross Section").

La potencia "P" reflejada a la antena de recepción está dada por la ecuación radar:



</doc>
<doc id="6792" url="https://es.wikipedia.org/wiki?curid=6792" title="Teoría de la probabilidad">
Teoría de la probabilidad

La teoría de la probabilidad es una rama de las matemáticas que estudia los fenómenos aleatorios y estocásticos. Los fenómenos aleatorios se contraponen a los fenómenos deterministas, los cuales son resultados únicos y/o previsibles de experimentos realizados bajo las mismas condiciones determinadas, por ejemplo, si se calienta agua a 100 ºC a nivel del mar se obtendrá vapor. Los fenómenos aleatorios, por el contrario, son aquellos que se obtienen de experimentos realizados, otra vez, bajo las mismas condiciones determinadas pero como resultado posible poseen un conjunto de alternativas, por ejemplo, el lanzamiento de un dado o de una moneda.

La teoría de probabilidades se ocupa de asignar un cierto número a cada posible resultado que pueda ocurrir en un experimento aleatorio, con el fin de cuantificar dichos resultados y saber si un suceso es más probable que otro. 

Muchos fenómenos naturales son aleatorios, pero existen algunos como el lanzamiento de un dado, donde el fenómeno no se repite en las mismas condiciones, debido a que las características del material hace que no exista una simetría del mismo, así las repeticiones no garantizan una probabilidad definida. En los procesos reales que se modelizan mediante distribuciones de probabilidad corresponden a modelos complejos donde no se conocen "a priori" todos los parámetros que intervienen; ésta es una de las razones por las cuales la estadística, que busca determinar estos parámetros, no se reduce inmediatamente a la teoría de la probabilidad en sí.

En 1933, el matemático soviético Andréi Kolmogórov propuso un sistema de axiomas para la teoría de la probabilidad, basado en la teoría de conjuntos y en la teoría de la medida, desarrollada pocos años antes por Lebesgue, Borel y Frechet entre otros.

Esta aproximación axiomática que generaliza el marco clásico de la probabilidad, la cual obedece a la regla de cálculo de "casos favorables sobre casos posibles", permitió la rigorización de muchos argumentos ya utilizados, así como el estudio de problemas fuera de los marcos clásicos. Actualmente, la teoría de la probabilidad encuentra aplicación en las más variadas ramas del conocimiento, como puede ser la física (donde corresponde mencionar el desarrollo de las difusiones y el movimiento Browniano), o la economía (donde destaca el modelo de Black y Scholes para la valuación de acciones).

La teoría de la probabilidad se desarrolló originalmente a partir de ciertos problemas planteados en el contexto de juegos de azar. Inicialmente, no existía una teoría axiomática bien definida y las definiciones iniciales de probabilidad se basaron en la idea intuitiva de un cociente de ocurrencias:

donde "A" es un suceso cualquiera y:

Este tipo de definiciones si bien permitieron desarrollar un gran número de propiedades, no permitían deducir todos los teoremas y resultados importantes que hoy forman parte de la teoría de la probabilidad. De hecho el resultado anterior se puede demostrar rigurosamente dentro del enfoque axiomático de la teoría de la probabilidad, bajo ciertas condiciones.

La primera axiomatización completa se debió a Andréi Kolmogórov (quien usó dicho enfoque por ejemplo para deducir su "ley 0-1 para sucesos cola" y otros resultados relacionados con la convergencia de sucesiones aleatorias). La definición axiomática de la probabilidad se basa en resultados de la teoría de la medida y en formalizaciones de la idea de independencia probabilística. En este enfoque se parte de un espacio de medida normalizada formula_3 donde formula_4 es un conjunto llamado espacio de sucesos (según el tipo de problema puede ser un conjunto finito, numerable o no-numerable), formula_5 es una σ-álgebra de subconjuntos de formula_4 y formula_7 es una medida normalizada (es decir, formula_8). Los sucesos posibles se consideran como subconjuntos "S" de eventos elementales posibles: formula_9 y la probabilidad de que cada suceso viene dada por la medida de dicho conjunto:

La interpretación de esta probabilidad es la frecuencia promedio con la que aparece dicho suceso si se considera una elección de muestras aleatorias sobre formula_4.

La definición anterior es complicada de representar matemáticamente ya que formula_4 debiera ser infinito. Otra manera de definir la probabilidad es de forma axiomática esto estableciendo las relaciones o propiedades que existen entre los conceptos y operaciones que la componen.

La probabilidad es la característica de un evento, que hace que existan razones para creer que éste se realizará. 

La probabilidad p de que suceda un evento S de un total de n casos posibles igualmente probables es igual a la razón entre el número de ocurrencias h de dicho evento (casos favorables) y el número total de casos posibles n.

La probabilidad es un número (valor) que varia entre 0 y 1. Cuando el evento es imposible se dice que su probabilidad es 0, si el evento es cierto y siempre tiene que ocurrir su probabilidad es 1. 

La probabilidad de no ocurrencia de un evento está dada por q, donde:

Sabemos que p es la probabilidad de que ocurra un evento y q es la probabilidad de que no ocurra, entonces p + q = 1

Simbólicamente el espacio de resultados, que normalmente se denota por formula_4, es el espacio que consiste en todos los resultados que son posibles. Los resultados, que se denota por formula_13, etcétera, son elementos del espacio formula_4.

Como se ha adelantado anteriormente la definción axiomática de probabilidad es una extensión de la teoría de la medida, en la que se introducen la noción de independencia relativa. Este enfoque permite reproducir los resultados de la teoría clásica de la probabilidad además de resultados nuevos referidos a la convergencia de variables aleatorias. Además de los procesos estocásticos, el cálculo de Ito y las ecuaciones diferenciales estocásticas.

Dentro del enfoque axiomático es posible demostrar que la ley débil de los grandes números implica que se cumplirá que:

Esto permite justificar rigurosamente la ecuación suponiendo que:

Donde se interpreta formula_15 con probabilidad "p" y que formula_16 con proabilidad 1-"p".

Una variable aleatoria es una función medible

que da un valor numérico a cada suceso elemental formula_17. 

Este tipo de probabilidad, es aquel que puede tomar sólo ciertos valores diferentes que son el resultado de la cuenta de alguna característica de interés. Más exactamente, un problema de probabilidad discreta es un problema definido por un conjunto de variables aleatorias que sólo pueden tomar un conjunto finito o infinito numerable de valores diferentes:

donde:

Un problema de probabilidad continua es uno en el que aparecen variables aleatorias capaces de tomar valores en algún intervalo de números reales (y por tanto asumir un conjunto no numerable de valores), por lo que continuando con la notación anterior:

La distribución de probabilidad se puede definir para cualquier variable aleatoria "X", ya sea de tipo continuo o discreto, mediante la siguiente relación:

Para una variable aleatoria discreta esta función no es continua sin constante a tramos (siendo continua por la derecha pero no por la izquierda). Para una variable aleatoria general la función de distribución puede descomponerse en una parte continua y una parte discreta:

Donde formula_20 es una y formula_21 es una función constante a tramos.

La función de densidad, o densidad de probabilidad de una variable aleatoria absolutamente continua, es una función a partir de la cual se obtiene la probabilidad de cada valor que toma la variable definida como:

Es decir, su integral en el caso de variables aleatorias continuas es la distribución de probabilidad. En el caso de variables aleatorias discretas la distribución de probabilidad se obtiene a través del sumatorio de la función de densidad. La noción puede generalizarse a varias variables aleatorias.

La teoría de la probabilidad moderna incluye temas de las siguientes áreas:




</doc>
<doc id="6800" url="https://es.wikipedia.org/wiki?curid=6800" title="Hidra">
Hidra

El término Hidra o Hydra puede referirse a:



</doc>

<doc id="2891" url="https://es.wikipedia.org/wiki?curid=2891" title="Unión Europea">
Unión Europea

La Unión Europea (UE) es una comunidad política de derecho constituida en régimen sui géneris de organización internacional nacida para propiciar y acoger la integración y gobernanza en común de los Estados y los pueblos de Europa. Está compuesta por veintiocho Estados europeos y fue establecida con la entrada en vigor del Tratado de la Unión Europea (TUE) el 1 de noviembre de 1993.

Con ese acto, la supraestructura «Unión Europea» aunaba y se fundaba sobre las tres Comunidades Europeas preexistentes —la Comunidad Europea del Carbón y del Acero (CECA), la Comunidad Europea de la Energía Atómica (Euratom) y la Comunidad Económica Europea (CEE/CE)— y les añadía la política exterior común y la cooperación judicial y policial, formando un sistema complejo conocido como «los tres pilares». Sin embargo, con la entrada en vigor el 1 de diciembre de 2009 del Tratado de Lisboa, la Unión Europea sucedió, por completo aunque con ciertas particularidades, a las Comunidades Europeas y asumió con ello su personalidad jurídica única como sujeto de derecho internacional.

La Unión Europea ha desarrollado un sistema jurídico y político, el comunitario europeo, único en el mundo, que se rige por mecanismos y procedimientos de funcionamiento interno complejos, que se han extendido y evolucionado a lo largo de su historia hasta conformar, en la actualidad, un sistema híbrido de gobierno transnacional difícilmente homologable que combina elementos próximos a la cooperación multilateral, si bien fuertemente estructurada e institucionalizada, con otros de vocación netamente supranacional, regidos ambos por una dinámica de integración regional muy acentuada.

Todo esto desemboca en una peculiarísima comunidad de Derecho, cuya naturaleza jurídica y política es muy discutida, si bien sus elementos fundacionales y su evolución histórica, todavía abierta, apuntan, en el presente, a una especial forma de moderna confederación o gobernanza supranacional, acusadamente institucionalizada y con una inspiración histórico-política de vocación federal —en el sentido de un federalismo internacional nuevo, no de un Estado federal clásico— que se detecta con cierta claridad en ámbitos como la ciudadanía europea, los principios de primacía y efecto directo que le son aplicables a su ordenamiento jurídico en relación con los ordenamientos nacionales, el sistema jurisdiccional o la unión monetaria (el sistema del euro).

La Unión Europea, y antes las Comunidades, promueve la integración continental por medio de políticas comunes que abarcan distintos ámbitos de actuación, en su origen esencialmente económicos y progresivamente extendidos a ámbitos indudablemente políticos. Para alcanzar sus objetivos comunes, los estados de la Unión le atribuyen a esta determinadas competencias, ejerciendo una soberanía en común o compartida que se despliega a través de los cauces comunitarios.

La Unión Europea se rige por un sistema interno en régimen de democracia representativa. Sus instituciones son siete: el Parlamento Europeo, el Consejo Europeo, el Consejo, la Comisión Europea, el Tribunal de Justicia de la Unión Europea, el Tribunal de Cuentas y el Banco Central Europeo. El Consejo Europeo ejerce funciones de orientación política general y de representación exterior, y nombra a los jefes de las altas instituciones constitucionales; el Parlamento Europeo y el Consejo ejercen la potestad legislativa en igualdad de condiciones, tomando decisiones conjuntas —a excepción de los procedimientos legislativos especiales, donde el Parlamento desempeña un papel meramente consultivo—; la Comisión o Colegio de Comisarios aplica el Derecho de la Unión, supervisa su cumplimiento y ejecuta sus políticas, y a ella corresponde en exclusiva la iniciativa legislativa ante el Parlamento y la Comisión; el Tribunal de Justicia ejerce las labores jurisdiccionales supremas en el sistema jurídico comunitario; el Tribunal de Cuentas supervisa y controla el buen funcionamiento y la adecuada administración de las finanzas y de los fondos comunitarios; y el Banco Central Europeo dirige y aplica la política monetaria única de la zona euro.

La Unión cuenta también con otros órganos, instancias y organismos de funciones y atribuciones diversas, como el Comité Económico y Social, el Comité de las Regiones, el Defensor del Pueblo Europeo, el Alto Representante de la Unión para Asuntos Exteriores y Política de Seguridad, entre otros.

En 2012 la Unión Europea ganó el Nobel de la Paz, que fue otorgado por unanimidad de todos los miembros del jurado, «por su contribución durante seis décadas al avance de la paz y la reconciliación, la democracia, y los derechos humanos en Europa». En 2017 fue galardonada con el Premio Princesa de Asturias de la Concordia por lograr «el más largo período de paz de la Europa moderna, colaborando a la implantación y difusión en el mundo de valores como la libertad, los derechos humanos, y la solidaridad».

Desde 2016, la Unión encara la inminente salida del Reino Unido, lo que ha precipitado, según las propias instituciones europeas, un proceso de refundación.

Con la entrada en vigor del Tratado de Lisboa, los símbolos de la UE como la bandera, el lema, el himno o el Día de Europa no son jurídicamente vinculantes, aunque todos ellos se encuentran en uso. Pese a esto, dieciséis países miembros declararon su lealtad a los símbolos de la Unión Europea en una declaración anexa al tratado.




Tras el final de la Segunda Guerra Mundial, Europa se encontraba sumida en la devastación. Alemania estaba destrozada, en términos de pérdidas de vidas humanas y daños materiales. Si bien Francia y Reino Unido resultaron oficialmente vencedoras frente a Alemania en el conflicto, ambos países sufrieron importantes pérdidas (aunque menores que las de Alemania) que afectaron gravemente a sus economías y su prestigio a nivel mundial.
La declaración de guerra de Francia y Reino Unido a la Alemania nazi tuvo lugar en septiembre de 1939. Una vez finalizado el conflicto en Europa el 8 de mayo de 1945, el régimen alemán fue responsabilizado por el inicio de la guerra, ya que su política expansionista le había llevado a ocupar y en algunos casos anexar territorios de otros países del continente. Alemania, que perdió una parte considerable de su territorio anterior a la guerra, fue ocupada por ejércitos extranjeros que dividieron su superficie territorial en cuatro partes, tal y como se consensuó en la Conferencia de Yalta.

En los años posteriores, los resentimientos y la desconfianza entre las naciones europeas, dificultaban una reconciliación. En este contexto el ministro francés de asuntos exteriores Robert Schuman defendió decididamente la creación de Alemania Occidental, resultado de la unión de las tres zonas de ocupación controladas por las democracias occidentales, dejando de lado la zona ocupada por la URSS. Schuman, de origen germano-luxemburgués, había poseído las tres nacionalidades (francesa, alemana, luxemburguesa) durante diferentes etapas de su vida. Este hecho le hizo comprender la complejidad de los conflictos europeos y desarrollar pronto un interés por la unificación europea.

En 1946, Winston Churchill dio un discurso en la Universidad de Zúrich, considerado por muchos como el primer paso hacia la integración durante la posguerra. Aunque, generalmente se considera que el verdadero primer paso se dio el 9 de mayo de 1950, cinco años después de la rendición del régimen nazi, cuando Schuman lanzó un llamamiento a Alemania Occidental y a los países europeos que lo deseasen para que sometieran bajo una única autoridad común el manejo de sus respectivas producciones de acero y carbón. Este discurso, conocido como Declaración Schuman, fue acogido de manera dispar dentro de los gobiernos europeos y marcó el inicio de la construcción europea, al ser la primera propuesta oficial concreta de integración en Europa. El hecho consistía en que al someter las dos producciones indispensables de la industria armamentística a una única autoridad, los países que participaran en esta organización encontrarían una gran dificultad en el caso de querer iniciar una guerra entre ellos.

La declaración marcó el inicio de la integración de los estados europeos como un movimiento en contraposición a la anterior tendencia nacionalista y las tensas rivalidades que ocasionó entre los estados de Europa. Esta nueva realidad fue propiciada en gran medida por el fin de la tradicional hegemonía europea en el mundo tras la II Guerra Mundial, que concienció a los europeos de su propia debilidad ante el surgimiento de dos nuevas superpotencias, Estados Unidos y la URSS, que tenían un poder superior al del heterogéneo grupo de estados europeos. Además, las consecuencias del conflicto favorecieron el deseo entre los ciudadanos de crear un continente más libre y justo en el que las relaciones entre países se desarrollaran de forma pacífica para evitar por todos los medios un nuevo enfrentamiento entre los estados europeos.

La propuesta de Robert Schuman fue acogida de forma entusiasta por el canciller de la República Federal de Alemania Konrad Adenauer. En la primavera de 1951, se firma en París el Tratado que institucionaliza la Comunidad Europea del Carbón y del Acero (CECA), concretando la propuesta de Schuman. Alemania, Francia, Italia, Países Bajos, Bélgica y Luxemburgo (conocidos como “los seis”), logran un entendimiento que favorece el intercambio de las materias primas necesarias en la siderurgia, acelerando de esta forma la dinámica económica, con el fin de dotar a Europa de una capacidad de producción autónoma. Este tratado fundador buscaba aproximar vencedores y vencidos europeos al seno de una Europa que a medio plazo pudiese tomar su destino en sus manos, haciéndose independiente de entidades exteriores. El tratado expiró en 2002, a pesar de que su función quedó obsoleta tras la fusión de los órganos ejecutivos y legislativos en el seno de la Comunidad Europea, que adquirió personalidad jurídica, y también gracias al Acta Única Europea de 1986.

En mayo de 1952, ya en plena Guerra fría, se firmó en París un tratado estableciendo la Comunidad Europea de Defensa (CED), que permitía el armamento de Alemania Occidental en el marco de un ejército europeo. Cinco miembros de la CECA ratificaron el tratado, pero en agosto de 1954, los parlamentarios franceses lo rechazaron, como consecuencia de la oposición conjunta de gaullistas y comunistas. Es así que el antiguo Tratado de Bruselas de 1948 es modificado para crear la Unión Europea Occidental (UEO) que fue hasta la entrada en vigor del Tratado de Ámsterdam en 1999 la única organización del continente encargada de la defensa y la seguridad europea. Aunque reforzó el antiguo tratado, la UEO fue una entidad a la sombra de la OTAN, pese a lo cual se encargó durante su existencia de la defensa de los países europeos ante un hipotético ataque.

Un impulso de importancia mayor llega en 1957 con la firma de los Tratados de Roma. "Los seis" deciden avanzar en la cooperación en los dominios económico, político y social. La meta planteada fue lograr un “mercado común” que permitiese la libre circulación de personas, mercancías y de capitales. La Comunidad Económica Europea (CEE) es la entidad internacional, de tipo supranacional, dotada de una capacidad autónoma de financiación institucionalizada por este tratado. Este documento formó una tercera comunidad de duración indefinida, el Euratom.

En 1965, se firma un tratado que fusiona los ejecutivos de las tres comunidades europeas por medio de la creación de la Comisión Europea (CE) y el Consejo de la Unión Europea (CUE). El Acta Única Europea firmada en febrero de 1986 entró en aplicación en julio de 1987, y tuvo por misión el redinamizar la construcción europea, fijando la consolidación del mercado interior en 1993 y permitiendo la libre circulación igualmente de capitales y servicios. Por este tratado, las competencias comunitarias son ampliadas a los dominios de la investigación y el desarrollo tecnológico, medio ambiente y política social. El Acta Única consagró también la existencia del Consejo Europeo, que reúne los jefes de estado y de gobierno e impulsa una iniciativa común en materia de política exterior (la Cooperación Política Europea) así como una cooperación en materia de seguridad.

El Tratado de Maastricht o de la Unión Europea, firmado en febrero de 1992 y en vigor a partir de 1993, introdujo una nueva estructura institucional, la cual se mantuvo hasta la entrada en vigor del Tratado de Lisboa. Dicha estructura institucional estaba compuesta por los conocidos tres pilares de la Unión Europea: el primer pilar era el pilar comunitario, que correspondía a las tres comunidades (la Comunidad Europea, la Comunidad Europea de la Energía Atómica y la antigua Comunidad Europea del Carbón y del Acero); el segundo era el pilar correspondiente a la política exterior y de seguridad común, que estaba regulada en el título V del Tratado de la Unión Europea; y el tercero era el pilar correspondiente a la cooperación policial y judicial en materia penal, cubierta por el título VI del Tratado de la Unión Europea. Estos tres pilares funcionaban siguiendo procedimientos de decisión diferentes, ya que el primer pilar funcionaba mediante el procedimiento comunitario, mientras que los otros dos se regían por el procedimiento intergubernamental. El Tratado de Maastricht también creó la ciudadanía europea y permitió circular y residir libremente en los países de la comunidad, así como el derecho de votar y ser elegido en un estado de residencia para las elecciones europeas o municipales. Con este tratado también se decidió la creación de una moneda única europea, el Euro, que entraría en circulación en 2002 bajo control del Banco Central Europeo.

A lo largo de estos años, la CEE/UE comenzó a expandirse por el continente europeo, fundamentalmente entre los países de la Europa occidental: Reino Unido, Irlanda y Dinamarca en 1973; Grecia en 1981; España y Portugal en 1986; Alemania oriental en 1990; y Austria, Finlandia y Suecia en 1995.

En 1999, entró en vigor el Tratado de Ámsterdam. Este tratado, recogía los principios de libertad, democracia y respeto a los derechos humanos, incluyendo explícitamente el principio de desarrollo sostenible. Dos años después se firmó el Tratado de Niza, que entraría en vigor en 2003. Mientras tanto, el año 2002, se extingue la CECA tras finalizar su periodo de validez (que fueron 50 años), y su ámbito de actuación quedó englobado en el de la Comunidad Europea.

El 1 de mayo de 2004 tuvo lugar la mayor ampliación que se ha dado en la Unión Europea, con la entrada de 10 nuevos miembros de Europa oriental: Estonia, Letonia, Lituania, Polonia, República Checa, Hungría, Eslovaquia, Eslovenia, Malta y Chipre. Más tarde, el 29 de octubre de 2004 se firmó en Roma el tratado constitucional. La ratificación del tratado fue iniciada por la aprobación del Parlamento, pero algunos estados convocaron referendos en 2005. El primero fue el que se celebró en España, donde el documento fue aprobado con el 76,73% de apoyo. Sin embargo, la ratificación alcanzó un obstáculo importante cuando los votantes de Francia y los Países Bajos rechazaron el documento. Esta ratificación en gran medida se detuvo, con sólo unos pocos estados tratando de aprobarlo aún. Luxemburgo siguió adelante con su voto y aprobó la constitución en un 57%. Esto no cambió las cosas, sin embargo, y los dirigentes anunciaron que entraban en un "período de reflexión" sobre el rechazo.

A comienzos del 2007 se incorporaron Rumania y Bulgaria a la Unión Europea, mientras que el 25 de marzo de 2007 (en el 50º aniversario de la firma de los Tratados de Roma) los líderes europeos pusieron fin formalmente al "período de reflexión" con la firma de la Declaración de Berlín. La declaración tenía por objeto dar un nuevo impulso a la búsqueda de un nuevo acuerdo institucional antes de realizar las elecciones europeas de 2009. Adentrado ya el año 2007, el Consejo Europeo acordó que la Constitución había fracasado, a pesar de que la mayoría de las propuestas que incluía el texto se incluyeron posteriormente en la reforma de los tratados de la Unión, en contraposición a la constitución, la cual iba a reemplazar todos los tratados anteriores. De este modo, el 13 de diciembre de 2007, se firmó el conocido como Tratado de Lisboa.

Este tratado tenía como objetivo mejorar el funcionamiento de la Unión Europea mediante la modificación del Tratado de Maastricht y el Tratado constitutivo de la Comunidad Europea (Tratado de Roma). Algunas de las reformas más importantes que introdujo el Tratado de Lisboa fueron la reducción de las posibilidades de estancamiento en la toma de decisiones del Consejo de la Unión Europea mediante el voto por mayoría cualificada, un Parlamento Europeo con mayor peso mediante la extensión del procedimiento de decisión conjunta con el Consejo de la UE, la eliminación de los para entonces obsoletos tres pilares de la Unión Europea, y la creación de las figuras de Presidente del Consejo Europeo y Alto Representante de la Unión para Asuntos Exteriores y Política de Seguridad para dotar de una mayor coherencia y continuidad a las políticas de la UE. El Tratado de Lisboa, que entró en vigor el 1 de diciembre de 2009 también hizo que la Carta de los Derechos Fundamentales de la Unión Europea fuese jurídicamente vinculante para los estados miembros.

Desde el 2008, la economía de la mayoría de los países miembros se ha visto gravemente afectada a raíz de la crisis económica, pese a la cual Croacia consiguió convertirse el 1 de julio de 2013 en el miembro número 28 de la Unión. Así, trascurrido más de medio siglo desde que se produjo la "Declaración Schuman", la UE enfrenta retos como la aplicación del Tratado de Lisboa, la adhesión de los países balcánicos y Turquía, y el "brexit".

<noinclude>

La Unión Europea está formada por 28 países europeos soberanos independientes que se conocen como los estados miembros. La Unión fue fundada por seis países de Europa occidental (Francia, Alemania, Italia, Bélgica, Países Bajos, y Luxemburgo) y se amplió en seis ocasiones, por los cuatro puntos cardinales de la geografía europea. A diferencia de los estados de los Estados Unidos, los estados miembros de la Unión Europea no están obligados a una forma republicana de gobierno. La Unión está compuesta de veintiuna repúblicas y siete monarquías, de las cuales seis son reinos y una es un ducado (Luxemburgo).

En el territorio de la Unión Europea, los tres países más extensos son 1° Francia (con una superficie más grande que el conjunto de la suma de Alemania, Reino Unido y Países Bajos), 2° España y 3° Suecia; y los tres países menos extensos son Malta (con una superficie más de dos mil veces inferior al país más grande de la Unión Europea), Luxemburgo y Chipre. En cuanto a jurisdicción sobre el Mar, un país de la Unión Europea, Francia (con más de 11 millones de Km²), tiene la más extensa Zona Económica Exclusiva del mundo.

El territorio de la Unión Europea consiste en el conjunto de territorios de sus 28 Estados miembros con algunas excepciones que se exponen a continuación. El territorio de la UE no es el mismo que el de Europa, ya que, en primer lugar, hay estados europeos que se encuentran fuera de la UE, como Islandia, Suiza, Noruega y Rusia. Además, ciertos territorios europeos de los estados miembros no forman parte de la UE (por ejemplo las Islas del Canal y las Islas Feroe). Tampoco forman parte de la UE varios territorios situados fuera del continente asociados a los estados miembros (por ejemplo, Groenlandia, Aruba, las Antillas Neerlandesas, las Colectividades Territoriales francesas, y todos los territorios no europeos asociados con el Reino Unido). Por el contrario, sí hay ciertos territorios de ultramar que son parte de la UE pese a estar situados fuera del continente europeo, como las Azores, Islas Canarias, Guayana Francesa, Guadalupe, Madeira, Martinica, San Martín, La Reunión y Mayotte.

La superficie combinada de los estados miembros de la UE cubre un área de 4 324 782 kilómetros cuadrados. El paisaje, el clima, y la economía de la UE se ven influidas por sus costas, que suman 65 992,9 kilómetros de largo. La UE tiene la tercera costa más larga del mundo después de Australia y Canadá. La combinación de los estados miembros comparte fronteras terrestres con 21 estados no miembros para un total de 13 271 kilómetros, la quinta frontera más larga del mundo.

En Europa la UE tiene fronteras con Noruega, Rusia, Bielorrusia, Ucrania, Moldavia, Suiza, Liechtenstein, Andorra, Mónaco, San Marino, Ciudad del Vaticano, Turquía, la República de Macedonia, Bosnia y Herzegovina, Albania, Montenegro y Serbia. Por último, tiene fronteras con: San Martín en el Mar Caribe; Brasil y Surinam en América del Sur y con Marruecos en África.

Algunos estados miembros poseen territorios fuera del continente europeo, los cuales pueden formar parte de la Unión; son denominados generalmente regiones ultraperiféricas o territorios de ultramar. Las ciudades españolas de Ceuta y Melilla, que como tales forman parte de la Unión Europea, se encuentran en África, junto a Marruecos, pero no son consideradas regiones ultraperiféricas por parte de la Unión Europea.

Hasta la entrada en vigor del Tratado de Lisboa no se especificaba cómo un país podía salir de la Unión (aunque Groenlandia, un territorio de Dinamarca, se retiró en 1985 siendo necesario para ello la modificación de varios tratados) pero esto ya no ocurre con el Tratado de Lisboa, ya que este contiene un procedimiento formal para la retirada.

Los miembros de la Unión han crecido desde los seis estados fundadores (Bélgica, Francia, Alemania, Italia, Luxemburgo y Países Bajos) a los 28 que conforman la Unión Europea:

Hay una serie de territorios de ultramar de los Estados miembros, que son legalmente parte de la Unión Europea, pero tienen ciertas exenciones en función de su lejanía de Europa. Estas "regiones ultraperiféricas" disponen de una aplicación parcial de la ley de la UE y en algunos casos se encuentran fuera del Espacio Schengen. Todos estos territorios utilizan el euro como moneda y son:

Las regiones ultraperiféricas están formadas por varias regiones insulares y una región en el noreste del continente sudamericano, a miles de kilómetros de Europa, pero que integran de derecho la Unión Europea y que forman un grupo peculiar y bien definido en el seno de esta. Esta situación compartida ha llevado a las regiones ultraperiféricas a estrechar lazos de unión y afirmar su voluntad de cooperar entre ellas para lograr que la Unión Europea no olvide las características de estas regiones, para conseguir un desarrollo sostenible a largo plazo, y dotarlas de una posición de igualdad respecto del resto del territorio de la Unión.

Los representantes de las regiones ultraperiféricas, están llevando a cabo unas reuniones periódicas de la Conferencia de Presidentes de las regiones ultraperiféricas, con la idea de preparar un programa de cooperación entre las RUP, el RUP PLUS.

Hay territorios de los Estados miembros en los que no se aplica toda la legislación de la Unión Europea, por lo que su estatus es entonces más próximo al de las regiones ultraperiféricas (RUP), aunque sin tener los fondos estructurales que tienen esos territorios asignados. Al igual que las RUP, y a diferencia de los países y territorios de ultramar, estos territorios si que forman parte del territorio de la Unión Europea.

Un caso especial es el Norte de Chipre, donde la legislación de la Unión Europea no se aplica, pese a ser parte del territorio jurídico de la Unión, ya que sus ciudadanos, los cuales votaron a favor de la adhesión de Chipre a la Unión Europea y de la reunificación de Chipre, también votan a los representantes chipriotas del Parlamento Europeo. Este caso, es una excepción del Tratado de adhesión de Chipre, ya que se está esperando una evolución en las negociaciones entre las dos repúblicas chipriotas. El resultado de esta división de la isla es la creación de una línea de demarcación bajo mandato internacional de la ONU, por el norte y el sur, y dos zonas de soberanía británica, la cual está exenta en parte a la aplicación del derecho comunitario, ya que en las bases soberanas del Reino Unido de Acrotiri y Dhekelia, la legislación de la UE se aplica sólo a los residentes del Reino Unido y a los de nacionalidad chipriota.

En el Mar Báltico, las Islas Åland de Finlandia también cuentan con un estatus especial. Estas islas, las cuales disfrutan de una amplia autonomía, tuvieron un referéndum separado del de Finlandia relativo a la adhesión de su país a la UE, en el cual se aprobó su adhesión a la Unión, aunque con algunas excepciones.

Otros casos son los municipios que tienen un estatus especial por razones geográficas o históricas. Estos estatus se refieren al uso del euro y del IVA. Sobre todo destacan los casos de los exclaves alemanes e italianos de Büsingen am Hochrhein y Campione d'Italia, respectivamente. También tiene estatus de territorio especial la localidad italiana de Livigno, la cual pese a no ser un exclave se beneficia del estatus extraterritorial regulado desde los siglos XIX y XX.

En el Mar del Norte, la isla alemana de Heligoland, aunque es parte del territorio de la Unión Europea, está excluido de la unión aduanera y no está sujeta a régimen fiscal alemán. Al igual que las ciudades autónomas de Ceuta y Melilla y el resto de plazas de soberanía española en África, las cuales tienen un estatus especial respecto al IVA, la PAC y la PPC.

El Monte Athos o Estado Monástico Autónomo de la Montaña Sagrada también tiene un estatus especial dentro de República Helénica, ya que el Monte Athos forma parte del espacio Schengen y de la Unión Europea, aunque sólo están autorizados a entrar en su territorio aquellos varones que cuenten con una autorización, además de esto, el acceso está prohibido a toda "criatura femenina" (excepto gallinas y gatos).

Finalmente, la isla Clipperton, bajo la administración directa del gobierno francés, por lo que es parte del territorio de la Unión. Sin embargo, la isla no es parte del espacio Schengen y, ya que no hay habitantes permanentes, no hay elecciones al Parlamento Europeo.

Los países y territorios de ultramar son países que no forman parte del territorio comunitario (a diferencia de las regiones ultraperiféricas). Los ciudadanos de los países y territorios de ultramar tienen la nacionalidad de los estados miembros de que dependen (sin embargo, en algunos casos sus ciudadanos no poseen una ciudadanía plena de tales estados).

Existen veinticinco países y territorios de ultramar:


Para que un Estado europeo se incorpore a la Unión Europea debe cumplir unas condiciones económicas y políticas conocidas como los criterios de Copenhague, por haberse tomado el correspondiente acuerdo en el Consejo Europeo de 1993 celebrado en la capital danesa. Los criterios de Copenhague establecen cuándo un país candidato está listo para adherirse a la Unión. Entre los principales criterios están los siguientes:
Hay cinco países candidatos oficiales para formar parte de la UE, los cuales son Turquía (desde 2004), la República de Macedonia (desde 2005), Montenegro (desde 2010), Serbia (desde 2012) y Albania (desde 2014). Un informe de la Comisión Europea de octubre de 2009 valoró positivamente a Macedonia para una futura ampliación, pero instó a retrasar el proceso con Turquía.

Actualmente son candidatos potenciales Bosnia y Herzegovina y el territorio de Kosovo (bajo administración interina de la ONU), según lo dispuesto en la resolución 1244 del Consejo de Seguridad de las Naciones Unidas. Aunque Bosnia y Herzegovina ha mostrado su interés en pertenecer al grupo europeo, su adhesión se enfrenta a muchos problemas económicos y políticos, lo cual llevará a que el país lleve a cabo grandes reformas en su sistema económico, político y judicial. El caso de Kosovo es diferente, ya que este territorio cuenta con un estatus especial a la hora de su posible entrada en la Unión Europea, ya que la Comisión Europea lo reconoce como candidato potencial pero no como un país independiente, sino que se refiere a él con la denominación "Kosovo según la Resolución 1244", ya que los Estados miembros se encuentran divididos entre aquellos que lo reconocen como un país independiente y los que no han aceptado la declaración de independencia de Kosovo y lo consideran parte integrante de Serbia.

A pesar de que Noruega se encuentra en el Espacio Económico Europeo, es parte del espacio de Schengen, y que participa en muchos de los programas, instituciones y actividades de la UE, los noruegos impidieron el cumplimiento de la agenda de su gobierno para incorporarse a la UE en dos ocasiones, mediante referéndum, en 1972 y 1994. Del mismo modo Islandia es miembro del EEE y del espacio de Schengen, y debido a una grave crisis económica inició los trámites para ser un miembro de la UE, pero tras las elecciones del 2013, donde vencieron los partidos de centroderecha en contra de la adhesión de Islandia a la UE, el nuevo ministro de Exteriores de Islandia informó al comisario de Ampliación de la decisión del nuevo Ejecutivo de no seguir adelante con las negociaciones de adhesión a la Unión Europea hasta la convocatoria de un referéndum sobre esta cuestión. Algo parecido ocurre con Suiza que, aunque también pertenece al espacio de Schengen, rechazó su adhesión en votaciones realizadas en 1994 y 2001.

En los Tratados anteriores al Tratado de Lisboa no había previsto ningún procedimiento jurídico que regulara la retirada de los estados. Así por ejemplo, en la Convención de Viena no se preveía ni la denuncia ni el retiro de un Estado miembro. El propio Tribunal de Justicia de las Comunidades Europeas reconocía el carácter irrevocable de los compromisos asumidos por los Estados. Sin embargo, la no previsión de un procedimiento jurídico de retirada en los Tratados no es motivo suficiente para impedir que un Estado decida sobre su continuidad en la Unión Europea.

El 23 de junio de 2016 se realizó el Referéndum sobre la permanencia del Reino Unido en la Unión Europea, en el cual la opción de "Salir de la UE" gana con un 51.9% mientras que "Continuar en la UE" obtiene un 48.1%, sin embargo, en Escocia, Irlanda del Norte y Gibraltar además de la mayoría de Londres, predominó la opción de la permanencia. Tras los resultados del referéndum, el Primer ministro David Cameron anunció su dimisión del cargo, así, el 13 de julio del mismo año, Theresa May asumió el cargo en su reemplazo, David Cameron argumentó que un liderazgo fresco debe llevar al país a la opción elegida en la votación.
Este referéndum inicia el proceso de retirada del Reino Unido de la Unión Europea, siendo este proceso a largo plazo, estimándolo al menos a unos dos años de tramitación concluyendo el 2019.

Las Instituciones de la Unión Europea son los organismos políticos e instituciones en los que los estados miembros delegan parte de sus poderes y soberanía. Con ello se busca que determinadas decisiones y actuaciones institucionales provengan de órganos de carácter supranacional cuya voluntad se aplica en el conjunto de los estados miembros, desapoderando así a los órganos nacionales de cada país.

Las normas y procedimientos que las instituciones deben seguir se establecen en los tratados, negociados por el Consejo Europeo y en conferencias intergubernamentales y ratificadas por los parlamentos nacionales de cada Estado. El Tratado de Lisboa, modifica nuevamente el Tratado de la Unión Europea, pero también el TCE, que pasaría a llamarse "Tratado sobre el Funcionamiento de la Unión Europea" (TFUE).
La legitimidad de la producción normativa de la Unión tiene una doble vertiente: legitimidad internacional en la acción del Consejo y el Consejo Europeo, por un lado, en tanto que la Unión es una organización internacional regida por Derecho Internacional y convencional; y democrática, por otro, ya que el Parlamento Europeo recoge el principio de formación democrática del Derecho, al ser una Institución cuyos miembros son elegidos en unas elecciones directamente por los ciudadanos.

El Tratado de Lisboa ha consolidado la transformación formal del marco institucional supremo con siete instituciones. Las tres principales en el proceso de toma de decisiones son el Parlamento Europeo, el Consejo de la Unión Europea y la Comisión Europea. También cobra gran importancia el Consejo Europeo como institución que determina la dirección y las prioridades de la Unión.

El Parlamento Europeo es el parlamento de la Unión Europea. Desde 1979, es elegido directamente cada cinco años en las elecciones europeas. Por lo tanto, es la primera institución supranacional directamente elegida del mundo y el órgano representativo de alrededor de 490 millones de personas, quienes constituyen el segundo electorado democrático más grande del mundo (después de la India). El Parlamento es considerado la "primera institución" de la Unión Europea: es mencionado en primer lugar en los tratados y su Presidente tiene preferencia protocolaria sobre todas las demás autoridades a nivel europeo. Comparte con el Consejo la competencia legislativa y presupuestaria, teniendo el control sobre el presupuesto de la Unión Europea. La Comisión Europea, el órgano ejecutivo de la Unión, es responsable ante el Parlamento. En concreto, el Parlamento Europeo elige al Presidente de la Comisión, aprueba (o rechaza) la designación de la Comisión en su conjunto, e incluso le puede destituirla como órgano presentando una moción de censura.

El actual Presidente del Parlamento Europeo es el italiano Antonio Tajani que fue elegido en enero de 2017 y que preside una cámara compuesta por una gran variedad de partidos asociados en grupos. Los dos principales grupos del Parlamento Europeo (juntos poseen el 61% de los escaños) son el Grupo del Partido Popular Europeo y el Grupo de la Alianza Progresista de Socialistas y Demócratas.

El Consejo, antes Consejo de la Unión Europea (CUE), comúnmente conocido como Consejo de Ministros, reúne en su seno a los representantes de los Gobiernos de los Estados miembros en distintas formaciones, cuyos intereses nacionales incrusta en el proceso decisorio guiado por la búsqueda de un acuerdo común. El Consejo ejerce junto con el Parlamento Europeo el poder legislativo de la Unión. Ostenta la titularidad formal de importantes potestades ejecutivas, pero cuyo ejercicio debe atribuir por imperativo constitucional a la Comisión. Si bien en los últimos tiempos sus funciones legislativas, antes exclusivas, han ido debilitándose en favor de la igualdad con el Parlamento Europeo, el paralelo declive político de la Comisión parece estar propiciando un desplazamiento de retorno simultáneo al Consejo del centro de gravedad del poder decisorio y ejecutivo, que en ocasiones más parece residir en este órgano que en el propio Ejecutivo comunitario. Ello no obstante, el Consejo aparece cada vez más deslumbrado por su "alter ego" en las alturas, el Consejo Europeo.

El Consejo es, pues, cámara co-legisladora donde se hallan representados los Estados de la Unión a través de sus gobiernos nacionales, asegura su plena participación en igualdad de condiciones, en garantía del llamado "principio de representación nacional". Cuando delibera y decide sobre un acto legislativo, las sesiones del Consejo son públicas.

La Presidencia del Consejo cambia entre estados miembros cada seis meses: de enero a junio y de julio a diciembre. Los Gobiernos trabajan aunando fuerzas para manifestarse con una sola voz en cuestiones de política exterior, asistidos por el Alto Representante de la Unión para Asuntos Exteriores y Política de Seguridad.

El Consejo Europeo, que no debe confundirse con el Consejo de Europa o con el Consejo de la Unión Europea, es un organismo político de carácter predominantemente intergubernamental, conformado por los jefes de Estado o de gobierno de los estados miembros de la Unión Europea junto con el presidente permanente del Consejo y el presidente de la Comisión Europea. Sus funciones son de orientación política y de jefatura colectiva simbólica, fijando las grandes directrices y objetivos de la Unión en los ámbitos más relevantes; la potestad legislativa le está expresamente vedada por los Tratados. Los miembros del Consejo Europeo se citan periódicamente en reuniones conocidas como "Cumbres europeas". Sus oficinas se encuentran en el Justus Lipsus de Bruselas, sede del Consejo de la Unión Europea.

El presidente del Consejo Europeo o informalmente el presidente de la Unión, es una de las más altas posiciones institucionales de la Unión Europea, y sin duda la más simbólica. Su proyección exterior se corresponde con la más alta representación de la UE en el ámbito de la política exterior y de seguridad común, al nivel de los jefes de Estado. Su mandato tiene una duración de dos años y medio renovables una sola vez, sustituyéndose así el viejo sistema rotatorio anterior al Tratado de Lisboa.

La Comisión Europea ("Comisión de las Comunidades Europeas" hasta la entrada en vigor del Tratado de Niza) es la rama ejecutiva de la Unión Europea. Este cuerpo es responsable de proponer la legislación, la aplicación de las decisiones, la defensa de los tratados constitutivos y, en general, se encarga del funcionamiento ordinario de la UE. Se le encomienda la vigilancia en el cumplimiento del interés supremo de la Unión, separado del individual de cada Estado miembro.

Una vez nombrada, la Comisión Europea sólo puede ser destituida mediante una moción de censura aprobada por una mayoría de dos tercios en el Parlamento Europeo, lo que la dota de un margen de autonomía superior al de la mayoría de los ejecutivos en sistemas parlamentarios.

Al frente de la Comisión se encuentra el Presidente, que ostenta la máxima representación de la misma y ocupa el primer puesto en la cadena de jerarquía, preeminencia que viene reforzada por la legitimidad democrática directa e individualizada que le aporta al cargo su elección directa por el Parlamento Europeo.

Conforme a su posición principal, el Presidente es también quien está al frente de los demás miembros de la Comisión, los denominados Comisarios, que tienen atribuidas las competencias y los servicios que decida asignarles el Presidente de la Comisión a través de la carta de nombramiento que envía a los titulares y al Parlamento Europeo. Cada Comisario europeo es responsable de los departamentos (direcciones generales y servicios) y, en su caso, Agencias ejecutivas que les asigne el Presidente. Estos ámbitos competenciales se conocen en la jerga comunitaria por el nombre de "carteras", y dado que no tienen estructura administrativa propia, son gestionados por el propio Comisario y, en su nombre, por su gabinete.

Además, de las ya mencionadas, el marco institucional de la Unión Europea cuenta con otras tres instituciones no políticas: el Tribunal de Justicia de la Unión Europea, el Tribunal de Cuentas y el Banco Central Europeo.

Los órganos son asimilables a instituciones de menor rango (aunque no gozan de ese estatuto). Aunque sus funciones son concretas, tienen competencias que van mucho más allá de la simple gestión y gozan de independencia en el ejercicio de sus funciones. Los organismos son estructuras subsidiarias de otras instituciones pero con autonomía funcional, generalmente versada en ámbitos especializados de gestión vicaria.


Las agencias europeas son organismos especializados que se encargan de un aspecto específico (científico, técnico, jurídico o social) de la estructura de la Unión Europea. Se encuentran distribuidas en los países miembros de la UE. Su función es proporcionar cooperación entre los estados miembros y ayuda a sus ciudadanos en las áreas de su competencia.

Estas agencias han contribuido de manera significativa al funcionamiento efectivo de la UE, gracias a su especialización en áreas determinadas de la arquitectura comunitaria. Al ser, en su mayoría, instituciones descentralizadas e independientes han servido para fortalecer el carácter plurinacional de la Unión.

Las agencias se dividen en cuatro categorías englobadas en dos tipos: las agencias “reguladoras” y las “ejecutivas”. Las agencias reguladoras, que se dividen a su vez en tres categorías, se encuentran descentralizadas y se encargan de una competencia concreta sin límite de tiempo; se conocen como agencias de los “tres pilares”. Las últimas, las agencias ejecutivas, se encuentran en la sede de la Comisión Europea (Bruselas o Luxemburgo) y se han creado por un tiempo determinado, para realizar una tarea específica.

Actualmente, el presidente del Consejo Europeo es el político polaco Donald Tusk, que fue designado en agosto de 2014 y asumió su mandato el 1 de diciembre del mismo año. Mientras que la comisión actual está presidida por Jean-Claude Juncker desde el 1 de noviembre de 2014. En estos momentos, la Comisión cuenta con 28 comisarios, 7 de ellos son vicepresidentes.

Uno de los rasgos diferenciadores de la Unión Europea frente a otras organizaciones internacionales es el alto grado de desarrollo de sus instituciones de gobierno. El gobierno de la Unión Europea siempre ha oscilado entre el modelo de conferencia intergubernamental, donde los estados conservan el conjunto de sus prerrogativas y el modelo supranacional donde una parte de la soberanía de los estados es delegada a la Unión. En el primer caso, las decisiones comunitarias son de hecho tratadas entre estados y deben adoptarse por unanimidad. Este modelo, cercano al principio de las organizaciones intergubernamentales clásicas, es defendido por la corriente euroescéptica. Según ellos, son los jefes de estado o de gobierno quienes tienen la legitimidad democrática para representar a los ciudadanos y son entonces las naciones quienes deben controlar las instituciones de la Unión. El segundo caso es el de la corriente eurófila, que estima que las instituciones deben representar directamente a los ciudadanos mediante un modelo de federalismo y elecciones directas. Para ellos, una Unión Europea federal resolvería muchos problemas relacionados con la soberanía, la legitimación democrática, la división de poderes comunitaria, el reparto de competencias, la fiscalidad y la aspiración a un modelo de bienestar común.

De este modo, el modelo de gobierno de la Unión es un modelo híbrido: por un lado está el Consejo de la Unión Europea, el cual es el representante de los estados, y en el que las decisiones no requieren unanimidad, y donde los votos de cada estado son ponderados por su peso demográfico; y por otro lado está el Parlamento Europeo, el cual es la única institución europea elegida por sufragio universal, es decir, es la única que representa a los ciudadanos.

Por lo que la Unión Europea, en su calidad de comunidad de Derecho y de acuerdo con su personalidad jurídica única, se ha dotado desde la entrada en vigor del Tratado de Maastricht de un marco institucional y de gobierno único que funciona en régimen de democracia representativa. De acuerdo con el enunciado del artículo 3.1 del Tratado de la Unión, el marco institucional ""tiene como finalidad promover sus valores, perseguir sus objetivos, defender sus intereses, los de sus ciudadanos y los de los estados miembros, así como garantizar la coherencia, eficacia y continuidad de sus políticas y acciones"". En el funcionamiento y la estructura orgánica de la Unión se distinguen los que de acuerdo con la denominación que les otorgan los Tratados son, por este orden, las instituciones, los órganos y los organismos, incluidas las agencias de la Unión.

Las competencias que tiene el gobierno de la Unión Europea, son las que se citan a continuación:

Un partido político europeo es una organización que sigue un programa político y está formada por partidos e individuos de distintos países y que, por consiguiente, está representada en algún Estado miembro de la Unión Europea. En concreto, según el Tratado de la UE los partidos políticos a escala europea contribuyen a formar la conciencia política europea y a expresar la voluntad de los ciudadanos de la Unión.

Desde julio de 2004 los partidos políticos europeos tienen a su disposición una financiación anual por parte del Parlamento Europeo. Esta financiación está destinada a abarcar aproximadamente hasta el 85% de los gastos de los partidos sufragando gastos que estén directamente relacionados con los objetivos establecidos en el programa político del partido (reuniones, gastos administrativos, campañas relacionadas con las elecciones europeas, publicaciones, etc.). Sin embargo, tal subvención no se puede utilizar en gastos de campañas o financiación de partidos o candidatos de comicios no europeos, así como el pago de las deudas y gastos relacionados con su amortización.

Para ser poder aspirar a dicha financiación pública, las organizaciones antes mencionadas deben poseer personalidad jurídica propia en el Estado miembro donde tenga su sede, tener representación significativa en al menos una cuarta parte de los estados de la Unión, respetar la libertad, la democracia, los derechos humanos, las libertades fundamentales, así como el Estado de derecho (y manifestarlo en particular en su programa, estatutos y actividades); y finalmente haber participado en las elecciones europeas, o haber manifestado su intención de hacerlo.

Los partidos políticos europeos forman los grupos políticos en el Parlamento Europeo, y para ello se necesitan al menos 25 diputados de una quinta parte de los estados miembros. Tras la aprobación de los presidentes de los grupos, los escaños en el hemiciclo del Parlamento Europeo se asignan a los diputados con arreglo a su adscripción política. También puede darse el caso de que algún diputado no pertenezca a ningún grupo, por lo que formará parte de los "no inscritos".

Actualmente el Parlamento está compuesto por 7 grupos políticos: el Grupo del Partido Popular Europeo, el Grupo de la Alianza Progresista de Socialistas y Demócratas, el Grupo de la Alianza de los Demócratas y Liberales por Europa, el Grupo de los Conservadores y Reformistas Europeos, el Grupo de Los Verdes / Alianza Libre Europea, Grupo Confederal de la Izquierda Unitaria Europea / Izquierda Verde Nórdica y Europa de la Libertad y la Democracia.

Dado que el Parlamento Europeo no elige a ningún gobierno, en él no hay grupos "gubernamentales" ni "de oposición". En vez de la confrontación predomina la búsqueda de consensos entre los partidos mayoritarios, en los que tradicionalmente tienen un peso especial los dos grupos más grandes, el PPE (democristianos) y el PSE (socialdemócratas). Esto se debe a que ningún grupo político alcanza la mayoría absoluta necesaria para ganar una votación, a diferencia de otros parlamentos donde sólo es necesaria la mayoría simple.

El derecho de la Unión Europea es el conjunto de normas y principios que determinan el funcionamiento, corporación y competencias de la Unión Europea. Se caracteriza por tratarse de un orden jurídico "sui generis", diferenciado tanto del Derecho internacional como del orden jurídico interno de los estados miembros. El sistema legal comunitario se articula sobre el conjunto de competencias que los estados han atribuido a la Unión por la vía del Derecho.

El Derecho originario es aquel contenido en los diversos tratados que los estados miembros suscriben, siendo las fuentes de mayor rango, y aquellas que posibilitan la aparición del Derecho derivado, que está sometido al originario. El Derecho derivado no sólo cederá en caso de contradicción con el originario, sino que además debe estar fundamentado y originado en los diferentes Tratados que lo componen.

Los Tratados de la Unión Europea son de dos tipos fundamentalmente. De un lado están los tratados fundacionales, en los cuales se incluyen todas las normas contenidas en el Tratado de la Comunidad Europea del Carbón y del Acero (mientras existió), el Tratado de la Comunidad Económica Europea y el Tratado de la Comunidad Europea de la Energía Atómica.

El resto de tratados son modificativos y complementarios, incluyéndose en esta categoría los tratados que han modificado las disposiciones fundacionales. Los más importantes son: el Tratado de fusión, el Acta Única Europea, el Tratado de la Unión Europea, el Tratado de Ámsterdam, el Tratado de Niza y el Tratado de Lisboa. Aunque también son tratados modificativos los Tratados de adhesión de cada uno de los estados que se han ido adhiriendo a la Unión.

El derecho derivado es aquel que se ha desarrollado a través de las distintas normas que han aprobado las distintas instituciones europeas. Las normas que pueden aprobar estas instituciones son: los reglamentos, las directivas y las decisiones.

Los reglamentos son normas jurídicas emanadas de las instituciones europeas que poseen efecto directo en los países miembros, y que prevalecen sobre el Derecho nacional de cada uno de ellos. Existen cuatro procedimientos para la aprobación de reglamentos. En primer lugar, el reglamento será adoptado por el Consejo a propuesta de la Comisión y con la aprobación del Parlamento. Por otro lado, la Comisión podrá dictar reglamentos por iniciativa propia en los casos previstos por los Tratados, así como cuando reciba la correspondiente delegación del Consejo para tal emisión reglamentaria.

Las directivas comunitarias son mandatos dirigidos a uno o varios países miembros, siendo competentes para su emisión el Consejo; la Comisión; y el Consejo junto con el Parlamento. Su rasgo más característico es la ausencia de eficacia directa en los Ordenamientos a los que va dirigida, necesitando de una transposición por parte del Estado miembro para que entren en vigor y hagan nacer en los ciudadanos derechos y obligaciones. De esta manera, la directiva contiene unos objetivos que los estados habrán de cumplir usando los medios del Derecho interno, dentro del plazo indicado. El incumplimiento del deber de transponer las directivas no hace decaer el derecho del ciudadano de exigirle al Estado el cumplimiento de sus obligaciones (responsabilidad vertical limitada).

Finalmente, las decisiones son más limitadas porque, aun teniendo carácter obligatorio, no suelen tener carácter general, sino que se dirigen a destinatarios precisos. Se pueden comparar con los actos administrativos en el ámbito interno.

La Carta de los derechos fundamentales de la Unión Europea es el texto en el que se recogen todos los derechos civiles, políticos, económicos y sociales de los ciudadanos europeos y de todas las personas que viven en el territorio de la Unión.

La carta no forma parte del Tratado de Lisboa (estaba previsto que formara parte de la Constitución Europea, pero al no aprobarse esta, se modificó la previsión), pero por la remisión en el artículo 6 del Tratado de la Unión Europea tras la reforma de Lisboa se hace vinculante para todos los estados, excepto Reino Unido y Polonia. En 2009, el Consejo Europeo aseguró a la República Checa que en la siguiente reforma del Tratado, esa cláusula de excepción se extendiese también a este país.

Los derechos fundamentales son la dignidad, la libertad, la igualdad, la solidaridad, la ciudadanía y la justicia, los cuales ya se recogen en el Convenio Europeo para la Protección de los Derechos Humanos, en la Carta Social Europea del Consejo de Europa, en la Carta Comunitaria de los Derechos Sociales Fundamentales de los Trabajadores, y a su vez en las propias constituciones de los estados miembros de la Unión, así como en otros convenios internacionales que han firmado los estados de la Unión Europea.

Entre las grandes prioridades de la Unión Europea figura la de crear un espacio de justicia, libertad y seguridad. El Tratado de Lisboa introduce cambios importantes en las actuales normas europeas sobre libertad, seguridad y justicia y facilita una actuación más amplia, legítima, eficaz, transparente y democrática de la UE en este campo. Antes de su entrada en vigor, las decisiones importantes en esta materia tenían que adoptarse por unanimidad en el Consejo, mientras que al Parlamento y al Tribunal de Justicia Europeos les correspondía un papel menor.
La actuación de la Unión Europea con respecto a la cooperación policial y judicial en asuntos penales se ve facilitada al suprimirse la distinción entre diferentes ámbitos políticos (los denominados "pilares") que antes caracterizaba a la estructura institucional.

No obstante, los estados miembros tienen la posibilidad de emprender iniciativas legislativas sobre cooperación policial operativa, justicia penal y cooperación administrativa (siempre que cuenten con el respaldo de una cuarta parte del total de países). La Comisión Europea asume el papel como guardiana de los Tratados y como garante, junto al Tribunal de Justicia Europeo, de la correcta aplicación de todas las decisiones. Los Parlamentos nacionales participan de manera más activa en el examen y la elaboración de dictámenes sobre temas de justicia, libertad y seguridad.

El Tratado de Lisboa garantiza las libertades y los principios enunciados en la Carta de los Derechos Fundamentales de la Unión Europea, cuyas disposiciones pasan a ser jurídicamente vinculantes. Por su parte, el Tribunal de Justicia obtiene más poderes para asegurar la correcta aplicación de la Carta. Todos estos cambios contribuyen a un proceso decisorio más amplio, legítimo, eficaz, transparente y democrático para el espacio común de libertad, seguridad y justicia, y ponen fin a los repetidos bloqueos de propuestas a que daba lugar el principio de unanimidad.

Es de señalar, no obstante, que tres estados miembros (Irlanda, Reino Unido y Dinamarca) han juzgado necesario negociar o prorrogar ciertas disposiciones particulares sobre aspectos concretos de justicia, libertad y seguridad para mantener algunos puntos de sus normativas nacionales.

La Alta Representante de la Unión para Asuntos Exteriores y Política de Seguridad dirige un nuevo servicio diplomático en el que se integran de inmediato, fusionadas, las delegaciones internacionales del Consejo y de la Comisión presentes en cerca de 125 países, así como las Representaciones especiales de la Política Exterior y de Seguridad Común de la Unión Europea.

Este servicio es el Servicio Europeo de Acción Exterior ("SEAE", o también simplemente "Servicio Exterior"), creado el 1 de diciembre de 2010, según lo previsto por el Tratado de Lisboa. Como servicio diplomático que es, tiene por cometido el apoyar y asistir en el ejercicio de sus funciones al Alto Representante, como máximo responsable de la acción exterior de la Unión, en todos los ámbitos de su actividad.

Además, la Unión Europea también cuenta con una política de vecindad, la cual persigue que la UE no sea un ente ajeno a su entorno ni desvinculado de sus vecinos. Con estas políticas se busca intensificar las relaciones bilaterales con algunas antiguas repúblicas soviéticas así como los estados de la cuenca sur del Mediterráneo. Dentro de este contexto, se está desarrollando un gran proyecto, el , enfocado a largo plazo a buscar una relación de acercamiento entre la UE y la Liga Árabe. En relación con los vecinos del este, existe otra iniciativa, la Asociación Oriental, llevada a cabo entre el bloque comunitario y las antiguas repúblicas soviéticas.

Todo esto hace que el efecto de la política exterior de la Unión Europea se sienta a través del proceso de ampliación, ya que el atractivo que para varios estados tiene adquirir la calidad de miembro es un factor importante que contribuye a la reforma y a la estabilización de los países del antiguo bloque comunista en Europa.

Por lo que en los últimos años la Unión Europea, a través de la Comisión Europea, ha ganado mayor representación en organismos como el G8 o el G20, a través del Alto Representante de la Unión, aunque los estados miembros se representan en la Organización Mundial del Comercio a través de su comisario comercial.

Finalmente también hay que destacar que la Unión Europea es el mayor donante mundial de ayuda humanitaria, y la principal financiadora de las agencias de Naciones Unidas implicadas en la ayuda humanitaria y la cooperación para el desarrollo, a través del Departamento de Ayuda Humanitaria y Protección Civil de la Comisión Europea (ECHO).

La defensa y la seguridad son tradicionalmente materias de soberanía nacional, aunque la política de la Unión Europea en esta área fue establecida como el segundo de los tres pilares de la Unión en el Tratado de Maastricht de 1992, aunque no fue hasta el Tratado de Ámsterdam (1997) cuando se definieron los objetivos de la "Política Exterior y de Seguridad Común" (PESC).

En la cumbre de Helsinki de diciembre de 1999, el Consejo Europeo aprobó la creación de nuevos órganos políticos y militares permanentes, como el Estado Mayor de la Unión Europea ("EMUE"). El EMUE es un departamento de la Unión Europea, responsable de supervisar las operaciones en el ámbito de la Seguridad Común y Política de Defensa. Este departamento depende directamente del gabinete del Alto Representante de la Unión para Asuntos Exteriores y Política de Seguridad, actualmente Federica Mogherini, y se ocupa de la alerta temprana, la evaluación de la situación y el planeamiento estratégico de las misiones Petersberg (misiones humanitarias, mantenimiento de la paz, gestión de crisis) y de todas las operaciones dirigidas por la UE. Formalmente, el EMUE, forma parte de la Secretaría General del Consejo de la Unión Europea, y ha dirigido una serie de despliegues militares desde su creación.

El potente impulso que el Tratado de Lisboa ha supuesto en el ámbito institucional para la política común de seguridad y defensa, parece apuntar, con el fuerte impulso de un núcleo duro de países encabezados por Francia (Italia, España, Polonia y, en menor medida, Alemania), a un relanzamiento significativo de la política común de seguridad y defensa.

En 2011, la Unión Europea era, en su conjunto, la primera potencia económica del mundo, superando a los Estados Unidos. Según los datos del FMI ese año, el PIB (nominal) de la UE fue de 15,65 billones de dólares (el estadounidense fue de 15,29 billones). Por su parte, el PIB (nominal) per cápita de la UE en 2011 fue de 34 500 dólares, por lo que se sitúa en el puesto número 38 a escala global.

Aun así, desde 2009 la economía europea se encuentra en una crisis económica, la denominada crisis del euro, que ha provocado que el crecimiento económico en estados como Grecia, Irlanda, Portugal, Chipre, España o Italia haya sido negativo en algunos ejercicios. Las causas de la crisis eran diferentes según el país. En algunos de ellos, la deuda privada surgida como consecuencia de una burbuja en el precio de los activos inmobiliarios fue transferida hacia la deuda soberana, y ello como consecuencia del rescate público de los bancos quebrados y de las medidas de respuesta de los gobiernos a la debilidad económica post burbuja. La estructura de Eurozona como una unión monetaria (esto es, una unión cambiaria) sin unión fiscal (esto es, sin reglas fiscales ni sobre las pensiones) contribuyó a la crisis y tuvo un fuerte impacto sobre la capacidad de los líderes europeos para reaccionar. Los bancos europeos tienen en su propiedad cantidades considerables de deuda soberana, de modo que la preocupación sobre la solvencia de los sistemas bancarios europeos o sobre la solvencia de la deuda soberana se refuerzan negativamente.

Como consecuencia de esta crísis económica, la Unión Europea intenta aumentar la integración económica y política entre sus estados miembros, habiendo aprobado para ello medidas comunes de carácter fiscal, una mayor coordinación económica de la eurozona, el refuerzo de los fondos de rescate para países en dificultades económicas y adelantando la puesta en funcionamiento del Mecanismo Europeo de Estabilidad. Así mismo, la mayor parte de los Estados de la UE acordaron adoptar el Pacto del Euro, consistente en una serie de reformas políticas dirigidas a mejorar la solidez fiscal y la competitividad de sus miembros.

Esta política tiene el objetivo declarado de mejorar el bienestar económico de determinadas regiones de la Unión. Alrededor de un tercio del presupuesto de la UE se dedica a esta política. El objetivo que se persigue es la eliminación de las disparidades de riqueza en toda la UE, la reestructuración de las zonas industriales en declive y la diversificación de las zonas rurales con un sector agrícola en declive.

La ampliación más importante de la UE tuvo lugar en mayo de 2004 con diez nuevos estados miembros, en su mayoría procedentes de Europa Central u Oriental, seguida por la adhesión de Bulgaria y Rumanía en enero de 2007. La mayoría de estos países son más pobres que los miembros pretéritos y esto ha significado que la "renta per cápita media" de la UE se ha reducido, lo cual ha hecho que algunas regiones de la anterior UE-15 ya no puedan optar a la ayuda financiera comunitaria, ya que la mayoría de las regiones de los nuevos estados miembros cumplen los requisitos para recibir dichos fondos.

Para conseguir esta convergencia socioeconómica la Unión dispone de varios Fondos Estructurales:

La Unión Europea, dispone además de una serie de iniciativas reservadas para acciones de carácter innovador, las cuales en origen eran 13, y actualmente tan sólo se mantienen cuatro:

El presupuesto de la Unión Europea contiene todos los ingresos y todos los gastos de la UE. Si bien ha ido aumentando a lo largo del tiempo, actualmente su límite está fijado en el 1,27% del PIB de la Unión. El presupuesto anual se fija dentro de un marco financiero plurianual previamente establecido para un período no inferior a cinco años (actualmente 7 años).

Debido a que la Unión Europea tiene un parlamento y una administración distinta e independiente de sus estados miembros, gestiona también de forma independiente los gastos dirigidos a las políticas comunes de la Unión. Para hacer frente a estos gastos, la Unión Europea tiene un presupuesto acordado de más 133 800 millones de euros anuales, lo que equivale a un 1% de la riqueza que generan cada año los países miembros.

La Unión Europea se nutre de los recursos que le transfieren los estados miembros y que le corresponden por derecho, los conocidos como recursos propios, los cuales provienen fundamentalmente de las exacciones agrícolas, de los derechos de aduanas, de una cuota sobre el IVA y de una cuota en relación con el PIB. Mientras que el resto de recursos tienen fundamentalmente un carácter testimonial, ya que suponen sólo un 1% de los ingresos de la Unión, como son las multas impuestas por la Comisión Europea o el excedente positivo, si hay, del año anterior.

Los gastos de la Unión Europea se dividen en cinco bloques principales: crecimiento sostenible (empleo, innovación, educación, política social, etc.); ciudadanía, seguridad y justicia; política exterior de la UE; gastos de administración y compensaciones (ayudas a los países miembros con menor desarrollo).

El Mercado interior de la Unión Europea (MIUE) es una combinación de unión aduanera y zona de libre comercio. Así, los miembros de la Unión actúan como bloque, definiendo los mismos aranceles al comerciar con el exterior (para evitar la competencia interna), anulando entre ellos los aranceles en frontera y permitiendo el libre tránsito de personas, así como de capitales y servicios (libre prestación de servicios y libertad de establecimiento de empresas).

En 2007, se decidió darle un nuevo rumbo al MIUE otorgándole prioridad al consumidor y a las pequeñas empresas.

El Acuerdo de Schengen, firmado en 1985, tiene como objetivo finalizar con los controles fronterizos dentro del "espacio de Schengen" para armonizar los controles fronterizos externos, con la creación de una zona de libre circulación.<ref name="Espacio/cooperación Schengen"></ref>

Todos los países del espacio de Schengen, con la excepción de Suiza, Noruega, Liechtenstein e Islandia, son miembros de la Unión Europea. Por otra parte, dos miembros de la Unión, Irlanda y el Reino Unido, han optado por permanecer fuera del acuerdo de Schengen aunque participan en ciertos asuntos.

El Espacio Económico Europeo (EEE) comenzó a existir el 1 de enero de 1994, con motivo de un acuerdo entre países miembros de la Unión Europea y de la Asociación Europea de Libre Comercio (EFTA). Su creación permitió a los países de la EFTA participar en el mercado único europeo sin tener que adherirse a la UE. Los miembros de la asociación son los 28 países integrantes de la UE, e Islandia, Liechtenstein y Noruega.

Por su parte Suiza, como miembro de la EFTA, también tenía derecho a entrar en el Espacio Económico Europeo, pero tras un resultado negativo a la entrada en un referéndum nacional en diciembre de 1992, no ratificó el acuerdo. Las relaciones de Suiza con la UE están regidas por un conjunto de tratados bilaterales y entró a formar parte del espacio Schengen en noviembre de 2008.

La UE opera una política de competencia destinada a garantizar una sana competencia económica y empresarial en el mercado único. La Comisión como regulador de la competencia en dicho mercado, es responsable de aprobar fusiones, desmontar carteles y busca la liberalización económica y la prevención de las ayudas estatales.

El Comisario de Competencia, actualmente Joaquín Almunia, es una de las posiciones más poderosas de la Comisión, que destaca por la capacidad de afectar los intereses comerciales de las corporaciones transnacionales. Por ejemplo, en 2001 la Comisión por primera vez impidió un fusión entre dos empresas con sede en los Estados Unidos (GE y Honeywell), que ya había sido aprobado por la autoridad nacional. Otro caso de alto perfil dio lugar a que la Comisión multara a Microsoft en más de € 777 millones tras nueve años de acción legal.

El Tratado de la Unión Europea, en vigor desde 1993, prevé la creación de una unión económica y monetaria con la introducción de una moneda única (que por aquel entonces se pensaba llamar ECU). De ella formarían parte los países que cumplieran una serie de condiciones y se introduciría de forma gradual. La fecha inicialmente prevista se fue retrasando hasta que, finalmente, los estados miembros de la Unión Europea acordaron el 15 de diciembre de 1995 en Madrid la creación de una moneda común europea —ya bajo la denominación de "euro"— con fecha de puesta en circulación en enero del año 2002.

El euro es la moneda de la Eurozona o zona del Euro, compuesta en 2014 por 18 de los 28 estados miembros de la UE que comparten esta moneda única. Los billetes y monedas de euro se pusieron en circulación el 1 de enero de 2002, fecha en la que 1 euro se cambiaba por 0,9038 dólares estadounidenses (USD). Otros hitos de la moneda europea se dieron en julio de 2002, cuando el euro sobrepasó la paridad con el dólar en el mercado de divisas, y en julio de 2008 cuando el euro alcanzó su valor máximo hasta el momento, al cambiarse 1 euro por 1,5990 dólares.

Por su parte, el Banco Central Europeo (BCE) fue creado en 1998, de conformidad con el TUE, para introducir y gestionar la nueva moneda, efectuar operaciones con divisas y garantizar el buen funcionamiento de los sistemas de pago. Es también responsable de fijar las grandes líneas y ejecutar la política económica y monetaria de la UE. Una de las principales tareas del BCE es mantener la estabilidad de precios en la zona euro, preservando el poder adquisitivo del euro.

En 2006 el Consejo Europeo aprobó la entrada de Eslovenia en el Euro para el 1 de enero de 2007. Un año después, los jefes de Estado y de Gobierno aprobaron la entrada en la zona euro de Malta y Chipre para el 1 de enero de 2008. Después, en 2008, los ministros de Economía y Finanzas de la Unión Europea aprobaron la entrada de Eslovaquia en la zona euro a partir del 1 de enero de 2009, y en 2010, los ministros aprobaron la entrada de Estonia en la zona euro a partir del 1 de enero de 2011. Finalmente, en 2013, se aprueba que Letonia entre en la eurozona a partir del 1 de enero de 2014. El resto de los estados que ingresaron a la UE con las ampliaciones de 2004 y 2007 están tomando las medidas para implementarlo como divisa propia. Por su parte, tanto Dinamarca como el Reino Unido decidieron quedarse fuera (opt-outs) de la zona euro cuando se ratificó el Tratado de Maastricht, aunque se espera que Dinamarca realice un referéndum en los próximos años sobre esta cuestión.

La UE es miembro de la Organización Mundial del Comercio (OMC) desde el 1 de enero de 1995, y a su vez, los 28 estados miembros de la Unión son miembros de la OMC. Es importante destacar que la UE es la primera potencia comercial del planeta, ya que representa más del 20% del comercio internacional (importaciones y exportaciones). En su interior, Alemania tiene el mayor mercado de la Unión atendiendo a su PIB.

La UE es el principal socio comercial de Rusia, la mayoría de países africanos, los países europeos no pertenecientes a la UE y, a partir de 2005, también de la República Popular China, con la que las transacciones superan los 100 000 millones de euros al año.

La UE ha señalado que está interesada en cerrar acuerdos de libre comercio con los países latinoamericanos, los cuales están integrados en varios grupos regionales. Uno es la Comunidad Andina constituida por Bolivia, Colombia, Ecuador, y Perú, y otro es el Mercado Común Centroamericano, a la vez que también ha celebrado acuerdos de cooperación con México y Chile, y está en negociaciones para la liberalización del comercio con el Mercosur. Las negociaciones entre la Unión Europea y el Mercosur comenzaron en 1995 y continúan hasta el día de hoy. El 21 de febrero, ambos bloques se reunieron en Asunción para concretar el acuerdo, sin embargo aún persisten ciertas diferencias.

En 2007, los estados miembros de la UE (27) tenían un consumo interior bruto de energía de 1.825 millones de toneladas equivalentes de petróleo (tep), de las cuales, alrededor del 46% de la energía consumida se producía en los propios estados miembros, mientras que el 54% restante se importó. En estas estadísticas, la energía nuclear es tratada como la energía primaria producida en la UE, independientemente de la fuente del uranio, del que menos del 3% es producido en la UE.

La UE ha tenido el poder legislativo en el ámbito de la política energética a lo largo de su existencia, teniendo sus raíces en la originaria Comunidad Europea del Carbón y del Acero. La introducción de una política obligatoria e integral de la energía fue aprobada en la reunión del Consejo Europeo en octubre de 2005, y el borrador de la política fue publicado en enero de 2007.

La Comisión tiene cinco puntos clave en su política energética: aumentar la competencia en el mercado interior, fomentar la inversión y aumentar las interconexiones entre las redes de electricidad, diversificar las fuentes de energía con mejores sistemas para responder a una crisis, establecer un nuevo marco para la cooperación energética con Rusia, al tiempo que pretende mejorar las relaciones con los estados ricos en energía de Asia Central y del Norte de África, el uso de las fuentes de energía existentes de manera más eficiente y el aumento del uso de las energías renovables y, finalmente, aumentar la financiación de nuevas tecnologías energéticas.

La UE importaba en 2007 el 82% del petróleo, el 57% del gas y el 97,48% del uranio. Existe la preocupación de que la dependencia de Europa respecto a la energía de Rusia pone en peligro a la Unión y a sus países miembros. Por lo que la UE está tratando de diversificar su suministro de energía.

El Consejo Europeo de marzo de 2007 aprobó un plan energético obligatorio que incluye un recorte del 20% de sus emisiones de dióxido de carbono antes del año 2020 y consumir más energías renovables para que representen el 20% del consumo total de la UE (contra el 7% en 2006).
Por otra parte se estableció el compromiso de lograr una cuota mínima de un 10% de biocombustibles en el consumo total de gasolina y gasóleo de transporte en 2020.

El futuro reparto del esfuerzo de ese porcentaje del 20% tendrá en cuenta las especificidades energéticas de cada estado. Además, la UE se compromete a llegar hasta un 30% en la reducción de gases de efecto invernadero en caso de compromiso internacional que involucre tanto a otras potencias como a los nuevos países industrializados.

La UE está trabajando para mejorar sus infraestructuras transfronterizas, por ejemplo a través de las redes transeuropeas (RTE). Los proyectos de las RTE incluyen el túnel del Canal, el corredor Mediterráneo, el LGV Est, el túnel ferroviario de Fréjus, el puente de Oresund y el túnel de base del Brennero. En 2001, se calculó que en 2010 la red comprendería 75 200 kilómetros de carreteras, 78 000 kilómetros de vías de ferrocarril, 330 aeropuertos, 270 puertos marítimos y 210 puertos interiores.

Otro proyecto de infraestructura es el sistema global de navegación por satélite Galileo, construido por la Unión Europea y puesto en marcha por la Agencia Espacial Europea (ESA). Tras años de retraso, el proyecto pretende completar una red de 26 satélites en órbita para 2017, más seis de repuesto. Galileo fue lanzado en parte para reducir la dependencia de la UE sobre el Sistema de Posicionamiento Global (GPS) estadounidense, y también para dar una cobertura mundial más completa y permitir una exactitud mucho mayor, dada la antigüedad del sistema GPS. Algunos han criticado al sistema Galileo debido a su elevado coste, sus varios retrasos, y por su percepción de redundancia, dada la existencia del sistema GPS.

La Política Agrícola Común (PAC) es una de las políticas más antiguas de la Unión Europea y uno de sus propósitos originales. La política tiene como objetivos el incrementar la producción agrícola, asegurar la certeza del suministro de los alimentos, mejorar la calidad de vida de los agricultores y estabilizar los mercados al asegurar precios razonables para los consumidores. Hasta hace poco, operaba mediante un sistema de subsidios y de intervención en el mercado; hasta la década de 1990 la política representaba el 60% del presupuesto anual de la Unión Europea, y hoy en día aún representa el 40%.

Los controles de precios y la intervención en los mercados tuvieron como resultado la sobreproducción, la cual se almacenaba para mantener los niveles mínimos de precios. Para disponer de este superávit, a menudo se vendían en el mercado mundial internacional a precios por debajo de los precios garantizados por la Unión o, por otra parte, los agricultores a menudo recibían subsidios que equivalían a la diferencia entre los precios mundiales y los de la Unión. Este sistema se ha criticado por vender más barato que la producción de los países en vías de desarrollo. La sobreproducción también ha sido criticada por los ambientalistas por los métodos de producción intensivos. Por otra parte, los que apoyan a la Política Agrícola Común, argumentan que la ayuda económica para los agricultores, les asegura un estándar de vida razonable imposible económicamente si no existiera.

Desde el comienzo de la década de 1990, la política se ha ido reformando. Al principio, estas reformas incluían la política de separar una porción de tierra de la producción, imponer cuotas en la producción lechera, etc. Los gastos agrícolas abandonarán los subsidios relacionados con la producción específica por relacionarlos con el tamaño de las fincas agrícolas, para permitir que el mercado establezca los niveles de producción y a la vez asegurar la renta de los agricultores. Las reformas también incluyen la abolición del régimen de azúcar entre los estados miembros y las naciones africanas y caribeñas y su relación privilegiada.

En el terreno de la investigación y exploración espacial en Europa existe la Agencia Espacial Europea (ESA en sus siglas en inglés). En ella colaboran 18 estados europeos, aunque se espera que los miembros que trajo la ampliación de la Unión Europea de 2004 y 2007 vayan incorporándose a la Agencia en los próximos años. Su sede central se encuentra en París. El lugar desde el que se efectúan los lanzamientos de los vehículos Ariane de la Agencia es el Puerto espacial de Kourou, situado en la Guayana Francesa.

En el terreno de la física nuclear, destaca la Organización Europea para la Investigación Nuclear (más conocido por sus antiguas siglas, CERN), el mayor laboratorio de investigación en física de partículas a nivel mundial.
Está situado en la frontera entre Francia y Suiza, entre la comuna de Meyrin (en el Cantón de Ginebra) y la comuna de Saint-Genis-Pouilly (en el departamento de Ain). En la actualidad hay 20 estados miembros, y la Comisión Europea actúa como observador. Uno de sus proyectos estrella es el Gran Colisionador de Hadrones, sobre el que los científicos e investigadores han puesto grandes expectativas.

La primera estrategia de desarrollo sostenible en la Unión Europea se realizó en 2001, y posteriormente se actualizó en 2006, ya que se quería mejorar las deficiencias derivadas de los nuevos retos. Esta política se centra fundamentalmente en el cambio climático, en la política energética, así como en la educación, la investigación y la financiación pública para conseguir instalar patrones sostenibles de producción y consumo.<ref name="Europa/Env"> "Actividades de la Unión Europea. Medio ambiente". europa.eu.</ref>

La Unión Europea cuenta con una de las legislaciones de medio ambiente más severas del mundo, la cual se introdujo después de estar varias décadas estudiando los principales problemas medioambientales existentes en la Unión. La UE cuenta con la Agencia Europea de Medio Ambiente, que tiene por misión facilitar a la UE y a los países miembros la toma de decisiones sobre la mejora del medio ambiente, y coordinar la Red europea de información y observación del medio ambiente.

Las actividades prioritarias en relación al medio ambiente se enfocan en la lucha contra el cambio climático, mantener la biodiversidad, reducir los problemas de salud derivados de la contaminación y el uso de los recursos naturales de manera más responsable. De esta forma, lo que se persigue con estas políticas es la protección del medio natural, de una manera que se contribuya al crecimiento económico, impulsando la innovación y la empresa. La Unión Europea ha puesto en marcha a través de un libro de medidas, un paquete ambicioso que marque su propio liderazgo en la preparación para un acuerdo mundial. En ese sentido, se ha convertido en la primera potencia mundial que adopta objetivos jurídicos vinculantes de tal alcance en materia de clima y energía. El primer compromiso que los países miembros han adquirido ha sido la reducción de emisiones de gases de efecto invernadero en un 10%.

La red Natura 2000 es una red ecológica europea de áreas de conservación de la biodiversidad. Consta de Zonas Especiales de Conservación designadas de acuerdo con la Directiva Hábitat, así como de Zonas de Especial Protección para las Aves (ZEPA) establecidas en virtud de la Directiva de Aves. Su finalidad es asegurar la supervivencia a largo plazo de las especies y los hábitats más amenazados de Europa, contribuyendo a detener la pérdida de biodiversidad ocasionada por el impacto adverso de las actividades humanas. Es el principal instrumento para la conservación de la naturaleza en la Unión Europea.

Esta red de espacios coherentes se fundamenta en la política de conservación de la naturaleza de la Unión Europea según su Directiva de Hábitats, que complementa la Directiva de Aves de 1979.

La Red Natura 2000 se creó a través de la Directiva 92/43/CEE sobre la conservación de los hábitats naturales de fauna y flora silvestres (más conocida como Directiva de Hábitats), de 21 de mayo de 1992. Esta red debe permitir alcanzar los objetivos establecidos por el Convenio sobre la Diversidad Biológica, aprobado en la Cumbre de la Tierra en Río de Janeiro en 1992.

El programa LIFE, creado en 1992, financia medidas que contribuyen al desarrollo, la aplicación y actualización de la política y la legislación comunitaria de medio ambiente. Este instrumento financiero pretende igualmente facilitar la integración del medio ambiente en las demás políticas y lograr un desarrollo sostenible en la Unión Europea.

En 2016 se crea un catálogo de especies invasoras de la Unión

La Unión Europea ocupa el 3º puesto en el "ranking" mundial de población, con un total de 501 105 661 personas que se estiman que viven en la UE en 2011 frente a las 313 232 044 en Estados Unidos. Es decir, la Unión Europea tiene aproximadamente 188 millones de habitantes más que los Estados Unidos (siguiente en la lista).

El número de habitantes de la Unión podrá incrementarse en el próximo decenio, en parte debido a la inmigración pero sobre todo gracias al proceso de ampliación, que podría dar cabida a Islandia, a Albania, a varios estados de la antigua República Federal Socialista de Yugoslavia (Serbia, Macedonia, Bosnia y Herzegovina, Montenegro y Kosovo) e incluso a Turquía, con lo cual el total de la población de la Unión aumentará en cerca de 100 millones de habitantes.

Pese a que la población de la UE constituye la tercera potencia demográfica del mundo, por detrás de China y la India, sólo contribuyó en 2003 en menos de un 2% al aumento de la población mundial, que se incrementó en 75 millones.

En la mayoría de los países del Sur de Europa se ha producido un cambio desde una situación de altos índices de nacimientos y defunciones a una de bajas tasas de nacimientos y defunciones, aunque este fenómeno apareció décadas después que en otros países europeos más desarrollados. Actualmente ninguno de los países miembros de la Unión Europea registra niveles suficientes de natalidad. Aun así, los niveles de natalidad están creciendo en los últimos años, lo que junto con la inmigración, hace que el crecimiento de la Unión Europea sea positivo.

En España, la natalidad se redujo en más de la mitad entre 1960 y 1990, de 21,7 a 10,2 nacimientos por mil habitantes. En ningún otro país de la Unión la tasa de nacimiento bajó tanto como en España, pero por otra parte este país ostenta la mayor tasa inmigratoria (2003). En 1900 la esperanza de vida en España era de 35 años, la continua caída en la tasa de mortalidad la elevó a 62 años en 1950, para llegar en 1985 casi a los 80 años para las mujeres y 73 para los hombres.

Antes de la ampliación de 2004, la población de la Unión crecía a una tasa anual de 0,23% (2,3 por mil) debido principalmente al incremento de la población inmigrante cuyo saldo adicional en el año 2000 fue de 735 000 personas, mientras que el crecimiento natural de la población, durante el mismo año, fue de 372 000 habitantes.

Sobre la tasa de crecimiento natural de la población debe anotarse que la tasa de natalidad de casi todos los países de la Unión está creciendo, con excepción de Alemania, Italia, Grecia y Suecia. Las tasas más altas de natalidad se observan en Irlanda (16,1 nacidos por mil habitantes), Francia (12,29) y Países Bajos (10,2). En el otro extremo aparecen Alemania (8,3) y Grecia (9,1).

La inmigración es responsable de aproximadamente tres cuartas partes del crecimiento total del número de habitantes de la UE, según datos de 2001. Alemania y España fueron los principales responsables de este crecimiento en términos absolutos con cerca de 230 000 inmigrantes netos cada uno (sumados suponen el 44% del total).

Sin embargo en términos porcentuales, los mayores crecimientos se dan en Luxemburgo y Portugal (ambos con 6,7 inmigrantes por cada 1000 habitantes), seguidos de España (5,6) e Irlanda (5,1). Aunque aún con migración neta positiva, las menores tasas se dan en Francia, Bélgica, Países Bajos, y Reino Unido. La media de la Unión Europea se cifra en 3 inmigrantes por cada 1000 habitantes.

En los últimos años debido a la crisis económica del 2008-09 el número de inmigrantes ha descendido en países como España, llegando incluso a producirse un proceso migratorio de estos países hacia otros europeos como Alemania.

En la Unión Europea la esperanza de vida es de las más altas del mundo, con 79,4 años de vida media (76,4 para los hombres y 82,4 en el caso de las mujeres), y un Índice de Desarrollo Humano superior al de las potencias emergentes y al de Estados Unidos.

En este contexto la población de la UE, experimenta un proceso marcadamente desigual entre sus regiones. Por una parte países como Alemania, donde durante varios años la población envejece exponencialmente, debido a la disminución del número de nacimientos y el constante aumento en la esperanza de vida. Por otra parte Francia es el único gran estado (en cuanto a número de habitantes se refiere) de toda la unión que ha logrado mantener una tasa de natalidad suficiente. A esta base la situación francesa añade un alto promedio inmigratorio y una reducida tasa de emigración.

Considerando tanto el crecimiento vegetativo como el saldo migratorio, los países que más crecieron en 2010 han sido Irlanda y Luxemburgo, y los que menos Alemania e Italia.

La Unión Europea cuenta, en todas sus instituciones, con 24 idiomas oficiales y de trabajo. Sin embargo, en la Comisión Europea, por ejemplo, el colegio de comisarios negocia sobre la base de documentos presentados en inglés, francés, alemán, italiano y español. De facto, las principales lenguas oficiales y de trabajo son el inglés y el francés, tanto por su uso hablado, así como por su uso en la primera redacción de los documentos oficiales.

En los estados miembros se utilizan, además de los 24 idiomas señalados, unas 60 lenguas más, cooficiales sólo en parte del territorio o no oficiales (lenguas regionales y minoritarias). Una de las políticas claves de la UE es la de promover el aprendizaje por todos los ciudadanos de por lo menos dos idiomas aparte de su lengua materna. El objetivo no es únicamente facilitar la comunicación entre ciudadanos, sino también fomentar una mayor tolerancia hacia los demás y un respeto para la diversidad cultural y lingüística de la Unión.

Varios programas de cooperación promueven el aprendizaje de los idiomas y la diversidad lingüística mediante, por ejemplo, intercambios escolares, el desarrollo de nuevos métodos, o becas para el profesorado de idiomas. De alguna manera de esto habla su lema "unida en la diversidad" (latín: «"In varietate concordia"»).

En la Unión Europea, como lenguas maternas las dos más habladas son el alemán (18%) y el francés. El inglés como lengua materna es la tercera más hablada (13%), sin embargo, es la primera por el número total de hablantes (51%), seguida por el francés.

Hay divergencias en el nivel de conocimiento de idiomas entre los estados miembros. Mientras que el 89% de suecos y 86% de daneses es capaz de comunicarse en inglés, seis de los estados miembros todavía tienen, según un barómetro de 2006, mayoría de población monolingüe: Irlanda (el 66% de habitantes sólo habla su lengua materna), el Reino Unido (62%), Italia (59%), Hungría (58%), Portugal (58%) y España (56%).

Según un estudio de la Comisión Europea en 2005, cuatro de cada cinco ciudadanos de la Unión Europea tienen creencias religiosas o espirituales. Más en concreto, el 52% de la población afirma creer en la existencia de Dios y un 27% creen en la existencia de alguna clase de espíritu o fuerza vital. Sólo el 18% de los ciudadanos declararon no tener ningún tipo de creencia religiosa.

De los 28 estados de la Unión Europea, así como de los estados en negociaciones de adhesión (Turquía, Macedonia, Islandia, Montenegro y Serbia), existen algunas diferencias entre sus ciudadanos por motivos religiosos. En general, la mayoría de ciudadanos europeos que profesan alguna religión se adscriben al cristianismo, en alguna de sus diferentes ramas (católica, protestante u ortodoxa). Así, mientras que en algunos países como Italia, España, Francia o Irlanda predomina la religión católica; en otros como Suecia o Dinamarca predomina la religión protestante, y en otros como Grecia o Rumania la religión ortodoxa. Asimismo, en otros países como Turquía, la religión musulmana es la mayoritaria.
Además, el porcentaje de agnósticos y ateos en los estados de la UE también varía según el estado en el que se encuentre. Según los últimos eurobarómetros, mientras que tan sólo menos de un 2% de malteses no creen en la existencia de Dios, en España, el 24% de la población se declara no creyente o atea y en Francia entre el 30-35%. Los países con porcentajes más altos de ateísmo o agnosticismo son República Checa (60%) y Estonia (76%).

En el conjunto de la UE, según un eurobarómetro de 2006, el 46% de los ciudadanos europeos considera que la religión ocupa una posición muy importante en la sociedad. Así, entre los países donde la religión tendría mayor importancia para la población se encuentran Chipre, Italia, Malta, Eslovaquia, Eslovenia, Polonia, Reino Unido, Portugal y España, en ese orden.

Entre los países miembros de la Unión Europea se encuentran casos, como Dinamarca, donde existe una religión de carácter estatal. La aconfesionalidad del estado no es por tanto un requisito para entrar en la Unión (siendo este un aspecto interno de cada país) pero sí que se garantice la libertad religiosa para cualquier credo, según recoge la Carta de los Derechos Fundamentales:

El Tratado de la Unión Europea o Tratado de Maastricht dio reconocimiento oficial a la dimensión cultural de la integración europea, al atribuir ciertas competencias (bastante reducidas) de acción cultural a la Comunidad Europea. Según el tratado, la Comunidad Europea debe impulsar las culturas de los estados miembros, teniendo especial cuidado en preservar la diversidad, pero poniendo también de manifiesto el "patrimonio cultural común".

Sobre cultura, la Comisión Europea dispone de un comisario que agrupa en una sola cartera la educación, la formación, la cultura y la juventud, pero no el multilingüismo.

Uno de los proyectos culturales más importantes es la designación de la Capital Europea de la Cultura. Este es un título conferido por la comisión y el parlamento europeo a una o dos ciudades europeas, que durante un año tienen la posibilidad de mostrar su desarrollo y vida culturales. Algunas ciudades europeas han aprovechado esta designación para transformar completamente sus estructuras culturales y ser reconocidas en el ámbito internacional. Cuando una ciudad es nombrada capital europea de la cultura, en ella se desarrollan todo tipo de manifestaciones artísticas.

Otro proyecto cultural comunitario es el de la Joven Orquesta de la Unión Europea. En ella coinciden músicos jóvenes con talento procedentes de la Unión Europea junto con profesores reconocidos internacionalmente para conformar una orquesta renombrada a escala internacional.
La Unión Europea ha sido siempre conocida como una de las zonas de mayor prestigio educativo y es famosa por sus proyectos y por su gran evolución y experiencia y aunque tenga déficits en algunas de sus características, dedica mucho esfuerzo económico y social para que estas debilidades sean superadas, sobre todo en países que se han cohesionado recientemente.

Aun así, existen grandes diferencias de nivel cultural, social y moral entre los países potenciales y desarrollados de la unión respecto a los menos desarrollados e incluso en algunos de los países ricos hay niveles profesionales muy débiles y contraproducentes a los esquemas de la Unión.

En 1995, la Comisión Europea publicó el "Libro Blanco sobre la educación y la formación". En él se explicó ampliamente la importancia de que los ciudadanos europeos puedan recibir formación a lo largo de toda la vida, lo que se conoce como aprendizaje permanente. El objetivo es mantener la competitividad y combatir la exclusión social.

Recogiendo estas ideas, en 2000, el "Memorándum sobre el aprendizaje permanente", documento de trabajo de servicios de la Comisión Europea, convoca un debate europeo para hacer realidad el aprendizaje durante toda la vida a nivel individual e institucional. Al final del memorándum, plantean seis claves a tener en cuenta para esta estrategia: garantizar el acceso universal y continuo al aprendizaje para obtener y renovar las cualificaciones de los ciudadanos, aumentar la inversión en recursos humanos, crear métodos eficaces para el aprendizaje permanente, valorar el aprendizaje no formal e informal, asesorar e informar de las oportunidades de aprendizaje durante toda la vida y ofrecer oportunidades próximas de aprendizaje permanente.

Proceso de Bolonia es el nombre que recibe el proceso iniciado a partir de la Declaración de Bolonia, acuerdo que en 1999 firmaron los ministros de educación de diversos países de Europa (tanto de la UE como de otros países como Rusia o Turquía) en la ciudad italiana de Bolonia. Se trató de una declaración conjunta (La UE no tiene competencias en materia de educación) que dio inicio a un proceso de convergencia que tenía como objetivos facilitar el intercambio de titulados y adaptar el contenido de los estudios universitarios a las demandas del mercado.

La declaración de Bolonia condujo a la creación del Espacio Europeo de Educación Superior, un ámbito que serviría de marco de referencia a las reformas educativas que muchos países, los que se incorporaron a dicho espacio, habrían de iniciar en los primeros años del siglo XXI. Este acuerdo se enmarca dentro del Acuerdo General de Comercio de Servicios, firmado en 1995, y cuyo objetivo declarado es "liberalizar el comercio de servicios" a escala mundial (porque la OMC integra a 151 estados, incluyendo a toda la Unión Europea) para introducirlos en el mercado, ya que "la financiación pública es un elemento de distorsión de los mercados".

Para muchos sectores de la sociedad, el Proceso de Bolonia va más allá de lo firmado en Bolonia, comprendiendo aspectos relativos a toda la reforma universitaria que se consideran más importantes, especialmente aquellos referidos a la financiación de la universidad pública, y cuenta con muchos detractores y opositores.


Los programas educativos europeos más importantes son Comenius, en el ámbito escolar, Leonardo da Vinci, para la formación profesional, programa Erasmus, para la enseñanza universitaria y Grundtvig, para la enseñanza de adultos.

El programa eLearning promueve la integración efectiva de las tecnologías de la información y la comunicación (TIC) en los sistemas de educación y formación en la UE. La línea más importante de este programa es la iniciativa eTwinning, que pone a disposición de los centros escolares un portal de internet con herramientas y apoyo para facilitar la realización de proyectos de hermanamiento entre centros de diferentes países. Los hermanamientos de eTwinning permiten a los profesores de todas las asignaturas desarrollar proyectos pedagógicos comunes, compartir experiencias y recursos didácticos e introducir la dimensión europea en el aula. Los alumnos tienen la oportunidad de aprender de y con sus compañeros de otros países, de practicar idiomas extranjeros y de desarrollar destrezas relacionadas con las TIC.

Europa ha sido origen a lo largo de la historia de muchos de los deportes más populares, así como del Movimiento Olímpico. En la actualidad, el deporte en la Unión Europea tiende a estar altamente organizado, y las distintas modalidades suelen contar con ligas profesionales. Además, la mayoría de las organizaciones deportivas internacionales de relevancia están situadas en Europa.

En países miembros de la Unión Europea, se han creado diversos deportes que hoy en día son muy populares e importantes en el mundo, como lo es el fútbol, este es un deporte creado en Reino Unido, Inglaterra siendo precisos, este deporte es el más extendido y popular en la Unión Europea y del mundo. Los clubes deportivos de fútbol en Europa son generalmente los que han cosechado mayor éxito a lo largo de la historia en el mundo, así como los mejores pagados. La UEFA Champions League, el campeonato de fútbol a nivel europeo, es uno de las competiciones futbolísticas más prestigiosas. A nivel estatal, las ligas más populares son la Liga SANTANDER española, la Premier League inglesa, la competición italiana Serie A, la Ligue 1 en Francia o la Bundesliga en Alemania.

El Rugby, por su parte, a nivel profesional es popular en el sur de Francia, ciertas partes del Reino Unido, Irlanda y el norte de Italia. Otros deportes, como el baloncesto, el balonmano ("handball"), el ciclismo, el voleibol, el waterpolo, o el hockey también son practicados con popularidad en algunos estados miembros.

El deporte en la Unión Europea es principalmente una responsabilidad de los distintos estados miembros o de otras organizaciones internacionales. Sin embargo, algunas políticas de la UE han tenido un impacto sobre el deporte, tales como la libre circulación de trabajadores, a través de la sentencia Bosman, la cual prohíbe a las ligas nacionales de fútbol la imposición de cuotas de jugadores extranjeros con ciudadanía europea.

En el marco del Tratado de Lisboa se ha propuesto un estatuto especial para a los deportes, el cual eximirá a este sector de gran parte de las reglas económicas de la UE. Durante la formulación de la política de deportes de la UE, varias asociaciones deportivas europeas fueron consultados, incluida la FIBA, la UEFA, la Federación Europea de Balonmano, la Federación Internacional de Hockey sobre Hielo, la FIRA y la Confederación Europea de Voleibol. Todos los estados miembros de la UE y sus respectivas asociaciones nacionales de deporte pueden participar en las organizaciones deportivas europeas como la UEFA.




</doc>
<doc id="2892" url="https://es.wikipedia.org/wiki?curid=2892" title="Universidad de Upsala">
Universidad de Upsala

La Universidad de Upsala (en sueco: "Uppsala universitet") es una universidad ubicada en la ciudad de Upsala, Suecia, y es la casa de estudios más antigua de Escandinavia, habiendo sido fundada en 1477. Constantemente es clasificada entre las mejores universidades de Europa septentrional, y generalmente se la considera como una de las instituciones de educación superior más prestigiosas del viejo continente.

La universidad alcanzó un papel sobresaliente durante el desarrollo del Imperio sueco a fines del siglo XVI, obteniendo más tarde cierta estabilidad financiera a partir de una gran donación del rey Gustavo II a principios del siglo XVII. Upsala, además, representa un importante sitio histórico para la cultura e identidad nacional sueca, e incluso para el surgimiento de la actual nación, ya sea en términos de historiografía, literatura, política y música. Muchos aspectos de la cultura académica del país se originaron en Upsala, como por ejemplo, el birrete blanco.

La Universidad de Upsala pertenece al Grupo Coimbra de universidades europeas. Esta institución cuenta con 9 facultades distribuidas en tres "dominios disciplinarios": el primero corresponde al de humanidades y ciencias sociales; el segundo a las áreas de medicina y farmacia; y finalmente, el último cubre los sectores de ciencia y tecnología. Tiene alrededor de 20 000 estudiantes de tiempo completo, y cerca de 2 000 estudiantes de doctorado. Por otro lado, posee un personal académico de 3 600 profesores e investigadores, de un total de 5 500 empleados.

Su presupuesto anual es de alrededor 4,3 miles de millones de coronas suecas en promedio, lo que equivaldría aproximadamente a unos 715 millones de dólares, del cual cerca del 60% se destina a los estudios de grado e investigación.

En términos de arquitectura, la Universidad de Upsala tradicionalmente ha tenido una fuerte presencia en el área que rodea la catedral de la ciudad, al lado oeste del río Fyris. A pesar del desarrollo de edificios y construcciones más modernas y alejadas del centro, el casco histórico de la ciudad aún es dominado por la presencia de la universidad.

La Universidad de Upsala fue fundada en 1477, convirtiéndose en la primera universidad escandinava. La iniciativa en el asunto se le atribuye al arzobispo de la Iglesia Católica Sueca, Jakob Ulvsson. La nueva casa de estudios era pequeña, teniendo como máximo 50 alumnos y varios profesores. La universidad comenzó a decaer en la primera década del siglo XVI debido a los conflictos políticos de la época.

Entre 1520 y 1530, el nuevo monarca sueco Gustavo I llevó a cabo la reforma luterana, lo que significó que la universidad, dependiente de la Iglesia católica, perdiera su base económica e ideológica.

Sin embargo, esta situación cambió a fines del siglo XVI, cuando el clero protestante había ganado un dominio sólido en la enseñanza de la religión y sintió la necesidad de centrarse en los estudios desde un enfoque más académico, para contrarrestar la reforma católica. Es por esto que en 1593, en el sínodo de la Iglesia luterana, se decidió restaurar los privilegios de la institución. El nuevo decreto se firmó el 15 de marzo de 1595.

Durante el reino de Gustavo II Adolfo, que se extendió entre los años 1611 y 1632, Suecia se consolidó como una potencia militar líder en Europa del Norte, y también se desarrolló como un Estado burocrático avanzado; por lo tanto, el reino necesitaba funcionarios competentes. Junto con su principal asesor, el canciller Axel Oxenstierna, Gustavo II proporcionó muchos subsidios a la universidad, tanto en lo económico como en lo administrativo. En particular, hizo una donación de más de 300 estancias a la casa de estudios, las cuales todavía son administradas por esta institución. A la universidad llegaron profesores del extranjero, y el número de estudiantes aumentó. Durante este tiempo, el sistema de naciones universitarias se importó de las universidades medievales del continente, lo que significaba que los estudiantes provenientes de una misma región se reunían para colaborar y ayudarse mutuamente, y también para tener vida social. Este sistema persiste hasta nuestros días en esta casa de estudios.

Entre los años 1660 y 1670, la institución fue dominada por Olaus Rudbeck, científico y escritor sueco, quien además fue profesor de medicina en la universidad. Rudbeck era un erudito muy versátil, y era una persona que disfrutaba experimentar. Dentro de sus logros podemos encontrar al extraordinario teatro anatómico, el cual erigió en la cumbre del nuevo edificio universitario llamado Gustavianum, el cual hoy es un museo dedicado a la historia de la ciencia y de las ideas.

Carlos Linneo, destacado científico, naturalista, botánico y zoólogo sueco, quien llegó a ser profesor de la universidad en el año 1741 después de haber estudiado en su país de origen y en los Países Bajos, es el nombre que domina el siglo XVIII. Gracias a él, muchos estudiantes provenientes de toda Europa emigraron para estudiar en Upsala. Linneo envió a sus propios alumnos en expediciones de investigación a diferentes partes del mundo, tales como Japón, Sudáfrica y Australia. Así, ya por la mitad del siglo, se produjo un florecimiento de las ciencias naturales en la universidad. Además, también mereciendo mención además de Linneo podemos encontrar a eruditos como Anders Berch, economista sueco; Anders Celsius, el astrónomo que diseñó la escala de medición de temperatura más usada en el mundo; o a Torbern Bergman, químico y ex profesor de la universidad.

A finales de siglo, el rey Gustavo III (1771-1792) se interesó enérgicamente por esta casa de estudios. Una de las formas a través de las cuales demostró este interés fue la donación de un extenso jardín perteneciente al Castillo Real de Upsala. Desde entonces, a esa extensión de terreno se le conoce como el jardín botánico de la universidad, el cual fue erigido en memoria de Linneo y en honor al Gustavo III.

El siglo XIX ha sido llamado como "el siglo de los estudiantes" en Upsala. Previamente, los alumnos habían sido más bien un grupo anónimo, pero bajo las doctrinas de la Revolución francesa y con la creciente importancia, independencia y autoestima de la clase media alta educada, los estudiantes fueron gradualmente involucrándose más en asuntos políticos, y también contaban más en la opinión pública. A mediados de siglo, las tendencias nacionalistas escandinavas tuvieron también una fuerte influencia en los alumnos, la cual se dejó sentir tanto en Upsala como en otras ciudades universitarias.

Si el siglo XVIII fue el de las ciencias naturales, el siglo XIX fue la era de los historiadores, eruditos literarios y escritores; tanto así que se construyó una estatua en frente del edificio principal en honor al más sobresaliente de estos ilustrados, el historiador Erik Gustaf Geijer. Ocurrieron muchos cambios a mitad de siglo, que reformaron la organización de la universidad, y que actualizó el sistema de examinación.

La Universidad de Upsala solemnemente su cuatrigentésimo aniversario en el año 1877. Como un presente del Estado sueco, en esta ocasión, la universidad recibió un nuevo edificio, el cual todavía está en uso, aunque fue oficialmente inaugurado diez años más tarde en el año 1887.

A las mujeres se les permitió estudiar en la Universidad al empezar la década de los 1870s. Sin embargo, fue una larga y ardua lucha para que las mujeres que estudiaban en Upsala consiguieran una igualdad de reconocimiento en sus estudios y en sus carreras académicas. La primera mujer en Escandinavia que logró recibir un título de doctorado en investigación fue la historiadora Ellen Fries, quien recibió su grado en la Universidad de Upsala en el año 1833.

Durante este periodo, la universidad albergó a muchos eruditos prominentes, con algunos de ellos laureados con el Premio Nobel. Incluso, Alfred Nobel recibió un grado de doctor honoris causa de parte de la universidad en 1893.

Este periodo se caracteriza por una serie de cambios incluyendo amplias reformas educativas y la expansión radical del número de estudiantes. En los años 50, la universidad poseía alrededor de 5 000 estudiantes, creciendo dramáticamente durante los próximos diez años llegando a 20 000 alumnos en total. En los 90 una nueva expansión tomó lugar, con más de 30 000 esudiantes de pregado en total matriculados en la casa de estudios.

El crecimiento vigoroso de la universidad ha implicado una necesidad de nuevas premisas para la educación y la investigación. Mientras que en la década de 1950 las actividades universitarias se concentraban en la casa central, ubicada cerca de la catedral, hoy la institución se propaga por vastas áreas, con campus de carácter multi e interdisciplinarios.

Al mismo tiempo, el financiamiento externo también ha ganado terreno. Contacto intensivo con el mundo circundante, ya sea a nivel nacional e internacional, amplían el rol de la universidad en la comunidad académica mundial.

La Universidad de Upsala constantemente es considerada como una de las universidades más prestigiosas de Europa y del país. La clasificación académica de universidades mundiales desarrollada por la Universidad de Shanghái Jiao Tong sitúa a la casa de estudios a segundo nivel nacional, después de la universidad médica Instituto Karolinska, posicionándose en el lugar 66 a nivel mundial y decimoctava a nivel continental.

El ranking de universidades de la revista británica "Times Higher Education" sitúa a la universidad en segundo lugar a nivel nacional y la clasifica en el lugar 71-80 a nivel mundial.





</doc>
<doc id="2893" url="https://es.wikipedia.org/wiki?curid=2893" title="Universidad de Lund">
Universidad de Lund

La Universidad de Lund (en sueco "Lunds universitet") es una de las más antiguas y prestigiosas universidades del norte de Europa y consistentemente es considerada dentro de las 100 mejores universidades del mundo. Sus orígenes se remontan al año 1425 cuando un Studium Generale franciscano fue fundado a un costado de la Catedral de Lund, lo que la convertiría en la institución de educación superior más antigua de Escandinavia, seguida por las studia generalia de Upsala en 1477 y Copenhague en 1479. Sin embargo la universidad en su forma actual no fue fundada sino hasta 1666, después de que Suecia adquiriera la región de Escania en 1658 tras el acuerdo de paz firmado con Dinamarca.

La Universidad de Lund cuenta con ocho facultades y dos campuses externos ubicados en las ciudades de Malmö and Helsingborg, con una población estudiantil de alrededor de 42.000 estudiantes distribuidos en 276 programas y alrededor de 2.200 cursos. La universidad mantiene acuerdos y relaciones internacionales con cerca de 600 otras universidades en más de 70 países y pertenece a la Liga de Universidades de Investigación Europeas así como a la red global Universitas 21.

Dos importantes instalaciones para la investigación en materiales se ubican en la Universidad de Lund: MAX IV, que se estima será un laboratorio de radiación sincrotrónica líder a nivel mundial y la Fuente Europea de Neutrones por Espalación (ESS), una instalación de la Comunidad Europea que alojará la fuente de neutrones más poderosa del mundo.

La casa central de la universidad y sus edificios más tradicionales se concentran alrededor del parque Lundagård (adyacente a la Catedral de Lund), con departamentos repartidos en diferentes ubicaciones de la ciudad pero que en su mayoría se ubican en una franja que va desde el parque hacia el norte, conectando con la zona del hospital universitario y continuando hasta el campus de la Facultad de Ingeniería en la periferia noreste de la ciudad. 



</doc>
<doc id="2894" url="https://es.wikipedia.org/wiki?curid=2894" title="Universal Networking Language">
Universal Networking Language

El lenguaje universal (Universal Networking Language) es una herramienta informática utilizada para permitir que usuarios que utilizan distintos idiomas puedan comunicarse entre sí a través de la red Internet.

El desarrollo de este lenguaje está auspiciado por el Instituto de las Naciones Unidas (www.ias.unu.edu).



</doc>
<doc id="2895" url="https://es.wikipedia.org/wiki?curid=2895" title="Urbanismo">
Urbanismo

El urbanismo es el conjunto de disciplinas que se encarga del estudio de los asentamientos humanos para su diagnóstico, comprensión e intervención. El urbanismo utiliza a la geografía urbana como herramienta fundamental, e intenta comprender los procesos urbanos a fin de planificar las intervenciones para la cualificación del espacio.

La urbanística, es el conjunto de técnicas que derivadas del urbanismo sirven para la intervención urbana, en ellas se sistematizan los procesos urbanos a fin de lograr una eficacia de la intervención urbana. Existen diversas corrientes del pensamiento urbanístico a decir de: La planificación estratégica, el planeamiento urbanístico, la renovación urbana, entre otras.

De manera concreta es la acción de urbanización la que interviene en búsqueda de la organización de la ciudad y el territorio.

La denominación de quienes se dedican a esta profesión son los urbanistas, sin embargo de acuerdo a las normas de los países y las regiones estos pueden llamarse planificadores urbanos, peritos en urbanismo, técnicos en urbanismo, ingenieros catastrales. En muchos países, el urbanismo es una especialización o extensión de las profesiones de geografía, arquitectura o ingeniería civil.

La urbanística es la planificación de los diversos lugares y ambientes en los que se desarrolla la vida material, sentimental y espiritual en todas sus manifestaciones, individuales y colectivas, y comprende tanto los asentamientos urbanos como los rurales.
La urbanística no puede someterse en exclusiva a las normas de un esteticismo gratuito sino que su naturaleza es esencialmente funcional.

Generalmente se entiende que el urbanismo no es más que la práctica de la urbanística, la cual es la disciplina científica correspondiente a la ciencia y arte de la planificación urbana. El urbanismo tradicionalmente se ha asociado a la arquitectura en cuanto a que esta disciplina se aplica al conjunto de conocimientos prácticos que proporcionan las bases fundamentales para resolver los problemas de las ciudades. Esta dualidad permite entrever el carácter descriptivo y explicativo de la urbanística como ciencia frente al carácter prescriptivo del urbanismo como práctica o técnica, incluso como arte, aunque ambos enfoques son parcialmente correctos y se realimentan mutuamente.


El bienestar de la población (residente o forastera) que habita o se encuentra ocasionalmente en la ciudad o el territorio constituye el objeto último de la urbanización, término que fue acuñado por Ildefonso Cerdá el cual describía así la referida actividad:

El urbanismo empezó siendo una teoría compleja que interesó desde el primer momento a los estudiosos de la ciudad, y acabó siendo una disciplina que reúne una suma de conocimientos sustanciales relacionados con la construcción y conservación de las ciudades y con el estudio de las relaciones socio-económico-ambientales que tiene lugar dentro del fenómeno urbano, de la que se ocupa actualmente una multiplicidad de profesionales: arquitectos, economistas, geógrafos, ingenieros, sociólogos, y de forma exclusiva los urbanistas.

Hipodamo de Mileto (considerado por muchos el primer urbanista de la historia) hizo el plan urbanístico de El Pireo, el puerto de Atenas, sobre una cuadrícula que ahora se conoce como "hipodámica", y que se ha repetido multitud de veces. Nerón también se comportó como un urbanista cuando, tras el incendio de Roma, hizo reconstruir la ciudad sobre un plan distinto del trazado original.

Felipe II recoge varias ideas urbanísticas en las leyes de Indias, cuando trata de la construcción de nuevas ciudades en el Nuevo Mundo (proceso en el que España llevó a cabo una de las mayores creaciones de ciudades de nueva planta de la historia). Desde el siglo XV en toda Europa también se fundan ciudades, aunque probablemente, en la mayoría, la idea directriz era más demostrar el poder del monarca que hacer ciudades útiles, lo que no quita para que haya unas cuantas de gran belleza.

El Barón Haussmann diseñó la renovación de París. Camilo Sitte fue el miembro más destacado de la Escuela Urbana de Viena. Otros urbanistas destacados son Otto Wagner. En 1928 tuvo lugar el primer Congreso Internacional de Arquitectura Moderna y en 1947 se aprueba la ley británica de urbanismo (la "Town and Country Planning Act 1947"), como resultado del New towns movement.

A iniciativa del Instituto Superior de Urbanismo de la Universidad de Buenos Aires, en 1949 la Organización de las Naciones Unidas (ONU) declaro el 8 de noviembre Día Mundial del Urbanismo como fecha para recordar acciones necesarias para el bien común como el aumento de parques y zonas recreativas, la remodelación de algunas áreas ciudadanas, la terminación de obras de desarrollo urbano, la descongestión de zonas superpobladas y aquellas medidas que disminuyan la contaminación del aire y del agua. Esta fecha es el inicio de diversas iniciativas para el desarrollo urbano sostenible y un hito para las celebraciones de los urbanistas de todo el mundo.

En 1970, surge la ciudad postmoderna y rebasando el marco en el que por etimología y definición estaba constreñido el urbanismo –la ciudad-, hoy es una disciplina de objetivo mucho más amplio y se utiliza para la ordenación integral del territorio. El urbanismo, sinónimo de planificación y ordenación, se ocupa de proporcionar modelos territoriales sectorializados, donde cada uno de esos ámbitos tiene asignado un desarrollo acorde con sus aptitudes. Así, habrá unos suelos netamente urbanos, otros urbanizables, esto es, susceptibles de llegar a ser urbanos cuando las necesidades de crecimiento y expansión lo determinen, y, por fin, suelos no urbanizables sin ninguna expectativa de evolución hacia espacios cívicos.

Con el colapso de la Unión Soviética y el fin de la Guerra Fría simbólicamente representado por la caída del muro de Berlín se fue diluyendo el enfrentamiento bipolar que caracterizó la historia del siglo XX de posguerra. Se acentuaron entonces preocupaciones de índole global, como el Cambio climático o el Calentamiento global que derivaron por ejemplo en la inclusión del Protocolo de Madrid de 1991 como parte del Sistema del Tratado Antártico. Esta multiplicación de las preocupaciones ambientales incentivó la creación de carreras universitarias conexas y la incorporación del planeamiento urbano en una mirada ambientalista. Si bien es tradicional la premisa que expresa la organicidad de las ciudades el concepto no pasó de una simple enunciación hasta el desarrollo de la teoría que las explica como organismos y su justificación ontológica como parte de un proceso de generación de entes de complejidad creciente dentro del campo del planeamiento ambiental y que pretende explicar el porqué hacia la primera década del siglo XXI la población mundial se ha vuelto mayoritariamente urbana.

El término "urbanismo" procede de la palabra latina "urbs" (‘ciudad’),la cual se desarrolló en la antigüedad se refería por antonomasia a la capital del mundo romano, Roma. Aparece por vez primera en el diccionario de la Real Academia Española en 1956, donde se define como "“conjunto de conocimientos que se refieren al estudio de la creación, desarrollo, reforma y progreso de los poblados en orden a las necesidades de la vida urbana”". Es claro que la idea de poblado no se ajusta a la dimensión actual del urbanismo, siendo la idea de ciudad, en el sentido moderno del término, la que se adecúa más al campo de esta disciplina.

Lo urbano tiene una condición que más profundamente distingue la vida moderna de aquella tradicional-rural, no es una condición espacial ni una delimitación demográfica o productiva, sino una conducta, una forma de vida, que está determinada por las singulares características de la ciudad en tanto entidad material: específicamente su tamaño, densidad y heterogeneidad. Lo urbano es el efecto que el tamaño, la densidad y la heterogeneidad de la ciudad tienen sobre el carácter social de la vida colectiva.

Aunque el término urbanismo se utilizó inicialmente para designar todos los fenómenos de ordenación urbana, a medida que el fenómeno constructivo y edificatorio ha traspasando el espacio propiamente urbano, dicho término ha sido desplazado en la práctica por el de "Ordenamiento territorial" cuando se quiere hacer referencia a intervenciones en suelos extra urbanos, donde entran en juego intereses supralocales protegidos desde instancias públicas superiores: defensa nacional, carreteras, medio ambiente, etc. En España, el término Ordenación del Territorio se emplea también para la planificación en ámbitos supramunicipales, en los que generalmente existen relaciones funcionales importantes entre los municipios y se aprecia la necesidad de coordinar los planes urbanísticos municipales.

En la actualidad el término urbanismo se aplica a la ordenación urbana; a todos los conocimientos relacionados con la construcción de ciudades o núcleos urbanos, y se distingue del término “urbanización”, el cual está, hoy en día, directamente relacionado con los procesos constructivos, pero no con la ordenación urbana. El término ordenación del territorio se utiliza, en cambio, para designar la actividad urbanística orientada a la planificación del suelo interlocal, desde una óptica más amplia de ordenación espacial, abarcando ámbitos de carácter rural.

En el mundo desde hace varias décadas, el urbanismo se imparte en las universidades como disciplina liberal e independiente de otras profesiones. Podemos encontrar más de 100 universidades de distintos países, que brindan esta carrera universitaria empleando denominaciones como: Urbanismo, Licenciatura en Urbanismo, Planificación del Territorio y Medio Ambiente, Ingeniería Urbana, Planeamiento Urbano, Planificación de Ciudades, Urbanística y Medio Ambiente, Topografía urbana, entre otros.

En Latinoamérica la primera carrera de urbanismo a nivel licenciatura (pre-grado) se implantó en la Universidad Simón Bolívar de Venezuela; luego la carrera se implementó en México; en la máxima casa de estudios, Universidad Nacional Autónoma de México (UNAM), Universidad Autónoma Metropolitana (UAM)‚ la Universidad Autónoma de la Ciudad de México (UACM)‚ En el Centro Universitario de Arte, Arquitectura y Diseño de la Universidad de Guadalajara, Perú. En la Argentina se dicta la Licenciatura en Urbanismo en la Universidad Nacional de General Sarmiento (UNGS). Brasil y en Colombia se implantó el Programa de Gestión y Desarrollo Urbanos en la Universidad del Rosario y el Programa de Urbanismo en la Universidad de La Salle (Bogotá). En Bolivia existe la Carrera de Planificación Territorial en la Universidad Autónoma Gabriel René Moreno y en la Universidad Mayor de San Simón. El caso europeo es liderado por Holanda y Francia y en América del Norte por Canadá.

No obstante, aún perdura la formación de urbanistas como una especialización al nivel de postgrado de disciplinas afines, tales como la Arquitectura, la Ingeniería Civil, la Ecología, la Geografía, la Economía y la Sociología, entre otras.

El Urbanismo Táctico incluye en la forma en la que las ciudades crecen, interviniendo espacios existentes sub utilizados, asignándoles un uso que no es necesariamente de ocupación arquitectónica o permanente, rescatando posibilidades para que los espacios sean utilizados por las personas y valorizar la ciudad. Este tipo de acciones tiene la finalidad de hacer la ciudad un lugar más humano (Mike, 2012, pág. 54)

El Urbanismo Táctico trata de transformar espacios públicos, barrios, vías, en intervenciones con privilegio al peatón, dejando al vehículo en segundo plano. También proponer estrategias para fomentar medios de transportes alternativos y sustentables . Las intervenciones son realizadas por grupos de acciones, rescatando espacios públicos hechos para el vehículo, casa propuesta es de bajo costo y materiales reciclables. Cada intervención revaloriza la ciudad ,propone cuidar el medio ambiente dando un cambio visual a los espacios públicos, también presentar posibles soluciones a problemas de planificación, dando una identidad local de gran impacto.

Mike Lydon (2012) se refiere a ésta disciplina como “una aproximación deliberada a hacer ciudad, un ofrecimiento de ideas locales para retos de planificación local con compromisos a corto plazo y expectativas realistas, planteando intervenciones de bajo riesgo con posibilidad de altas recompensas”.

En Latino américa el Urbanismo Táctico, comprende acciones de corto plazo para generar cambios de largo plazo, trata de solucionar problemáticas ciudadanas, inequidad, y más.

Por otro lado, busca destacar el valor de lugares públicos dando soluciones livianas, rápidas y baratas hechas por personas con creatividad. Ya sea en el ámbito de la vivienda, la economía local de un barrio o el transporte. Éstas nacen como respuesta frente a un histórico escenario de escasez que hoy se encuentra en un cruce de caminos: entre una heredada informalidad y una necesaria formalización de los procesos urbanos. Esto se refleja en múltiples casos de estudio tales como vendedores ambulantes, ferias libres, ocupaciones de predios por juntas de vecinos o prácticas informales que por falta de canales institucionales o voluntad política, operan de forma reactiva buscando acortar la brecha de inequidad y representatividad en la ciudad (Steffens, 2013).​

En Latino América lugares como Argentina, Chile, Perú, Ecuador y mas ,replantean lugares públicos trasformando en oportunidades para la misma ciudad. Trata de fomentar la participación ciudadana para dar posibles soluciones a las problemáticas sociales, económicas y urbanísticas. Proponer estrategias que impulsen fomentar medios alternativos con materiales reciclables y económicos.Las ciclovias ofrecen rehabilitar calles ocupadas por el vehículo privado y transporte publico para mejorar el desarrollo sostenible de barrios, ciudades y mas. Es muy importante que el espacio publico los peatones puedan movilizare sin ninguna dificultad como son las barreras arquitectónicas y automóviles.

Uno de los objetivos del urbanismo táctico en latinoamericana es humanizar los espacios públicos urbanos rehabilitando cada sector dando posibles soluciones a problemas de planificación en las ciudades. Esta estrategia no solo lo pueden realizar urbanistas y arquitectos si no personas dispuestas a dar una nueva visual a espacios públicos promoviendo a utilizar la bicicleta o transporte alternativos, rehabilitando áreas verdes, pintar pasos cebras de colores,privilegiando al peatón.

·       Es un enfoque intencionado y progresivo para promover el cambio.

·       Ofrece ideas locales para desafíos en la planificación local.

·       Comprende compromisos a corto plazo y expectativas realistas.

·       Supone un bajo riesgo, con una posible gran recompensa.

·       Desarrolla el capital social entre ciudadanos y construye la capacidad de organización entre instituciones públicas/privadas, ONG’s y sociedad civil.

·       Ayuda a generar ciudadanía, ya que estimula el sentido de colaboración entre vecinos, se convierte en un ejercicio de opinión y trabajo comunitario (de arquitectura, creo., 2017)​.

·       “Bombardeo de sillas” es una táctica urbana de fácil implementación, con el objetivo de activar el espacio público de forma ágil y experimental.

·       “Cine vivo" es una agrupación de cine que trata de un furgón preparado para proyectar una película al aire libre.

·       “Biblioteca Móvil A47" trata de una biblioteca móvil que transporta más de tres mil libros para su consulta gratuita, además de tener incorporada un espacio para conferencias y proyecciones.

·       “Paradero para libros de parques" es un programa de la Secretaría de Cultura, Recreación y Deporte y el Instituto Distrital de las Artes, en convenio con la lectura.

·       “Plantón Móvil" grupo de flores, arbustos y árboles marchan entre los edificios, el tráfico , transportar por carros, carretillas, bicicletas, mochilas, manos o cabezas, en la búsqueda de su lugar en la ciudad.

·       “Malón Urbano” el objetivo es invitar a los vecinos a sacar sus mesas a la calle.

·       “Jardinería de guerrilla” colocar elementos de jardinería en espacios donde no existe y donde no hay un permiso legal para hacerlo.

·       “Calles abiertas” espacios temporales para caminar,ir en bicicleta o asistir a actividades sociales.

·       “Cebra de colores” donde calles enteras se convierten en espacio público.

·       “Des-pavimentado” quitar pavimento innecesario para colocar área verde.

·       “ Parques o Tiendas Pop-up” espacios residuales que se convierten temporalmente en áreas públicas o comerciales.

·       “Iniciativas de mejoras de cuadra” Con materiales baratos o donados en calles comerciales transformándolas en EP, ciclo vías y quitando espacio vehicular.      










</doc>
<doc id="2897" url="https://es.wikipedia.org/wiki?curid=2897" title="Uniola">
Uniola

Uniola es un género de plantas herbáceas, perteneciente a la familia de las poáceas. Es originario de Norteamérica.



</doc>
<doc id="2899" url="https://es.wikipedia.org/wiki?curid=2899" title="Utopía">
Utopía

Utopía deriva del griego "οὐ" ("no") y "τόπος" ("lugar") y significa literalmente ""no-lugar"" o, como glosó Quevedo; "no hay tal lugar". La palabra fue acuñada por Tomás Moro para describir una sociedad ideal, y por lo tanto inexistente. Esta "república" es imaginada como mejor que las conocidas, en especial la europea del Renacimiento, por lo cual el término puede ser interpretado como "Eutopia", también derivado del griego; "εὖ" ("bueno" o "bien") y "τόπος" ("lugar"), significando "el buen lugar", en oposición a la distopía o "mal lugar".
En un sentido estricto, el término hace referencia a la obra homónima de Tomás Moro;"Dē Optimo Rēpūblicae Statu dēque Nova Insula Ūtopia". En ella, "Utopía" es el nombre dado a una isla y a la comunidad ficticia que la habita, cuya organización política, económica y cultural contrasta en numerosos aspectos con la sociedad inglesa de la época. Con esta obra Moro crea el género de las utopías políticas y por ello en términos más generales la palabra «utopía» se emplea para referirse a una sociedad política ideal, con un plan, proyecto, doctrina o sistema deseables que parecen muy difíciles de realizar, o representación imaginativa de una sociedad futura con características favorables para la salud, el bienestar común de la sociedad, que por lo general contiene una crítica más o menos implícita de la sociedad política realmente existente. En otro sentido, se emplea el término "utópico" para referirse de modo peyorativo a las teorías o programas políticos que se consideran irrealizables.

Aunque Moro fue el creador del género de las utopías, hay narraciones clásicas que tienen elementos utópicos y que pueden considerarse como precursoras del género. Así, en la misma obra de Moro puede verse una fuerte influencia e incluso directa referencia a La República, de Platón, donde se describe una sociedad idealizada. Además de "La República", hay otras ideas utópicas anteriores, por ejemplo, la tierra de Dilmún de la mitología mesopotámica, Panquea (o Pancaya), la isla de la "Inscripción sagrada" del relato de Evémero, la "isla del Sol" de Yambulo, "La fuente del jardín de los duraznos " de Tao Yuanming, "La Ciudad Virtuosa" (que no es sino una idealización de Medina en tiempos de Mahoma) de Al Farabi y La ciudad de las damas de Christine de Pizan.

Íntimamente relacionadas con el deseo de dar un sentido a la vida y alcanzar la felicidad, se encuentran la necesidad y la búsqueda de un mundo mejor, más solidario y más justo. Existe una estrecha relación entre la justicia y las utopías.

Ya Platón puso de manifiesto que un mundo ideal en el que todos sus miembros viviesen felices y satisfechos sólo era posible si ese mundo era un mundo justo, pues un Estado es ideal si en él reina la justicia.

El anhelo de mundos ideales y perfectos es tan antiguo como el ser humano. Sin embargo, la invención y descripción de sociedades que lo sean no recibe el nombre de 'utopía' hasta el siglo XVI. Por ello, no es paradójico afirmar que existen utopías desde siempre, incluso antes de que se acuñase este nombre para referirse a ellas.

El término utopía se debe a Tomás Moro, quien tituló así una de las obras más importantes de este género. Tomás Moro bautizó con este término una isla idílica, ubicada cerca de las costas (entonces inexploradas) de América de Sur, cuyos habitantes habían logrado el Estado perfecto: un Estado caracterizado por la convivencia pacífica, el bienestar físico y moral de sus habitantes, y el disfrute común de los bienes.

Asimismo laten las narraciones extraordinarias de Américo Vespucio sobre las recién avistadas islas de Fernando de Noronha, en 1503 y en general el espacio abierto por el descubrimiento de un Nuevo Mundo a la imaginación, ambos son factores que estimularon el desarrollo de la utopía de Moro.

En general, se puede definir una utopía como un Estado imaginario que reúne todas las perfecciones y que hace posible una existencia feliz porque en él reinan la paz y la justicia. En Utopía hay un importante componente ideal, surgen de los defectos de la sociedad y se basan en las posibilidades de cambio y transformación que ésta tiene en cada momento. Las utopías hunden las raíces en la realidad más auténtica y concreta, aunque sea para criticarlas e intentar transformarla en una cosa mejor. La palabra está ligada estrechamente a "utopismo".

Todas las utopías tienen en común dos rasgos: describen sociedades que están fuera del mundo, en ningún lugar, y describen sociedades cerradas, sin contaminación exterior, inmóviles y férreamente ordenadas. La pretensión que las distingue a todas es la de dibujar las condiciones necesarias para conseguir lo que las sociedades reales jamás muestran: que todos los seres humanos son iguales. Pero en ese empeño se ignora el valor de la libertad individual, el valor que asoma con más fuerza precisamente con el humanismo renacentista...

En oposición al concepto de utopía, existe el término distopía. Al lado de ella, o convergente con ella, está la ucronía, esto es lo que no está en tiempo alguno.

A pesar de este carácter novelado o ficticio de las utopías, a lo largo de la historia del pensamiento se les han atribuido funciones que van más allá del simple entretenimiento.

En Occidente, el primer modelo de sociedad utópica lo debemos a Platón. En uno de sus diálogos más conocidos, "La República", además de la defensa de una determinada concepción de la justicia, hallamos una detallada descripción de como seria el Estado ideal, es decir, el Estado justo. Platón, profundamente descontento con los sistemas políticos que se habían sucedido en Atenas, especialmente con la democracia, imagina cómo se organizaría un Estado que tuviese como objetivo el logro de la justicia y el bien social.

Según Platón, la república o el Estado perfecto estaría formado por tres clases sociales: los gobernantes, los guardias y los productores. Cada una de estas clases tendría en la república una función, unos derechos y unos deberes rígidamente diferenciados.

A los gobernantes les concerniría la dirección del Estado; a los guardias su protección y defensa; a los productores el abastecimiento de todo lo necesario para la vida: la alimentación, ropa, viviendas...

Cada uno sería educado para desempeñar eficientemente las funciones de su grupo: la sabiduría para los gobernantes; el coraje para los guardias, y la apetencia para los productores. Pues para Platón, la buena marcha del Estado depende de que cada clase cumpla eficientemente con su cometido.

En definitiva "La República" de Platón sería, según él, una sociedad justa porque en ella gobernarían los más sabios (filósofos) y las otras dos clases desempeñarían las funciones que les habían sido asignadas.

En "La ciudad de Dios", Agustín de Hipona expresa su interpretación de la utopía siguiendo los preceptos de su visión cristiana. Según este organizador del cristianismo, la acción terrena (que simboliza para él todos los estados históricos) es fruto del pecado, pues habría sido fundada por Caín y en ella sus habitantes serían esclavos de las pasiones y sólo perseguirían bienes materiales. Esta ciudad, por tanto, no podría según él dejar de ser imperfecta e injusta. Sin embargo, Agustín de Hipona concibe la utopía en una ciudad espiritual. Ésta habría sido según él fundada por Dios y en ella reinarían el amor, la paz y la justicia. Para Agustín la utopía tan sólo sería alcanzable en este reino espiritual, lo que él y el cristianismo definen como el Reino de Cristo.

Durante el Renacimiento se produjo un florecimiento espectacular del género utópico. La mayoría de los pensadores consideraba que la influencia del humanismo era la causa de este fenómeno.
El Renacimiento es una época que, además de caracterizarse por el auge espectacular de las artes y las ciencias, destaca también por los cambios sociales y económicos. Sin embargo, estas transformaciones no fueron igual de positivas para todos, ya que ocasionaron enormes desigualdades entre unos miembros y otros de la sociedad.

Muchos de los pensadores de la época, conscientes de estas injusticias, pero también de la capacidad reformadora del ser humano, reaccionaron frente a la cruda realidad de su tiempo. Esta reacción se plasmó en la reivindicación de una racionalización de la organización social y económica que eliminase una gran parte de estas injusticias.

De ésta creencia y confianza en que la capacidad racional puede contribuir a mejorar la sociedad y a hacerla más perfecta, surgen los modelos utópicos renacentistas. El principal y más importante modelo utópico de esta época es, indiscutiblemente, "Utopía" de Tomás Moro.

"Utopía" se divide en dos partes: la primera supone una aguda crítica a la sociedad de la época; la segunda es propiamente la descripción de esa isla localizada en ningún lugar, en la que sus habitantes han logrado construir una comunidad justa y feliz. Básicamente, el secreto de la Utopía se debe a una organización política fundada racionalmente, en la que destaca la abolición de la propiedad privada, considerada la causa de todos los males e injusticias sociales.

La ausencia de propiedad privada comporta que prevalezca el interés común frente a la ambición y el interés personal que rige en las sociedades reales. En "Utopía", además, impera una estricta organización jerárquica de puestos y funciones, a los que se accede como en la república platónica, por capacidad y méritos.

Esta estricta organización es, sin embargo, completamente compatible con la total igualdad económica y social de los utopianos, pues todos disfrutan de los mismos bienes comunes, al margen de su función y su tarea en la comunidad.

También pertenece al Renacimiento la comunidad ideal de Telema, dedicada a cultivar el amor (aunque también incluye una fina sátira de la vida monástica), que brevemente presenta François Rabelais en su "Gargantúa" (1532). Aunque ya del S. XVII, pueden considerarse como utopías renacentistas tardías "La ciudad del Sol", del religioso italiano Tommaso Campanella, y "La Nueva Atlántida", de Francis Bacon. Esta última añade un elemento novedoso e importante, como es el aprovechamiento de los avances científicos y técnicos que entonces empezaban a darse (y más aún quizá, los que se esperaban para el futuro próximo), en la mejora de las condiciones de vida de los seres humanos.

En los siglos XVII y XVIII se asoció la utopía con la literatura de viajes, en la cual las sociedades civilizadas proyectaban sólo en ocasiones sus angustias y sus críticas al progreso "El origen de la desigualdad entre los hombres" (1755) de Jean-Jacques Rousseau sería un ejemplo clásico de esta concepción de la historia como un proceso de decadencia.

Pero este no es más que un caso particular en el desarrollo impresionante de las utopías en el siglo XVIII, y en su vinculación a la crítica social (a veces comunista) y a la idea de progreso a finales de la Ilustración.

Otro de los momentos fecundos en la ideación de sociedades utópicas fue principios del siglo XIX. Los profundos cambios sociales y económicos producidos por el industrialismo cada vez más individualista e insolidario abonaron el terreno del descontento y la crítica, así como el deseo de sociedades mejores, más humanas y justas.

De esta época de injusticias y desigualdades proviene el socialismo utópico. El socialismo utópico venía con diseño de soluciones para males e imperfecciones flagrantes. Charles Fourier (1772-1837), Saint-Simon y Robert Owen tenían en común un interés imperioso por transformar la precaria situación del proletariado de ese momento. A pesar de las diferencias que hay entre ellos, tienen en común su interés por mejorar y transformar la precaria situación del proletariado en ese momento. Para ello, propusieron reformas concretas para hacer de la sociedad un lugar más solidario, en el que el trabajo no fuera una carga alienante y en el que todos tuviesen las mismas posibilidades de auto-realizarse.

A diferencia de muchas de las utopías anteriores, la de estos socialistas fue diseñada con el objetivo inmediato de llevarse a la práctica. Más que relatos fantásticos de mundos perdidos o inalcanzables, constituyeron descripciones detalladas de comunidades igualitarias que, en ocasiones, fueron copiadas en la realidad. Algunos de estos socialistas compaginaron la reflexión teórica con labores prácticas y concretas de reforma social. Así, por ejemplo, Fourier propuso comunidades autosuficientes, a las que llamó falansterios, y Owen llegó a fundar Nueva Armonía, una pequeña comunidad en la que se abrió el primer jardín de infancia y la primera biblioteca pública de EE. UU..

Muchos autores, como Arnhelm Neusüss, han indicado que las utopías modernas son esencialmente diferentes a sus predecesoras. Otros en cambio, señalan que en rigor las utopías sólo se dan en la modernidad y llaman "cronotopías" o "protoutopías" a las utopías anteriores a la obra de Moro. Desde esta perspectiva, las utopías modernas están orientadas al futuro, son teleológicas, progresistas y sobre todo son un reclamo frente al orden cósmico entendido religiosamente, que no explica adecuadamente el mal y la explotación. Así las utopías expresan una rebelión frente a lo dado en la realidad y propondrían una transformación radical, que en muchos casos pasa por procesos revolucionarios, como expresó en sus escritos Karl Marx.

Se ha criticado que las utopías tienen un carácter coercitivo. Pero también se suele añadir que las utopías le otorgan dinamismo a la modernidad, le permiten una ampliación de sus bases democráticas y han sido una especie de sistema reflexivo de la modernidad por la cual esta ha mejorado constantemente. Por ello no sería posible entender la modernidad sin su carácter utópico.

Las utopías han tenido derivaciones en el pensamiento político -como por ejemplo en las corrientes socialistas ligadas al marxismo y el anarquismo-, literario e incluso cinematográfico a través de la ciencia ficción social. La clasificación más usada, hereda la pretensión del marxismo de estar elaborando un socialismo científico y por tanto restringe el nombre de socialismo utópico a las formulaciones ideológicas anteriores a éste, aunque todas ellas comparten su origen en la reacción a la revolución industrial, especialmente a la condición del proletariado, siendo su vinculación al movimiento obrero más o menos próxima o cerca a ello.

Las utopías socialistas y comunistas se centraron en la distribución equitativa de los bienes, con frecuencia anulando completamente la existencia del dinero. Los ciudadanos desempeñan las labores que más les agradan y que se orientan al bien común, permitiéndoles contar con mucho tiempo libre para cultivar las artes y las ciencias. Experiencias prácticas que habían sido plasmadas en Comunidades utópicas en el siglo XIX y XX.

Las utopías capitalistas o de mercado libre se centran en la libre empresa, en una sociedad donde todos los habitantes tengan acceso a la actividad productiva, y unos cuantos (o incluso ninguno) a un gobierno limitado o mínimo. Allí los hombres productivos desarrollan su trabajo, su vida social, y demás actividades pacíficas en libertad, apartados de un Estado intromisorio y expoliador. Se relacionan en especial al ideal del liberalismo libertario.

La utopía ecologista se ha plasmado en el libro Ecotopía, en el cual California y parte de los estados de la costa Oeste se han secesionado de los Estados Unidos, formando un nuevo estado ecologista.

Una utopía global de paz mundial es con frecuencia considerada uno de los finales de la historia posiblemente inevitables.

La visión que tienen tanto el Islam como el cristianismo respecto al paraíso es el de una utopía, en especial en las manifestaciones populares: la esperanza de una vida libre de pobreza, pecado o de cualquier otro sufrimiento, más allá de la muerte (aunque la escatología cristiana del "cielo" al menos, es casi equivalente a vivir con el mismo Dios, en un paraíso que asemeja a la Tierra en el cielo). En un sentido similar, el nirvana del budismo se puede asemejar a una utopía. Las utopías religiosas, concebidas principalmente como un jardín de las delicias, una existencia libre de toda preocupación con calles cubiertas de oro, en una gozosa iluminación con poderes casi divinos.

Las utopías tecnológicas o tecnoutopías se basan en la creencia de que los avances en ciencia y tecnología conducirán a una utopía, o al menos ayudarán a cumplir de algún ideal utópico.


Aunque se ha argüido que los ideales utópicos pueden ser realizables, la confianza en la posibilidad y la necesidad de sociedades perfectas sufrió durante el siglo XX un considerable revés. Por varias razones, muchos pensadores defendieron que dedicarse a inventar sociedades utópicas era más perjudicial que beneficioso. Los motivos de esta consideración pueden variar de un pensador a otro.





</doc>
<doc id="2902" url="https://es.wikipedia.org/wiki?curid=2902" title="Sistema astronómico de unidades">
Sistema astronómico de unidades

El sistema astronómico de unidades, llamado formalmente «Sistema de constantes astronómicas de la IAU (1976)» (en inglés, "IAU (1976) System of Astronomical Constants"), es un sistema de unidades desarrollado para su uso en astronomía. Fue adoptado por la Unión Astronómica Internacional (UAI) en 1976, y ha sido ligeramente actualizado desde entonces. 

El sistema fue desarrollado debido a las dificultades en la medición y expresión de los datos astronómicos en el Sistema Internacional de Unidades (unidades SI), al tratar con magnitudes muy grandes. En particular, hay una enorme cantidad de datos muy precisos relativos a la posición de los objetos dentro del sistema solar que no pueden expresarse, o ser tratados convenientemente, en unidades del SI. A través de una serie de modificaciones, el sistema de unidades astronómico reconoce ahora explícitamente las consecuencias de la relatividad general, que es un complemento necesario para el Sistema Internacional de Unidades, a fin de tratar con precisión los datos astronómicos. 

El sistema de unidades astronómico es un sistema tridimensional, en el que están definidas las unidades de longitud, masa y tiempo. Las constantes astronómicas asociadas también fijan los distintos sistemas de referencia que son necesarios para informar sobre las observaciones. El sistema es un sistema convencional, en el que ni la unidad de longitud, ni la unidad de masa son verdaderas constantes físicas, y hay al menos tres medidas diferentes de tiempo. 




Aunque no son unidades SI ni unidades de la UAI, a veces se utilizan en astronomía las siguientes unidades. 

La masa de Júpiter (M o M), es la unidad de masa igual a la masa total del planeta Júpiter, 1,8986 × 10 kg . La masa de Júpiter se utiliza para describir las masas de gigantes gaseosos, como los planetas exteriores y de los planetas extrasolares. También se utiliza en la descripción de las enanas marrones. 

La masa de la Tierra (M) es la unidad de masa igual a la masa de la Tierra. 1 M = 5.9742 × 10 kg . La masa de la Tierra se utiliza a menudo para describir las masas de los planetas rocosos terrestres. La masa de la Tierra es 0,00315 veces la masa de Júpiter. 

Las distancias a las galaxias distantes no suelen ser citadas en unidades de distancia en absoluto, sino más bien en términos de desplazamiento al rojo. Las razones para esto son que la conversión de los corrimientos al rojo a distancias requieren el conocimiento de la constante de Hubble, que no fue medida con precisión hasta principios del siglo XXI, y que, a distancias cosmológicas, la curvatura del espacio-tiempo permite llegar a varias definiciones de distancia. Por ejemplo, la distancia definida como la cantidad de tiempo que tarda un haz de luz en viajar hasta nosotros es diferente de la distancia definida según el tamaño aparente de un objeto. 

Otras unidades de distancias usadas de forma más o menos informal, son las siguientes:

También se emplean algunas unidades complementarias, como el Jansky, que mide el Brillo'




</doc>
<doc id="2905" url="https://es.wikipedia.org/wiki?curid=2905" title="Valladolid">
Valladolid

Valladolid es un municipio y una ciudad española situada en el cuadrante noroeste de la península ibérica, capital de la provincia de Valladolid y sede de las Cortes y la Junta de la comunidad autónoma de Castilla y León. Cuenta, según los datos del INE de 2017, con 299 715 habitantes, siendo el 13. municipio más poblado de España y el primero de todo el noroeste español. Por su parte, el área metropolitana de la ciudad, conformada por 23 municipios, es la 20.ª de España, con una población de 414 281 habitantes (INE 2013). Tiene un área de influencia socio-económica directa de más de 600.000 personas, distando solamente 39 km a Palencia y otros municipios importantes.

Aunque existen indicios de asentamientos pertenecientes al Paleolítico inferior, y yacimientos vacceos y tardorromanos, Valladolid no tuvo una población estable hasta la repoblación de la cuenca del Duero, cuando Alfonso VI entregó a su valido Pedro Ansúrez su señorío, en 1072. Durante la Edad Media fue sede de la corte de Castilla siendo dotada de ferias y Fuero Real y de distintas instituciones como Iglesia Colegial (elevada a rango de Catedral en 1595), Universidad, Real Audiencia y Chancillería o Casa de la Moneda.

Carlos I hizo de Valladolid capital política y, posteriormente, entre 1601 y 1606, fue capital del Imperio español hasta que esta función pasó definitivamente a Madrid. A partir de entonces se inicia un período de decadencia hasta la pujanza de la industria harinera y la llegada del ferrocarril a mediados del siglo XIX a cuyo amparo aparecen los primeros establecimientos siderúrgicos y la circulación del capital dando lugar en 1857 a la creación del Banco de Valladolid. En 1854, se funda "El Norte de Castilla", decano de la prensa diaria española. Tras la posguerra, la ciudad experimenta un importante cambio, debido a la instalación de industrias automovilísticas y de otros sectores.

En Valladolid, San Fernando fue proclamado rey de Castilla y se casaron los Reyes Católicos, nacieron Enrique IV, Felipe II, Felipe IV y Ana de Austria, reina de Francia, Magallanes firmó las capitulaciones de la primera circunnavegación del mundo y murió Colón. En la ciudad castellana Cervantes terminó de escribir El Quijote y también escribió Quevedo. Además establecieron sus talleres los más grandes imagineros y orfebres del Renacimiento hispano.

Conserva en su casco antiguo un conjunto histórico compuesto por palacios, casas nobles, iglesias, plazas, avenidas y parques, junto con un patrimonio museístico en el que destacan el Museo Nacional de Escultura, el Museo de Arte Contemporáneo Patio Herreriano o el Museo Oriental, así como las casas-museo de José Zorrilla, Colón y de Cervantes. Entre los acontecimientos que cada año se celebran en la ciudad destacan su Semana Santa, la Semana Internacional de Cine de Valladolid (SEMINCI), la Feria Internacional de Turismo de Interior (INTUR), Pingüinos, el Concurso Nacional de Pinchos y Tapas "Ciudad de Valladolid" o el Festival de Teatro y Artes de Calle (TAC).

Su estratégica posición y comunicación a través de una amplia red de autovías, alta velocidad (AVE), ferrocarril convencional, aeropuerto, y su carácter de nodo logístico en el Corredor Atlántico europeo, seguirán permitiendo su especialización como polo industrial de Castilla y León.

Sobre el origen del nombre hay varias teorías pero poco evidencia. Una teoría afirma que en la época andalusí se llamó Balad al-Walīd بلد الوليد, que significa "puebla de Walid" en alusión quizá al califa omeya Walid I, que gobernaba el Imperio islámico en el momento de la conquista árabe. Relacionadas con esta, existen también las etimologías "Valledolit", "Vallis Oleti" o "Valle de Olit", un árabe que supuestamente poseía la ciudad;. Otro posible origen pudiera ser "Vallis olivetum"; es decir, Valle de los Olivos, aunque dado el clima con fríos inviernos y con frecuentes heladas entrada ya la primavera que tiene la ciudad no es muy probable que hubiera gran cantidad de olivos en la zona. Otra teoría, más aceptada que las anteriores, afirma que el origen de la palabra proviene de la expresión celta "Vallis tolitum" (Valle de Aguas), ya que por la ciudad pasan el río Pisuerga y el río Esgueva, que antes de su canalización, en el siglo XIX, se extendía por varios ramales. Otra teoría, y esta más probable, es por el gentilicio "vallisoletano", que se cree que proviene de "valle del sol" o "valle soleado"; en la Edad Media era llamada "Vallisoletum".

También existe la teoría de Valladolid como contracción de "valle de lid", lugar, por su llanura, donde se reunían los clanes y tribus prerromanos para sus enfrentamientos armados.

El historiador Ángel Montenegro Duque sostiene que bien podría ser la "Tola" del itinerario de Antonino de Ptolomeo, y apunta al origen céltico del topónimo, por la raíz "tollo" (lugar de aguas). Pero, siendo un poblado de los vacceos, "Vaccea tollit" (Solevantado de los Vacceos, o lugar elevado de los vacceos) parece un nombre más probable que "valle tollitum", dado que "Tolitum" evoluciona a "Toledo". El origen latino de Valladolid sería así un caso de "falsos amigos" entre "Tollo" y "Tollere". "Vaccea Tollit" parece el origen etimológico de "Vallatolit" (siglo XI), que fonéticamente evolucionó de forma natural a "Valladolid" .

El término "Pucela" se utiliza también, de forma popular, para denominar a la ciudad. De la procedencia de esta palabra existen varias teorías, que sitúan su aparición en el siglo XV.




Por último, se encuentra el término de Pintia, que parece tener un origen mucho más culto. Cerca de Peñafiel, en la localidad de Padilla de Duero, se encuentran las ruinas de una importante ciudad, presuntamente celta: Pintia, perteneciente al pueblo prerromano de los vacceos. El identificar a Valladolid con esta ciudad proviene del Renacimiento y la costumbre que imperaba en aquella época de relacionarlo todo con las civilizaciones griega y romana. Posteriormente, se demostró la inexistente relación entre Valladolid y Pintia.

Hay indicios datables en el Paleolítico Inferior, esencialmente Achelense, recogido en superficie en las terrazas cuaternarias del río Pisuerga, en "Canterac" (que actualmente es un gran parque situado a las afueras); pero no se puede decir que la ciudad tuviera una ocupación estable hasta la Edad Media, que es posiblemente cuando surgió el topónimo que le da nombre.
Los asentamientos posteriores en la actual provincia de Valladolid datan de épocas prerromanas, existiendo en la zona yacimientos de pueblos vacceos, que fueron pobladores de cultura muy avanzada, y, como el resto de pueblos célticos, llegaron a la península procedentes del norte de Europa. El máximo exponente de esta cultura en las cercanías, que fue arrasada por los romanos, es Pincia ("Pintia"), en la actual localidad de Padilla de Duero.

Durante años, se creyó que Valladolid era la antigua Pincia, hasta que las excavaciones arqueológicas demostraron la verdadera ubicación de la ciudad vaccea. En varias zonas del casco antiguo de la ciudad han aparecido restos de época romana: junto a la Iglesia de la Antigua aparecieron evidencias constructivas de una villa de cierta entidad (siglos I-III), así como en las calles Angustias, Arribas, Juan Mambrilla y en las del Empecinado y Padilla, donde se tiene constancia de la aparición de varios mosaicos romanos. También ha habido hallazgos en puntos periféricos de la ciudad; en los alrededores del Monasterio de Nuestra Señora de Prado se descubrió en los años 50 otra villa: la "Villa romana de Prado", la cual acoge un amplio conjunto arquitectónico residencial, acompañado de mosaicos. De hecho, un gran mosaico de mármol y caliza, el "Mosaico de los cantharus" (datado en el siglo IV), preside el hemiciclo de las Cortes de Castilla y León (depositado por el Museo de Valladolid).

En el siglo X Alfonso III de Asturias consolidó la frontera del Reino de Asturias hasta el Duero, pasando a formar parte del Condado de Castilla. En el siglo XI, durante la repoblación de la Meseta, el rey Alfonso VI de León encargó al conde de Saldaña y Carrión, Pedro Ansúrez, y a su esposa, doña Eylo Alfonso, el poblamiento y expansión del primitivo núcleo agrario, que ya existía y se organizaba mediante Concejo abierto. Alfonso VI otorgó el señorío de la misma al conde en 1072, fecha a partir de la cual se produjo el crecimiento de la ciudad. Éste hizo construir un palacio para él y su esposa, doña Eylo, que no se conserva, así como la Colegiata de Santa María (lo que le otorgó el rango de villa) y la iglesia de La Antigua. En 1208, el rey Alfonso VIII de Castilla la nombró ciudad cortesana y en 1255 Alfonso X le otorgó el Fuero Real.

Tras la temprana muerte de Enrique I de Castilla, nacido en Valladolid, y la abdicación de su madre, Fernando III "el Santo" fue proclamado en 1217 rey de Castilla, en acto realizado en la Plaza Mayor de Valladolid. Durante los siglos XII y XIII Valladolid experimentó un rápido crecimiento, favorecido por las ferias y privilegios comerciales otorgados por los monarcas Alfonso VIII y Alfonso X "El Sabio". Durante estos siglos, la ciudad servía ocasionalmente como residencia real y sede de las Cortes. El primer "Alcazarejo" fue transformado en Alcázar Real, y la reina María de Molina, reina y regente de Castilla, se hizo edificar un palacio y estableció allí su residencia en torno al 1300. En 1346, el Papa Clemente VI otorgó la bula que permitió el paso del Estudio Particular vallisoletano, existente desde la segunda mitad del siglo XIII, a Estudio General o Universidad.

Juan II de Castilla se crio y murió en Valladolid habiendo reinado desde esta ciudad de la que diría que es "la villa más notable de estos mis regnos e aun fuera de ellos". Este rey fue sepultado en la Iglesia de san Pablo, hasta el traslado definitivo de sus restos a la Cartuja de Miraflores. En 1425 nacía Enrique IV de Castilla en la desaparecida Casa de las Aldabas de la calle de Teresa Gil. En 1453 Álvaro de Luna, todopoderoso valido de Juan II, es juzgado, condenado y finalmente decapitado en cadalso público en la plaza Mayor. El 7 de diciembre de 1453 se firmó en la ciudad la Concordia de Valladolid, poniendo paz entre Juan de Navarra (futuro rey de Aragón) y su hijo Carlos de Viana.

El 19 de octubre de 1469 Isabel de Castilla y Fernando de Aragón (que sería Fernando II de Aragón) celebraron su matrimonio secreto en el Palacio de los Vivero (luego emplazamiento de la Real Audiencia y Chancillería), y pasaron su luna de miel en el Castillo de Fuensaldaña. Ya en 1481 contaba Valladolid con imprenta, situada en el Monasterio de Prado, de la Orden de San Jerónimo, y bajo los Reyes Católicos la ciudad vivió una etapa de gran dinamismo universitario, que culmina en la creación de los Colegios Mayores de Santa Cruz (por el Cardenal Mendoza) y San Gregorio (por Fray Alonso de Burgos), lo que hizo de Valladolid uno de los semilleros de la burocracia moderna.

En 1489 se estableció definitivamente el tribunal de Chancillería, y en 1500 el de la Inquisición, para juzgar actos de herejía, dando lugar a la celebración de los Autos de Fe. En 1506 murió en Valladolid Cristóbal Colón, y fue enterrado en la ciudad, en el desaparecido convento de San Francisco. Otro navegante, Magallanes, firmó en Valladolid las capitulaciones con el rey Carlos I de España, antes de iniciar su ruta occidental hacia las Indias, el 22 de marzo de 1518. En 1509 nace en Valladolid Juan de Aragón y Foix, único hijo de Fernando el Católico y su segunda esposa Germana de Foix, que murió a las pocas horas de nacer.
En 1518 las Cortes de Castilla, reunidas en Valladolid, juraron como Rey a Carlos I. Durante la Guerra de las Comunidades de Castilla, el incendio de Medina del Campo provocó el levantamiento de Valladolid y, tras la derrota comunera en Tordesillas, los rebeldes comenzaron a reagruparse en la ciudad, donde se estableció la Junta. Tras la victoria del emperador, y el perdón a los sublevados exceptuando sus cabecillas, Valladolid se convirtió en una de las capitales del Imperio español de Carlos I de España y V de Alemania, cobrando gran importancia política, judicial y financiera.

El 21 de mayo de 1527 nació el futuro rey, Felipe II, en el Palacio de Pimentel.

La célebre Controversia de Valladolid tuvo lugar en 1550 y 1551 en el Colegio de San Gregorio y enfrentó dos formas antagónicas de concebir la conquista de América, representadas por Bartolomé de las Casas y Juan Ginés de Sepúlveda. Aquel debate se considera hoy pionero y una vital aportación en la Historia a la construcción de los Derechos humanos. Su resultado fueron nuevas ordenanzas que regulaban las conquistas, la creación de la figura del defensor de indios y un notable impulso del "derecho de gentes".

En 1559 se celebrarón los Autos de fe de mayo y octubre, famosísimos por su severidad. En 1561 la ciudad fue arrasada por un enorme incendio, tras el que Felipe II se comprometió a reconstruir la ciudad, dotándola de la primera Plaza Mayor regular de España.
Este rey concedió también a su villa natal el título de Ciudad el 9 de enero de 1596 en virtud de una Real Provisión, y consiguió del Papa Clemente VIII la creación de una diócesis en 1595 (elevada a archidiócesis en 1857).
San Juan de la Cruz y Santa Teresa de Jesús coincidieron en Valladolid cuando la religiosa llegó a fundar en 1568 el primer convento de la reforma de la Orden del Carmen que habitó durante un tiempo. También Fray Luis de León, que ya había pasado con su familia años de infancia en Valladolid, fue puesto preso en 1572 en las cárceles del Santo Oficio de la ciudad, para hacer frente a un proceso inquisitorial por cuestionar la forma tradicional de entender la Teología.

Los más insignes imagineros del Renacimiento español, Alonso Berruguete, Juan de Juni o Gaspar Becerra, establecieron sus talleres en Valladolid a su llegada de Italia.
En 1601, a instancias del valido del rey Felipe III de España, el Duque de Lerma, se trasladó de nuevo la corte a Valladolid, pero se volvió a mudar en 1606. Durante este tiempo nacieron el futuro Felipe IV, y su hermana, Ana de Austria, que sería reina de Francia y madre de Luis XIV. Cabe reseñar que en este periodo llegó, en misión diplomática, el artista Peter Paul Rubens y Cervantes publicó su primera edición del Quijote, en 1604. También residieron en la ciudad Quevedo y Góngora, y la gran gubia del barroco Gregorio Fernández.

La pérdida de la Corte supuso un gran cambio para la ciudad, que sufrió un grave proceso de decadencia, sólo mitigado a partir de 1670 con la implantación de talleres textiles que anuncian la industrialización posterior.
La segunda boda del rey Carlos II, con Mariana de Neoburgo, se llevó a cabo en 1690 en la iglesia del Convento de San Diego, dentro del conjunto del Palacio Real de Valladolid.

Durante la Guerra de Sucesión Española, la ciudad tomó partido por Felipe V de España. En la segunda mitad del siglo XVIII, la Ilustración apareció en Valladolid de una forma muy tímida, aunque influyente. Así, se arbolan espacios de la ciudad como Las Moreras, se protegen y estimulan las manufacturas, se alienta el saneamiento urbano, se empiedran calles e intentan racionalizar los vertidos de basuras. El semanario de ideología ilustrada "Diario Pinciano", sale a la luz en 1787. Se crearon la Real Academia Geográfico-Histórica de los Caballeros, la Real Academia de Bellas Artes de la Purísima Concepción en 1779, o la Real Sociedad Económica de Amigos del País de Valladolid en 1783. La economía local y de la meseta se beneficiaría de la construcción del Canal de Castilla, el proyecto más importante de ingeniería civil de la España Ilustrada, iniciativa del Marqués de la Ensenada, secretario de Fernando VI, y cuyo Ramal Sur finaliza en Valladolid. En 1746 el franciscano vallisoletano Pedro Regalado fue canonizado. La ciudad sufrió grandes inundaciones en 1788, provocadas al desbordarse el río Esgueva.

Valladolid fue la ciudad elegida para albergar a las tropas francesas a su llegada a España, debido principalmente a su situación en el eje París-Madrid-Lisboa. Durante la estancia de las tropas francesas se sucedieron altercados en la ciudad, entre los vecinos y los soldados, a pesar de los continuos llamamientos a la calma por parte de las autoridades de ambos.

Tras las noticias del motín de Aranjuez, la ciudad también se amotinó desde el 24 de marzo, durante varios días; se humilló la figura de Manuel Godoy (su retrato acabó hecho pedazos y arrojado al Pisuerga), y culminó con el asentamiento del Marqués de Revilla en la regiduría fernandista.
El 31 de mayo de 1808 se produce el "dos de mayo vallisoletano": el pueblo se agolpa en plazas y calles al grito de "¡Viva Fernando VII!", exigiendo, frente a las casas consistoriales, el alistamiento general, la entrega de armas, la designación de un jefe, y la proclamación de Fernando VII. El Cabildo condescendió en ello, y los manifestantes pasaron a la Chancillería. La insurrección despertó la preocupación del mariscal de Bessières. Como consecuencia, se preparó la batalla de Cabezón, que se produjo el 12 de julio, con una derrota absoluta y retirada en desbandada del ejército dirigido por García de la Cuesta, reunido en condiciones muy precarias.

Joaquín Blake participó en numerosas acciones de guerra. El 14 de julio fue derrotado junto con Cuesta en la batalla de Medina de Rioseco. Este gran militar español de origen irlandés, que fue Presidente del Consejo de Regencia de España e Indias (1810-1811) y Jefe del Estado Mayor, murió en Valladolid en 1827.

La ciudad fue finalmente liberada por el ejército mandado por Wellington, en julio de 1812. El vallisoletano Evaristo Pérez de Castro, fue diputado y primer secretario en las Cortes de Cádiz teniendo un papel activo en reclamar la soberanía nacional para las mismas tras la invasión napoleónica. Una placa en el Oratorio de San Felipe Neri en Cádiz le recuerda.

A partir de 1830, con la desamortización de Mendizábal y la reordenación en provincias del territorio español, se reactivan tímidamente el comercio y la administración. Cuando Mendizábal transfiere los inmensos huertos y jardines de los conventos y sus edificios, se aprovecha la oportunidad para abrir nuevas calles o crear servicios públicos en los nuevos edificios.

Con el desarrollo del sistema financiero, aparecieron las primeras sociedades de crédito, y en 1855 se crea el Banco de Valladolid.
En 1856 se fundó en Valladolid el decano de la prensa diaria española, "El Norte de Castilla", resultado de la fusión de otros dos diarios: "El Avisador" y "El Correo de Castilla".

La llegada del ferrocarril - Compañía del Norte a partir de 1860 y Compañía de Ferrocarriles Secundarios de Castilla en 1884- a Valladolid supone un gran impulso y marca la dirección de crecimiento de la ciudad. Durante este siglo la ciudad no crece notablemente, pero su estructura interna cambia, se abren nuevas calles, se abren nuevas plazas y jardines, como el del Poniente, se reforma el Campo Grande, y se encauza y desvía el río Esgueva, lo que supone el fin de las inundaciones en la ciudad. Todo esto es posible gracias a la gestión de grandes alcaldes, como Miguel Íscar.

El 22 de octubre de 1887 se inauguró el alumbrado eléctrico público en Valladolid: por la noche, tuvo lugar la iluminación del Teatro Zorrilla y del Círculo de Recreo Mercantil, así como de algunos cafés y casas particulares. La central suministradora, de carácter térmico, estaba ubicada en una antigua fábrica de tejidos, en la margen derecha del río Pisuerga; era popularmente conocida como «La Electra».

Los vallisoletanos Claudio Moyano, Germán Gamazo o José Muro serían importantes políticos en la España del siglo XIX.

La ciudad se expande, creciendo del otro lado de la vía férrea en el barrio que se llamará de Las Delicias. El abogado y político vallisoletano Santiago Alba ocuparía varias carteras ministeriales en diferentes gobiernos entre 1906 y 1923, y sería Presidente del Congreso de los Diputados durante la II República. La ciudad vivió la inestabilidad propia de la política española de las primeras décadas del siglo XX y saludó la instauración de la República en 1931. El 4 de marzo de 1934 se fusionaron Falange Española (el partido de Primo de Rivera) y las JONS (movimiento fundado por el vallisoletano Onésimo Redondo) en un acto celebrado en el Teatro Calderón.

El levantamiento del 18 de julio de 1936 con el que comenzó la Guerra Civil, triunfó en Valladolid, quedando en la zona nacional, siendo uno de los 12 centros del levantamiento militar. La guardia de asalto se sublevó a las 5 de la tarde del 18 de julio y los militares sublevados en la noche del 18 al 19 de julio de 1936 se hicieron con el control de las fuerzas militares tras detener violentamente a su legítimo jefe, el general Molero. 

Valladolid se convirtió en la primera gran ciudad peninsular en la que triunfó la sublevación. Con el importante apoyo de los falangistas y de los monárquicos alfonsinos, controlaron en poco tiempo toda la provincia, procediendo a organizar una columna que marchó sobre Madrid a través de los puertos de Guadarrama (Alto del León) y de Navacerrada. Así, la ciudad quedó desde el principio de la contienda en el interior de la zona sublevada, no perteneciendo al frente en ningún momento de la guerra. 

Durante la guerra y también una vez finalizada ésta, la represión franquista fusiló en Valladolid en torno a 40 personas cada día. Allí, como en otras ciudades de la zona sublevada, los presos eran sacados por la noche en camiones para ser fusilados en las afueras de la ciudad sin siquiera el simulacro de un juicio. El general Mola enviaría un comunicado pidiendo que estas ejecuciones se hiciesen en lugares más discretos y que se enterrase a los muertos, algo que hasta entonces no se hacía.Se estima en al menos 2.500 víctimas mortales, y más de 7.000 represaliados en toda la provincia. En la capital, destaca como lugar de ejecuciones el Campo de San Isidro. El Cementerio del Carmen es por su parte uno de los lugares donde se ubica una de las mayores fosas comunes de la guerra civil a nivel nacional. Entre las víctimas, destaca el caso del propio alcalde de Valladolid durante la Segunda República entre 1932 y 1934, que había sido reelegido nuevamente en las elecciones de 1936, Antonio García Quintana. Tras el levantamiento, permaneció escondido hasta que fue delatado y fusilado en el Campo de San Isidro de Valladolid el 8 de octubre de 1937.

La ciudad también sufrió bombardeos de la Aviación republicana, siendo la sexta ciudad de la retaguardia más bombardeada. El ataque más severo se produjo el 25 de enero de 1938, cuando la ciudad fue víctima de un bombardeo de la aviación republicana, cobrándose la vida de catorce personas e hiriendo a otras setenta. La ciudad permanecería en el bando sublevado hasta el final de la guerra, en 1939.

En 1940 tiene lugar la peor catástrofe de este siglo en la ciudad cuando explota el polvorín del Pinar de Antequera provocando más de 100 muertos.

Tras la postración de los primeros años de la posguerra, desde los años 1950, Valladolid experimenta un importante cambio, debido a la instalación de industrias automovilísticas (como FASA-Renault) y de otros sectores (Endasa, Michelin, Nicas, Pegaso, Indal...). La absorción de miles de emigrantes procedentes del éxodo rural terracampino provoca un importante crecimiento demográfico y urbanístico. Este hecho provocó la puesta en marcha de un planeamiento urbanístico, proyectado y parcialmente ejecutado en 1938: el Plan César Cort. Como consecuencia de su aprobación, se produce la mayor pérdida de patrimonio urbano en el casco viejo de la ciudad: edificios antiguos, conventos y claustros, incluyendo decenas de palacios renacentistas, fueron demolidos para construir bloques de pisos de gran altura, que rompen la armonía arquitectónica de la ciudad. En los últimos años de la década de 1960, se inicia la construcción del Edificio Duque de Lerma, que sería el más alto de Valladolid. Durante tres décadas permaneció deshabitado y en varias ocasiones a punto de ser derribado, convirtiéndose su exterior en un importante muro reivindicativo.
A partir de la década de 1970, la conflictividad social en Valladolid fue incrementándose debido a la cada vez mayor actividad de los movimientos estudiantiles y los trabajadores de la industria del automóvil, principalmente. Trabajadores de FASA promovieron paros laborales con el apoyo de asociaciones obreras de la ciudad. El 20 de enero de 1975 fueron juzgados y condenados en Madrid siete estudiantes vallisoletanos por asociación ilícita. Como respuesta a la condena, tres días después, representantes de todas las Escuelas y Facultades llevaron a cabo un encierro en el Hospital Provincial de Valladolid que terminó con el desalojo y detención por parte de la policía. Manifestaciones frente al rectorado y protestas contra el entonces rector de la Universidad de Valladolid, dieron lugar a una respuesta fulminante por parte del Ministerio de Educación que decretó los cierres de facultades y finalmente, el 8 de febrero, se dio la orden de clausurar la Universidad.

Valladolid continúa su crecimiento con la llegada de la democracia a España. Con las primeras elecciones municipales democráticas (1979), llegan los socialistas a la alcaldía: el socialista Tomás Rodríguez Bolaños se mantiene como alcalde desde 1979 a 1995, en el periodo 1991-1995 gracias a un pacto con IU, ya que el ganador de aquellas elecciones, el Partido Popular, no pudo alcanzar la mayoría absoluta. En 1995 el Partido Popular gana las elecciones por segunda vez, esta vez ya sí con mayoría absoluta y Francisco Javier León de la Riva es nombrado alcalde manteniéndose en el cargo hasta que en 2015 el Partido Popular gana las elecciones locales ( por séptima vez consecutiva ) pero pierde la mayoría absoluta y el socialista Óscar Puente Santiago pasó a ser el nuevo alcalde de la ciudad con el apoyo de Valladolid Toma la Palabra y Si se Puede Valladolid.

En la década de 1980 surgen nuevos barrios residenciales (como Parquesol), que provocan un crecimiento de la ciudad en su extensión. La ciudad se convierte en sede definitiva de los poderes ejecutivo (Junta) y legislativo (Cortes) de Castilla y León mediante ley aprobada en 1987, aunque las Cortes siguieron ubicadas en el Castillo de Fuensaldaña hasta la inauguración en 2007 de su nueva sede en el barrio de Villa del Prado de la ciudad.
Como ciudad significativa en la evolución de la lengua castellana, se celebró entre el 16 y el 19 de octubre de 2001, en el Teatro Calderón, el II Congreso Internacional de la Lengua Española, foro de reflexión sobre el idioma español, presidido por los Reyes de España.

Personas relevantes durante el periodo democrático, muy vinculadas a la capital, son Gregorio Peces-Barba que, como diputado por Valladolid en 1977, fue uno de los Padres de la Constitución española, los expresidentes del Gobierno de España José María Aznar, que fue también presidente de la Junta de Castilla y León, y José Luis Rodríguez Zapatero o la actual Vicepresidenta del Gobierno Soraya Sáenz de Santamaría.

Valladolid fue premiada por la asociación internacional LUCI en 2011 con el Premio al Mejor Proyecto de Iluminación Urbana City People Light por la "Ruta Ríos de Luz" y en 2012 con el Premio del Jurado Popular al Mejor Proyecto de Iluminación Urbana de los Premios City People Light Awards. En 2012 Unicef declara a Valladolid Ciudad Amiga de la Infancia. En abril de 2013, Valladolid fue premiada con el Premio Reina Sofía de Accesibilidad de Municipios Españoles por su esfuerzo en la integración, normalización y participación activa de toda la ciudadanía sea cual sea su capacidad funcional.

El origen de las llamas que en él aparecen podrían hacer referencia al incendio de 1561. No obstante, si este acontecimiento histórico fue el que determinó la inclusión de las llamas en el escudo lo más lógico sería que estas nacieran de la parte inferior y no de la parte derecha. Otro dato en contra de esta teoría es que estos jirones aparecen presentes ya antes de dicha catástrofe, en un escudo que data de 1454. Otra teoría, vinculada a un origen "legendario" se correspondería con la "bandera oriflama", guion guerrero de Castilla. Este pendón estaría terminado en diferente número de puntas, aunque finalmente se establecerían en cinco, pues era el número que figuraba en la heráldica de las dos familias más importantes e influyentes del municipio: los Tovar y los Reoyo. Dejando a un lado estas explicaciones, lo cierto es que estas puntas eran muy habituales en la heráldica europea medieval, presentes en los escudos de alguna de las grandes familias de la nobleza castellana, como los Girón.

La corona real es abierta, de origen medieval, más antigua que la corona real cerrada. Habría sido otorgada por los Reyes Católicos, como símbolo de villa de realengo, con fueros propios.

La bordura de gules con los ocho castillos de oro en el escudo de armas de la ciudad aparece por primera vez en la portada de uno de los más de diez ejemplares de la "Historia de Valladolid" de Juan Antolínez de Burgos que data de 1722 (si bien la obra original fue concluida en 1641). Hasta entonces nunca el escudo municipal había figurado con semejante incremento armero.
La bordura viene a ser un trasunto historicista, con afán también ornamental, del antiguo sello medieval de la ciudad en donde también aparecían ocho muescas o torres formando parte del cerco o muralla que envolvía simbólicamente a la villa, identificando estos castillos con las ocho puertas de las dos cercas o murallas que llegó a tener la población representadas por la bordura.
Esta composición tuvo éxito y fue paulatinamente adoptada por los diferentes gremios de la ciudad y finalmente por el concejo.

Por último, la Cruz Laureada de San Fernando, máxima condecoración militar española, creada en el siglo XIX, le fue otorgada por las nuevas autoridades franquistas por decreto de 17 de julio de 1939 al municipio de Valladolid por acciones de guerra llevadas a cabo por el bando sublevado para controlar la ciudad e inmediaciones en la Guerra Civil española.

La bandera de Valladolid es de color carmesí con el escudo de Valladolid situado en el centro.

Valladolid adquirió la categoría de villa a mediados del siglo XIII para seguir sumando títulos: buenos y leales (Muy leal) en el año 1329; Muy Noble en 1422; Ciudad en 1596; Heroica en 1854 y Laureada en 1939.

Valladolid desborda sus propios límites y salta a municipios del entorno. Esta transformación urbana ha sido definida por el catedrático emérito de Geografía urbana Jesús García como el paso «de la ciudad a la aglomeración».

Valladolid posee una población de 303.905 habitantes al 1 de enero de 2015.

Partiendo del primer dato de población recogido por el Instituto Nacional de Estadística, que data de 1842, se observa un crecimiento constante de población en toda la segunda mitad del siglo XIX, que coincide en el tiempo con la construcción del Canal de Castilla y con la llegada del ferrocarril a Valladolid.

A lo largo de los tres primeros tercios del siglo XX, Valladolid experimentó un importante aumento de población, gracias al éxodo rural. Este crecimiento, lento durante las dos primeras décadas e interrumpido por la Guerra Civil, fue especialmente significativo desde los años sesenta, con la llegada de mano de obra foránea, y supuso el momento de mayor crecimiento demográfico en la historia de la capital. Sin embargo, a partir de los años ochenta se produjo un giro en esta tendencia, que supuso un estancamiento en el crecimiento de la población, debido a dos motivos: al cese de los flujos migratorios que habían impulsado el crecimiento en épocas pasadas y a un descenso brusco en la tasa de natalidad.

En los últimos años, la ciudad de Valladolid ha ido perdiendo población en favor de su franja periurbana, donde prolifera el crecimiento de nuevas áreas residenciales. Esta cuenta con poco más de 400 000 habitantes, y es la 20.ª área de España en población. El encarecimiento de la vivienda en la capital, la falta de una política adecuada de planeamiento urbano y, como consecuencia de ello, el incremento de los problemas asociados al tráfico rodado, originaron cambios residenciales de carácter centrífugo. Las parejas jóvenes que no emigran a otras provincias optan por la adquisición de una vivienda en los municipios de la periferia, cuyo crecimiento demográfico deriva del propio vaciamiento de la ciudad (de 330 700 habitantes en 1991 a 303 905 en 2015) y del asentamiento de familias procedentes, en menor medida, de otros municipios de la provincia.

En el último lustro, Valladolid ha sufrido una paulatina pérdida de población, principalmente debido a movimientos hacia su área metropolitana.


En el año 2005 se produjeron en Valladolid un total de 2600 nacimientos. Esto supone la confirmación de una tendencia ascendente que se remonta al año 1999. Esta tasa de nacimiento es la más alta registrada desde 1992, año en el que se registraron 2658 nacimientos. Por su parte, la tasa bruta de natalidad de Valladolid se sitúa en el 8,10 ‰, que es la cifra más alta desde 1992.

En 2005 se registraron 2735 defunciones en la ciudad de Valladolid, lo que supuso un incremento con respecto a los años anteriores. Es, de hecho, la cifra más alta desde 1920, año en el que se registraron 3206 defunciones. La tasa bruta de mortalidad se cifró en 8,52 ‰, siguiendo la tendencia ascendente reflejada en el número de defunciones, y es la más alta desde 1969.

Según los datos de 2002, llegaron a Valladolid un total de 9072 personas. De este total, 2246 procedían de la propia provincia, 1721 de otras provincias de Castilla y León, 2407 de otra Comunidad Autónoma y por último 2698 personas llegaron procedentes del extranjero.

Por continentes, Europa es el más representado en Valladolid con 8680 residentes en el 2010. En cuanto al país de origen, Bulgaria aporta el mayor número de extranjeros, con 3983 frente a los 3881 de 2009. El colectivo rumano se consolida en el segundo puesto de europeos presentes en la capital con un saldo positivo de 42 habitantes (se ha pasado de los 2490 que residían en el 2009 a los 2532 que lo hacen en la actualidad).

El área metropolitana de Valladolid, como tal, no está constituida ni legal ni administrativamente, aunque existen propuestas de algunos partidos para crearla. No obstante, recibe este nombre el conjunto de municipios, que, centrados en Valladolid, están definidos por las Directrices de Ordenación del Territorio de Valladolid y su Entorno (DOTVAENT), documento realizado por el instituto de urbanística de la Universidad de Valladolid a instancias de la Junta de Castilla y León.

Precisamente esta ausencia de definición legal impide conocer con certeza su tamaño, por lo que las cifras proceden de estudios independientes o de los datos indirectos de los organismos oficiales. De este modo, según el proyecto "AUDES5 - Áreas Urbanas de España 2005", el área metropolitana de Valladolid cuenta con una población de 388.555 habitantes, mientras que según los datos indirectos procedentes del Instituto Nacional de Estadística (2007) su población sería de 407 148 habitantes.

La ciudad de Valladolid se encuentra en la mitad norte de la península ibérica. Está situada en el centro de la Meseta Norte, división de la Meseta Central, por lo que presenta un paisaje típico, llano y con escasa vegetación. El relieve vallisoletano lo conforma una llanura interrumpida por pequeñas series de colinas que originan un paisaje montañoso de cerros testigos como el de San Cristóbal (843 m), a pocos kilómetros de la capital. Las coordenadas de la ciudad son 41º 38' N 4º 43' O. La altitud del centro de la ciudad es de 690 msnm, mientras que la altitud máxima del municipio es de 863 msnm, la cual se da al noreste del mismo, entre Páramo de Cabezón y Barco de San Pedro; y la altitud mínima es de 671 msnm, la cual se da en el último tramo del río Duero dentro del municipio, a unos metros de su confluencia con el río Pisuerga.

El clima de Valladolid es Clima mediterráneo continentalizado. De acuerdo a la clasificación climática de Köppen el clima de Valladolid en el periodo de referencia 1981-2010 es, en general, de tipo "Csa" (mediterráneo). Sin embargo, la temperatura media en julio y agosto supera solo ligeramente los 22 °C en la zona urbana (concretamente en el observatorio de Valladolid), pero este valor baja de los 22 °C en algunas zonas del municipio de mayor altitud, a las afueras, dándose así en esos lugares un clima de tipo "Csb" (mediterráneo de veranos suaves). El clima de Valladolid está determinado en gran medida por la ubicación de la ciudad en el centro de la cuenca sedimentaria del Duero, que, al estar casi completamente rodeada de montañas que la aíslan del mar, tiene un clima extremado y seco para lo que cabría esperar a casi 700 metros de altitud y a solo 190 kilómetros del mar Cantábrico en línea recta. Las montañas que delimitan la meseta retienen los vientos y las lluvias, excepto por el Oeste, por donde la ausencia de grandes montañas abre un pasillo abierto al océano Atlántico y es por aquí, por Portugal, por donde penetran la mayoría de las precipitaciones que llegan a Valladolid. Los vientos del norte llegan a Valladolid secos y fríos, y los del sur suelen ser cálidos y húmedos, pero es por el Oeste y Suroeste por donde suele llegar la lluvia a Valladolid. Los vientos predominantes en Valladolid son los del Suroeste, y así lo vemos reflejado por ejemplo en la orientación de la pista del Aeropuerto de Villanubla.

Las precipitaciones están repartidas de forma bastante irregular a lo largo del año, si bien hay un mínimo acusado en verano y un máximo en otoño y primavera. La precipitación anual es de 433 mm y la humedad relativa media a lo largo del año es del 64 %. Al año hay 2624 horas de sol y 67 días de lluvia.

En cuanto a las temperaturas tal vez lo más destacado sea la importante oscilación térmica diaria. Las diferencias térmicas entre el día y la noche superan en muchas ocasiones los 20 grados. La temperatura media anual es de 12,7 °C. Los inviernos son fríos con frecuentes nieblas y heladas (56 días de heladas de media). La ciudad cuenta con 9 días de nieve al año; aunque son infrecuentes las grandes nevadas, por la particular situación geográfica de la ciudad, no son tampoco imposibles. Los veranos son, por lo general, calurosos y secos, con máximas entre 30 °C y 35 °C, pero mínimas suaves, superando ligeramente los 14 °C. Los récords de temperaturas son los 40.2 °C, del 19 de julio de 1995, y los -11,5 °C del 14 de febrero de 1983, medidos en el observatorio de la Agencia Estatal de Meteorología (AEMET) situado el barrio de Parquesol, el más alto de la ciudad.

Aunque este dato sea el oficial, en la ola de frío de enero de 1971, concretamente el 3 de dicho mes, se alcanzaron los -16,4 °C en el Aeropuerto de Valladolid, situado a las afueras de la ciudad. Sí es así en el caso de Villanubla, cuya mínima absoluta se produce en esta ola de frío, alcanzándose, el día 3 de enero de 1971, -18,8 °C. En el observatorio de Villanubla las temperaturas son más bajas, debido a que se encuentra a 849 metros de altitud, unos 150 metros más que la ciudad.

Como se indicaba anteriormente, el origen más probable del nombre de la ciudad proviene de la expresión celta "Vallis tolitum" (Valle de Aguas), y es que Valladolid se encuentra enmarcada en la confluencia del río Pisuerga con el río Esgueva. Este último atravesaba la ciudad en dos ramales, hasta que a finales del siglo XIX se llevó a cabo su canalización.

La relación de Valladolid con el río Esgueva era ambivalente. Servía de colector de aguas residuales, por lo que impedía beber sus aguas, la insalubridad máxima y contaba con olores fétidos, pero a la vez se utilizaba para lavar y era fuerza motriz para fábricas y talleres.
A partir de 1840 y hasta 1864 Valladolid experimenta un importante desarrollo económico: se pone en servicio el Canal de Castilla y completa la línea ferroviaria Madrid-Irún, por lo que el equilibrio se rompe. De este modo el Esgueva se decide cubrir en las zonas centrales de Valladolid, y encauzar en las zonas periféricas. Además, también el río Duero atraviesa el municipio por el núcleo de Puente Duero, al sur de Valladolid.

El Pisuerga, principal río de la ciudad, ofrece en la actualidad diversas opciones de ocio y cultura. La embarcación "Leyenda del Pisuerga" permite realizar un viaje por el río, desde la Estación de Embarque, situada en el Parque de las Moreras, río abajo, hasta la vecina localidad de Arroyo de la Encomienda. Se trata de un barco de 25 metros de eslora y 6 de puntal. Durante el trayecto se puede observar de cerca la flora y fauna del Pisuerga. Además, Valladolid dispone de una playa artificial, la Playa de las Moreras, que permite a los vallisoletanos tomar el sol en pleno centro e incluso darse un chapuzón en el propio Pisuerga.

Valladolid también cuenta con dos canales artificiales: el Canal de Castilla, realizado entre mediados del siglo XVIII y el primer tercio del XIX para facilitar el transporte del trigo de Castilla hacia los puertos del norte; y el Canal del Duero, construido en el siglo XIX para asegurar el abastecimiento de agua a la capital y permitir la creación de superficies de regadío al sur de la ciudad.

Valladolid alberga las sedes de las Cortes de Castilla y León y la Junta de Castilla y León, incluyendo la Presidencia de esta y sus doce consejerías.

La actual sede de las Cortes de Castilla y León fue inaugurada en junio de 2007. Se encuentra en la Avenida de Salamanca, en el barrio residencial Villa de Prado, y es obra del arquitecto granadino Ramón Fernández Alonso. La anterior sede se encontraba de forma provisional en el Castillo de Fuensaldaña, en la localidad vallisoletana de Fuensaldaña.

La ubicación del ejecutivo regional, presidido por Juan Vicente Herrera, se encuentra en el colegio de la Asunción. Dicho edificio está ubicado en la plaza de Castilla y León del Barrio de Covaresa, mientras que las sedes de las diferentes consejerías se encuentran repartidas en diferentes puntos de la ciudad.

La Diputación Provincial de Valladolid también tiene su sede en la ciudad, en concreto, en el Palacio de Pimentel. Tras las elecciones municipales del 2011 está presidida por Jesús Julio Carnero García del Partido Popular, sustituyendo en el cargo a Ramiro Ruiz Medrano, del mismo partido.

Valladolid está gobernada por el alcalde y los concejales, que componen la corporación municipal, que tiene a su cargo el municipio. El Ayuntamiento de Valladolid tiene su sede en la Plaza Mayor, en el edificio de la Casa Consistorial. Los concejales son elegidos cada cuatro años, mediante sufragio universal, por los mayores de 18 años. El actual alcalde es Óscar Puente Santiago, del Partido Socialista Obrero Español (PSOE) desde el 13 de junio de 2015, que gobierna en coalición junto con Valladolid Toma la Palabra y Si se Puede Valladolid, de los cuales sólo el primero forma parte de la corporación.

Los partidos políticos presentes en el ámbito local son el Partido Socialista Obrero Español, a cuyo frente se encuentra Óscar Puente Santiago, el Partido Popular a cuyo frente se encuentra José Antonio Martínez Bermejo, Valladolid Toma la Palabra (Izquierda Unida, Equo y otros movimientos sociales), Sí Se Puede Valladolid (Podemos) y Ciudadanos. Así, tras las elecciones municipales de 2015 la composición del Ayuntamiento de Valladolid es la siguiente:

El término municipal de Valladolid está compuesto por tres territorios separados entre sí: el principal, donde radica la ciudad de Valladolid, y dos exclaves, conocidos como Navabuena y El Rebollar, al noroeste de aquel. El de Navabuena es el más septentrional y el de mayor extensión de los dos, estando el de El Rebollar deshabitado.

Demográficamente hablando, la población del municipio se reparte en cinco entidades singulares de población, que comprenden a su vez siete núcleos de población. Las entidades y sus poblaciones son, según el nomenclátor de 2012:

Con la renovación del padrón municipal de habitantes que se realizó en el año 1986, se procedió a dividir oficialmente el término municipal en diferentes zonas, pues antes de esta fecha ya existía una división popular, en barrios, que no tenía ninguna función administrativa. Para ejecutar esta división se emplearon diferentes criterios, tales como la continuidad física del territorio, criterios sociológicos y la denominación popular de las mismas.

A partir de ese momento Valladolid se divide en un total de doce distritos, que a su vez se subdividen en cuarenta y siete zonas estadísticas, no necesariamente coincidentes con los barrios tradicionales.

Tras su repoblación, y una vez el valle se vio libre de la ocupación árabe, la ciudad comenzó a expandirse. A finales del siglo XI comenzaron a aparecer una gran variedad de barrios de carácter gremial, que fueron estableciéndose en distintas zonas, abriéndose calles bajo la influencia directa de la cuestión económica. Por estas fechas, se celebraban en Valladolid ferias de periodicidad anual, a las que habitualmente concurrían hombres de negocio de diversos lugares.

En los inicios del siglo XIV, atraídos fundamentalmente por el bullicio comercial, la actividad agrícola y la atención que en la Villa favorecía la Corte, vinieron a Valladolid gentes, no solo de territorios hispánicos, sino también de otros países, de ascendente cristiano, judío o mudéjar, que compartían el mismo espacio geográfico.
En 1359 la ciudad obtuvo el Privilegio de tener Casa de la Moneda, la cual pervivió hasta el siglo XVIII cuando Felipe V concentró la fabricación de este metal.

En el siglo XVI, la ciudad fue la capital del Reino, y en ella se centralizaron los principales órganos político-administrativos. A ello se sumó el hecho de que Felipe II, poco antes de morir, otorgó a Valladolid el título de ciudad, y, aunque mediado el siglo XVI se trasladó la capital a Madrid (hasta 1601), Valladolid siguió conociendo un momento de gran esplendor económico.
A partir de la definitiva marcha de la Corte, en tiempos de Felipe III, la ciudad padeció en los siglos siguientes una etapa de cierta decadencia, apenas mitigada por los efectos de la Ilustración, protagonizada por un fuerte descenso demográfico, y sobre todo una paulatina depresión económica.

Valladolid no experimentaría grandes cambios hasta la segunda mitad del siglo XIX, momento en el que renació con la ayuda de la industria harinera y el desarrollo de las comunicaciones, que favorecieron el transporte de la producción y de las importaciones. El funcionamiento del Canal de Castilla y la aparición de los primeros focos industriales en torno a la dársena, y la posterior llegada del ferrocarril a Valladolid, constituyeron la piedra angular de este despegue urbano. También se desarrolló el sistema financiero; aparecieron las primeras sociedades de crédito, y en 1857 se crea el Banco de Valladolid.

En 1864 se da una grave crisis económica, produciéndose el hundimiento del Banco de Valladolid y la aparición de hambrunas. En el último tercio del siglo XIX, la ciudad, aún marcada por la crisis, avanza muy lentamente. El sector secundario es minoritario, mientras que el terciario se sitúa al frente de los sectores productivos.

Ya en la década de 1950, conoció un potente desarrollo industrial, en torno, fundamentalmente, a la fabricación de automóviles; y también comercial, como consecuencia de lo anterior. En la actualidad, la industria vallisoletana continúa fundamentalmente ligada a la industria del automóvil. En paralelo con esa producción de gran escala, varios polígonos urbanizados albergan a pequeñas y medianas empresas, dedicadas a suministros de todo tipo para el mercado español. El comercio es otra de las grandes fuentes económicas de la ciudad, que debido a esa secular tradición, cuenta, desde 1965, con la Feria Internacional de Muestras para exhibir las constantes innovaciones en el sector.

El principal sector económico de Valladolid es el sector servicios, que da trabajo a 104 168 personas, lo que representa el 72,7 % de los trabajadores vallisoletanos afiliados a la Seguridad Social. Asimismo, el 82,5 % de los centros de trabajo de la ciudad corresponde a empresas del sector terciario. La rama con mayor número de establecimientos es la de comercio al por menor de productos no alimenticios, que representa más del 50 % del total.
A continuación se sitúan el sector de la industria y la construcción: 22 013 personas están empleadas en centros de trabajo industriales y 15 710 encuentran trabajo en el sector de la construcción, lo que representa el 15,4 % y el 11 % del total de trabajadores, respectivamente. Por centros de trabajo, el 6,0 % corresponde a centros industriales y el 10,3 % a empresas de la construcción. La industria predominante de la ciudad corresponde a los sectores derivados de las actividades agrarias, metalúrgica, la industria del automóvil, químicas, de la construcción, artes gráficas, etc. El polígono industrial de San Cristóbal es uno de los dos polígonos industriales de la ciudad de Valladolid. Este polígono acoge a gran cantidad de empresas. Está delimitado por la ronda interior (VA-20), por la ronda exterior (VA-30) y por las carreteras de Soria (A-11) y de Segovia (A-601)

Por último, la actividad agrícola, muy minoritaria, da empleo a 1491 personas, apenas el 1 % del total, con tan solo 153 centros de trabajo (el 1,2 %) dedicados a esta actividad. De esta escasa dedicación agrícola, el tipo de cultivo predominante es de secano, representado en la producción de trigo, cebada y remolacha azucarera, principalmente.

Las principales empresas de la ciudad son: Renault-España, Indal, Michelín, Iveco, Ambuibérica, Aquagest, ACOR, Grupo Norte, Panibérica de Levaduras (Lessafre), Helios, Lingotes Especiales, o Queserías Entrepinares.

La educación en Valladolid depende de la Consejería de Educación de la Junta de Castilla y León, que asume las competencias de educación a nivel regional, tanto en los niveles universitarios como en los no universitarios. Según datos de la propia Consejería, se calcula que en el curso académico 2005-2006 el total de estudiantes no universitarios fue superior a los 52 000, los cuales tienen a su disposición 141 centros de enseñanza, con 2399 aulas y 4487 profesores.

En cuanto a la enseñanza universitaria, Valladolid cuenta con dos universidades y un campus de la Universidad Isabel I (la universidad online de Castilla y León):

En la actualidad, la Universidad de Valladolid cuenta con cuatro campus en la ciudad: "Huerta del Rey", "Centro", "Río Esgueva" y "Miguel Delibes". Repartidos en sus 25 facultades y centros asociados, unos 2000 profesores dan clase a más de 23 800 alumnos matriculados en 2011.
Dispone, además de los 25 centros, de una serie de edificios administrativos, como por ejemplo el Palacio de Santa Cruz, donde se encuentra el rectorado, y el Museo de la Universidad de Valladolid (MUVa), La Casa del Estudiante, donde están el resto de los servicios administrativos, o el CTI (Centro de Tecnologías de la Información), que se encuentra en el sótano de la Residencia Universitaria Alfonso VIII, junto a la antigua Facultad de Ciencias.



Valladolid cuenta con 410 equipamientos sanitarios, entre los que se encuentran tanto ambulatorios, como centros de salud u hospitales, de carácter tanto público como privado.

Los dos hospitales públicos de Valladolid, ambos dependientes de SACYL (Sanidad de Castilla y León), son el Hospital Clínico Universitario de Valladolid, heredero del histórico Hospital de la Resurrección, con 777 camas, y el Hospital Universitario Río Hortega, con 589. Se ha construido un tercer hospital en el barrio de Las Delicias, el nuevo Río Hortega, que abrió sus puertas en enero de 2009 y reemplazó al antiguo Río Hortega. Valladolid cuenta con los siguientes centros sanitarios: Barrio España, Canterac, Circunvalación, Delicias I, Delicias II, Magdalena, Pilarica, Plaza Circular, Rondilla I, Rondilla II, San Pablo, Tórtola, Arturo Eyríes, Casa del Barco, Gamazo, Huerta del Rey, La Victoria, Parquesol, Plaza del Ejército, Parque Alameda-Covaresa; de los cuales Rondilla, Arturo Eyríes, Delicias y Pilarica disponen servicios de urgencias PAC.

El grupo sanitario Recoletas dispone de dos hospitales en la ciudad, el Hospital Felipe II y el Hospital Campo Grande, siendo este último el más importante de Castilla y León de este grupo privado. Además dispone de un tercer centro, el Centro Paracelso que funciona como centro de atención primaria y con algunas especialidades. 

Además de la cobertura sanitaria, la Universidad de Valladolid cuenta con una Escuela Universitaria de Enfermería y una Facultad de Medicina, en la que se imparten los estudios de Medicina, Logopedia y Nutrición y Dietética. Los estudios de Medicina en Valladolid se remontan al siglo XV, siendo la primera facultad de Medicina erigida en España, y la ciudad cuenta con la segunda Real Academia de Medicina más antigua de España.

Asociados a la institución universitaria, se encuentran diversos centros de investigación sanitaria: el Instituto de Oftalmobiología Aplicada (IOBA), creado en 1994; el Instituto de Farmacoepidemiología (IFE), dedicado a la investigación sobre la seguridad y los efectos de los medicamentos en la población; el Instituto de Ciencias Médicas (ICIME); el Instituto de Biología y Genética Molecular (IBGM), adscrito al Consejo Superior de Investigaciones Científicas (CSIC) o el Centro Nacional de la Gripe.

Cerca del río Pisuerga, junto con el que por mucho tiempo fue el único camino de entrada a la ciudad, el Puente Mayor, atravesando las calles de la antigua judería de la ciudad, se disponen una serie de plazas y calles con abundancia de antiguos templos y edificios nobiliarios civiles. En este entorno se emplazan el Palacio de los Condes de Benavente, en la plaza de la Trinidad, la conventual calle de Santo Domingo de Guzmán y la iglesia de San Agustín, reconvertida hoy en archivo municipal.

En la Plaza de San Pablo, núcleo de la vida cortesana en tiempos de Felipe III y que vio nacer a su predecesor Felipe II, se halla la Iglesia de San Pablo, que presenta una fachada de Simón de Colonia, en estilo gótico isabelino, que se asemeja a un retablo en piedra. Corresponde al último periodo del estilo gótico. Fue escenario de numerosas ceremonias reales, primera sepultura del infante Alfonso y Juan II, o lugar de bautismo de Enrique IV, Felipe II, Felipe IV y Ana de Austria. Aquí contrajeron matrimonio Maximiliano II y María de Austria, y tomó el capelo cardenalicio Adriano de Utrecht, que sería con el tiempo el Papa Adriano VI. Fue lugar predilecto de numerosos obispos que después desempeñaron su actividad pastoral en el Nuevo Mundo.
En el lateral opuesto de la plaza, el Palacio Real, residencia de los monarcas españoles Carlos I, Felipe II y Felipe III y también de Napoleón Bonaparte durante la Guerra de Independencia, ha llegado al presente con numerosas alteraciones estructurales de sus primitivas trazas, concluidas en torno a 1528. Aquí nació en 1605 Felipe IV. Fue construido por Luis de Vega arquitecto de Carlos I y su patio renacentista posee decoración de medallones atribuidas a Esteban Jamete y escudos de los diferentes territorios pertenecientes al Imperio español. En el siglo XVIII Ventura Rodríguez construyó la escalera neoclásica.

La esquina con la calle de Las Angustias está ocupada por el Palacio de Pimentel, en el que, por no contar entonces la emperatriz Isabel con residencia propia en Valladolid, nació, en 1527, Felipe II. El edificio, construido en ladrillo, tiene dos notables detalles en piedra: la portada con arco carpanel y la esquina con ventana angular plateresca.
La calle Cadenas de San Gregorio alberga las cuatro dependencias del Museo Nacional de Escultura: el Colegio de San Gregorio, la Iglesia de San Benito el Viejo, el Palacio de Villena y el Palacio del Conde de Gondomar (Casa del Sol).
Junto al Palacio de Villena, en la calle Fray Luis de Granada, se encuentra la casa donde nació y vivió el poeta romántico José Zorrilla, y que acoge la Casa Museo de Zorrilla.
En las inmediaciones, la iglesia de San Martín destaca por su esbelta torre, realizada en traza románica a principios del siglo XIII. Por su parte el clasicismo impera en la fachada de la Iglesia Penitencial de Nuestra Señora de las Angustias, erigida a principios del siglo XVII, con escultura monumental de Francisco del Rincón.

Frente a este último templo, inaugurado en 1864 según proyecto de Jerónimo de la Gándara, se encuentra el Teatro Calderón. Su emplazamiento y estructura sigue las corrientes del momento. La fachada se mueve dentro del gusto clasicista y en su interior se encuentra la sala de espectáculos, en forma de herradura, a la italiana. Está decorada con pinturas de Augusto Ferri, escenógrafo de la época. En el escenario existe un sistema de tramoya debida al ingeniero italiano Egidio Piccoli. Detrás del teatro se encuentra el Palacio Arzobispal, que fue propiedad del Juan de Villasante y María de Villarroel, construido a mediados del XVI. En 1857 se convirtió en sede del primer arzobispo vallisoletano, Luis de la Lastra y Cuesta.

Con origen en el "trazo a cordel" de las calles con soportales que sucedieron al incendio de 1561, el llamado núcleo histórico de Valladolid se articula a partir de la Plaza Mayor mediante los siete viales que la atraviesan.

Urbanizada en el siglo XVI, la Plaza Mayor de Valladolid es la primera plaza mayor regular de España, y sirvió de modelo, desde el siglo XVII, para otras muchas en España y Sudamérica: en el siglo XIX, la antigua "Plaza del Mercado" se convirtió en centro cultural, político, económico y social de la ciudad.

En 1908 abrió sus puertas la actual Casa Consistorial, un palacio de cuatro torres, planta rectangular y patio interior, de cuyo frontal sobresale una tribuna que soporta el balcón principal. Este edificio es obra de Enrique María Repullés, inspirado en el proyecto de Antonio de Iturralde, pero modificándolo para imitar los modelos de la arquitectura renacentista española.

Frente a la Casa Consistorial, en el lugar que ocupó hasta el siglo XIX el Convento de San Francisco, donde falleció Cristóbal Colón, se encuentra el Teatro Zorrilla. El teatro fue inaugurado en octubre de 1884, con la obra "Traidor, inconfeso y mártir", contando con la presencia del propio autor de la obra, José Zorrilla, y del poeta vallisoletano Emilio Ferrari.

En uno de los laterales de la Casa Consistorial, la Iglesia de Jesús mantiene una fachada de modelo neorrománico de tipo catalán, en ladrillo prensado.

Atravesando la Plaza de la Rinconada, a espaldas del edificio del Ayuntamiento, en la que se levanta el Palacio de Correos y Telégrafos, se accede a la Iglesia de San Benito el Real, de la orden benedictina, uno de los templos más antiguos de Valladolid. Fue erigido sobre el antiguo Alcázar Real, y está realizado en estilo gótico, aunque la fachada es posterior: fue diseñada por Rodrigo Gil de Hontañón a mediados del siglo XVI. En el interior destaca la reja del mismo siglo, que abarca las tres naves de la iglesia. Junto a ésta se halla el Mercado del Val, que data del siglo XIX.
Muy cerca, la Iglesia de San Miguel y San Julián, sobre el punto topográfico más alto de la ciudad, fue templo de la Compañía de Jesús en Valladolid, como lo atestiguan la fachada y la estructura interior, conformes al modelo romano. En su interior se pueden contemplar obras de Gregorio Fernández y relieves de Adrián Álvarez y Francisco de Rincón.

En la misma calle de San Ignacio se conservan algunos de los muchos palacios edificados en esta zona en tiempos de Felipe II, tales como el Palacio del marqués de Valverde, en cuyo exterior destaca una ventana en ángulo y la decoración de medallones, y el Palacio de Fabio Nelli, obra del clasicismo renacentista de Juan de Lastra y Diego de Praves. Junto a estos palacios, por una pequeña entrada, se accede a la Plaza del Viejo Coso, la primitiva plaza de toros de Valladolid.

En la Plaza de las Brígidas se encuentra el Convento de las Brígidas, antigua casa Palacio del Licenciado Butrón, convertido ahora en el Archivo General de Castilla y León. La iglesia adosada tiene una fachada de ladrillo de uniones a regla.

La Iglesia Penitencial de Nuestra Señora de la Vera Cruz, en el extremo de la Calle de la Platería, fue diseñada por Diego de Praves en 1596. Alberga esculturas procesionales en madera policromada, pertenecientes a la Cofradía de la Vera-Cruz.

A orillas del hoy desviado ramal meridional del río Esgueva, fue erigida por Pedro Ansúrez la Colegiata de Santa María, destinada a ser la cabeza religiosa de su nuevo y próspero feudo. Durante la primera mitad del siglo XII se celebraron en el templo tres Concilios nacionales, y resultando insuficiente o de poco rango el edificio, se levanta uno nuevo a partir del siglo XIII siguiendo la nueva arquitectura del Císter.
La inacabada Catedral de Nuestra Señora de la Asunción fue proyectada por Juan de Herrera con una traza de gran monumentalidad, proporción dupla para dos cuadrados iguales con crucero y torres en cuatro esquinas, pero la escasez de rentas del recién creado obispado vallisoletano, la muerte del arquitecto y de Felipe II, principales promotores de la obra, y la falta de recursos y de interés en su término durante los siglos posteriores, dio lugar a que solamente llegara a construirse casi la mitad de lo ideado por Herrera. Adosados a sus muros, perviven los restos románicos y góticos de la Colegiata, del siglo XIII, a la que sustituyó como iglesia mayor de la ciudad. El retablo principal de la Catedral es obra de Juan de Juni.
En la Plaza de la Universidad se levanta el edificio principal de esta institución. El edificio histórico de la Universidad de Valladolid fue construido en el siglo XVIII según la traza de Fray Pedro de la Visitación; su decoración escultórica es obra de Antonio Tomé e hijos.

Muy cerca se levanta, desde el siglo XI, la Iglesia de Santa María La Antigua, fundación del conde Pedro Ansúrez, señor de Valladolid, con un esbelto campanario de influencia francesa, conocida como "La reina de las torres románicas de Castilla", rematada con un tejado apiramidado y un claustro, ambos de principios del siglo XIII. El resto de la estructura y el interior son de estilo gótico. A comienzos del siglo XX las naves góticas del templo amenazaban ruina y en 1917 se procedió a su derribo iniciándose la reconstrucción en estilo neogótico.

A medio camino entre estos lugares y la Plaza Mayor, se conserva el Pasaje Gutiérrez, galería comercial construida en 1885 al estilo europeo del momento, siguiendo el diseño de Jerónimo Ortiz de Urbina, y que constituye junto al Pasaje de Lodares de Albacete, los únicos ejemplos de este tipo de construcciones en toda España. Cerca del Pasaje, se encuentra la Iglesia de El Salvador, donde, según la tradición, fue bautizado el patrón de Valladolid, San Pedro Regalado.

Bordeando el Campo Grande, en la Acera de Recoletos, gran arteria de expansión de la burguesía, se mantienen edificios de finales del siglo XIX y principios del XX: la Casa Mantilla, de 1891, de estilo ecléctico, con inspiración renacentista, o la modernista Casa del Príncipe, de 1906, obra de Jerónimo Arroyo, arquitecto palentino formado en la escuela de Barcelona.
Tras atravesar la Acera de Recoletos se llega a la Plaza de Colón, donde hasta el siglo XIX se ubicó el Convento de San José. A unos metros de la plaza se encuentra la Estación de Valladolid-Campo Grande, la principal estación de ferrocarril de la ciudad.

Rodeando el Campo Grande, en el Paseo de Filipinos, la Iglesia de San Juan de Letrán destaca por su fachada y sus bóvedas cubiertas con yeserías barrocas, ambas del siglo XVIII, obra de Matías Machuca; el cuerpo de la iglesia es de finales del XVII.

Del siglo XVIII es también el Convento de los Agustinos Filipinos. El edificio, cuya fachada encara con la puerta posterior del Paseo del Príncipe del Campo Grande, fue diseñado por Ventura Rodríguez.

Siguiendo el recorrido alrededor del parque, encontramos la Academia de Caballería de Valladolid, que data de 1915 y es un edificio historicista rematado por chapiteles similares a los que caracterizaron a los palacios de los Austrias.

Junto a la Academia de Caballería, la Plaza de Zorrilla es un punto clave en el trazado urbano vallisoletano. Presidida por la Estatua de Zorrilla, obra de Aurelio Carretero, enlaza las principales calles de Valladolid: la anteriormente mencionada Acera de Recoletos, el Paseo de Zorrilla, principal bulevar de la ciudad, la Calle Santiago, que desemboca en la Plaza Mayor y las calles Miguel Íscar, Duque de la Victoria y la Plaza España.

Cristóbal Colón murió en Valladolid, el 20 de mayo de 1506. El Ayuntamiento decidió en 1968 levantar una edificación en estilo gótico-isabelino que reprodujese una casa palaciega propiedad de Diego Colón, hermano menor del Almirante, ubicada en la ciudad de Santo Domingo, en la República Dominicana. Este edificio alberga hoy la Casa Museo de Colón.
En la misma calle, en la acera opuesta, la Iglesia de la Magdalena, del siglo XVI, luce en su fachada un gran escudo en piedra, blasón de su patrón, el virrey del Perú y obispo, Pedro de la Gasca. En el interior se pueden contemplar el retablo Mayor, el de Santiago, y el sepulcro en alabastro del mencionado obispo, obra de Esteban Jordán.
El Palacio de Santa Cruz, primer edificio renacentista de España, erigido a partir de 1486 por el arquitecto Lorenzo Vázquez de Segovia con el patrocinio del cardenal Pedro González de Mendoza, presenta una portada de arco de medio punto y un patio de tres pisos, dos en estilo tardogótico y el tercero de matices barrocos, consecuencia de una reforma del siglo XVIII. En su biblioteca, a la que se accede por una puerta plateresca, se conservan en sus estanterías de madera dorada en dos pisos, valiosos documentos, entre ellos el "Beato de Valcabado", del año 970.

En las cercanías, el Monasterio de las Huelgas Reales, de estilo palladiano, conserva un arco mudéjar del que fuera palacio de la reina de Castilla María de Molina. Y en el Convento de Santa Clara, del siglo XV, contrasta la severidad franciscana del exterior con las yeserías barrocas del XVII.

El Convento de las Descalzas Reales fue encargado por Felipe III y Margarita de Austria, en el siglo XVII; tiene un torreón de tres alturas, de tipo palacial, con celosías en los balcones. En el interior es posible apreciar el claustro de estilo toscano, y, en la iglesia, el retablo realizado conjuntamente por Juan de Muniátegui, Gregorio Fernández y Santiago Morán.

El Palacio de los Vivero, construido en el siglo XV, encabeza un complejo de edificios que fue agrandándose a la medida de las necesidades de la administración de justicia. Los Reyes Católicos contrajeron en él matrimonio (1469), y luego decidieron su destino como Real Audiencia y Chancillería.

Durante el siglo XIX y, fundamentalmente, a lo largo del siglo XX numerosos monumentos históricos fueron derribados a partir de la ejecución de distintos planes urbanísticos diseñados para intentar asumir el descontrolado éxodo rural y el crecimiento demográfico de la ciudad durante este periodo, a lo que contribuyó el estado de ruina en el que se encontraban muchos de ellos. De esta forma, multitud de edificios antiguos como el Hospital de la Resurrección, donde Miguel de Cervantes situó su novela "El coloquio de los perros", conventos y claustros como el de San Francisco o el de San José, iglesias como la de San Julián y Santa Basilisa o la de San Miguel, incluyendo decenas de palacios medievales y renacentistas como el de la Ribera, el palacio Gardoqui o la casa de las Aldabas fueron demolidos para construir bloques de pisos de gran altura que rompían con la armonía arquitectónica de la ciudad.

En julio de 1978, el Consejo de Ministros declaró conjunto histórico-artístico a la ciudad, pero para muchos estudiosos la declaración llegó demasiado tarde y no tuvo repercusión posterior. El arquitecto Fernando Chueca Goitia llegó a afirmar, que la destrucción del patrimonio histórico-artístico de Valladolid era de nueve sobre diez.

La escultura urbana en Valladolid está protagonizada por obras que representan a ilustres personajes a los que se les ha recordado de esta manera. Así, en el año 1887 se instaló en la Plaza de la Universidad la estatua de Miguel de Cervantes en pie, con traje de época, pluma y libro en ristre, realizada por Nicolás Fernández de la Oliva. El escritor José Zorrilla también posee una escultura en la plaza que lleva su nombre. El fundador de la ciudad, el conde Pedro Ansúrez, cuenta con un monumento en el centro de la Plaza Mayor, realizada en 1903 por Aurelio Carretero. El Monumento a Colón, obra del artista Antonio Susillo, e inaugurado en 1905, recuerda a la figura del descubridor.

También destacan las esculturas que adornan el Campo Grande de Valladolid, y cerca de él, a la entrada de la Academia de Caballería, el monumento a los Cazadores de Alcántara, de 1931, obra de Mariano Benlliure.

Lejos de ahí, preside la Plaza de San Pablo una estatua de Felipe II; realizada en 1964 por Federico Coullaut-Valera, es copia de la que está en la Plaza de la Armería de Madrid e imita el modelo de Pompeo Leoni. A la entrada de la calle Cadenas de San Gregorio, se instaló, en 1982, la escultura en hierro "Lo profundo es el aire", de Eduardo Chillida, un homenaje al poeta vallisoletano Jorge Guillén.

Entre las esculturas a "escala humana" destacan "El Comediante", en la Plaza de Martí Monsó, obra de Eduardo Cuadrado; la escultura de Rosa Chacel que se encuentra en uno de los bancos de los jardines del Poniente y que fue realizada por Luis Santiago Pardo en 1996. Otros ejemplos son "El Encuentro", obra de Feliciano Álvarez Buenaposada, que se encuentra desde 1997 en la Plaza Madrid; la titulada "Candia", situada en el Parque Ribera de Castilla; el monumento al torero Fernando Domínguez, en la plaza de toros; la escultura "Baile en bronce", homenaje al bailarín Vicente Escudero; o las dedicadas a Einstein y a Pío del Río Hortega, en la plaza del Museo de la Ciencia.

Entre las nuevas construcciones cabe destacar las instaladas en la prolongación del Paseo de Zorrilla: "Stage Set for a Film" (Decorado para una Película), de Dennis Oppenheim; las "Puertas de Valladolid", de Cristóbal Gabarrón; y la "Columna forma de sonido", de Lorenzo Frechilla. Otra es el "Monumento al IV Centenario de la ciudad de Valladolid", construida en 1999 por Ángel Mateos Bernal, situada frente al recinto ferial de Castilla y León, en la Avenida de Salamanca.

En otras ocasiones las esculturas comparten su protagonismo con el agua, en fuentes como la de "Los Colosos" (Pedro Monje, 1996), en la Plaza de la Rinconada; la "Fuente de las Sirenas" (Concha Gay, 1996), en la Plaza de Martí Monsó; la titulada "Jorge Guillén y la infancia" (Luis Santiago Pardo, 1998), situada en la glorieta central de los jardines del Poniente; la "Fuente Dorada" (Fernando González Poncio, 1998), en la plaza del mismo nombre; y otras.

El parque más antiguo y más emblemático de la ciudad es el Campo Grande; se trata de un gran jardín romántico, ubicado en pleno centro de Valladolid, ideado en su actual fisonomía por Miguel Íscar, alcalde de Valladolid entre 1877 y 1880. Acoge una gran variedad de árboles que constituyen un verdadero jardín botánico. Habitan diferentes aves y son famosos los pavos reales y, recientemente, las ardillas.
Sobre el antiguo ramal norte del Esgueva se construyeron, también a finales del siglo XIX, los jardines del Poniente:

Se trata de un sencillo jardín en cuyo centro hay dos pérgolas que albergan una pequeña plaza en la que se encuentra una fuente que recuerda la obra del escritor vallisoletano Jorge Guillén.

A lo largo del curso del Pisuerga también abundan las zonas verdes. Comenzando por el norte, el Parque Ribera de Castilla (inaugurado el 20 de marzo de 1988), con una superficie de 12 hectáreas, está poblado de distintas especies de chopos, álamos o tilos. Siguiendo el discurrir del agua, el Parque de las Moreras cuenta con varios paseos, zonas deportivas y una playa fluvial. Junto a él se encuentra la Rosaleda Francisco Sabadell, un pequeño jardín exclusivamente formado por rosas.

Otras zonas verdes son el Pinar de Antequera, principal recurso natural de la capital vallisoletana;el Parque Forestal de La Fuente del Sol, histórico espacio verde junto al barrio de La Victoria, el parque de Las Norias de Santa Victoria, que ocupa las antiguas instalaciones de la fábrica azucarera Santa Victoria, el Jardín Botánico de la Victoria, que cuenta con 30 especies distintas de árboles junto con una muestra de especies autóctonas, el Parque Fuente de la Salud del Barrio Los Pajarillos, el Parque de Canterac y de la Paz en Las Delicias o el Parque del Mediodía en Parquesol.

Siguiendo el curso del río Pisuerga, lo atraviesan los siguientes puentes:

Sobre el río Esgueva cruzan un importante número de puentes. Como el río es bastante más reducido y la configuración actual obedece a su canalización por un extremo de la ciudad, suprimiendo su paso por el centro y sus varios ramales, estas estructuras no tienen nombre propio, sino que toman el correspondiente a la calle que atraviesa el río.

En el parque situado detrás de la Facultad de Filosofía y Letras, junto al cauce del río, se ha diseñado un estanque en el que se encuentran los restos de una de las cercas de la ciudad.



Además, por su cercanía a la capital es destacable el Archivo General de Simancas que guarda la documentación de la Monarquía Hispánica desde los Reyes Católicos hasta la instauración del Régimen Liberal. En cuanto a privados, el más importante es el Archivo Diocesano de Valladolid que atesora los fondos generados por la colegiata y luego por la catedral, la documentación de la curia diocesana, el conjunto de archivos parroquiales de la diócesis y un gran archivo musical con más de 6000 partituras.














Se pueden visitar 3 casas museo:




La Casa de la India, en unión con los centros homólogos de Londres y Berlín, es una institución cultural creada para favorecer el diálogo entre los pueblos de la India y España, y para promover el desarrollo de sus relaciones en los ámbitos culturales, sociales e institucionales.

La Semana Santa es el evento cultural más importante de la ciudad, debido a sus valiosas tallas policromadas de los siglos XVI y XVII de Juan de Juni, Gregorio Fernández o Francisco del Rincón, muchas de ellas expuestas durante el resto del año en el Museo Nacional de Escultura, atrayendo anualmente a visitantes de toda España y el resto del mundo.
Esta celebración fue declarada de Interés Turístico Internacional en 1980, siendo de esta manera la primera celebración de la Semana Santa en España en ostentar dicha declaración. En 2014 se iniciaron los trámites para conseguir su reconocimiento como patrimonio cultural inmaterial de la Humanidad.

En la Semana de Pasión, y siempre que no haya lluvia, las veinte cofradías vallisoletanas procesionan por el casco histórico de la ciudad. La historia de la Semana Santa en Valladolid se remonta al siglo XV, si bien anteriormente hubo procesiones en el interior de los conventos, donde nacieron las cofradías más antiguas como Santa Vera Cruz, Angustias, La Piedad, La Pasión y Nuestro Padre Jesús Nazareno.

Durante la Semana Santa vallisoletana se pueden contemplar por las calles una de las principales exposiciones de imaginería religiosa del mundo. Pasos como la "Virgen de las Angustias", una de las principales tallas de Juan de Juni, "La Sagrada Cena", de Juan Guraya, "La Oración del Huerto", de Andrés de Solanes, "El Señor Atado a la Columna" y "El Descendimiento", de Gregorio Fernández, o "Las lágrimas de San Pedro", de Pedro de Ávila, recuerdan al ciudadano el vínculo existente entre la religión y el arte.

La Semana Santa de Valladolid no sólo se distingue por la singularidad artística y gran valor de sus pasos sino también por la sobriedad, el silencio y el respeto que reina en cada acto.

Dentro de la Semana de Pasión vallisoletana destacan actos como el Pregón y el Sermón de las Siete Palabras que transforma la Plaza Mayor de Valladolid en un escenario que parece remontarse al siglo XVI y la Procesión General de la Sagrada Pasión del Redentor del Viernes Santo que hace un recorrido desde la Última Cena hasta la soledad de la Virgen y en el que se pueden contemplar los 32 conjuntos escultóricos más importantes.

El Corpus Christi es una celebración religiosa católica.

Anualmente se celebra la Semana Internacional de Cine de Valladolid (SEMINCI), a finales de octubre. Creada en 1956 como «Semana de cine religioso de Valladolid», se celebraba en Semana Santa, evolucionando hasta convertirse en uno de los principales festivales de cine de España, y el segundo más antiguo, teniendo como objetivo la difusión y promoción de películas de categoría artística, que contribuyan al conocimiento de la cinematografía mundial.

El festival tiene su sede principal en el Teatro Calderón, donde se celebran la gala de inauguración, la proyección de películas de la "Sección Oficial" y la gala de clausura, en la que se entrega la "Espiga de Oro", principal galardón del festival.

Por la SEMINCI han desfilado personajes del mundo cinematográfico, como Ken Loach, Brad Pitt, Kenneth Branagh, Ang Lee, Sophia Loren, Julie Christie, John Cleese, María de Medeiros, Liv Ullmann, Abbas Kiarostami, Atom Egoyan o Mira Sorvino.

A lo largo del año se celebran numerosos eventos culturales en la ciudad. Cronológicamente, el primer fin de semana después de la festividad de Reyes se celebra la concentración motorista invernal Pingüinos, la más numerosa de Europa, en la que se realizan todo tipo de actividades relacionadas con el mundo de las dos ruedas.

Entre finales de abril y principios del mes de mayo se celebra en la Plaza Mayor la Feria del Libro de Valladolid. En su cuadragésimo sexta edición, la feria congregó a más de 130 autores de todo el mundo. Por ella han pasado Fernando Savater, Juan Manuel de Prada o Antonio Gamoneda entre otros. Anteriormente y en el paseo central del Campo Grande se celebra también la Feria del Libro Antiguo y de Ocasión en la que participan más de 20 librerías de toda España.

Durante el mes de mayo se desarrolla la Semana del Renacimiento, con la celebración de un mercado renacentista, con la recreación de sabores, olores y personajes del Valladolid del siglo XVI. Estos días además se teatraliza por las calles "La Ruta del Hereje", popularizada tras la obra de Miguel Delibes, mientras que los restaurantes ofrecen menús gastronómicos rescatados del siglo XVI y actualizados por restauradores vallisoletanos. También en mayo se celebra el Festival Internacional de Teatro y Artes de Calle de Valladolid (TAC). Los espectáculos son nacionales y extranjeros, concebidos para ser exhibidos en espacios sin butacas.
En primavera tenía también lugar el afamado e internacionalmente consolidado festival de música Valladolid Latino en el que desde 2006 y hasta 2015 han intervenido artistas como Alejandro Sanz, Juanes, Paulina Rubio, Julieta Venegas o Marc Anthony entre otros muchos.

Durante los meses veraniegos se celebran Las Noches de San Benito con conciertos y cine al aire libre.

En 2007 se ha creado un ciclo denominado Música en la Catedral, aprovechando la adquisición de un órgano electrónico Allen en detrimento del antiguo órgano de la Catedral construido en dos fases (1904 y 1932) por Aquilino Amezua y Leocadio Galdós y que es un valioso instrumento con tres teclados y pedal y 36 juegos, de estilo romántico-sinfónico.

Se ha celebrado en 2007 la primera edición de la Bienal de Escultura de Valladolid, de escultura contemporánea, que en cierta medida viene a complementar al Museo Nacional de Escultura de la ciudad.

Por último, se celebran las Fiestas Patronales de San Pedro Regalado, en plena primavera -13 de mayo-, con una corta celebración en la que en la que el mercado medieval, la gastronomía y la música son los principales protagonistas y, luego, la Feria y Fiestas de Nuestra Señora de San Lorenzo que se celebran a principios de septiembre cuyo día central es el 8 de septiembre festividad de la patrona. El programa da paso a diversas actuaciones musicales, teatro, ferias gastronómicas, corridas de toros, citas artesanales, fuegos de artificio o exposiciones entre otras actividades. Durante esta semana festiva destaca la Feria de Día, los conciertos de la Plaza Mayor, las casetas gastronómicas regionales, la Feria de Muestras o los Fuegos Artificiales.

El Concurso Nacional de Pinchos y Tapas "Ciudad de Valladolid", celebrado desde el año 2005, reúne a los representantes de todas las comunidades autónomas de España en torno a la disciplina más característica de la gastronomía española: la elaboración de tapas y pinchos.
El encuentro tiene lugar en la primera quincena de noviembre. En él participan especialistas de máximo nivel y la propia hostelería de Valladolid que ofrece, en sus establecimientos, las creaciones de los finalistas. Además a principios de junio se celebra el Concurso Provincial de Tapas y Pinchos.

La Feria de Valladolid dispone de un parque ferial integrado por cuatro pabellones cubiertos, auditorio, centro de congresos, salas de conferencias y espacios al aire libre, en el que es posible celebrar cualquier tipo de la actividad ferial. Durante el año se suceden diferentes eventos o salones: La Feria Internacional de Muestras celebrada durante el mes de septiembre, INTUR (Feria de Turismo de Interior), Expobioenergía (Feria Tecnológica en Bioenergía), AR&PA (Bienal de Restauración y Gestión del Patrimonio), Alimentaria (Salón de Alimentación) (Bienal) o Agraria (de maquinaria agrícola) entre otros. El Centro de Congresos es otro escenario para el desarrollo de diferentes actividades profesionales. Se trata de un conjunto de salas versátiles con aforos que oscilan entre 60 y 240 plazas, un auditorio con capacidad para 600 personas equipado con la tecnología necesaria para atender las demandas de este tipo de reuniones y pabellones en los que tienen cabida hasta 10.000 personas.
Existen alrededor de una treintena de salas expositivas públicas que, durante todo el año, exhiben en Valladolid las diferentes muestras de creatividad artística tanto de artífices vallisoletanos como de los procedentes de otros puntos, españoles o foráneos, de épocas actuales o anteriores. Destacan la Sede de la Fundación Municipal de Cultura, la Sala Municipal de Las Francesas, sita en la iglesia del antiguo convento de las Francesas la moderna Cúpula del Milenio, la Sala de San Benito dedicada desde 1994 exclusivamente a la fotografía con proyectos internacionales en su gran mayoría, o la Sala de la Pasión, en el acondicionado espacio de la antigua iglesia barroca de la Cofradía de la Pasión, dedicada a la pintura, escultura, dibujo, grabado, video diseño, y otras artes plásticas. También la Sala de exposiciones del Teatro Calderón, dedicada a la presentación de obras de artistas locales a partir de una convocatoria pública anual, además de otras exposiciones en colaboración con instituciones, o los espacios reservados a estas funciones en los diferentes Centros Cívicos de la ciudad.

Valladolid es citada tópicamente como lugar donde se habla "el mejor castellano". Esta tradición parece remontarse al siglo XVII, a partir de la referencia que a Marie-Catherine d'Aulnoy (a propósito de su viaje por España, que quedó reflejado en su obra "Relato del viaje a España") se le hizo sobre la pureza del castellano en la ciudad. Este hecho se está concretando en el fomento de iniciativas para la creación de centros especializados en la enseñanza de la lengua española para extranjeros. Pero a pesar de esta fama, el habla de los vallisoletanos se caracteriza por rasgos diatópicos como el leísmo, el laísmo y otros propios del dialecto castellano septentrional.

Valladolid acogió en el año 2001 el II Congreso Internacional de la Lengua Española que se desarrolló bajo el título «El español en la Sociedad de la Información» entre el 16 y el 19 de octubre de dicho año.

Miguel de Cervantes máximo exponente de la literatura española y universalmente conocido, vivió en Valladolid, durante 2 etapas de su vida, la primera ocasión que Miguel de Cervantes residió en Valladolid contaba solamente 4 años, pasados los años volvió a recalar en Valladolid con la llegada a dicha ciudad de la corte del rey Felipe III en 1601, durante esta última estancia escribió parte de su obra culmen, Don Quijote de la Mancha y en esta ciudad se encontraba el autor cuando en 1605 se publica dicha novela.

Hasta cuatro premios Cervantes, la máxima distinción literaria en lengua castellana, están vinculados a Valladolid: Miguel Delibes, Jorge Guillén (ambos naturales de la ciudad), Francisco Umbral y José Jiménez Lozano (residentes durante muchos años). Otros autores destacados nacidos en la ciudad o muy vinculados a ella son Miguel de Cervantes, José Zorrilla, Gaspar Núñez de Arce, Rosa Chacel, Francisco Pino, Blas Pajarero, Gustavo Martín Garzo, José María Luelmo, Fernando de Orbaneja o José Manuel de la Huerga.

El Centro Cultural Miguel Delibes, inaugurado en 2007, es sede de la Orquesta Sinfónica de Castilla y León, del Conservatorio Profesional de Música de la ciudad, de la Escuela Superior de Arte Dramático y de la Escuela Profesional de Danza y Teatro Experimental. Además, está equipado con un auditorio con capacidad para 1.700 espectadores, una sala para música de cámara y otra más para teatro experimental; su apertura ha sido clave en la cultura musical.

Los teatros Calderón (remodelado en 1999) y Zorrilla (reconstruido entre 2005 y 2009) ofrecen una programación que abarca la mayoría de las artes escénicas y musicales.

El Teatro Carrión, reabierto en 2013, acoge desde 2014 la sede de la Orquesta Filarmónica de Valladolid, con temporada de ópera, zarzuela y conciertos.

Existen numerosas agrupaciones corales y dos jóvenes orquestas: la Joven Orquesta Sinfónica de Valladolid y la Joven Orquesta de la Universidad de Valladolid. Además, proceden de la ciudad formaciones de música folclórica como Candeal o Tradere, de música infantil como La Carraca, y en el ámbito de la música moderna, destacan los Celtas Cortos, grupo de rock celta de gran éxito en los años 90, y otros como Greta y los Garbo, Los Mismos, Triquel o Arizona Baby.

Entre los personajes reconocidos dedicados al mundo de la interpretación destacan grandes actores como Lola Herrera, Concha Velasco, Emilio Gutiérrez Caba, Diego Martín, Roberto Enríquez, Elvira Mínguez, Ágata Lys, la actriz y modelo Inés Sastre, Juanjo Pardo, Emilio Laguna, Julia Torres, Paloma Valdés, Daniel Muriel, Sara Rivero, Nacho López, Fernando Cayo, Ana Otero o las hermanas Loreto y Marta Valverde. En el mundo de la televisión han adquirido gran relevancia Patricia Conde, Deborah Ombres o Manu Carreño. También es vallisoletano de nacimiento el expresidente del Gobierno José Luis Rodríguez Zapatero así como la actual vicepresidenta del Gobierno Soraya Sáenz de Santamaría, la ministra de Agricultura Isabel García Tejerina o el ex ministro del Interior Jorge Fernández Díaz.

La gastronomía vallisoletana se inserta en la gastronomía castellana. Ocupa un lugar preferente la carne y los asados; uno de los platos más típicos es el asado de lechazo condimentado con agua y sal y cocinado en horno de leña (asado al estilo castellano). Les siguen, el cochinillo o el cabrito y los alimentos de la caza como perdices, codornices y conejo, se cocinan aquí braseados o escabechados. El queso de la zona se elabora con leche de oveja, lo que significa un fuerte sabor en varios grados de curación. Son quesos de Valladolid marcas como Entrepinares o Flor de Esgueva.

Naturalmente se trata de platos que necesitan para su completo disfrute del pan y del vino, dos elaboraciones que desde hace siglos se elaboran en esta zona. Se pueden degustar decenas de texturas de pan de cereal castellano. De ellos, el más famoso es el pan "lechuguino" pero también destacan el pan de picos o el cuatro canteros. En Valladolid pueden degustarse vinos de gran calidad como son los adscritos a las cinco denominaciones de origen de la provincia vallisoletana: los tintos de la Denominación de Origen Ribera del Duero, los blancos de Rueda o los rosados de la Denominación de Origen Cigales, la de Toro y la DO Tierra de León.

Un buen postre empieza en Valladolid por la repostería salida de las manos artesanas de los conventos y se complementa con el "café de puchero". Se puede acompañar de pastas artesanales, como los mantecados de Portillo (popularmente conocidos como "zapatillas") o con los "bizcochos de Santa Clara", los empiñonados, los buñuelos de crema o las almendras garrapiñadas.


Valladolid es el centro del deporte en Castilla y León, así como un referente deportivo de primera categoría a nivel nacional, al poseer equipos de élite en la mayoría de los deportes más populares, destacando particularmente en la ciudad la práctica del rugby, con dos de los equipos punteros de la División de Honor de rugby, El Salvador y el VRAC, que suman entre los dos, once Campeonatos Nacionales de Liga, ocho Copas del Rey y siete Supercopas de España, habiendo aportado históricamente un importante número de jugadores a la selección española de rugby.

Los equipos más representativos de la ciudad son: el Real Valladolid con más de cuarenta temporadas en la primera división del fútbol español, campeón de una Copa de la Liga en 1984 y dos veces subcampeón de la Copa del Rey de Fútbol. El club disputa sus partidos locales en el Nuevo José Zorrilla. Anualmente se celebra tanto el Trofeo Ciudad de Valladolid como el Trofeo Diputación de Valladolid.

También destacan el BM Aula Cultural que milita en la máxima categoría del balonmano femenino español y el BM Atlético Valladolid creado en 2014 y que milita en la temporada 2016/2017 en la Liga Asobal (sustituyendo al desaparecido Club Balonmano Valladolid, que ganó una Recopa de Europa, una Copa ASOBAL y dos ediciones de la Copa del Rey de Balonmano); el extinto Club Baloncesto Valladolid, uno de los equipos históricos de la liga ACB de baloncesto que ha dejado paso al actual Club Baloncesto Ciudad de Valladolid desde junio del año 2015 y que ha iniciado su andadura en LEB Plata, logrando recientemente el ascenso a LEB Oro; y los dos equipos de rugby antes mencionados, el VRAC y el Club de Rugby El Salvador, siendo este último el que más triunfos ha brindado a los vallisoletanos.
La oferta deportiva de Valladolid se completa con destacados equipos de tenis de mesa (Collosa Telecyl), baloncesto en silla de ruedas (BSR Valladolid), fútbol sala, hockey en línea (CPLV) y varios importantes clubs de piragüismo con base en el Pisuerga. Cuenta la ciudad, además, con cuatro campos de golf, múltiples clubs de fútbol, baloncesto, balonmano, tenis, atletismo, natación, ciclismo, voleibol, artes marciales, deportes autóctonos, caza y pesca, así como clubs e instalaciones deportivas de otras disciplinas. Además es la ciudad de deportistas de alto nivel como Mayte Martínez, Rubén Baraja, Laura López Valle, Isaac Viciosa, Miriam Blasco o Roldán Rodríguez (la mayoría ya retirados de la alta competición) y jóvenes atletas como Álvaro Rodríguez o Mohamed Elbendir y la nadadora paralímpica Amaya Alonso.

Valladolid ha acogido varios eventos deportivos relevantes, habiendo sido sede de la Copa Mundial de Fútbol de 1982, del Campeonato Mundial de Gimnasia Rítmica de 1985, de la final del Campeonato de Europa de Fútbol sub-21 de 1986, de la Fase Final de la Willi Brinkmann Eurocup de baloncesto en silla de ruedas en el 2009, de la Liga Europea de voleibol, así como de importantes pruebas ciclistas (entre ellas múltiples etapas de la Vuelta Ciclista a España), campeonatos de tenis, veladas de boxeo, concursos hípicos, etc. En 2016 se celebró la final de la Copa del Rey de Rugby en el Estadio José Zorrilla con más de 26.000 espectadores en las gradas, que lo convirtió en el partido entre dos equipos españoles (El Salvador y VRAC) con más público en las gradas. En las cercanías de Valladolid también se encuentran las sedes de dos importantes concentraciones motociclistas anuales de carácter internacional: "Pingüinos" y "Motauros" (Tordesillas). En 2018 será sede del Campeonato Europeo de Gimnasia Rítmica.
Los medallistas olímpicos vallisoletanos han sido: Adolfo Mengotti (Plata en fútbol en París 1924, compitiendo con Suiza), Marcelino Gavilán y Ponce de León (Plata en equitación en Londres 1948), Ángel León Gozalo (Plata en pistola libre 50 m. en Helsinki 1952), José Luis Llorente (Plata en Baloncesto en Los Ángeles 1984), Narciso Suárez Amador (Bronce en Piragüismo en aguas tranquilas en Los Ángeles 1984), Miriam Blasco (Oro en Yudo en Barcelona 1992), Fernando Hernández Casado y Raúl González Gutiérrez (ambos Bronce en Balonmano en Atlanta 1996), Laura López Valle (Plata en Natación sincronizada en Pekín 2008) y Juan Carlos Pastor (Bronce en Balonmano en Pekín 2008, como entrenador).

Las principales vías de acceso a la ciudad son:

El ayuntamiento de Valladolid, tiene desde hace años un Sistema de Préstamo de Bicicletas llamado VallaBici, modernizado en 2013, como medio de transporte público individualizado, cómodo, saludable, ecológico y fácil de usar. Así mismo, el sistema es totalmente electrónico y sus horarios son desde las 6 hasta las 24 horas todos los días del año. Hay repartidos por la ciudad 34 puntos de préstamo y 260 bicicletas para poder elegir en cada momento dónde coger o dejar la bicicleta de una forma rápida y sencilla.

Los servicios de autobús sustituyeron en la década de los años 20 a la red de Tranvía de Valladolid. Tras un largo periodo de gestión mediante concesión privada, desde 1982 el transporte urbano de Valladolid está gestionado por la sociedad municipal Autobuses Urbanos de Valladolid S.A. (AUVASA), que cuenta con una flota de 150 vehículos con una antigüedad media de 13,12 años aunque si tenemos en cuenta únicamente los vehículos de las líneas regulares su vida media es de 10,33 años.

Desde 1988, AUVASA inició junto con otras tres empresas municipales (Badalona, Palma de Mallorca, Barcelona) un proyecto pionero en España para estudiar la viabilidad del Gas licuado de petróleo (GLP) como carburante en el transporte público con el objetivo de reducir las emisiones contaminantes y sonoras.

En la actualidad, del total de la flota el 68% (103 autobuses) funciona con GLP, 46 con biodiésel (31%), un autobús es híbrido (0,6%) y 5 son híbrido-eléctrico (3,31%). En el momento actual 86 autobuses de la flota total (57%) tienen rampa para minusválidos. Además, toda la flota es de piso bajo.

Posee 19 líneas ordinarias, 2 circulares, 9 líneas laborables a polígonos industriales, 2 líneas "lanzadera" al Campus Universitario Miguel Delibes, 7 líneas de servicio especial matinal y 5 de nocturno ("Búho"), 6 líneas "F" que dan servicio al Estadio José Zorrilla en los días de partido, y 5 líneas especiales para diferentes ferias u otros eventos culturales al Real de la Feria.

Existen varias líneas de autobuses urbanos que conectan la capital con los municipios de su área metropolitana. Estos buses dan servicio a municipios como Zaratán, Laguna de Duero, Simancas, La Cistérniga, Tudela de Duero o Arroyo de la Encomienda, suelen tener una frecuencia de media hora o menos. Estos buses suelen tener su última parada o su inicio en la estación de autobuses de Valladolid que se encuentra en la calle Puente Colgante, en el centro de la ciudad a escasos metros de la estación de ferrocarril y de la gran arteria vial de la ciudad, el Paseo de Zorrilla.

A través de los servicios de las diferentes compañías conecta diariamente con varias y de otras provincias de España. También se realizan trayectos internacionales a países de Europa, como Francia, Suiza, Holanda, Bélgica, Gran Bretaña o Alemania.

Valladolid será una de las ciudades pioneras en integración del coche eléctrico en España (junto a Madrid, Barcelona, Sevilla y Palencia), mediante la creación en 2010 de un plan piloto para la instalación de puntos de recarga en la ciudad –similar al proyecto Movele–, pero impulsado desde la Junta.

Valladolid es la primera ciudad española donde se fabrica en serie un coche eléctrico, el Renault Twizy.

El Aeropuerto de Valladolid-Villanubla (IATA: VLL, ICAO: LEVD) está situado a 10 km de Valladolid, en el término municipal de Villanubla, a 846 metros sobre el nivel del mar; fue inaugurado en 1938. La pista del aeropuerto pertenece a la base aérea militar de Villanubla, situada enfrente de la terminal, al otro lado de la pista, y la administración del aeródromo corre a cargo del Ejército del Aire.

Con un tráfico total de 218 293 pasajeros, 4 650 operaciones y 71 553 kilogramos de tráfico de carga en el año 2015 según fuentes oficiales de AENA, es el 31º aeropuerto español por volumen de pasajeros.

Cuenta con 6 destinos regulares:

En el municipio de Valladolid también se encuentra el aeródromo privado de Torozos, situado al norte del aeropuerto de Villanubla.

A través de la Estación de Valladolid-Campo Grande de ADIF, anteriormente de RENFE (también conocida como "Estación del Norte"), Valladolid queda conectada con diversas localidades de la provincia y de Castilla y León y también con el resto de España, con trenes regulares a Madrid, Barcelona, Santander y Bilbao entre otros.

La estación se sitúa sobre la línea convencional Madrid-Irún, una de las principales líneas de la red española. Desde 2007 es también final de línea de la LAV Madrid-Valladolid, que en el futuro se prolongará hacia el norte (País Vasco, Asturias, Galicia...), conformando el denominado Eje Norte-Noroeste de Alta Velocidad. Hasta ese momento se ha dispuesto sobre las vías de la estación un cambiador de ancho dual, que permite que los trenes de ancho variable aprovechen la LAV Madrid-Valladolid y se dirijan posteriormente a otras ciudades del norte de España (Gijón, Santander, Bilbao, Vitoria e Irún).

En 1985, tras 89 años de funcionamiento, se suprimió al tráfico de viajeros el ferrocarril Valladolid-Ariza. Por dicha línea de 245 km circulaban los trenes de Barcelona a Salamanca así como el La Coruña-Barcelona. La línea siguió abierta al tráfico de mercancías hasta 1993. Actualmente solo está en servicio hasta La Carrera, para dar servicio a FASA Renault.

Así mismo, al norte de la ciudad existe un apeadero denominado Valladolid-Universidad que da servicio al Campus Miguel Delibes de la Universidad de Valladolid, y a los barrios de Pilarica y Belén. En este apeadero efectúan parada algunos de los trenes regionales y de Media Distancia que desde Valladolid se dirigen hacia Palencia, Burgos o León.

El sindicato CCOO ha propuesto un tren de cercanías entre Palencia, Valladolid y Medina del Campo, dando servicio a esta conurbación urbana. Dicho proyecto ha sido respaldado por los alcaldes de las localidades implicadas.

El 22 de diciembre de 2007 se inauguró la línea de Alta Velocidad que une la estación de Campo Grande con Madrid en cincuenta y seis minutos a velocidades de 300 km/h y con el uso de trenes Talgo de la Serie 102, apodados «pato». Desde el 26 de enero de 2009, hay servicios de trenes Avant, conocidos como «lanzaderas», que unen Valladolid con Segovia y Madrid a precios muy inferiores a los de los primeros, y más aún con el uso de bonos de viaje. La duración en lanzadera del viaje entre Valladolid y Madrid es de aproximadamente una hora.

El 29 de septiembre de 2015 se inauguró la línea de alta velocidad Valladolid-Palencia-León por lo que estas tres ciudades quedaron conectadas por AVE. Los tiempos de viaje desde Valladolid a estas capitales se han reducido notablemente: veintinueve minutos a Palencia y setenta a León. Esta línea es empleada por diversos servicios comerciales desde Madrid: un total de cuarenta y cinco servicios semanales por sentido entre Madrid y León, más los veintiuno entre Madrid y Santander, que circulan por la línea hasta el cambiador de Villamuriel. Hay dos servicios AVE diarios entre Madrid y León (con trenes de la Serie 112), cuatro servicios Alvia hasta Gijón (Serie 130), tres servicios Alvia hasta Santander (Serie 130), un servicio Alvia hasta Ponferrada (Serie 121) y un AV City hasta León (Serie 121).

Se encuentra en avanzado estado de construcción la fracción hasta Burgos de la línea de alta velocidad Venta de Baños-Burgos-Vitoria, que dotará de nuevos servicios a Valladolid cuando sea inaugurada. Las obras de plataforma de todos sus tramos fueron adjudicadas a lo largo del año 2009, el montaje de las vías entre 2014 y 2015.

Dado que el trazado del ferrocarril atraviesa el centro del casco urbano, dividiéndolo en dos partes con una barrera de difícil comunicación, se han planteado desde los años 1980 diversas soluciones al problema, y con mayor intensidad desde que ya fue inminente la llegada de la alta velocidad a la capital. Las opciones barajadas iban desde la mejora de la integración urbana del trazado, manteniéndolo en superficie, al desvío de las líneas por un nuevo trazado externo a la ciudad, pasando por el hundimiento del trazado urbano en trinchera o su soterramiento con tuneladora o falso túnel mediante muros pantalla, desde las afueras hasta la estación ferroviaria.

En 2002 se alcanzó un acuerdo entre el Ayuntamiento de Valladolid, la Junta de Castilla y León y el Ministerio de Fomento para soterrar la totalidad del trazado urbano, entre el puente de Daniel del Olmo y el apeadero de la Universidad; el 6 de noviembre de 2002 se firmó el correspondiente convenio de colaboración entre las administraciones implicadas; y el 10 de enero de 2003 se constituyó una sociedad gestora llamada Valladolid Alta Velocidad 2003, con un 50% de capital de las sociedades del Grupo Fomento y un 25% de cada una de las otras dos administraciones. Como objeto de esta sociedad se definió la promoción de la transformación urbanística derivada de las obras de integración de la red arterial ferroviaria en Valladolid. Para ello, la sociedad cuenta como principal activo con un compromiso de cesión de los terrenos donde aún se asienta el Taller Central de Reparaciones de Renfe y el resto de la superficie que se libere de usos ferroviarios y asociados, para su promoción urbanística y venta. La intención era financiar la totalidad de la operación ferroviaria con los beneficios obtenidos de la urbanística.

El soterramiento del tren en Valladolid supondría una importante modificación de los usos del suelo en toda la franja que actualmente ocupa el tendido férreo. La desaparición del mismo eliminaría la línea divisoria que parte actualmente la ciudad, dejando espacio para nuevos usos públicos y áreas residenciales. Así, no solo despejaría un gran espacio, sino que liberaría también un conjunto de construcciones históricas que constituyen un ejemplo de edificación industrial singular, como el Arco de Ladrillo o el Depósito de Locomotoras. Para realizar las obras tendría que desmontarse el arco necesariamente.

Valladolid participa activamente en la iniciativa de hermanamiento de ciudades promovida, entre otras instituciones, por la Unión Europea. A partir de esta iniciativa se pretenden establecer lazos con las siguientes ciudades con la celebración de ciclos culturales, intercambios o eventos deportivos:





</doc>
<doc id="2906" url="https://es.wikipedia.org/wiki?curid=2906" title="Verticilo">
Verticilo

En botánica, en plantas organizadas en tallos y hojas, de forma que el tallo está compuesto por nudos y entrenudos, y las hojas se originan en los nudos, se llama verticilo a un conjunto de tres o más hojas que brotan de un tallo en el mismo nudo aparente.

Las hojas modificadas que forman la flor (piezas florales) también comúnmente se disponen en verticilos, cuando no es así y hay una sola pieza por nudo, se disponen en espiral.



</doc>
<doc id="2907" url="https://es.wikipedia.org/wiki?curid=2907" title="Velocidad de la luz">
Velocidad de la luz

La velocidad de la luz en el vacío es por definición una constante universal de valor 299 792 458 m/s (aproximadamente 186 282,397 millas/s)(suele aproximarse a 3·10 m/s), o lo que es lo mismo 9,46·10 m/año; la segunda cifra es la usada para definir la unidad de longitud llamada año luz.

Se simboliza con la letra c, proveniente del latín "celéritās" (en español celeridad o rapidez).

El valor de la velocidad de la luz en el vacío fue incluido oficialmente en el Sistema Internacional de Unidades como constante el 21 de octubre de 1983, pasando así el metro a ser una unidad derivada de esta constante.

La rapidez a través de un medio que no sea el "vacío" depende de su permitividad eléctrica, de su permeabilidad magnética, y otras características electromagnéticas. En medios materiales, esta velocidad es inferior a "c" y queda codificada en el índice de refracción. En modificaciones del vacío más sutiles, como espacios curvos, efecto Casimir, poblaciones térmicas o presencia de campos externos, la velocidad de la luz depende de la densidad de energía de ese vacío.

De acuerdo con la física moderna toda radiación electromagnética (incluida la luz visible) se propaga o mueve con una rapidez constante en el vacío, conocida común —aunque impropiamente— como "velocidad de la luz" (magnitud vectorial), en vez de "rapidez de la luz" (magnitud escalar). Esta es una constante física denotada como "c". La rapidez "c" es también la rapidez de la propagación de la gravedad en la teoría general de la relatividad.

Una consecuencia que se obtiene a partir de las leyes del electromagnetismo (tales como las ecuaciones de Maxwell) es que la rapidez "c" de la radiación electromagnética no depende de la rapidez del objeto que emite tal radiación. Así, por ejemplo, la luz emitida por una fuente de luz que se mueve muy rápidamente, viajaría con la misma rapidez que la luz proveniente de una fuente estacionaria (aunque el color, la frecuencia, la energía y el momentum de la luz cambiarán; fenómeno que se conoce como efecto Doppler).

Si se combina esta observación con el principio de relatividad, se concluye que todos los observadores medirán la rapidez de la luz en el vacío como una misma cantidad, sin importar el marco de referencia del observador o la rapidez del objeto que emite la luz. Debido a esto, se puede ver a "c" como una constante física fundamental. Este hecho, entonces, puede ser usado como base en la teoría de la relatividad especial. La constante es la rapidez "c", en vez de la luz en sí misma, lo cual es fundamental para la relatividad especial. De este modo, si la luz es de alguna manera retardada para viajar a una rapidez menor de "c", esto no afectará directamente a la teoría de la relatividad especial.

Observadores que viajan con gran rapidez encontrarán que las distancias y los tiempos se distorsionan de acuerdo con la transformación de Lorentz. Sin embargo, las transformaciones distorsionan tiempos y distancias de manera que la rapidez de la luz permanece constante. Una persona viajando con una rapidez cercana a "c" también encontrará que los colores de la luz al frente se tornan azules y atrás se tornan rojos.

Si la información pudiese viajar más rápido que "c" en un marco de referencia, la causalidad sería violada: en otros marcos de referencia, la información sería recibida antes de ser mandada; así, la causa podría ser observada después del efecto. Debido a la dilatación del tiempo de la relatividad especial, el cociente del tiempo percibido entre un observador externo y el tiempo percibido por un observador que se mueve cada vez más cerca de la rapidez de la luz se aproxima a cero. Si algo pudiera moverse más rápidamente que la luz, este cociente no sería un número real. Tal violación de la causalidad nunca se ha observado.

Un cono de luz define la ubicación que está en contacto causal y aquellas que no lo están. Para exponerlo de otro modo, la información se propaga de y hacia un punto de regiones definidas por un cono de luz. El intervalo AB en el diagrama a la derecha es de "tipo tiempo" (es decir, hay un marco de referencia en el que los acontecimientos A y B ocurren en la misma ubicación en el espacio, separados solamente por su ocurrencia en tiempos diferentes, y si A precede a B en ese marco entonces A precede a B en todos los marcos: no hay marco de referencia en el cual el evento A y el evento B ocurren simultáneamente). De este modo, es hipotéticamente posible para la materia (o la información) viajar de A hacia B, así que puede haber una relación causal (con A la causa y B el efecto).

Por otra parte, el intervalo AC es de "tipo espacio" (es decir, existe un marco de referencia donde el evento A y el evento C ocurren simultáneamente). Sin embargo, también existen marcos en los que A precede a C, o en los que C precede a A. Confinando una manera de viajar más rápido que la luz, no será posible para ninguna materia (o información) viajar de A hacia C o de C hacia A. De este modo no hay conexión causal entre A y C.

En acuerdo con la definición actual, adoptada en 1983, la rapidez de la luz es exactamente 299 792 458 m/s (aproximadamente 3 × 10 metros por segundo, 300 000 km/s o 300 m por millonésima de s).

El valor de "c" define la permitividad eléctrica del vacío (formula_1) en unidades del SIU como:

La permeabilidad magnética del vacío (formula_3) no es dependiente de "c" y es definida en unidades del SIU como:

Estas constantes aparecen en las ecuaciones de Maxwell, que describen el electromagnetismo y están relacionadas por:

Las distancias astronómicas son normalmente medidas en años luz (que es la distancia que recorre la luz en un año, aproximadamente 9.46 × 10 km (9.46 billones de km).

Históricamente, el metro había sido definido como la diezmillonésima parte de la longitud del arco de meridiano terrestre comprendido entre el polo norte y el ecuador a través de París, con referencia a la barra estándar, y con referencia a una longitud de onda de una frecuencia particular de la luz.

En 1967 la XIII Conferencia General de Pesos y Medidas definió el segundo del tiempo atómico como la duración de 9 192 631 770 períodos de radiación correspondiente a la transición entre dos niveles hiperfinos del estado fundamental del átomo cesio-133, que en la actualidad sigue siendo la definición del segundo.

En 1983 la Conferencia General de Pesos y Medidas resolvió modificar la definición del "metro" como unidad de longitud del Sistema Internacional, estableciendo su definición a partir de la velocidad de la luz:

En consecuencia, este reajuste efectuado en la definición del "metro" permite que la velocidad de la luz tenga un valor exacto de 299 792 458 m/s cuando se expresa en metros/segundo. Esta modificación aprovecha de forma práctica una de las bases de la teoría de la relatividad de Einstein, que establece que la magnitud de la velocidad de la luz en el vacío es independiente del sistema de referencia utilizado para medirla.

La motivación en el cambio de la definición del metro, así como todos los cambios en la definición de unidades, fue proveer una definición precisa de la unidad que pudiese ser fácilmente usada para calibrar homogéneamente dispositivos en todo el mundo. La no era práctica en este sentido, ya que no podía ser sacada de su cámara o utilizada por dos científicos al mismo tiempo. También era propensa a cambios significativos en su longitud debido a variaciones de temperatura, desgaste de los extremos, oxidación, etc., incompatible con la exactitud necesaria para establecer una de las unidades básicas del Sistema Internacional de unidades.

La rapidez de la luz es de gran importancia para las telecomunicaciones. Por ejemplo, dado que el perímetro de la Tierra es de 40 075 km (en la línea ecuatorial) y "c" es teóricamente la velocidad más rápida en la que un fragmento de información puede viajar, el período más corto de tiempo para llegar al otro extremo del globo terráqueo sería 0.067 s.

En realidad, el tiempo de viaje es un poco más largo, en parte debido a que la velocidad de la luz es cerca de un 30% menor en una fibra óptica, y raramente existen trayectorias rectas en las comunicaciones globales; además se producen retrasos cuando la señal pasa a través de interruptores eléctricos o generadores de señales. En 2004, el retardo típico de recepción de señales desde Australia o Japón hacia los EE.UU. era de 0.18 s. Adicionalmente, la velocidad de la luz afecta al diseño de las comunicaciones inalámbricas.

La velocidad finita de la luz se hizo aparente a todo el mundo en el control de comunicaciones entre el Control Terrestre de Houston y Neil Armstrong, cuando este se convirtió en el primer hombre que puso un pie sobre la Luna: después de cada pregunta, Houston tenía que esperar cerca de 3 s para el regreso de una respuesta aun cuando los astronautas respondían inmediatamente.

De manera similar, el control remoto instantáneo de una nave interplanetaria es imposible debido a que una nave suficientemente alejada de nuestro planeta podría tardar algunas horas desde que envía información al centro de control terrestre y recibe las instrucciones.

La velocidad de la luz también puede tener influencia en distancias cortas. En los superordenadores la velocidad de la luz impone un límite de rapidez a la que pueden ser enviados los datos entre procesadores. Si un procesador opera a 1 GHz, la señal solo puede viajar a un máximo de 300 mm en un ciclo único. Por lo tanto, los procesadores deben ser colocados cerca uno de otro para minimizar los retrasos de comunicación. Si las frecuencias de un reloj continúan incrementándose, la rapidez de la luz finalmente se convertirá en un factor límite para el diseño interno de chips individuales.

Es importante observar que la velocidad de la luz no es un límite de velocidad en el sentido convencional. Un observador que persigue un rayo de luz lo mediría al moverse paralelamente él mismo viajando a la misma velocidad como si fuese un observador estacionario. Esto se debe a que la velocidad medida por este observador depende no solo de la diferencia de distancias recorridas por él y por el rayo, sino también de su tiempo propio que se ralentiza con la velocidad del observador. La ralentización del tiempo o dilatación temporal para el observador es tal que siempre percibirá a un rayo de luz moviéndose a la misma velocidad.

La mayoría de los individuos están acostumbrados a la regla de la adición de velocidades: si dos coches se acercan desde direcciones opuestas, cada uno viajando a una velocidad de 50 km/h, se esperaría (con un alto grado de precisión) que cada coche percibiría al otro en una velocidad combinada de 50 + 50=100 km/h. Esto sería correcto en todos los casos si pudieramos ignorar que la medida física del tiempo transcurrido es relativa según el estado de movimiento del observador.

Sin embargo, a velocidades cercanas a la de la luz, en resultados experimentales se hace claro que esta regla no se puede aplicar por la dilatación temporal. Dos naves que se aproximen una a otra, cada una viajando al 90% de la velocidad de la luz relativas a un tercer observador entre ellas, no se percibirán mutuamente a un 90% + 90%=180% de la velocidad de la luz. En su lugar, cada una percibirá a la otra aproximándose a menos de un 99.5% de la velocidad de la luz. Este resultado se da por la fórmula de adición de la velocidad de Einstein:

donde "v" y "w" son las velocidades de las naves observadas por un tercer observador, y "u" es la velocidad de cualquiera de las dos naves observada por la otra.

Contrariamente a la intuición natural, sin importar la velocidad a la que un observador se mueva relativamente hacia otro observador, ambos medirán la velocidad de un rayo de luz que se avecina con el mismo valor constante, la velocidad de la luz.

La ecuación anterior fue derivada por Einstein de su teoría de relatividad especial, la cual toma el principio de relatividad como premisa principal. Este principio (originalmente propuesto por Galileo Galilei) requiere que actúen leyes físicas de la misma manera en todos los marcos de referencia.

Ya que las ecuaciones de Maxwell otorgan directamente una velocidad de la luz, debería ser lo mismo para cada observador; una consecuencia que sonaba obviamente equivocada para los físicos del siglo XIX, quienes asumían que la velocidad de la luz dada por la teoría de Maxwell es válida en relación al "éter lumínico".

Pero el experimento de Michelson y Morley, puede que el más famoso y útil experimento en la historia de la física, no pudo encontrar este éter, sugiriendo en su lugar que la velocidad de la luz es una constante en todos los marcos de referencia.

Aunque no se sabe si Einstein conocía los resultados de los experimentos de Michelson y Morley, él dio por hecho que la velocidad de la luz era constante, lo entendió como una reafirmación del principio de relatividad de Galileo, y dedujo las consecuencias, ahora conocidas como la teoría de la relatividad especial, que incluyen la anterior fórmula auto-intuitiva.

Debe tenerse presente, especialmente si se consideran sistemas de referencia no inerciales, que la observación experimental de constancia de la luz se refiere a la velocidad física de la luz. La diferencia entre ambas magnitudes ocasionó ciertos malentendidos a los teóricos de principios de siglo XX. Así Pauli llegó a escribir:

Sin embargo, ese comentario es cierto predicado de la velocidad coordenada de la luz (cuya definición no involucra los coeficientes métricos del tensor métrico), sin embargo, una definición adecuada de velocidad física de la luz involucrando las componentes del tensor métrico de sistemas de referencia no inerciales lleva a que la velocidad física sí sea constante.

El índice de refracción de un material indica cuán lenta es la velocidad de la luz en ese medio comparada con el vacío. La disminución de la velocidad de la luz en los materiales puede causar el fenómeno denominado "refracción", como se puede observar en un prisma atravesado por un rayo de luz blanca formando un espectro de colores y produciendo su dispersión.

Al pasar a través de los materiales, la luz se propaga a una velocidad menor que "c", expresada por el cociente denominado «índice de refracción» del material.

La rapidez de la luz en el aire es solo levemente menor que "c". Medios más densos, como el agua y el vidrio, pueden disminuir más la rapidez de la luz, a fracciones como 3/4 y 2/3 de "c". Esta disminución de velocidad también es responsable de "doblar" la luz (modificando su trayectoria según un quiebro con un ángulo dado) en una interfase entre dos materiales con índices diferentes, un fenómeno conocido como refracción. Esto se debe a que dentro de los medios transparentes, la luz en tanto que onda electromagnética interacciona con la materia, que a su vez produce campos de respuesta, y la luz a través del medio es el resultado de la onda inicial y la respuesta de la materia. Esta onda electromagnética que se propaga en el material tiene una velocidad de propagación menor que la luz en el vacío. El índice de refracción "n" de un medio viene dado por la siguiente expresión, donde "v" es la velocidad de la luz en ese medio (debido a que, como ya se ha señalado, la velocidad de la luz en un medio es menor que la velocidad de la luz en el vacío):

Ya que la velocidad de la luz en los materiales depende del índice de refracción, y el índice de refracción depende de la frecuencia de la luz, la luz a diferentes frecuencias viaja a diferentes velocidades a través del mismo material. Esto puede causar distorsión en ondas electromagnéticas compuestas por múltiples frecuencias; un fenómeno llamado dispersión.

Los ángulos de incidencia (i) y de refracción (r) entre dos medios, y los índices de refracción, están relacionados por la Ley de Snell. Los ángulos se miden con respecto al vector normal a la superficie entre los medios:

A escala microscópica, considerando la radiación electromagnética como una partícula, la refracción es causada por una absorción continua y re-emisión de los fotones que componen la luz a través de los átomos o moléculas por los que está atravesando. En cierto sentido, la luz por sí misma viaja solo a través del vacío existente entre estos átomos, y es obstaculizada por los átomos. Alternativamente, considerando la radiación electromagnética como una onda, las cargas de cada átomo (primariamente electrones) interfieren con los campos eléctricos y electromagnéticos de la radiación, retardando su progreso.

Una evidencia experimental reciente demuestra que es posible para la velocidad de grupo de la luz exceder "c". Un experimento hizo que la velocidad de grupo de rayos láser viajara distancias extremadamente cortas a través de átomos de cesio a 300 veces "c". Sin embargo, no es posible usar esta técnica para transferir información más rápido que "c": la rapidez de la transferencia de información depende de la velocidad frontal (la rapidez en la cual el primer incremento de un pulso sobre cero la mueve adelante) y el producto de la velocidad agrupada y la velocidad frontal es igual al cuadrado de la velocidad normal de la luz en el material.

El exceder la velocidad de grupo de la luz de esta manera, es comparable a exceder la velocidad del sonido emplazando personas en una línea espaciada equidistantemente, y pidiéndoles a todos que griten una palabra uno tras otro con intervalos cortos, cada uno midiendo el tiempo al mirar su propio reloj para que no tengan que esperar a escuchar el grito de la persona previa.

La rapidez de la luz también puede parecer superada en cierto fenómeno que incluye ondas evanescentes, tales como túneles cuánticos. Los experimentos indican que la velocidad de fase de ondas evanescentes pueden exceder a "c"; sin embargo, parecería que ni la velocidad agrupada ni la velocidad frontal exceden "c", así, de nuevo, no es posible que la información sea transmitida más rápido que "c".

En algunas interpretaciones de la mecánica cuántica, los efectos cuánticos pueden ser retransmitidos a velocidades mayores que "c" (de hecho, la acción a distancia se ha percibido largamente como un problema con la mecánica cuántica: ver paradoja EPR). Por ejemplo, los estados cuánticos de dos partículas pueden estar enlazados, de manera que el estado de una partícula condicione el estado de otra partícula (expresándolo de otra manera, uno debe tener un espín de +½ y el otro de -½). Hasta que las partículas son observadas, estas existen en una superposición de dos estados cuánticos (+½, –½) y (–½, +½). Si las partículas son separadas y una de ellas es observada para determinar su estado cuántico, entonces el estado cuántico de la segunda partícula se determina automáticamente. Si, en algunas interpretaciones de mecánica cuántica, se presume que la información acerca del estado cuántico es local para una partícula, entonces se debe concluir que la segunda partícula toma su estado cuántico instantáneamente, tan pronto como la primera observación se lleva a cabo. Sin embargo, es imposible controlar qué estado cuántico tomará la primera partícula cuando sea observada, así que ninguna información puede ser transmitida de esta manera. Las leyes de la Física también parecen prevenir que la información sea transmitida a través de maneras más astutas, y esto ha llevado a la formulación de reglas tales como el teorema de no clonación.

El llamado movimiento superluminar también es visto en ciertos objetos astronómicos, tales como los jet de Galaxia activa, galaxias activas y cuásares. Sin embargo, estos jets no se mueven realmente a velocidades excedentes a la de la luz: el movimiento aparente superluminar es una proyección del efecto causado por objetos moviéndose cerca de la velocidad de la luz en un ángulo pequeño del horizonte de visión.

Aunque puede sonar paradójico, es posible que las ondas expansivas se hayan formado con la radiación electromagnética, ya que una partícula cargada que viaja a través de un medio insolado, interrumpe el campo electromagnético local en el medio. Los electrones en los átomos del medio son desplazados y polarizados por el campo de la partícula cargada, y los fotones que son emitidos como electrones se restauran a sí mismos para mantener el equilibrio después de que la interrupción ha pasado (en un conductor, la interrupción puede ser restaurada sin emitir un fotón).

En circunstancias normales, estos fotones interfieren destructivamente unos con otros y no se detecta radiación. Sin embargo, si la interrupción viaja más rápida que los mismos fotones, los fotones interferirán constructivamente e intensificarán la radiación observada. El resultado (análogo a una explosión sónica) es conocido como radiación Cherenkov.

La habilidad de comunicarse o viajar más rápido que la luz es un tema popular en la ciencia ficción. Se han propuesto partículas que viajan más rápido que la luz, taquiones, doblados por la física de partículas, aunque nunca se han observado.

Algunos físicos (entre ellos João Magueijo y John Moffat) han propuesto que en el pasado la luz viajaba mucho más rápido que a la velocidad actual. Esta teoría se conoce como velocidad de la luz variable, y sus proponentes afirman que este fenómeno tiene la habilidad de explicar mejor muchos enigmas cosmológicos que su teoría rival, el modelo inflacionario del universo. Sin embargo, esta teoría no ha ganado suficiente aceptación.

En septiembre de 2011, en las instalaciones del CERN en Ginebra, del laboratorio subterráneo de Gran Sasso (Italia), se observaron unos neutrinos que aparentemente superaban la velocidad de la luz, llegando (60.7 ± 6.9 (stat.) ± 7.4 (sys.)) nanosegundos antes (que corresponde a unos 18 metros en una distancia total de 732 kilómetros). Desde el primer momento, la comunidad científica se mostró escéptica ante la noticia, ya que varios años antes, el proyecto Milos de la Fermilab de Chicago había obtenido resultados parecidos que fueron descartados porque el margen de error era demasiado alto. Y, efectivamente, en este caso también resultó ser un error de medición. En febrero de 2012, los científicos del CERN anunciaron que las mediciones habían sido erróneas debido a una conexión defectuosa.

Fenómenos refractivos tales como el arco iris tienden a retardar la velocidad de la luz en un medio (como el agua, por ejemplo). En cierto sentido, cualquier luz que viaja a través de un medio diferente del vacío viaja a una velocidad menor que "c" como resultado de la refracción. Sin embargo, ciertos materiales tienen un índice de refracción excepcionalmente alto: en particular, la densidad óptica del condensado de Bose-Einstein puede ser muy alta.

En 1999, un equipo de científicos encabezados por Lene Hau pudo disminuir la velocidad de un rayo de luz a cerca de 17 m/s, y en 2001 pudieron detener momentáneamente un rayo de luz.

En 2003, Mijaíl Lukin, junto con científicos de la Universidad Harvard y el Instituto de Física Lébedev (de Moscú), tuvieron éxito en detener completamente la luz al dirigirla a una masa de gas rubidio caliente, cuyos átomos, en palabras de Lukin, se comportaron como «pequeños espejos» debido a los patrones de interferencia en dos rayos de control.

Hasta tiempos relativamente recientes, la velocidad de la luz fue un tema sujeto a grandes conjeturas. Empédocles creía que la luz era algo en movimiento, y que por lo tanto en su viaje tenía que transcurrir algún tiempo.

Por el contrario, Aristóteles creía que «la luz está sujeta a la presencia de algo, pero no es el movimiento». Además, si la luz tiene una velocidad finita, esta tenía que ser inmensa. Aristóteles afirmó: «La tensión sobre nuestro poder de creencias es demasiado grande para creer esto».

Una de las teorías antiguas de la visión es que la luz es emitida por el ojo, en lugar de ser generada por una fuente y reflejada en el ojo. En esta teoría, Herón de Alejandría adelantó el argumento de que la velocidad de la luz debería ser infinita, ya que cuando uno abre los ojos objetos distantes como las estrellas aparecen inmediatamente.

Los filósofos islámicos Avicena y Alhacén creían que la luz tenía una velocidad finita, aunque en este punto otros filósofos convinieron con Aristóteles.

La escuela Ayran de filosofía en la antigua India también mantuvo que la velocidad de la luz era finita.

Johannes Kepler creía que la velocidad de la luz era finita ya que el espacio vacío no representa un obstáculo para ella. Francis Bacon argumentó que la velocidad de la luz no es necesariamente finita, ya que algo puede viajar tan rápido como para ser percibido.

René Descartes argumentó que si la velocidad de la luz era finita, el Sol, la Tierra y la Luna estarían perceptiblemente fuera de alineación durante un eclipse lunar. Debido a que tal desalineación no se ha observado, Descartes concluyó que la velocidad de la luz es infinita. De hecho, Descartes estaba convencido de que si la velocidad de la luz era finita, todo su sistema de filosofía sería refutado. 

La historia de la medición de la velocidad de la luz comienza en el siglo XVII en los albores de la revolución científica. Un estudio histórico relativo a las mediciones de la velocidad de la luz señala una docena de métodos diferentes para determinar el valor de "c". La mayor parte de los primeros experimentos para intentar medir la velocidad de la luz fracasaron debido a su alto valor, y tan solo se pudieron obtener medidas indirectas a partir de fenómenos astronómicos. En el siglo XIX se pudieron realizar los primeros experimentos directos de medición de la velocidad de la luz confirmando su naturaleza electromagnética y las ecuaciones de Maxwell.

En 1629 Isaac Beeckman, un amigo de René Descartes, propuso un experimento en el que se pudiese observar el fogonazo de un cañón reflejándose en un espejo ubicado a una milla (1.6 km) del primero. En 1638, Galileo propuso un experimento para medir la velocidad de la luz al observar la percepción del retraso entre el lapso de destapar una linterna a lo lejos. René Descartes criticó este experimento como algo superfluo, dado el hecho de que la observación de eclipses, los cuales tenían más poder para detectar una rapidez finita, dio un resultado negativo. En 1667, este experimento se llevó a cabo por la Accademia del Cimento de Florencia, con las linternas separadas una milla entre sí, sin observarse ningún retraso. Robert Hooke explicó los resultados negativos tal como Galileo había hecho: precisando que tales observaciones no establecerían la velocidad infinita de la luz, sino tan solo que dicha velocidad debía ser muy grande.

En 1676 Ole Rømer realizó la primera estimación cuantitativa de la velocidad de la luz estudiando el movimiento del satélite Ío de Júpiter con un telescopio. Es posible medir el tiempo de la revolución de Ío debido a sus movimientos de entrada y salida en la sombra arrojada por Júpiter en intervalos regulares. Rømer observó que Ío gira alrededor de Júpiter cada 42.5 h cuando la Tierra esta más cerca de Júpiter. También observó que, cuando la Tierra y Júpiter se mueven separándose, la salida de Ío fuera de la proyección de la sombra comenzaba progresivamente más tarde de lo predicho. Las observaciones detalladas mostraban que estas señales de salida necesitaban más tiempo en llegar a la Tierra, ya que la Tierra y Júpiter se separaban cada vez más. De este modo el tiempo extra utilizado por la luz para llegar a la Tierra podía utilizarse para deducir la rapidez de esta. Seis meses después, las entradas de Ío en la proyección de la sombra se adelantaban, ya que la Tierra y Júpiter se acercaban uno a otro. Con base a estas observaciones, Rømer estimó que la luz tardaría 22 min en cruzar el diámetro de la órbita de la Tierra (es decir, el doble de la unidad astronómica); las estimaciones modernas se acercan más a la cifra de 16 min y 40 s.

Alrededor de la misma época, la unidad astronómica (radio de la órbita de la Tierra alrededor del Sol) se estimaba en cerca de 140 millones de km. Este dato y la estimación del tiempo de Rømer fueron combinados por Christian Huygens, quien consideró que la velocidad de la luz era cercana a 1000 diámetros de la Tierra por minuto, es decir, unos 220 000 km/s, muy por debajo del valor actualmente aceptado, pero mucho más rápido que cualquier otro fenómeno físico entonces conocido.

Isaac Newton también aceptó el concepto de velocidad finita. En su libro "Opticks" expone el valor más preciso de 16 minutos para que la luz recorra el diámetro de la órbita terrestre, valor que al parecer dedujo por sí mismo (se desconoce si fue a partir de los datos de Rømer o de alguna otra manera).

El mismo efecto fue subsecuentemente observado por Rømer en un punto en rotación con la superficie de Júpiter. Observaciones posteriores también mostraron el mismo efecto con las otras tres lunas Galileanas, en las que era más difícil de observar al estar estos satélites más alejados de Júpiter y proyectar sombras menores sobre el planeta.

Aunque por medio de estas observaciones la velocidad finita de la luz no fue establecida para la satisfacción de todos (notablemente Jean-Dominique Cassini), después de las observaciones de James Bradley (1728), la hipótesis de velocidad infinita se consideró totalmente desacreditada. Bradley dedujo que la luz de las estrellas que llega sobre la Tierra parecería provenir en un ángulo leve, que podría ser calculado al comparar la velocidad de la Tierra en su órbita con la velocidad de la luz. Se observó esta llamada aberración de la luz, estimándose en 1/200 de un grado.

Bradley calculó la velocidad de la luz en alrededor de 298 000 km/s. Esta aproximación es solamente un poco menor que el valor actualmente aceptado. El efecto de aberración fue estudiado extensivamente en los siglos posteriores, notablemente por Friedrich Georg Wilhelm Struve y Magnus Nyren.

La segunda medida acertada de la velocidad de la luz, primera mediante un aparato terrestre, fue realizada por Hippolyte Fizeau en 1849. El experimento de Fizeau era conceptualmente similar a aquellos propuestos por Beeckman y Galileo. Un rayo de luz se dirigía a un espejo a cientos de metros de distancia. En su trayecto desde la fuente hacia el espejo, el rayo pasaba a través de un engranaje rotatorio. A cierto nivel de rotación, el rayo pasaría a través de un orificio en su camino de salida y en otro en su camino de regreso. Pero en niveles ligeramente menores, el rayo se proyectaría en uno de los dientes y no pasaría a través de la rueda. Conociendo la distancia hasta el espejo, el número de dientes del engranaje y el índice de rotación, se podría calcular la velocidad de la luz. Fizeau reportó la velocidad de la luz como 313 000 km/s. El método de Fizeau fue refinado más tarde por Marie Alfred Cornu (1872) y Joseph Perrotin (1900), pero fue el físico francés Léon Foucault quien más profundizó en la mejora del método de Fizeau al reemplazar el engranaje por un espejo rotatorio. El valor estimado por Foucault, publicado en 1862, fue de 298 000 km/s. El método de Foucault también fue usado por Simon Newcomb y Albert Michelson, quien comenzó su larga carrera replicando y mejorando este método.

En 1926, Michelson utilizó espejos rotatorios para medir el tiempo que tardaba la luz en hacer un viaje de ida y vuelta entre la montaña Wilson y la montaña San Antonio en California. De las mediciones cada vez más exactas, resultó una velocidad de 299 796 km/s.

Otra forma de obtener la velocidad de la luz es medir independientemente la frecuencia formula_8 y la longitud de onda formula_9 de una onda electromagnética en el vacío. El valor de c puede entonces ser calculado mediante el uso de la relación formula_10. Una opción es medir la frecuencia de resonancia en una "cavidad de resonancia". Si se conocen con precisión sus dimensiones, estas pueden ser utilizadas para determinar la longitud de onda de un haz de luz. En 1946, Louis Essen y AC Gordon-Smith utilizaron este método (las dimensiones de la cavidad de resonancia se establecieron con una precisión de alrededor de ± 0,8 micras utilizando medidores calibrados por interferometría), obteniendo un resultado de 299 792 ±9 kilómetros/s, sustancialmente más preciso que los valores calculados usando técnicas ópticas. En 1950, las mediciones repetidas establecieron un resultado de 299 792,5 ±3,0 kilómetros/s.
La interferometría es otro método para encontrar la longitud de onda de la radiación electromagnética para determinar la velocidad de la luz. Un haz de luz coherente (por ejemplo, un láser), con una frecuencia conocida formula_8, se divide siguiendo dos recorridos distintos y luego se recombina. Mediante el ajuste de la longitud del camino recorrido mientras se observa el patrón de interferencia, midiendo cuidadosamente el cambio en la longitud de la trayectoria, se puede determinar la longitud de onda de la luz formula_9.

La velocidad de la luz se calcula como en el caso anterior, utilizando la ecuación formula_10.

Antes de la llegada de la tecnología láser, se utilizaron fuentes coherentes de radio para las mediciones de interferometría de la velocidad de la luz. Sin embargo el método interferométrico se vuelve menos preciso con longitudes de onda reducidas, y los experimentos fueron por tanto limitados a la precisión de la longitud de onda larga (~ 0,4 cm ) de las ondas de radio. La precisión puede ser mejorada mediante el uso de luz con una longitud de onda más corta, pero a continuación, se hace difícil medir directamente su frecuencia. Una forma de evitar este problema es comenzar con una señal de baja frecuencia (cuyo valor se puede medir con precisión), y a partir de esta señal sintetizar progresivamente señales de frecuencias superiores, cuya frecuencia puede entonces relacionarse con la señal original. La frecuencia de un láser se puede fijar con notable precisión, y su longitud de onda se puede determinar entonces utilizando interferometría. Esta técnica la desarrolló un grupo del National Bureau of Standards (NBS) (que más tarde se convirtió en el NIST). Se utilizó en 1972 para medir la velocidad de la luz en el vacío con una incertidumbre fraccionaria de 3,5 × 10.

Con base en el trabajo de James Clerk Maxwell, se sabe que la velocidad de la radiación electromagnética es una constante definida por las propiedades electromagnéticas del vacío (constante dieléctrica y permeabilidad).

En 1887, los físicos Albert Michelson y Edward Morley realizaron el influyente experimento Michelson-Morley para medir la velocidad de la luz relativa al movimiento de la Tierra. La meta era medir la velocidad de la Tierra a través del éter, el medio que se pensaba en ese entonces necesario para la transmisión de la luz. Tal como se muestra en el diagrama del interferómetro de Michelson, se utilizó un espejo con media cara plateada para dividir un rayo de luz monocromática en dos rayos que viajaban en ángulos rectos uno respecto del otro. Después de abandonar la división, cada rayo era reflejado de ida y vuelta entre los espejos en varias ocasiones (el mismo número para cada rayo para dar una longitud de trayectoria larga pero igual; el experimento Michelson-Morley actual usa más espejos) entonces una vez recombinados producen un patrón de interferencia constructiva y destructiva.

Cualquier cambio menor en la velocidad de la luz en cada brazo del interferómetro cambiaría la cantidad de tiempo utilizada en su tránsito, que sería observado como un cambio en el patrón de interferencia. Durante los ensayos realizados, el experimento dio un resultado nulo.

Ernst Mach estuvo entre los primeros físicos que sugirieron que el resultado del experimento era una refutación a la teoría del éter. El desarrollo en física teórica había comenzado a proveer una teoría alternativa, la contracción de Lorentz, que explicaba el resultado nulo del experimento.

Es incierto si Einstein conocía los resultados de los experimentos de Michelson y Morley, pero su resultado nulo contribuyó en gran medida a la aceptación de su teoría de relatividad. La teoría de Einstein no requirió un elemento etérico sino que era completamente consistente con el resultado nulo del experimento: el éter no existe y la velocidad de la luz es la misma en cada dirección. La velocidad constante de la luz es uno de los postulados fundamentales (junto con el principio de causalidad y la equivalencia de los marcos de inercia) de la relatividad especial.






</doc>
<doc id="2908" url="https://es.wikipedia.org/wiki?curid=2908" title="Vector (desambiguación)">
Vector (desambiguación)

El término vector puede aludir, en esta enciclopedia, a alguno de los siguientes artículos:






</doc>
<doc id="2909" url="https://es.wikipedia.org/wiki?curid=2909" title="Venus (planeta)">
Venus (planeta)

Venus es el segundo planeta del sistema solar en orden de distancia desde el Sol, el sexto en cuanto a tamaño, ordenados de mayor a menor. Al igual que Mercurio, carece de satélites naturales. Recibe su nombre en honor a Venus, la diosa romana del amor (gr. "Afrodita"). Se trata de un planeta de tipo rocoso y terrestre, llamado con frecuencia el planeta hermano de la Tierra, ya que ambos son similares en cuanto a tamaño, masa y composición, aunque totalmente diferentes en cuestiones térmicas y atmosféricas (temperatura media de 463,85ºC). La órbita de Venus es una elipse con una excentricidad de menos del 1%, formando la órbita más circular de las de todos los planetas; apenas supera la de Neptuno. Su presión atmosférica es 90 veces superior a la terrestre; es, por tanto, la mayor presión atmosférica de las de todos los planetas rocosos del sistema solar.

Pese a situarse más lejos del Sol que Mercurio, Venus posee la atmósfera más caliente del sistema solar; esto se debe a que está principalmente compuesta por gases de efecto invernadero, como el dióxido de carbono, atrapando mucho más calor del Sol. Actualmente carece de agua líquida y sus condiciones en superficie se consideran incompatibles con la vida conocida. No obstante, el Instituto Goddard de Estudios Espaciales de la NASA y otros han postulado que en el pasado Venus pudo tener océanos con tanta agua como el terrestre y reunir condiciones de habitabilidad planetaria.

Este planeta además posee el día más largo del sistema solar: 243 días terrestres, su movimiento es dextrógiro, es decir, gira en el sentido de las manecillas del reloj, contrario al movimiento de los otros planetas. Por ello, en un "día" venusiano el Sol "sale" por el oeste y se oculta por el este. Sus nubes, sin embargo, pueden dar la vuelta al planeta en cuatro días. De hecho previamente a estudiarlo con nave no tripuladas en su superficie o con radares se pensaba que el período de rotación de Venus era de unos cuatro días.

Al encontrarse Venus más cercano al Sol que la Tierra (es un planeta interior), siempre se puede encontrar en las inmediaciones del Sol (su mayor elongación es de 47,8°), por lo que desde la Tierra se puede ver sólo durante unas pocas horas antes del orto (salida del Sol) en unos determinados meses del año, también durante unas pocas horas después del ocaso (puesta del Sol) en el resto del año. A pesar de ello, cuando Venus es más brillante puede ser visto durante el día, siendo uno de los tres únicos cuerpos celestes que pueden ser vistos de día a simple vista además de la Luna y el Sol. Conocido como la estrella de la mañana ("Lucero del alba") o de la tarde ("Lucero vespertino"), cuando es visible en el cielo nocturno es el segundo objeto más brillante del firmamento tras la Luna, por lo que Venus debió ser ya conocido desde los tiempos prehistóricos.

La mayoría de antiguas civilizaciones conocían los movimientos en el cielo de Venus, con lo que adquirió importancia en casi todas las interpretaciones astrológicas del movimiento planetario. En particular, la civilización maya elaboró un calendario religioso basado en los ciclos astronómicos, incluyendo los ciclos de Venus. El símbolo del planeta Venus es una representación estilizada del espejo de la diosa Venus: un círculo con una pequeña cruz debajo, utilizado también hoy para denotar el sexo femenino.

Los adjetivos "venusiano/a", "venusino/a" y "venéreo/a" (poéticamente) son usados para denotar las características habitualmente atribuidas a Venus-Afrodita. El adjetivo "venéreo" suele asociarse a las enfermedades de transmisión sexual. Venus y la Tierra (diosa griega Gea) son los únicos planetas del sistema solar con nombre femenino.

Aunque todas las órbitas planetarias son elípticas, la órbita de Venus es la más parecida a una circunferencia, con una excentricidad inferior a un 1%.

El ciclo entre dos elongaciones máximas (período orbital sinódico) dura 584 días. Después de esos 584 días Venus aparece en una posición a 72° de la elongación anterior. Dado que hay cinco períodos de 72° en una circunferencia, Venus regresa al mismo punto del cielo cada ocho años (menos dos días correspondientes a los años bisiestos). Este periodo se conocía como el ciclo Sothis en el Antiguo Egipto.

En la conjunción inferior, Venus puede aproximarse a la Tierra más que ningún otro planeta. El 16 de diciembre de 1850 alcanzó la distancia más cercana a la Tierra desde el año 1800, con un valor de kilómetros (0,26413854UA). Desde entonces nunca ha habido una aproximación tan cercana. Una aproximación casi tan cercana será en el año 2101, cuando Venus alcanzará una distancia de kilómetros (0,26431736UA).

Venus gira sobre sí mismo muy lentamente en un movimiento retrógrado, en el mismo sentido de las manecillas del reloj si se toma como referencia el polo norte, de este a oeste en lugar de oeste a este como el resto de los planetas (excepto Urano, que está muy inclinado), tardando en hacer un giro completo sobre sí mismo 243,0187 días terrestres. No se sabe el porqué de la peculiar rotación de Venus. Si el Sol pudiese verse desde la superficie de Venus aparecería subiendo desde el oeste y posándose por el este, con un ciclo día-noche de 116,75 días terrestres y un año venusiano de menos de un día (0,92 días venusianos).

Además de la rotación retrógrada, los periodos orbital y de rotación de Venus están sincronizados de manera que siempre presenta la misma cara del planeta a la Tierra cuando ambos cuerpos están a menor distancia. Esto podría ser una simple coincidencia pero existen especulaciones sobre un posible origen de esta sincronización como resultado de efectos de marea afectando a la rotación de Venus cuando ambos cuerpos están lo suficientemente cerca.

Venus tiene una densa atmósfera, compuesta en su mayor parte por dióxido de carbono y una pequeña cantidad de nitrógeno. La presión al nivel de la superficie es 90 veces superior a la presión atmosférica en la superficie terrestre (una presión equivalente en la Tierra a la presión que hay sumergido en el agua a una profundidad de un kilómetro). La enorme cantidad de dióxido de carbono de la atmósfera provoca un fuerte efecto invernadero que eleva la temperatura de la superficie del planeta hasta cerca de 464 °C en las regiones menos elevadas cerca del ecuador. Esto hace que Venus sea más caliente que Mercurio, a pesar de hallarse a más del doble de la distancia del Sol que este y de recibir solo el 25 % de su radiación solar ( en la atmósfera superior y en la superficie). Debido a la inercia térmica de su masiva atmósfera y al transporte de calor por los fuertes vientos de su atmósfera, la temperatura no varía de forma significativa entre el día y la noche. A pesar de la lenta rotación de Venus (menos de dos rotaciones por año venusiano, equivalente a una velocidad de rotación en el Ecuador de solo 6,5 km/h), los vientos de la atmósfera superior circunvalan el planeta en un intervalo de solo 4 días, distribuyendo eficazmente el calor. Además del movimiento zonal de la atmósfera de oeste a este, hay un movimiento vertical en forma de célula de Hadley que transporta el calor del ecuador hasta las zonas polares e incluso a latitudes medias del lado no iluminado del planeta.

La radiación solar casi no alcanza la superficie del planeta. La densa capa de nubes refleja al espacio la mayoría de la luz del Sol y la mayor parte de la luz que atraviesa las nubes es absorbida por la atmósfera. Esto impide a la mayor parte de la luz del Sol que caliente la superficie. El albedo bolométrico de Venus es de aproximadamente el 60%, y su albedo visual es aún mayor, lo cual concluye que, a pesar de encontrarse más cercano al Sol que la Tierra, la superficie de Venus no se calienta ni se ilumina como era de esperar por la radiación solar que recibe. En ausencia del efecto invernadero, la temperatura en la superficie de Venus podría ser similar a la de la Tierra. El enorme efecto invernadero asociado a la inmensa cantidad de dióxido de carbono en la atmósfera atrapa el calor provocando las elevadas temperaturas de este planeta.

Los fuertes vientos en la parte superior de las nubes pueden alcanzar los 350 km/h, aunque a nivel del suelo los vientos son mucho más lentos. A pesar de ello, y debido a la altísima densidad de la atmósfera en la superficie de Venus, incluso estos flojos vientos ejercen una fuerza considerable contra los obstáculos. Las nubes están compuestas principalmente por gotas de dióxido de azufre y ácido sulfúrico, y cubren el planeta por completo, ocultando la mayor parte de los detalles de la superficie a la observación externa. La temperatura en la parte superior de las nubes (a 70 km sobre la superficie) es de –45 °C. La medida promedio de temperatura en la superficie de Venus es de 464 °C. La temperatura de la superficie nunca baja de los 400 °C, lo que lo hace el planeta más caliente del sistema solar.

Venus tiene una lenta rotación retrógrada, lo que significa que gira de este a oeste, en lugar de hacerlo de oeste a este como lo hacen la mayoría de los demás planetas mayores (Urano también tiene una rotación retrógrada, aunque el eje de rotación de Urano, inclinado 97.86°, prácticamente descansa sobre el plano orbital). Se desconoce por qué Venus es diferente en este aspecto, aunque podría ser el resultado de una colisión con un asteroide en algún momento del pasado remoto. Además de esta inusual rotación retrógrada, el período de rotación de Venus y su órbita están casi sincronizados, de manera que siempre presenta la misma cara a la Tierra cuando los dos planetas se encuentran en su máxima aproximación (5001 días venusianos entre cada conjunción inferior). Esto podría ser el resultado de las fuerzas de marea que afectan a la rotación de Venus cada vez que los planetas se encuentran lo suficientemente cercanos, aunque no se conoce con claridad el mecanismo.

Venus tiene dos mesetas principales a modo de continentes, elevándose sobre una vasta llanura. La meseta norte se llama Ishtar Terra y contiene la mayor montaña de Venus (aproximadamente dos kilómetros más alta que el monte Everest), llamada Maxwell Montes en honor de James Clerk Maxwell. Ishtar Terra tiene el tamaño aproximado de Australia. En el hemisferio sur se encuentra Aphrodite Terra, mayor que la anterior y con un tamaño equivalente al de Sudamérica. Entre estas mesetas existen algunas depresiones del terreno, que incluyen Atalanta Planitia, Guinevere Planitia y Lavinia Planitia. Con la única excepción del monte Maxwell, todas las características distinguibles del terreno adoptan nombres de mujeres mitológicas.

La densa atmósfera de Venus provoca que los meteoritos se desintegren bruscamente en su descenso a la superficie, aunque los más grandes pueden llegar a la superficie, originando un cráter si tienen suficiente energía cinética. A causa de esto, no pueden formarse cráteres de impacto más pequeños de 3,2 kilómetros de diámetro.

Aproximadamente el 90 % de la superficie de Venus parece consistir en un basalto recientemente solidificado (en términos geológicos) con muy pocos cráteres de meteoritos. Las formaciones más antiguas presentes en Venus no parecen tener más de 800 millones de años, siendo la mayor parte del suelo considerablemente más joven (no más de algunos cientos de millones de años en su mayor parte), lo cual sugiere que Venus sufrió un cataclismo que afectó a su superficie no hace mucho tiempo en el pasado geológico.

El interior de Venus es probablemente similar al de la Tierra: un núcleo de hierro de unos 3000 km de radio, con un manto rocoso que forma la mayor parte del planeta. Según datos de los medidores gravitatorios de la sonda Magallanes, la corteza de Venus podría ser más dura y gruesa de lo que se había pensado. Se piensa que Venus no tiene placas tectónicas móviles como la Tierra, pero en su lugar se producen masivas erupciones volcánicas que inundan su superficie con lava «fresca». Otros descubrimientos recientes sugieren que Venus todavía está volcánicamente activo.

El campo magnético de Venus es muy débil comparado con el de otros planetas del sistema solar. Esto se puede deber a su lenta rotación, insuficiente para formar el sistema de «dinamo interno» de hierro líquido. Como resultado de esto, el viento solar golpea la atmósfera de Venus sin ser filtrado. Se supone que Venus tuvo originalmente tanta agua como la Tierra pero que, al estar sometida a la acción del Sol sin ningún filtro protector, el vapor de agua en la alta atmósfera se disocia en hidrógeno y oxígeno, escapando el hidrógeno al espacio por su baja masa molecular. El porcentaje de deuterio (un isótopo pesado del hidrógeno que no escapa tan fácilmente) en la atmósfera de Venus parece apoyar esta teoría. Se supone que el oxígeno molecular se combinó con los átomos de la corteza (aunque grandes cantidades de oxígeno permanecen en la atmósfera en forma de dióxido de carbono). A causa de esta sequedad, las rocas de Venus son mucho más pesadas que las de la Tierra, lo cual favorece la formación de montañas mayores, profundos acantilados y otras formaciones.

Durante algún tiempo se creyó que Venus poseía un satélite natural llamado Neith, llamado así por la diosa Sais del Antiguo Egipto, cuyo velo ningún mortal podía levantar. Fue aparentemente observado por primera vez por Giovanni Cassini en 1672. Otras observaciones esporádicas continuaron hasta 1892, pero estos avistamientos fueron desacreditados (eran en su mayor parte estrellas tenues que parecían estar en el lugar correcto en el momento correcto), y hoy se sabe que Venus no tiene ningún satélite, si bien el asteroide 2002 VE casi lo es.

El Instituto Goddard de Estudios Espaciales de la NASA y otros han postulado que Venus pudo tener un océano poco profundo, con tanta agua como el terrestre, que contribuyera a mantener condiciones de habitabilidad durante un máximo de 2 000 millones de años. Dependiendo de los parámetros suministrados a sus modelos teóricos, el agua líquida venusiana pudo terminar de evaporarse hace 715 millones de años. Hoy en día, toda el agua conocida en Venus está en forma de una pequeña cantidad de vapor atmosférico (20ppm.) No obstante, la sonda Venus Express de la Agencia Espacial Europea detectó en 2008 que Venus todavía está perdiendo cantidades mensurables de hidrógeno, uno de los dos elementos constituyentes del agua.

Sin información sísmica o detalles, momento de inercia, existen pocos datos directos sobre la geoquímica y la estructura interna de Venus. Sin embargo, la similitud en tamaño y densidad entre Venus y la Tierra sugiere que ambos comparten una estructura interna afín: un núcleo, un manto, y una corteza planetaria. Al igual que la Tierra, se especula que el núcleo de Venus es al menos parcialmente líquido. El menor tamaño y densidad de Venus indica que las presiones en su interior son considerablemente menores que en la Tierra. La diferencia principal entre los dos planetas es la carencia de placas tectónicas en Venus, probablemente debido a la sequedad del manto y la superficie. Como consecuencia, la pérdida de calor en el planeta es escasa, evitando su enfriamiento y proporcionando una explicación viable sobre la carencia de un campo magnético interno.

Venus es el astro más característico en los cielos de la mañana y de la tarde de la Tierra (después del Sol y la Luna), y es conocido por el hombre desde la prehistoria. Uno de los documentos más antiguos que sobreviven de la biblioteca babilónica de Ashurbanipal, datado sobre el 1600 a. C., es un registro de 21 años del aspecto de Venus (que los primeros babilonios llamaron "Nindaranna"). Los antiguos sumerios y babilonios llamaron a Venus "«Dil-bat»" o "«Dil-i-pat»"; en la ciudad mesopotámica de Akkad era la estrella de la madre-diosa Ishtar, y en chino su nombre es "«Jīn-xīng»" (金星), el planeta del elemento metal. Venus se consideró como el más importante de los cuerpos celestes observados por los mayas, que lo llamaron «Chak ek» (la gran estrella). Los antiguos griegos pensaban que las apariciones matutinas y vespertinas de Venus eran de dos cuerpos diferentes, y les llamaron "Hesperus" cuando aparecía en el cielo del Oeste al atardecer, y "Phosphorus" cuando aparecía en el cielo del Este al amanecer.

Al encontrarse la órbita de Venus entre la Tierra y el Sol, desde la Tierra se pueden distinguir sus diferentes fases de una forma parecida a las de la Luna. Galileo Galilei fue la primera persona en observar las fases de Venus en diciembre de 1610, una observación que sostenía la entonces discutida teoría heliocéntrica de Copérnico. También anotó los cambios en el tamaño del diámetro visible de Venus en sus diferentes fases, sugiriendo que este se encontraba más lejos de la Tierra cuando estaba lleno y más cercano cuando se encontraba en fase creciente. Estas observaciones proporcionaron una sólida base al modelo heliocéntrico.
Venus es más brillante cuando el 25 % de su disco (aproximadamente) se encuentra iluminado, lo que ocurre 37 días antes de la conjunción inferior (en el cielo vespertino) y 37 días después de dicha conjunción (en el cielo matutino). Su mayor elongación y altura sobre el horizonte se produce aproximadamente 70 días antes y después de la conjunción inferior, momento en el que muestra justo media fase; entre estos intervalos, Venus es visible durante las primeras o últimas horas del día si el observador sabe dónde buscarlo. El período de movimiento retrógrado es de veinte días en cada lado de la conjunción inferior.

En raras ocasiones, Venus puede verse en el cielo de la mañana y de la tarde el mismo día. Esto sucede cuando se encuentra en su máxima separación respecto a la eclíptica y al mismo tiempo se encuentra en la conjunción inferior; entonces desde uno de los hemisferios terrestres se puede ver en los dos momentos. Esta oportunidad se presentó recientemente para los observadores del hemisferio norte durante unos días sobre el 29 de marzo de 2001, y lo mismo sucedió en el hemisferio sur el 19 de agosto de 1999. Estos eventos se repiten cada ocho años conforme al ciclo sinódico del planeta.
En el siglo XIX, muchos observadores atribuyeron a Venus un período de rotación aproximado de 24 horas. El astrónomo italiano Giovanni Schiaparelli fue el primero en predecir un período de rotación significativamente menor, proponiendo que la rotación de Venus estaba bloqueada por el Sol (lo mismo que propuso para Mercurio). Aunque realmente no es verdad para ninguno de los dos cuerpos, era una estimación bastante aproximada. El período de rotación de Venus fue observado por primera vez durante la conjunción de 1961 con radar desde una antena de 26 metros en Goldstone, California, desde el observatorio de radioastronomía Jodrell Bank en el Reino Unido y en las instalaciones de espacio profundo de la Unión Soviética de Yevpatoria. La precisión fue refinada en las siguientes conjunciones, principalmente desde Goldstone y Yevpatoria. El hecho de que la rotación era retrógrada no fue confirmado sino hasta 1964.

Antes de las observaciones de radio de los años sesenta, muchos creían que Venus contenía un entorno como el de la Tierra. Esto era debido al tamaño del planeta y su radio orbital, que sugerían claramente una situación parecida a la de la Tierra, así como por la gruesa capa de nubes que impedían ver la superficie. Entre las especulaciones sobre Venus estaban las de que este tenía un entorno selvático o que poseía océanos de petróleo o de agua carbonatada. Sin embargo, las observaciones mediante microondas en 1956 por C. Mayer "et al.", indicaban una alta temperatura de la superficie (600K). Extrañamente, las observaciones hechas por A. D. Kuzmin en la banda milimétrica indicaban temperaturas mucho más bajas. Dos teorías en competición explicaban el inusual espectro de radio: una de ellas sugería que las altas temperaturas se originaban en la ionosfera y la otra sugería una superficie caliente.

Uno de los fenómenos de la atmósfera de Venus observado por astrónomos desde la Tierra y aún no explicado es el de las llamadas luces Ashen.

Los tránsitos de Venus acontecen cuando el planeta cruza directamente entre la Tierra y el Sol y son eventos astronómicos relativamente raros. La primera vez que se observó este tránsito astronómico fue en 1639 por Jeremiah Horrocks y William Crabtree. El tránsito de 1761, observado por Mijaíl Lomonosov, proporcionó la primera evidencia de que Venus tenía una atmósfera, y las observaciones de paralaje del siglo XIX durante sus tránsitos permitieron obtener por primera vez un cálculo preciso de la distancia entre la Tierra y el Sol. Los tránsitos solo pueden ocurrir en junio o diciembre, siendo estos los momentos en los que Venus cruza la eclíptica (al plano en el que la Tierra orbita alrededor del Sol), y suceden en pares a intervalos de ocho años, separados dichos pares de tránsitos por más de un siglo. El anterior par de tránsitos sucedió en 1874 y 1882, y el presente par de tránsitos son los de 2004 y 2012.

El tránsito de Venus ocurre porque la órbita de Venus está inclinada 3.5 grados respecto a la de la Tierra de modo que el plano de la órbita de Venus se interseca con el de la Tierra en dos puntos que son opuestos, a modo de los puntos equinocciales de la órbita de la Tierra en relación con su propio plano ecuatorial. Venus pasa con frecuencia regular cada 584 días entre la Tierra y el Sol, pero el tránsito ocurre cuando Venus y la Tierra coinciden en alinearse en algo de esos dos puntos de intersección y pueden hacerlo dos veces seguidas en 8 años, como el caso de los tránsitos de 2004 y 2012. Dado que los encuentros de Venus y Tierra al mismo lado del Sol acusan una precesión de unos 2 días cada 8 años, la coincidencia de ambos en el punto de intersección ocurre cada un poco más de un centenar de años.

La órbita de Venus es un 28 % más cercana al Sol que la de la Tierra. Por este motivo, las naves que viajan hacia Venus deben recorrer más de 41 millones de kilómetros adentrándose en el pozo gravitatorio del Sol, perdiendo en el proceso parte de su energía potencial. La energía potencial se transforma entonces en energía cinética, lo que se traduce en un aumento de la velocidad de la nave. Por otro lado, la atmósfera de Venus no invita a las maniobras de frenado atmosférico del mismo tipo que otras naves han efectuado sobre Marte, ya que para ello es necesario contar con una información extremadamente precisa de la densidad atmosférica en las capas superiores y, siendo Venus un planeta de atmósfera masiva, sus capas exteriores son mucho más variables y complicadas que en el caso de Marte.

La primera sonda en visitar Venus fue la sonda espacial soviética Venera 1 el 12 de febrero de 1961, siendo la primera sonda lanzada a otro planeta. La nave resultó averiada en su trayecto y la primera sonda exitosa en llegar a Venus fue la americana Mariner 2, en 1962. El 1 de marzo de 1966, la sonda soviética Venera 3 se estrelló sobre Venus, convirtiéndose en la primera nave espacial en alcanzar la superficie del planeta. A continuación diferentes sondas soviéticas fueron acercándose cada vez más en el objetivo de posarse sobre la superficie venusiana. La Venera 4 entró en la atmósfera de Venus el 18 de octubre de 1967 y fue la primera sonda en transmitir datos medidos directamente en otro planeta. La cápsula midió temperaturas, presiones y densidades, y realizó once experimentos químicos para analizar la atmósfera. Sus datos mostraban un 95 % de dióxido de carbono, y en combinación con los datos de ocultación de la sonda Mariner 5, mostró que la presión en la superficie era mucho mayor de lo previsto (entre 75 y 100 atmósferas). El primer aterrizaje con éxito en Venus lo realizó la sonda Venera 7 el 15 de diciembre de 1970. Esta sonda reveló unas temperaturas en la superficie de entre 457 y 474 grados Celsius. La Venera 8 aterrizó el 22 de julio de 1972. Además de dar datos sobre presión y temperaturas, su fotómetro mostró que las nubes de Venus formaban una capa compacta que terminaba a 35 kilómetros sobre la superficie.

La sonda soviética Venera 9 entró en la órbita de Venus el 22 de octubre de 1975, convirtiéndose en el primer satélite artificial de Venus. Una batería de cámaras y espectrómetros devolvieron información sobre la capa de nubes, la ionosfera y la magnetosfera, así como mediciones de la superficie realizadas por radar. El vehículo de descenso de 660 kilogramos de la Venera 9 se separó de la nave principal y aterrizó, obteniendo las primeras imágenes de la superficie y analizando la corteza con un espectrómetro de rayos gamma y un densímetro. Durante el descenso realizó mediciones de presión, temperatura y fotométricas, así como de la densidad de las nubes. Se descubrió que las nubes de Venus formaban tres capas distintas. El 25 de octubre, la Venera 10 realizó una serie similar de experimentos.

En 1978, la NASA envió la sonda espacial Pioneer Venus. La misión Pioneer Venus consistía en dos componentes lanzados por separado: un orbitador y una multisonda. La multisonda consistía en una sonda atmosférica mayor y otras tres más pequeñas. La sonda mayor fue desplegada el 16 de noviembre de 1978, y las tres pequeñas lo fueron el 20 de noviembre. Las cuatro sondas entraron en la atmósfera de Venus el 9 de diciembre, seguidas por el vehículo que las portaba. Aunque no se esperaba que ninguna sobreviviera al descenso, una de las sondas continuó operando hasta 45 minutos después de alcanzar la superficie. El vehículo orbitador de la Pioneer Venus fue insertado en una órbita elíptica alrededor de Venus el 4 de diciembre de 1978. Transportaba 17 experimentos y funcionó hasta agotar su combustible de maniobra, momento en el que perdió su orientación. En agosto de 1992 entró en la atmósfera de Venus y fue destruida. Los estudios que se llevaron a cabo con el Pioneer Venus fueron principalmente sobre la Interacción de la Ionosfera de Venus con el Viento Solar.

La exploración espacial de Venus permaneció muy activa durante finales de los 70 y los primeros años de la década de los 80. Se comenzó a conocer en detalle la geología de la superficie de Venus, y se descubrieron volcanes ocultos inusualmente masivos denominados como "coronae" y "arachnoids". Venus no presenta evidencias de placas tectónicas, a menos que todo el tercio norte del planeta forme parte de una sola placa. Las dos capas superiores de nubes resultaron estar compuestas de gotas de ácido sulfúrico, aunque la capa inferior está compuesta probablemente por una solución de ácido fosfórico. Las misiones Vega desplegaron globos aerostáticos que flotaron a unos 53 kilómetros de altitud durante 46 y 60 horas respectivamente, viajando alrededor de un tercio del perímetro del planeta. Estos globos midieron velocidades del viento, temperaturas, presiones y densidad de las nubes. Se descubrió un mayor nivel de turbulencias y convección de lo esperado, incluyendo ocasionales baches con caídas de uno a tres kilómetros de las sondas.

El 10 de agosto de 1990, la sonda estadounidense Magallanes llegó a Venus, realizando medidas por radar de la superficie del planeta y obteniendo mapas de una resolución de 100m en el 98 % del planeta. Después de una misión de cuatro años, la sonda Magallanes, tal como estaba planeado, se sumergió en la atmósfera de Venus el 11 de octubre de 1994 y se vaporizó en parte, aunque se supone que algunas partes de la misma alcanzaron la superficie del planeta. Desde entonces, varias sondas espaciales en ruta hacia otros destinos han usado el método de sobrevuelo de Venus para incrementar su velocidad mediante el impulso gravitacional. Esto incluye a las misiones Galileo a Júpiter, la Cassini-Huygens a Saturno (con dos sobrevuelos) y la Messenger a Mercurio (dos sobrevuelos).

La Agencia Espacial Europea maneja una misión llamada Venus Express, que estudia la atmósfera y las características de la superficie desde la órbita. La Venus Express fue lanzada desde el Cosmódromo de Baikonur (Kazajistán) el 9 de noviembre de 2005, y pese a que se esperaba que permanezca operativa hasta diciembre de 2009, la ESA decidió prolongar oficialmente la misión hasta 2015. La Agencia Japonesa de Exploración Espacial (JAXA) lanzó la misión PLANET-C el 20 de mayo de 2010, pero debido a que la sonda no desaceleró lo suficiente para entrar en la órbita del planeta Venus, pasó de largo y entró en órbita solar.

El planeta Venus ha inspirado numerosas referencias religiosas y astrológicas en las civilizaciones antiguas. La inspiración mitológica de Venus se extiende también a obras de ficción como:


Algunas obras más recientes que tratan de manera más realista el planeta son:







</doc>
<doc id="2911" url="https://es.wikipedia.org/wiki?curid=2911" title="Viella (Siero)">
Viella (Siero)

Viella es una parroquia del concejo de Siero, en el Principado de Asturias (España); y un lugar de dicha parroquia. Alberga una población de 5.416 habitantes (INE 2011) en 1.548 viviendas. Ocupa una extensión de 2,74 km². 

Está situada en la zona occidental del concejo y limita al norte con la parroquia de Pruvia, en el concejo de Llanera; al oeste, con la de Bobes; al sur, con la de Granda; al oeste, con la de Lugones; y al noreste con la de Lugo, de nuevo en el concejo de Llanera.

El lugar de Viella tiene una población de 676 habitantes (INE 2011), está situado a una altura de 210 msnm y dista 11,6 km de Pola de Siero, capital del concejo.

Según el nomenclátor de 2011 la parroquia está formada por las poblaciones de:


</doc>
<doc id="2912" url="https://es.wikipedia.org/wiki?curid=2912" title="Violaceae">
Violaceae

Violaceae es una familia del orden Malpighiales. Consta de plantas herbáceas (en la península ibérica); en los trópicos, por su parte, de especies arbustivas, arbóreas y sobre todo lianas. 

Hojas simples, alternas o en roseta (con escapo floral acaule), de ordinario estipuladas y entonces estas llamativas y con valor sistemático. Flores hermafroditas, generalmente zigomorfas, pentámeras; cáliz con 5 sépalos, entre cuyas piezas, es frecuente la presencia de apéndices verdes y membranosos, generalmente reflejos; corola con 5 pétalos, 2 dorsales e iguales entre sí, 2 laterales, iguales entre sí, o no, erguidos o no, y un pétalo ventral, o prolongado en un espolón (Viola) o en un saco obtuso (Hipantus) de ovario súpero; androceo con 5 o 3 + 2 estambres, con las anteras aplicadas sobre el estilo, en ocasiones 2 ventrales prolongadas en apéndices estaminales que penetran en el espolón, a veces filamentos dilatados y aplicados; gineceo súpero, tricarpelar, de carpelos abiertos, con placentación parietal; el estilo es variable y con carácter taxonómico, globoso, capitado, o/y acodado; flores a veces cleistógamas (entonces, sin néctar), en general solitarias. <br>Fruto en cápsula de dehiscencia valvar (a veces cápsulas elásticas, autocoria) o raramente en baya. Semillas con carúncula (mirmidocoria). Unas 800 especies, por casi todo el mundo.

Tres subfamilias están reconocidas: Violoideae, Leonioideae y Fusispermoideae.








Alsodeiaceae , Leoniaceae y Retrosepalaceae 


</doc>
<doc id="2913" url="https://es.wikipedia.org/wiki?curid=2913" title="Violales">
Violales

Violales es un Orden de plantas pertenecientes a la clase de las magnoliópsidas, disgregado según la última revisión realizada por el grupo para la Filogenia de las Angiospermas.

Cuentan generalmente con flores dialipétalas. Gineceo en general supero y sincárpico de carpelos abiertos; placentación parietal (en cistáceas con tabicación interna tabiques placentarios); algunas familias primitivas: Fluconitiaceas con carpelos cerrados y placentación axial (semejante a Teales). Origen desde Teales. Óvulos crasinucelados, endosperma muy rico en aceites y pobre en féculas. Diferencia con Caparales (crucíferas): 2 en Caparales, 3 en Violales, estas nunca tienen células con mirosina. Muy amplio.

En el sistema APG II de 2003, el orden "Violales" no es reconocido y las familias que lo integraban han sido trasladas a otros órdenes según se lista a continuación:


</doc>
<doc id="2914" url="https://es.wikipedia.org/wiki?curid=2914" title="Virus informático">
Virus informático

Un virus es un "software" que tiene por objetivo de alterar el funcionamiento normal de cualquier tipo de dispositivo informático, sin el permiso o el conocimiento del usuario, principalmente para lograr fines maliciosos sobre el dispositivo. Los virus, habitualmente, reemplazan archivos ejecutables por otros infectados con el código de este. Los virus pueden destruir, de manera intencionada, los datos almacenados en una computadora, aunque también existen otros más inofensivos, que solo producen molestias.

Los virus informáticos tienen básicamente la función de propagarse a través de un "software", son muy nocivos y algunos contienen además una carga dañina ("payload") con distintos objetivos, desde una simple broma hasta realizar daños importantes en los sistemas, o bloquear las redes informáticas generando tráfico inútil.
El funcionamiento de un virus informático es conceptualmente simple. Se ejecuta un programa que está infectado, en la mayoría de las ocasiones, por desconocimiento del usuario. El código del virus queda residente (alojado) en la memoria RAM de la computadora, incluso cuando el programa que lo contenía haya terminado de ejecutar. El virus toma entonces el control de los servicios básicos del sistema operativo, infectando, de manera posterior, archivos ejecutables que sean llamados para su ejecución. Finalmente se añade el código del virus al programa infectado y se graba en el disco, con lo cual el proceso de replicado se completa.

El primer virus atacó a una máquina IBM Serie 360 (y reconocido como tal). Fue llamado Creeper, (ENMS) creado en 1972. Este programa emitía periódicamente en la pantalla el mensaje: «I'm the creeper... catch me if you can!» («¡Soy una enredadera... píllame si puedes!»). Para eliminar este problema se creó el primer programa antivirus denominado "Reaper" (segador).

Sin embargo, el término virus no se adoptaría hasta 1984, pero éstos ya existían desde antes. Victor Vyssotsky, Robert Morris Sr. y Doug McIlroy, investigadores de Bell Labs (se cita erróneamente a Dennis Ritchie o Ken Thompson como cuarto coautor) desarrollaron un juego de ordenador llamado "Darwin" (del que derivará "Core Wars") que consiste en eliminar al programa adversario ocupando toda la RAM.

Después de 1984, los virus han tenido una gran expansión, desde los que atacan los sectores de arranque de disquetes hasta los que se adjuntan en un correo electrónico.

Los virus informáticos afectan en mayor o menor medida a casi todos los sistemas más conocidos y usados en la actualidad.

Cabe aclarar que un virus informático mayoritariamente atacará solo el sistema operativo para el que fue desarrollado, aunque ha habido algunos casos de virus multiplataforma.

Las mayores incidencias se dan en el sistema operativo Windows y Android debido, entre otras causas:

En otros sistemas operativos como las distribuciones GNU/Linux, BSD, Solaris, Mac OS X iOS y otros basados en Unix las incidencias y ataques son raros. Esto se debe principalmente a:

La mayoría de equipos contienen un sistema operativo de disco de la década de 1990 (equipos de 8, 16 y 32 bits) ehan sufrido de las diferentes variantes de virus, principalmente de sector de arranque y de ficheros infectados. La única excepción parecen haber sido las versiones de CP/M, CP/M-86 y DOS Plus, pero no así su descendiente DR-DOS. En los directorios de BBS y la incipiente Internet, siempre está presente un apartado de antivirus. Sin embargo las versiones más actualizadas de estos sistemas operativos solo lo contemplan como algo histórico, al no haber desarrollos específicos para el OS (lo que no elimina, por ej., los ataques a través de navegado web). Esta pujanza se basa sobre todo en videojuegos que necesitan tener el disquete desprotegido de escritura para almacenar puntuaciones o estados del juego, o en determinadas protecciones. Varios están situados en ROM, por lo que no es posible infectar al sistema en sí, pero al necesitar cargar parte desde el disquete, no se realiza comprobación.

Dado que una característica de los virus es el consumo de recursos, los virus ocasionan problemas tales como: pérdida de productividad, cortes en los sistemas de información o daños a nivel de datos.

Una de las características es la posibilidad que tienen de diseminarse por medio de réplicas y copias. Las redes en la actualidad ayudan a dicha propagación cuando éstas no tienen la seguridad adecuada.

Otros daños que los virus producen a los sistemas informáticos son la pérdida de información, horas de parada productiva, tiempo de reinstalación, etc.

Hay que tener en cuenta que cada virus plantea una situación diferente.

Existen dos grandes clases de contagio. En la primera, el usuario, en un momento dado, ejecuta o acepta de forma inadvertida la instalación del virus. En la segunda, el programa malicioso actúa replicándose a través de las redes. En este caso se habla de gusanos.

En cualquiera de los dos casos, el sistema operativo infectado comienza a sufrir una serie de comportamientos anómalos o imprevistos. Dichos comportamientos pueden dar una pista del problema y permitir la recuperación del mismo.

Dentro de las contaminaciones más frecuentes por interacción del usuario están las siguientes:

En el sistema Windows puede darse el caso de que la computadora pueda infectarse sin ningún tipo de intervención del usuario (versiones Windows 2000, XP y Server 2003) por virus como Blaster, Sasser y sus variantes por el simple hecho de estar la máquina conectada a una red o a Internet. Este tipo de virus aprovechan una vulnerabilidad de desbordamiento de "buffer" y puertos de red para infiltrarse y contagiar el equipo, causar inestabilidad en el sistema, mostrar mensajes de error, reenviarse a otras máquinas mediante la red local o Internet y hasta reiniciar el sistema, entre otros daños. En las últimas versiones de Windows 2000, XP y Server 2003 se ha corregido este problema en su mayoría.

Los métodos para disminuir o reducir los riesgos asociados a los virus pueden ser los denominados activos o pasivos. 




Para no infectar un dispositivo, hay que:

Existen diversos tipos de virus, varían según su función o la manera en que este se ejecuta en nuestra computadora alterando la actividad de la misma, entre los más comunes están:
Otros tipos por distintas características son los que se relacionan a continuación:

La característica principal de estos virus es que se ocultan en la memoria RAM de forma permanente o residente. De este modo, pueden controlar e interceptar todas las operaciones llevadas a cabo por el sistema operativo, infectando todos aquellos ficheros y/o programas que sean ejecutados, abiertos, cerrados, renombrados, copiados.
Algunos ejemplos de este tipo de virus son: Randex, CMJ, Meve, MrKlunky.

Al contrario que los residentes, estos virus no permanecen en memoria. Por tanto, su objetivo prioritario es reproducirse y actuar en el mismo momento de ser ejecutados. Al cumplirse una determinada condición, se activan y buscan los ficheros ubicados dentro de su mismo directorio para contagiarlos.

Estos virus se caracterizan por destruir la información contenida en los ficheros que infectan. Cuando infectan un fichero, escriben dentro de su contenido, haciendo que queden total o parcialmente inservibles.

Los términos boot o sector de arranque hacen referencia a una sección muy importante de un disco o unidad de almacenamiento CD, DVD, memorias USB, etc. En ella se guarda la información esencial sobre las características del disco y se encuentra un programa que permite arrancar el ordenador. Este tipo de virus no infecta ficheros, sino los discos que los contienen. Actúan infectando en primer lugar el sector de arranque de los dispositivos de almacenamiento. Cuando un ordenador se pone en marcha con un dispositivo de almacenamiento, el virus de boot infectará a su vez el disco duro. 

Los virus de boot no pueden afectar al ordenador mientras no se intente poner en marcha a este último con un disco infectado. Por tanto, el mejor modo de defenderse contra ellos es proteger los dispositivos de almacenamiento contra escritura y no arrancar nunca el ordenador con uno de estos dispositivos desconocido en el ordenador. 

Algunos ejemplos de este tipo de virus son: Polyboot.B, AntiEXE.

Los ficheros se ubican en determinadas direcciones (compuestas básicamente por unidad de disco y directorio), que el sistema operativo conoce para poder localizarlos y trabajar con ellos.

Los virus de enlace o directorio alteran las direcciones que indican donde se almacenan los ficheros. De este modo, al intentar ejecutar un programa (fichero con extensión EXE o COM) infectado por un virus de enlace, lo que se hace en realidad es ejecutar el virus, ya que éste habrá modificado la dirección donde se encontraba originalmente el programa, colocándose en su lugar.

Una vez producida la infección, resulta imposible localizar y trabajar con los ficheros originales.

Más que un tipo de virus, se trata de una técnica utilizada por algunos de ellos, que a su vez pueden pertenecer a otras clasificaciones.
Estos virus se cifran a sí mismos para no ser detectados por los programas antivirus. Para realizar sus actividades, el virus se descifra a sí mismo y, cuando ha finalizado, se vuelve a cifrar.

Son virus que en cada infección que realizan se cifran de una forma distinta (utilizando diferentes algoritmos y claves de cifrado).
De esta forma, generan una elevada cantidad de copias de sí mismos e impiden que los antivirus los localicen a través de la búsqueda de cadenas o firmas, por lo que suelen ser los virus más costosos de detectar.

Virus muy avanzados, que pueden realizar múltiples infecciones, combinando diferentes técnicas para ello. Su objetivo es cualquier elemento que pueda ser infectado: archivos, programas, macros, discos, etc.

Infectan programas o ficheros ejecutables (ficheros con extensiones EXE y COM). Al ejecutarse el programa infectado, el virus se activa, produciendo diferentes efectos.

La tabla de asignación de ficheros o FAT (del inglés "File Allocation Table") es la sección de un disco utilizada para enlazar la información contenida en éste. Se trata de un elemento fundamental en el sistema. Los virus que atacan a este elemento son especialmente peligrosos, ya que impedirán el acceso a ciertas partes del disco, donde se almacenan los ficheros críticos para el normal funcionamiento del ordenador.

Son programas que secuestran navegadores de internet principalmente el explorer. Los hijackers alteran las páginas iniciales del navegador e impide que el usuario pueda cambiarla, muestra publicidad en pops ups. Instala nuevas herramientas en la barra del navegador y a veces impiden al usuario acceder a ciertas páginas web. Un ejemplo puede ser no poder acceder a una página de antivirus.

Son programas que secuestran computadoras de forma que es controlada por terceros. Se utiliza para diseminar virus, keyloggers y procedimientos invasivos en general. Esto puede ocurrir cuando la computadora tiene el firewall y su sistema operativo desactualizado. 

Este virus se encarga de registrar cada tecla que sea pulsada, en algunos casos también registran los clics. Son virus que quedan escondidos en el sistema operativo de manera que la víctima no tiene como saber que está siendo monitorizada. Los keyloggers se utilizan usualmente para robar contraseñas de cuentas bancarias, obtener contraseñas personales como las del E-mail, Facebook, etc.

Algunas de las acciones de algunos virus son:

El primer trabajo académico en la teoría de los programas de ordenador auto-replicantes fue publicado por John von Neumann en 1949 quien dio conferencias en la Universidad de Illinois sobre la "Teoría y Organización de Autómatas Complicados" (Theory and Organization of Complicated Automata). El trabajo de von Neumann fue publicado más tarde como la "Teoría de los autómatas autorreproductivos". En su ensayo von Neumann describió cómo un programa de ordenador puede ser diseñado para reproducirse a sí mismo. El diseño de Von Neumann de un programa informático capaz de copiarse a sí mismo se considera el primer virus de computadoras del mundo, y es considerado como el padre teórico de la virología informática.

En 1960 Victor Vyssotsky, Robert Morris Sr. y Doug McIlroy, investigadores de Bell Labs, implementaron un juego de ordenador llamado "Darwin" en un mainframe IBM 7090. En él, dos programas jugadores compiten en la "arena" por controlar el sistema, eliminando a su enemigo, intentado sobreescribir o inutilizar todas sus copias. Una versión mejorada del mismo se conocerá como "Core Wars". Muchos de los conceptos de este se basan en un artículo de Alexander Dewdney en la columna Computer Recreations de la revista Scientific American.

En 1972 Veith Risak publica el artículo "Selbstreproduzierende Automaten mit minimaler Informationsübertragung" (autómata auto reproducible con mínimo intercambio de información). El artículo describe un virus por escrito con fines de investigación. Este contenía todos los componentes esenciales. Fue programado en Lenguaje ensamblador para el equipo SIEMENS 4004/35 y corrió sin problemas.

En 1975 el autor Inglés John Brunner publica la novela El jinete de la onda de shock, en la que anticipa el riesgo de virus de Internet. Thomas Joseph Ryan describió 1979 en " The Adolescence of P-1" (la adolescencia de P-1), como una Inteligencia Artificial se propaga de forma similar a un virus en la red informática nacional.

En 1980, Jürgen Kraus escribió una tesis en la Universidad técnica de Dortmund, en la que compara a algunos programas con los virus biológicos.

En 1982 Rich Skrenta, un estudiante de instituto de 15 años, programa el Elk Cloner para los Apple II, el primer virus informático conocido que tuvo una expansión real y no como un concepto de laboratorio. Puede ser descrito como el primer virus de sector de arranque.

En 1984 Leonard M. Adleman utilizó en una conversación con Fred Cohen por primera vez el término «virus informático».




</doc>
<doc id="2915" url="https://es.wikipedia.org/wiki?curid=2915" title="Vitaceae">
Vitaceae

Vitaceae es una familia de plantas leñosas, principalmente lianas provistas de zarcillos opositifolios. Hojas alternas generalmente palmatilobuladas o palmaticompuestas. Flores pequeñas, hermafroditas o dioicas, actinomorfas, pentámeras o tetrámeras; cáliz gamosétalo, poco desarrollado; corola de pétalos libres o concrescentes por su parte superior, caduca; ovario súpero, con 2 óvulos por lóculo. Inflorescencias diversas, generalmente paniculiformes. Frutos en bayas. Unas 600 especies la mayoría de países cálidos.





</doc>
<doc id="2918" url="https://es.wikipedia.org/wiki?curid=2918" title="Vitamina">
Vitamina

Las vitaminas (del inglés "vitamine", hoy "vitamin", y este del latín "vita" ‘vida’ y el sufijo "amina", término acuñado por el bioquímico Casimir Funk en 1912) son compuestos heterogéneos imprescindibles para la vida, ya que al ingerirlos de forma equilibrada y en dosis esenciales promueven el correcto funcionamiento fisiológico. La mayoría de las vitaminas esenciales no pueden ser elaboradas por el organismo, por lo que este no puede obtenerlas más que a través de la ingesta equilibrada de vitaminas contenidas en los alimentos naturales. Las vitaminas son nutrientes que junto con otros elementos nutricionales actúan como catalizadoras de todos los procesos fisiológicos (directa é indirectamente).

Las vitaminas son precursoras de coenzimas, (aunque no son propiamente enzimas) grupos prostéticos de las enzimas. Esto significa que la molécula de la vitamina, con un pequeño cambio en su estructura, pasa a ser la molécula activa, sea esta coenzima o no.

Los requisitos mínimos diarios de las vitaminas no son muy altos, se necesitan tan solo dosis de miligramos o microgramos contenidas en grandes cantidades (proporcionalmente hablando) de alimentos naturales. Tanto la deficiencia como el exceso de los niveles vitamínicos corporales pueden producir enfermedades que van desde leves a graves e incluso muy graves como la pelagra o la demencia entre otras, e incluso la muerte. Algunas pueden servir como ayuda a las enzimas que actúan como cofactor, como es el caso de las vitaminas hidrosolubles.

La deficiencia de vitaminas se denomina hipovitaminosis mientras que el nivel excesivo de vitaminas se denomina hipervitaminosis.

Está demostrado que las vitaminas del grupo B son imprescindibles para el correcto funcionamiento del cerebro y el metabolismo corporal. Este grupo es hidrosoluble (solubles en agua) debido a esto son eliminadas principalmente por la orina, lo cual hace que sea necesaria la ingesta diaria y constante de todas las vitaminas del complejo “B” (contenidas en los alimentos naturales).

Las vitaminas se pueden clasificar según su solubilidad: si lo son en agua "hidrosolubles" o si lo son en lípidos "liposolubles". En los seres humanos hay 13 vitaminas que se clasifican en dos grupos: (9) hidrosolubles (8 del complejo B y la vitamina C) y (4) liposolubles (A, D, E y K). 
Las vitaminas liposolubles, A, D, E y K, se consumen junto con alimentos que contienen grasa.

Son las que se disuelven en grasas y aceites. Se almacenan en el hígado y en los tejidos grasos. Debido a que se pueden almacenar en la grasa del cuerpo no es necesario tomarlas todos los días por lo que es posible, tras un consumo suficiente, subsistir una época sin su aporte.

Si se consumen en exceso (más de 10 veces las cantidades recomendadas) pueden resultar tóxicas. Esto les puede ocurrir sobre todo a deportistas, que aunque mantienen una dieta equilibrada recurren a suplementos vitamínicos en dosis elevadas, con la idea de que así pueden aumentar su rendimiento físico. Esto es totalmente falso, así como la creencia de que los niños van a crecer más si toman más vitaminas de las necesarias.

Las vitaminas liposolubles son:

Estas vitaminas no contienen nitrógeno, son solubles en grasa, y por tanto, son transportadas en la grasa de los alimentos que la contienen. Por otra parte, son bastante estables frente al calor (la vitamina C se degrada a 90º en oxalatos tóxicos). Se absorben en el intestino delgado con la grasa alimentaria y pueden almacenarse en el cuerpo en mayor o menor grado (no se excretan en la orina). Dada a la capacidad de almacenamiento que tienen estas vitaminas no se requiere una ingesta diaria.

Las vitaminas hidrosolubles son aquellas que se disuelven en agua. Se trata de coenzimas o precursores de coenzimas, necesarias para muchas reacciones químicas del metabolismo.

En este grupo de vitaminas, se incluyen las vitaminas B (tiamina), B (riboflavina), B (niacina o ácido nicotínico), B (ácido pantoténico), B (piridoxina), B/B (biotina), B (ácido fólico), B (cobalamina) y vitamina C (ácido ascórbico).

Estas vitaminas contienen nitrógeno en su molécula (excepto la vitamina C) y no se almacenan en el organismo, a excepción de la vitamina B, que lo hace de modo importante en el hígado. El exceso de vitaminas ingeridas se excreta en la orina, por lo cual se requiere una ingesta prácticamente diaria, ya que al no almacenarse se depende de la dieta. Por otro lado, estas vitaminas se disuelven en el agua de cocción de los alimentos con facilidad, por lo que resulta conveniente aprovechar esa agua para preparar caldos o sopas.

La deficiencia de vitaminas puede producir trastornos más o menos graves, según el grado de deficiencia, llegando incluso a la muerte. Respecto a la posibilidad de que estas deficiencias se produzcan en el mundo desarrollado hay posturas muy enfrentadas. Por un lado están los que aseguran que es prácticamente imposible que se produzca una avitaminosis, y por otro los que responden que es bastante difícil llegar a las dosis de vitaminas mínimas, y por tanto, es fácil adquirir una deficiencia, por lo menos leve.

Normalmente, los que alegan que es “poco probable” una avitaminosis son mayoría. Este grupo mayoritario argumenta que:

Por el lado contrario se responde que:

Por estos motivos un bando recomienda consumir suplementos vitamínicos si se sospecha que no se llega a las dosis necesarias. Por el contrario, el otro bando lo ve innecesario, y avisan que abusar de suplementos puede ser perjudicial.

Las vitaminas aunque son esenciales, pueden ser tóxicas en grandes cantidades. Unas son muy tóxicas y otras son inocuas incluso en cantidades muy altas.
La toxicidad puede variar según la forma de aplicar las dosis. Como ejemplo, la vitamina D se administra en cantidades suficientemente altas como para cubrir las necesidades para 6 meses; sin embargo, no se podría hacer lo mismo con vitamina B3 o B6, porque sería muy tóxica.
Otro ejemplo es el que la suplementación con vitaminas hidrosolubles a largo plazo, se tolera mejor debido a que los excedentes se eliminan fácilmente por la orina.

Las vitaminas más tóxicas son la D, y la A, también lo puede ser la vitamina B3.
Otras vitaminas, sin embargo, son muy poco tóxicas o prácticamente inocuas.
La B no posee toxicidad incluso con dosis muy altas. A la tiamina le ocurre parecido, sin embargo con dosis muy altas y durante mucho tiempo puede provocar problemas de tiroides. En el caso de la vitamina E, solo es tóxica con suplementos específicos de vitamina E y con dosis muy elevadas. También se conocen casos de intoxicaciones en esquimales al comer hígado de mamíferos marinos (el cual contiene altas concentraciones de vitaminas liposolubles).

La principal fuente de vitaminas son los vegetales crudos, por ello, hay que igualar o superar la recomendación de consumir 5 raciones de vegetales o frutas frescas al día.

Por eso hay que evitar los procesos que produzcan perdidas de vitaminas en exceso:

Aunque la mayoría de los procesamientos perjudica el contenido vitamínico, algunos procesos biológicos pueden incrementar el contenido de vitaminas en los alimentos, como por ejemplo:

Los procesos industriales, normalmente suelen destruir las vitaminas. Pero alguno puede ayudar a que se reduzcan las pérdidas:
No consumir vitaminas en los niveles apropiados (contenidas en los alimentos naturales) puede causar graves enfermedades.

El valor de comer ciertos alimentos para mantener la salud era reconocido mucho antes de que se identificaran las vitaminas. Los antiguos egipcios sabían que la alimentación de una persona con hígado podía ayudar a curar la ceguera nocturna, una enfermedad que ahora se sabe que es causada por una deficiencia de vitamina A. El avance de los viajes océanicos durante el Renacimiento dio lugar a que las expediciones pasaran largos periodos sin acceso a frutas frescas y vegetales y a que apareciesen enfermedades por deficiencias vitamínicas, bastante comunes entre las tripulaciones de los buques.

En 1747, el cirujano escocés James Lind descubrió que los alimentos cítricos ayudaban a prevenir el escorbuto, una enfermedad particularmente mortal en la que el colágeno no se forma correctamente, causando mala cicatrización de las heridas, el sangrado de las encías, dolores agudos y, finalmente, la muerte. En 1753, Lind publicó su "Treatise on the Scurvy" [Tratado sobre el escorbuto], que recomendaba el uso de limones y limas para evitarlo, práctica que fue adoptada por la Armada Real Británica. (Esto dio lugar al apodo "Limey" para los marineros de la Royal Navy). El descubrimiento de Lind, sin embargo, no fue aceptado por todos y en las expediciones árticas de la misma Royal Navy, en el siglo XIX, en lugar de prevenir el escorbuto con una dieta de alimentos frescos, se creía evitarlo con una buena higiene, el ejercicio regular y el mantenimiento de la moral de la tripulación a bordo. Como resultado, las expediciones árticas continuaron siendo afectadas por el escorbuto y otras enfermedades de deficiencias vitamínicas. A principios del siglo XX, cuando Robert Falcon Scott realizó sus dos expediciones a la Antártida, la teoría médica que prevalecía en ese momento era que el escorbuto era causado por la comida enlatada «contaminada».

Desde finales del siglo XVIII y principios del XIX, el uso de estudios de privación permitió a los científicos aislar e identificar una serie de vitaminas. Los lípidos del aceite de pescado se utilizaron para curar el raquitismo en ratas, y por ello los nutrientes solubles en grasa se llamaron antirraquitismo A ("antirachitic A"). Así, el primer bioactivo “vitamínico” nunca aislado, que curó el raquitismo, se llamó inicialmente “vitamina A”; sin embargo, la bioactividad de este compuesto se llama ahora vitamina D. En 1881, el cirujano ruso Nikolai Lunin estudió los efectos del escorbuto mientras estaba en la Universidad de Tartu, en la actual Estonia. Alimentó ratones con una mezcla artificial de todos los constituyentes separados de la leche conocidos en ese momento, a saber, proteínas, grasas, carbohidratos, y sales. Los ratones que recibieron solo los componentes individuales murieron, mientras que los ratones alimentados con la leche en sí se desarrollaron normalmente. Lunin llegó a la conclusión de que «un alimento natural, como la leche, debe por lo tanto contener, además de estos ingredientes principales conocidos, pequeñas cantidades de sustancias desconocidas esenciales para la vida». Sin embargo, sus conclusiones fueron rechazadas por otros investigadores cuando fueron incapaces de reproducir sus resultados. La diferencia fue que él había utilizado el azúcar de mesa (sacarosa), mientras que otros investigadores habían utilizado el azúcar de la leche (lactosa) que todavía contenía pequeñas cantidades de vitamina B.

En Asia oriental, donde el arroz blanco refinado era el alimento básico común de la clase media, el beriberi resultante de la falta de vitamina B era endémico. En 1884, Takaki Kanehiro, un experimentado médico británico de la Armada Imperial Japonesa, observó que el beriberi era endémico entre la tripulación de bajo rango que a menudo solo comía arroz, pero que no aparecía entre los oficiales que consumían una dieta al estilo occidental. Con el apoyo de la marina japonesa, experimentó con las tripulaciones de dos barcos de guerra; una tripulación fue alimentada solo con arroz blanco, mientras que la otra lo fue con una dieta de carne, pescado, cebada, arroz y frijoles. En el grupo que solo comía arroz blanco se documentaron 161 casos de beriberi y 25 muertes en la tripulación, mientras que en el segundo grupo solo se dieron 14 casos de beriberi y ninguna muerte. Esto convenció a Takaki y a la marina de guerra japonesa que la dieta era la causa del beriberi, pero se equivocaron cuando creyeron que con cantidades suficientes de proteínas lo impedirian. Que las enfermedades podrían ser el resultado de algunas deficiencias en la dieta fue además investigado por Christiaan Eijkman, quien en 1897 descubrió que la alimentación con arroz integral en lugar de la variedad refinada para pollos, ayudaba a prevenir el beriberi en las gallinas. Al año siguiente, Frederick Hopkins postuló que algunos alimentos contenían «factores accesorios» —además de proteínas, carbohidratos, grasas, etc.— que eran necesarios para las funciones del cuerpo humano. Hopkins y Eijkman fueron galardonados con el en 1929 por su descubrimiento de varias vitaminas.

En 1910, el científico japonés Umetaro Suzuki logró aislar el primer complejo vitamínico, extrayendo un complejo hidrosoluble de micronutrientes a partir del salvado de arroz, al que llamó ácido abérico (más tarde "Orizanin"). Publicó este descubrimiento en una revista científica japonesa. Cuando el artículo fue traducido al alemán, en la traducción no se hacía constar que se trataba de un nutriente recién descubierto (afirmación sí hecha en el artículo original en japonés) y por ello su descubrimiento paso inadvertido. En 1912, el bioquímico polaco Casimir Funk aisló el mismo complejo de micronutrientes y propuso que el complejo se llamará «vitamina» (de «vital amina», nombre sugerido por Max Nierenstein un amigo y lector de bioquímica en la Universidad de Bristol.) El nombre pronto se convirtió en sinónimo de los «factores accesorios» de Hopkins, y, cuando se demostró que no todas las vitaminas eran aminas, la palabra ya estaba en todas partes. En 1920, Jack Cecil Drummond propuso que la “e” final se suprimiera para restarle importancia a la referencia “amina”, cuando los investigadores empezaron a sospechar que no todas las “vitaminas” (en particular, la vitamina A) tenían un componente de amina.

En 1930, Paul Karrer dilucidó la estructura correcta del beta-caroteno, el principal precursor de la vitamina A, e identificó otros carotenoides. Karrer y Norman Haworth confirmaron el descubrimiento de Albert Szent-Györgyi del ácido ascórbico e hicieron importantes contribuciones a la química de las flavinas, lo que llevó a la identificación de la lactoflavina. Por sus investigaciones sobre los carotenoides, las flavinas y las vitaminas A y B2, ambos recibieron el Premio Nobel de Química en 1937.

En 1931, Albert Szent-Györgyi y uno de sus investigadores Joseph Svirbely sospecharon que el “ácido hexurónico” era en realidad la vitamina C, y dieron una muestra a Charles Glen King, que probó su eficacia contra el escorbuto en ensayos con conejillos de indias. En 1937, Szent-Györgyi fue galardonado con el Premio Nobel de Fisiología o Medicina por su descubrimiento. En 1943, Edward Adelbert Doisy y Henrik Dam fueron galardonados con el Premio Nobel de Fisiología o Medicina por su descubrimiento de la vitamina K y su estructura química. En 1967, George Wald fue galardonado con el Premio Nobel (junto con Ragnar Granit y Haldan Keffer Hartline) por su descubrimiento de que la vitamina A podría participar directamente en un proceso fisiológico.




</doc>
<doc id="2919" url="https://es.wikipedia.org/wiki?curid=2919" title="Velocidad (desambiguación)">
Velocidad (desambiguación)

El término velocidad puede referirse al:



</doc>
<doc id="2922" url="https://es.wikipedia.org/wiki?curid=2922" title="Vacío">
Vacío

El vacío (del latín "vacīvus") es la ausencia total de material en los elementos (materia) en un determinado espacio o lugar, o la falta de contenido en el interior de un recipiente. Por extensión, se denomina también vacío a la condición de una región donde la densidad de partículas es muy baja, como por ejemplo el espacio interestelar; o la de una cavidad cerrada donde la presión del aire u otros gases es menor que la atmosférica.

Puede existir naturalmente o ser provocado en forma artificial, puede ser para usos tecnológicos o científicos, o en la vida diaria. Se aprovecha en diversas industrias, como la alimentaria, la automovilística o la farmacéutica.

De acuerdo con la definición de la Sociedad Estadounidense del Vacío o AVS (1958), el término se refiere a cierto espacio lleno con gases a una presión total menor que la presión atmosférica, por lo que el grado de vacío se incrementa en relación directa con la disminución de presión del gas residual. Esto significa que cuanto más se disminuya la presión, mayor vacío se obtendrá, lo que permite clasificar el grado de vacío en correspondencia con intervalos de presiones cada vez menores. Cada intervalo tiene características propias.

La presión atmosférica es la que ejerce la atmósfera o aire sobre la Tierra. A temperatura ambiente y presión atmosférica normal, un metro cúbico de aire contiene aproximadamente en movimiento a una velocidad promedio de . Una manera de medir la presión atmosférica es con un barómetro de mercurio; su valor se expresa en términos de la altura de la columna de mercurio de sección transversal unitaria y de alto. Con base en esto, se dice que una atmósfera estándar es igual a . Se utilizara por conveniencia la unidad torricelli (símbolo, Torr) como medida de presión; = 1 mmHg, por lo que = ; por lo tanto = 1/760 de una atmósfera estándar, o sea = .

Uno de los métodos más conocidos para medir bajas presiones es el método desarrollado por Pirani.
Consiste en un puente de Weaston donde una resistencia del puente se encuentra expuesta al vacío a medir. La resistencia de ese elemento sensor variará según cambie la presión, debido a que a vacíos cerca de presión atmosférica el filamento estará en contacto con más moléculas, generando una baja de temperatura y por consiguiente una baja en su valor resistivo.
A medida que mejora el vacío este filamento ira encontrando menos moléculas para disipar su calor, por consiguiente aumentara su temperatura. Este aumento de temperatura producirá un aumento de su valor resistivo generando un desequilibrio en el puente de weaston. Este desequilibrio se mide con un microamperimetro. Luego solo queda interpolar los microamperes generados por el puente de weaston con los valores de vacío. Estos valores se vuelcan en una tabla con la que se dibuja una escala, donde por ejemplo en los vacuómetros CINDELVAC, se tendrá 0 microamperios cuando el sensor esté en alto vacío y 50 microamperios a presión atmosférica. La tabla de respuesta del puente de Weaston CINDELVAC es la siguiente:

Tienen el mismo fundamento que las bombas de ionización, hasta el punto que estas pueden considerarse como una consecuencia de aquellas. Cuando se trata de medir presiones de vacío muy bajas, se utilizan las variantes propuestas por Bayard-Alpert de aquellos aparatos capaces de suministrar con gran exactitud presiones de hasta .

El aire está compuesto por varios gases; los más importantes son el nitrógeno y el oxígeno, pero también contiene en menores concentraciones gases como dióxido de carbono, argón, neón, helio, criptón, xenón, hidrógeno, metano, óxido nitroso y vapor de agua.

Durante toda la Antigüedad y hasta el Renacimiento se desconocía la existencia de la presión atmosférica. No podían por tanto dar una explicación de los fenómenos debidos al vacío. En Grecia se enfrentaron por ello dos teorías. Para Epicuro y sobre todo para Demócrito (420 a. C.) y su escuela, la materia no era un todo continuo sino que estaba compuesta por pequeñas partículas indivisibles (átomos) que se movían en un espacio vacío y que con su distinto ordenamiento daban lugar a los distintos estados físicos. Por el contrario, Aristóteles excluía la noción de vacío y para justificar los fenómenos que su propia Física no podía explicar recurría al célebre aforismo según el cual «la Naturaleza siente horror al vacío» (teoría que resultó dominante durante la Edad Media y hasta el descubrimiento de la presión).

Este término de "horror vacui" fue el utilizado incluso por el propio Galileo a comienzos del siglo XVII al no poder explicar ante sus discípulos el hecho de que una columna de agua en un tubo cerrado por su extremo no se desprenda, si el tubo ha sido invertido estando sumergido el extremo libre del mismo dentro de agua. Sin embargo, supo transmitir a sus discípulos la inquietud por explicar el hecho anterior y asociado a él, por qué las bombas aspirantes-impelentes (órgano hidráulico inventado por el alejandrino Ctesibio, contemporáneo de Arquímedes) no podían hacer subir el agua de los pozos a una altura superior a los .

En 1630 Giovanni Battista Baliani envió una carta a Galileo Galilei donde le notificaba que no lograba que el agua en los sifones subiera más allá de . Galileo le propuso que la explicación era que el vacío no tenía fuerza suficiente nada más que para levantar esa cantidad de agua. En 1640 el italiano Gasparo Berti tratando de explicar lo que ocurría con los sifones realizó el primer experimento con el vacío. Creó lo que constituye, primordialmente, un barómetro de agua, el cual resultó capaz de producir vacío.

Al analizar el informe experimental de Berti, Evangelista Torricelli captó con claridad el concepto de presión de aire, por lo que diseñó, en 1644, un dispositivo para demostrar los cambios de presión en el aire. Construyó un barómetro que en lugar de agua empleaba mercurio, y de esta manera, sin proponérselo, comprobó la existencia del vacío.

El barómetro de Torricelli constaba de un recipiente y un tubo lleno de mercurio (Hg) cerrado en uno de sus extremos. Al invertir el tubo dentro del recipiente se formaba vacío en la parte superior del tubo. Esto era algo difícil de entender en su época, por lo que se intentó explicarlo diciendo que esa región del tubo contenía vapor de mercurio, argumento poco aceptable ya que el nivel de mercurio en el tubo era independiente del volumen del mismo utilizado en el experimento.

La aceptación del concepto de vacío se dio cuando en 1648, Blaise Pascal subió un barómetro con de mercurio a una montaña a . Sorprendentemente, cuando el barómetro estaba en la cima, el nivel de la columna de Hg en el tubo era mucho menor que al pie de la montaña. Torricelli aseguraba la existencia de la presión de aire y decía que debido a ella el nivel de Hg en el recipiente no descendía, lo cual hacía que el tamaño de la columna de mercurio permaneciera constante dentro del tubo. Así pues, al disminuir la presión del aire en la cima de la montaña, el nivel de Hg en el recipiente subió y en la columna dentro del tubo bajó inmediatamente (se vació de manera parcial).

El paso final que dio Torricelli fue la construcción de un barómetro de mercurio que contenía en la parte vacía del tubo otro barómetro para medir la presión de aire en esa región. Se hicieron muchas mediciones y el resultado fue que no había una columna de mercurio en el tubo del barómetro pequeño porque no se tenía presión de aire. Esto aclaró que no existía vapor de mercurio en la parte vacía del tubo. Así, se puso en evidencia la presión del aire y, lo más importante, la producción y existencia del vacío.

Entonces, después de varios experimentos se puede explicar bien el funcionamiento del barómetro de Torricelli: la atmósfera ejerce una presión, lo cual impide que el mercurio salga del tubo y del recipiente; es decir, cuando la presión atmosférica se iguale a la presión ejercida por la columna de mercurio, el mercurio no podrá salir del tubo. Cuando el aire pesa más, soporta una columna mayor de mercurio; y cuando pesa menos, no es capaz de resistir la misma columna de mercurio, así que se escapa un poco de mercurio del tubo.

En muchas ocasiones, en los laboratorios modernos, ocurren situaciones donde un contenedor lleno de un gas debe ser vaciado. La evacuación debe ser el primer paso para crear un nuevo ambiente gaseoso. Durante el proceso de destilación, se debe de remover de manera continua el gas a medida que se desarrolla el proceso. Algunas veces es necesario evacuar el contenedor para prevenir que el aire contamine alguna superficie limpia o que interfiera con alguna reacción química. Haces de partículas atómicas deben ser tratadas al vacío para prevenir la pérdida de momentum a través de las colisiones con las moléculas de aire. Muchas formas de radiación son absorbidas por el aire y por lo tanto solamente pueden ser propagadas sobre largas distancias en el vacío. Un sistema de vacío es una parte esencial para los instrumentos de laboratorio, tales como el espectómetro de masa y los microscopios electrónicos. Sistemas de vacío simples son utilizados para la deshidratación al vacío y la congelación al vacío. Aceleradores de partículas nucleares y dispositivos termonucleares requieren de sistemas de vacío muy sofisticados y de enormes proporciones. En procesos industriales modernos, dentro de los más notables la fabricación de semiconductores, se requieren de ambientes cuidadosamente controlados al vacío.

La presión y composición de los gases residuales en un sistema de vacío varía considerablemente con su diseño e historia. Para algunas aplicaciones una densidad de gas residual de decenas de miles de millones de moléculas por centímetro cúbico es tolerable. En otros casos, no más de unos cientos de miles de moléculas por centímetro cúbico constituyen un vacío aceptable.

Para presiones por debajo de la atmosférica se suele categorizar el vacío de la siguiente forma:

La composición del gas en un sistema de vacío se modifica a la vez que el sistema evacua debido a que la eficiencia de las bombas de vacío es diferente para diferentes gases. A bajas presiones las moléculas de las paredes del contenedor comienzan ser des absorbidas y se conforma el gas residual. Inicialmente, el grueso del gas que deja las paredes es vapor de agua y dióxido de carbono; a muy bajas presiones, en contenedores que han sido horneados, se tiene hidrógeno.




</doc>
<doc id="2923" url="https://es.wikipedia.org/wiki?curid=2923" title="Verrucariaceae">
Verrucariaceae

Verrucariaceae es una familia de líquenes que se caracteriza por tener el talo crustáceo y los gonidios de color verde intenso, además de no presentar nunca como gonidios clorófitas del orden Trentepohliales. La familia se divide en 21 géneros, los más conocidos de los cuales son "Verrucaria", "Polyblastia" y "Staurothele" que se diferencian en la configuración de las esporas y en la presencia o ausencia de gonidios himeniales.

Los géneros de esta familia son:
http://www.biologie.uni-hamburg.de/b-online/ibc99/botanica/botanica/liquen2.htm

http://davesgarden.com


</doc>
<doc id="2924" url="https://es.wikipedia.org/wiki?curid=2924" title="VPN (desambiguación)">
VPN (desambiguación)

La sigla VPN puede referirse a:



</doc>
<doc id="2926" url="https://es.wikipedia.org/wiki?curid=2926" title="Vulcanismo del Campo de Calatrava">
Vulcanismo del Campo de Calatrava

La región volcánica del Campo de Calatrava (también llamada Provincia Volcánica de Calatrava) constituye, junto con la de Olot, en Gerona, y la de Cabo de Gata, en Almería, una de las tres zonas de vulcanismo reciente más importantes de la Península Ibérica. Su actividad se desarrolló entre hace 8,7 y 1,75 millones de años, es decir, durante el Plioceno y el Cuaternario. Es, por tanto, una actividad bastante reciente, lo que ha permitido que los edificios volcánicos conserven en buena parte su morfología original, y sus productos se hayan preservado en buenas condiciones de observación hasta la actualidad. 

La región volcánica tiene una extensión total de unos 5.000 km², e incluye unos 240 edificios volcánicos diferenciados. Algunas de las principales localidades que quedan incluidas dentro del área son Ciudad Real, Miguelturra, Almagro, Daimiel y Bolaños. Puertollano se sitúa próxima a su extremo Sur, mientras que los edificios volcánicos más próximos a Almadén son los de La Bienvenida y Cabezarados.

El vulcanismo estromboliano originó pequeños volcanes cónicos, actualmente degradados a cerros redondeados, de formas troncocónicas a semiesféricas, dependiendo del grado de erosión. Sus diámetros van desde los 100 m a los 2 km, y sus alturas, desde 20 a 120 m. Sólo ocasionalmente se identifican depresiones tipo cráter. De estos volcanes suelen partir coladas de lavas de diferente importancia, que pueden llevar a alcanzar los 6-7 km de longitud. Algunos de los mejores ejemplos de este tipo de volcán son los de La Yezosa, entre Bolaños de Calatrava y Almagro, y Cerro Gordo, en Valenzuela de Calatrava.

El vulcanismo hidromagmático es el más frecuente en la región, y da origen a unas depresiones volcánicas muy características, pero a menudo difíciles de identificar como tales en el terreno: se trata de los denominados "maares", que llegan a alcanzar diámetros de 1-1.5 km. Uno de los ejemplos más típicos puede ser la Hoya del Mortero, en Poblete. 

Por otra parte, es relativamente frecuente que se sucedan momentos de actividad hidromagmática y estromboliana a partir del mismo centro emisor.

En el relleno sedimentario de uno de los antiguos maares, en Alcolea de Calatrava, se encuentra el yacimiento de Las Higueruelas, un yacimiento paleontológico del Plioceno Superior, que ha proporcionado una abundante y variada fauna de mamíferos terrestres, aves, anfibios y reptiles.

Los materiales volcánicos que aparecen en el Campo de Calatrava son variados, tanto explosivos como efusivos, presentando cenizas, lapillis, escorias, bloques lávicos y bombas; y por otro lado, coladas de variada morfología, se manifiestan en fondos de valle y en vertientes. Los magmas son siempre básicos, por lo que la existencia de actividad explosiva que se ha registrado en ocasiones no parece muy acorde, esto se puede explicar por la existencia de acuíferos subterráneos, que con el calor del magma generaron una potente explosión freática o hidromagmática; o sobre todo por un exceso de gas carbónico en algunos magmas, que dieron lugar a la apertura de diatremas.

Las rocas volcánicas emitidas por estos volcanes corresponden a basaltos en sentido amplio: se pueden diferenciar una serie de variedades, tanto composicionales: melilititas olivínicas, limburgitas, nefelinitas olivínicas, basaltos y basanitas o leucititas olivínicas, como texturales: rocas porfídicas masivas, piroclastos escoriáceos, y depósitos hidromagmáticos. 

Las variedades porfídicas masivas presentan textura porfídica, y están constituidas por fenocristales de olivino o de olivino y piroxeno en matriz microcristalina a vítrea, formada por microcristales de augita, óxidos de hierro y titanio (magnetita-ilmenita) y olivino. Además pueden presentar plagioclasa, feldespatoides, melilita y vidrio, en proporciones variables, lo que permite la clasificación petrográfica más fina antes mencionada. 

En lo que se refiere a sus aplicaciones, estas variedades masivas se han empleado hasta fechas recientes en la obtención de adoquines para la pavimentación de calles. Su principal aplicación actual es la obtención de áridos de trituración y, en especial, para la obtención de balasto para el Tren de Alta Velocidad. Una de las principales canteras existentes sobre este tipo de materiales es la del Morrón de Villamayor. También tienen utilidad como rocas de construcción.

Las variedades piroclásticas escoriáceas son rocas muy vacuolares, de tipo "piedra pómez", que aparecen formando masas constituidas por fragmentos de estas rocas de tamaños muy variables: desde acúmulos de material de grano muy fino, pulverulento (cenizas), hasta acúmulos de grandes bloques, pasando por acumulaciones muy heterométricas de fragmentos de tamaño medio centi- a decimétrico (lapilli), con presencia ocasional de fragmentos de tamaño muy superior (bombas). A continuación se muestran algunos ejemplos de este tipo de rocas: 

Estos materiales se explotan en varias canteras de la región para la obtención de puzolanas, lo que constituye su principal aplicación industrial. Hay que indicar, por otra parte, que han sido también utilizados como piedra de construcción, en monumentos tan significados como el Castillo de Calatrava La Nueva, o la ermita visigótica de la Virgen de Zuqueca, en Oreto, antigua e importante ciudad romana (Granátula de Calatrava).

Los depósitos hidromagmáticos constituyen normalmente depósitos bien estratificados, en los que se suelen diferenciar facies planares, con laminación/estratificación paralela, y facies con estratificación cruzada. Además, suelen presentar grandes bombas de material no volcánico (cuarcitas, fundamentalmente). 

Corresponden a tobas líticas o lítico-cristalinas, poco consolidadas y heterométricas, formadas mayoritariamente por fragmentos de rocas paleozoicas (cuarcitas, pizarras) o terciarias, siendo poco abundantes los componentes volcánicos cogenéticos (fragmentos basálticos, cristales de olivino, piroxenos, etc.). 

No presentan utilidad industrial, más que para la obtención de áridos clasificados.

Desde el punto de vista geoquímico, las rocas volcánicas de la región del Campo de Calatrava corresponden a un magmatismo alcalino de intraplaca, generado a partir de bajas tasas de fusión parcial del manto superior. Los magmas serían líquidos primarios, como indican los altos contenidos en Ni y el alto valor del parámetro #Mg (=MgO/MgO+FeO). 

La tabla adjunta muestra la composición química media y la norma CIPW calculada de las diferentes variedades petrográficas porfídicas. 

Estos caracteres geoquímicos, y el estudio de su evolución espacial y temporal, permiten establecer que el magmatismo de la región del Campo de Calatrava podría estar relacionado con la existencia de un punto caliente asociado a un proceso de elevación cortical y posiblemente de "rifting" abortado. 

Asociado a este magmatismo encontramos una serie de yacimientos minerales, de escasa importancia minera, ya que por lo general presentan escaso tonelaje, pero que constituyen un tipo único a nivel mundial. Se trata de mineralizaciones de óxidos de Fe y de Mn, las segundas con el interés añadido de que presentan contenidos relativamente elevados en Co, lo que ha hecho que hasta fecha reciente hayan sido objeto de prospección minera, con ánimo de localizar alguna masa de suficiente volumen como para permitir su explotación. 

Son yacimientos de origen sedimentario, que encontramos como niveles dentro de las secuencias del Plioceno y Cuaternario, constituyendo masas lenticulares de cierta potencia (hasta varios metros) y extensión lateral (varios cientos de metros, en los mejores casos). Su origen parece estar relacionado con el de otras manifestaciones características del área y mucho más conocidas: los manantiales de "agua agria" o "hervideros", el más conocido de los cuales podría ser la "Fuente Agria" de Puertollano. El nexo genético sería que ambos, mineralizaciones y manantiales, serían manifestaciones de actividad hidrotermal póstuma ligada al magmatismo. 

En lo que se refiere a las mineralizaciones, se pueden establecer dos grandes tipos: 

Desde el punto de vista mineralógico, los minerales que podemos encontrar en estos yacimientos son óxidos e hidróxidos complejos de Mn (criptomelana y litioforita, fundamentalmente). Son minerales de hábito terroso, micro- o criptocristalinos, sin apenas interés para coleccionismo. 

Las costras de óxidos de Fe-Mn son formaciones lenticulares de algunos metros de espesor por varios centenares de metros cuadrados de extensión, en general asociadas a alguna surgencia de aguas agrias. Las costras están formadas por nódulos de óxidos de Mn cobaltífero recubiertos por una corteza de 1-1.5 cm de espesor de óxidos e hidróxidos de hierro. Uno de los yacimientos más representativos de esta tipología es el de la mina de La Zarza, localizada a unos 2 km al SSO de Pozuelo de Calatrava. 

Las capas de "canutillos" constituyen acumulaciones de pequeñas estructuras vegetales reemplazadas por óxidos de Mn cobaltífero, que aparecen formando niveles de hasta 2-3 m de potencia entre materiales de tipo aluvial. La mina de El Chorrillo, situada en proximidad de la de La Zarza, es uno de los mejores ejemplos de este tipo de mineralizaciones.

Las capas con pisolitos de óxidos de Mn corresponden a mineralizaciones que han sufrido un cierto transporte con respecto a los focos hidrotermales. Están formadas por niveles lenticulares en los que son muy abundantes las estructuras pisolíticas, de diámetro centimétrico, constituidas por los óxidos e hidróxidos de Mn. El yacimiento de Los Ardales se puede considerar representativo de esta tipología. 






</doc>
<doc id="2927" url="https://es.wikipedia.org/wiki?curid=2927" title="Encefalopatía espongiforme bovina">
Encefalopatía espongiforme bovina

La encefalopatía espongiforme bovina, también conocida popularmente como la enfermedad de las vacas locas, es una enfermedad causada por priones, y que se puede transmitir a los seres humanos a través del consumo de partes de animales infectados, sobre todo tejidos nerviosos.

La encefalopatía espongiforme bovina (EEB) o «enfermedad de las vacas locas» es una enfermedad que pertenece a una misteriosa familia de enfermedades emparentadas, muy raras en su mayoría. Los primeros casos de animales enfermos se declararon en el Reino Unido en 1986. En 1996 se detectó en el ser humano una nueva enfermedad, una variante de la Enfermedad de Creutzfeldt-Jakob, que se relacionó con la epidemia de EEB en el ganado vacuno.

Es una enfermedad degenerativa del sistema nervioso central de los bovinos, que se caracteriza por la aparición de síntomas nerviosos en los animales adultos que, progresivamente, finaliza con la muerte del animal.

La enfermedad está causada por una proteína que ha modificado su estructura tridimensional (en Bioquímica, se denominan estructuras secundaria y terciarias de las proteínas), debido un proceso denominado cambio conformacional, y que las convierte en un agente patológico. Estas proteínas infecciosas se denominan priones. El periodo de incubación de la enfermedad es de 4 ó 5 años. Esta proteína es la Prp, que en su variante normal (conformación Nativa) es c pero al entrar en contacto con la proteína en la conformación no nativa pasa a ser Prp (Sc) y en cadena. Ésta, al entrar en contacto con la proteína normal (c) del organismo le induce un cambio conformacional y provoca el paso a la Sc. Es una proteína fisiológica y no se ha podido eliminar del organismo.

Los síntomas que se observan están motivados por la acumulación del prion en las células neuronales originando la muerte celular. Un análisis microscópico revela lesiones como vacuolas que dan al tejido nervioso un aspecto de esponja.

La vía de transmisión de esta enfermedad conocida hasta la fecha es la ingestión de alimentos contaminados con el prion, la administración de fármacos de origen bovino y provenientes de animales enfermos (típicamente hormona del crecimiento) y posiblemente de madre a hijo. El único método disponible para detectar la infección en fase terminal es la inoculación parenteral de tejido encefálico en ratones. No obstante, esta técnica no es utilizable en la práctica ya que los períodos de incubación son de unos 300 días.

La enfermedad se acumula sobre todo en el cráneo (incluidos encéfalo y ojos), la amígdala, la médula espinal, el intestino (del duodeno al recto) y el bazo.

Alan Colchester de la Universidad de Kent propuso en septiembre de 2005 en la revista médica "The Lancet" que la enfermedad pudo haberse originado a través de alimento para ganado procedente de la India, contaminado con restos humanos. El gobierno de la India lo negó rotundamente, calificando a la investigación de "engañosa, maliciosa; producto de la imaginación; absurda," añadiendo que la India mantiene controles constantes y que no han tenido ningún caso de EEB o vECJ. La mayoría de los científicos piensan que la enfermedad se originó en los propios animales y en el consumo de restos no humanos.

Los científicos han aceptado que la aparición de esta enfermedad estuvo determinada por la alimentación suplementaria del ganado bovino con restos de ganado ovino y caprino (que ya presentaban la enfermedad pero no se trasmitía a humanos, denominada scrapie), lo que conllevó a que en 1998 en Reino Unido se sacrificaran e incineraran a los animales sospechosos de haber adquirido la enfermedad. Se sacrificaron más de 20.000 vacas.

Hasta 2007, inclusive, se declararon 336.770 reses enfermas de EEB en la Unión Europea y 516 más en el resto del mundo, la inmensa mayoría en el Reino Unido: el 98,38%. Solo en Gran Bretaña fueron sacrificadas más de 2 millones de reses.

Por otra parte, hasta junio de 2010 se diagnosticaron 220 pacientes humanos afectados por la nueva variante de la Enfermedad de Creutzfeldt-Jakob, 217 casos primarios y 3 secundarios (por una transfusión de sangre).




</doc>
<doc id="2928" url="https://es.wikipedia.org/wiki?curid=2928" title="Integración a muy gran escala">
Integración a muy gran escala

VLSI es la sigla en inglés de Very Large Scale Integration, integración a escala muy grande.

La integración en escala muy grande de sistemas de circuitos basados en transistores en circuitos integrados comenzó en los años 1980, como parte de las tecnologías de semiconductores y comunicación que se estaban desarrollando.

Los primeros chip semiconductores contenían sólo un transistor cada uno. A medida que la tecnología de fabricación fue avanzando, se agregaron más y más transistores, y en consecuencia más y más funciones fueron integradas en un mismo chip. El microprocesador es un dispositivo VLSI.

La primera generación de computadoras dependía de válvulas de vacío. Luego vinieron los semiconductores discretos, seguidos de circuitos integrados. Los primeros CIs contenían un pequeño número de dispositivos, como diodos, transistores, resistencias y condensadores (aunque no inductores), haciendo posible la fabricación de compuertas lógicas en un solo chip. La cuarta generación (LSI) consistía de sistemas con al menos mil compuertas lógicas. El sucesor natural del LSI fue VLSI (varias decenas de miles de compuertas en un solo chip). Hoy en día, los microprocesadores tienen varios millones de compuertas en el mismo chip.

Hacia principios de 2006 se comercializaban microprocesadores con tecnología de hasta 65 nm, en 2010 se comercializan chipsets con tecnología de 32 nm.





</doc>
<doc id="2929" url="https://es.wikipedia.org/wiki?curid=2929" title="Ventenata">
Ventenata

Ventenata, es un género de plantas herbáceas perteneciente a la familia de las poáceas. Es originario de Eurasia.
Son plantas anuales con culmos de 10-70 cm de alto; herbácea. Los culmos con nodos glabros. Los entrenudos huecos. Los brotes no aromáticos. Hojas no agregadas basalmente; no auriculadas. Las láminas de las hojas son lineales; estrechas; de 1-2.5 de ancho, dobladas o enrolladas (convolutas); sin venación. Lígula una membrana no truncada (aguda, a menudo lacerada); 2-4 mm de largo. Inflorescencia paniculada, o un solo racimo; abierto, o comprimido.
El género fue descrito por Georg Ludwig Koeler y publicado en "Descriptio Graminum in Gallia et Germania" 272. 1802. 


</doc>
<doc id="2930" url="https://es.wikipedia.org/wiki?curid=2930" title="Vulpia">
Vulpia

Vulpia es un género de plantas herbáceas perteneciente a la familia de las poáceas. Es originario del norte de Asia y Norteamérica.
Son plantas anuales, a veces cespitosas o estoloníferas. Hoja con vaina con márgenes libres; lígula truncada, a menudo lacerada; limbo setáceo y convoluto, al menos en la desecación, rara vez plano. Inflorescencia en panícula o en racimo, generalmente unilateral. Espiguillas con pedúnculos generalmente dilatados, cuneiformes, marcadamente comprimidas, con 3-10 flores, las superiores frecuentemente estériles; raquilla escábrida o pelosa. Espiguillas desarticulándose por debajo de las flores o por la base de los pedúnculos. Glumas 2, muy desiguales, generalmente más cortas que las flores, herbáceas, agudas, múticas o aristadas; la inferior sin nervios o uninervadas, rara vez trinervadas; la superior más larga que la inferior, trinervada. Lema con (3-) 5 nervios, lanceoladas, papiráceas, generalmente con 1 arista apical, antrorso-escábrida. Pálea algo más corta que la lema, membranosa, bidentada, con 2 quillas antrorso-escábrida o ciliadas. Androceo con 1-3 estambres fértiles. Ovario glabro. Cariopsis estrechamente elipsoidea, soldadas a la pálea.
El género fue descrito por Carl Christian Gmelin y publicado en "Flora Badensis Alsatica" 1: 8. 1805. La especie tipo es: "Vulpia myuros"
El nombre del género fue nombrado en honor de J.S.Vulpius (1760–1840) 
Tiene un número de cromosomas de: x = 7. 2n = 14, 28, y 42. 2, 4, y 6 ploidias. Cromosomas ‘grandes’.

Existen híbridos intergenéricos entre especies de "Vulpia" y de "Festuca" formando - "×Festulpia" Melderis ex Stace & R.Cotton.



</doc>
<doc id="2931" url="https://es.wikipedia.org/wiki?curid=2931" title="Videojuego">
Videojuego

Un videojuego o juego de video es un juego electrónico en el que una o más personas interactúan, por medio de un controlador, con un dispositivo que muestra imágenes de video. Este dispositivo electrónico, conocido genéricamente como «plataforma», puede ser una computadora, una máquina "arcade", una videoconsola o un dispositivo portátil (un teléfono móvil, por ejemplo). Los videojuegos son, año por año, una de las principales industrias del arte y el entretenimiento.

Al dispositivo de entrada usado para manipular un videojuego se lo conoce como controlador de videojuego, o mando, y varía dependiendo de la plataforma. Por ejemplo, un controlador podría únicamente consistir de un botón y una palanca de mando o "joystick", mientras otro podría presentar una docena de botones y una o más palancas (mando). Los primeros juegos informáticos solían hacer uso de un teclado para llevar a cabo la interacción, o bien requerían que el usuario adquiriera un "joystick" con un botón como mínimo. Muchos juegos de computadora modernos permiten o exigen que el usuario utilice un teclado y un ratón de forma simultánea. Entre los controladores más típicos están los "gamepads", "joysticks", teclados, ratones y pantallas táctiles.

Generalmente, los videojuegos hacen uso de otras maneras, aparte de la imagen, de proveer la interactividad e información al jugador. El audio es casi universal, usándose dispositivos de reproducción de sonido, tales como altavoces y auriculares. Otro tipo de realimentación se hace a través de periféricos hápticos que producen vibración o retroalimentación de fuerza, usándose a veces la vibración para simular la retroalimentación de fuerza.

Los orígenes del videojuego se remontan a la década de 1950, cuando poco después de la aparición de las primeras computadoras electrónicas tras el fin de la Segunda Guerra Mundial, se llevaron a cabo los primeros intentos por implementar programas de carácter lúdico. Así, fueron creados el "Nim" (1951) o el "Oxo" (1952), juegos electrónicos pero que aún no son realmente videojuegos, y el "Tennis for Two" (1958) o el "Spacewar!" (1962), auténticos pioneros del género. Todos ellos eran todavía prototipos, juegos muy simples y de carácter experimental que no llegaron a comercializarse, entre otras cosas, porque funcionaban en unas máquinas que solo estaban disponibles en universidades o en institutos de investigación.
No sería hasta la década de los 70 en que, con el descenso de los costes de fabricación, aparecieron las primeras máquinas y los primeros videojuegos dirigidos al gran público. Títulos como "Computer Space" (1971) o "Pong" (1972), de Atari, inauguraron las primeras máquinas recreativas construidas al efecto, que funcionaban con monedas. Poco después llegarían los videojuegos a los hogares gracias a las consolas domésticas, la primera de las cuales fue la Magnavox Odyssey (1972), y más tarde la exitosa Atari 2600 o VCS (de 1977), con su sistema de cartuchos intercambiables. Por aquel entonces las máquinas arcade empezaron a hacerse comunes en bares y salones recreativos, una expansión debida en parte a un matamarcianos que alcanzó gran popularidad, el "Space Invaders" (1978). Otros juegos que marcaron esta primera época fueron "Galaxian" (1979), "Asteroids" (1979) o "Pac-Man" (1980).
En los años 80, la empresa norteamericana Atari tuvo que compartir su dominio en la industria del videojuego con dos compañías llegadas de Japón: Nintendo (con su famosa consola NES) y SEGA (con la Master System). Paralelamente, surgió una generación de ordenadores personales asequibles y con capacidades gráficas que llegaron a los hogares de millones de familias, como fueron el Spectrum, el Amstrad CPC, el Commodore 64 o el MSX. A partir de entonces, los videojuegos empezaron a convertirse en una poderosa industria. Fue además una época muy creativa para los desarrolladores de videojuegos; muchos de los principales géneros que existen hoy en día (conducción, lucha, plataformas, estrategia, aventura...) tomaron forma en esta década. Por otra parte, aparecieron también las primeras consolas de bolsillo, también conocidas como "maquinitas", que aunque hasta la llegada de la Game Boy de Nintendo (1989) solo ejecutaban un juego cada una, alcanzaron gran popularidad entre los más jóvenes.

Los años 90 traen el salto a la tecnología de 16-bit (como la SNES y la Mega Drive), lo que significa importantes mejoras gráficas. Entra en escena el gigante Sony con su primera Playstation (1994), mientras Nintendo y Sega actualizan sus máquinas (Nintendo 64 y Sega Saturn). En cuanto a las computadoras, el progreso de los PC termina por barrer del mapa a los demás sistemas salvo el de Apple. Aparecen juegos cada vez más avanzados tecnológicamente, como los shooters en 3D. En el año 2002 entra Microsoft en el sector de las videoconsolas con su Xbox, y en el 2006 Nintendo lanza su innovadora Wii. Entretanto, Sony actualiza su exitosa Playstation (versiones II y III), y en los PC, gracias a la expansión de internet, cobran protagonismo los juegos en línea y multijugador.

Por último, en la década de 2010 emergen como plataformas de juegos los dispositivos táctiles portátiles, como los teléfonos inteligentes y las tabletas, llegando a un público muy amplio. Por otro lado, varias empresas tecnológicas empiezan a desarrollar cascos de realidad virtual, que prometen traer nuevas experiencias al mundo del entretenimiento electrónico.

Típicamente, los videojuegos recrean entornos y situaciones virtuales en los que el videojugador puede controlar a uno o varios personajes (o cualquier otro elemento de dicho entorno), para conseguir uno o varios objetivos dentro de unas reglas determinadas.

Dependiendo del videojuego, una partida puede disputarla una sola persona contra la máquina, dos o más personas en la misma máquina, o bien múltiples jugadores a través de una red LAN o en línea vía Internet, compitiendo colaborativamente contra la máquina o entre sí.

Existen videojuegos de muchos tipos. Algunos de los géneros más representativos son los videojuegos de acción, rol, estrategia, simulación, deportes o aventura.

Un videojuego se ejecuta gracias a un programa de "software "(el videojuego en sí) que es procesado por una máquina (el "hardware") que cuenta con dispositivos de entrada y de salida.

El programa de software o soporte lógico contiene toda la información, instrucciones, imágenes y audio que componen el videojuego. Va grabado en cartuchos, discos ópticos, discos magnéticos, tarjetas de memoria especiales para videojuegos, o bien se descarga directamente al aparato a través de Internet.

En la década de 1980 el soporte habitual para el software era el cartucho en las videoconsolas, y el disco magnético o la cinta de casete en los ordenadores. Posteriormente se usó el CD-ROM, pues tenía más capacidad que los cartuchos ya que estos habían llegado a su tope tecnológico y además resultaba más económico para producir en masa. Actualmente se usa el sistema DVD de alta capacidad y, en las consolas de sobremesa como PlayStation 4 y Xbox One, el Blu-Ray, de capacidad muy alta. Sin embargo desde hace unos años está creciendo la descarga desde Internet, al ser una tecnología extendida masivamente, de fácil acceso y menos costosa que la distribución física de discos, aparte de las ventajas de seguridad al evitar pérdidas por daños o extravío de discos (ya que el videojuego estará virtualmente siempre disponible).
Los dispositivos de entrada son los que permiten al jugador manejar el juego. Si bien es habitual el uso de un dispositivo de entrada externo —como son los clásicos teclado y ratón, el mando, o el "joystick"—, las plataformas portátiles de hoy en día ("smartphones", "tablets", videoconsolas de bolsillo...) permiten jugar mediante su pantalla táctil o mediante el movimiento del propio aparato (gracias al uso de giroscopios y acelerómetros). Otros dispositivos de entrada son los detectores de movimiento, entre los que destacan los dispositivos de mano (por ejemplo el Wiimote de Wii), los de presión (alfombras o soportes con sensores), los de dispositivos de realidad virtual como el Playstation VR y los de captura de imágenes, caso del Kinect de Xbox. También se puede emplear la voz en aquellos videojuegos que la soporten a través de procesadores de voz.

Los dispositivos de salida son aquellos que muestran las imágenes y los sonidos del videojuego: un televisor, un monitor o un proyector para el vídeo, y unos altavoces o auriculares para el audio. Los equipos más modernos utilizan sonido digital con Dolby Surround con efectos EAX y efectos visuales modernos por medio de las últimas tecnologías en motores de videojuego y unidades de procesamiento gráfico.

La pieza central del hardware lo constituye la CPU o unidad central de procesamiento, que interpreta las instrucciones contenidas en los programas y procesa los datos. Su capacidad de procesamiento, mayor en cada nueva generación de dispositivos, marca el límite de las posibilidades técnicas y gráficas de los videojuegos.

Todos estos dispositivos (de entrada, de salida, de procesamiento...) pueden constituir unidades físicamente separadas pero conectadas entre sí —como es el caso de los PC o las videoconsolas de sobremesa—, o bien estar integradas en un solo aparato —como sucede en los teléfonos móviles y otros dispositivos portátiles—.

Los distintos tipos de dispositivo en los que se ejecutan los videojuegos se conocen como plataformas. Los cuatro tipos de plataformas más populares son el PC, las videoconsolas, los dispositivos portátiles y las máquinas arcade.
Las videoconsolas o consolas de videojuegos son aparatos electrónicos domésticos destinados exclusivamente a reproducir videojuegos. Creadas por diversas empresas desde los años 70, han generado un inmenso negocio de trascendencia histórica en la industria del entretenimiento. La videoconsola por antonomasia es un aparato de sobremesa que se conecta a un televisor para la visualización de sus imágenes, pero existen también modelos de bolsillo con pantalla incluida, conocidos como videoconsolas portátiles.

El PC u ordenador personal es también una plataforma de videojuegos, pero como su función no es solo ejecutar videojuegos, no se considera como videoconsola. El PC no entra en ninguna generación, ya que cada pocos meses salen nuevas piezas que modifican sus prestaciones. El PC por regla general puede ser mucho más potente que cualquier consola del mercado. Los más potentes soportan modos gráficos con resoluciones y efectos de postprocesamiento gráfico muy superiores a cualquier consola.

Las máquinas recreativas de videojuegos están disponibles en lugares públicos de diversión, centros comerciales, restaurantes, bares, o salones recreativos especializados. En los años 80 y 90 del siglo pasado disfrutaron de un alto grado de popularidad al ser entonces el tipo de plataforma más avanzado técnicamente. Los progresos tecnológicos en las plataformas domésticas han supuesto a comienzos del siglo XXI una cierta decadencia en el uso de las máquinas arcade.

Las videoconsolas portátiles y otros aparatos de bolsillo cuentan con la capacidad para reproducir videojuegos. Entre estos últimos destacan hoy en día los teléfonos móviles (en particular los teléfonos inteligentes) que, sin ser los videojuegos su función primaria, los han ido incorporando a medida que se han ido incrementando sus prestaciones técnicas. Otro dispositivo portátil de creciente popularidad en los últimos años son los tabletas.

Los videojuegos se pueden clasificar en géneros atendiendo a factores como el sistema de juego, el tipo de interactividad con el jugador, sus objetivos, etc. La evolución de los videojuegos desde sus comienzos ha dado lugar a una variedad creciente y cambiante de géneros, muchas veces en relación con lo que los avances en la tecnología han ido haciendo posible. Entre los géneros de videojuegos más populares están los de acción, estrategia, rol, aventura, rompecabezas, simulación, deportes o carreras, cada uno de ellos con varios subgéneros. Por otro lado, hoy en día son habituales los videojuegos que toman elementos de más de un género, lo que ha dado lugar a géneros mixtos (por ejemplo rol-acción, aventura-acción, etc.).

Junto a los géneros, existen otras formas de clasificar o caracterizar los juegos como puede ser por su temática (fantástico-medieval, futurista, de guerra...), su complejidad (juegos AAA, juegos casuales...), su finalidad (educativos, promocionales, artísticos...), tipo de desarrollo, etc.

Por otra parte, también se distingue a unos juegos de otros, incluso dentro de un mismo género, por la perspectiva visual que adoptan (o dicho de otra manera, la posición de la cámara). Así, hay juegos con perspectiva 2D (ya sea con proyección paralela, vista lateral o vista cenital), 2.5D (mediante proyección isométrica, oblicua, o parallax scrolling, entre otras), y 3D (en perspectiva fija, en primera persona, o en tercera persona).

En muchos juegos se puede encontrar la opción de multijugador, es decir, que varias personas puedan participar simultáneamente en la misma partida, ya sea empleando todos la misma máquina (como suele ocurrir con las videoconsolas) o bien usando cada uno su propio dispositivo (el caso habitual en los PC o dispositivos portátiles, en lo que se conoce como videojuegos en línea). Existen juegos en que un jugador humano se enfrenta contra otros jugadores controlados por la máquina, mediante inteligencia artificial, pero en este caso no se considera que sea un videojuego multijugador. Por último, hay videojuegos que están pensados para congregar a un gran número de jugadores de todo el mundo conectados a través de Internet, son los conocidos como videojuegos MMO (de "massively multiplayer online").

La creación de videojuegos es una actividad llevada a cabo por empresas conocidas como desarrolladoras de videojuegos. Estas se encargan de diseñar y programar el videojuego, desde el concepto inicial hasta el videojuego en su versión final. Ésta es una actividad multidisciplinaria, que involucra profesionales de la informática, el diseño, el sonido, la actuación, etc. El proceso es similar a la creación de software en general, aunque difiere en la gran cantidad de aportes creativos (música, historia, diseño de personajes, niveles, etc.) necesarios. El desarrollo también varía en función de la plataforma objetivo (PC, móviles, consolas), el género (estrategia en tiempo real, RPG, aventura gráfica, plataformas, etc.) y la forma de visualización (2d, 2.5d y 3d). Algunas de las más importantes desarrolladoras de videojuegos a nivel internacional son: Blizzard Entertainment, Valve, Rockstar North, Bungie, Microsoft, Nintendo, BioWare, Sega , Sierra Entertainment o Zynga, a las cuales hay que añadir los estudios internos (a menudo homónimos) de las principales distribuidoras.

La comercialización de los juegos creados por las desarrolladoras es labor de las distribuidoras de videojuegos. Estas se ocupan de su distribución (ya sea a través de tiendas físicas o por internet), publicidad, presentación, traducción... pero también ejercen a menudo un papel fundamental antes y durante el desarrollo del juego, como es su concepción, su financiación, los estudios de mercado, el control de calidad, etc. Muchas distribuidoras tienen uno o varios estudios de desarrollo propio, al margen de que puedan o no trabajar con desarrolladoras externas. Algunas de las más importantes distribuidoras de videojuegos son: Electronic Arts, Ubisoft, Activision Blizzard, Nintendo, Sony Computer Entertainment, Microsoft Studios, Take-Two Interactive, Capcom, Konami, Bethesda Softworks, Square Enix, Bandai Namco, Valve, Rockstar Games, Gameloft y SEGA.

Nacida con la aparición de la primera máquina recreativa a monedas en 1971, la industria del videojuego ha pasado en unas pocas décadas de ser una mera curiosidad tecnológica a convertirse en una de las mayores industrias del entretenimiento por volumen de facturación. Se estima éste en 81.500 millones de dólares en 2014 a nivel mundial, llegando a duplicar el de la industria del cine en el mismo año. Los ingresos proceden fundamentalmente de la venta de videojuegos, de videoconsolas, de accesorios y de máquinas recreativas. Los principales países en ingresos por videojuegos son EEUU, China y Japón, seguidos de Alemania y Reino Unido. España se sitúa en la décima posición, facturando anualmente cerca de 1.500 millones de dólares.

La industria del videojuego da trabajo a más de 100.000 personas en todo el mundo, gente de muy diversas disciplinas entre las que se incluye la programación, el diseño, la ingeniería, la interpretación, las finanzas, la mercadotecnia, la música, la comunicación o el comercio. La cadena de valor en la industria del videojuego se puede dividir en 6 partes: los inversores, los desarrolladores de videojuegos, los creadores del software empleado por los desarrolladores, los fabricantes de hardware, las distribuidoras de videojuegos, y los consumidores.

Los costes de desarrollo de un videojuego comercial varían enormemente desde los pocos miles de dólares que puede representar un título pequeño, desarrollado por una sola persona, hasta los más de 100 millones de dólares de algunos videojuegos AAA, en los que intervienen equipos de hasta un centenar de trabajadores. El videojuego con mayor coste de desarrollo hasta la fecha es "Grand Theft Auto V", de la desarrolladora Rockstar Games, con 167 millones de dólares, seguido por "Destiny", de Bungie, con 154 millones de dólares. Las cifras son aún bastante mayores si se suma la inversión en mercadotecnia.

Las constituyen uno de los principales escaparates donde la industria presenta sus más recientes creaciones cada año. Las más conocidas mundialmente son la E3 en Los Ángeles (EEUU), la Gamescom en Colonia (Alemania) y la Tokyo Games Show (Japón). En Francia la feria más importante es la Paris Games Week y en España destacan la Madrid Games Week, GameLab y GameFest. En Chile destaca sobre todo la Festigame, siendo la más importante de Latinoamérica. Los consumidores se informan de las novedades del sector principalmente a través de medios de comunicación especializados. Entre los pertenecientes al ámbito hispano se pueden destacar revistas en papel como Micromanía, New Superjuegos o Hobbyconsolas, y revistas en internet como Meristation, Vandal, Eurogamer, Gamercafe, LagZero o Niubie. Existen también numerosos blogs y canales de Youtube centrados esta temática. Es menor sin embargo su presencia en televisión o en radio.

La venta de videojuegos se ha realizado tradicionalmente en grandes almacenes o en tiendas físicas especializadas; en España las dos principales cadenas de tiendas de videojuegos son Game y, hasta su cierre en 2014, Gamestop. En Chile las más grandes son Zmart, MicroPlay y TodoJuegos. Sin embargo, la tendencia en estos últimos años en todo el mundo es hacia la venta por internet mediante descarga, tanto en PC como en consolas. En dispositivos móviles, la venta por internet —a través de las tiendas de aplicaciones— es de hecho el único canal disponible.

Las principales asociaciones sectoriales en España son AEVI (Asociación Española de Videojuegos), que engloba a compañías que facturan el 90 % de los ingresos totales del sector, y DEV (Desarrollo Español de Videojuegos), que agrupa a las principales compañías desarrolladoras del país. En Chile está el grupo VGChile, donde se agrupan los desarrolladores chilenos.

En un mercado dominado por las grandes distribuidoras de videojuegos, cabe destacar en esta última década el auge de los videojuegos independientes (también conocidos como "indie"), que han llegado a constituir un apartado propio en tiendas y en plataformas de distribución como Steam o GameJolt. Estos juegos son desarrollados por pequeños grupos de no más de 20 personas, sin la ayuda financiera de ninguna distribuidora. Se caracterizan habitualmente por un desarrollo artístico variado y particular, tanto en gráficos como en banda sonora, con historias de tramas innovadoras y que generalmente no tienen una continuidad o no están diseñados para crear una saga. La originalidad de sus planteamientos, alejándose muchas veces de los estereotipos establecidos, le ha valido el interés de una parte de los aficionados.

La ciencia que estudia los videojuegos se llama "ludología" o estudio de los juegos. Uno de los aspectos principales estudiados por esta ciencia y la información que recolecta son los impactos positivos y negativos de los videojuegos en las personas.

La ludología (o "games studies") se ocupa del estudio crítico de los juegos, de su diseño, de los jugadores y de la interacción entre ambos, así como su papel en la sociedad y la cultura. Los métodos usados para recolectar información van desde encuestas e investigaciones etnográficas, hasta experimentos controlados de laboratorio.

Una encuesta online realizada por la ISFE entre europeos con edades comprendidas entre 16 y 64 años reveló que el 48 % de ellos juega a videojuegos, ya sea de manera habitual (1 o más veces por semana, 25 %) u ocasional (23 %). En España dicho porcentaje se situaría en el 40 %, y en EEUU (según la ESA) en el 59 %. Por edades, el 51 % de los videojugadores europeos son menores de 35 años y el 49 % mayores, situándose la edad promedio en 34 años (el estudio realizado en EEUU la sitúa en 31 años). Por sexos, el 55 % son hombres, el 45 % mujeres.

Cabe señalar que la demografía de los videojuegos no ha sido siempre la misma. En el pasado los videojuegos eran un tipo de entretenimiento casi exclusivamente para personas jóvenes, pero esta situación ha ido cambiando con el paso de los años como demuestran diversos estudios recientes. Además de los ya señalados, un estudio de Parra, David et al. (2009) en el que se realizaron 974 encuestas a españoles mayores de 35 años concluyó que "los videojuegos se están implantando con singular intensidad en el conjunto de la población española. Más de la mitad de los españoles mayores de 35 años (53,5 por ciento) juega con videojuegos (bien de manera esporádica bien de manera habitual)".

Los efectos que pueda tener el uso habitual de videojuegos en las personas, y en especial en los niños, han sido objeto de interés y de controversia.

Entre los efectos positivos que se les atribuyen están capacidades tales como: «coordinación ojos-manos, capacidad lógica, capacidad espacial, resolución de problemas, desarrollo de estrategias, concentración, atención, colaboración, cooperación, discriminación y selección de información relevante, estimulación auditiva, entre otras». Según un estudio, el niño desarrolla habilidades mentales y su capacidad de razonamiento es más activa en comparación a un niño de hace 20 años que no contaba con esta tecnología. En adultos pueden funcionar como un liberador de estrés, contribuyendo a una buena salud. Otros afirman que mejoran la salud visual e incluso ciertas habilidades como por ejemplo las necesarias para práctica de la cirugía. Hay que señalar también que los efectos varían según el tipo de juego.  Un catedrático de la Universidad de Nottingham también ha afirmado que pueden tener el efecto de atenuar el dolor. Según un estudio, la exposición a corto plazo tiene un efecto positivo en la atención en unos niños sin problemas psiquiátricos, estos obtuvieron una mejor puntuación en la prueba de Stroop después de estar expuestos durante una hora a un videojuego que jugaron por primera vez.

En cuanto a los aspectos negativos de los videojuegos, cabe señalar factores como la adicción. El fácil acceso a ordenadores, "smartphones "y consolas, sumado a una falta de control por parte de los padres o el ambiente de un hogar disfuncional, puede dar lugar a que niños o adolescentes hagan un uso abusivo de los videojuegos. Ello tiene efectos negativos como es el ser más propensos a la agresividad, falta de asertividad y bajo rendimiento académico.

Otro aspecto controvertido de los videojuegos en los niños es que pueden frenar algunos aspectos de su desarrollo motriz, y conducir a una falta de socialización, aunque esto último está rebatido por otros estudios que apuntan a todo lo contrario, a que los videojuegos aumentan su sociabilidad."Los videojuegos son un entretenimiento que se adecua bien a la realidad del niño nacido en la era de la informática ya que suponen una socialización en la cultura de simulación que caracteriza a las sociedades avanzadas contemporáneas”(Turkle 1997). 

En un estudio se asocia la salud mental y jugar videojuegos, se encontró que los jugadores que jugaban de forma moderada tenían la mejor salud mental, los que jugaban de forma excesiva tenían un leve incremento en comportamientos problemáticos y los que no jugaban videojuegos tuvieron la peor salud mental.

Diversos expertos han señalado el valor de los videojuegos como herramientas para inculcar conocimientos. Gros, B. y sus colaboradores (1997) escriben: «Pensamos que los juegos de ordenador constituyen un material informático de gran valor pedagógico», y enumeran una serie de características:

Los juegos educativos se presentan en los últimos tiempos como una alternativa a los videojuegos violentos. Incluso existe una colección de juegos cuya carátula versa "«la alternativa inteligente a los videojuegos violentos»."

A pesar de las cuestiones positivas, se deben tener en cuenta todos aquellos aspectos negativos, como el uso ilimitado y no vigilado, así como la falta de compromiso, responsabilidad o esfuerzo con actividades que no estén relacionadas con el juego. Por ello, lo ideal es no perder de vista que aunque los videojuegos están en función del entretenimiento, son utilizados como herramientas para posibilitar o potencializar el aprendizaje, lo cual se logrará siempre y cuando exista un buen uso y control por parte de los usuarios o los responsables de estos.

Gee (2004) establece que “la teoría de aprendizaje incorporada a los buenos videojuegos se acerca más a lo que yo creo que son las mejores teorías del aprendizaje planteadas por la ciencia cognitiva”. De esta forma, establece una similitud entre las teorías del aprendizaje que se utilizan diariamente en el ámbito educativo con las teorías incorporadas en los videojuegos, encontrando entre ellas pocos puntos en los que diverjan.

Los videojuegos, como otras formas de expresión audiovisual, han despertado controversia entre personas o colectivos que consideran que tienen o pueden tener efectos perniciosos sobre los jugadores. Entre estos se arguyen por ejemplo los efectos que puede tener en el desarrollo emocional el hecho de pasar demasiado tiempo ante la pantalla e inhibirse por completo en un universo de fantasía. Existen asimismo casos de ludopatía y de ciberadicción. También se argumenta un posible fomento de la violencia, gráficamente presente en muchos videojuegos. Por otro lado, se ha comprobado que la rapidez con que se mueven los gráficos puede provocar ataques en las personas que padecen diversos tipos de epilepsia.

Estudios científicos demuestran que, en general, los videojuegos enriquecen la vida del jugador, le enseñan a resolver problemas técnicos, y estimulan sus habilidades neuro-cinéticas, reflejos visuales y enfoque de múltiples puntos de visión (objetivos). Incluso mejoran la comunicación cuando se juega en familia o en línea.

Diferentes estudios con niños y adolescentes (Castells y Bofarull, 2002; Bringas, Rodríguez y Herrero, 2008) demuestran que el rendimiento escolar puede verse afectado debido al uso de los videojuegos. Ahora bien, existen diferentes artículos que establecen que niveles moderados de juego no se asocian con un bajo rendimiento escolar (Ferguson, 2011); incluso, podrían relacionarse con un mejor rendimiento (Llorca, Bueno, Villar y Díez, 2010). Esto se debe a que los jugadores que utilizan videojuegos adquieren mejores estrategias de conocimiento, estrategias de resolución de problemas, y sus capacidades espaciales, su precisión y capacidad de reacción se ven mejoradas gradualmente (McFarlane, 2002).

En España, la imposibilidad de registrar el videojuego como tal, obedece a que no está reconocido jurídicamente por no estar contemplado en el marco del artículo 145.1 del Real Decreto Legislativo 1/1996, de 12 de abril (en adelante, Ley de Propiedad Intelectual o LPI), como creación intelectual original y unitaria, obliga a sus creadores y titulares a desistir a registrarlo como videojuego, al tener que pasar por separar cada obra de acuerdo a su propia naturaleza, artística, científica o literaria, lo que se evidencia como un obstáculo ante la falta de una regulación específica que permita a la industria proteger, exportar e internacionalizar estos activos intangibles en constante crecimiento socioeconómico, bajo un marco legal estable en el ámbito de la propiedad intelectual de sus creadores.

En el año 2009, la «Proposición No de Ley (PNL)» del Congreso de los Diputados, fue aprobada por unanimidad con dos enmiendas del Partido Popular y CiU: «La Comisión de Cultura del Congreso de los Diputados establece que el videojuego constituye un ámbito fundamental de la creación y la industria cultural de España. En consecuencia, insta al Gobierno a reconocer a sus creadores y emprendedores como protagonistas de nuestra cultura. Asimismo, en el marco de sus competencias y en coordinación con las administraciones autonómicas competentes, se insta al Gobierno a facilitar su acceso a todas las ayudas factibles para la promoción de su actividad, la financiación como industria cultural y la internacionalización de sus iniciativas».

Lo más relevante de la PNL, es que se acuerda por unanimidad que «no debe ser válido contemplarlos como obra audiovisual y, por otro, tampoco se les debe asimilar simplemente a creadores de software».




</doc>
<doc id="2934" url="https://es.wikipedia.org/wiki?curid=2934" title="Vallenato">
Vallenato

El vallenato o música vallenata es un género musical autóctono de la Región Caribe de Colombia con su origen en la antigua provincia de Padilla (actuales sur de La Guajira, norte del Cesar y oriente del Magdalena). Tiene notable influencia de la inmigración europea, ya que el acordeón fue traído por pobladores alemanes a Riohacha, La Guajira, a finales del siglo XIX, y tanto la organización estrófica como la métrica se valen de la tradición española; por otra parte, el componente de los esclavos afrocolombianos hace presencia con la caja vallenata, una especie de tambor que en gran medida le da el ritmo a la melodía del acordeón, y por último lo indígena se evidencia con la guacharaca. Su popularidad se ha extendido hoy a todas las regiones de Colombia, a países vecinos como Ecuador, Panamá y Venezuela, y a más alejados como Argentina, México y Paraguay. Se interpreta tradicionalmente con tres instrumentos: el acordeón diatónico, la guacharaca y la caja vallenata. Los ritmos o aires musicales del vallenato son el paseo, el merengue, la puya, el son y la tambora. El vallenato también se interpreta con guitarra y con la instrumentación de la cumbia en cumbiambas y grupos de millo. 
El 29 de noviembre de 2013, el vallenato tradicional fue declarado Patrimonio Cultural Inmaterial de la Nación por el Consejo Nacional de Patrimonio del Ministerio de Cultura. El 1 de diciembre de 2015 fue incluido en la lista de Patrimonio Cultural Inmaterial de la Humanidad, en la lista de salvaguardia urgente por la Unesco.

No se sabe con exactitud de dónde proviene la palabra "vallenato", a pesar de las muchas hipótesis que han sido expuestas. Sin embargo, a principios del siglo XX, tenía una connotación despectiva y a los propios habitantes de Valledupar no les gustaba. Por tal motivo, en 1915 don Miguel Vence, educador de primaria, fundó una Academia de la Lengua de Valledupar, la cual sesionó una sola vez y determinó que el gentilicio de los nacidos en Valledupar fuera "valduparense".

Generalmente se define al vallenato como un género musical de la Costa Caribe colombiana, más precisamente del área de influencia de Valledupar, capital del departamento de Cesar. Se sostiene que el nombre proviene del gentilicio popular de los nacidos en la ciudad donde tiene mayor arraigo este género. Según algunos, se trata de un neologismo que nació con los nativos viajeros en mulas, que cuando se les preguntaba en otras tierras de dónde eran, en su decir campesino respondían "Soy nato del Valle", que es como decir "Soy del Valle nato".

No obstante que el término "vallenato" puede referirse a los nacidos o a las cosas que se originan en Valledupar (Valle de Upar, el valle de Eupari, cacique indígena legendario de la región), existen otras versiones de la denominación: según Barrameda Morán, el vocablo "ballenato" pasó a designar a todas las personas que padecieran la contaminación sanguínea producida por el jején, fueran oriundos o no de Valledupar y dice: "La tendencia popular a confundir V con B en su pronunciación, terminó por generar el nuevo vocablo: Vallenato".

De manera similar, otra versión con poco fundamento sostiene que en las áreas rurales de los bancos del río Cesar, muchos de los habitantes extremadamente pobres sufrieron de una enfermedad producida por un mosquito que les dejó la piel seca y escamosa, con parches descoloridos. La gente asoció la enfermedad con las ballenas recién nacidas (ballenatos), también llamadas "pintaos", que tienen un color manchado de blanco y rosado, parecido a la enfermedad dérmica llamada "carate" o "jovero", por lo cual se identificaba a quienes la padecían como "caratejos" o "ballenatos". De tal forma que "vallenatos" llegó a ser un nombre para menospreciar a la gente pobre del río.

Las melodías de estos cantos se interpretaron primero con la flauta de caña de millo o carrizo, abierta en sus dos extremos con cuatro orificios en su longitud y una lengüeta que forma la embocadura y pisa un hilo, sostenido por los dientes, para modular el sonido; a ella se sumaron la caja, tambor pequeño hecho artesanalmente del tronco hueco de los árboles secos y sellado en uno de sus extremos con un pedazo de cuero templado, y la guacharaca, instrumento ancestral indígena que se fabrica utilizando un pedazo de cañabrava a la que se le hacen pequeñas ranuras sucesivas para producir un sonido raspativo al ser frotadas con un hueso (originalmente).

A finales del siglo XIX, décadas después de su invención, el acordeón llegó a Colombia por el puerto de Riohacha; los vaqueros y campesinos lo incorporaron a sus expresiones musicales, y paulatinamente fue sustituyendo al carrizo hasta convertirse en el instrumento principal del conjunto típico de música vallenata.

Además de estos tres instrumentos, caja, guacharaca y acordeón, que representan la trietnia que dio origen a la raza y cultura de la Costa Caribe colombiana, el conjunto típico vallenato presenta un cuarto elemento básico que es el cantante, de más o menos reciente incorporación a raíz de los festivales vallenatos, ya que hasta los años 1960 la costumbre era que el acordeonero llevaba la voz cantante e interpretara él mismo la letra de las canciones que tocaba.

El vallenato también se interpreta con guitarra y con los instrumentos de la cumbia en cumbiambas y grupos de millo.

El vallenato o la música vallenata hace parte de la música folclórica de la Costa Caribe colombiana. Es el ritmo musical colombiano que ha alcanzado más popularidad, tanto a nivel nacional como internacional.

Lo que hace característico al vallenato tradicional es ser interpretado sólo con tres instrumentos que no requieren de amplificación alguna: dos de percusión (la caja y la guacharaca), que marcan el ritmo, y el acordeón diatónico (de origen europeo) con el que se interpreta la melodía. No obstante, en algunas ocasiones las canciones se componen o interpretan con otros instrumentos: la guitarra, la flauta, la gaita, el acordeón cromático y la armónica. Por otra parte, para el vallenato comercial es común no sólo la incorporación de estos instrumentos, sino también del bajo eléctrico y otros de percusión, como las congas y los timbales.

La importancia que adquirió el vallenato en las últimas décadas del siglo XX llevó a la organización de festivales en los que los acordeoneros compiten por el honor de ser declarado el más hábil ejecutor de cada uno de los aires tradicionales (a excepción, inexplicablemente, de la tambora). El más célebre de estos festivales es el Festival de la Leyenda Vallenata, que se celebra anualmente a fines de abril en Valledupar, y cuya primera versión se disputó en 1968. Desde 1987, el Festival Cuna de Acordeones de Villanueva, Guajira, se ha convertido en el segundo de mayor importancia.

En el vallenato el modo de uso del acordeón diatónico requiere usar simultáneamente ambos lados del acordeón. Lo anterior caracteriza al acordeonero colombiano y diferencia al vallenato de los otros géneros musicales con acordeón, donde generalmente se suprime o subutiliza la parte de los bajos (ejecutados con la mano izquierda): en Colombia, la forma armónica y rítmica con que el acordeonero maneja los bajos es un factor relevante de calificación en los festivales vallenatos.

El vallenato nace en una vasta región enmarcada por los ríos Magdalena, Cesar y Ranchería, el mar Caribe, la Sierra Nevada de Santa Marta y las estribaciones de la serranía del Perijá, hace más de doscientos años.

Los cantos de vaquería con que los peones de las grandes haciendas acompañaban sus jornadas vespertinas para recoger y encerrar el ganado, fueron la base de lo que más tarde se convertiría en las historias cantadas que derivaron en las canciones vallenatas.

Los primeros acordeoneros de que se tiene memoria fueron a la vez autores de los cantos que interpretaban; cantos que ya tenían una clara diferencia rítmica y una estructura musical propia que les valieron ser clasificados como "paseos", "merengues", "puyas", "tamboras" y "sones". Entonces no había, como hoy, una persona especializada únicamente en componer el canto, otra en ejecutar la melodía en el acordeón y una tercera que los cantara. El acordeonero era un músico integral que con igual destreza hacía sonar el acordeón como interpretaba cantos de su propia inspiración o, en ocasiones, de un tercero. Y hechos los primeros cantos, los acordeoneros se convirtieron en correos cantados, en periodistas musicales, juglares, que iban de pueblo en pueblo y de vereda en vereda llevando la información de los últimos sucesos narrados en los merengues, paseos, puyas, sones y tamboras que cantaban cuando se reunían a descansar y, en ocasiones especiales, a bailar en cumbiambas que se formaban con motivo de las fiestas patronales, entre otras ocasiones.

En relación con los cantares de vaquería como uno de los orígenes del vallenato, el investigador cultural y musical Ciro Quiroz anota sobre la cumbia:

En cuanto al sitio de origen del vallenato, Quiroz anota:

Sobre la transición de pitos y flautas a los instrumentos actuales del vallenato, el mismo autor dice sobre la primitiva denominación de los aires:

A diferencia de todos los demás aires de este folclor, el paseo vallenato tiene una cuadratura de compás de cuatro tiempos. La marcación de los bajos es de uno por tres y a veces, de acuerdo con la pieza, de dos por uno. Para los intérpretes es el aire más fácil de tocar. Este ritmo recoge literariamente y de forma espontánea las historias y relatos del pueblo.

Musicalmente hablando, el merengue vallenato tradicional tiene una cuadratura de compás de seis por ocho, un compás derivado, ya que los compases originales son el de cuatro tiempos, el de tres y el de dos.

Al igual que la puya el merengue fue de los primeros ritmos en ser tocados con acordeón e igualmente su auge se dio en los primeros años del presente siglo. Sus mayores exponentes fueron Chico Bolaños, Octavio Mendoza y Chico Sarmiento.

En lo que a las letras se refiere son muy pocas las veces en que son románticas; la misma naturaleza del ritmo se presta muy poco para ello. La gran mayoría de los merengues describen situaciones vividas por el compositor o simplemente son dedicados a un amigo. Los merengues de ahora no tienen temas específicos, casi siempre son compuestos para el jolgorio del pueblo. En el aspecto comercial el merengue, junto con el paseo es el que más se graba y se vende, aunque en los últimos trabajos discográficos, en promedio por cada cuatro paseos hay un merengue.

En Valledupar y demás pueblos del antiguo departamento del Magdalena Grande, el ritmo más antiguo era llamado "puya". Su nombre deriva del verbo puyar, y tiene un compás de seis por ocho. Este ritmo, en su forma indígena, nunca tuvo canto y consistía en la imitación hecha por el carricero –pitero o caña sillero-, en ritmo rápido, del canto de algunos pájaros; se bailaba en hileras, llevando cada persona las dos manos cerradas a la altura del pecho con los dedos apuntando hacia delante y simulando que se puyaba repetidamente a quien danzaba adelante. Posteriormente, a través del tiempo, se fueron fusionando los distintos elementos triétnicos típicos de la cultura costeña y ribereña colombiana, logrando sumarse la puya negroide, género cantado, a la puya indígena, dándose como resultado la puya vallenata con su actual equilibrio entre el canto, la melodía y el ritmo.

La puya y el merengue en su patrón rítmico y armónico son iguales. La diferencia está marcada en su concepción melódica: en el ritmo, en la música y naturalmente en la interpretación que se haga, propia de cada pieza. Así, la puya tiene una marcación en los bajos de dos por dos y, a veces, de dos por uno en ciertos pasajes de la interpretación, aunque no en todas las piezas. La velocidad que se le imprima no supone una diferencia, porque el intérprete la toca a su gusto.

La puya se destaca por ser el aire más rápido, y el que exige más habilidad en el intérprete del acordeón. Se utiliza más comúnmente en las contiendas y competencias de acordeonistas en los festivales vallenatos de Colombia.

El son vallenato tiene una cuadratura de compás de dos por cuatro. Una característica esencial en la ejecución de este aire es la prominente utilización de los bajos del acordeón en la interpretación de cada pieza, tanto que los bajos pueden ser más notorios que la misma melodía emitida por el teclado, principalmente en los acordeoneros de las nuevas generaciones.

El son tiene una marcación en los bajos de uno por uno muy marcada, sobre todo en intérpretes sabaneros o de influencia bajera – viejo Bolívar -; a diferencia de los acordeoneros de la provincia, quienes interpretan el son más fluido, menos marcado, más sutil y le dan una marcación de bajo de uno por dos y de dos por uno, en ocasiones.

Como el paseo, los sones son una especie de crónica en donde la singular narrativa del cantor deja plasmados los acontecimientos de su existencia, particularmente en esta especie se representan dramas nostálgicos que han constituido parte importante en la vida del autor.

La tambora es un ritmo que tiende a desaparecer. Tomó denominación femenina debido al predominio de voces de mujeres cuando estos aires eran solo cantados.

Unas son politemáticas, en las que cada verso expresa una circunstancia diferente a la del otro, pero existe uno que es constante. Algunas tienen la particularidad de intercalar el inmodificable verso fijo cada dos versos, y otras mantienen la unidad de escritura de un tema, pero sin tener en cuenta concordancia y armonía en las frases poéticas.

En general, todas tienen condición satírica, lograda en la descoordinación que resalta más el contraste. Todavía existen algunas puramente instrumentales, interpretadas únicamente con tambores. De ahí su designación.

Ejemplos de tamboras: "La candela viva" (de Alejandro Durán), "Mi compadre se cayó", "La perra".

La tambora tradicional es de conformación triétnica (negro, blanco, indio) y que su entorno geográfico está centrado a orillas del río Grande de la Magdalena en la sub-región denominada Depresión Momposina. Los pueblos del departamento del Cesar que han tenido la tambora como identidad cultural son, entre otros: Tamalameque, La Gloria, Gamarra, Chimichagua, Chiriguaná, El Paso.

Los instrumentos de la tambora tradicional son la tambora, instrumento bimembranófono que se ejecuta con dos "mambacos" o baquetas y el guache, acompañado de palmas. En este ritmo una voz versea (la cantadora o cantador) y un coro de voces responden un estribillo, ya que es un canto responsorial.

La tambora tradicional consta de cuatro aires: la tambora ("La candela viva", "La perra"), la guacherna ("La zaragozana"), el chandé ("Vamos a bailar chandé") y el berroche ("El Negro").

En el municipio de Tamalameque se realiza en el mes de diciembre el Festival Nacional de la Tambora y la Guacherna, donde se dan cita los mejores exponentes de este folclor de resistencia.

Antecedido de una gran polémica en el mundo vallenato, un quinto aire para concurso fue institucionalizado en Villanueva durante la versión 29 del Festival Cuna de Acordeones de 2007. El llamado "quinto aire" fue bautizado como "Romanza Vallenata", en este mismo festival en el año 2006, y fue aceptado como tal con el respaldo de autoridades del vallenato como Rafael Escalona, Francisco Zumaqué, Hernán Urbina Joiro, Rosendo Romero y el expresidente Alfonso López Michelsen.

De esta manera se aceptó que el llamado "paseo" que comercialmente se escucha hoy, lo dejó de ser hace algún tiempo. Así como en su momento del "son" surgió el "paseo", hoy surge un nuevo aire de este. Las romanzas vallenatas, por su carácter lírico o poético, son un canto al amor, al desamor, al perdón y a la mujer, distinto del paseo clásico que se interpreta en los festivales, por eso se decidió darle un espacio en ellos. Además, se tuvo en cuenta que este aire ha sido motor trascendental para la internacionalización del vallenato. Este aire, hijo del paseo, adquirió independencia gracias a su aceptación mundial y después de voces en contrario que no admiten la evolución del género musical. Igualmente no se determina esta denominación como un ritmo oficial vallenato.

La piqueria (de "pique", enfrentamiento) es una competencia usualmente entre dos verseadores improvisadores y repentistas, en la que gana quien produzca mejores versos y se equivoque menos, a juicio de un jurado. Existen las modalidades de versos de cuatro palabras, décima de tema libre y pie forzado. Al momento de elegir al ganador se tienen en cuenta factores como la capacidad para improvisar con agilidad, gracia y exactitud métrica y rítmica versos de cuatro palabras (cuartetas) o de diez (décimas) para desafiar o responder el requerimiento musical de un contrincante en iguales condiciones. A juicio del jurado, el pique puede tener como punto de partida un solo verso de cuatro palabras con un tema determinado, una décima de tema libre o un pie forzado. El jurado puede imponer cualquiera de estas tres modalidades o imponerlas todas si así lo considera.

La vallenatóloga Consuelo Araújo identificó tres escuelas en el vallenato:

Esta clasificación no es aceptada generalmente y ha sido criticada por músicos y juglares. Los músicos sabaneros en particular hablan de música de acordeones o música sabanera de acordeón, la cual fue desarrollada en paralelo a la escuela vallenata, de ejecución distinta, no comparte completamente su organología, y no incluye la puya como ritmo, pero adiciona otros como cumbia, paseaíto, porro y chandé.

De carácter eminentemente folclórico, es ejecutado en los festivales como el de la Leyenda Vallenata, el Festival Cuna de Acordeones, el Festival Francisco el Hombre y el Festival del Río Grande de la Magdalena. Comprende cuatro (4) de los cinco (5) ritmos tradicionales: la puya, el paseo, el son y el merengue. Su temática abarca hechos de la vida cotidiana, la amistad, la parranda, la tierra y el amor. Es la música que cultivaron los juglares
como Juancho Polo Valencia, Alejandro Durán, Abel Antonio Villa, Luis Enrique Martínez, "Toño" Salas, Lorenzo Morales, Leandro Díaz, "Pacho" Rada, "Colacho" Mendoza, Rafael Escalona, Emiliano Zuleta, entre otros.

Es una de las primeras corrientes vallenatas, conocida como "vallenato yuca" en los años 1980. Se empezó a escuchar en las emisoras comerciales a principios de los años 1970. Sus principales representantes son Otto Serge y Rafael Ricardo, Jorge Oñate, Los Hermanos Zuleta, Diomedes Díaz, el Binomio de Oro (con Rafael Orozco), Los Betos, Iván Villazón, Daniel Celedón, entre otros. Predomina el paseo y, en menor proporción, el merengue.

Estilo influido por otros ritmos como la balada, impulsado por Iván Calderón a finales de los años 80´s y comienzos de los 90´s, se basa principalmente en el paseo y últimamente en la denominada "romanza vallenata". Su principal característica radica en la letra, donde exclusivamente se le canta al amor. Sus temas incluyen amores, despechos, distanciamientos, reconciliaciones. Es el subgénero más escuchado y aceptado en el interior de Colombia y en el extranjero (Monterrey y Saltillo en México, Venezuela, Panamá, Ecuador, Paraguay, norte de Argentina y las colonias de colombianos en el exterior), pero a la vez goza de muy poca popularidad en la Costa Caribe. Algunos de sus representantes más importantes son: Binomio de Oro de América, Las Estrellas Vallenatas, Duo Sensacional, Patricia Teherán, Las Musas, Adriana Lucía (en su época de interprete vallenata), Los Diablitos, Los Gigantes, Los Inquietos, Los Chiches, Los Embajadores, Miguel Morales, Jesús Manuel y Alex Manga (los tres ex-Diablitos), Fabián Corrales, Luis Mateus, Nelson Velásquez (ex-Inquietos), Jean Carlo Centeno, Junior Santiago y Jorge Celedón (los tres ex-Binomio de Oro de América), Amín Martínez (ex-Chiches), Luis Miguel Fuentes y Heberth Vargas (ambos ex-Gigantes), entre otros.

Una nueva corriente vallenata que ha empezado a ser aceptada por el público colombiano desde comienzos de los años 2000, impulsada por Kaleth Morales, hijo del cantautor vallenato Miguel Morales, el cual combina elementos y arreglos de corte carnavalesco y electrónico; con instrumentos de percusión y de viento de otras zonas de la región Caribe (bombardino, redoblante, tamboras, tuba, etc.). Sus artistas más destacados son Silvestre Dangond, Peter Manjarres, Martín Elías (hijo de Diomedez Díaz), Luifer Cuello, Penchy Castro, Kvrass, Mono Zabaleta, Churo Díaz, Elder Díaz (hijo de Diomedes Díaz), Daniel Calderón, Felipe Peláez, La Gente de Omar Geles, Los K Morales (sucesores del legado de Kaleth Morales), Orlando Liñan, Kbto Zuleta, Cayito Dangond (hermano de Silvestre Dangond), Los Comandantes del Vallenato, entre otros.

Los verdaderos juglares vallenatos se han perdido entre la historia y la leyenda. Entre ellos se encuentran desde la figura legendaria de Francisco el Hombre, pasando por Pedro Nolasco Martínez, Emiliano Zuleta, Guillermo Buitrago, Lorenzo Morales, Leandro Díaz, Luis Enrique Martínez, Tobías Enrique Pumarejo, Juancho Polo Valencia, Abel Antonio Villa, Rafael Escalona, y el que ha sido el más grande icono del folclor vallenato, el primer Rey Vallenato, Alejandro Durán. Muchos de ellos murieron en la pobreza a pesar de que sus cantos se escuchaban en toda América Latina y de que dieron fisonomía al vallenato mucho antes de que se convirtiera en un fenómeno de ventas.

A pesar de existir compositores e intérpretes del vallenato tradicional de gran popularidad en Colombia, el máximo "embajador" de esta música en el mundo es el cantante samario Carlos Vives, que lo ha dado a conocer a través de una variante que se podría denominar vallenato-pop, también conocido como vallenato alternativo. Hoy por hoy, se hace una diferenciación entre el vallenato tradicional y un vallenato más comercial, en el que se han destacado cantantes como Silvestre Dangond, Jorge Celedón e Iván Villazón y agrupaciones como el El Binomio de Oro de América. Otros intérpretes como Diomedes Díaz lograron que el vallenato ganara popularidad entre los colombianos, sin distinción social ni cultural. En Venezuela, Rafael Orozco es considerado un ídolo musical, aún muchos años después de su muerte; tal fue el cariño expresado por el público venezolano, que uno de sus mayores éxitos está dedicado a este país: "Recorriendo a Venezuela".

El festival de música vallenata más importante de Colombia es el Festival de la Leyenda Vallenata, que se celebra desde 1968 en Valledupar, Cesar. En él se premia al mejor ejecutante del acordeón con el título de Rey Vallenato. El ganador del primer festival fue Alejandro Durán, quien derrotó en la tarima "Francisco el Hombre" a Emiliano Zuleta. Cabe resaltar que desde 1987 se realiza, cada 10 años, el torneo "Rey de Reyes", donde participan únicamente quienes han sido coronados como reyes vallenatos en el festival.

El segundo festival en importancia para la música vallenata es el Festival Cuna de Acordeones de Villanueva, Guajira, población fuente de intérpretes del acordeón, que se realiza desde 1979. El Festival Cuna de Acordeones fue nombrado Patrimonio Cultural y Artístico de Colombia por el Congreso Nacional mediante la Ley 1052 de 2006. Al igual que el festival de Valledupar, el Festival Cuna de Acordeones corona desde 1993 y cada 10 años al "Rey de Reyes". 

El tercero en importancia es el Festival del Río Grande de la Magdalena, que se realiza en el municipio santandereano de Barrancabermeja desde el año 1983, el cual también fue declarado Patrimonio Cultural de la Nación mediante la Ley 1007 de 2006 por el Congreso de la República. Igual que en Valledupar y Villanueva, este festival corona su "Rey de Reyes" desde 1995 cada 10 años.

En Riohacha se realiza desde el año 2009 el Festival Francisco El Hombre.

Fuera de Colombia, se han empezado a realizar festivales vallenatos como el Festival Internacional Vallenato de Monterrey (México) que se realiza desde 2007, logrando coronar hasta el 2015 nueve reyes vallenatos, siempre teniendo como invitados especiales intérpretes y compositores de Colombia como Sergio Moya Molina, Adolfo Pacheco, Fernando Meneses, Isaac Carrillo "Tijito", Los Hermanos Lora, Roy Rodríguez, Alberto Rada, El Dúo Sensacional, Jorge Luis Ortiz, entre otros. En el año 2016, este festival coronó su primer "Rey de Reyes", buscando seguir la tradición de los grandes festivales vallenatos de Colombia. Otro festival importante que se realiza en la ciudad regiomontana es el Festival Voz de Acordeones, de mayor tradición (se realiza desde 1998) e importancia para los acordeoneros mexicanos, ya que el ganador del festival viaja a Valledupar representando al país azteca en el Festival de la Leyenda Vallenata del año siguiente.




</doc>
<doc id="2935" url="https://es.wikipedia.org/wiki?curid=2935" title="Vertebrata">
Vertebrata

Los vertebrados (Vertebrata) son un subfilo muy diverso de cordados que comprende a los animales con espina dorsal o columna vertebral, compuesta de vértebras. Incluye casi 62 000 especies actuales y muchos fósiles.

Los vertebrados han logrado adaptarse a diferentes ambientes, incluidos los más difíciles e inhóspitos. Aunque proceden inicialmente del medio dulceacuícola, han conseguido evolucionar en el mar y pasar posteriormente al medio terrestre.

El término «vertebrata», usado en sentido amplio, es sinónimo de "Craniata", e incluye a los mixinos, que no poseen auténticas vértebras; si se usa Vertebrata en sentido estricto (solo los cordados con vértebras), debe excluirse dicho grupo. Sin embargo, hay nuevas evidencia que postula que los mixinos sí deberían ser incluidos.

Los vertebrados tienen simetría bilateral y están provistos de un cráneo que protege el cerebro, y esqueleto cartilaginoso u óseo, que comprende una parte axial metamerizada (columna vertebral). Según los autores, se conocen entre 50 000 y casi 62 000 especies actuales.

Los vertebrados típicos tienen el cuerpo dividido en tres regiones: cabeza, tronco y cola; el tronco está a su vez subdivido en tórax y abdomen. Del tronco sobresalen las extremidades, que son impares en las lampreas y pares en el resto de vertebrados. Presentan notocordio en la fase de embrión, que es sustituido por la columna vertebral en estado adulto; la cabeza está bien diferenciada, y en ella se agrupan y centralizan la mayoría de órganos sensoriales y nerviosos. La estructura craneal de los vertebrados fosiliza con facilidad. lo cual ha sido fundamental para conocer su evolución.

Durante el desarrollo embrionario, las paredes del cuerpo de los vertebrados desarrollan unos orificios o hendiduras branquiales, que dan lugar a las branquias (en los peces) y a diferentes estructuras. El esqueleto puede ser óseo, cartilaginoso, y en ocasiones presentar dermoesqueleto, consistente en unas formaciones cutáneas esqueléticas.

El tegumento adquiere notable importancia en los vertebrados por los múltiples papeles que desempeña, y puede presentar variadas diferenciaciones córneas. En el tegumento se distinguen formaciones de estructuras protectoras y sensoriales, glándulas con funciones excretoras, aislamiento del medio, etc. Consta de tres capas: epidermis, dermis e hipodermis. Por su parte, la coloración del tegumento es debida sobre todo a los cromatóforos o células pigmentarias ramificadas de la piel.

La piel origina dos formaciones importantes, epidérmicas y dérmicas:


El aparato locomotor de los vertebrados se ha adaptado de su función inicial (la natación), a otras acciones múltiples que permiten movimientos complejos según las condiciones registradas por los órganos sensitivos.

Los peces, habitantes del medio primigenio, sufrieron cambios evolutivos importantes a partir de la aparición de las aletas pares, que posteriormente se convirtieron en quiridios o extremidades locomotoras pentadáctilas (de cinco dedos) cuando comenzaron la conquista del medio terrestre, y que sufrirían posteriormente adaptaciones específicas, tales como las manos prensoras de los primates, las manos desgarradoras de los felinos, o las alas de sustentación aérea de las aves.

En los vertebrados el aparato circulatorio es cerrado, mediante el cual se transporta oxígeno y nutrientes a los distintos tejidos y células (presentan glóbulos rojos que transportan el oxígeno mediante la hemoglobina). Consta de un sistema sanguíneo y sistema linfático. Está dotado de un corazón dividido en cámaras, arterias, arteriolas, venas, vénulas y capilares. En los peces hay un circuito sistémico y otro branquial. En muchos vertebrados terrestres el sistema sanguíneo es doble (circulación mayor o general, y circulación menor o pulmonar), es decir no se mezclan la sangre arterial y venosa. El corazón de los peces presenta dos cámaras, una aurícula y un ventrículo (dos aurículas y un ventrículo en los anfibios y reptiles). En las aves y mamíferos es tetracameral (dos aurículas y dos ventrículos), y con una serie de válvulas cardíacas. En los vertebrados existe además un sistema linfático, encargado de recoger el líquido intersticial.

El aparato respiratorio de los vertebrados es branquial en los animales acuáticos (ciclóstomos, peces y larvas de anfibios), y pulmonar en los terrestres, parte de los acuáticos y también los anfibios que tienen dos tipos de respiración: la pulmonar y a través de la piel.

Las branquias son un órgano o apéndice filiforme (en forma de laminillas vascularizadas), externa o interna según se disponga en el cuerpo. Tienen una función respiratoria, y están especializadas para el intercambio gaseoso en el medio acuático. Todas las branquias presentan en común una amplia superficie de contacto con el medio, y en ellas la irrigación sanguínea se encuentra mucho más desarrollada que en otras partes del cuerpo.

En las aves, el aparato respiratorio es sumamente eficaz; proporciona el oxígeno necesario para generar la energía que el cuerpo demanda por el esfuerzo desarrollado durante el vuelo. Consta de un sistema de bronquios que están conectados a unos sacos aéreos; los pulmones están divididos en alvéolos y lobulillos.

El sistema nervioso de los vertebrados comprende el sistema nervioso central, que a su vez consta de encéfalo y médula espinal; y el sistema nervioso periférico, que consta de numerosos ganglios y nervios (raquídeos o espinales); existe además un sistema nervioso autónomo que inerva las vísceras (sistema simpático y parasimpático). Los órganos sensitivos, así como las funciones motoras, son muy perfeccionados y desarrollados. Los nervios raquídeos se ramifican a diferentes niveles de la médula, e inervan los distintos músculos, glándulas y órganos. En el caso de los tetrápodos, aparecen dos engrosamientos en la médula, las intumescencias cervicales y lumbar, como consecuencia del desarrollo de las patas.

Los sentidos incluyen: ojos, dispuestos en cámara de visión lateral, salvo en algunas aves y mamíferos primates, que es binocular; tangorreceptores, que incluyen los órganos táctiles de los mamíferos y la línea lateral (captadoras de ondas de presión) de los ciclóstomos, peces y algunos anfibios acuáticos; órganos auditivos, en los tetrápodos consta de oído interno y oído medio, ventanas oval y redonda, membrana timpánica y huesecillos, los cuales transmite la vibración del tímpano a la cóclea o caracol. El oído medio comunica con la faringe a través de la trompa de Eustaquio; los mamíferos disponen además de un oído externo. En los peces solo hay oído interno.

El sistema endocrino de los vertebrados está muy perfeccionado; mediante las hormonas regula múltiples funciones del organismo. Está controlado por el hipotálamo y la hipófisis, que mediante la elaboración de mensajes bioquímicos ejercen su acción sobre las gónadas, páncreas, glándulas suprarrenales, etc.

El aparato digestivo de los vertebrados evolucionó a partir de las primeras formas que se alimentaban mediante sistemas filtradores, hasta los vertebrados macrofágicos, que supuso una serie de adaptaciones de los diferentes elementos intervinientes: dentales, masticadores, musculares, e incluso de las propias cavidades internas, tales como los componentes enzimáticos necesarios para realizar la digestión.

El aparato digestivo de los vertebrados consiste en una cavidad oral, faringe, esófago, estómago, intestino y ano. Estos órganos están asociados a otras formaciones glandulares anexas, tales como las salivales, hígado y páncreas. En los tetrápodos, la cavidad bucal es de complejidad creciente; en ella se desarrollan un conjunto de estructuras auxiliares, tales como labios, lengua, paladar y dientes.

El estómago está típicamente dividido en tres regiones; en el caso de los rumiantes (por su adaptación a dietas herbívoras) presentan un estómago de cuatro cavidades. En las aves se distingue un proventrículo y una molleja trituradora; y en el esófago un divertículo o buche.

El intestino está compuesto de una porción estrecha (el intestino delgado), y otras más corta y ancha (el intestino grueso). En el primero se vierten la bilis del hígado y el jugo pancreático, que realizan una función proteolítica (hidrólisis de las proteínas), y se absorben los nutrientes a través de las microvellosidades. En el intestino grueso se absorbe el agua y se forman los desechos o heces.

Inicialmente, los vertebrados primitivos se alimentaban mediante sistemas de filtración, los cuales pronto fueron reemplazados por otros más evolucionados. El resultado fue una reducción del tamaño de la faringe y del número de hendiduras branquiales. Excepto en los agnatos, que son los vertebrados más primitivos, los dos primeros arcos branquiales del resto de vertebrados evolucionaron hasta transformarse en las mandíbulas, que se han especializado en la "captura" del alimento. Su aparato digestivo es completo.

El aparato excretor de los vertebrados está formado por el aparato renal y las glándulas sudoríparas. Está muy perfeccionado en comparación con los cordados inferiores. Mediante estructuras especializadas se consigue filtrar los líquidos internos al margen del medio externo, a la vez que mantiene en equilibrio el nivel de todos ellos dentro del cuerpo.

La reproducción de los vertebrados es sexual salvo excepciones (ejemplo de algunos peces con casos de hermafroditismo), habitualmente mediante géneros separados, con fecundación interna o externa, y tanto vivíparos como ovíparos. Los mamíferos presentan la mayor complejidad, en los cuales el embrión se desarrolla en el interior de la madre recibiendo el alimento a través de la placenta. Después de nacidas las crías la administración del alimento se efectúa mediante la leche segregada por las glándulas mamarias.

Los vertebrados se originaron durante la explosión cámbrica, a principios del Paleozoico, junto con otros muchos grupos de animales. El vertebrado más antiguo que se conoce es "Haikouichthys", con una antigüedad de 525 millones de años. Se asemajaban a los mixinos actuales; ya que igualmente carecían de mandíbulas (agnato), y tanto su cráneo como su esqueleto eran cartilaginosos. Otro vertebrado ancestral es "Myllokunmingia". Ambos proceden de Chengjiang (China).

Los primeros peces con mandíbulas (gnatóstomos) aparecieron en el Ordovícico, y se hicieron abundantes durante el Devónico, que por ello se denomina a veces la "edad de los peces"; durante este periodo desaparecieron muchos de los agnatos ancestrales y aparecieron los laberintodontos, formas transicionales entre peces y anfibios.

Los primeros reptiles hicieron su aparición en el siguiente periodo, el Carbonífero. Los reptiles anápsidos y sinápsidos abundaron durante el Pérmico, en el tramo final del Paleozoico, mientras que los diápsidos fueron los vertebrados dominantes durante el Mesozoico. Los dinosaurios originaron a las aves en el Jurásico. La extinción de los dinosaurios al final del Cretácico propició la expansión de los mamíferos, que se habían originado hacía ya mucho tiempo a partir de reptiles sinápsidos, pero que habían permanecido en un segundo plano durante el Mesozoico.

Los vertebrados se han venido clasificando durante décadas en diez clases vivientes, agrupadas de la siguiente manera:

Subfilo Vertebrata

Intensos estudios basados en métodos cladísticos, sobre todo a partir de la década de los 80, han producido una revolución en la clasificación de los vertebrados. El debate sigue abierto y las clasificaciones que sigan no deben considerarse definitivas. Las relaciones filogenéticas que se presentan a continuación se basan en los estudios compilados por Hickman 2006 y Swartz 2012, el cladograma podría ser:

Nótese que diversos taxones de la clasificación linneana tradicional son parafiléticos: Agnatha (Myxini + Cephalaspidomorphi), Osteichthyes (Actinopterygii + Sarcopterygii), Reptilia (Testudines + Lepidosauria + Crocodilia), y que las aves son un clado más dentro de los "reptiles"; si se tienen en cuenta las formas fósiles, grupos como los anfibios aparecen también como parafiléticos. Los "reptiles", en sentido cladista, incluyen las aves y no incluye a los "reptiles" que condujeron a los mamíferos (Sinápsidos).




</doc>
<doc id="2936" url="https://es.wikipedia.org/wiki?curid=2936" title="Vértebra">
Vértebra

Se denomina vértebra a cada uno de los huesos que conforman la columna vertebral. En los seres humanos hay 33 vértebras durante la etapa fetal y en la niñez (7 cervicales + 12 torácicas + 5 lumbares + 5 sacras + 4 del cóccix). Cada una de ellas se encuentra separada de la inmediata inferior por medio de un disco vertebral, exceptuando las 5 vértebras del sacro y las 4 del cóccix, debido a su unión. 

Las vértebras se alinean entre sí por los llamados cuerpos vertebrales y por sus apófisis articulares. Entre una vértebra y otra existen núcleos de tejido conectivo laxo que se denominan discos intervertebrales.

Con excepción de la primera y segunda vértebra cervical, las llamadas vértebras verdaderas o movibles (pertenecientes a las citadas tres regiones superiores) presentan ciertos rasgos comunes que son mejor reconocidos examinando una vértebra de en medio de la región torácica.

Excepto la primera y la segunda vértebras cervicales, que tienen una configuración algo especial, el resto de las vértebras muestra una estructura similar: un cuerpo, dos láminas vertebrales, dos pedículos, una apófisis espinosa, dos apófisis transversas y cuatro apófisis articulares.

Son generalmente pequeñas y delicadas. Sus apófisis espinosas son pequeñas y bífidas (la C7 es la primera vértebra, cuya apófisis espinosa puede ser palpada). Se las puede diferenciar por tener un agujero en la base de las apófisis transversas (agujero para la arteria vertebral). Numeradas de arriba abajo como C1 hasta C7, son las vértebras que permiten la rotación del cuello. Específicamente el atlas (C1) permite al cráneo subir y bajar, y el axis (C2) es el responsable de que la parte superior del cuello gire de izquierda a derecha, luego está la vértebra de rixi (C3) que es la vértebra patrón, a partir de ella todas las vértebras son prácticamente iguales. La vértebra C6 posee lo que se conoce como "Tubérculo de Chassaignac". La vértebra C7 se conoce como "Vértebra Prominente". Además, poseen un canal raquídeo muy ancho, porque coincide con el comienzo de la médula espinal.
Los discos intervertebrales de la región cervical crean lo que se llama la lordosis cervical (curvatura cóncava dorsal) de la columna.

Sus procesos espinosos apuntan hacia abajo en forma casi vertical, y son más pequeñas en relación con las de las otras regiones. Poseen en sus caras laterales unas facetas articulares (fositas costales), que articulan con la cabeza de las costillas, y otra carilla articular en sus procesos transversos destinadas a articular con el tubérculo costal. Tienen un pequeño grado de rotación entre ellas pero, al estar articuladas con la caja torácica, se vuelven casi inmóviles. Los discos intervertebrales de la región torácica crean lo que se llama la cifosis torácica (curvatura convexa dorsal) de la columna.

Son vértebras mucho más robustas que las anteriores ya que han de soportar pesos mayores. Tiene un agujero vertebral de forma triangular, sus apófisis son largas y delgadas. Permiten una considerable flexión y extensión, una moderada flexión lateral y un pequeño grado de rotación (5º). Los discos intervertebrales de la región lumbar crean lo que se llama la lordosis lumbar (curvatura cóncava dorsal) de la columna. Además, su apófisis espinosa es cuadrilátera y se presenta casi horizontalmente.

Durante los cuatro primeros meses de desarrollo embrionario el esclerotomo cambia su posición para rodear la médula espinal y el notocordio. El esclerotomo es formado a partir del mesodermo
y se origina en la parte ventromedial de los somitas. Esta columna de tejido tiene una apariencia segmentada, con partes alternadadas de áreas densas y menos densas.
A medida que el esclerotomo se desarrolla, se condensa más y se empieza a conformar lo que se llama el cuerpo vertebral. El desarrollo de las formas adecuadas de los cuerpos vertebrales está regulado por los genes de tipo HOX. La parte menos densa que se separa del esclerotomo durante el desarrollo acaba convirtiéndose en los discos intervertebrales
El notocordio desaparece en los segmentos del esclorotomo (cuerpo vertebral), pero persistirá en la región de los discos intervertebrales como el núcleo pulposo. El núcleo pulposo y las fibras del llamado annulus fibrosus (anillo fibroso) conformaran el disco intervertebral
Las curvas primarias de la columnas (torácica y sacral) se conforman durante el desarrollo fetal. Las curvas secundarias se forman después del nacimiento. La curvatura cervical se forma como resultado de la elevación de la cabeza y la curvatura lumbar como resultado del proceso natural de caminar.
Existen varias patologías asociadas al desarrollo vertebral. La escoliosis puede ser el resultado de una fusión anómala de las vértebras. En el síndrome de klippel-feil, los pacientes tienen menos vértebras de las normales junto con otros defectos de nacimiento. Un defecto serio durante la gestación puede ser el cierre incompleto del arco vertebral que da lugar a la llamada espina bífida. Hay diferentes tipos de espina bífida que son un reflejo de la mayor o menor gravedad del problema.



</doc>
<doc id="2937" url="https://es.wikipedia.org/wiki?curid=2937" title="Ciudad del Vaticano">
Ciudad del Vaticano

La Ciudad del Vaticano, oficialmente Estado de la Ciudad del Vaticano (en latín: "Status Civitatis Vaticanæ"; en italiano: "Stato della Città del Vaticano"), o simplemente el Vaticano, es un país soberano sin salida al mar, cuyo territorio consta de un enclavedentro de la ciudad de Roma, en la península Itálica. Es uno de los seis microestados europeos.

La Ciudad del Vaticano tiene una extensión de 0,44 km² (44 hectáreas) y una población de aproximadamente 800 habitantes, por lo que resulta un híbrido de ciudad elevada al rango de Estado independiente, siendo además el país más pequeño del mundo. Es tan pequeño que solo la basílica de San Pedro es un 7 % de su superficie; la basílica y la plaza de San Pedro ocupan un 20 % del territorio, lo que lo convierte en el territorio independiente más urbanizado del mundo. La Ciudad del Vaticano comenzó su existencia como Estado independiente en 1929 tras la firma de los Pactos de Letrán celebrados entre la Santa Sede y el entonces Reino de Italia, que en 1870 había conquistado los Estados Pontificios.

La Ciudad del Vaticano alberga la Santa Sede, máxima institución de la Iglesia católica. Aunque los dos nombres, «Ciudad del Vaticano» y «Santa Sede», se utilizan a menudo como si fueran equivalentes, el primero se refiere a la ciudad y a su territorio, mientras que el segundo se refiere a la institución que dirige la Iglesia y que tiene personalidad jurídica propia como sujeto de Derecho internacional. En rigor, es la Santa Sede, y no el Estado del Vaticano, la que mantiene relaciones diplomáticas con los demás países del mundo. Por otro lado, el Vaticano es quien da el soporte temporal y soberano (sustrato territorial) para la actividad de la Santa Sede.

La máxima autoridad del Vaticano y jefe de Estado del mismo es el papa de la Iglesia católica, por lo que puede considerarse la única teocracia de Europa. El sumo pontífice delega las funciones de gobierno en el secretario de Estado.

El conjunto arquitectónico e histórico-artístico que conforma la Ciudad del Vaticano fue declarado Patrimonio de la Humanidad por la Unesco en 1984.

Su nombre viene del monte Vaticano (probablemente del latín "vaticinĭum": predicción, vaticinio; o "vāticinātio": profecía, vaticinio, pues antiguamente la colina era la sede de un oráculo etrusco o tal vez del nombre de un poblado del mismo origen, Vaticum).

En italiano la denominación completa es "Stato della Città del Vaticano". En latín, idioma oficial de la Santa Sede, se traduce como "Status Civitatis Vaticanæ".


El Estado de la Ciudad del Vaticano nació con el objeto de un ser instrumento de la independencia de la Santa Sede y de la Iglesia católica respecto a cualquier otro poder externo. El papa, que es cabeza suprema de la Iglesia católica, es también soberano de la Ciudad del Vaticano y ostenta la plenitud de los poderes ejecutivo, legislativo y judicial,por lo que se puede considerar a este país como una teocracia en forma de monarquía absoluta.

El papa administra el Estado mediante la Pontificia Comisión para el Estado de la Ciudad del Vaticano, salvo en los casos que entienda reservarse a sí mismo o a otras instancias. Equivale al poder legislativo y está compuesta por cardenales nombrados por el papa para un quinquenio. El papa delega el poder ejecutivo en el presidente de la Comisión, coadyuvado por el secretario general y el vicesecretario general. El presidente de la Comisión tiene también facultad legislativa: puede emitir ordenanzas, y en casos de urgente necesidad puede adoptar disposiciones con carácter de ley, siempre que la Comisión las confirme en los 3 meses siguientes. Asume también la representación diplomática del Estado excepto ante los Estados extranjeros, función que es reservada al papa. Actualmente el presidente de la Pontificia Comisión para el Estado de la Ciudad del Vaticano y de la Gobernación del Estado de la Ciudad del Vaticano es el cardenal Giuseppe Bertello.

El cargo de gobernador del Estado de la Ciudad del Vaticano fue, en una época, unipersonal y ejercido por el marqués y conocido numismático Camillo Serafini, desde 1929, año de la fundación del Estado, hasta la muerte de este en 1952. Ulteriormente, no fue designado sucesor de Serafini, y el cargo propiamente tal tampoco fue mencionado en la Ley Fundamental del Estado, emitida por el papa Juan Pablo II el 26 de noviembre de 2000, y que entró en vigor el 22 de febrero de 2001. El presidente de la Comisión Pontificia para el Estado de la Ciudad del Vaticano ha ejercido desde 1952 las funciones que antes eran atribuidas al gobernador, y desde 2001 también recibe el título de presidente de la Gobernación del Estado de la Ciudad del Vaticano.

Durante el periodo de sede vacante, producido tras la muerte o renuncia del papa, los poderes recaen en el Colegio Cardenalicio, aunque este únicamente podrá dictar leyes en caso de urgencia y con su duración limitada a dicho espacio de tiempo. Será tarea de este colegio de cardenales elegir a un nuevo pontífice en cónclave.

El idioma más hablado es el italiano. La moneda, según un acuerdo suscrito con la Unión Europea (UE), es el euro.

En enero de 2014 eran 180 Estados los que mantenían relaciones diplomáticas con la Santa Sede, reconociendo la existencia del microestado. Entre los países que no tienen relaciones diplomáticas con la Santa Sede se encuentran China, Corea del Norte, Vietnam y Arabia Saudita.

Es el único país del mundo en donde no hay votaciones para elegir cargos de gobierno.

La Guardia Suiza es el cuerpo militar encargado de la seguridad de la Ciudad del Vaticano. Está compuesta por unos 100 soldados (todos varones): cuatro oficiales, 23 mandos intermedios, 70 alabarderos, 2 tamborileros y un capellán. Se les entrena en procedimientos y manejo de armas modernas (como el fusil suizo Sig 550), aunque también se enseña a manejar la espada y la alabarda.

La Guardia Suiza tiene su cuartel frente al Palacio Apostólico Pontificio.
Según el Tratado de Letrán, se ha establecido que la Policía italiana custodie, junto con la Guardia Suiza y los Servicios Vaticanos de Seguridad, la plaza de San Pedro.

La defensa de la Ciudad del Vaticano es proporcionada por Italia.

El Estado de la Ciudad del Vaticano consta de la ciudad vaticana propiamente dicha, cuya extensión aproximada es de unas 44 hectáreas y sobre la que ejerce total soberanía, y de otros edificios y lugares, tanto en la ciudad de Roma como en el resto de Italia, que gozan del derecho de extraterritorialidad. Se encuentra a la orilla derecha del río Tíber y la colina vaticana, lugar donde se establecieron los primeros asentamientos en tiempos del cristianismo primitivo.

Entre ellos, cabe destacar la residencia estival de los papas, el palacio de Castel Gandolfo con sus jardines, cuya extensión ronda las 55 ha, y que dista unos 34 km de la Urbe; las basílicas patriarcales de San Juan de Letrán, Santa María la Mayor y San Pablo Extramuros, varios edificios más en la ciudad de Roma: la Cancillería Apostólica, el palacio de San Calixto en el Trastévere, la Curia General de los Jesuitas, el Vicariato y el palacio de Propaganda Fide, entre otros, así como el Centro Televisivo de Santa María de Galería.

Durante el siglo XV, debido a que la basílica paleocristiana se encontraba bastante deteriorada y amenazaba con poder derrumbarse, el papa Nicolás V encargó su reconstrucción a Bernardo Rosellino en 1452, pero los trabajos se interrumpieron tres años después a la muerte del papa y los muros tan solo alcanzaban a levantarse un metro del suelo.

Es con Julio II en 1506 cuando se reinician las obras acogiendo el diseño propuesto por Bramante, y se terminan con Paulo V en 1626, gracias a las ventas de indulgencias. El nombre de este papa aparece en la fachada de la basílica.

Hubo dos proyectos iniciales, realizados por Bramante y Rafael, respectivamente. El primero es un proyecto de cruz griega y el segundo de cruz latina. Posteriormente, Miguel Ángel retoma el proyecto de cruz griega de Bramante, diseñando también la cúpula de la basílica. El último arquitecto que intervino en la construcción fue Gian Lorenzo Bernini. En la cúpula, con letras de dos metros de alto está escrito "Tu est Petrus, et super hanc petram aedificabo eclessia meam", es decir: "Tú eres Pedro, y sobre esta piedra edificaré mi iglesia". En ese mismo lugar se construyó unos mil años antes otra basílica de tres naves longitudinales, paleocristiana. Esta, en el siglo XV, amenazaba con derrumbarse y fue sustituida.

En la actualidad está permitida su visita, incluida la cúpula, siempre teniendo en cuenta que hay que vestir con recato. Está prohibida la entrada con tirantes y pantalones cortos tanto a hombres como a mujeres.

Desde 1277, está conectada con el Castillo Sant'Angelo por un corredor fortificado, llamado "Passetto", de unos 800 metros de longitud.

En el año 1939, siendo papa Pío XII, y cuando se llevaban a cabo las excavaciones para preparar la tumba de Pío XI, se descubrió un mosaico. Existía una tradición que decía que debajo del altar papal, debajo del baldaquino de Bernini, bajo la cúpula de Miguel Ángel, había una necrópolis, un cementerio, donde había sido enterrado San Pedro, pero de esto todavía no había certeza. Pío XII mandó que siguieran excavando y apareció la necrópolis.

Constantino I el Grande, para agradecer a Cristo que, según él, le había dado la victoria en la batalla de Puente Milvio sobre Majencio, se convierte al cristianismo; en Roma, hay un obelisco en el que se lee: «Aquí fue bautizado Constantino por el papa Silvestre.» Después de ello, Constantino comenzó a construir una serie de templos cristianos; uno de ellos fue la basílica en honor de San Pedro, que según él, estaría edificada sobre la tumba del Apóstol. Hay indicios que llevan a pensar que él estaba seguro de la localización exacta de la tumba: por ejemplo, empieza a edificar su basílica en la ladera de un monte que tiene mucho desnivel (11 metros), lo que hace realizar un gran trabajo de movimiento de tierra para lograr una explanada (sin maquinarias), aunque no tan lejos tenía un sitio que parecía ideal: la explanada del circo de Nerón, que medía trescientos metros de largo y unos cien de ancho. Construyendo en este sitio se hubiera evitado grandes costos y trabajo. Otras dificultades que se deben haber presentado, además de las técnicas, serían las morales y jurídicas, ya que bajo esta construcción quedaba enterrada una necrópolis que era muy importante en Roma y en la que estaban enterrados personajes importantes de aquella sociedad, como los Flavios y los Valerios.

El papa Pío XII anunció por radio en el tiempo de Navidad de 1950 que se había encontrado la tumba de San Pedro. Una vez culminada la investigación sobre dicha tumba en 1952, la profesora Margherita Guarducci, autoridad en epigrafía griega, comenzó a descifrar los grafitos que hay en uno de los muros contiguos a esa tumba. Algunos de ellos, que estaban casi escritos unos sobre otros son: «Pedro, ruega por los cristianos que estamos sepultados junto a tu cuerpo.» También se consiguió el logotipo de San Pedro, que era como una P y en el palo vertical tres rayas horizontales en forma de llave. Esta profesora concluyó que por allí está la tumba de San Pedro, pues entre los grafitos plasmados en el muro denominado G (de color blanco) y en el adyacente (de color rojo), descifró un grafito que significa: «Pedro está aquí». Al excavar descubrieron un nicho forrado de mármol blanco, que contenía huesos.

La responsabilidad de estudiar estos huesos recayó en Venerato Correnti, profesor y catedrático de Antropología de la Universidad de Palermo. En el estudio definió que en el nicho había huesos humanos y el de un ratón. Con respecto al animal, indicó que se coló por alguna rendija y al no poder salir murió allí. Un detalle interesante es que los huesos de los pies no aparecieron entre los restos hallados, y se puede recordar, que quien era crucificado cabeza abajo (entre los diferentes modos que existían en la crucifixión), se le descolgaba cortando los pies y así el cuerpo caía al suelo.

El papa comunicó al mundo tal hallazgo, en junio de 1968, asegurando que se habían encontrado los restos (reliquias) de San Pedro Apóstol.
La editorial Vaticana publicó un libro escrito por la profesora Guarducci con toda la información y que se titula "Las Reliquias de San Pedro".

El clima de la Ciudad del Vaticano, como el de Roma, es un clima templado, mediterráneo con inviernos moderados y lluviosos desde septiembre a mitades de mayo y veranos calurosos y secos que van de mayo a agosto.

La nacionalidad vaticana no se obtiene por nacimiento, sino por concesión. Es la única en ese tipo. Son ciudadanos de nacionalidad vaticana todos los diplomáticos empleados en las nunciaturas como las embajadas vaticanas de todo el mundo y aquellas personas que ejercen funciones para el Estado de la Ciudad. La nacionalidad vaticana se añade a la nacionalidad de origen y se pierde cuando las personas dejan de ejercer estas funciones.

En el Vaticano residen el sumo pontífice, los cardenales —que viven dentro de los muros, o en Roma—, los miembros del cuerpo diplomático, los sacerdotes y hermanos religiosos, los guardias suizos y algunos hombres y mujeres seglares, en su mayoría empleados en el Estado, junto con sus respectivos cónyuges e hijos.

El Vaticano no puede mantenerse a merced de la actividad productiva de su propio territorio, limitada a la venta de recuerdos turísticos, libros, sellos y entradas a museos. Pero cuenta con los ingresos de la organización católica en todo el mundo, provenientes de: las aportaciones económicas de los Estados donde cuenta con acuerdos (llamados Concordatos) de financiación (por su tradición católica); las donaciones de los católicos (a nivel personal o empresarial); y los beneficios de las empresas, escuelas, universidades y bancos propiedad de la Iglesia.

La economía estaba seriamente dañada hacia 1979, y tres años más tarde se produjo la quiebra de uno de los bancos más ilustres de Italia, el Banco Ambrosiano, que llevaba las finanzas internacionales del Vaticano, y el asesinato de su director Roberto Calvi: las investigaciones consiguientes revelarían que el banco se dedicaba al blanqueo del dinero de la mafia. Más tarde, el papa Juan Pablo II trasladó la responsabilidad de la economía vaticana, el cual, a partir de 1984, se encargaría de las finanzas. Cinco años más tarde, el papa lleva a cabo una reestructuración de la organización económica y la dirección de la economía había sido encargada a cinco financieros reconocidos internacionalmente (bajo la supervisión de una comisión de cinco cardenales). La Administración del Patrimonio de la Sede Apostólica se encarga de estos controles.

La lira vaticana estaba a la par con la lira italiana, que también circulaba como moneda válida dentro de la Ciudad del Vaticano. Estas unidades monetarias cambiaron con la entrada en vigor de la moneda única europea. Por acuerdo con Italia, en representación de la Unión Europea, la moneda vaticana es el euro, con diseño propio y aceptación en toda la zona euro. Dado que el Vaticano no tiene casa de moneda propia, ha establecido un acuerdo con Italia para su acuñación. El valor total de las monedas acuñadas no puede exceder de un millón de euros anuales.

La Ciudad del Vaticano emite sus propios sellos postales, tiene su propio periódico ("L'Osservatore Romano"), una emisora de radio (Radio Vaticano) y una televisión (Centro Televisivo Vaticano).

Además tiene distintas fundaciones, academias y universidades pontificias.

El Estado Vaticano cuenta con un servicio de teléfonos que dispone de modernas instalaciones para la comunicación tanto interna como externa de la ciudad y personal altamente capacitado. Téngase en cuenta que no existe el derecho a la confidencialidad de las comunicaciones telefónicas o de otro tipo. La ciudad-Estado consta de una compleja infraestructura de redes por la que consta de una completa autonomía.

La red ferroviaria de la Ciudad del Vaticano conecta la estación Ciudad del Vaticano, con la red ferroviaria de Italia, en la estación Roma San Pietro.

La ciudad posee también un helipuerto. Los servicios de ómnibus desde la ciudad de Roma son frecuentes.

El Vaticano otorga a través del registro de vehículos vaticanos una patente a los automóviles de dicha ciudad-Estado. Los automóviles que son del gobierno llevan las siglas SCV y los que son de los ciudadanos CV.

La cultura del Vaticano es obviamente correspondiente a la cultura de la religión católica, aunque se abre también al arte de otras culturas, y su mayor exponente son las obras de arquitectura, como la Basílica de San Pedro, la plaza de San Pedro, la Capilla Sixtina y los Museos Vaticanos.

Entre los Museos Vaticanos se encuentran: el Museo Gregoriano de arte egipcio y de arte etrusco, el Museo Pío Clementino, el Museo Chiaramonti y la Pinacoteca Vaticana.

Muchos artistas y arquitectos famosos como Bramante, Miguel Ángel, Rafael y Bernini trabajaron en importantísimas obras artísticas que hoy se pueden admirar en los edificios vaticanos.

En noviembre de 2006 se publicó un libro que revela la cocina del Vaticano, y que realiza un recorrido por la gastronomía histórica desde el primer papa hasta nuestros días, aderezado con una colección de recetas que incluyen menús tan representativos como el de la Última Cena o los platos favoritos de muchos de los papas. La autora del libro comenta que la gastronomía de este país es «una de las más complejas y ricas del mundo, mucho más que la de cualquier Casa Real».

El libro contiene además datos curiosos sobre los orígenes de algunas de las numerosas recetas que se inventaron en esta Ciudad, como por ejemplo la salsa verde (también llamada salsa vaticana), la salsa carmelita o la cocción al baño maría, además de información sobre protocolo y numerosas referencias que muestran la indisoluble unión entre la historia y la gastronomía de los papas.

El Vaticano cuenta con la Selección de fútbol de la Ciudad del Vaticano, que no está asociada a la UEFA ni a la FIFA ni al COI. El equipo está compuesto por voluntarios de la Guardia Suiza, miembros del Consejo Papal y por guardias de los museos. Desde 2007, la ciudad cuenta con un campeonato; además los seminaristas que, mientras estudian en Roma, viven en los diversos colegios pontificios participan en la Copa Clerical, un campeonato formado por religiosos que se empezó a jugar desde febrero del mismo año.

La relación de la Santa Sede con los organismos deportivos está encomendada al Consejo Pontificio para los Laicos, donde el papa Juan Pablo II creó una sección llamada "Iglesia y Deporte" en 2004.

Hasta la fecha, es el único país que no ha participado en los Juegos Olímpicos.





</doc>
<doc id="2942" url="https://es.wikipedia.org/wiki?curid=2942" title="Medicina veterinaria">
Medicina veterinaria

La medicina veterinaria es la rama de la medicina que se ocupa de la prevención, diagnóstico y tratamiento de enfermedades, trastornos y lesiones en los animales. El ámbito de la medicina veterinaria es amplio, cubriendo todas las especies, tanto domésticas como silvestres.

El profesional universitario que pone en práctica esta ciencia es llamado veterinario, médico veterinario o médico cirujano veterinario, mientras que en algunos países de Latinoamérica, el profesional que se dedica a la productividad agropecuaria es llamado zootecnista. El profesional técnico es llamado técnico veterinario o enfermero veterinario.

Denominación oficial para los médicos veterinarios en cada país
Esta palabra tiene varios orígenes posibles. La más comúnmente reconocida es que esta palabra proviene del idioma latín culto. Veterinarius, según el escritor Catón, era el conocedor y practicante del arte de curar las veterinae o veterina, es decir, las bestias de carga. El nombre de estos animales parece proceder de vetus (viejo), porque se trataría de animales envejecidos, y por ende no aptos ya para las carreras ni para los carros de guerra y sólo útiles para el transporte. 

Otras fuentes afirman que veterina pudo nacer del verbo veho, vehere, de donde se derivaría vehículo, que significa precisamente transportar. 

Para los árabes está la palabra "albéitar" que hace referencia a la persona encargada de curar las patologías de los caballos, animales tan importantes para la cultura árabe.

Los papiros egipcios de Lahun (1900 a.C.) y la literatura vedas de la antigua India ofrecen uno de los primeros registros escritos sobre la medicina veterinaria. El emperador budista de la India, Aśoka, ordeno lo siguiente: ""En todas partes del reino se harán dos tipos de medicamentos (चिकित्सा), medicina para las personas y la medicina para animales. Cuando no hubiese hierbas curativas para las personas y animales, se ordena comprarlas y sembrarlas"".

Los primeros intentos de organizar y regular la práctica veterinaria tienden a centrarse en los caballos, debido a su importancia como medio de transporte y arma de guerra. Durante la Edad Media (año 1356), el alcalde de Londres, Henry Picard, preocupado por la mala calidad de la atención prestada a los caballos en la ciudad, pidió que todos los herradores que operan dentro de un radio de siete millas de la ciudad forman una "beca" para regular y mejorar sus prácticas. Esta última instancia condujo a la creación del Gremio de herradores en 1674.

El primer tratado completo sobre la anatomía de una especie no humana corresponde al libro "Anatomia del Cavallo" (Anatomía del caballo), publicado por el italiano Carlo Ruini en el año 1598.

La primera facultad veterinaria data del año 1761, siendo fundada por Claude Bourgelat como Escuela Nacional Veterinaria de la Universidad de Lyon. Poco tiempo después se fundan otras siguiendo este modelo en otros países de Europa, como la de Padua en 1765, Viena en 1768 o la de Turín en 1769.

En Alemania destacan las contribuciones de Johann Christian Erxleben a la medicina veterinaria moderna en Gotinga.

La Sociedad Agrícola Odiham fue fundada en 1783 en Inglaterra para promover la agricultura y la industria, jugó un papel importante en la fundación de la profesión veterinaria en Gran Bretaña. Thomas Burgess, miembro fundador de la sociedad, comenzó a asumir la causa del bienestar animal y tratamiento más humanitario de los animales enfermos. En una de las reunión de la Sociedad en 1785, se resolvió "promover el estudio de herraje con principios científicos racionales". El Real Colegio de Veterinarios del Reino Unido fue establecido por carta real en 1844.

La ciencia veterinaria alcanzaría "mayoría de edad" a finales del siglo XIX, con notables contribuciones de Sir John McFadyean, acreditados por muchos como el fundador de la investigación veterinaria moderna. En Estados Unidos, las primeras escuelas fueron establecidas en el siglo XIX en Boston, Nueva York y Filadelfia.

Para el año 2016, la publicación QS Top Universities, indicaba que las las diez mejores universidades del mundo en esta disciplina eran:

La escuela veterinaria en México surge el 17 de agosto de 1853 por el decreto 4001, expedido por el entonces presidente de la República Mexicana, Antonio López de Santa Anna . Esta escuela pertenecía al Colegio Nacional de Agricultura, siendo la primera escuela de medicina veterinaria de su género en México y en el continente . Durante esa época el Rector del Colegio Nacional de Agricultura era José María Arreola .

En el decreto del presidente Santa Anna se estableció además que después de seis años desde la creación de la carrera no se permitiría la práctica veterinaria a quien no hubiese obtenido el título correspondiente (Artículo 13º) , el cual era otorgado por el Colegio Nacional de Agricultura, aunque tuvieron que pasar diez años desde la creación de la escuela hasta el egreso de los primeros estudiantes debido a diversas vicisitudes .

La duración de la carrera de veterinario sería de cuatro años, los cuales serían cursados después de la educación secundaria . Durante aquella época la educación veterinaria en México y en el mundo estaba casi totalmente encaminada hacia la medicina equina, debido a la importancia de este animal tanto en el aspecto militar como en el económico, de tal forma que no es una sorpresa que las asignaturas del primer plan de estudios estuvieran encaminadas hacia esta especie.

En la elaboración del plan de estudios de la carrera, la mayor parte se le atribuye al ilustre militar y veterinario francés Pascal Eugène Bergeyre, egresado de la Escuela Nacional de Veterinaria de Tolsa, el cual además fungía como el médico encargado de las caballerizas del Presidente de la República . De acuerdo con Eugène Bergeyre ""las cátedras se imparten basándose en textos extranjeros correspondientes a las enseñanzas de las escuelas europeas de medicina veterinaria y principalmente las francesas"" . Principalmente, el libro de Philippe Etinne Lafosse "Cours d' Hippiatrique, ou Traité de la médicine des" "chevaux "fue la piedra angular en el principio de la educación veterinaria en México , de la que se conserva un ejemplar original en la Biblioteca MV José de la Luz Gómez en la actual Facultad de Medicina Veterinaria y Zootecnia de la UNAM.

El plan de estudios para la carrera de veterinaria en 1853 estaba compuesto por las siguientes materias: En el primer año, lección alternada de zoología y dibujo anatómico, lección diaria de química, lección diaria de inglés, ejercicios de equitación y manipulaciones químicas; en el segundo año, lección diaria de anatomía y fisiología hipiátricas y a fin de año, curso comprendido de higiene hipiátrica, natación e inglés; en el tercer año, lección diaria de patología interna y externa hipiátricas, lección diaria de clínica interna y externa hipiátricas, práctica anatómica y patológica hipiátrica, lección diaria de idioma alemán; en el cuarto año, lecciones diarias de operaciones terapéuticas, lecciones alternadas de los principios de economía rural y práctica de herrajes, lección diaria de idioma alemán .

Las labores académicas preparativas iniciaron el 22 de febrero de 1854, en el antiguo hospicio de San Jacinto, el cual se encuentra ubicado en la actual Calzada México - Tacuba .

Los profesores del colegio de San Jacinto de acuerdo a un siguiente decreto establecido el 4 de enero de 1856 por el entonces presidente Ignacio Comonfort propusieron como director del Colegio Nacional de Agricultura al eminente médico cirujano y científico Leopoldo Río de la Loza, el cual había sido presidente de la Academia de Medicina, para iniciar finalmente las actividades académicas .

Además, este decreto declaraba que la enseñanza veterinaria estuviera orientada a la formación de mariscales veterinarios, para la cual se cursaban tres años de estudios y por otra parte, la formación de los profesores de veterinaria duraría cinco años .

Para el inicio de las clases, las cátedras de anatomía y fisiología serían impartidas por Ignacio Alvarado (considerado el iniciador de la fisiología experimental en México y médico de cabecera Don Benito Juárez) mientras que las relacionadas a la Medicina Veterinaria eran impartidas por Eugène Bergeyre .

Será hasta el año de 1857 que ingresarían a la carrera de medicina veterinaria los primeros siete alumnos, iniciando los cursos formalmente el 9 de abril de 1858 .

Estos alumnos fueron José de la Luz Gómez, José E. Mota, Manuel y Mariano G. Aragón, José María Lugo, Narciso Aguirre e Ignacio Salazar . Desgraciadamente se suscitaba la Guerra de Reforma desde el 17 de diciembre de 1857, la cual causaría complicaciones para la enseñanza veterinaria y para el colegio de San Jacinto, ya que este fue utilizado como cuartel .

Sería durante enero 1861, que el presidente Benito Juárez entraría a la Ciudad de México, y a los pocos días retiró de su cargo al Dr. Río de la Loza, mismo que fue sustituido por Juan N. Navarro, un médico militar originario de Michoacán. El Sr. Navarro dirigiría la escuela desde 1861 a 1867 . 

Su enseñanza en Chile comienza de forma estable y definitiva el 1 de mayo de 1898 por parte del Ejército de Chile, mediante los cursos de Veterinaria Militar, con una duración de 3 años. Luego, en 1905 se creó la Escuela Militar de Veterinaria

La enseñanza de la profesión paso de lo militar a lo civil el 10 de noviembre de 1915, mediante decreto Supremo N° 1853 que crea en la Escuela de Medicina Veterinaria Civil, en dependencias de la Quinta Normal de Agricultura.

La Universidad de Chile fue la primera casa de estudios superior en dictar la carrera en dicho país, el 12 de abril de 1928, al fundar la primera Facultad de Ciencias Veterinarias y Pecuarias del país.

En el año 2017 trece universidades chilenas impartían la carrera, nombradas desde la más antigua a la más reciente en ofrecer la carreraː Universidad de Chile (1928), Universidad Austral (1955), Universidad de Concepción (1972), Universidad Santo Tomás (1990), Universidad Iberoamericana de Ciencias y Tecnología (1991), Universidad Mayor (1991), Universidad Católica de Temuco (1993), Universidad de las Américas (2000), Universidad de Viña del Mar (2002), Universidad San Sebastián (2002), Universidad Andrés Bello (2004), Universidad Pedro de Valdivia (2004) y Universidad del Pacífico (2006).
La extinta Universidad Regional San Marcos dictó la carrera entre los años 2004 y 2011.

El veterinario o médico veterinario (conocido también como médico cirujano veterinario), es aquel profesional universitario encargado de la salud animal, con estudios equivalentes a una licenciatura o grado (en España). El rol profesional es el equivalente de un médico en humanos. En la lengua española (de manera coloquial), también se denomina doctor a estos profesionales de la salud, aunque no hayan obtenido el grado académico de doctorado.

Entre las funciones que realiza el médico cirujano veterinario se destacan el diagnóstico y tratamiento de la patología de los animales, mejorar el rendimiento animal y la ganadería productiva, vigilar la fabricación y puesta en circulación, así como su estado, de los productos alimenticios de origen animal destinados al consumo humano (bromatologia), la epidemiología y salud pública, la investigación y la docencia.

El médico veterinario puede necesitar la sistencia de ayudantes o técnicos veterinarios para llevar a cabo tareas inherentes a la profesión, ya que son ellos los encargados de ejecutar las indicaciones médicas de la forma correcta para con los pacientes en tratamiento y generar la relación profesional con los propietarios de los pacientes.

El técnico veterinario, es el profesional calificado para colaborar en la administración de clínicas de mascotas y planteles pecuarios a pequeña y mediana escala, siempre bajo la supervisión de un veterinario o médico veterinario. Su papel es fundamental para el desarrollo de la profesión; ya que sobre ellos recae la responsabilidad de asistir y apoyar los procedimientos médicos. Para obtener este título es necesario cursar la carrera de Técnico de nivel Superior en Veterinaria con al menos 3 años de estudio.



</doc>
<doc id="2945" url="https://es.wikipedia.org/wiki?curid=2945" title="Vilvestre">
Vilvestre

Vilvestre es un municipio y localidad española de la provincia de Salamanca, en la comunidad autónoma de Castilla y León. Se integra dentro de la comarca de Vitigudino y la subcomarca de La Ribera (Las Arribes). Pertenece al partido judicial de Vitigudino.

El mirador del cerro del castillo, el monte gudín, la iglesia, el rollo de justicia, el molino de la Luisa y el paraje del muelle de La Barca son los lugares más destacados de este municipio del Parque natural de Arribes del Duero, que a su vez forma parte de la reserva de la biosfera transfronteriza denominada como Meseta Ibérica. Está formado por un único núcleo de población, ocupa una superficie total de 46,52 km² y según datos del padrón municipal elaborado por el INE en , cuenta con una población de habitantes.

El escudo heráldico que representa al municipio fue aprobado el 11 de abril de 1997 con el siguiente blasón:

Vilvestre siempre ha estado vinculado al Duero. El Catastro de la Ensenada lo sitúa a poniente de este término y Madoz (Diccionario Geográfico, Estadístico e Histórico) explica cómo los naturales mediado el siglo pasado, lo atravesaban en barca. Y a este río aluden las cuatro bandas azules colocadas en punta. Dado el microclima de este término municipal, siempre han abundado los árboles frutales, dedicando gran parte del territorio al cultivo de olivos. Hay una zona bastante extensa conocida por “Los Olivares”. Por ello existe un olivo en el escudo.

Este pueblo, según afirma el Padre Morán (reseña Histórico- Artística de la provincia de Salamanca), juntamente con Yecla, Moronta y Palacios de Arzobispo, Perteneció al Arzobispado Compostelano. Este es el motivo de que aparezca la Cruz-Espada de Santiago. Fue famoso por su castillo frontero con Portugal, y parece que fue gobernado por Ruy Gómez de Silva. Príncipe de Eboli. La respuesta 23 de la generales del catastro de ensenada dice textualmente “ que esta Villa y su concejo goza en calidad de propios un fuerte arruinado que llaman el castillo” ( fecha de 28 de junio de 1752). En el siglo XVIII era ya una defensa inservible, seguramente quedaría fuera de combate en la Guerra de Secesión. Un dicho popular, coetáneo al tiempo de la fortificación, cuando estaban levantados muros y cortinas con piedra rosada. Lo más seguro es que la construcción fuera de líneas abaluartadas y una estancia estuviera destinada a capilla de la Virgen. Nuestra Señora del Castillo, que en 1757 tuvo ermita propia, seguramente procediera de la fortaleza.

"Asómate al castillode la Hinojosaverás al de Vilvestrecara de rosa"

La topografía del municipio de Vilvestre podría definirse en general como suavemente ondulada en la parte norte y este del municipio, aumentando las pendientes hacia la parte sur y oeste. 

La altitud media de la zona es de 592 metros sobre el nivel del mar, destacando como punto más elevado el “Cerro de Homomula” con 791 metros, descendiendo en el río Duero hasta los 190 metros, que es la cota del Nivel Máximo Normal del embalse de 
Saucelle. 

Vilvestre pertenece a la cuenca fluvial del Duero, siendo éste el principal accidente hidrográfico del término municipal. Los arroyos son de escasa importancia, con un caudal muy variable después de encauzar las aguas de escorrentía y de manantiales según la estación y pluviometría, destacando el Arroyo de la Nava, el Arroyo de Fuentesalsa, el Arroyo de Las Payitas y el Arroyo de Los Lagares. Estos arroyos se suelen secar habitualmente en verano debido a la falta de precipitaciones.

Los recursos hídricos de aguas subterráneas no son demasiado relevantes y se asocian con acuíferos someros, como fuentes, pozos y charcas, que se aprovechan, tanto para el abastecimiento de la población, si bien este es complementado mediante una red de abastecimiento procedente de la Presa de Almendra, como para abrevar a la ganadería en régimen extensivo. 

Vilvestre se encuentra situado en el en la parte sur de las Arribes del Duero y al noroeste de la Provincia de Salamanca. Limita al oeste con el Río Duero, que actúa como frontera natural separándolo de Portugal. Dista 96 km de Salamanca capital.

Se integra dentro de la comarca de La Ribera. Pertenece a la Mancomunidad Centro Duero y al partido judicial de Vitigudino.

Su término municipal se encuentra dentro del Parque natural de Arribes del Duero, un espacio natural protegido de gran atractivo turístico.

Vilvestre tiene un clima mediterráneo continentalizado suavizado por la cercanía del río Duero y la relativa baja altitud del municipio en el contexto de la meseta norte. Se trata de un "Csa" (templado con verano seco y caluroso) según la clasificación climática de Köppen. Debido a estas condiciones climáticas se pueden encontrar naranjos, chumberas, olivos y melocotoneros en la localidad.

En 2011, el municipio contaba con un total de 296 vehículos de motor, que representan 404,1 automóviles por cada 1000 habitantes. Los puntos de Inspección Técnica de Vehículos más cercanos se encuentran en la capital provincial y en Vitigudino.

A Vilvestre se accede desde la capital charra por la carretera autonómica CL-517, de Salamanca a La Fregeneda. Llegando a Vitigudino, desviación por la carretera Provincial SA-320, de Vitigudino a Mieza hasta El Milano. Desde El Milano bien podemos desviarnos por la DSA-572, de la red secundaria de la Diputación Provincial, hasta Barruecopardo y de aquí por la DSA-575 hasta Vilvestre, o bien continuar desde El Milano, por la SA-320, hasta Cerezal de Peñahorcada y desviación por la DSA-576, de la red secundaria de la Diputación Provincial, de Cerezal a Vilvestre, recorriéndose en ambos casos una distancia total de 96 km. desde Salamanca. 

"* Nota: en aquellas vías en las que aparecen dos referencias la que aparece en cursiva es la oficial, y la que aparece escrita normal es la de utilización habitual en señalización y en mapas."

Para el transporte de viajeros por autobús, la compañía Arribes Bus S.L. ofrece servicios por carretera entre el municipio y distintos destinos provinciales y comarcales como por ejemplo Salamanca o Vitigudino.

Horarios:


De lunes a viernes: 13:15 h. y 18:00* h.


De lunes a viernes: 07:30 h. y 15:00* h.


Solo martes: 11:00 h.

Nota: Las salidas marcadas con un asterisco (*) no se realizan martes y jueves

La línea de ferrocarril más próxima es la de Salamanca-Valladolid.

En lo que concierne al transporte aéreo, el aeropuerto más cercano es el de Salamanca, localizado entre las localidades de Machacón, Calvarrasa de Abajo y Villagonzalo de Tormes, a 116 kilómetros de Vilvestre. Asimismo, las otras opciones a menor distancia son los aeropuertos de Valladolid y Madrid, situados a 223 y 330 kilómetros respectivamente.


Su economía está basada principalmente en la ganadería, la agricultura y la industria transformadora (Fábrica de Quesos). Además se está desarrollando también el turismo con la apertura en los últimos años de varias casas rurales además de un centro de turismo rural, y el fomento de las actividades turísticas con la construcción de nuevos miradores, el marcado de nuevas rutas de senderismo, rutas fluviales por el río Duero, y la oferta municipal de dos museos, y un albergue.

La concentración parcelaria de la zona de Vilvestre (Salamanca), fue declarada de Utilidad Pública y Urgente Ejecución por Acuerdo 18/2008, de 13 de marzo, de la Junta de Castilla y León. 

Las Bases Provisionales se publicaron con fecha 31 de marzo de 2012, y el plazo para presentar Alegaciones finalizó el día 10 de mayo de 2012.
Actualmente se encuentra en fase de Bases Definitivas habiéndose publicado con fecha 28 de junio de 2014. El plazo para presentar Recurso finalizó el día 2 de agosto de 2014. Además se encuentra en trámite la Petición de Lotes por parte de los propietarios.


En el municipio de Vilvestre existen varias rutas de senderismo marcadas para que aquellas personas que lo deseen puedan realizar senderismo por ellas.

Además el Ayuntamiento de Vilvestre organiza anualmente la Marcha de Senderismo Arribes del Duero el segundo domingo de marzo para dar a conocer algunos rincones que habitualmente no son recorridos por los senderos ya prefijados.



Esta estructura de la población se debe a la emigración que tuvo lugar desde finales del siglo XIX, cuando el número de habitantes alcanzó su máximo histórico. A partir de entonces la población empezó a desplazarse mayoritariamente a América y, más tarde a Europa y a otras regiones de España y de la provincia de Salamanca. El proceso se aceleró a partir de los años 60 debido a la baja rentabilidad de las explotaciones agrícolas en una economía moderna y la escasez de alternativas en la zona, entre otras por el cierre de la mina de wolframio de Barruecopardo. La emigración no solo causó un importante despoblamiento, sino también el envejecimiento de la población tal y como se aprecia en la pirámide de población.
Además se está produciendo una gran despoblación, con una pérdida del 28,93% de la población durante el siglo XXI; que a diferencia de lo ocurrido entre los años 1960 y 1981, en los que se produjo un descenso histórico de más del 50% de la población por la emigración, en este caso la bajada de población se debe a la mortalidad, principalmente causada por el envejecimiento de la población.

La actual Corporación Municipal de Vilvestre fue elegida en las elecciones municipales celebradas el 24 de mayo de 2015, tomando posesión de sus cargos el día 13 de junio de 2015, y está dirigida por segunda legislatura consecutiva por D.Manuel Domínguez Hernández, miembro del Partido Popular que gobierna con mayoría absoluta, al contar con cuatro de los 7 miembros de la Corporación. Los otros tres concejales pertenecen a Ciudadanos.

Los anteriores alcaldes desde 1979 fueron Casimiro Hernández Calvo, (1979-1995) y José Manuel Guarido Mateos, (1995-2007).

La fundación de Vilvestre se sitúa en el siglo XII, dentro del proceso repoblador emprendido en la zona por los reyes de León, siendo probablemente repoblado con gentes provenientes de Vilviestre del Pinar, en la Tierra de Lara, actual provincia de Burgos, según sostiene el medievalista Ángel Barrios. En la provincia de Soria, en el actual municipio de Royo, encontramos otro Vilviestre, éste y el anterior de la cercana Tierra de Lara, se encuentran en el área del Sistema Ibérico, en torno a la Sierra de la Demanda y los Picos de Urbión. Asimismo, en el alfoz de la ciudad de Burgos se ubica un tercer Vilviestre (de Muñó), que forma parte del municipio de Estépar.

Sea como fuere su repoblación, lo que si está documentado es la donación de Vilvestre con su término al Arzobispado de Santiago en 1192 por parte del rey Alfonso IX de León, pasando a pertenecer la localidad a la diócesis de León de Santiago, la cual agrupaba los territorios leoneses de dicha Orden, no pasando a pertenecer a la diócesis de Salamanca hasta el siglo XVI.

Por otro lado, la zona de Las Arribes del Duero fue fortificada entre la segunda mitad del siglo XII y principios del XIII una vez que Portugal se independizó del Reino de León en 1143, siendo Vilvestre una de las localidades fortificadas para defender la frontera leonesa. Asimismo, junto al de Vilvestre se erigieron otros castillos y fortalezas en Barruecopardo (anterior a 1212), Mieza, Aldeadávila (se conserva la mayor parte de la torre del Homenaje, aunque recrecida), Masueco y Pereña, en algunos casos reutilizadas parte de ellos en las iglesias parroquiales. Parte de ellos pasaron posteriormente al infantado de don Pedro de Molina.

Posteriormente, en el siglo XV, Vilvestre, como la mayor parte del territorio histórico leonés, tomó partido por Juana la Beltraneja, en la guerra civil abierta entre ésta e Isabel la Católica por los tronos de León y de Castilla, estando defendido su castillo por las tropas juanistas hasta la derrota de dicho bando, siendo en el siglo XVII arruinado el castillo de Vilvestre en el ataque que las tropas portuguesas hicieron a la localidad dentro de la Guerra de Restauración portuguesa (1640-1668).

En otro orden de cosas, la expulsión de los judíos dictada por los Reyes Católicos tuvo en estas tierras una de sus fases señeras: "Igualmente, los justicias de Ledesma recibieron orden de ir a Pereña, o donde fuera necesario, para prender a Pedro de Miranda, que era pasador de los judíos fuera de los caminos señalados, y a García de Ledesma, y a Pedro Herrero, quienes habían intentado matar a Alonso de Sejas, encargado de que se cumpliera la salida de los judíos por los caminos estipulados para tal cosa en el término de la mencionada villa -Ledesma- y en Vilvestre". Y la Reina Isabel la Católica mandó detenerle y ajusticiarle en 1481, y nos dice el cronista Pulgar: "Cuarenta y seis fortalezas fueron derrribadas entonces, y veinte más tarde: ajusticiados como principales malhechores Pedro de Miranda y el mariscal Pero Pardo...". Hechos que prueban la existencia de una comunidad importante de judíos en Vilvestre.

Durante el pontificado de Clemente VII se resolvió a favor de la sede salmantina la posesión de las villas de Vilvestre, Yecla de Yeltes, Vitigudino y Palacios del Arzobispo; siendo D. Francisco de Bobadilla (1510-1529) el primer obispo que las poseyó.

Con la creación de las actuales provincias en 1833, Vilvestre quedó encuadrado en la provincia de Salamanca, dentro de la Región Leonesa.






</doc>
<doc id="2947" url="https://es.wikipedia.org/wiki?curid=2947" title="VRML">
VRML

El lenguaje de modelado de realidad virtual o VRML (sigla del inglés "Virtual Reality Modeling Language") es un formato de archivo normalizado que tiene como objetivo la representación de escenas u objetos interactivos tridimensionales diseñado particularmente para web. Se usa por medio de comandos en inglés, los cuales agregan y determinan las características.

El lenguaje VRML posibilita la descripción de una escena compuesta por objetos 3D a partir de prototipos basados en formas geométricas básicas o de estructuras en las que se especifican los vértices y las aristas de cada polígono tridimensional y el color de su superficie. VRML permite también definir objetos 3D multimedia, a los cuales se puede asociar un enlace de manera que el usuario pueda acceder a una página web, imágenes, vídeos u otro fichero VRML de Internet cada vez que haga clic en el componente gráfico en cuestión.

El Consorcio Web3D fue creado para desarrollar este formato. Su primera especificación fue publicada en 1995; la versión actual funcionalmente completa es la VRML 97 (ISO/IEC DIS 14772-1). 

VRML es la base en la que se ha desarrollado X3D (Extensible 3D Graphics).



</doc>
<doc id="2951" url="https://es.wikipedia.org/wiki?curid=2951" title="Wiki">
Wiki

El término wiki (del hawaiano "wiki", «rápido») alude al nombre que recibe un sitio web, cuyas páginas pueden ser editadas directamente desde el navegador, donde los mismos usuarios crean, modifican o eliminan contenidos que, generalmente, comparten. No tiene por qué ser necesariamente un sitio en la web, puesto que hay wikis instalables para uso en el escritorio de un computador personal o que pueden portarse en un llavero USB que lleven un entorno LAMP, como por ejemplo XAMPP.

Los textos o «páginas "wiki"» tienen títulos únicos. Si se escribe el título de una página "wiki" en algún sitio de la "wiki" entre dobles corchetes (<nowiki></nowiki>"Título de la página"<nowiki></nowiki>), esta palabra se convierte en un «enlace web» a la página correspondiente. De este modo, en una página sobre «alpinismo» puede haber una palabra como «piolet» o «brújula» que esté marcada como palabra perteneciente a un título de página "wiki". La mayor parte de las implementaciones de "wikis" indican en el localizador de recursos uniforme (URL) de la página el propio título de la página "wiki" (en Wikipedia, ocurre así: <nowiki><https://es.wikipedia.org/wiki/Alpinismo></nowiki> es el URL de la página "wiki" "Alpinismo"), lo que facilita el uso y la aplicación general del enlace fuera del propio sitio web. Además, esto permite formar en muchas ocasiones una coherencia terminológica, y genera una ordenación "natural" del contenido.

Su principal tarea, a la que le debe su fama hasta el momento, ha sido la creación de enciclopedias colectivas, género al que pertenece Wikipedia. Existen muchas otras aplicaciones más cercanas a la coordinación de informaciones y acciones, o la puesta en común de conocimientos o textos dentro de grupos.

La mayor parte de las "wikis" actuales conservan un historial de cambios que permite recuperar fácilmente cualquier estado anterior y ver qué usuario hizo cada cambio, lo cual facilita el mantenimiento conjunto y el control de usuarios nocivos. Habitualmente, sin necesidad de una revisión previa, se actualiza el contenido que muestra la página "wiki" editada.

El origen de las "wikis" está en la comunidad de patrones de diseño, cuyos integrantes los utilizaron para escribir modelos de programación. La primera WikiWikiWeb fue creada por Ward Cunningham, quien inventó y dio nombre al concepto "wiki", y produjo la primera implementación de un servidor WikiWiki para el repositorio de patrones del Portland (Portland Pattern Repository) en 1995. En palabras del propio Cunningham, una "wiki" es «la base de datos en línea más simple que pueda funcionar» ("the simplest online database that could possibly work"). 

En enero de 2001, los fundadores del proyecto de enciclopedia Nupedia, Jimmy Wales y Larry Sanger, decidieron utilizar un "wiki" como base para el proyecto de enciclopedia Wikipedia. Originalmente se usó el software UseMod, pero luego crearon un software propio, MediaWiki, que ha sido adoptado después por muchos otros wikis.

La "wiki" más grande que existe es la versión en inglés de Wikipedia, seguida por varias otras versiones del proyecto. Las "wikis" que no pertenecen a Wikipedia son mucho más pequeños y con menor participación de usuarios, generalmente debido al hecho de ser mucho más especializados. Es muy frecuente, por ejemplo, la creación de "wikis" para proveer de documentación a programas informáticos, especialmente los desarrollados en software libre.

La principal utilidad de una "wiki" es que permite crear y mejorar las páginas de forma inmediata, dando una gran libertad al usuario, y por medio de una interfaz muy simple. Esto hace que más gente participe en su modificación, a diferencia de los sistemas tradicionales, donde resulta más difícil que los usuarios del sitio contribuyan a mejorarlo. En entornos privados, como empresas u organizaciones, permite que los usuarios gestionen contenidos e información de forma procesional, pudiendo hacer uso de todo tipo de extras, como envío de correos entre usuarios y chat en línea, entre muchas otras extensiones disponibles.

Dada la gran rapidez con la que se actualizan los contenidos, la palabra "wiki" adopta todo su sentido. El «documento» de hipertexto resultante, denominado también «"wiki"» o «WikiWikiWeb», lo produce típicamente una comunidad de usuarios. Muchos de estos lugares son inmediatamente identificables por su particular uso de palabras en mayúsculas, o texto "capitalizado" –uso que consiste en poner en mayúsculas las iniciales de las palabras de una frase y eliminar los espacios entre ellas– como por ejemplo en "EsteEsUnEjemplo". Esto convierte automáticamente a la frase en un enlace. Este "wiki", en sus orígenes, se comportaba de esa manera, pero actualmente se respetan los espacios y sólo hace falta encerrar el título del enlace entre dos corchetes.

Lo más importante se describe como la posibilidad de introducir adendos y modificaciones carentes de autenticidad y rigor siempre y cuando sea una "wiki" que haya sido configurada para permitir a cualquier usuario editar contenido, ya que su amplio sistema de extensiones y configuración permite que se impongan todo tipo de restricciones a los usuarios o a los artículos. En una "wiki" que se permita editar a cualquier persona, se podrá intervenir sin que su información o comentarios estén suficientemente contrastados, lo cual no sería un inconveniente directo del sistema "wiki", sino de la decisión del propietario o administrador respecto a quién puede editar el contenido. Debido a ello, en las "wikis" donde se permite editar a todas las personas, se toman las medidas más adecuadas al alcance de los mecanismos editoriales con objeto de optimizar la fiabilidad de las informaciones introducidas.

Según su creador una "wiki" es “la base de datos en línea más simple que pueda funcionar". Se trata de un tipo de página web que brinda la posibilidad de que multitud de usuarios puedan editar sus contenidos a través del navegador web, con ciertas restricciones mínimas. De esta forma permite que múltiples autores puedan crear, modificar o eliminar los contenidos. Se puede identificar a cada usuario que realiza un cambio y recuperar los contenidos modificados, volviendo a un estado anterior. 
Estas características facilitan el trabajo en colaboración así como la coordinación de acciones e intercambio de información sin necesidad de estar presentes físicamente ni conectados de forma simultánea. El ejemplo más conocido y de mayor tamaño de este tipo de páginas web es la enciclopedia colaborativa Wikipedia (www.wikipedia.org).
A favor: Es una fuente de información y bibliográfica de construcción colectiva.
Problemas: La información publicada puede provenir de fuentes erróneas o no válidas. 
Solución/recomendaciones: Es recomendable trabajar criterios sobre el empleo de fuentes de información confiables y formas de validar los contenidos.

Una "wiki" permite que se escriban artículos colectivamente (co-autoría) por medio de un lenguaje de wikitexto editado mediante un navegador. Una página "wiki" singular es llamada «página "wiki"», mientras que el conjunto de páginas (normalmente interconectadas mediante hipervínculos) es «la "wiki"». Es mucho más sencillo y fácil de usar que una base de datos.

Una característica que define la tecnología "wiki" es la facilidad con que las páginas pueden ser creadas y actualizadas. En general no hace falta revisión para que los cambios sean aceptados. La mayoría de las "wikis" están abiertos al público sin la necesidad de registrar una cuenta de usuario. A veces se requiere conectarse para obtener una cookie de «wiki-firma», para autofirmar las ediciones propias. Otras "wikis" más privadas requieren autenticación de usuario. 

Por lo explicado, las "wikis" son una muy buena opción pedagógica para realizar actividades educativas, ya que como explica Mariana Maggio, se pueden generar propuestas que los alumnos puedan integrar en las "wikis" a partir de la reconstrucción de las mismas en un sentido didáctico. En la actualidad los documentos Web, como lo es el ejemplo de las "wikis", crean tendencias y cuando éstas configuran los usos de los niños y los jóvenes, es importante que los educadores las reconozcan y se preocupen por entenderlas a partir de su exploración.

Para Maggio, «un proyecto didáctico maravilloso puede ser, cuando el tema lo justifique, generar contenidos para Wikipedia o revisar los publicados allí: entender el tema de un modo profundo, verificar los contenidos, transparentar y discutir los criterios, ampliar lo publicado, ofrecer versiones y especificaciones de alto valor local».

Una "wiki" también puede ser un espacio usado para seguimiento individual de los alumnos, donde ellos puedan crear sus proyectos independientemente y el profesor pueda intervenir guiando y corrigiendo. Se ha utilizado también en procesos de formación docente ayudando al mejoramiento de sus habilidades tecnológicas, pero también los procesos de colaboración entre pares.
Permite la creación colectiva de documentos en un lenguaje simple de marcas utilizando un navegador web.
Generalmente no se hacen revisiones previas antes de aceptar las modificaciones y la mayoría de las "wikis" están abiertas.
Permite a los participantes trabajar juntos en páginas web, para añadir o modificar su contenido.
Las versiones antiguas nunca se eliminan y pueden restaurarse.
Se puede seleccionar diferentes tipos de "wiki", profesor, grupo, alumno.

En una "wiki" tradicional, existen tres representaciones por cada página:


El código fuente es potenciado mediante un lenguaje de marcado simplificado para hacer varias convenciones visuales y estructurales. Por ejemplo, el uso del asterisco «*» al empezar una línea de texto significa que se generará una lista desordenada de elementos ("bullet-list"). El estilo y la sintaxis pueden variar en función de la implementación, alguno de las cuales también permite etiquetas HTML.

La razón de este diseño es que el HTML, con muchas de sus etiquetas crípticas, es difícil de leer para usuarios no habituados a la tecnología. Hacer visibles las etiquetas de HTML provoca que el texto en sí sea difícil de leer y editar para la mayoría de usuarios. Por lo tanto, se promueve el uso de edición en texto llano con convenciones para la estructura y el estilo fáciles de comprender.

A veces es deseable que los usuarios no puedan usar ciertas funcionalidades que el HTML permite, tales como JavaScript, CSS y XML. Se consigue consistencia en la visualización, así como seguridad adicional para el usuario. En muchas inserciones de "wiki", un hipervínculo es exactamente tal como se muestra, al contrario de lo que ocurre en el HTML.

Durante años el estándar de facto fue la sintaxis del WikiWikiWeb original. Actualmente las instrucciones de formateo son diferentes dependiendo del motor de la "wiki". Las "wikis" simples permiten sólo formateo de texto básico, mientras que otros más complejos tienen soporte para cuadros, imágenes, fórmulas e incluso otros elementos más interactivos tales como encuestas y juegos. Debido a la dificultad de usar varias sintaxis, se están haciendo esfuerzos para definir un estándar de marcado (ver esfuerzos de Meatball y Tikiwiki).

Los wikis son un auténtico medio de hipertexto, con estructuras de navegación no lineal. Cada página contiene un gran número de vínculos a otras páginas. En grandes "wikis" existen las páginas de navegación jerárquica, normalmente como consecuencia del proceso de creación original, pero no es necesario usarlas. Los vínculos se usan con una sintaxis específica, el «patrón de vínculos».

Originalmente, la mayoría de las "wikis" usaban CamelCase como patrón de vínculos, poniendo frases sin espacios y poniendo la primera letra de cada palabra en mayúscula (por ejemplo, la palabra «CamelCase»). Este método es muy fácil, pero hace que los hiperenlaces se escriban de una manera que se desvía de la escritura estándar. Las "wikis" basadas en CamelCase se distinguen instantáneamente por los enlaces con nombres como: «TablaDeContenidos», «PreguntasFrecuentes». Por consiguiente, comenzaron a desarrollarse otras soluciones.

Los «vínculos libres», usados por primera vez por CLiki, usan un formato tipo _(vínculo). Por ejemplo, _(Tabla de contenidos), _(Preguntas frecuentes). Otros motores de "wiki" usan distintos signos de puntuación.

Interwiki permite vínculos entre distintas comunidades "wiki". Las nuevas páginas se crean simplemente creando un vínculo apropiado. Si el vínculo no existe, se acostumbra a destacar como «vínculo roto». Siguiendo el vínculo se abre una página de edición, que permite al usuario introducir el texto para la nueva página "wiki". Este mecanismo asegura que casi no se generen páginas huérfanas (es decir, páginas que no tienen ningún vínculo apuntando a ellas). Además se mantiene un nivel alto de conectividad.

La mayoría de las "wikis" permite al menos una búsqueda por títulos, a veces incluso una búsqueda por texto completo. La escalabilidad de la búsqueda depende totalmente del hecho de que el motor de la "wiki" disponga de una base de datos o no: es necesario el acceso a una base de datos indexada para hacer búsquedas rápidas en "wikis" grandes. En Wikipedia, el botón «Ir» permite a los lectores ir directamente a una página que concuerde con los criterios de búsqueda. El motor de MetaWiki se creó para habilitar búsquedas en múltiples "wikis".

Las "wikis" suelen diseñarse con la filosofía de aumentar la facilidad de corrección de los errores, y no la de reducir la dificultad de cometerlos. Las "wikis" son muy abiertas, pero incluso así proporcionan maneras de verificar la validez de los últimos cambios al contenido de las páginas. En casi todos las "wikis" hay una página específica, «Cambios recientes», que enumera las ediciones más recientes de artículos, o una lista con los cambios hechos durante un período. Algunas "wikis" pueden filtrar la lista para deshacer cambios hechos por vandalismo.

Desde el registro de cambios suele haber otras funciones: el «Historial de revisión» muestra versiones anteriores de la página, y la característica «diff» destaca los cambios entre dos revisiones. Usando el historial, un editor puede ver y restaurar una versión anterior del artículo, y la característica «diff» se puede usar para decidir cuándo eso es necesario. Un usuario normal de la "wiki" puede ver el «diff» de una edición listada en «Cambios recientes» y, si es una edición inaceptable, consultar el historial y restaurar una versión anterior. Este proceso es más o menos complicado, según el software que use la "wiki".

En caso de que las ediciones inaceptables se pasen por alto en «Cambios recientes», algunos motores de "wiki" proporcionan control de contenido adicional. Se pueden monitorizar para asegurar que una página o un conjunto de páginas mantienen la calidad. A un usuario dispuesto a mantener esas páginas se le avisará en caso de modificaciones, y así se le permitirá verificar rápidamente la validez de las nuevas ediciones.

Consiste en realizar ediciones (generalmente hechas por desconocidos o gente mal intencionada) que borran contenido importante, introducen errores, agregan contenido inapropiado u ofensivo (por ejemplo, insultos) o simplemente incumplen flagrantemente las normas de la "wiki". También son frecuentes los intentos de spam, por ejemplo:


Algunas soluciones que se utilizan para luchar contra el vandalismo son:


Existen varios programas, generalmente scripts de servidor en Perl o PHP, que implementan una "wiki". Con frecuencia, suelen utilizar una base de datos, como MySQL. 

Suelen distinguirse por:

Algunos de los más utilizados son:





</doc>
<doc id="2968" url="https://es.wikipedia.org/wiki?curid=2968" title="Red de área amplia">
Red de área amplia

Una red de área amplia, o WAN, ("Wide Area Network" en inglés), es una red de computadoras que une varias redes locales, aunque sus miembros no estén todos en una misma ubicación física. 
Muchas WAN son construidas por organizaciones o empresas para su uso privado, otras son instaladas por los proveedores de internet (ISP) para proveer conexión a sus clientes. 

Hoy en día, internet brinda conexiones de alta velocidad, de manera que un alto porcentaje de las redes WAN se basan en ese medio, reduciendo la necesidad de redes privadas WAN, mientras que las redes privadas virtuales que utilizan cifrado y otras técnicas para generar una red dedicada sobre comunicaciones en internet, aumentan continuamente.

Una definición de las redes WAN, en el término de aplicación de protocolos y conceptos de redes de ordenadores, sería: tecnologías de redes de ordenadores que se utilizan para transmitir datos a través de largas distancias, y entre las diferentes redes LAN, MAN y otras arquitecturas de redes de ordenadores localizadas. Esta distinción se debe al hecho de que las tecnologías LAN comunes que operan en la capa media (como Ethernet o Wifi) a menudo están orientados a redes localizadas físicamente, y por lo tanto no pueden transmitir datos a través de decenas, cientos o incluso miles de millas o kilómetros.

Las WAN no necesariamente tienen que estar conectadas a las LAN. Puede, por ejemplo, tener un esqueleto localizado de una tecnología WAN, que conecta diferentes LANs dentro de un campus. Esta podría ser la de facilitar las aplicaciones de ancho de banda más altas, o proporcionar una mejor funcionalidad para los usuarios. 

Las WAN se utilizan para conectar redes LAN y otros tipos de redes. Así los usuarios se pueden comunicar con los usuarios y equipos de otros lugares. Muchas WAN son construidas por una organización en particular y son privadas. Otros, construidas por los proveedores de servicios de Internet, que proporcionan conexiones LAN a una organización de Internet. WAN a menudo se construyen utilizando líneas arrendadas. En cada extremo de la línea arrendada, un enrutador conecta la LAN en un lado con un segundo enrutador dentro de la LAN en el otro. Las líneas arrendadas pueden ser muy costosas. En lugar de utilizar líneas arrendadas, WAN también se puede construir utilizando métodos menos costosos de conmutación de circuitos o conmutación de paquetes. La red de protocolos incluyendo TCP/IP tiene la función de entrega de transporte y funciones de direccionamiento. Los protocolos, incluyendo paquetes como SONET/SDH, MPLS, ATM y Frame Relay son utilizados a menudo por los proveedores de servicios que ofrecen los vínculos que se usan en redes WAN. X.25 fue pronto un protocolo WAN importante, y es a menudo considerado como el "abuelo" de Frame Relay ya que muchos de los protocolos subyacentes y funciones de X.25 todavía están en uso hoy en día (con actualizaciones) por Frame Relay. 

La investigación académica en redes de área amplia puede ser dividido en tres áreas: modelos matemáticos, emulación de red y de simulación de red. 

Mejoras en el rendimiento a veces se entregan a través de los servicios de archivos de área extensa o por servicios de optimización de la WAN.

Hay varias opciones disponibles para la conectividad WAN:

Las tasas de transmisión han aumentado con el tiempo, y seguirán aumentando. Alrededor de 1960 a 110 bits/s (bits por segundo) de la línea fue normal en el borde de la WAN, mientras que los enlaces centrales de 56 kbit/s a 64 kbit/s se consideraron "rápida". En este momento (2016) los hogares están conectados a Internet con ADSL o Fibra óptica a velocidades que van desde 1 Mbit/s hasta 300 Mbit/s, y las conexiones en el núcleo de una WAN puede variar de 1 Gbit/s de 300 Gbit/s.

Recientemente, con la proliferación del bajo coste de conexión a Internet muchas empresas y organizaciones han recurrido a las VPN para interconectar sus redes, creando una red WAN de esa manera. Empresas como Citrix, Cisco, New Edge Networks y Check Point ofrecen soluciones para crear redes VPN.




Existen varios tipos de red WAN, y tres de ellos se agrupan bajo la clasificación de red conmutada (en física, la conmutación consiste en el cambio del destino de una señal o de una corriente eléctrica):

Por circuitos

Son redes de marcación de (dial-up), como la red de telefonía básica (RTB) y RDSI. Durante el tiempo que dura la llamada, el ancho de banda es dedicado.

Por mensaje

Sus conmutadores suelen ser ordenadores que cumplen la tarea de aceptar el tráfico de cada terminal que se encuentre conectado a ellas. Dichos equipos evalúan la dirección que se encuentra en la cabecera de los mensajes y pueden almacenarla para utilizarla más adelante. Cabe mencionar que también es posible borrar, redirigir y responder los mensajes en forma automática.

Por paquetes

Se fracciona cada mensaje enviado por los usuarios y se transforman en un número de pequeñas partes denominadas paquetes,que se vuelven a unir una vez llegan al equipo de destino, para reconstruir los datos iniciales. Dichos paquetes se mueven por la red independientemente, y esto repercute positivamente en el tráfico, además de facilitar la corrección de errores, ya que en caso de fallos sólo se deberán reenviar las partes afectadas. El ancho de banda es compartido entre todos los usuarios que usan la red.

Topologías de los enrutadores en una red de área amplia:





</doc>
<doc id="2970" url="https://es.wikipedia.org/wiki?curid=2970" title="Wangenheimia lima">
Wangenheimia lima

Wangenheimia lima es la única especie de planta herbácea perteneciente a la familia de las poáceas que contiene el género monotípico Wangenheimia. Es originaria de la región del Mediterráneo meridional.
Es una pequeña gramínea anual de hojas finas y acintadas. El tallo es corto y rígido y está terminado en una única espiga terminal corta y unilateral (aspecto pectinado) con espiguillas más o menos sésiles.
Es originaria de la región del Mediterráneo meridional. En España se distribuye por Alicante, Castellón, Lérida, Tarragona y Valencia, donde se encuentra en terrenos baldíos y pastizales secos muy transitados en áreas de montaña. 
"Wangenheimia lima" fue descrita por (L.) Trin. y publicado en "Fundamenta Agrostographiae" 132. 1820. 



</doc>
<doc id="2971" url="https://es.wikipedia.org/wiki?curid=2971" title="Web (desambiguación)">
Web (desambiguación)

La palabra web (del inglés: red, malla, telaraña, entramado) puede referirse a:



</doc>
<doc id="2977" url="https://es.wikipedia.org/wiki?curid=2977" title="WWW (desambiguación)">
WWW (desambiguación)

WWW puede referirse a:

</doc>
<doc id="2984" url="https://es.wikipedia.org/wiki?curid=2984" title="Weber (unidad)">
Weber (unidad)

El wéber (símbolo Wb) es la unidad de flujo magnético o flujo de inducción magnética en el Sistema Internacional de Unidades equivalente al flujo magnético que al atravesar un circuito de una sola espira produce en la misma una fuerza electromotriz de 1 voltio si se anula dicho flujo en 1 segundo por decrecimiento uniforme. Es representado simbólicamente por Wb. El nombre de esta unidad fue dado en honor al físico alemán Wilhelm Eduard Weber.

1 Wb = 1 V·s = 1 T·m = 1 m·kg·s·A.

Su equivalente en el Sistema Cegesimal de Unidades (CGS) es el máxwell. 1 máxwell = 10 Wb.

A continuación una tabla de los múltiplos y submúltiplos del Sistema Internacional de Unidades:



</doc>
<doc id="2987" url="https://es.wikipedia.org/wiki?curid=2987" title="XHTML">
XHTML

XHTML ("eXtensible HyperText Markup Language"), es básicamente HTML expresado como XML válido. Es más estricto a nivel técnico, pero esto permite que posteriormente sea más fácil al hacer cambios o buscar errores entre otros. En su versión 1.0, XHTML es solamente la versión XML de HTML, por lo que tiene, básicamente, las mismas funcionalidades, pero cumple las especificaciones, más estrictas, de XML. Su objetivo es avanzar en el proyecto del World Wide Web Consortium de lograr una web semántica, donde la información, y la forma de presentarla estén claramente separadas. La versión 1.1 es similar, pero parte a la especificación en módulos. En sucesivas versiones la W3C planea romper con los tags clásicos traídos de HTML.

Las principales ventajas del XHTML sobre el HTML son:

El estándar XHTML indica en un apéndice informativo una manera de escribir XHTML de modo tal que los navegadores actuales que sólo entienden HTML, lo procesen como si fuera éste. Para esto se deberá crear un documento con algunas restricciones y consideraciones, y servirlo con el «content-type» text/html, en vez del correcto para XHTML.

Algunas de las reglas propuestas para que XHTML «parezca» HTML son:


Para algunos autores, la inclusión de este apéndice en el estándar fue un error y consideran que es un error usar XHTML de esta manera.

La siguiente lista muestra algunas reglas de XHTML 1.0 que lo diferencian de HTML 4.01. Muchas de estas diferencias vienen con el cambio de ser una aplicación SGML a ser una aplicación del más estricto XML:





</doc>
<doc id="2988" url="https://es.wikipedia.org/wiki?curid=2988" title="XPath">
XPath

XPath ("XML Path Language") es un lenguaje que permite construir expresiones que recorren y procesan un documento XML. La idea es parecida a las expresiones regulares para seleccionar partes de un texto sin atributos ("plain text"). XPath permite buscar y seleccionar teniendo en cuenta la estructura jerárquica del XML. XPath fue creado para su uso en el estándar XSLT, en el que se usa para seleccionar y examinar la estructura del documento de entrada de la transformación.

Todo el procesamiento realizado con un fichero XML está basado en la posibilidad de direccionar o acceder a cada una de las partes que lo componen, de modo que podamos tratar cada uno de los elementos de forma diferenciada. 

El tratamiento del fichero XML comienza por la localización del mismo a lo largo del conjunto de documentos existentes en el mundo. Para llevar a cabo esta localización de forma unívoca, se utilizan los URI (Uniform Resource Identifiers), de los cuales los URL (Uniform Resource Locators) son sin duda los más conocidos. 

Una vez localizado el documento XML, la forma de seleccionar información dentro de él es mediante el uso de XPath, que es la abreviación de lo que se conoce como XML Path Language. Con XPath podremos seleccionar y hacer referencia a texto, elementos, atributos y cualquier otra información contenida dentro de un fichero 
XML.

XPath en sí es un lenguaje sofisticado y complejo, pero distinto de los lenguajes procedurales que solemos usar (C, C++, Basic, Java...). Además, como casi todo en el mundo de XML, aún está en estado de desarrollo, por lo que no es fácil encontrar herramientas que incorporen todas sus funcionalidades. 

XPath es a su vez la base sobre la que se han especificado nuevas herramientas que aprovechan para el tratamiento de documentos XML. Herramientas tales como XPointer, XLink y XQuery (el lenguaje que maneja los documentos XML como si de una base de datos se tratase). Así, XPath sirve para decir cómo una hoja de estilo debe procesar el contenido de una página XML, pero también para poder poner enlaces o cargar en un navegador zonas determinadas de una página XML, en vez de toda la página.

Un documento XML es procesado por un analizador (o parser) construyendo un árbol de nodos. Este árbol comienza con un elemento raíz, que se diversifica a lo largo de los elementos que cuelgan de él y acaba en nodos hoja, que contienen solo texto, comentarios, instrucciones de proceso o incluso que están vacíos y solo tienen atributos.

La forma en que XPath selecciona partes del documento XML se basa precisamente en la representación arbórea que se genera del documento. De hecho, los "operadores" de que consta este lenguaje nos recordarán la terminología que se utiliza a la hora de hablar de árboles en informática: raíz, hijo, ancestro, descendiente, etc. 

Un caso especial de nodo son los nodos atributo. Un nodo puede tener tantos atributos como desee, y para cada uno se le creará un nodo atributo. No obstante, dichos nodos atributo NO se consideran como hijos suyos, sino más bien como etiquetas añadidas al nodo elemento. 

A continuación se muestra un ejemplo de cómo se convierte en árbol un documento XML. Este mismo ejemplo será usado a lo largo de todo el tutorial. En primer lugar se muestra el documento XML y a continuación el árbol que genera. 

Documento XML :
Árbol generado :

Existen distintos tipos de nodos en un árbol VJB.J.LKM a partir de un documento XML, a saber: raíz, elemento, atributo, texto, comentario e instrucción de procesamiento (respectivamente; root, elements, attribute, text, comment y processing instruction). Todo esto es muy beneficioso.

Se identifica por /. No se debe confundir el nodo raíz con el elemento raíz del documento. Así, si el documento XML de nuestro ejemplo tiene por elemento raíz a libro, éste será el primer nodo que cuelgue del nodo raíz del árbol, el cual es: /. 

Insisto: / hace referencia al nodo raíz del árbol, pero no al elemento raíz del documento XML, por más que un documento XML solo pueda tener un elemento raíz. De hecho, podemos afirmar que el nodo raíz del árbol contiene al elemento raíz del documento. 

Cualquier elemento de un documento XML se convierte en un nodo elemento dentro del árbol. Cada elemento tiene su nodo padre. El nodo padre de cualquier elemento es, a su vez, un elemento, excepto el elemento raíz, cuyo padre es el nodo raíz. Los nodos elemento tienen a su vez hijos, que son: nodos elemento, nodos texto, nodos comentario y nodos de instrucciones de proceso. Los nodos elemento también tienen propiedades tales como su nombre, sus atributos e información sobre los "espacios de nombre" que tiene activos. 

Una propiedad interesante de los nodos elemento es que pueden tener identificadores únicos (para ello deben ir acompañados de un DTD que especifique que dichos atributos toman valores únicos), esto permite referenciar a dichos elementos de una forma mucho más directa.

Por texto vamos a hacer referencia a todos los caracteres del documento que no están marcados con alguna etiqueta. Un nodo texto no tiene hijos, es decir, los distintos caracteres que lo forman no se consideran hijos suyos.

Como ya hemos indicado, los nodos atributo no son tanto hijos del nodo elemento que los contiene como etiquetas añadidas a dicho nodo elemento. Cada nodo atributo consta de un nombre, un valor (que es siempre una cadena) y un posible "espacio de nombres". 

Aquellos atributos que tienen por valor el valor por defecto asignado en el DTD se tratarán como si el valor se les hubiese asignado al escribir el documento XML. Al contrario, no se crea nodo para atributos no especificados en el documento XML, y con la propiedad "#IMPLIED" definida en su DTD. Tampoco se crean nodos atributo para las definiciones de los espacios de nombre. Todo esto es normal si tenemos en cuenta que no es necesario tener un DTD para procesar un documento XML.

Aparte de los nodos indicados, en el árbol también se generan nodos para cada nodo con comentarios y con instrucciones de proceso. Al contenido de estos nodos se puede acceder con la propiedad "string-value".



</doc>
<doc id="2989" url="https://es.wikipedia.org/wiki?curid=2989" title="Yellow Dog Linux">
Yellow Dog Linux

Yellow Dog Linux (abreviado, "YDL") es una distribución Linux creada para tener soporte para el procesador PowerPC. Se lanzó por primera vez en 1999 para el Apple Macintosh Inc. Yellow Dog Linux es un producto de Terra Soft Solutions, Colorado (EE. UU.), una compañía especializada en software basado en Linux para la arquitectura Power, y está basada en CentOS y Fedora.

El 11 de noviembre de 2008 Fixstars compra Terrasoft, haciéndose con todos los derechos de "Yellow Dog Linux".

Yellow Dog Linux está principalmente dirigida a los ordenadores Macintosh de Apple, IBM BladeCenter JS2x y servidores Series P5, Mercury XR9, PlayStation 3, y varios otros sistemas y plataformas en torno a la arquitectura Power PC, como los comercializados por la propia Fixstars (como el YDL PowerStation).

La última versión estable es la 6.2 del 29 de junio de 2009 y posiblemente sea la última para PowerPC. La anterior fue la 6.1, que fue publicada el 19 de noviembre de 2008.

Yellow Dog Linux es un derivado de Fedora Core y se basa en el gestor de paquetes RPM. A través de las sucesivas versiones de Yellow Dog Linux, Terra Soft Solutions ha invertido mucho en la aplicación de apoyo específicamente en el párrafo hardware de IBM y Apple. Como resultado de ello, Yellow Dog Linux soporta aceleración de gráfica y de audio de serie, aunque algunos otros componentes de hardware, como las tarjetas inalámbricas IEEE 802.11g AirPort Extreme de Apple 802.11g (presentes en PowerBooks e iBooks) no funcionan adecuadamente sin modificaciones en el núcleo. 

Desde v5.0, Yellow Dog Linux incorpora por defecto un gestor de ventanas Enlightenment como escritorio predeterminado, aunque soporta e incorpora igualmente otros entornos como KDE, GNOME y Xfce.

Gcalctool: es una poderosa calculadora gráfica con modos financiero, lógico y científico. Utiliza un paquete de precisión múltiple para hacer la aritmética, proporcionando un alto grado de precisión.

Xpad: editor de textos con la posibilidad de poner notas en la pantalla

Evolution: gestor personal de contenidos con cliente de correo.

Gnome-dictionary: es un programa que permite acceder de forma cómoda a los diversos servidores de diccionarios en formato DICT disponibles en internet, como Freedict.de o bien hacer uso de servidores y diccionarios instalados localmente.

Gnome-terminal: es un emulador de terminal para XFree86 escrito por Havoc Pennington y otros.

Vncviewer: es una herramienta para tu dispositivo con la cual podrás visualizar y controlar el escritorio de tu PC desde tu propio dispositivo.

Screen Capture: aplicación para hacer capturas

Gedit: editor de textos se caracteriza principalmente por su facilidad de uso, conseguida en gran parte gracias a una interfaz gráfica clara y limpia, mostrando únicamente las funcionalidades principales que suelen requerir la mayoría de usuarios.

Funciones:


Distribuida en dos DVD (uno de instalación y otro de fuentes), una YDL 4.1 tiene más de 1000 paquetes.
Yellow Dog Linux 5,0 es una de las primeras distribuciones de Linux para funcionar en PlayStation 3. Se ha diseñado específicamente para HDTV SDTV con lo que los usuarios tendrán que utilizar los comandos 'installtext ' y 'ydl480i' para ser capaz de instalar y ejecutar.

Yellow Dog Linux está disponible en dos ediciones que ofrecen dos DVD (de instalación y de fuentes) por un precio entre $50 y $100 (sin o con documentación). Parte del dinero producto de estas distribuciones financia el desarrollo del sistema operativo. Los envases están diseñados para hacer coincidir el recubrimiento de policarbonato blanco de la última PowerPC, iMacs e iBooks, equipos en los que la versión de escritorio es probable que se ejecute.

Como con la mayoría de las distribuciones Linux, Terra Soft Solutions también hace Yellow Dog Linux disponible como una descarga gratuita desde FTP público.




</doc>
<doc id="2991" url="https://es.wikipedia.org/wiki?curid=2991" title="Yves Saint Laurent">
Yves Saint Laurent

Yves Henri Donat Mathieu-Saint-Laurent, más conocido como Yves Saint Laurent (Orán, Argelia, 1 de agosto de 1936-París, Francia, 1 de junio de 2008), fue un diseñador de moda y empresario francés.

Nació en Orán, por entonces colonia francesa de Argelia, en el seno de una de las más ricas familias de la ciudad. Su padre, descendiente de un barón francés, era presidente de una compañía de seguros y propietario de varias salas de cine. Su abuela materna era española. 
En Argelia, la Segunda Guerra Mundial y la ocupación nazi de Francia parecían sucesos lejanos, y no incidieron demasiado en la vida de Yves Saint Laurent y su familia. Siendo niño le gustaba interpretar personajes de Molière y leía con avidez la revista "Vogue". Le atraía el mundo de los diseños para teatro. Por su carácter "peculiar" sufrió acoso escolar, que él intentaba superar prometiéndose: «Algún día seré famoso».

En 1950, Saint Laurent envió tres diseños a París, a un concurso convocado por el "Secretariado Internacional de la Lana". Quedó en tercera posición, y acudió a recibir el premio acompañado de su madre. Sus diseños sorprendieron a Michel de Brunhoff, redactor jefe de "Vogue", que le recomendó que estudiase en la "Chambre Syndicale de la Couture". Saint Laurent le hizo caso y tras graduarse en Orán se mudó a París. 

En 1951 volvió a participar en el concurso del "Secretariado Internacional", y esta vez resultó ganador, derrotando a un joven Karl Lagerfeld. Remitió más diseños a De Brunhoff, quien vio en ellos similitudes con un diseñador consagrado: Christian Dior. El responsable de "Vogue" envió estos diseños a Dior, que vio al instante el talento de Saint Laurent y decidió sumarlo a su taller.

Con 18 años, entró a trabajar en "Dior", si bien sus tareas iniciales fueron más bien prosaicas: decorar el estudio y diseñar algunos accesorios. Sorprendentemente, Christian Dior le eligió como su sucesor en el cargo de diseñador jefe de la casa. Saint Laurent y su madre se extrañaron por la decisión de Dior, quien parecía demasiado joven para jubilarse. Moriría de un infarto ese mismo año.
En 1957, con 21 años, Saint Laurent se convirtió en el modisto más joven de la alta costura francesa. Su colección de primavera de 1958 alcanzó resonante éxito, al prolongar el estilo "New Look" acuñado por Dior. Este éxito contribuyó a "rescatar" la firma de una quiebra que parecía segura. En 1959, fue elegido por Farah Diba para que diseñase el vestido de su boda con el Sha de Irán. Pero las creaciones posteriores de Saint Laurent cosecharon duras críticas, y su carrera en "Dior" se interrumpió en el año 1960, cuando fue llamado para cumplir con el servicio militar francés, coincidiendo con la guerra de independencia de Argelia. Saint Laurent había eludido la milicia hasta entonces gracias a influencias del propietario de "Dior", Marcel Boussac, y se ha conjeturado que cuando Boussac quiso prescindir de él, movió los "hilos" necesarios para que le llamasen a filas.

Saint Laurent duró apenas 20 días en el ejército. Debido a las humillaciones infligidas por unos compañeros, sufrió un ataque de estrés y fue ingresado en un hospital militar. Allí supo que la casa "Dior" no le reservaba el empleo y que más bien había prescindido de él; esta noticia empeoró su estado emocional y fue ingresado en el psiquiátrico de Val-de-Grâce, un centro tristemente conocido por sus terapias agresivas. Saint Laurent sufrió electroshocks y le administraron sedantes y otras drogas, una etapa sombría que ayuda a explicar sus posteriores problemas emocionales y adicciones. 

A finales de 1960 Saint Laurent abandonó el psiquiátrico y, al volver a París, vio que su sustituto en la casa "Dior" era Marc Bohan, diseñador que se acercaba más al estilo "ladylike" (femenino a la antigua usanza) que se buscaba. Saint Laurent demandó a la empresa por daños morales con la ayuda de su amigo Pierre Bergé, y con el dinero recibido, sumado al apoyo financiero del empresario J. Mack Robinson de Atlanta, creó su propia casa de costura.

La primera colección de ese mismo año, "Ligne Trapéze" (‘línea trapecio’ en francés), se convirtió en un éxito instantáneo. La imagen y el logotipo de la empresa (un monograma con las iniciales YSL superpuestas) se encargaron al diseñador gráfico francés Cassandre en 1961, y siguen en uso hoy en día.

La colaboración de Pierre Bergé fue sustancial para que Saint Laurent llegase a erigir una empresa sólida. Aunque interrumpieron su relación sentimental en 1976, siguieron conviviendo en la misma casa y colaborando, y Bergé fue el apoyo imprescindible que permitió a Saint Laurent seguir creando y superar sus crisis emocionales.

Sus colecciones en los años 60 destacaron por la incorporación del esmoquin al vestuario femenino y por la implantación del "prêt-à-porter" como una línea comercial completa; de hecho fue el primer creador de alta costura que presentó una línea de esta nueva categoría de moda. En 1966 inauguró su primer local que comercializaba "prêt-à-porter". También fue el primer diseñador que incorporó mujeres de color como modelos en sus desfiles.

Diseñó decorados y vestuario para filmes y obras teatrales como "Cyrano de Bergerac" y "La Pantera Rosa", colaborando con Roland Petit, Claude Régy, Jean-Louis Barrault, Luis Buñuel, François Truffaut, Alain Resnais ("Stavisky", 1974), Jean Marais, Zizi Jeanmaire, Arletty...

Vistió a divas del cine como Jeanne Moreau, Claudia Cardinale ("La panthère rose", 1963) e Isabelle Adjani, y convirtió a Catherine Deneuve en icono de estilo y musa personal.

Sus diseños nunca dejaban indiferentes a los críticos. El desfile de otoño de 1966, inspirado en Mondrian, causó sensación, pero otras propuestas no se libraron de críticas negativas. En 1971 Saint Laurent lanzó una colección inspirada en los colaboracionistas franceses durante la ocupación nazi de Francia en los años 1940 que fue "masacrada" porque se entendió que enaltecía los tiempos de la ocupación nazi («que él no conoció») y el «feo utilitarismo de la posguerra». 

Las exigencias de producción en alta costura y "prêt-à-porter" (dos colecciones al año de cada categoría) le acarrearon un estrés creciente, que combatía con alcohol y drogas. En 1987 sufrió un "traspié" en críticas a raíz de un fallido desfile en Nueva York; exhibió chaquetas con aplicaciones de joyas de 100 000 dólares pocos días después de que un "crack" financiero sacudiese la ciudad. Desde entonces, fue delegando el diseño del "prêt-à-porter" en ayudantes, y esta gama de su producción apenas retuvo pujanza entre sus fanes.

La marca "Saint Laurent" suscitó otro escándalo en la última etapa, cuando para promocionar un perfume masculino se recurrió a una fotografía de desnudo frontal, donde un modelo posaba con los genitales visibles; fue la primera (y única) imagen de este tipo que se recuerda dentro de la publicidad de alcance global.

Saint Laurent fue también conocido por su faceta mundana; acudía a discotecas como "Studio 54", y era consumidor habitual de cocaína. Cuando dejó las drogas, sumó otra adicción más inocua: bebía al día varios litros de Coca-Cola. Su vida íntima daba que hablar, aunque su compañero Pierre Bergé siempre le apoyó y contribuyó a que la empresa no naufragase ni en los peores momentos del diseñador.

Saint Laurent y Bergé reunieron una importante colección de arte en su mansión de la calle Babylon de París, gracias en gran medida al éxito económico del perfume "Opium" (1977), el más vendido del mundo. Adquirieron obras como un importante retrato de Goya que perteneció a los Rockefeller ("El niño don Luis María de Cistué") y una escultura de madera de Constantin Brancusi (actualmente quedan solo tres en manos privadas). Sumaron pinturas de otros muchos artistas como Frans Hals, Cornelis de Vos, Ingres, Géricault, Picasso, Fernand Léger, Giorgio de Chirico, Édouard Vuillard, Edvard Munch, Matisse y Mondrian, así como dibujos y acuarelas de Manet, Paul Klee, Paul Gauguin, Degas, Toulouse-Lautrec, Alberto Giacometti y Cézanne. En fechas posteriores la pareja declaró que «con los precios actuales, ya no podemos seguir comprando».

Su afición por el arte le llevó a ‘homenajear’ a maestros como Piet Mondrian, Picasso y Braque, con vestidos que reproducen sus motivos. Una exposición en La Coruña (febrero de 2008) ilustró esta influencia en su trabajo, mostrando sus diseños junto con obras de arte que los inspiraron.

Yves Saint Laurent anunció su retirada del diseño de moda y las pasarelas en enero de 2002. Se mostró decepcionado por la moda predominante, que a su juicio arrinconaba la ambición artística a favor del simple lucro «como si fuese hacer cortinas para ventanas».

Yves Saint-Laurent falleció en París el 1 de junio de 2008, a la edad de 71 años, tras padecer cáncer. Pocos días antes, formalizó su prolongada relación con Pierre Bergé mediante una unión civil, seguramente para solucionar cuestiones de herencia. Habían mantenido su relación amistosa hasta el final, si bien viviendo separados desde 1992. 

Al funeral de Saint Laurent acudieron el entonces presidente de Francia Nicolas Sarkozy y su esposa Carla Bruni, que había trabajado como modelo durante muchos años con la firma; así como por importantes personalidades vinculadas a la moda como Valentino, John Galliano, Jean Paul Gaultier, Claudia Schiffer, Naomi Campbell , Catherine Deneuve y Farah Diba. 

Tras la muerte del diseñador, su colección de arte se dispersaba; su compañero Bergé comentó que tal conjunto se había formado como un proyecto de los dos que, al fallecer Saint Laurent, había perdido sentido. Se celebró una subasta de tres días, entre el 23 y el 25 de febrero en el Grand Palais de París a cargo de la firma "Christie's". De los 733 lotes se vendieron 730 por un total de 373 millones de euros, dinero que se destinó a una fundación y a la lucha contra el sida. El citado retrato de Goya se excluyó de la venta para ser donado al Museo del Louvre, y Bergé se quedó con algunas obras de Andy Warhol (retratos de Saint Laurent y de su perro favorito). Curiosamente, uno de los pocos lotes que no encontraron comprador fue el cuadro de Picasso "Instrumentos de música sobre una mesa", la pintura con valoración más alta. Por el contrario, un cuadro de Mondrian fue adquirido para el futuro museo Louvre Abu Dhabi.

En mayo de 2009, se ha rumoreado que la antigua vivienda de Saint Laurent y Bergé en la calle Babylon podría ser adquirida por el presidente Nicolas Sarkozy y Carla Bruni como su nueva residencia.

En 2010, se hallaron diversos dibujos hechos por Saint Laurent que databan de los años 1970 y 1980, entre estos cuadros se encuentra un retrato desnudo del líder de Queen, Freddie Mercury, posiblemente hecho en Múnich, Alemania en 1984.

Saint Laurent ha pasado a la historia como el primer diseñador de moda que ha expuesto en un museo, el Metropolitan Museum de Nueva York. Creó, con Pierre Bergé, una fundación para la custodia y difusión de su legado creativo.




</doc>
<doc id="2994" url="https://es.wikipedia.org/wiki?curid=2994" title="Yukón">
Yukón

Yukón (aunque aún se le denomina "Yukon Territory", «Territorio del Yukón», en algunos documentos ingleses) es uno de los tres territorios que, junto con las diez provincias, conforman las trece entidades federales de Canadá. Su capital es Whitehorse. Está ubicado en el extremo noroeste del país, limitando al norte con el océano Ártico, al este con Territorios del Noroeste, al sur con Columbia Británica y al oeste con Alaska (Estados Unidos). Con 31 530 habs. en 2008 es la segunda entidad menos poblada —por delante de Nunavut— y con 0,06 hab/km², la tercera menos densamente poblada, por delante de Territorios del Noroeste y Nunavut, la menos densamente poblada. 

La cresta de los montes Mackenzie da forma a gran parte de la frontera oriental. 

La etimología de su nombre proviene de una lengua aborigen local, el gwich'in, y quiere decir "río grande". El territorio es famoso entre otras cosas por haber sido el escenario de la Fiebre del oro de Klondike, un hecho histórico que ocurrió en 1897 y que fue de gran trascendencia para la región.

Se han encontrado restos humanos de la que fuera la población más antigua y primitiva de Norteamérica, aunque su datación es bastante discutida. Una gran cantidad de huesos modificados y de rasgos humanoides que fueron descubiertos en la región de Old Crow, al norte del Yukón, tienen entre 25 000 o 40 000 años de antigüedad, según el estudio por el método del carbono 14. El centro y norte del Yukón no fueron afectados por las glaciaciones, como sí lo fue parte de Beringia.

Una erupción volcánica en Mount Churchill, próximo a la frontera con Alaska, cubrió de cenizas el sur de Yukón. Aquel depósito de brasas y restos de magma puede apreciarse aún a lo largo de la autopista de Klondike. Las historias de las Primeras Naciones del Yukón hablan sobre la muerte de los animales y peces a consecuencia de este suceso, al igual que otras provenientes de las tribus de lenguas atabasca, navajo y apache, lo que ha llevado a muchos antropólogos a la conclusión de que la emigración de pueblos athabaskanos a lo que es hoy conocido como el suroeste de Estados Unidos pudo haber sido consecuencia de dicha erupción. Poco después, las innovaciones en la tecnología de la caza favorecieron la sustitución de los atlatles por el arco y la flecha.

Se desarrollaron redes extensivas de comercio e intercambio entre los tlingits de la costa y las "Primeras Naciones" del interior. Se cree que los primeros intercambiaban aceite de eulacón y otros productos de su entorno por cobre y pieles de las "Primeras Naciones".

Los primeros europeos en visitar este territorio fueron los españoles, quienes realizaron sus exploraciones iniciales a partir del siglo XVI. La parte Oeste de las costas del Pacífico fueron territorios en disputa, explorados y de escasos asentamientos del imperio español, Yukón junto con la provincia de Columbia Británica, pasó a formar también parte del Virreinato de la Nueva España. Más adelante fue cedido al imperio británico.

Las incursiones europeas en lo que más tarde se conocería como el Yukón dieron comienzo en la primera mitad del siglo XIX. Los exploradores y comerciantes de la Compañía de la Bahía de Hudson, que llegaron desde los puestos comerciales del río Mackenzie, emplearon dos rutas diferentes para penetrar en el territorio, creando puestos comerciales por toda la ruta. La ruta del norte nacía en Fort McPherson, Territorios del Noroeste, a orillas del Mackenzie, cruzaba las montañas por los ríos Bell y Porcupine y llegaba hasta el río Yukón. La del sur comenzaba, en cambio, en Fort Liard (Territorios del Noroeste), y se dirigía hacia el oeste por el río Liard hasta el lago Frances, seguía hacia el oeste siguiendo el curso del río Liard hasta el lago Frances, y luego por el río Pelly hasta que éste desembocaba en el Yukón.

Tras fundar Fort McPherson, John Bell cruzó las montañas para llegar a la cuenca del Yukón en 1845, y descendió por el río Rat (hoy conocido como Bell) hasta su confluencia con el Porcupine. Tras organizar el comercio de pieles en Fort McPherson regresó al Bell, y siguió río abajo por el Porcupine hasta llegar de nuevo al río Yukón, en el lugar en que más adelante se levantaría el fuerte del mismo nombre (Fort Yukón). No mucho tiempo después, Alexander Hunter Murray estableció puestos comerciales en Lapierre House (1846), y en Fort Yukon (1847), en la confluencia de los ríos Porcupine y Yukón. Murray hizo varios dibujos de las tiendas de venta de piel y de los habitantes de la zona, y escribió el "Journal of the Yukon, 1847–48" (Diario del Yukón), una valiosa fuente de información acerca de la cultura local de los gwich’in en la época. Como el puesto estaba en realidad en la Alaska rusa, la Compañía de la Bahía de Hudson continuó su actividad comercial allí hasta que fue expulsada tras la adquisición de Alaska por Estados Unidos en 1869. Un nuevo puesto comercial, Rumpert House, fue establecido aguas arriba del Porcupine, pero se demostró que se hallaba también dentro de Alaska. Los gwich’in, especialmente bajo el liderazgo de Sahneuti, enfrentaron a la Compañía de la Bahía de Hudson con los comerciantes estadounidenses de la Compañía Comercial de Alaska.

Por esa misma fecha, Robert Campbell, procedente de Fort Simpson, exploró buena parte del sur del Yukón y fundó Fort Frances (1842) sobre el lago homónimo en la cuenca del río Liard, y Fort Selkirk (1848) en la confluencia de los ríos Yukón y Pelly. En 1852, Fort Selkirk fue saqueado por guerreros tlingit de la costa, que se opusieron violentamente a esta injerencia en su actividad comercial. A raíz de este incidente, Fort Selkirk quedó abandonado y no se restableció hasta 1889.

Los misioneros anglicanos y católicos siguieron la estela del comercio de pieles, siendo digno de mención William Carpenter Bompas, que se convirtió en el primer obispo anglicano del Yukón. Por su parte, los misioneros católicos pertenecían a la orden de los Oblatos de María Inmaculada, aún presente en el territorio. 

En 1859, Robert Kennicott emprendió una expedición para recolectar especímenes de historia natural en los valles del hoy llamado río Mackenzie y del río Yukón, y en la tundra ártica. Kenicott adquirió popularidad entre los traficantes de la Compañía de la Bahía de Hudson, y los incentivó a buscar especímenes de historia natural y objetos manufacturados por las "Primeras Naciones" y a enviar lo recabado al Instituto Smithsoniano. En 1865, se organizó la "Expedición del Telégrafo de Western Union", con el fin de encontrar alguna ruta posible para establecer una línea de telégrafo entre Norteamérica y Rusia a través del mar de Bering. Kennicott fue el jefe científico de la expedición, y entre el grupo de naturalistas que la integraban se encontraba W. H. Dall. Lamentablemente, Kennicott falleció de un ataque cardíaco cuando remontaba el río Yukón. Sin embargo, sus esfuerzos dieron a conocer al mundo este territorio canadiense. 

A pesar de los rumores sobre la presunta presencia de oro en la región yukoniana, no se procedió a grandes investigaciones. Tras la compra de Alaska por parte del gobierno de Estados Unidos y el consecuente abandono de Rampart House, los comerciantes de la Compañía Comercial de Alaska empezaron a trabajar en el curso superior del río Yukón. Tres mineros — Alfred Mayo, Jack McQuesten y Arthur Harper — habiendo oído de estos rumores, se unieron a los trabajos de la compañía, si bien su principal interés radicaba en la búsqueda de oro. En 1874, Mayo y McQuesten fundaron Fort Reliance, a unos kilómetros río abajo de lo que sería más tarde Dawson City. Otros mineros y buscadores se adhirieron pronto a la empresa, y se encontró oro en muchas áreas, aunque raramente en cantidades suficientes como para que supusiera un buen negocio. Hacia 1885, una buena cantidad de este metal fue hallada en el río Stewart, y McQuesten logró convencer a la compañía de que empleara a los mineros en lugar de centrar su actividad en el comercio de pieles. Al año siguiente, se encontraron cantidades rentables de oro en bruto en el río Fortymile, y se fundó un nuevo puesto comercial, llamado también Fortymile, en la confluencia de este río con el Yukón. 

Paralelamente, el Ejército de los Estados Unidos envió al teniente Frederick Schwatka para reconocer el río Yukón para el ejército estadounidense. Atravesando el Paso de Chilkoot, la expedición construyó balsas y navegó por el Yukón hasta su estuario en el mar de Bering, dando nombre a muchas zonas geográficas durante ese tramo. La expedición de Schwatka alarmó al gobierno canadiense, que envió a su propio grupo de expedicionarios al mando de George Mercer Dawson en 1887. William Ogilvie, un agrimensor que saltaría a la fama durante la fiebre del oro de Klondike, midió los terrenos para fijar con precisión la frontera natural con Alaska.

En 1894, preocupado por la afluencia de mineros estadounidenses y el tráfico de licor, el gobierno canadiense encargó al inspector Charles Constantine de la Policía montada del Canadá investigar las condiciones bajo las que se encontraba el distrito del Yukón. Constantine declaró que se acercaba una fiebre del oro y reclamó, de forma urgente, la presencia de una fuerza policial que fuera capaz de controlar la zona. Un año después, regresó al Yukón en compañía de 20 hombres que se encontraban allí cuando comenzó la "Fiebre del oro de Klondike", en 1897.

La "Fiebre del oro de Klondike" constituye un hecho crucial en la historia del Yukón. Un grupo comandado por Skookum Jim Mason descubrió oro en un afluente del río Klondike en agosto de 1896. Entre 30.000 y 40.000 personas desafiaron un sinfín de dificultades para alcanzar el yacimiento de oro de Klondike en el otoño, invierno y la primavera de 1897-1898, después de que el hallazgo se hiciera oficial en 1897. Con la afluencia de inmigrantes estadounidenses, el gobierno canadiense decidió crear un territorio separado para controlar mejor la situación. En 1901, tras el regreso de muchos a sus hogares, el censo arrojaba una población de 27.219 habitantes, una cifra que no volvería a alcanzarse hasta 1991. La masiva afluencia de inmigrantes en la región estimuló la exploración minera en otras partes del Yukón y propició dos "fiebres del oro" de menor importancia en Atlin (Columbia Británica) y Nome (Alaska), así como varias pequeñas incursiones. La necesidad de transporte hacia los campos de oro llevó a la construcción del ferrocarril de White Pass y Yukón.

El escritor estadounidense Jack London reflejó la vida de los buscadores de oro en varias de sus novelas y relatos. Fue seducido, como tantos otros, por la idea de hacerse rico en poco tiempo, pero después de pasar varios meses allí enfermó de escorbuto y regresó con las manos vacías. Algunos de sus mejores cuentos ambientados en la dura vida del Norte son "La hoguera" ("To buil a fire", 1908), "El silencio blanco" ("The white silence", 1899), "El filón de oro" ("All gold canyon", 1905), "El amor a la vida" ("Love of life", 1905), y las novelas "La llamada de la selva" y "Colmillo Blanco".

Al término de la fiebre de oro, la población del territorio declinó rápidamente, alcanzando un mínimo de 4.157 en 1921 y permaneciendo bastante estable hasta los años 1940. Esto a pesar del desarrollo de otras áreas mineras, incluyendo yacimientos de plata en Conrad y especialmente en Mayo, de oro en la región del lago Kluane, y de cobre cerca de Whitehorse. En Klondike, los derechos de varios mineros particulares fueron adquiridos y consolidados, con apoyo del gobierno, por un reducido número de compañías, entre ellas la "Yukon Gold Corporation" de Solomon R. Guggenheim, que utilizaba dragas flotantes. La "Yukon Consolidated Gold Company" ("Compañía del Oro Consolidada del Yukón") continuó dragando en busca de oro hasta la década de 1960, disfrutando de un breve período de prosperidad durante los años 1930 con la subida del precio del oro.

Hacia 1920 el consejo territorial electo había quedado reducido a tres miembros, y Yukón pasó a ser gobernado por un Comisionado del Oro, funcionario federal dependiente del Ministerio del Interior de Canadá.

El siguiente hecho importante en la historia del Yukón fue la construcción, durante la Segunda Guerra Mundial, de la autopista Alaska, la cual, tras su renovación llevada a cabo por el gobierno canadiense a fines de la década de 1940, abrió el territorio al tráfico por carretera. La guerra también fue testigo de la construcción de varios aeródromos como parte de la plataforma de la ruta del Noroeste. No obstante, la afluencia de trabajadores para las obras de la autovía del sur tuvo efectos devastadores para algunas de las Primeras Naciones, que sufrieron un gran número de muertes al verse expuestas a enfermedades a las que no eran inmunes.

En las décadas de 1950 y 1960 se construyeron otras autopistas, lo que tuvo como consecuencia la decadencia y consecuente desaparición de los barcos fluviales, que habían sido hasta entonces el principal medio de transporte en la zona. En la segunda mitad del siglo XX, la "White Pass & Yukon Route" (Ruta de White Pass y Yukón) inició los fletes de transporte intermodal en contenedores. La minería también resucitó, incluyendo la explotación de cobre en Whitehorse, plata y plomo en Keno City y Elsa, y asbesto en Clinton Creek. La mayor mina de cinc a cielo abierto de todo el mundo se abrió en Faro a comienzos de los años 1970. La minería del oro regresó a Klondike y a otras zonas con la importante subida de los precios de este metal en los años 1970.
Entre las décadas de 1980 y 1990, la minería decayó y el papel del gobierno aumentó considerablemente, con transferencias económicas que fueron incrementando su importancia. En 1978, se consiguió establecer un gobierno responsable y partidos políticos que lo sustentaran. Por otro lado, las "Primeras Naciones" comenzaron a organizarse e iniciaron negociaciones para hacer valer sus derechos territoriales, que culminaron con la firma del "Umbrella Final Agreement" (Acuerdo Final de Umbrella) en 1992. Aunque la mayoría de las "Primeras Naciones" firmaron acuerdos, sus reivindicaciones territoriales y de autogobierno continúan en la actualidad. Las "Primeras Naciones" son consideradas actualmente un cuarto nivel de gobierno, y la naturaleza específica de las relaciones intergubernamentales es un aspecto en el que se sigue trabajando.

El territorio del Yukón se localiza en el extremo noroeste de Canadá. Escasamente poblado, destaca por su paisaje natural de lagos de hielo derretido y montañas perennemente nevadas, entre las que se encuentran muchas de las más altas de Canadá. El clima es ártico, subártico y muy seco, con largos inviernos y breves veranos. Sin embargo, las prolongadas horas de sol en el estío son suficientes para el florecimiento de brotes y frutos comestibles. La mayor parte del territorio se encuentra cubierta de bosques y matorrales boreales, siendo la tundra el tipo más común de paisaje sólo en el extremo septentrional y sobre elevaciones altas. El campo de hielo no polar más grande del mundo, el Kluane, se sitúa mayoritariamente en el Yukón. 

La región yukoniana posee una forma similar a la de un triángulo rectángulo, limitando con el estado de Alaska al oeste, con los Territorios del Noroeste al este, y con la provincia de Columbia Británica al sur. Comprende una superficie aproximada de 482.443 km, de los cuales 474.391 km son tierra y 8.052 km, agua. 

El Yukón está delimitado por el paralelo 60º de latitud en el sur. Su costa norte se halla sobre el mar de Beaufort, y su ribera occidental se circunscribe a los 141 grados oeste de longitud. Su límite este (algo desparejo) sigue la división del curso fluvial que subyace entre la cuenca del río Yukón y el estuario del Mackenzie hasta la cadena montañosa del mismo nombre. La totalidad del Yukón se ubica al oeste de Vancúver, por lo que alberga a las comunidades más occidentales de Canadá. 
Excepto por la llanura costera del mar de Beaufort (océano Ártico), el resto del territorio forma parte de las Montañas Rocosas. El terreno incluye cadenas montañosas, mesetas y valles fluviales. 

El suroeste se encuentra dominado por los campos nevados de Kluane ("Kluane National Park and Reserve"). Las montañas de Saint Elias, también en esta zona, representan a cinco de las más altas elevaciones canadienses. Un buen número de glaciares emergen de estos campos, entre los que destacan el Logan, el Hubbard y el Kaskawulsh.

Asimismo, el permafrost es muy frecuente, sobre todo en el norte y en el centro (donde se halla más extendido). El sur, en cambio, carece de grandes concentraciones de este fenómeno, sólo viéndose en escarchas heladas bastante aisladas. 

Dos grandes fallas, la de Denali y la de Tintina, han sido las responsables de la creación de vastos valles denominados trincheras: Shakwak y Tintina. La primera separa a las cordilleras de Kluane de otras montañas del norte. Tanto la vía de Haines como la de Alaska, en el punto septentrional de la intersección Haines, fueron construidas sobre esta trinchera. La de Tintina rodea al Yukón de noroeste a sureste, y sus orillas son ricas en depósitos minerales, como los yacimientos de oro de Klondike o los de plomo y zinc cercanos a Faro. 

Fuentes: Precisiones geológicas del Yukón en "Yukon Geoprocess File User Guide" (archivo PDF, 1.2MB)
Las montañas de Saint Elias forman parte del relieve costero que se extiende desde el sur de la Columbia Británica hasta Alaska, abarcando al sureste del Yukón. Mientras que éstas son las más elevadas, existe otra buena cantidad de montes como, por ejemplo, las Montañas Británicas bien al norte, que componen la cordillera de Brooks; las Montañas Mackenzie y Richardson al este; las de Cassiar al sureste; las de Pelly en el centro; y las Montañas Ogilvie en el Yukón nórdico, pasando Dawson City por la vía de Dempster. 

Las cordilleras del Yukón incluyen:


La mayor parte del territorio se encuentra en la cuenca del río homónimo, que desemboca en el Mar de Bering. El sur del Yukón es abundante en lagos glaciares, angostos y alpinos, que fluyen, en gran medida, hacia el sistema del Yukón. Los más grandes son: Teslin, Atlin, Tagish, Marsh, Laberge, Kusawa, y Kluane. El lago Bennett, donde se dio la fiebre del oro, es de menor tamaño y muere en el Tagish.

Otra gama de ríos y riachuelos desembocan en el océano Pacífico o directa o indirectamente en el Ártico. El drenaje del Alsek-Tatshenshini sigue su curso hacia el Pacífico desde el suroeste territorial, y los dos ríos principales del Yukón, el Liard y el Peel, fluyen hacia el Mackenzie y terminan en los Territorios del Noroeste (sureste y noreste respectivamente).

El clima predominante es el subártico, caracterizado por inviernos profundos y breves veranos templados. La pista de aterrizaje de Snag, 25 kilómetros al este de Beaver Creek, sobre la frontera con Alaska, experimentó la temperatura más baja registrada en la historia de Norteamérica, -63,0 °C (-81,4 °F) el 3 de febrero de 1947. La costa del océano Ártico posee un clima polar. El promedio yukoniano es el de una constante seca, de pocas precipitaciones, y ligeramente húmedo en el sureste. Las lluvias son más abundantes en las montañas, y el hielo comienza a derretirse en verano, propiciando las caídas intensas de agua en los meses de julio y agosto. 

La tundra predomina en casi todo el Yukón. De acuerdo con las definiciones de la ecozona de Environment Canada, el sur y centro del territorio representan la "Ecozona de la Cordillera Boreal", mientras que el bosque del norte constituye la "Ecozona de la Cordillera de Taiga". El sector del río Peel en el noreste forma parte de la llanura taigana, y el litoral ártico, de la "Ecozona del Ártico meridional".

La pícea negra ("Picea mariana") y la blanca ("Picea glauca"), el aspa ("Populus tremuloides") y el álamo balsámico ("Populus balsamifera") están presentes a lo largo de todo el territorio. Aunque poco común, el abedul de Alaska ("Betula neoalaskana") puede encontrarse también en el panorama vegetal del Yukón. Una variedad de conífera ("Pinus contorta") alcanza su extremo septentrional y central-meridional, mientras que el tamarack ("Larix laricina") se remonta al sureste, y el abeto subalpino ("Abies lasiocarpa") a las elevadas alturas de la región sur.

Los mamíferos más grandes son: el caribú ("Rangifer tarandus", de baldío y de bosque), el alce ("Alces alces"), el lobo ("Canis lupus"), el oso pardo ("Ursus arctos horribilis") y el oso negro americano ("Ursus americanus"). En la altura se pueden ver ovejas ("Ovis dalli") y, en sur, a la cabra de las Rocosas ("Oreamnos americanus"). Los osos polares ("Ursus maritimus") se adentran en la costa ártica. El ciervo mulo ("Odocoileus hermionus") y su predador, el puma ("Puma concolor"), se han vuelto muy populares en el sur, y los coyotes ("Canis latrans") han ganado terreno en el norte. El uapití y el bisonte fueron introducidos tardíamente.

Hay un gran número de roedores, incluyendo ardillas, ardillas de tierra, lemmings, pikas, castores, varias ratas de campo, puercoespines, ratas almizcleras, etc. Los mustélidos son también muy representativos: glotón ("Gulo gulo"), marta ("Martes americana"), mustela erminea ("Mustela erminea"), comadreja ("Mustela nivalis"), visón americano ("Mustela vison"), y nutria de río ("Lontra canadensis"). Otros pequeños carnívoros son: el lince canadiense ("Lynx canadensis"), el zorro rojo ("Vulpes vulpes") y el zorro polar ("Alopex lagopus") en el litoral norteño. 

Más de 250 especies de aves sobrevuelan el Yukón. La grajilla común ("Corvus corax") es el ave más extendida. Entre otros pájaros autóctonos encontramos el águila calva ("Haliaeetus leucocephalus"), el águila real ("Aquila chrysaetos"), el halcón rústico ("Falco rusticolus") y el halcón peregrino ("Falco peregrinus"), y cinco tipos de urogallo (de la pícea, azul, gorguera, ptarmigano, y el ptarmigano de cola blanca). Muchas aves migratorias se aparean y crían en el Yukón, como ocurre en el perímetro norte del Pacífico.

Además del burbot y el lucio norteño, muchos de los peces que habitan las aguas yukonianas son salmónidos. Cuatro especies de salmón viven en los ríos y cuencas del Yukón, y en los lagos del Pacífico. El río Yukón tiene la corriente de agua más fresca para el hábitat de cualquier salmón; el chinook nada río arriba unos 3.000 kilómetros desde su boca en el Mar de Bering hasta Whitehorse, donde deposita sus huevas. También hay salmones "sockeye" y truchas arcoiris en lagunas interiores.

Los salvenilus se componen de truchas de lago, presentes en gran parte de las lagunas del Yukón, así como de truchas autóctonas (Dolly Varden, toro y del Ártico). El pez grisáceo polar es ubicuo, mientras que los lagos poseen varios corégonos e inconos. 

No hay reptiles en el territorio, salvo por unas pocas ranas.

El Yukón posee abundantes fuentes minerales, siendo la minería el principal pilar de su economía hasta hace poco tiempo. No en vano, el oro encontrado en Klondike provocó la fiebre de este metal en 1897. Actualmente se encuentra oro en muchos arroyos y ríos, habiendo industrias que se dedican a su explotación activa.

Otros minerales que se han hallado en mayor o menor medida son: cobre en la región de Whitehorse, plomo y zinc, en Faro, y estos dos últimos más algunos agregados de plata, en Mayo y Kenik. Se ha descubierto además asbesto en Clinton Creek, y cobre, oro, y carbón en la zona de Carmacks. El yacimiento de tungsteno más grande del mundo se encuentra en Macmillan Pass, en las montañas Mackenzie, próximo a la frontera con los Territorios del Noroeste. Los minerales no metálicos incluyen al jade y a la baritina.

La venta de pieles había sido el sostén económico de las tribus de las Primeras Naciones, pero la baja de precios y el crecimiento de las críticas por parte del sector defensor de los animales terminaron por poner fin a dicha actividad.

El Yukón dispone de tres centrales hidroeléctricas: una en Schwatka Lake, en Whitehorse, otra cerca de Mayo y una tercera en Aishihik Lake.

Mientras que los bosques predominan en el paisaje, la mayoría de los árboles son pequeños y de lento crecimiento a consecuencia del clima frío seco. Se practica la silvicultura a pequeña escala, siendo en el sur donde se perciben los aportes industriales más fructíferos debido a su ligera humedad. Sin embargo, la distancia que existe respecto al mercado y los elevados precios han resultado en una empresa poco provechosa. 

Una pequeña cantidad de gas natural se produce en el sureste, si bien poco se ha explorado en otros puntos del territorio. Se cree que puede haber grandes reservas de gas en el área de Eagle Plains sobre la vía de Dempster Highway, y posiblemente en zonas allegadas a Whitehorse, pero, una vez más, la distancia desde el gasoducto ha obstaculizado la investigación.

El calentamiento global está afectando más al norte que a cualquier otro punto del planeta, y el Yukón no es la excepción. Si bien es cierto que sus residentes recibirían con entusiasmo una buena temporada de calor, los efectos colaterales de tal fenómeno son desconocidos. La subida de las temperaturas incurriría en el aumento de la evaporación y en la sequía de un ambiente que es de por sí árido, provocando incendios forestales y reduciendo la productividad biológica de los bosques boreales, cuyo crecimiento se ve más limitado a la falta de humedad que a la de una temperatura favorable. 

El territorio es además el blanco de la contaminación proveniente de otros sectores del mundo, sobre todo de origen orgánico, por lo que el consumo de animales salvajes y de pescado ya no es aconsejable. 

Localmente, la demanda minera y su correspondiente explotación son causantes de la aparición de ácido en los confines de su campo laboral, costando cientos de millones de dólares en reparos y limpieza.

En un intento por incentivar la investigación de recursos naturales, en 2005 el Partido del Yukón, liderado por Dennis Fentie, ha suspendido la protección de áreas que se hallaban anteriormente respaldadas por el gobierno del Partido Demócrata, y ha señalado su intención de no crear parques restringidos adicionales. 

La tribu gwichʼin de Old Crow depende del caribú de Porcupine para autoabastecerse con comida y abrigo, como otras tantas del entorno. Esta especie se traslada a las llanuras de la costa para aparearse en el "Refugio Salvaje Nacional del Ártico" (Arctic National Wildlife Refuge) en Alaska. Esa manada suele ser seriamente castigada por la actividad petrolera del ambiente.

De escasa población, y con cerca de 30.000 habitantes en un territorio casi tan extenso como España o Suecia, Yukón tiene una densidad de 0,06 personas por km². Cerca de tres cuartas partes de la población se concentran en la zona de Whitehorse, y el resto vive en otras pequeñas comunidades. Todas ellas, excepto Old Crow, son accesibles por carretera. 

La capital, Whitehorse, es también su ciudad más grande y poblada; la segunda en importancia es Dawson City (1.800 hab.), que fue la primera en población hasta 1952. 

Tradicionalmente, el Yukón estaba habitado por tribus atabascas de las Primeras Naciones, que habían establecido fuertes redes de comercio con los tinglits del Pacífico. Se estima que el 20% de su población actual es de origen indígena. Los inuit que habitaban en las costas del Ártico se extinguieron a causa de una epidemia. 

El siguiente cuadro presenta la población de muchas comunidades territoriales. Es importante especificar que el censo de 2001 comprende a todos aquellos que residen entre los límites comunitarios, mientras que el Departamento de Estadísticas del Yukón ("Yukon Bureau of Statistics" YBS) incluye a todo ciudadano con dirección de correo postal. Comúnmente, muchas personas viven en las afueras de los polos urbanos, de ahí que haya más registros en el YBS. 
 
 

Población del Yukón desde 1901:
"Fuente: Statistics Canada" 

Al ser un territorio federal, los idiomas oficiales del Yukón son el inglés y el francés, aunque de acuerdo al censo de 2011 y al igual que en el resto de Canadá, a excepción de Quebec (francés mayoritario) y Nunavut (inuktitut mayoritario), el idioma más hablado es el inglés ya que el 82.9% de los habitantes del territorio la tienen como lengua materna.

La economía está basada en los recursos minerales (plomo, zinc, plata, oro, asbesto, cobre, tungsteno, jade y baritina). La industria de fabricación, incluyendo los muebles, ropa y artesanías, le sigue en importancia, junto con la hidroelectricidad. En la actualidad, el sector público es, de lejos el mayor empleador en el territorio, dando empleo directo a aproximadamente 5.000 personas de una fuerza laboral de 12.500.

El mayor atractivo del Yukón es su naturaleza casi virgen y el turismo del lugar depende en gran medida de esto, hay muchos proveedores de equipo organizado y guías a disposición de los cazadores y pescadores y amantes de la naturaleza de todo tipo. Los amantes del deporte pueden remar en los lagos y ríos con canoas y kayaks, tomar rutas de viaje o de paseo, esquiar o hacer snowboarding en un entorno organizado o acceder en moto de nieve, escalar las cumbres más altas de Canadá o tener una pequeña caminata por las montañas, o tratar de escalada en hielo y trineos tirados por perros.

Yukón también tiene una amplia gama de actividades culturales y deportivas que atraen a artistas, participantes y turistas de todas partes del mundo.

Antiguamente, el principal medio de transporte era la red fluvial del río Yukón, tanto antes como después de la Fiebre del Oro. Los tinglits de la costa comerciaban con la gente atabasca empleando caminos montañosos.

Desde la fiebre del oro hasta los años 1950, las embarcaciones navegaron por el Yukón, principalmente entre Whitehorse como punto de concentración y Dawson City, algunos incluso llegando a Alaska y al mar de Bering, mientras que otros lo hacían por el río principal (el Yukón) y sus afluentes (río Stewart, etc.).

Muchos barcos pertenecían a la compañía de navegación del Yukón británico, una extensión de las rutas del White Pass (y Yukón), que operaba asimismo en un área reducida entre Skagway (Alaska) y Whitehorse. El ferrocarril dejó de funcionar en la década de 1980 con la primera clausura de la mina de Faro. Hoy en día sólo es utilizado como medio de transporte y traslado de turistas durante el verano, y no en la totalidad del territorio.

En tiempos actuales, el principal medio de comunicación terrestre lo representa, sin duda alguna, la autopista Alaska, que atraviesa Whitehorse. La ruta de Klondike comprende el tramo que va desde Skagway, pasando por la capital yukoniana hasta llegar a Dawson City; la de Haines se extiende desde la ciudad del mismo nombre en Alaska hasta Haines Junction, Yukón; y la de Dempster lo hace desde la ruta de Klondike hasta Inuvik, en los Territorios del Noroeste. Todas estas carreteras, a excepción de la última, se hallan pavimentadas. Otros caminos menos transitados son el Campbell, que comunica Camaracks con Watson Lake, en la vía de Alaska; y el "Silver Trail" que interseca con la ruta de Klondike sobre el puente del río Stewart para unirse a las viejas comunidades mineras de plata en Mayo, Elsa y Keno City. La casi totalidad de los municipios del Yukón son accesibles por carretera, siendo el transporte aéreo la única forma de llegar a la remota comunidad de Old Crow en el extremo norte.

El aeropuerto internacional de Whitehorse sirve de conexión con otras regiones cercanas, entre ellas Vancúver, Calgary, Edmonton, Fairbanks, Juneau y Frankfurt (en verano). Cada comunidad posee un aeropuerto, y la empresa de vuelo está esencialmente al servicio del turismo y las prospecciones mineras.

En el siglo XIX, Yukón fue parte primero del Territorio Noroeste administrado por la Compañía de la Bahía de Hudson, y luego de los Territorios del Noroeste gobernados por Canadá. Consiguió cierto nivel de autogobierno solo en 1895, cuando se convirtió en un distrito separado de los Territorios del Noroeste. En 1898 se convirtió en un territorio separado, con su propio Comisionado y Consejo Territorial. 

Antes de 1979, Yukón era administrado por un Comisionado nombrado por el Ministro federal de Asuntos Indígenas y Desarrollo del Norte. Este comisionado presidía y jugaba un cierto papel en el nombramiento de un Consejo ejecutivo, cuyas competencias eran únicamente consultivas. En 1979, el gobierno federal y el comisionado delegaron parte de su poder en una asamblea territorial que, en ese año, adoptó un sistema de partidos de gobierno responsable. Este trámite fue efectuado a través de una carta acreditativa expedida por el ministro Jake Epp, más que por un procedimiento legislativo.

La "Yukon Act", aprobada el 1 de abril de 2003, formalizó los poderes del gobierno del Yukón y añadió una serie de competencias adicionales de índole territorial al gobierno del territorio (por ejemplo, el control sobre la tierra y los recursos naturales). Desde 2003, excepto en materia de persecuciones criminales, el Yukón dispone casi de los mismos poderes que los gobiernos provinciales, algo que también están buscando los otros dos territorios (Noroeste y Nunavut). Hoy, el papel del Comisionado es análogo al de un Teniente-Gobernador provincial; no obstante, a diferencia de estos últimos, los comisionados no representan a la Reina de Canadá, sino que son empleados del gobierno federal. 

En previsión del gobierno responsable, se organizaron partidos políticos y se postularon candidatos para la Asamblea Legislativa del Yukón por primera vez en 1978. Los Conservadores Progresistas ganaron las elecciones y constituyeron el primer gobierno de partido en enero de 1979. El Nuevo Partido Democrático del Yukón se mantuvo en el poder desde 1985 hasta 1992, bajo el mandato de Tony Penikett, y de nuevo en 1996, con Piers McDonald, hasta su derrota en el 2000. Los conservadores regresaron al poder en 1992 de la mano de John Ostashek, tras cambiar su nombre por el de Partido del Yukón. El gobierno de Pat Duncan, del Partido Liberal del Yukón, fue derrotado en las elecciones de noviembre de 2002 frente a Dennis Fentie del Partido del Yukón, quien fue nombrado "premier". 

Si bien se han presentado debates en torno a si el Yukón pudiera ser considerado como la undécima provincia de Canadá, se suele poner de relieve su escasez poblacional, razón más que suficiente para desmerecer dicha categorización. Como resultado, el gobierno de la Columbia Británica ha propuesto en un sinfín de ocasiones el hacerse cargo de su administración. 

A nivel federal, el territorio está representado en el Parlamento de Canadá por un diputado y un senador. A diferencia de lo que sucede con Estados Unidos, todos los miembros del Parlamento tienen el mismo valor, y los residentes en Yukón gozan de los mismos derechos que otros ciudadanos canadienses. Uno de los representantes del Yukón en el Parlamento —Eric Nielsen— fue el diputado primer ministro durante el gobierno de Brian Mulroney, mientras que otro —Audrey McLaughlin— fue el líder del Nuevo Partido Demócrata. 

El Yukón fue una de las nueve jurisdicciones de Canadá en proponer el matrimonio entre homosexuales antes de la aprobación de la Ley de Matrimonios Civiles canadiense, junto con Ontario, Columbia Británica, Quebec, Manitoba, Nueva Escocia, Saskatchewan, Terranova y Labrador y Nuevo Brunswick.

La gran mayoría de la población forma parte de las Primeras Naciones. En 1991 se firmó un acuerdo territorial entre 7.000 representantes de catorce pueblos indígenas diferentes y el gobierno federal; desde entonces, cada "nación" de forma particular debe negociar sus reclamaciones específicas de tierras y de autogobierno. En noviembre de 2005, 11 de 14 "Primeras Naciones" han firmado acuerdos con el gobierno. A continuación se ofrece una lista de las 14 Primeras Naciones: 

El territorio tuvo una vez un establecimiento inuit, localizado en Herschel Island sobre la costa del Ártico. El mismo fue desmantelado en 1987 y sus habitantes trasladados a los Territorios del Noroeste. Como consecuencia del Acuerdo Final Inulaviut, la isla es hoy un parque territorial y se conoce oficialmente como Qikiqtaruk, el nombre de la isla en idioma inuktitut.



En inglés:



</doc>
<doc id="2999" url="https://es.wikipedia.org/wiki?curid=2999" title="Zygophyllaceae">
Zygophyllaceae

Las Zigofiláceas (Zygophyllaceae) son plantas herbáceas o raramente leñosas, a menudo xerófilas y halófilas (plantas que crecen en ambientes salinos). 
Son hierbas, arbustos o árboles, anuales o perennes; ramas usualmente divaricadas, con nudos angulados o abultados, simpódicas; plantas hermafroditas. Hojas opuestas u ocasionalmente alternas, usualmente paripinnadas, a veces simples o 2-folioladas, raramente 3–7-folioladas, persistentes, frecuentemente carnosas a coriáceas, pecioladas a subsésiles; folíolos enteros u ocasionalmente lobados, inequiláteros, peciolulados a subsésiles; estípulas apareadas, libres, persistentes o raramente caducas, foliáceas o carnosas o espinescentes. Flores (4) 5-meras, hipóginas, regulares o a veces ligeramente irregulares; pedúnculos terminales o pseudoaxilares, con 1 flor, solitarios u ocasionalmente varios juntos; sépalos (4) 5, libres o ligeramente connados en la base, imbricados cuando en yema, persistentes o a veces deciduos; pétalos (4) 5, libres o raramente connados en la base, frecuentemente unguiculados, a veces retorcidos e imbricados o convolutos, deciduos, raramente marcescentes; disco glandular extrastaminal y/o intrastaminal usualmente presente y conspicuo; estambres en (1) 2 verticilos de 5 cada uno, el verticilo más externo usualmente opuesto a los pétalos, con frecuencia verticilos alternamente desiguales o estériles, filamentos libres, subulados a filiformes o raramente alados, frecuentemente glandulosos o con apéndices en la base, los del verticilo exterior ocasionalmente adnados a los pétalos, insertos en el disco o debajo de éste, las anteras ditecas, subbasifijas a versátiles, introrsas, longitudinalmente dehiscentes; gineceo (2–) 5-carpelar, sincárpico, ovario súpero, (2–) 5 (–10)-lobado, (2–) 5 (–10)-locular, sésil o raramente en un ginóforo corto, los óvulos (1) 2–numerosos por lóculo, péndulos o ascendentes, anátropos, placentación axial o raramente basal, estilo terminal, usualmente simple, estigma menuda e inconspicuamente lobado a obviamente acostillado. Frutos cápsulas (2–) 5-lobadas, loculicidas o septicidas, o esquizocarpos que se separan longitudinalmente en 5 ó 10 mericarpos duros, tuberculados a espinosos o alados, o raramente drupas o bayas; semillas 1 (–numerosas) por lóculo, endosperma presente o ausente, embrión con cotiledones aplanados.
Existen más de 280 especies de los países cálidos. Algunas especies son:




</doc>
<doc id="3000" url="https://es.wikipedia.org/wiki?curid=3000" title="Zoología">
Zoología

Zoología (del griego «ζωον» "zoon" = "animal", y «-λογία» "-logía", tratado, estudio, ciencia) es la disciplina biológica que se encarga del estudio de los animales. La zoología estudia diversos ámbitos como la biología, fisiología, morfología, comportamiento, distribución y ecología de cada una de las especies.

El interés del hombre por los animales y por la gran diversidad de sus formas comenzó en la antigüedad. En Grecia, en el siglo IV a.C., Aristóteles describió numerosas especies y realizó un esbozo de clasificación del reino animal; pero muchas de sus conclusiones carecían de rigor científico, pues no estaban basadas en experimentaciones.

Con el Renacimiento, las investigaciones zoológicas adoptaron carácter verdaderamente científico y se desecharon algunas teorías aristotélicas y muchos conceptos fantasiosos sostenidos hasta entonces. La invención del microscopio por el holandés Anton van Leeuwenhoek permitió abordar el estudio de los tejidos de los animales y de seres hasta entonces desconocidos porque eran demasiado pequeños para ser observados a simple vista: los microbios o microorganismos.

Ya avanzado el siglo XVIII, el sueco Carl von Linné fue el primero en encarar una clasificación sistemática de los animales y las plantas. Su obra fue continuada por el naturalista francés Georges Cuvier. En 1859 Charles Darwin dio a conocer su teoría de la evolución de las especies, que significó un gran aporte a los estudios zoológicos.

Se encarga de todos los aspectos genéricos y comunes que poseen los animales antes de proceder a una descripción taxonómica.

Una vez que se ha estudiado el mundo animal en los aspectos embriológicos, histológicos, funcionales, etc. Cabe describir un prototipo para cada una de las especies, pero previamente es imprescindible proceder a la exposición de una serie de múltiples consideraciones relativas de la historia denominada clasificación sistemática.

Es el estudio de las plantas y en el de los animales, los especialistas se interesan tanto por las semejanzas como por las diferencias que presentan las especies para lograr la agrupación lógica y sistemática de las mismas. Admitido este procedimiento clasificatorio, es evidente la necesidad de adoptar una nomenclatura que sea universalmente comprometida para superar de este modo la limitación que supondrían las denominaciones locales o nacionales.

La taxonomía abarca la exploración y tabulación sistemática de los hechos concernientes al reconocimiento de todas las especies existentes y extintas de animales y su distribución en el espacio y el tiempo.

Las principales variedades de trabajadores zoológicos situados bajo éste encabezado son:


Gradualmente, desde los tiempos de Hunter y Cuvier, el estudio anatómico se ha ido disociando cada vez de la morfografía, hasta que al día de hoy nadie considera de valor un estudio animal que no incluya en su enfoque la estructura interna, la histología y la embriología.

El auténtico surgimiento de la zoología después del período legendario de la Edad Media está ligado al nombre de un inglés, Edward Edward Wotton, nacido en Oxford en 1492, quien ejerció como médico en Londres y murió en 1555. Publicó un tratado titulado "De differentiis animalium" en París en 1552. En muchos sentidos Wotton era simplemente un exponente de Aristóteles, cuya doctrina (con varias adiciones imaginarias), constituyera la verdadera base de conocimiento zoológico a lo largo de la Edad Media. El mérito de Wotton fue el rechazó de los argumentos legendarios y fantásticos, y su regreso a Aristóteles y a la observación de la naturaleza.

El método más efectivo para notar el progreso de la zoología durante los siglos XVI, XVII y XVIII es comparar las concepciones clasificatorias de Aristóteles con las de los sucesivos naturalistas, aquellos que pueden ser encontrados en las obras de Caldon.




</doc>
<doc id="3004" url="https://es.wikipedia.org/wiki?curid=3004" title="Zosteraceae">
Zosteraceae

Las zosteráceas (nombre científico Zosteraceae) son una familia de plantas monocotiledóneas marinas sumergidas, con aspecto de hierbas, perennes, rizomatosas, monoicas, distribuidas en los mares templados a subtropicales de todo el mundo. La familia es reconocida por sistemas de clasificación modernos como el sistema de clasificación APG III (2009) y el APWeb (2001 en adelante). Es una de las familias a las que se llama de "pastos marinos", debido a su aspecto de pastos, sus espádices son aplanados, sus flores están en la superficie adaxial, las inflorescencias están encerradas por una espata. La mayoría de los taxones posee unas estructuras opuestas a los estambres a las que se llamó "retináculos", que quizás sean tépalos o brácteas.
Son hierbas perennes, acuáticas, sumergidas, marinas, glabras, con hojas dispuestas en braquiblastos y macroblastos, con polinización en el interior del agua –hipohidrofilia. Hojas alternas, diferenciadas en vaina y limbo, liguladas, lineares, obtusas, paralelinervias. Inflorescencias en espádice. Flores hermafroditas o unisexuales –en especies extrapeninsulares–, monómeras, sésiles, sin perianto, o con perianto muy reducido (retináculo) situado junto a una de las tecas. Androceo con 1 estambre, con el conectivo poco desarrollado entre las dos tecas. Gineceo súpero, con 1 carpelo; carpelos con un rudimento seminal, péndulo, de placentación apical. Fruto aqueniforme, con pico diferenciado, sésil, dehiscente por rotura del pericarpo membranáceo. Semillas ± estriadas, con el embrión curvado, sin endosperma.

La familia fue reconocida por el APG III (2009), el Linear APG III (2009) le asignó el número de familia 38. La familia ya había sido reconocida por el APG II (2003).

Hay dos géneros:



</doc>
<doc id="3005" url="https://es.wikipedia.org/wiki?curid=3005" title="Zannichelliaceae">
Zannichelliaceae

Las zaniqueliáceas (nombre científico Zannichelliaceae) son una familia de hierbas perennes, rizomatosas, monoicas, que habitan aguas dulces o saladas. Hojas alternas u opuestas, filiformes. Flores pequeñas, unisexuales, solitarias o dispuestas en inflorescencias falciformes, aclamídeas o de perianto cupuliforme; las masculinas con 13 estambres; las femeninas de gineceo con 1 hasta 9 carpelos libres. Frutos aqueniformes o nuciformes. Se conocen unas 7 especies, extendidas por una gran parte del mundo. Esta familia era reconocida en las clasificaciones
"clásicas" de las angiospermas, y aun hoy algunos botánicos lo continúan haciendo. En los sistemas de clasificación modernos como el sistema de clasificación APG II del 2003, el APG III (2009
), y el APWeb (2001 en adelante), los géneros que pertenecían a esta familia ("Althenia", "Lepilaena", "Pseudalthenia", "Vleisia", "Zannichellia") se disponen dentro de las Potamogetonáceas. 



</doc>
<doc id="3007" url="https://es.wikipedia.org/wiki?curid=3007" title="Zea">
Zea

El género Zea comprende varias especies de poáceas o gramíneas de origen americano, de las cuales la única que cuenta con valor económico es "Z. mays" ssp. "mays", conocida como "maíz", un cereal de alto valor energético cultivado para el consumo humano y animal. Todas las otras especies y subespecies reciben el nombre común de teosinte. 
Son plantas anuales robustas o perennes, cespitosas o rizomatosas; tallos con muchos entrenudos, sólidos, a menudo con raíces fúlcreas; plantas monoicas. Hojas en su mayoría caulinares; lígula una membrana; láminas grandes, lineares, aplanadas. Inflorescencias unisexuales; inflorescencia estaminada una panícula de racimos, terminal, entrenudos del raquis no articulados, delgados; espiguillas estaminadas pareadas, unilaterales, una espiguilla de cada par sésil o subsésil, la otra pedicelada, los pedicelos libres, glumas herbáceas, multinervias, flósculos superiores e inferiores similares, ambos estaminados, lema y pálea hialinas, lodículas 3, estambres 3; inflorescencia pistilada una espiga solitaria, axilar, delgada, envuelta en 1–numerosas espatas, entrenudos del raquis desarticulándose, hinchados, espiguillas pistiladas sésiles, solitarias, dísticas en 2 hileras, profundamente hundidas y casi envueltas por el entrenudo del raquis (cúpula), callo oblicuo, truncado o aplanado, gluma inferior endurecida, lisa, inconspicuamente alada en la punta, gluma superior membranácea, flósculo inferior estéril, lema inferior pequeña, hialina, pálea inferior pequeña, hialina, flósculo superior pistilado, lodículas ausentes, estilo y estigma solitarios, muy largos, las puntas extendiéndose más allá de las espatas envolventes. Fruto una cariopsis; hilo punteado. En Zea mays ssp. mays la inflorescencia pistilada es una mazorca masiva, dura, fibrosa, entrenudos del raquis no desarticulándose, espiguillas pareadas, sésiles, polísticas en 4–36 hileras, insertadas superficialmente en la mazorca, callo agudo, glumas membranáceas, flósculo inferior generalmente estéril o raramente pistilado, flósculo superior pistilado, lemas y páleas membranáceas.

Varias otras especies del género, conocidas colectivamente como teosintes, han desarrollado un aspecto similar al del maíz como contramedida a su erradicación selectiva por los granjeros.

Las especies del género "Zea" presentan por lo general un tallo hueco, similar al del bambú, del que, según la especie, pueden derivar o no ramificaciones. Si bien "Z. nicaraguensis" y "Z. perennis" son perennes, la mayoría de las especies son anuales. Pese a su breve ciclo vital, alcanzan varios metros de altura.

Virtualmente todas las poblaciones de teosinte están amenazadas o en riesgo: "Zea diploperennis" existe en un área de solo unos pocos km²; "Zea nicaraguensis" sobrevive en aproximadamente una sola subpoblación de 6000 plantas en un área de 200×150 m. Los gobiernos de México y de Nicaragua han reaccionado recientemente para proteger las poblaciones silvestres de teosinte, usando tanto métodos de conservación "in-situ" y "ex-situ". Hay mucho interés científico en estos trabajos benéficos, y en otros como resistencia a insectos, perennialismo, tolerancia a inundación. 

El género fue descrito por Carlos Linneo y publicado en "Species Plantarum" 2: 971. 1753. 
Zea: nombre genérico que sería una voz de origen griego, derivada de "zeo" que significa "vivir". Pero Plinio el Viejo ("Historia naturalis","18, 81") emplea el término "Zĕa, æ" para referirse a "Triticum spelta" (espelta, también conocido como escaña mayor o escanda mayor).
Comprende 40 especies descritas y de estas, solo 6 aceptadas.



</doc>
<doc id="3008" url="https://es.wikipedia.org/wiki?curid=3008" title="Zaragoza">
Zaragoza

Zaragoza es una ciudad y un municipio de España, capital de la comarca de Zaragoza, de la provincia homónima y de la comunidad autónoma de Aragón. 

Con una población de 664 938 habitantes (INE, 2017)—697 895 según los datos del padrón municipal de 2018—, es la quinta ciudad más poblada de España, tras Madrid, Barcelona, Valencia y Sevilla. 

Está situada a orillas de los ríos Ebro, Huerva y Gállego y del Canal Imperial de Aragón, en el centro de un amplio valle. Su privilegiada situación geográfica la convierte en un importante nudo logístico y de comunicaciones; se encuentra a unos 300 km de Madrid, Barcelona, Valencia, Bilbao y Toulouse. Parte de su término municipal está ocupado por la Reserva natural dirigida de los Sotos y Galachos del Ebro. 

Su nombre actual procede del antiguo topónimo romano, Caesaraugusta, que recibió en honor al emperador César Augusto en el 14 a. C. El origen de la ciudad se remonta a Salduie, que fue el nombre de la ciudad ibérica sedetana situada en el solar de la actual Zaragoza desde la segunda mitad del siglo III a. C. Está documentado en monedas ibéricas y con el nombre de «Salduvia» en un texto de Plinio el Viejo. Con la fundación de Caesar Augusta, la ciudad-estado íbera pasó a convertirse en colonia inmune de ciudadanos romanos. Su denominación romana fue evolucionando a través del árabe Saraqusta.

La ciudad ostenta los títulos de Muy Noble, Muy Leal, Muy Heroica, Muy Benéfica, Siempre Heroica e Inmortal, otorgados en su mayoría tras su resistencia frente al ejército napoleónico en los Sitios de Zaragoza durante la Guerra de la Independencia. Todos estos títulos quedan reflejados en su escudo, mediante las iniciales de cada uno de ellos. Celebra su fiesta mayor en honor a la Virgen del Pilar el 12 de octubre. El patrón de la ciudad es San Valero (29 de enero). Entre el 14 de junio y el 14 de septiembre de 2008, año del bicentenario de los Sitios de Zaragoza y centenario de la exposición Hispano-Francesa de 1908, Zaragoza acogió la exposición internacional Expo Zaragoza 2008 dedicada al agua y el desarrollo sostenible. En 2010, Zaragoza fue la quinta ciudad española con más turismo. Además, la ciudad de Zaragoza fue sede del Secretariado de Naciones Unidas para la Década del Agua 2005-2015.

Se asienta en mitad del valle del Ebro, en la ribera media del Ebro, en el punto en el que desembocan los ríos Huerva y Gállego, los cuales también atraviesan la ciudad. 

En la ribera y en el área urbana, el terreno es llano por lo general, sobre todo en la parte norte de la ciudad asentada en la margen derecha de la desembocadura del río Gállego, mientras que la sur tiene una inclinación más pronunciada conforme se aleja del Ebro. La altura del río Ebro a su paso por Zaragoza es de 199 msnm, aunque gran parte de la ciudad está por encima de los 210 msnm y los barrios del sur como Torrero y La Paz, se encuentran a más de 250 msnm. La ribera está rodeada de escarpes, cornisas o cárcavas dando lugar a relieves abruptos, en ocasiones con fuertes desniveles. En contraste altitudinal respecto al área metropolitana, dentro del municipio, destacan la Plana de Moses: un segmento meridional de los Montes de Castejón con 680 msnm; y las Planas de María con 645 msnm, resultado de la dureza de sus materiales geológicos a la erosión. 

Los materiales que afloran en la ciudad y su entorno son fundamentalmente gravas, arenas y arcillas producto de la propia sedimentación del Ebro durante el Cuaternario, que se disponen en terrazas fluviales de varios metros de espesor sobre las unidades de yesos y limos depositadas durante el Terciario. Estos materiales se consideran evaporitas, al formarse por evaporación de las aguas en extensas lagunas de una depresión endorreica que delimitaban las cordilleras pirenaica, ibérica y costero catalana. Esta zona endorreica se abrió al mediterráneo por la erosión remontante de uno o varios barrancos, precursores del actual río Ebro.

La naturaleza salina de las evaporitas, unida a una pluviometría escasa y un fuerte coeficiente de evaporación, han favorecido el desarrollo de una singular vegetación esteparia en el entorno de Zaragoza, que constituye una rareza botánica y paisajística a nivel europeo, si bien es poco valorada en general por sus habitantes. El contraste entre este paisaje y la vegetación exuberante de los sotos fluviales es uno de los atractivos de los descensos en piragua que pueden realizarse aguas arriba y aguas abajo de la ciudad.

De acuerdo con la clasificación climática de Köppen, Zaragoza tiene un clima semiárido frío (BSk), propio de la depresión del Ebro. Los inviernos son ligeramente fríos, siendo normales las heladas nocturnas (23 días de heladas de media al año) y las nieblas que produce la inversión térmica en los meses de diciembre y enero. Los veranos son cálidos, superando casi siempre los 35 °C e incluso pasando los 40 °C muchos días. Las lluvias escasas se concentran en primavera. El promedio anual es bastante escaso, de unos 315 mm influenciado sobre todo por el efecto foehn o efecto adiabático. Las temperaturas más altas desde que existen registros históricos son los 44,5 °C del 7 de julio de 2015; los 43,1 °C del 22 de julio de 2009; los 42,8 °C del 26 de agosto de 2010 y los 42,6 °C del 17 de julio de 1978 y las más bajas -15,2 °C registrada tanto el 1 como el 18 de enero de 1918, -14,9 °C, del 31 de diciembre de 1887 y -11,4 °C registrada el 5 de febrero de 1963. Zaragoza tiene de media 2,4 días de nieve al hallarse situada a poca altitud.

Según la Agencia Estatal de Meteorología, la velocidad media del viento es de 19 km/h. El cierzo sopla con frecuencia durante el invierno y a comienzos de la primavera.

La ciudad de Zaragoza cuenta con más de dos mil años de historia. La población más antigua documentada data del siglo VII a. C., en los restos de unos poblamientos del final de la Edad del Bronce. Las primeras noticias de un asentamiento urbano datan de la segunda mitad del siglo III a. C. y nos hablan de una ciudad ibérica llamada Salduie que se identifica con el nombre de «Salduvia» en un texto de Plinio el Viejo. 

La ciudad romana de Caesaraugusta fue una colonia inmune refundada sobre la ciudad ibera por Octavio Augusto con veteranos de las guerras cántabras entre el año 25 y el 12 a. C., muy probablemente el 14 a. C. Entonces tenía una planta rectangular y una extensión de 47 hectáreas, coincidente con el trazado urbanístico del actual casco antiguo, y su perímetro estaba delimitado por la calle del Coso al sur y al este, la avenida de César Augusto al oeste y el río Ebro al norte. Poco después se convirtió en el centro urbano más importante del valle medio del Ebro. La ciudad no decayó durante el Bajo Imperio romano de Occidente.

El año 452 fue conquistada por los suevos y el 466 por los visigodos, quienes la incorporaron al reino de Tolosa. En 541, fue asediada por los francos, aunque la ciudad no llegó a caer en su poder.

En el siglo VII su sede episcopal conoció un periodo de esplendor con las figuras de los obispos Braulio de Zaragoza y Tajón. El año 714 fue ocupada por el sarraceno Musa ibn Nusair y se convirtió en un centro musulmán importante llamado «Medina al-Baida Saraqusta» (Zaragoza la Blanca), que Carlomagno intentó ocupar sin éxito el año 788.

En el siglo IX los Banu Qasi, familia noble de origen visigodo convertida al islam al poco de la invasión, la designaron capital de sus extensos dominios poco después del 852 con Musa ibn Musa, conocido en la tradición cristiana como el «Moro Muza». Más tarde, el emir Mohamed I de Córdoba se la compró en 884 por 15 000 dinares de oro. En 890, obtuvieron la hegemonía los tuyibíes, yemeníes provenientes de las zonas de Calatayud y Daroca, y Muhammad Alanqar pasó a ser el gobernador de Zaragoza.

Capital de la frontera superior con los reinos cristianos bajo el califato Omeya, gozó de cierta autonomía respecto de Córdoba. La Zaragoza musulmana del siglo X acogió a comunidades de las otras religiones del Libro en la judería y el barrio mozárabe. Tras la descomposición del califato andalusí, se erigió en la capital de un importante reino, la Taifa de Saraqusta, en 1018, con el reinado del tuyibí Mundir I.
El periodo de esplendor de la ciudad islámica se dio en el siglo XI, especialmente con el reinado de Al-Muqtadir (1046-1081), ya perteneciente a la dinastía de los Banu Hud, quien amplió su reino con la anexión de la taifa de Tortosa y la taifa de Denia y sometió a vasallaje a la de taifa de Valencia. Construyó un espléndido palacio fortificado de recreo: la Aljafería, cuyas obras comenzaron en 1065. La dinastía hudí consiguió mantener su independencia frente al Imperio almorávide y a la presión de un joven reino de Aragón, hasta que en 1110 la ciudad tuvo que ser entregada al poder morabita, que puso al frente de la gobernación de la urbe al exregidor de Valencia Muhammad ibn al-Hayy. En 1115 le sustituyó Ibn Tifilwit, quien nombró visir al gran filósofo Avempace.

Con la ayuda de sus aliados occitanos y cruzados francos, y su ejército de aragoneses, Alfonso I el Batallador pudo conquistar Zaragoza en 1118, que se convertiría pronto en la capital del Reino de Aragón, y fue la sede en la que se coronaron los reyes de la Corona de Aragón. La población musulmana se tuvo que trasladar fuera de los muros de la ciudad, donde fundó el nuevo barrio de la morería, mientras que el núcleo urbano era repoblado por francos y dado en feudo a Gastón IV de Bearne.

Desde el final del siglo XIII fue el centro de la Unión Aragonesa (asociación de nobles para limitar el poder real y mantener sus privilegios), hasta que esta fue derrotada por Pedro el Ceremonioso el año 1384. La unión dinástica de la Corona de Castilla y la de Aragón la transformó en una ciudad más de la monarquía de los Austrias. El establecimiento de la Inquisición fue causa de importantes revueltas y del asesinato del inquisidor Pedro Arbués en 1485. En el siglo XV se incorporaron a la ciudad los arrabales de labradores de San Pablo y de pescadores de las Tenerías. Durante el reinado de Fernando el Católico se fundó la universidad y se construyó la Lonja. La expulsión de los judíos en 1492 y de los moriscos en 1609 provocaron un cierto estancamiento en su crecimiento, pero a pesar de eso, no dejó de ser una ciudad importante (con 25 000 habitantes en 1548).

Fue escenario de revueltas a causa del encarcelamiento de Antonio Pérez, secretario de Felipe II, que procesado por orden del rey, se acogió a la protección de los Fueros Aragoneses el año 1591. Los disturbios acabaron con la ejecución del Justicia Juan de Lanuza y la introducción de algunas restricciones en sus privilegios. Durante la Guerra de Sucesión, la ciudad, en defensa de las libertades y soberanía de Aragón, de sus instituciones y del Derecho aragonés, se declaró partidaria del archiduque Carlos de Austria. Al ser conquistada por las tropas borbónicas, perdió la autonomía de la que había disfrutado hasta aquel momento (1707), y que solo pudo recuperar brevemente en 1710, al derogarse sus fueros por los Decretos de Nueva Planta, con lo que la ciudad dejó de ser sede de importantes instituciones del Reino de Aragón.

Durante el siglo XVIII la población pasó de 30 000 habitantes en 1725 a 43 000 en 1787. En 1760 se produjo un motín paralelo de Esquilache, y en 1776 se fundó la Sociedad Económica de Amigos del País.

Durante la Guerra de la Independencia Española (1808-1814), Zaragoza resistió los enfrentamientos con las tropas francesas. En la guerra contra Napoleón se hizo famosa por toda Europa por sus asedios, siendo un símbolo de la resistencia a Napoleón. En el primer asedio (junio-agosto de 1808), el [[general Verdier tuvo que desistir de tomarla. En el segundo asedio (final diciembre de 1808-21 de enero de 1809) capituló después de una serie de combates violentísimos, donde la población colaboró de forma heroica con las tropas de los defensores, a las órdenes de [[José de Palafox]], que se encerró con 30 000 hombres. Moncey y después Lannes dirigieron el segundo asedio. Se calcula que murieron 8000 franceses y 40 000 defensores, ya que dentro de la ciudad se propagó una epidemia de tifus. 
[[Archivo:Zaragoza antigua, plaza del Mercado, en El Museo Universal.jpg|thumb|left|200px|Plaza del Mercado de Zaragoza, 1867, en "[[El Museo Universal]]".]]
Durante las [[Guerras Carlistas]] el general carlista [[Juan Cabañero]] intentó ocupar la ciudad la madrugada del 5 de marzo de 1838, pero fue rechazado por la guarnición. El 2 de enero de 1854 hubo un intento frustrado de pronunciamiento. 

El [[Pandemias de cólera en España|cólera de 1885]] causó muchas víctimas. Sin embargo, el año 1900 la ciudad tenía unos 100.000 habitantes. También en el siglo XIX se produjeron las primeras transformaciones importantes que han configurado la ciudad actual: el emplazamiento de la estación de ferrocarril (estación del Norte), que generó un núcleo residencial e industrial, y la construcción paulatina del paseo de la Independencia (iniciado en 1815), con sus porches, que creó un eje que iba desde el Coso hasta la Huerta de Santa Engracia y articulaba el crecimiento hacia lo que constituiría el ensanchamiento de principios del siglo XX, con la [[Gran Vía (Zaragoza)|Gran Vía]] y el [[Paseo de Sagasta (Zaragoza)|paseo de Sagasta]] como calles principales. A finales del siglo XIX se convirtió en el foco de una fuerte inmigración rural atraída por el reciente proceso de industrialización de la ciudad.

Tras 1898, el azúcar de caña que suministraban las colonias perdidas comenzó a ser sustituido por el derivado de la remolacha, lo que originó en Zaragoza el auge de las industrias azucareras y la aparición de una burguesía que animó los movimientos regeneracionistas de la Liga Nacional de Productores (1899) y la Unión Nacional (1900) de [[Joaquín Costa]] y [[Basilio Paraíso]]. La acumulación de capital provocada por el fuerte aumento de los beneficios dio origen a la creación de varias entidades financieras regionales, como el [[Banco Zaragozano]] o el [[Banco de Aragón]]. Se produjo una inmigración rural, la aparición de un proletariado y el crecimiento urbanístico de la ciudad. La expansión económica trajo consigo períodos de carestía e inflación y provocaron las consecuentes reivindicaciones obreras. Estas fueron desatendidas por una burguesía local intransigente amparada en la represión violenta de las fuerzas del orden. Todo ello propició el fracaso del reformismo obrero y el crecimiento sostenido de la afiliación a la [[Confederación Nacional del Trabajo]]. Zaragoza fue durante las primeras décadas del siglo XX, tras Barcelona, la segunda ciudad cenetista de España. Al inicio de la [[Guerra Civil Española]], el 19 de julio de 1936 los sublevados, mandados por el general [[Miguel Cabanellas]], tomaron fácilmente el control de la ciudad. En 1937, los republicanos emprendieron la [[ofensiva de Zaragoza]] para tratar de recuperar el control de la ciudad, sin éxito. 

Durante la dictadura franquista se reabrió la [[Academia General Militar]] y se instaló la [[Confederación Hidrográfica del Ebro]]. Después de diversos planes urbanísticos que completaron el trazado del siglo XIX, se produjo en los últimos treinta años del siglo un enorme crecimiento del casco urbano con la superación de la barrera natural que constituye el Ebro, y que ha llevado a la construcción de populosos nuevos barrios. Desde la segunda mitad del siglo XIX hasta nuestros días, Zaragoza ha seguido pujante, siendo actualmente la quinta ciudad de España en términos demográficos.

Desde la fundación de [[Caesaraugusta]] como colonia inmune romana sus límites acogieron una población que se podría situar en torno a los 20 000 habitantes. Durante el periodo visigodo su pujanza como ciudad clave en el norte de la Península no decayó, si acaso la ciudad pudo disminuir en algo su población.
[[Archivo:Demografía Zaragoza (España).PNG|thumb| Evolución demográfica de Zaragoza (1900-2005).]]

La llegada de los musulmanes a la península y la creación de la marca y posterior [[Taifa de Zaragoza]] hizo de Saraqusta una de las capitales más importantes de los [[primeros reinos de taifas]]. En su época de máximo esplendor (hacia 1080) dominaba, como centro de un reino islámico fundamentalmente urbano, gran parte del Levante peninsular, incluyendo la costa de Tortosa y Denia. Por aquel tiempo Zaragoza pudo llegar a contar con 25 000 habitantes, e incluso, los cálculos más optimistas, llevarían a la ciudad a los cerca de 50 000.

Es difícil establecer un cálculo preciso sobre las variaciones experimentadas durante la etapa medieval cristiana. Tras la reconquista de [[Alfonso I de Aragón]] en 1118 la población islámica fue obligada a habitar en los arrabales, quedando la antigua medina para los nuevos pobladores. Parte de la población, sobre todo la que pertenecía a estratos sociales más elevados, marchó hacia [[al-Ándalus]]. Se sabe que el [[siglo XII]] fue un periodo de crisis, lo que unido a la emigración comentada, pudo hacer descender notablemente la población, pese a los privilegios jurídicos que los reyes aragoneses se esforzaron en promulgar para estimular su asentamiento en la capital del reino. En todo caso, a fines del siglo XV Zaragoza recuperó la población que tuvo en el máximo apogeo de la capital del reino taifal con 20 000 habitantes aproximadamente.

La primera mitad del siglo XVI contempla un Renacimiento ciudadano, con la construcción de [[Lonja de Zaragoza|La Lonja]] y la presencia de un paisaje urbano dominado por numerosas torres de estilo mudéjar, que valieron a Zaragoza el apelativo de «la de las cien torres» o la «Florencia española». Así, en 1548 la ciudad cuenta con 25 000 habitantes. No debió de haber cambios significativos en la demografía durante el siglo XVII, etapa de crisis demográfica en general en España y en particular en Aragón, donde a pestes, hambrunas y crisis económica se unió la [[Expulsión de los moriscos|expulsión definitiva de los moriscos]] en 1609.

En cambio, durante el siglo XVIII (siglo en que comienzan las primeras estimaciones censales de la demografía) la población experimentó un importante auge, y evolucionó de unos 30 000 habitantes en 1725 a los 43 000 en 1787.

Pero la [[Guerra de la Independencia Española|Guerra de la Independencia]] y los [[Sitios de Zaragoza|dos sitios que sufrió]] por parte de las [[Grande Armée|tropas napoleónicas]] asolaron Zaragoza que, de los 55 000 ciudadanos que la habitaban en 1808, pasó a 12 000 supervivientes. A pesar de todo ello, el dinamismo de la capital del [[valle medio del Ebro]] le permitió recuperarse y hacia 1850 ya tenía 60 000 habitantes; muy notablemente continuó este ascenso demográfico en la segunda mitad del siglo XIX en que se vio favorecida por la actividad industrial (azucareras de remolacha e industria alimentaria fundamentalmente) y comercial que llevó su población a superar los 100 000 habitantes a comienzos del siglo XX.

A la llegada de la [[Segunda República Española|Segunda República]], la ciudad frisa los 200 000 habitantes y en 1960 alcanza los 300 000. Pero el verdadero despegue demográfico se produce entre 1960 y 1980, un intervalo de veinte años en los que Zaragoza casi duplica su población, rebasando a mediados de los ochenta el medio millón de almas y a finales las 600 000. En la primera década del siglo XXI la urbe siguió creciendo, en parte por su condición de imán de la población aragonesa, que emigra desde las zonas rurales más empobrecidas de la región, y según el padrón municipal en 2018 tenía una población de 697 895 habitantes.

Desglose de población según el Padrón Continuo por Unidad Poblacional del [[Instituto Nacional de Estadística (España)|INE]].

En 2013 vivían en Zaragoza 107 864 extranjeros, lo que suponía el 15% del total. Desde 2004 hasta 2013 dicha cifra pasó de 43 355 a 107 864, es decir, aumentó casi un 150%. Los barrios con mayor población extranjera eran Delicias (25 428 habitantes, el 23% del total del distrito) y el Casco Histórico (11 881, el 25%).

Acuerdos de investidura y/o coaliciones de gobierno

Desde 2015, el gobierno municipal corresponde a [[Zaragoza en Común]] ([[Izquierda Unida de Aragón|IU]], [[Podemos (partido político)|Podemos]], [[Equo]], [[Puyalón de Cuchas]], [[Partido Pirata (España)|Piratas de Aragón]], [[Demos+]] y [[Somos (partido político)|Somos]]). El [[Partido de los Socialistas de Aragón|PSOE]] y la [[Chunta Aragonesista|CHA]] votaron a favor de Pedro Santisteve en la sesión de investidura, pero no gobiernan. En la oposición también se encuentran el [[Partido Popular de Aragón|PP]] y [[Ciudadanos - Partido de la Ciudadanía|Ciudadanos]].

De acuerdo con el "Reglamento de Órganos Territoriales y Participación Ciudadana", de 28 de julio de 2005, Zaragoza se divide en 15 distritos; el número quince, el Distrito Rural, comprende catorce barrios rurales:

El concepto de deuda viva contempla solo las deudas con cajas y bancos relativas a créditos financieros, valores de renta fija y préstamos o créditos transferidos a terceros, excluyéndose, por tanto, la deuda comercial.

La deuda viva municipal por habitante en 2015 ascendía a 1608,65 €.

[[Archivo:WTCZ World Trade Center Zaragoza.JPG|thumb|upright|[[World Trade Center Zaragoza]].]]
Zaragoza es la cuarta ciudad de España según su [[Economía de España|Índice de Actividad Económica]]. Los sectores estratégicos de la economía maña son la industria del automóvil, la logística y los transportes, las energías renovables, los servicios a empresas, la agroindustria y el turismo. En la economía de la ciudad ocupa un lugar destacado la fábrica de [[Opel]] ([[General Motors]]) en [[Figueruelas]], una localidad del área metropolitana, alrededor de la cual se ha desarrollado un entramado de industrias auxiliares del motor.
En el terreno industrial también sobresalen:

Destaca el transporte de mercancías del [[aeropuerto de Zaragoza]], que en 2017 cargó 117.000.000 kg, lo hace colocarse el tercer mejor aeropuerto español (en cuanto a mercancías) tan solo por detrás de Madrid y disputándose el segundo puesto con Barcelona 

Proyectos como la [[Plataforma Logística de Zaragoza]] (PLAZA), que con 12 500 000 m² es la mayor del sur de Europa, han impulsado el sector logístico en estos últimos años. Además, la apertura de [[Puerto Venecia]], el mayor centro comercial de Europa, ha creado cerca de 4000 puestos de trabajo, siendo un importante centro de atención turística nacional e internacional.

A esta clase de grandes iniciativas comerciales van unidas otras que estimulan la implantación de oficinas en la ciudad, como por ejemplo, el edificio [[World Trade Center Zaragoza]] o el espacio de la [[Exposición Internacional de 2008]], convertido en un complejo empresarial y la Ciudad de la Justicia.

Empresas del área de influencia de Zaragoza por facturación:

El [[Servicio Aragonés de Salud]] es el organismo que integra todos los centros de la sanidad pública en Aragón. Está estructurado en ocho sectores sanitarios y, dentro de esta estructura, Zaragoza contiene tres de ellos, denominados Zaragoza I, Zaragoza II y Zaragoza III. Cada uno de estos sectores cuenta con sus correspondientes centros de atención primaria, centros de especialidades, hospitales y centros de salud mental.

La relación de hospitales, tanto del sector público como del privado, existentes en Zaragoza es la siguiente: 

[[Archivo:Zaragoza Airport.jpg|thumb|Terminal 2 del aeropuerto (T2)]] 
El nombre oficial es Aeropuerto de Zaragoza ([[Código de aeropuertos de IATA|código IATA]]: ZAZ). Su inauguración se remonta a septiembre de 1947 y está situado en el barrio de [[Garrapinillos]], a 10 kilómetros del centro de la ciudad. El aeropuerto está conectado por una línea de autobús con una frecuencia de 30 minutos. El traslado en taxi tiene un coste aproximado de 20 euros. Cuenta con servicios de alquiler de coches (Atesa, Europcar, Hertz, Sixt). Además de cajeros automáticos, cafeterías, restaurantes, tiendas de viaje, de compras, tiene servicio de internet WIFI. En el año 2011 transportó a 751.097 pasajeros (según [[AENA]]). En marzo de 2008, con motivo de la [[Exposición Internacional del Agua]], se abrió una nueva terminal. El moderno edificio tiene capacidad para un millón de pasajeros por año aunque es ampliable hasta el millón y medio. Actualmente, en el aeropuerto operan seis compañías aéreas ([[Ryanair]], [[Air Europa]], [[Air Nostrum]], [[Volotea]], [[Vueling]] y [[Wizz Air]]) y existen conexiones permanentes con nueve destinos, seis europeos ([[Londres]], [[París]], [[Bruselas]], [[Milán]], [[Cluj-Napoca]] y [[Bucarest]]) y tres nacionales ([[Mallorca]], [[Tenerife]] y [[Lanzarote]]), a los que se le suman otros cuatro enlaces veraniegos ([[Gran Canaria]], [[Fuerteventura]], [[Ibiza]] y [[Menorca]]). 

En lo referente al transporte de mercancías, el [[aeropuerto de Zaragoza]] está experimentando un fuerte crecimiento que lo ha situado como el tercero de España desde el año 2009, posición consolidada en 2011 con 48.647 [[tonelada]]s de carga, un 14,3% más que en 2010.
Aunque se había previsto implantar las mejoras en el sistema [[ILS]] para permitir el aterrizaje con baja o nula visibilidad y que entrara en funcionamiento en 2012, el Gobierno español señaló en marzo de ese año que la instalación de esos niveles de seguridad no está sujeta a plazo alguno y «se realizará tras un cuidadoso análisis de la inversión».

[[Archivo:Estación de Zaragoza-Delicias, tren AVE Madrid-Barcelona, Siemens Velaro E, serie 103 de Renfe.JPG|thumb|Estación de Zaragoza-Delicias, con AVE procedente de Madrid y con destino Barcelona. El tren es de la [[Serie 103 de Renfe]].]]
[[Archivo:Tren Zaragoza.JPG|thumb|[[Estación Intermodal de Zaragoza - Delicias|Estación AVE Zaragoza - Delicias]]: vista interior mostrando un [[Regional (Renfe)|tren regional]].]]
El 11 de octubre de 2003 se inauguró la línea de [[alta velocidad]] ([[Alta Velocidad Española|AVE]]) Madrid-Zaragoza-Lérida que asegura la conexión de Zaragoza con Madrid en 90 minutos. La extensión de la línea de [[alta velocidad]] hasta Barcelona se inauguró el 20 de febrero de 2008. Desde mayo de 2007, la nueva [[Estación de Zaragoza-Delicias|Estación Intermodal de Zaragoza-Delicias]], situada en el distrito de [[La Almozara]], alberga también la estación central de autobuses de la ciudad.

La ciudad cuenta con otras tres estaciones ferroviarias: la [[Estación de Zaragoza-Portillo|estación de El Portillo]], la [[Estación de Zaragoza-Goya|de Goya]], y la [[estación de Miraflores|de Miraflores]]. Todas ellas dan servicio a la línea de [[Cercanías Zaragoza|cercanías de Zaragoza]] y a varias de [[MD (Renfe)|media distancia]] de Renfe.

Entre 2008 y 2010 comenzaron a operar el puerto seco y el área logística intermodal ferroviaria de PLAZA (Plataforma Logística de Zaragoza) que suman, entre ambas superficies, 2,6 millones de metros cuadrados. Además, la ciudad también cuenta con el puerto seco de «MercaZaragoza» y con los muelles ferroviarios de varias empresas como SAICA o la Montañanesa.

Excepto con Soria, Zaragoza está comunicada mediante autovías y autopistas con todas las capitales de las provincias limítrofes, además de con [[Barcelona]], Bilbao, Madrid y Valencia. 

[[Archivo:Puente del Tercer Milenio (Zaragoza).jpg|thumb|[[Puente del Tercer Milenio]], construido con motivo de la [[Expo 2008]].]]

Zaragoza dispone de varias rondas circunvalatorias, de las cuales adquieren mayor protagonismo: 


[[Archivo:Renfe Cercanías Interior.jpg|thumb|Vista del interior de los nuevos trenes Civia.]]
Zaragoza dispone, desde el 11 de junio de 2008, de su primera línea de [[Cercanías Zaragoza|Cercanías]]: [[Estación de Casetas|Casetas]]-[[Estación de Utebo|Utebo]]-[[Estación Intermodal de Zaragoza - Delicias|Delicias]]-[[Estación del Portillo|Portillo]]-[[Estación de Goya (Zaragoza)|Goya]]-[[Estación de Miraflores|Miraflores]]. 

El 4 de abril de 2012 se abrió al público la estación de Goya, situada entre el Portillo y Miraflores. Este apeadero está llamado a ser la piedra angular del sistema de Cercanías dada su centralidad y su correspondencia con la Línea 1 del [[Tranvía de Zaragoza|tranvía]]. En el futuro, la C-1 deberá extenderse hacia el oeste y hacia el este llegando a [[Alagón (Zaragoza)|Alagón]] y a [[El Burgo de Ebro]], respectivamente.

La segunda línea de cercanías, todavía en proyecto, tendrá una disposición Norte-Sur, perpendicular a la C-1 a escala metropolitana, aunque compartiendo vías en el casco urbano de Zaragoza. A largo plazo se espera que llegue desde [[Zuera]] hasta [[María de Huerva]], si bien, en una primera fase, podría cubrir el trayecto El Portillo-Estación Intermodal-Plataforma Logística ([[Plataforma Logística de Zaragoza|PLAZA]]).


[[Archivo:210 CTAZ.jpg|thumb|Bus del [[Consorcio de Transportes del Área de Zaragoza|CTAZ]], prestando servicio.]]
El servicio de buses periurbanos de Zaragoza sirve a una treintena de municipios situados en un radio de 30 km con respecto a Zaragoza. Existen seis corredores principales:

El servicio es financiado por los diversos ayuntamientos, la [[Diputación Provincial de Zaragoza|DPZ]], la [[Diputación General de Aragón|DGA]] y por los propios usuarios. Es servido por varias compañías de transporte, aunque todas aglutinadas en el [[Consorcio de Transportes del Área de Zaragoza]]. El color coporativo habitual es el rojo con franjas naranjas ondulantes y en todas las líneas del consorcio está disponible la "tarjeta Interbús", que es compatible con los [[TUZSA|Autobuses Urbanos de Zaragoza]] y con el [[Tranvía de Zaragoza|tranvía]]. Además, existen 5 intercambiadores de transporte en la ciudad de Zaragoza a los que llegan los buses metropolitanos.


[[Archivo:Zaragoza Urbos 3 (img 1).JPG|thumb|[[Tranvía de Zaragoza]] (modelo [[Urbos#Urbos 3|Urbos 3]]) en la parada de "Emperador Carlos V".]]
La primera fase de la [[Tranvía de Zaragoza#Línea 1 (Parque Goya-Valdespartera)|línea 1 del tranvía]] fue inaugurada el 19 de abril de 2011 inaugurándose completamente el 26 de marzo de 2013. La línea 1, con una disposición Norte-centro-Suroeste, mide 12,8 km de longitud interconectando los barrios de [[Valdespartera]], [[Casablanca (Zaragoza)|Casablanca]], [[Romareda]], [[Universidad (Zaragoza)|Universidad]], [[Centro (Zaragoza)|Centro]], [[Casco Histórico (Zaragoza)|Casco Histórico]], [[Actur]], [[Juslibol]] y [[Parque Goya]] así como conectando los dos campus principales de la [[Universidad de Zaragoza]]. Las paradas de "Emperador Carlos V", "Fernando el Católico-Goya" y "León Felipe"/"Rosalía de Castro" sirven de intercambiador con los autobuses metropolitanos de [[CTAZ]] o con el [[Cercanías de Zaragoza]] La sociedad concesionaria del servicio de Tranvía es TRAZA. En el año 2016 prestó servicio a 27,9 millones usuarios.

Se encuentra en estudio la creación de una segunda línea con disposición Oeste-centro-Este. La [[Línea 2 del Tranvía de Zaragoza|Línea 2]] conectará los barrios de [[Valdefierro]], [[Oliver (Zaragoza)|Oliver]], [[Delicias]], [[Bombarda]], [[Centro (Zaragoza)|Centro]] y por medio de dos ramales los barrios de [[Las Fuentes(Zaragoza)|Las Fuentes]] y [[San José (Zaragoza)|San José]]. Además, esta línea permitirá conectar con la [[Estación Zaragoza-Delicias]] por tranvía. 

Asimismo se ha planteado la creación de una tercera línea, la cual todavía no se encuentra en fase de estudio, que uniría [[Torrero]] y [[La Jota (Zaragoza)|La Jota]] pasando por el centro urbano. También se ha planteado la ampliación de la [[Línea 1 del Tranvía de Zaragoza|Línea 1]] por el suroeste conectando de esta manera con el nuevo barrio de [[Arcosur]].

[[Archivo:Cityclass Euro5 Tuz.jpg|right|thumb|Iveco CityClass Euro-5 de [[Autobuses Urbanos de Zaragoza|Auzsa]], con carrocería Hispano Habit, prestando servicio.]]
El transporte urbano de Zaragoza se basa primordialmente en el transporte en autobús realizado por la empresa concesionaria [[Urbanos de Zaragoza|AUZSA]] (Autobuses Urbanos de Zaragoza S.A), anteriormente conocida como TUZSA (Transportes Urbanos de Zaragoza S.A), que posee una flota de cerca de 350 vehículos, una plantilla cercana a los 750 empleados, 47 líneas urbanas y que realiza cerca de 115 millones de viajes anuales. Esta empresa cuenta también con el funcionamiento del Bus Turístico, uno de los primeros servicios emergentes de la ciudad de Zaragoza.


El promedio de tiempo que las personas pasan en transporte público en Zaragoza, por ejemplo desde y hacia el trabajo, en un día de la semana es de 48 min, mientras que el 9% de las personas pasan más de 2 horas todos los días. El promedio de tiempo que las personas esperan en una parada o estación es de 11 min, mientras que el 12% de las personas esperan más de 20 minutos cada día. La distancia promedio que la gente suele recorrer en un solo viaje es de 4.2 km, mientras que el 5% viaja por más de 12 km en una sola dirección. 

[[Archivo:BiZi Zaragoza.jpg|thumb|BiZis en la [[Plaza de España (Zaragoza)|Plaza de España]] de Zaragoza.]]
En mayo de 2008, poco antes de la inauguración de la [[Expo Zaragoza 2008|Exposición Internacional]], se implantó en la ciudad un sistema público de bicicletas de alquiler llamado [[BiZi|Bizi Zaragoza]]. Al término de la primera fase, el sistema ponía a disposición de los ciudadanos 340 bicicletas distribuidas en 29 estaciones repartidas, principalmente, por las riberas del Ebro, el centro de la ciudad y las vías de acceso al recinto de la Exposición. En mayo de 2009 el número de estaciones pasó a 49 y en octubre de ese mismo año, con varios meses de antelación a lo previsto, el sistema pasó a contar con 1000 bicicletas en 100 puntos de acceso, alcanzándose en abril de 2010 la cifra de 29 034 abonados. Tras la última ampliación, con 130 estaciones y 1300 bicicletas, el número de usuarios de Bizi Zaragoza llegó a un máximo de 37 500 a fines de 2013, registrándose 35 500 en diciembre de 2014. Uno de los factores que más influyeron en la disminución de los abonados fue la ejecución de una sentencia del Tribunal Superior de Justicia de Aragón que obligó al Ayuntamiento a anular la ordenanza municipal que permitía la circulación de las bicicletas por las aceras.

Existe un servicio turístico de transporte en barco, el cual realiza el trayecto fluvial comprendido entre el puerto de Vadorrey y la zona Expo, haciendo parada obligatoria en el Club Naútico (al pie de la Basílica del Pilar). Dicho servicio presenta problemas con el calado de los barcos y nunca ha estado a pleno rendimiento.

También hay varios trenecitos turísticos que en verano unen el casco urbano con los espacios naturales periurbanos del galacho de Juslibol y los galachos de La Alfranca.

[[Archivo:Basilica del Pilar-sunset.jpg|thumb|Atardecer en Zaragoza. En primer término, el [[Ebro]]; al fondo, la [[Basílica de Nuestra Señora del Pilar de Zaragoza|Basílica de Nuestra Señora del Pilar]].]]
Zaragoza es una ciudad bimilenaria por la que han pasado la práctica totalidad de las civilizaciones que han dominado la Península Ibérica y de las que quedan restos y monumentos, a pesar del destructivo efecto que tuvieron para el patrimonio arquitectónico los sitios que padeció durante la Guerra de la Independencia.

Los tres principales lugares de interés son:
[[Archivo:Lonja y Seo.jpg|thumb|Plaza de las catedrales con [[Lonja de Zaragoza|la Lonja]] a la izquierda y [[Catedral del Salvador de Zaragoza|La Seo]] al fondo.]]
[[Archivo:Aljafería2.JPG|thumb|[[Palacio de la Aljafería]], siglo XI. Torreones semicirculares reconstruidos en el siglo XX.]]

Véase también:

[[Archivo:Torre del agua.jpg|thumb|upright|[[Torre del Agua]].]]
Dentro de la arquitectura de la ciudad de fines del siglo XX y principios del XXI se pueden destacar: 

Además, para la Exposición Internacional de 2008 se realizaron diversas infraestructuras, entre las que cabe destacar el [[Pabellón de España en la Expo 2008|Pabellón de España]], el [[Pabellón Puente]], la [[Torre del Agua]], el [[Palacio de Congresos de Zaragoza]], el [[Pabellón de Aragón en la Expo 2008|Pabellón de Aragón]], el [[Puente del Tercer Milenio]] y la [[Pasarela del Voluntariado]].

[[Archivo:Calle de Alfonso I (Zaragoza).jpg|thumb|Vista de la calle Alfonso I, con la basílica del Pilar al fondo.]]
Entre las calles y plazas más emblemáticas y concurridas se encuentran:

[[Archivo:Parque Grande by juanedc.jpg|thumb|[[Parque Grande José Antonio Labordeta|El Parque Grande]].]]
Antes del año 2008, Zaragoza contaba con once grandes parques urbanos de extensión superior a 10 [[hectárea]]s, entre los que se pueden citar los siguientes: 


A causa de la celebración de la EXPO 2008 se aumentaron significativamente las zonas verdes de la ciudad. En particular se reacondicionó parte del [[meandro de Ranillas]] para crear el [[Parque Metropolitano del Agua Luis Buñuel|Parque Metropolitano del Agua]] y se realizó un plan para la recuperación de las riberas del Ebro, Huerva y Gállego.

Entre las 204 especies de árboles existentes en las calles de Zaragoza en el año 2007, las más difundidas son: el [[Platanus × hispanica|plátano de sombra]], el [[pinus halepensis|pino carrasco]], el [[ligustrum japonicum|aligustre]], el [[prunus pissardii|ciruelo rojo]], el [[ailanthus altissima|ailanto]], la [[robinia pseudoacacia|robinia]], el [[populus bolleana|álamo]] y la [[catalpa bignonioides|catalpa]].

[[Archivo:Meandro Galacho.JPG|miniaturadeimagen|Galacho de la Alfranca]]
El entorno natural de Zaragoza tiene como eje fundamental a sus ríos, el [[Ebro]], el [[Gállego (río)|Gállego]] y el [[Río Huerva|Huerva]], donde se encuentran diferentes [[ecosistema]]s como varios bosques de ribera o [[Bosque en galería|sotos]]. En este contexto destacan los [[galacho]]s, que son [[meandro (geomorfología)|meandros]] abandonados del río Ebro que conservan parte del agua y forman singulares espacios naturales: El [[galacho de La Alfranca]] y el [[galacho de La Cartuja]] forman, junto al [[galacho de El Burgo de Ebro]], la [[Reserva natural dirigida de los Sotos y Galachos del Ebro]]. Otra área protegida es el [[Galacho de Juslibol]]. También se pueden destacar los [[humedal]]es de la [[balsa del Ojo del Cura]], [[balsa del Ojo del Fraile]], [[balsa de la Consejera]] (las tres en [[Casetas]]) y la [[balsa de Larralde]] (en [[Garrapinillos]]), aunque esta última fue en su origen una balsa artificial para el riego. El [[Anillo Verde de Zaragoza]] es un itinerario natural de unos 60 km de longitud que conecta diversos espacios naturales, parques y paseos urbanos. Se divide en el Anillo Verde Norte, que tiene gran parte del trayecto junto a los ríos Ebro y Gállego, y el Anillo Verde Sur, cuyo recorrido discurre en buena parte por el corredor que está a lo largo del [[Canal Imperial de Aragón]] y también junto al Ebro. Otro espacio natural es el [[Vedado de Peñaflor]].

[[Archivo:Vista de Zaragoza en 1647.jpg|thumb|240px|Vista de "[[Zaragoza en 1647]]", por [[J.B. Martínez del Mazo]].]]
La ciudad posee una [[Universidad de Zaragoza|universidad]] en funcionamiento desde 1583, tras haber sido aprobada por [[Carlos I de España|Carlos V]] en 1542 y por el papa en 1554. Sus orígenes se sitúan en el «Estudio General de Artes» creado en 1474 sobre una escuela anterior. Actualmente, se reparte entre varios campus, de los que dos se encuentran en la ciudad (Campus de San Francisco y Campus Río Ebro).

En Zaragoza se celebró la [[Exposición Hispano-Francesa de 1908]], que conmemoró el primer centenario de los Sitios de Zaragoza y también la [[Expo 2008|Exposición internacional de 2008]], con el tema «Agua y desarrollo sostenible». Así mismo fue elegida como sede de una Oficina de la Organización de Naciones Unidas para la Década del Agua 2005-2015.

Véase también: [[Anexo:Museos de Zaragoza]]

En Zaragoza hay una variada red de [[Museo|museos]] y salas de exposiciones: 
[[Archivo:Teatro Romano Cesaraugusta-vista desde arriba-3.jpg|thumb|240px|Teatro romano de Caesaraugusta.]]
[[Archivo:Zaragoza Museo Pablo Gargallo 530.jpg|miniaturadeimagen|Museo Pablo Gargallo]]
[[Archivo:Museo Provincial Zaragoza 3.jpg|miniaturadeimagen|Museo de Zaragoza]]


[[Archivo:Arapaima Zaragoza V.jpg|miniaturadeimagen|Arapaima en el Acuario Fluvial]]

Zaragoza cuenta con un sistema bibliotecario públicocompuesto por un centro coordinador y veinticuatro bibliotecas municipales,con 277.598 volúmenes [2006] en su mayoría abiertas entre 1983 y 2003.Cuenta también con la [[Biblioteca de Aragón]], dependiente del [[Gobierno de Aragón]], y la [[Biblioteca de la Universidad de Zaragoza]].

En Zaragoza también tienen su sede importantes archivos, entre ellos el [[Casa de Ganaderos de Zaragoza|Archivo de Casa de Ganaderos]], el [[Archivo Provincial de las Escuelas Pías de Aragón|Archivo de las Escuelas Pías de Aragón]], el [[Archivo de la Fundación Bernardo Aladrén]], el [[Archivo Histórico Provincial de Zaragoza]], el [[Archivo Municipal de Zaragoza]] y el Archivo de las Catedrales de Zaragoza (eclasiástico).

[[Archivo:JordanDeAsso Paraninfo Zaragoza.JPG|thumb|Estatua de [[Jordán de Asso]].]]
Entre los autores de obras literarias vinculados de algún modo con Zaragoza figura en época romana [[Prudencio]], que publicó odas de alabanza a los [[mártires de Zaragoza]]. En la [[Hispania visigoda|época visigoda]] una obra destacada fue la "[[Chronica Caesaraugustana]]", atribuida al obispo [[Máximo de Zaragoza|Máximo]] y sobresalió también el obispo [[Braulio de Zaragoza|Braulio]], autor de obras litúrgicas. En la [[Taifa de Zaragoza]] escribieron filósofos de la talla de [[Avempace]] y [[Avicebrón]]. En 1475 se estableció la primera imprenta en Zaragoza y en esos años iniciales sobresalieron las imprentas dirigidas por [[Pablo Hurus]] y [[Jorge Coci]]. En el [[renacimiento]], [[Antonio Agustín]] y [[Juan Verzosa]] fueron dos [[humanismo en España|humanistas]] que escribieron en latín la mayor parte de sus obras, y como historiador brilló [[Jerónimo Zurita]]. En el [[Siglo de Oro]], Zaragoza fue uno de los lugares donde [[Baltasar Gracián]] redactó su obra cumbre: "[[El Criticón]]", mientras en poesía destacaron los hermanos [[Bartolomé Leonardo de Argensola]] y [[Lupercio Leonardo de Argensola]]. A fines del siglo XVIII, [[Felix Latassa|Félix Latassa]] recogió en tres volúmenes una historia literaria de Aragón que comprendía 2866 escritores aragoneses. Entre el siglo XVIII y XIX escribió el científico, filólogo, jurista y economista [[Jordán de Asso]] y, en la misma época, el poeta, novelista y comediógrafo [[José Mor de Fuentes]]. La [[Aljafería]] fue el escenario del drama romántico del siglo XIX "[[El trovador]]", de [[Antonio García Gutiérrez]], basado en acontecimientos históricos del siglo XV. En 1844 se imprimió en Zaragoza la "[[Vida de Pedro Saputo]]" de [[Braulio Foz]]. Por otra parte, los [[Sitios de Zaragoza|sitios sufridos por Zaragoza]] generaron numerosas obras literarias, entre ellas "[[Zaragoza (episodio nacional)|Zaragoza]]", uno de los "[[Episodios nacionales]]" de [[Benito Pérez Galdós]]. Dos novelistas zaragozanos que nacieron en el siglo XIX fueron [[María del Pilar Sinués]] —cuyas obras de temática femenina fueron muy populares en su tiempo— y [[José María Matheu Aybar|José María Matheu]]. 
[[Archivo:Labordeta.jpg|miniaturadeimagen|left|El cantautor José Antonio Labordeta]]
Como periodista sobresalió [[Mariano de Cavia]]. Ya dentro del [[Modernismo (literatura en español)|modernismo]] figuran autores como [[Mariano Miguel de Val]] y [[Eduardo de Ory]] y entre los escritores del siglo XX posteriores merecen destacarse autores como [[Ildefonso Manuel Gil]], de la [[generación de 1936]]; [[Miguel Labordeta]], poeta de la generación de la posguerra y el vanguardista [[Tomás Seral y Casas|Tomás Seral]]. Entre los años 1930 y 1960 aparecieron las [[revista literaria|revistas literarias]] "[[Cierzo (revista)|Cierzo]]", "[[Noreste (revista)|Noreste]]", "[[Doncel (revista)|Doncel]]", "[[Proa (revista de Zaragoza)|Proa]]", "[[Pilar (revista)|Pilar]]", "[[Ansí]]", "[[Orejudín]]", "[[Papageno (revista)|Papageno]]" y "[[Despacho Literario]]", algunas de las cuales tuvieron una efímera existencia. Durante los años cuarenta, publica novelas la autora [[Rosa María Aranda]].
En la actualidad, destacan narradores como [[Ignacio Martínez de Pisón]], [[Soledad Puértolas]], [[Mariano Gistaín]], [[José María Conget]], [[José Luis Melero]], [[Antón Castro]], [[Miguel Mena]], [[Cristina Grande]], [[Ismael Grasa]], [[Daniel Gascón]], [[Rodolfo Notivol]], [[José Antonio Labordeta]], [[Ángela Labordeta]], [[Vital Citores]], [[Ignacio García-Valiño]], [[José Giménez Corbatón]], Javier Delgado, [[Santiago Gascón]], [[Adolfo Ayuso]], [[Ana Alcolea]], [[Javier Barreiro]], [[Ramón Acín Fanlo]], [[Ángeles de Irisarri]], [[Magdalena Lasala]], [[Félix Romeo]], [[Julio José Ordovás]]... Poetas como [[Fernando Ferreró]], [[Fernando Sanmartín]], [[Manuel Peláez González]], [[Manuel Vilas]], [[Fernando Andú]], [[Ángel Guinda]], [[Emilio Gastón]], [[Joaquín Sánchez Vallés]], [[Ana María Navales]], [[David Mayor]], [[Ignacio Escuín]], [[Fernando Mombiela]], [[Paco Rubio Sesé]]... Autores de teatro, como [[Alfonso Plou]], [[Mariano Anós]] y [[Rafael Campos]]. Autores de literatura infantil como [[Daniel Nesquens]], [[Fernando Lalana]], [[Francis Meléndez]]...

[[Archivo:Vicente López Portaña - el pintor Francisco de Goya.jpg|thumb|[[Francisco de Goya]] por [[Vicente López Portaña|Vicente López]].]]
La gran figura del arte plástico es [[Francisco de Goya]], cuyo alcance es universal. En Zaragoza se pueden encontrar obras suyas en la [[Basílica del Pilar]], la [[Pinturas de la Cartuja del Aula Dei|Cartuja del Aula Dei]], el [[Museo de Zaragoza]], el [[Museo Camón Aznar]] y el [[Museo Diocesano de Zaragoza|Museo Diocesano]], aparte de en colecciones privadas. 

Sin embargo, no han faltado grandes artistas desde el maestro gótico [[Blasco de Grañén]].

En el siglo XVI, pintores como [[Jerónimo Cósida]], [[Pedro Morone]], [[Roland de Mois]] o [[Francisco Lupicini]] introdujeron el estilo [[Pintura renacentista|renacentista]] en la ciudad.

Su influjo consolidaría en el [[siglo XVII|siglo siguiente]] una importante escuela pictórica barroca con autores como [[Jerónimo de Mora]], [[Miguel Jerónimo Lorieri]], [[Pablo Rabiella y Díez de Aux]], [[Juan de Orcoyen]], [[Francisco del Plano]], [[Rafael Pertús]] y, sobre todos, el [[Ejea de los Caballeros|ejeano]] [[Vicente Berdusán]] y el tratadista y pintor [[Jusepe Martínez]].

[[Archivo:Autorretrato de Francisco Pradilla.jpg|miniaturadeimagen|left|Autorretrato de Francisco Pradilla]]
Del siglo XVIII data [[José Luzán]], el primer maestro de Goya, pero el más reconocido junto con el pintor de Fuendetodos, fue su cuñado [[Francisco Bayeu]], cabeza de una saga que incluyó a sus hermanos [[Manuel Bayeu|Manuel]] y [[Ramón Bayeu|Ramón]].

En el siglo XIX de nuevo surgen grandes figuras, como [[Bernardino Montañés]], [[Marcelino de Unceta]], [[Mariano Barbasán]] y, especialmente, [[Francisco Pradilla]], nacido en [[Villanueva de Gállego]], a trece kilómetros de la capital.

En el siglo XX destaca la obra informalista de [[Manuel Viola]], quien estuvo adscrito al influyente grupo «[[El Paso (grupo)|El Paso]]», así como la de [[Ruizanglada]], [[Santiago Lagunas]], [[José Manuel Broto Gimeno]], [[Francisco Marín Bagüés]] o [[Fermín Aguayo]]. [[Pepe Cerdá]], [[Ángel Pascual Rodrigo|Ángel]], [[Vicente Pascual Rodrigo]], [[Fernando Sinaga]], [[Requena Nozal]], [[Lina Vila]] o [[Dino Valls]] son artistas que están dejando su impronta en el siglo XXI.

En escultura cabe citar a [[Gil Morlanes el Joven|Gil Morlanes «el Joven»]] —quien terminó la portada renacentista de la [[Iglesia basílica de Santa Engracia|Iglesia de Santa Engracia]], emprendida por [[Gil Morlanes el Viejo|su padre]]—, [[Jerónimo Secano]] en el barroco y [[Ponciano Ponzano]] en el siglo XIX. Del siglo pasado destacan [[Félix Burriel]] y [[Carlos Palao]], que fue director de la [[Real Academia de Bellas Artes de San Luis|Academia de Bellas Artes]].

En la fotografía, ya en 1837 (dos años antes del reconocimiento oficial del invento a [[Louis Daguerre]]), [[José Ramos Zapetti]] fijaba en Zaragoza una imagen sobre una plancha de cobre con el procedimiento de la [[cámara oscura]]. Entre 1856 y 1874 establece su gabinete fotográfico en el n.º 33 del [[Coso (Zaragoza)|Coso]] el renombrado [[Mariano Júdez]], quien traspasó su negocio a [[Anselmo María Coyne]], fundador del [[Estudio Fotográfico Coyne|Estudio Coyne]]. Posteriormente, su hijo [[Ignacio Coyne Lapetra]], amplió su actividad al [[cinematógrafo]], creando el [[Cine parlante Coyne]]. Otro importante fotógrafo, que desarrolló su labor en la capital aragonesa a fines del siglo XIX, fue [[Enrique Beltrán]]. Entre otros fotógrafos actuales se puede mencionar a [[Rafael Navarro Garralaga|Rafael Navarro]] y [[Pedro Avellaned]].

[[Archivo:Teatro del Mercado Zaragoza 4.jpg|miniaturadeimagen|Teatro del mercado]]
El más antiguo teatro de la ciudad es el [[Teatro Principal (Zaragoza)|Teatro Principal]]. También cuenta con el [[Teatro del Mercado]], el [[Teatro de la Estación]], el [[Teatro de las Esquinas]] en el barrio de las Delicias y el [[Teatro Arbolé]], dedicado exclusivamente a la programación infantil. El [[Teatro Fleta]] se encuentra desde hace varios años en obras paralizadas pendiente de un proyecto para el mismo.

Además del [[Centro de Arte Dramático de Aragón]], cuya sede se encuentra en Zaragoza, y de la [[Escuela Municipal de Teatro de Zaragoza|Escuela Municipal de Teatro]], dependiente del Ayuntamiento de Zaragoza y con una larga trayectoria, sobresalen las compañías independientes que han animado la escena local desde los años 50. Siguen en completa actividad el [[Teatro de la Ribera (Zaragoza)|Teatro de la Ribera]], el [[Teatro Imaginario]], [[Caleidoscopio (Zaragoza)|Caleidoscopio]], [[Muac]], [[Belladona teatro]] y compañías de gran proyección como [[Teatro del Temple]].

Zaragoza fue pionero en la producción cinematográfica. [[Eduardo Jimeno Correas]] rodó una de las primeras películas españolas "[[Salida de la misa de doce de la Iglesia del Pilar de Zaragoza|Salida de misa del Pilar]]", en [[1896]], y desde entonces la ciudad ha dado un buen número de directores, actores, críticos, guionistas y técnicos. Entre los directores, se deben citar a [[Florián Rey]], [[José Luis Borau]], [[José María Forqué]], [[Fernando Palacios]], [[Antonio Artero]], [[Santos Alcocer]], [[Alfredo Castellón]], [[José Antonio Maenza]], [[Raúl Artigot]], [[José Antonio Duce]], [[José Luis Gonzalvo]], [[Alberto Sánchez]] y, más recientemente, [[Miguel Ángel Lamata]] y [[Luis Alegre Saz|Luis Alegre]].
Entre los guionistas se cita a [[Santiago Aguilar Oliver]].

En Zaragoza han nacido reconocidos autores de [[cómic]], como [[Adolfo Buylla]], [[Tran]], [[Carlos Ezquerra]], [[Antonio Altarriba]], [[Calpurnio]], [[Fernando de Felipe]], [[Nacho Casanova]] o [[Furillo]], aunque nunca ha tenido una industria local importante, más allá de los [[fanzine]]s. Sí que celebra desde 2002 el "[[Salón del Cómic de Zaragoza]]".

[[Archivo:Sala Mozart Zaragoza.jpg|thumb|Vista del [[escenario]] de la sala Mozart del [[Auditorio de Zaragoza]].]]
[[Archivo:El Plata.jpg|miniaturadeimagen|Cabaré El Plata]]
El [[Auditorio de Zaragoza]], cuya "Sala Mozart" ha recibido elogios por la calidad de su acústica, acapara gran parte de los eventos de música clásica. En este Auditorio tienen su sede el Grupo Enigma-Orquesta de Cámara, el coro Amici Musicae y la Orquesta barroca Al Ayre Español. También organizan conciertos las entidades de ahorro [[Ibercaja]] y [[Caja Inmaculada|CAI]], así como Juventudes Musicales de Zaragoza, pero todas estas de un modo más esporádico. La pianista [[Pilar Bayona]] fue una de las grandes figuras de la música clásica zaragozana del siglo XX.

[[Archivo:Eva Amaral-02.jpg|miniaturadeimagen|left|Amaral]]
En cuanto a [[música popular]], Zaragoza cuenta con una nutrida escena musical. Hay cantautores, como los que comenzaron en torno a la [[Nueva Canción Aragonesa]], como [[José Antonio Labordeta]], [[Joaquín Carbonell]] y [[La Bullonera]] o como los más jóvenes [[Ángel Petisme]] y [[Carmen París]]. También son de Zaragoza [[Héroes del Silencio]], . Singular es la obra de [[Santiago Auserón]], que fue durante años líder de [[Radio Futura]], así como [[Amaral]]. Hay muchos grupos de pop independiente vinculados a la ciudad, como los desaparecidos [[El niño gusano]], que han sido el embrión de otros proyectos en este estilo musical, como [[Tachenko]], [[La Costa Brava]] o [[Da]]. Sin duda, el [[rap]] y el [[hip hop]] ocupan un lugar muy destacado en la escena musical, fundamentalmente por el grupo [[Violadores del verso]], también cabe mencionar a raperos que trabajan en solitario como [[Rapsusklei]] o [[Sharif]]. En el género del [[punk]] destaca [[Manolo Kabezabolo]]. Finalmente, en el género del blues destaca la voz de [[Ana Midón]]. Otros grupos y solistas destacados de la escena musical zaragozana son [[Los Especialistas]], [[Niños del Brasil]], [[Las Novias]], [[Más Birras]], [[Gabriel Sopeña]], [[Mauricio Aznar]], [[Bigott]], [[Ixo Rai]] y [[Comando Cucaracha]].

Dos locales emblemáticos de Zaragoza desde principios del siglo XX han sido el «[[Oasis (Zaragoza)|Oasis]]» y «[[El Plata]]», que ofrecían espectáculos de variedades donde destacaban algunas formas de música popular como el [[cuplé]]. El Oasis fue convertido en discoteca y sala de conciertos mientras El Plata abrió de nuevo sus puertas en 2008 con espectáculos de [[cabaré]].

A lo largo del año hay numerosos eventos relacionados con la [[jota aragonesa|jota]], principal manifestación de música folclórica de Zaragoza. El más importante, celebrado anualmente desde 1886, es el [[Certamen Oficial de Jota Aragonesa]].

Entre las revistas musicales de la ciudad destaca [[Zona de obras]], especialmente interesada en la música y la cultura iberoamericana. En cuanto a salas de conciertos destacan, además de la Sala Oasis, La Casa del Loco, el Auditorio, Sala Reset y diversos centros cívicos dependientes del ayuntamiento que albergan numerosos conciertos. 

[[Archivo:Zaragoza - Ofrenda de flores.jpg|right|thumb|Ofrenda de flores a la Virgen durante las [[Fiestas del Pilar]].]]
[[Archivo:La Santa Cena (Semana Santa de Zaragoza, Aragón).jpg|miniaturadeimagen|Paso de la Santa Cena en la Semana Santa]]

[[Archivo:Chilindrón - Polllo.JPG|miniaturadeimagen|Pollo a la chilindrón]]
En la gastronomía tradicional de Zaragoza son fundamentales muchos de los productos de huerta del entorno de la ciudad y de otras zonas de la ribera del Ebro dentro de los que sobresale por su calidad la [[borraja]], pero también el [[cardo]], la [[alcachofa]], el [[bisalto]], el [[tomate]], la [[acelga]], la [[coliflor]], el [[calabacín]], el [[puerro]], la [[patata]], la [[lechuga romana]], la [[escarola rizada]], el [[pimiento]], la [[espinaca]], la [[judía verde]], la [[cebolla]] o el [[ajo]]. Con algunos de estos y otros ingredientes se elaboran gran diversidad de platos como la "[[ensalada aragonesa]]" o la "[[fritada aragonesa|fritada]]". Legumbres tales como [[lentejas]], [[garbanzos]] y [[Phaseolus vulgaris|boliches]] conforman la materia prima de [[potaje]]s o [[estofado]]s. 

En Zaragoza también confluyen muchos de los productos alimenticios más valorados de otras partes de Aragón, como las [[aceituna|olivas negras]], las [[almendras]], los [[vino]]s, las [[trufas]], el [[aceite de oliva]] y el [[azafrán]]. En cuanto a carnes, abundan las recetas que tienen como protagonista el [[cordero]] o el [[pollo]], así como el [[conejo]] o la carne de caza, sin olvidar los productos derivados del cerdo entre los que destacan el [[jamón]], la [[longaniza]], el [[chorizo]] y la [[morcilla]]. También son diversos los guisos donde figuran como ingredientes principales los [[caracol]]es. Algunas de las recetas tradicionales son las "[[migas]]", las "[[magras con tomate]]", el "[[arroz a la zaragozana]]", el "pollo a la [[chilindrón]]" o el "[[ternasco]] [[asado]]". El pescado de referencia es el [[bacalao]] que, por su versatilidad, es el ingrediente principal de muchos platos. 

Frutas destacadas son las [[manzanas]], [[peras]], [[cerezas]], [[ciruelas]], [[Prunus persica|melocotones]], [[Prunus armeniaca|alberges]] e [[higos]]. En cuanto a repostería y dulces, se elaboran [[magdalenas]], [[tortas]], [[rosquillas]], [[frutas de Aragón]], [[piedras de río]], [[chocolate]], [[adoquín del Pilar|adoquines del Pilar]], [[roscón de San Valero|roscón]] en [[San Valero]] y [[guirlache]]s en Navidad.

[[Archivo:Zaragoza - Vistas Generales - "El Tubo" 4.jpg|miniaturadeimagen|Vista de El Tubo]]
[[Archivo:Puerto Venecia atardecer.jpg|miniaturadeimagen|Puerto Venecia]]
En Zaragoza hay una gran variedad de oferta de ocio. Abundan los bares de [[Tapa (alimento)|tapas]] y terrazas, que se concentran especialmente en diversos entornos dentro del [[Casco histórico de Zaragoza|Casco histórico]], zona centro y Universidad. En [[El Tubo (Zaragoza)|El Tubo]] conviven establecimientos populares y restaurantes como Bal d´Onsera, la primera estrella Michelin de la capital aragonesa. Dentro de El Tubo el café cantante [[El Plata]] mantiene el [[Cabaré|cabaret]] en Zaragoza. 

Dentro del Casco histórico, [[La Magdalena (Zaragoza)|La Magdalena]] es hoy en día la zona más alternativa e intercultural de la ciudad. Allí se pueden encontrar teterías árabes, bares con música reggae, galerías de arte, comercios alternativos, escuelas de aragonés y centros sociales con una intensa programación cultural y musical.

La mayor aglomeración de discobares y pubs de la ciudad se encuentra también dentro del Casco histórico, en torno a la calle del Temple y adyacentes. Cabe destacar también dentro del ocio nocturno la cercana discoteca Oasis. Otras zonas de ocio nocturno son la de la calle María Lostal y adyacentes, la de la zona de la calle Bretón o el entorno de la [[Plaza de Miguel Salamero|Plaza de Salamero]]. Los adolescentes suelen acudir a «El Rollo», zona situada en las calles Moncasi y Maestro Marquina. Cerca de la Puerta del Carmen abren sus puertas varios pubs "gay-friendly".

En el centro sobreviven algunas salas de cine, aunque otras, de mítica presencia en la ciudad, han cerrado sus puertas recientemente. El Casino de Zaragoza también se encuentra en este entorno. 

Numerosos centros comerciales abiertos en los últimos años ([[Plaza Imperial (Zaragoza)|Plaza Imperial]], [[Puerto Venecia]], [[Gran Casa|GranCasa]], [[Centro Comercial Augusta|Augusta]], [[Centro Comercial Aragonia|Aragonia]] o los Porches del Audiorama) también ofertan espacios de ocio con cines, restaurantes, boleras y parques infantiles, entre otros. Dentro de los espacios de ocio infantil, otro de los lugares destacables es el [[Parque de atracciones de Zaragoza]], inaugurado en 1974.

A raíz de las inversiones realizadas para la Expo 2008 se habilitaron en torno al río Ebro numerosos espacios en los que se podía practicar gran variedad de actividades. Dentro de las principales infraestructuras, algunas de ellas siguen siendo parte de la oferta de ocio de Zaragoza en determinadas épocas del año, como los paseos en barca con origen en el [[Azud Manuel Lorenzo Pardo|azud de Vadorrey]], o el descenso del [[Canal de aguas bravas de Zaragoza|canal de aguas bravas]] pero otras como la [[Telecabina Aramón Zaragoza 2008|telecabina]] dejaron de tener uso desde el año 2011.

[[Archivo:Rayo vallecano vs real zaragoza - Flickr - loren mzn (15).jpg|miniaturadeimagen|Alineación del Real Zaragoza]]
Zaragoza posee varios equipos que juegan en las principales ligas españolas. En fútbol masculino, el [[Real Zaragoza]], que juega en el [[estadio de La Romareda]], y en el femenino, el [[Zaragoza Club de Fútbol Femenino]]. En baloncesto, el equipo masculino [[Basket Zaragoza 2002|Tecnyconta Zaragoza]] y el femenino [[Club Deportivo Basket Zaragoza]]. En categorías masculinas nacionales, el balonmano está representado por el [[Club Deportivo Básico Balonmano Aragón]], el [[fútbol sala]] por la [[Agrupación Deportiva Sala 10]], el [[voleibol]] por el [[Club Voleibol Zaragoza]] y el [[atletismo]] por el [[Scorpio-71]]. También participa a nivel nacional el [[Escuela Waterpolo Zaragoza]] en [[waterpolo]] femenino. En [[gimnasia rítmica]] destaca el [[Club Deportivo Zaragozano de Gimnasia]] y el C.E.G.R. Zaragoza.

En cuanto a las principales instalaciones donde se celebran competiciones de élite pueden citarse los [[Pabellón Príncipe Felipe|pabellones Príncipe Felipe]] y [[Pabellón Siglo XXI|Siglo XXI]], el [[Palacio de Deportes de Zaragoza|Palacio de los Deportes de Zaragoza]] y el [[Estadio de Atletismo del Centro Aragonés del Deporte]].

[[Archivo:Mañanas Expo Programa de Aragón TV.jpg|miniaturadeimagen|Retransmisión de Aragón TV]]
En Zaragoza se editan los periódicos "[[Heraldo de Aragón]]" y "[[El Periódico de Aragón]]", el deportivo digital "Equipo", y el gratuito "[[20 minutos]]". 

Hay varios canales de televisión que emiten desde la ciudad: el canal autonómico [[Aragón TV]] y los canales locales [[Canal 44 (Aragón)|Canal 44]], [[Zaragoza TV]], [[La General Televisión]], [[15 TV]] ([[Radio Ebro]]) y [[Popular TV]]. Además, [[Televisión Española]] cuenta en Zaragoza con un centro territorial para Aragón desde 1979.

Gran parte de las emisoras de radio de Zaragoza están controladas por los grandes grupos radiofónicos de España: [[Radio Nacional de España]], [[Cadena SER]], [[COPE]] y [[Onda Cero]]. Al margen de ellas, operan en Zaragoza otras emisoras de ámbito autonómico, como [[Aragón Radio]] y numerosas emisoras locales.

Las ciudades hermanadas con Zaragoza son:

Además, tiene firmados acuerdos de colaboración con:




[[Categoría:Zaragoza| ]]

</doc>
<doc id="3009" url="https://es.wikipedia.org/wiki?curid=3009" title="Zizania">
Zizania

Zizania, es un género de plantas herbáceas perteneciente a la familia de las poáceas. Es originario de Eurasia y América del Norte.
Aunque conocido comúnmente como arroz silvestre o arroz salvaje, no está directamente relacionado con el arroz asiático ("Oryza sativa"), aunque ambos comparten la misma tribu. Se trata de hierbas acuáticas o palustres, robustas y erguidas. Tienen raíces delgadas y fibrosas que no penetran mucho, algunas adventicias; cañas hasta de 3 metros de altura; las hojas de 1 m de longitud por 4 cm de ancho; flores en panojas terminales. Los granos alargados contienen más de 13% de proteína y además carbohidratos, vitamina B, potasio y fósforo.

Las especies nortemericanas, llamadas ahora arroz indígena, fueron recolectadas para la alimentación por los aborígenes, desde hace por lo menos 10 mil años. Hoy en día son cultivadas.

"Zizania palustris" es una planta anual, originaria de la región de los Grandes Lagos, crece a temperaturas entre los 6° y 13° C. Los principales productores son los estados de Saskatchewan y Manitoba en Canadá y Minnesota y California en Estados Unidos.

"Zizania aquatica", también anual, es originaria de la cuenca del río San Lorenzo, Florida y el golfo de México. "Zizania texana", de la cuenca del Río San Marcos, se encuentra en peligro de extinción por la reducción de su hábitat.

"Zizania texana" es una planta perenne que se localiza únicamente en una pequeña área a lo largo del Río Saint Laurence y en las costas del Atlántico de los EEUU. Actualmente se encuentra en peligro de extinción debido a la polución y a la pérdida progresiva de terreno en el que es capaz de crecer, dado que su ecosistema es muy limitado.

"Zizania latifolia", el arroz silvestre de Manchuria, es una planta perenne. Aunque la producción y el consumo del grano han disminuido al ser sustituidos por arroz común, la planta se sigue utilizando en la alimentación, especialmente como verdura.

El género fue descrito por Carlos Linneo y publicado en "Species Plantarum" 2: 991. 1753. La especie tipo es: "Zizania aquatica" L.
Zizania: nombre genérico que deriva de la palabra griega: "zizanion", un antiguo nombre de un grano de maleza silvestre que normalmente crecía entre cultivos de trigo.



</doc>
<doc id="3010" url="https://es.wikipedia.org/wiki?curid=3010" title="Zeugites">
Zeugites

Zeugites es el nombre botánico de un género de plantas de la familia de las Poáceas. Comprende unas 19 especies distribuidas en Centroamérica, el norte de Sudamérica y su costa oeste desde Colombia hasta Bolivia. Se trata de plantas bisexuales, con tallo de 30 a 100 cm de altura, hojas perennes de lanceoladas a ovales. Flores abiertas, capilares con ramillas, sin inflorescencias parciales y órganos foliares.

Son plantas perennes; con tallos delgados y rastreros o robustos y erectos; plantas monoicas. Hojas generalmente caulinares; lígula una membrana; pseudopecíolos generalmente bien desarrollados; láminas lanceoladas a ovadas, aplanadas, generalmente delgadas, nervadas transversalmente. Inflorescencia una panícula terminal, abierta, eje principal y ramas a menudo víscidos; espiguillas bisexuales, pediceladas, solitarias, comprimidas lateralmente, con 3–15 flósculos dimorfos, unisexuales; glumas más cortas que la espiguilla, anchas, obtusas o truncadas, a menudo irregularmente dentadas o lobadas, con nervaduras transversales conspicuas, la superior generalmente más angosta que la inferior; unión de la raquilla entre los flósculos pistilados y estaminados generalmente alargada; desarticulación por debajo de las glumas y la base del flósculo estaminado más inferior, los flósculos estaminados caedizos como una unidad; flósculo más inferior pistilado; lema ancha, generalmente obtusa, pálea un poco más larga o un poco más corta que la lema; lodículas 2, truncadas, vascularizadas; estilos 2, los estigmas plumosos; flósculos superiores estaminados 2–14, generalmente más angostos que el flósculo pistilado, agudos o subobtusos, pálea casi tan larga como la lema, estambres 3. Fruto una cariopsis; embrión 1/4–1/3 la longitud de la cariopsis, hilo punteado.



</doc>
<doc id="3012" url="https://es.wikipedia.org/wiki?curid=3012" title="Zenkeria">
Zenkeria

Zenkeria, es un género de plantas herbáceas perteneciente a la familia de las poáceas. Es originario de la India, Ceilán, Birmania. Comprende 5 especies descritas y aceptadas.
Son plantas perennes; cespitosas de 60-130 cm de alto; herbáceas. Las láminas amplias o estrechas, de 5-25 mm de ancho, planas, o laminadas; pseudopeciolada. Lígula con una franja de pelos. Plantas bisexuales, con espiguillas bisexuales; con flores hermafroditas. La inflorescencia paniculada; abierta. 
El género fue descrito por Carl Bernhard von Trinius y publicado en "Linnaea" 11(2): 150. 1837. La especie tipo es: "Zenkeria elegans" Trin. 
A continuación se brinda un listado de las especies del género "Zenkeria" aceptadas hasta noviembre de 2013, ordenadas alfabéticamente. Para cada una se indica el nombre binomial seguido del autor, abreviado según las convenciones y usos. 



</doc>
<doc id="3014" url="https://es.wikipedia.org/wiki?curid=3014" title="Zacatecas">
Zacatecas

Zacatecas es uno de los treinta y un estados que, junto con la Ciudad de México, forman los Estados Unidos Mexicanos. Fue fundado el 23 de diciembre de 1588. Su capital y ciudad más poblada es la homónima Zacatecas. Está ubicado en la región centronorte del país, limitando al norte con Coahuila, al noreste con Nuevo León, al este con San Luis Potosí, al sur con Guanajuato, Jalisco y Aguascalientes, al suroeste con Nayarit y al oeste con Durango. Con 75 539 km² es el octavo estado más extenso —por detrás de Chihuahua, Sonora, Coahuila, Durango, Oaxaca, Tamaulipas y Jalisco—, con 1 579 209 habs. en 2015, el octavo menos poblado —por delante de Quintana Roo, Aguascalientes, Tlaxcala, Nayarit, Campeche, Baja California Sur y Colima, el menos poblado— y con 19,73 hab/km², el sexto menos densamente poblado, por delante de Sonora, Campeche, Chihuahua, Durango y Baja California Sur, el menos densamente poblado. 

Se divide en 58 municipios. La capital es la ciudad homónima: Zacatecas. Esta ciudad ostenta los títulos de "La Muy Noble y Leal Ciudad de Nuestra Señora de los Zacatecas", otorgados por el Rey Felipe II de España el día 20 de junio de 1588 en San Lorenzo de El Escorial, Madrid. Así mismo, le concedió el Escudo de Armas, emblema en el que fue incluido el cerro de la Bufa. Esta ciudad fue importante para la colonización, puesto que era un punto importante en la ruta hacia los territorios del norte de la Nueva España. Sus principales actividades económicas son la minería, la agricultura y el turismo. Es conocido por sus grandes depósitos de plata y otros minerales, su arquitectura colonial y su importancia durante la Revolución mexicana. Entre sus localidades más importantes están Jerez de García Salinas, Fresnillo de González Echeverría, Río Grande, Guadalupe, Sombrerete y Nochistlán.

Del náhuatl, "zacatl", y co; locativo: “lugar donde abunda el zacate”. El nombre significa personas que viven en el lugar del zacate. Antes de su conquista el lugar era habitada por indígenas llamados zacatecas, de donde precisamente se deriva el nombre de Zacatecas.

El escudo de armas de Zacatecas fue otorgado el 20 de junio de 1588 por el rey Felipe II, mediante una Cédula Real. Según la cédula real original, debe tener la forma de un escudo español. En un único campo, predomina una elevación que representa al emblemático cerro de la Bufa, en cuyos pies nace la ciudad en 1546, como producto del descubrimiento de las ricas minas de plata. En la parte más eminente del cerro aparece una imagen de la Virgen María, por haberse descubierto este cerro y las minas el día en el que la iglesia católica celebra la fiesta de la Natividad de la Virgen; abajo, el monograma del Felipe II, como testimonio de quien otorgó el escudo de armas a la ciudad. En los dos extremos superiores del escudo flotan el sol y la luna en un cielo de color azul intenso. En la falda del cerro hay cuatro retratos de personas en campo: el capitán Cristóbal de Oñate, Juan de Tolosa, Diego de Ibarra y Baltazar Temiño de Bañuelos, siendo los principales fundadores; debajo de ellos aparece el lema "Labor Vincit Omnia" (el trabajo lo vence todo); y en la orla, cinco manojos de flechas y entremetidos con otros cinco arcos, que son las armas de que usaban los referidos indios chichimecas.

La Marcha Zacatecas es obra del compositor zacatecano Genaro Codina. Fue compuesta en el año de 1892 y tocada por primera vez en público en la primavera de 1893, por la Banda del Estado, que era dirigida por Fernando Villalpando y reforzada por la Banda de Niños del Hospicio, además de una banda de guerra. El mérito de la instrumentación de la marcha corresponde a Fernando Villalpando.

Por su aceptación y frecuente interpretación en actos oficiales, está considerada como el segundo Himno Nacional Mexicano, así como el himno nacional de la Charrería Mexicana.

En el siglo XVI, los españoles llamaron "La gran chichimeca" al norte de la mesa central de México, territorio que nunca fue conquistada por los mexicas. Esto ahora es compuesto por los estados de Jalisco, Aguascalientes, Nayarit, Guanajuato, San Luis Potosí, Durango, Coahuila y Zacatecas. Los mexicas llamaron con el etnónimo "chichimeca" a los pobladores de esta gran región aunque fueran de distintas civilizaciones, lenguajes o tribus. Se reportó que en lo que ahora es el estado zacatecano habitaban cuatro etnias primigenias: los caxcanes, guachichiles, tepehuanes y zacatecos, siendo de estos últimos que el estado recibe su nombre moderno.

La mayoría de los pobladores eran nómadas dedicados a la caza, con pocos asentamientos humanos permanentes. El territorio sureño del estado estuvo bajo la influencia mesoamericana, mientras que la mayor parte del estado formaba parte de la región denominada Aridoamérica. Actualmente en Zacatecas se encuentran zonas arqueológicas como La Quemada, localizada en el municipio de Villanueva, y Altavista localizada en el municipio de Chalchihuites. En estas zonas se encuentran edificaciones ceremoniales y pirámides con rasgos arquitectónicos de las culturas mesoamericanas.

En 1531, Pedro Almíndez Chirino junto a sus tropas llegaron de lo que ahora es el noreste de Jalisco, guiados por "Xiconaque" cacique zacateco cuya tierra se encontraba en la actual Lagos de Moreno, Xiconaque los llevó a lo que se llamaba el "pueblo grande de los zacatecos" o Tlacuitlapán donde Chirino no pudo encontrar riquezas considerables, solo describió encontrar en la cima de un cerro con crestón (La Bufa) una aldea indígena cuyas casas eran circulares con techo de paja y hierba.
Sin embargo, en 1546 cuando Juan de Tolosa encabezaba una exploración en la parte sureña del estado, en lo que hoy es Tlatenango, se le acercaron unos indígenas y le mostraron piedras brillantes que contenían plata. De Tolosa fue al cerro de La Bufa, donde se encontraba una aldea zacateca, y se llevaron varias cargas de metal a lo que hoy es Nochistlan. Según se cree, en enero de 1548 Juan de Tolosa, Diego de Ibarra, Cristóbal de Oñate y Baltazar Temiño de Bañuelos se reunieron y fundaron las primeras casas, aunque no hicieron la fundación formal de la ciudad que sería conocida como Minas de los Zacatecas ya que a su llegada ya existían indígenas que habitaban dicha zona, conocida después como la "Civilizadora del Norte". Se supone que la fundación ocurrió el 8 de septiembre de 1546 fecha aproximada en que De Tolosa exploraba el cerro de la Bufa. Zacatecas llegó a formar parte de Nueva Galicia, el nombre que se le dio a un territorio del virreinato de Nueva España. Debido a su riqueza mineral y los fuertes ingresos a la Corona Española, en 1585 Minas de los Zacatecas recibió el título de "Muy Noble y Leal Ciudad de Nuestra Señora de los Zacatecas" y su correspondiente escudo por parte del Rey de España Felipe II.

En Zacatecas como en la mayor parte de México se suscitaron levantamientos y batallas con el fin de independizarse de España, uno de ellos fue la Toma de Zacatecas de 1811. Cuando se conoció captura de los comandantes insurgentes en Acatita de Baján, López Rayón huyó de Coahuila el 26 de marzo, dirigiéndose a

Zacatecas; siendo este seguido por el jefe realista José Manuel Ochoa, con el que libró la Batalla del Puerto de Piñones, derrotando a Ochoa y obteniendo armamento. Luego de varios combates, el 15 de abril López Rayón tomó Zacatecas, ahí fundió artillería, fabricó pólvora y dio uniforme a sus tropas.

Un personaje zacatecano que destacó durante la indepencia fue José María Cos quien participó en la guerra independentista de México casi desde el principio. Se le atribuye, entre otros méritos, el haber impedido que Zacatecas fuera arrasada por la violencia de la guerra y facilitar su incorporación como plaza simpatizante de los insurgentes con el mínimo de sangre derramada. Cuando se instituyó el Congreso de Chilpancingo, Cos participó en él como diputado por la provincia de Zacatecas. Participó en la redacción de la Constitución de Apatzingán, primera ley que rigió el gobierno de la naciente República Mexicana.

El Coronel don Manuel Orive y Novales fue el último intendente de la Provincia de Zacatecas, estando en el poder hasta el 18 de octubre de 1823, fecha en que la diputación provincial declaró el “Estado Libre y Federado de Zacatecas” nombrándose un nuevo cuerpo legislativo que nombró el primer gobernador interino y provisional al Coronel don Juan Peredo que tomó posesión el mismo 18 de octubre de 1823.
La diputación provincial declaró a Zacatecas estado libre y federado el 17 de junio, y el 19 de octubre de 1823 quedó instalado el primer Congreso estatal. Los tres poderes constituidos —Ejecutivo, Legislativo y Judicial— defendieron la autonomía del estado como condición para conservar la integridad nacional. Su postura se identificó con un confederalismo, opuesto a toda actitud separatista, y mantuvo la tradición constitucionalista y legalista que se había arraigado en Zacatecas entre 1810 y 1813. Don José María Hoyos, fue nombrado por la Junta Auxiliar como Gobernador Provisional del Nuevo Estado Federado de Zacatecas, y el mismo día 18 de marzo de 1824, tomó posesión del cargo, mientras se convocaba a los Municipios a que presentaran una terna para elegir al nuevo Gobernador. Las tareas más urgentes del primer Congreso fueron elaborar la constitución y mantener el estado a salvo de las pretensiones centralizadoras del gobierno nacional y, paradójicamente, de Guadalajara, que estaba interesada en conservar su antigua jurisdicción sobre Zacatecas. Dos de los principales temas de discusión de la legislatura fueron el manejo de los recursos económicos del estado, como garantía de su independencia y soberanía, y el equilibrio de los tres poderes.

José María García Rojas fue el primer gobernador elegido en Zacatecas conforme a la nueva constitución. Como gobernador García Rojas estableció la milicia cívica, que sería muy importante en la defensa del federalismo y apoyó la autoridad de poder civil sobre los militares y los religiosos. A finales de 1829, Francisco García Salinas fue electo gobernador de Zacatecas que defendía un modelo federal. Los conservadores zacatecanos favorecían un sistema de gobierno representativo. En dos ocasiones, los conservadores se rebelaron contra el gobierno federal. En la rebelión de 1835, las fuerzas federales del general Santa Anna saquearon la ciudad de Zacatecas y las minas de plata de Fresnillo.

En enero de 1825 se promulgó la Constitución política del estado libre de Zacatecas. Para su preparación, los legisladores analizaron las constituciones más avanzadas de la época, pero tuvieron una clara inspiración en la de Cádiz. La Constitución fue esencialmente un documento ideológico que estableció como forma de gobierno la república representativa popular federal y la división de los poderes; asimismo, definió los vínculos entre el estado y el resto de la nación. El territorio estatal quedó dividido en los partidos de Zacatecas, Fresnillo, Sombrerete, Aguascalientes, Juchipila, Nieves, Mazapil, Pinos, Jerez, Tlaltenango y Villanueva. A los ayuntamientos se les concedió mayor autonomía, con lo que ampliaron su participación en la vida política del estado.
A raíz del fracaso del sistema federal, el centralismo ganó terreno y el Congreso modificó la Constitución de 1824 a fin de crear una república centralista, limitando el poder de los estados y reduciendo el número de tropas militares.
Tales acontecimientos provocaron una rebelión en Zacatecas, donde el propio gobernador, Francisco García Salinas, encabezó un ejército de unos cuatro mil hombres en contra del gobierno. Para poner fin a los sublevados, el presidente Santa Anna, en persona se dirigió a combatirlo, dejando como encargado de la presidencia al general Miguel Barragán.
García Salinas, fue derrotado en la Batalla de Zacatecas (1835), y en castigo por su rebeldía, fue obligado el estado de Zacatecas a perder parte de su territorio, con la que se formó el estado de Aguascalientes.

Durante la Guerra de Reforma (1858—1861) Zacatecas se convirtió en campo de batalla entre liberales y conservadores. En 1859, Jesús González Ortega puso fin a los ideales conservadores del estado cuando decretó una ley en contra de ello.

Una de las batallas más importantes de la revolución ocurrió en la ciudad de Zacatecas el 23 de junio de 1914. Se le conoce como La toma de Zacatecas. En esta batalla, Francisco Villa —asistido por Felipe Ángeles y Pánfilo Natera— y sus dorados tomaron control de la ciudad de Zacatecas, asegurando la seguridad financiera de la revolución. En honor a Pancho Villa, se erigó una estatua en el Cerro de la Bufa y el estadio olímpico de la capital recibió su nombre, no obstante la economía estatal sufrió las consecuencias de estos hechos durante décadas de acentuada recesión económica.

Para conmemorar ese hecho desde 2004 se realiza anualmente en el mes de junio en la capital zacatecana una cabalgata que reúne cientos de jinetes de diferentes localidades del estado.

Zacatecas tiene una extensión territorial de 75.040 km², esto representa el 3,83% del territorio nacional. Sus coordenadas extremas son 25°09' al norte, 21°04' al sur de latitud norte; al este 100°49' y al oeste 104°19' de longitud oeste. Limita al norte con Coahuila, al noroeste con Durango, al oeste con Nayarit, al este con San Luis Potosí y Nuevo León, y al sur con Jalisco, Aguascalientes y Guanajuato.

El estado se encuentra en el norte de México específicamente en la Meseta Central de México, que abarca los estados de Zacatecas, Durango, Chihuahua y parte de Coahuila; entre la Sierra Madre Oriental y la Sierra Madre Occidental, al occidente y suroccidente existen algunas mesetas con una altitud máxima de 2850 metros sobre el nivel del mar (msnm) como el cerro La Aguililla.

En uno de los valles, existe el Cañón de Juchipila, con una altura mínima en el estado de 1000 metros. La Sierra Madre Occidental es la principal cadena montañosa que atraviesa el estado. La elevación más alta es la Sierra El Astillero con una altitud de 3200 metros sobre el nivel del mar, seguido por la Sierra de Sombrerete con 3100 y la Sierra Fría con 3030. El 38,82 % de la superficie estatal es matorral, el 27,38 % de la superficie se usa para la agricultura, el 15,67 % es pastizal, el 12,66 % es bosque, el 1,94 % selva y el resto tiene otros usos.
La mayor parte del estado forma parte del desierto Chihuahuense, caracterizándose por escasa precipitación pluvial y una gran diversidad cactácea.

La entidad carece de ríos importantes; los que hay, en su mayor parte son temporales y se forman al escurrir el agua de las montañas en la época de lluvias. El sistema hidrográfico está formado por dos cuencas: la cuenca del Pacífico a través de otros estados son: San Pedro, Juchipila, Jerez, Tlaltenango, San Andrés, Atengo, Valparaíso. Los ríos de la Cuenca Interior no tienen salida al mar y los principales son: Calabacillas, Zaragoza, Los Lazos, San Francisco y Aguanaval que desemboca en Torreón, Coahuila. En cuanto a agua subterránea, existen 20 zonas geohidrológicas en el estado. El estado cuenta con un total de 80 presas con una capacidad total de 595 337 millones de metros cúbicos destacándose las presas de: Leobardo Reynoso (Fresnillo), Miguel Alemán (Tlaltenango) y El Chique (Tabasco).

Existen 20 zonas geohidrológicas en el estado en las cuales se localizan 5891 pozos profundos con fines agrícolas con gastos hidráulicos que oscilan entre 15 y 60 l/s. Con profundidades de 150 a 250 metros. y niveles dinámicos promedio de 80 metros. Además se tienen 2441 norias o pozos a cielo abierto de poca profundidad y bajo costo de 5 a 10 l por segundo. Igualmente, en diversas regiones del estado se localizan 483 pozos de bajo gasto con fines de abrevadero para ganado. El principal problema que enfrenta el agricultor que extrae agua para usos agrícolas es el costo de la electricidad.

La vegetación de Zacatecas es muy variada. En las sierras existen bosques mixtos de pinos y encinos; los árboles se mantienen verdes todo el año. También hay regiones áridas y semidesérticas que albergan gran cantidad de plantas como las cactáceas. En llanos y valles abundan los mezquites, gobernadoras, huisaches, nopales, lechuguillas, guayules y pastizales.

La fauna de las sierras incluye es, venados cola blanca y liebres; en llanos y valles suelen encontrarse coyotes, tejones, codornices y patos. Otros animales de la región son la víbora de cascabel, chirrioneros, alicantre, rata canguro, ratón de campo, gato montés, murciélagos, águila, guajolote silvestre, topo, tuza, guacamaya enana y la guacamaya verde. Zacatecas es la entidad del país en la que se encuentran más ejemplares de águila real, el símbolo nacional mexicano.
Los matorrales abarcan la tercera parte de la superficie del estado; le siguen en extensión los pastizales y en las partes más elevadas los bosques de coníferas y encinos. Las zonas agrícolas abarcan 25% del territorio.

El estado de Zacatecas existen áreas que cubren varios municipios en las que el suelo, la vegetación, el clima, la fauna es característica sólo de esa parte del territorio se encuentra dividido en 4 zonas


La zona norte es caracterizada por el clima seco desértico, el suelo es duro y salado, dificulta la agricultura; solo se dan el maíz y el frijol. En cambio, las posibilidades del uso pecuario del suelo de esta región son mayores.
La zona centro tiene climas semidesértico y templado semidesértico, algunas partes de la región tienen posibilidades para la agricultura.
La zona sur es la región más grande del estado y cuenta con climas como elseco semidesértico, el templado semidesértico y el templado subhúmedo, abundan los bosques pero también en varias partes crecen matorrales y pastizales; también existes varios ríos, por lo que la región tiene grandes posibilidades para la explotación agrícola.

A nivel nacional, el estado tiene tres representantes en el Senado Mexicano: Carlos Alberto Puente Salas (PVEM), José Marco Antonio Olvera Acevedo (PRI) y Héctor Adrián Menchaca Medrano (PT). Zacatecas tiene ocho representantes en la Cámara de Diputados | LXI Legislatura representantes en la Cámara de Diputados: cuatro diputados por mayoría relativa del PRI, uno por representación proporcional del PAN, uno por representación proporcional del (PT) y uno más por representación proporcional del (PVEM) | LXI Legislatura.

Los poderes gubernamentales del estado tienen sus instalaciones en la Ciudad de Zacatecas.

El actual gobernador de Zacatecas es Alejandro Tello del PRI, quien ocupara el cargo durante el periodo 2016-2021.
Los miembros principales de su gabinete son:

La organización general del Poder Ejecutivo

El Poder Legislativo del estado, está compuesto por 30 diputados; 18 de mayoría relativa y 12 de representación proporcional, de entre estos últimos, por mandato constitucional y desde la quincuagésima octava legislatura hay dos diputados migrantes o binacionales, destacando a Zacatecas como una entidad que ha sabido dar su representación y lugar a los migrantes zacatecanos en puestos de elección popular.

Distritos Federales
Distritos Locales

La organización general de poder legislativo.


El Poder Judicial del estado está compuesto por 13 magistrados. Los miembros del congreso estatal y los ayuntamientos son elegidos por un periodo de 3 años.

Distritos Judiciales del estado

El estado de Zacatecas cuenta relaciones internacionales en las que se busca entre otras cosas atraer inversiones para el estado o hermanamientos, actualmente se cuenta con varios hermanamientos en varias ciudades del estado como las siguientes:


De acuerdo con la Secretaría de Relaciones Exteriores, el estado ha tenido 7 convenios ó acuerdos internacionales estos son:

"Vea: Municipios de Zacatecas."

Zacatecas está dividido en 58 municipios. El municipio de Mazapil es el más grande en el estado. Ocupa alrededor del 36% del área estatal y es más de dos veces la superficie del estado de Aguascalientes. El municipio de Momax es el más pequeño con solo 159 km². El estado tenía 56 municipios, cantidad que se incrementó cuando en 2000 se creó el municipio de Trancoso y en 2005 el municipio de Santa María de la Paz y totalizar los 58 actuales.

Según el Instituto Nacional de Estadística, Geografía e Informática, en el 2010, el Estado de Zacatecas tenía una población de 1,690,750 habitantes con una densidad de 57 habitantes/km². Esto ubica al estado en el 25º lugar de población en la nación y representa el 1,3% del total nacional. El 51,3% (863,771) de la población zacatecana fue compuesta por mujeres mientras que los hombres representan el 48,7% (826,897). Al igual que la nación, la población promedia de Zacatecas es bastante joven, pues es solo de 23 años, lo que lo ubica en el lugar 19°. En los últimos 10 años, la población de Zacatecas aumentó 6%. La densidad fue de 18,13 habitantes por km².

Según el último censo disponible (1921) étnicamente la entidad estaba formada por un 8,55% indígenas, 86,1% mestizos y 5,35% blancos. la población mestiza cuenta con un porcentaje de genes europeos superiores al resto del país.
Estas cantidades se han mantenido hasta la actualidad solo estimando una disminución del porcentaje indígena y un ligero aumento del sector mestizo. Actualmente solo 1,837 personas hablaban un idioma indígena.

Las ciudades consideradas como más importantes del estado, debido a su población son Zacatecas, Guadalupe, Fresnillo, Jerez y Río Grande.

En el estudio más reciente sobre zonas metropolitanas (ZM), publicado en 2010, por el Consejo Nacional de Población (CONAPO), el Instituto Nacional de Estadística y Geografía (INEGI) y la Secretaría de Desarrollo Social (SEDESOL), se estableció que en el Estado de Zacatecas existe sólo una zona Metropolitana.

La Zona Metropolitana de Zacatecas-Guadalupe se conforma por los municipios de Zacatecas, Guadalupe y Morelos. Donde la población asciende a 309 660 personas, una densidad de población de 88.1 habitantes por kilómetro cuadrado. La ZM de Zacatecas-Guadalupe tuvo una tasa de crecimiento media anual de 1,1 % de 2000 a 2010.

En cuanto a urbanismo, de acuerdo a los resultados que presentó el Censo General de Población y Vivienda de 2010, en el estado cuentan con un total de 372 662 viviendas en el estado.

La información más reciente ubica a Zacatecas entre los estados con desarrollo humano medio (IDH de 0.7057). Su posición en la clasificación nacional se ha bajado un lugar (lugar 27) en comparación de 2000 y 2005 cuando estaba ubicado en el lugar 26. En términos relativos, para el año 2005 el índice de desarrollo humano (IDH) estatal fue de 0.7872, valor menor al nacional (0.8200), aunque creció más rápidamente pues mientras el indicador nacional aumentó 1.57%, el del estado lo hizo en 3.19%.

Al interior de la entidad se tiene que el IDH en 2005, para los municipios de Zacatecas (IHD de 0.8900) y Guadalupe (IHD de 0.8799) registraron el mayor nivel de IDH; en contraparte las demarcaciones de El Salvador (IHD de 0.6792) y Jimémez del Teul (IHD de 0.6583) tuvieron los menores niveles de desarrollo humano.

La emigración de Zacatecas hacia Estados Unidos ha representado, históricamente, uno de los flujos más intensos a escala nacional, debido a las fluctuaciones de la minería y a las condiciones agrícolas, entre otros motivos. En 1956, 1957 y 1958, como consecuencia de la crisis agrícola severa originada por la sequía, las salidas de zacatecanos fueron de 9.7, 11.1 y 10.4 por ciento en cada año respectivamente, de las totales nacionales; en 1957, respecto de la población equivalió a 6 por ciento y 21 a la mano de obra (Padilla 2000). Navarro y Vargas (2000) identifican que de 1990 a 1995 cerca de 26 mil zacatecanos abandonaron anualmente la entidad y emigraron a otros estados del país y en especial hacia Estados Unidos.

El estado de Zacatecas inició su incursión como expulsor de fuerza de trabajo desde finales del siglo XIX. En trabajos de antaño como el de Gamio (1930), y en contemporáneos como el de Durand (2005), se reconoce que desde principios del siglo XX, Zacatecas junto con Jalisco, Michoacán y Guanajuato, ya conformaban la región expulsora de fuerza de trabajo.

En este proceso migratorio han influido diversos factores, como la precariedad y el carácter excluyente de la estructura productiva de Zacatecas, caracterizada, entre otras cosas, por un sector industrial limitado, una actividad agrícola poco tecnificada y orientada a la subsistencia familiar; una ganadería de corte extensivo especializada en la cría de bovinos en pie y un sector minero, que casi no incide en el empleo y la economía regional. Estas características sitúan al estado como uno de los de menor capacidad para generar empleo en el país, tal como lo señala el Plan Estatal de Desarrollo (1999–2004). Resulta interesante notar que, aunque la economía zacatecana ha tenido periodos de expansión importantes, en la actualidad se sigue ubicando como uno de los estados con mayor pobreza y marginación del país.

El flujo migratorio sigue siendo negativo, aunque menor que en décadas pasadas. Se estima que la mitad de los zacatecanos viven fuera del estado. En Estados Unidos viven entre 800.000 y 1.000.000. La mayoría reside en Chicago, Denver, Dallas, Houston, Los Ángeles y Phoenix.

La mayoría de los zacatecanos son católicos, a pesar de que el porcentaje de población católica en toda la entidad disminuyó 0,7 por ciento.

En el año 2010, según el más reciente Censo de Población y Vivienda del INEGI, el 94,4 por ciento de los zacatecanos profesaban la religión católica, con lo que superó a Guanajuato que una década atrás encabezaba la estadística. Del millón 490 mil zacatecanos en el estado, un millón 394 mil son católicos. Con excepción de la iglesia católica, prácticamente todas las religiones y cultos principales registraron aumento en su porcentaje de adeptos.

En lo que respecta a protestantes y evangélicos (históricas, pentecostales, neopentecostales) el INEGI reportó 41 mil 878, lo que equivale al 2.8 por ciento de la población, es decir, casi un punto más que lo registrado una década atrás. Grupos religiosos como los Adventistas, Testigos de Jehová, Iglesia de Jesucristo de los Santos de los Últimos Días) llegaron a 15 mil 581 personas, lo que habla de un aumento de apenas .1 por ciento. En todo el estado, solamente fueron contabilizados 94 judíos.

En Zacatecas los templos católicos juegan un papel importante en la cultura que, junto con las plazas, se consideran como el centro de las localidades. El templo más importante del estado es la Catedral Basílica de Zacatecas, considerada como el máximo exponente del barroco en toda América. El Obispo de la Diócesis de Zacatecas es el Monseñor Sigifredo Noriega Barceló, nombrado obispo de Zacatecas por el Papa Benedicto XVI el 2 de agosto de 2012.

Los servicios públicos son aquellos que cumplen una función económica o social (o ambas) y satisfacen primordialmente las necesidades de la comunidad o sociedad donde estos se llevan a cabo.

La tasa de natalidad del estado en 2012 se ubica en el 17.79%, en 2010 se registraron 36 323 nacimientos. Además de tener una población muy joven, en el año 2009, poco más de la cuarta parte de la población en el estado (26.1%) es joven (15 a 29 años). La esperanza de vida al nacer es de 74.8 a nivel estatal,72.1 años para los hombres y 77.7 años para las mujeres. En 2010 fueron registrados 8 202 defunciones, de lo anterior, las principales causas de muerte son las enfermedades del corazón, tumores malignos y diabetes mellitus.

Zacatecas cuenta con: Instituto Mexicano del Seguro Social, Instituto de Seguridad y Servicios Sociales de los Trabajadores del Estado, Cruz Roja Mexicana Delegación Zacatecas. La población de los servicios de salud de Zacatecas es estimada en 553 839 habitantes, mientras que la Delegación Estatal del ISSSTE tiene una población de 144 659 derechohabientes y en cuanto afiliados a oportunidades son en total 253 498. Cuenta con unidades de primer nivel de atención (426 unidades de atención y 16 unidades de especialidad); unidades de segundo nivel de atención (5 Hospitales generales, 15 unidades de atención y 11 hospitales comunitarios); El ISSSTE tiene un total de 35 unidades y 688 personas en recursos humanos.

El IMSS tiene 33 unidades de primer nivel y 2 de segundo nivel. Existen un total de 3240 recursos humanos.

Los Servicios de Salud cuentan con 142 Unidades de primer nivel, 6 de segundo nivel y 75 unidades móviles. Los principales hospitales del estado son: Hospital general de Zacatecas, Hospital general de Fresnillo, Hospital general de Jerez, Hospital general de Loreto y el Hospital de la Mujer Zacatecana.

El estado de Zacatecas tiene una biblioteca por cada 6.250 personas. En el 2005, el 92,7% de la población de 15 años o más fueron alfabetos. El 98% de la población de 8-14 tienen la aptitud de leer y escribir. En el estado existen 1.350 escuelas de preescolar, 2.031 primarias, 1.159 secundarias y 185 bachilleratos.

Cronología de la Educación Superior en Zacatecas

1759 – Fundación del Colegio de San Luis Gonzaga en el lugar que ocupa actualmente la Unidad Académica Preparatoria no. 1 de la UAZ.

1826 – Fundación de la Escuela Normal de Enseñanza Mutua.

1832 – Inicia labores el Instituto Literario de Jerez.

1868 - El Instituto Literario de Jerez cambia de nombre a Instituto Literario de García, se establece la “Junta de Instrucción Pública, Industria y Fomento”, buscando el perfeccionamiento de los escuelas en la ciudad.

1877 - Bajo el gobierno de Trinidad García de la Cadena, se promueve la enseñanza superior para mujeres.

1912 – Durante la breve administración del Lic. José Guadalupe González, se construyó en la población de Guadalupe el edificio de la Escuela de Agricultura quedando abandonado al morir el gobernante, el 29 de diciembre de 2012.

1927 – Comenzó a funcionar la imprenta de Enrique García, el primer linotipo del Estado.

1935 – Son fusionadas la Escuela de Artes y Oficios de Guadalupe y el asilo de niñas, debido a dificultades económicas, quedando administrada por la Secretaría de Educación Pública.

1937 – Reabre sus puertas el Instituto de Ciencias, habiendo siendo clausurada años antes junto con las escuelas normales de Zacatecas y Río Grande.

1940 – Reabre sus puertas la Escuela Normal de Zacatecas.

1947 – Se iniciaron los Juegos Florales Ramón López Velarde, siendo reprimidos en el año de 1972.

1948 – Se inauguraron los cursos de primavera, con la finalidad de difundir la cultura en todo el Estado. Siendo el mismo año que tuvo la novena sesión del Congreso Mexicano de Historia.

1950 – 1956 - Durante el periodo de la administración de José Minero Roque, se enriqueció la Biblioteca Pública del Estado.

1957 – 1972 Estuvo en funcionamiento la Escuela Normal del Colegio del Centro.

1968 – El Instituto de Ciencias Autónomo se convirtió en lo que ahora es la Universidad Autónoma de Zacatecas, siendo su primer rector Magdaleno Varela Luján.

En el Estado de Zacatecas las decisiones que el gobierno toma en materia de política pública pero dirigida a la cuestión educativa, son los lineamientos que han de conducir y acompañar al modelo educativo vigente en cada sociedad. Son las reglas a seguir, las disposiciones que el Estado establece y que se aplica al Sistema Educativo. México es un país con una extensa diversidad de cultura, lenguas, tradiciones y costumbres por lo que nos caracteriza como país en lo cual hace énfasis en los planes y programas de estudio, y en el mejoramiento de la educación en México.

El estado de Zacatecas cuenta con las siguientes universidades, centros de Investigación e institutos:

Públicas

Privadas

Escuelas Normales

En esta entidad se encuentran una gran cantidad de monumentos, reconocidos internacionalmente por sus estilos barroco, gótico, churriguresco y colonial. La conquista religiosa fue llevada a cabo prioritariamente por la orden Franciscana los que fundaron el hospicio en 1558; en 1567 tenían ya un gran convento y 1603 se creó la custodia de provincia de Zacatecas. Posteriormente llegaron los Agustinos, Dominicos y Jesuitas quienes no solo evangelizaron a los indígenas sino coadyuvaron a volverlos sedentarios, por lo que alrededor de sus capillas y conventos se fueron fundando poblaciones que en la actualidad son ciudades muy populosas.
El Acueducto el cubo hacía llegar el vital líquido

hasta el sitio donde se ubica el monumento al general Jesús González Ortega y de ahí hacia una pila de agua que se ubicaba en la Plaza Independencia, partiendo del tiro de la mina del Cubo, de ahí su nombre popular.
Durante los siglos XVII y XVIII también se desarrollaron interesantes muestras de arquitectura barroca, además en este periodo también se construyeron varias haciendas en varios municipios del estado como la hacienda del Condado de San Mateo en Valparaíso. Para el siglo XIX durante el porfiriato se construyeron en la entidad varios
edificios, principalmente en la capital, de los que destacan el teatro Calderon y el mercado González Ortega ubicados en el centro de la ciudad de Zacatecas.

De las construcciones religiosas se destacan la Catedral Basílica de Zacatecas, el ex convento de San Francisco, templo de Fátima, templo de Santo Domingo, ex templo de San Agustín, Santuario de Plateros entre otros.

Zacatecas cuenta con un gran número de museos, la mayoría ubicados en la ciudad de Zacatecas, tiene grandes acervos artísticos y con diversidad temática. Dichas colecciones son exhibidas en espacios históricos y artísticos (ex conventos, ex templos, edificios coloniales, señoriales residencias, ex reclusorios, centros de enseñanza, modernos recintos, etc), que para tal efecto fueron acondicionados adecuándolos a las exposiciones. Entre los museos del estado se encuentran el museo Rafael Coronel
donde se exhibe interesantes colecciones de arte popular mexicano, las cuales están encabezadas por la denominada "El rostro de México" y que está integrada por más de diez mil máscaras mexicanas, el museo Pedro Coronel donde se encuentra una colección de arte universalcon más de 1300 piezas de diversas culturas, el museo Zacatecano de cultura, el toma de Zacatecas, Museo Francisco Goitia, el museo Mina el Edén y el museo de arte religioso ubicado en la ciudad de Guadalupe, en este museo se encuentra una de las colecciones de pintura virreinal más importantes de México, así como el museo de arte abstracto Manuel Felguérez.

Otros museos del estado son: el museo de ciencias de la UAZ, la galería episcopal, el museo de Historia del Transporte y el Museo de las migraciones. De los museos arqueológicos los existentes son el museo arqueológico La Quemada y el museo arqueológico sitio Altavista; de los museos interactivos de la entidad son el museo interactivo Zig Zag, único museo en el estado de Cuarta Generación y el museo interactivo Casa Ramón López Velarde ubicado en la ciudad de Jerez. A lo largo del estado existen también varios museos regionales.

En la entidad se celebran las festividades de , también se destacan las conmemoraciones oficiales como el aniversario de la Independencia de México (16 de septiembre), Día de Muertos, aniversario de la Revolución Mexicana, etc. En la Semana Santa católica se llevan a cabo representaciones del viacrucis y crucifixión.

También ocurre el Internacional Festival de Teatro de Calle (la tercera semana de octubre), que presenta a compañías teatrales de México y el extranjero que ofrecen funciones en plazas, museos y otros espacios al aire libre de la ciudad de Zacatecas. Surgió en el año 2001, con el propósito de llevar el teatro a su recinto original, la calle, así lograr un mayor acercamiento de esta disciplina artística al público en general, especialmente a los niños y jóvenes.

En la entidad se celebran las festividades de , también se destacan las conmemoraciones oficiales como el Aniversario de la Independencia de México (16 de septiembre), Día de Muertos, Aniversario de la Revolución Mexicana, etc. En Semana Santa se llevan a cabo representaciones del viacrucis y crucifixión.

Las festividades en Zacatecas tienen características y matices muy especiales, aunque durante el año se realiza una programación de eventos, las galas festivas se muestran durante los grandes festivales como: el Festival Cultural Zacatecas (en las semanas Santa y Pascua); el Internacional del Folclor (celebrado en mes de julio); las tradicionales Morismas de Bracho (en la última semana de agosto); la Feria Nacional de Zacatecas (las dos primeras semanas de septiembre)

o el también Internacional Festival de Teatro de Calle (la tercera semana de octubre), que presenta a compañías teatrales de México y el extranjero que ofrecen funciones en plazas, museos y otros espacios al aire libre de la ciudad de Zacatecas. Surgió en el año 2001, con el propósito de llevar el teatro a su recinto original, la calle, así lograr un mayor acercamiento de esta disciplina artística al público en general, especialmente a los niños y jóvenes.

Las fiestas y ferias en el interior del estado son principalmente para festejar a un santo patrón determinado. Entre las festividades tradicionales que más destacan son las "callejoneadas", en donde se recorren los escondidos callejones de la ciudad, saboreando un exquisito mezcal de Huitzila y degustando una muestra de la gastronomía regional.

Otras fiestas del estado son: la jerezada, la feria de primavera de Jerez, el festival barroco del museo de Guadalupe y el festival de la poesía zacatecana. Las fiestas patronales de algunos municipios del estado son:


En el estado de Zacatecas, al igual que en otras regiones de México, el desarrollo económico o la adopción de nuevas pautas culturales provocaron en muchas comunidades urbanas o rurales, un progresivo abandono de la producción en diversas ramas del artesanado tradicional.

Sin embargo, en los últimos años se vienen realizando intentos desde algunos organismos oficiales para preservar la influencia de aquellos maestros artesanos que aún subsisten y propagar sus conocimientos. Una encuesta reciente del IDEAZ (Instituto de Desarrollo Artesanal de Zacatecas) ha permitido elevar el censo de artesanos de 150 a 1500 en toda la entidad. Entre otras iniciativas, se pretende crear un sello de alta calidad para la platería, ya que la plata zacatecana es de las mejores del mundo.

En Zacatecas las principales ramas artesanales actuales son la platería, la cerámica y alfarería, el tallado de cantera, los textiles, la talabartería y los trabajos con pita y lechuguilla. Se producen también artículos de madera tallada, cerámica, herrería artística, cestería, vidrio soplado, resinas, papel maché, y obras en rocas, piedras semipreciosas y metales como el cobre, el hierro y el oro. Objetos de cuero con pirograbados, muebles de madera confeccionados a mano, trabajos en piel y peluche, en macramé, muebles en miniatura, figuras de yeso y en chaquira. Se elaboran además productos artesanales comestibles, como bebidas a partir del agave, dulces típicos y comidas o platillos tradicionales.

Los trabajos de labrado en cantera, siempre han sido muy destacados en este estado. Esta actividad se ha preservado sobre todo en Fresnillo. Los textiles de Guadalupe, Jerez, Villa García y Zacatecas tienen son destacan. Se realizan allí sarapes (especie de manta campesina) y jorongos (con abertura a modo de poncho), chamarras (camperas) y otros artículos de lana, elaborados en telar de pedales.

El territorio huichol abarca zonas de la Sierra Madre Occidental principalmente en los estados de Jalisco y Nayarit, y menor en Durango y Zacatecas. Los huicholes producen collares, pulseras y colgantes elaborados con chaquiras (mostacillas), fajas y morrales de lana tejidos y bordados con motivos abstractos o naturistas, ropa y sombreros ceremoniales, etc. Estos objetos son originalmente valorados por su capacidad para brindar protección tanto física como espiritual para el usuario.

Por ser un estado ganadero, son muy valorados aquí los artículos en piel, sobre todo aquellos relacionados con la charrería, como ser monturas, cintos, botas, fundas de pistolas y arneses para caballos. En Jerez se elaboran sillas de montar y sus diferentes accesorios, al igual que productos utilitarios como bolsas de mujer (carteras) y monederos. Todos ellos son profusamente decorados (piteados) con hilos de pita que permiten resistir un uso intensivo. El trabajo con la fibra de pita es característico del suelo zacatecano.

En la región semiárida zacatecana, se produce una fibra conocida como lechuguilla, con la que se producen diversos objetos tejidos. Salvador Mazapil, Concepción del Oro y Melchor Ocampo son los tres municipios donde numerosos pobladores se sostienen del trabajo con este material El tallado de piedras preciosas o semipreciosas se hacen en la ciudad minera de Zacatecas, sobresaliendo las turquesas engarzadas en metal. A lo largo del estado existen varios museos regionales donde se exhiben artículos propios de la región.

Existen bailes muy reconocidos y recordados entre la población zacatecana como es el baile de mexicapan que representa un propósito vivido y que refleja el carácter del zacatecano: Recio, galante y fanfarron. Existe un conjunto de melodías La Jesusa, Las Barracas, La Varsovina, El Barretero, La Botella, y el Diablo Verde; indiscutiblemente que con influencia europea, como resultado del auge minero de Zacatecas. En el sur de Zacatecas es famosa la danza de los tastuanes en los municipios de Apozol, Juchipila y Moyahua de Estrada.

En todo el estado zacatecano es famoso el corrido, en sus múltiples manifestaciones. En el sur del altiplano y norte de la sierra suelen encontrarse las bandas de aliento, mejor conocido como tamborazo. "La Marcha Zacatecas" de Genaro Codina, se ha oído por años en toda la república y el extranjero; es el himno de las asociaciones charras y está considerado, por su frecuente interpretación en actos oficiales como el "segundo himno nacional". Asimismo se puede escuchar por todo el territorio estatal la música norteña identificada principalmente por el acordeón.

Han sido célebres las interpretaciones de la internacional Banda de música del estado de Zacatecas, dirigida por Octaviano Sigala, Juan Pablo García y Salvador García; así como de la Orquesta Típica de señoritas, dirigida por Fernando Villalpando. Es importante destacar el tamborazo (proveniente de Jerez) que no puede faltar en las fiestas zacatecanas. Los jaraberos de Nochistlán, música considerada de las más antiguas del estado.

A partir de 2004, el gobierno del estado otorga la medalla al mérito musical "Candelario Huízar" a personajes destacados por su trayectoria en favor del desarrollo musical en Zacatecas. Los músicos galardonados han sido:

La entrega de la presea se realiza el 2 de febrero de cada año en conmemoración del natalicio de Candelario Huízar.

La música coral ha sido una de las principales manifestaciones musicales en el estado. Han sido localizados manuscritos de obra vocal para agrupaciones musicales en la Catedral Basílica de Zacatecas, el Convento de Guadalupe y la biblioteca de la Undad Académica de Artes de la Universidad Autónoma de Zacatecas cuenta con un acervo importante de manuscritos de compositores zacatecanos de los siglo XIX y XX, entre otros: Fernando Villalpando, Francisco Aguilar y Urizar, Manuel Barrón y Soto, Severiano González, Isauro Félix, Candelario Huízar, Samuel de la Trinidad Herrera, Luis G. Araujo y Octaviano Sigala.

Las agrupaciones corales más destacadas en el siglo XX fueron

La Sociedad Coral de Zacatecas y el Ensamble Vocal Polifonía mantuvieron actividad constante durante más de una década realizando conciertos, grabaciones, giras y participaciones en puestas en escena de ópera. Ambas agrupaciones participaron en varias ediciones del Festival Cultural de Zacatecas. A su vez, el Ensamble Vocal Polifonía obtuvo en dos ocasiones (1985 y 1987) el primer lugar nacional del Concurso Nacional de Conjuntos Musicales del Magisterio, convocado por la Secretaría de Educación Pública y realizado en el Conservatorio Nacional de Música.

A inicios del siglo XXI surgió en la ciudad capital el proyecto "Coro Monumental de Zacatecas" conformado por un coro de cámara y varios coros de niños, jóvenes y adultos mayores. El proyecto tuvo corta duración con actuaciones trascendentes en los actos oficiales del Ayuntamiento de Zacatecas.

En 2007 se establece el Taller de Ópera de Zacatecas, que posteriormente derivó en lo que hoy es la Compañía de Ópera de Zacatecas. A partir de 2010 se conformó el Coro del Estado, única agrupación coral profesional activa en la actualidad.

El estado cuenta con estas agrupaciones orquestales:
Entre otras

Es clara la diferencia en la indumentaria regional para la mujer y el hombre.



La gastronomía de Zacatecas es amplia y variada, es una cocina rica en sabores y aromas, se basa principalmente en productos como las carnes, el maíz, el chile, el jitomate, frutas y los frijoles, birria y tacos de canasta, cómo dijo Delia.

Entre los platillos principales destaca el asado de bodas, el cual está hecho a base de carne de puerco con una salsa de chiles y chocolate, y que, como su nombre lo dice, tradicionalmente se servía durante las bodas. Otros de los platillos principales de Zacatecas son la birria de carnero, el popular menudo, las enchiladas, las gorditas rellenas de guisados y la carne adobada. Las gorditas con chile y manteca nacen durante la Revolución, y eran preparadas por las adelitas para sus hombres, y las hacían básicamente con esos dos ingredientes. Hoy en día, existe una gran variedad de gorditas y panecillos que juegan un importante papel en la alimentación popular, se hacen de maíz o de trigo, dulces o salados, en comal o en el horno. Las gorditas pueden ser rellenadas de una gran variedad de guisos (mole con arroz, nopalitos con huevo, chicharrón con chile, rajas con papas, lengua, hígado, alambre, yesca, entre otros. También se acostumbran las panuchos y las semitas, que se elaboran con harina de trigo, leche, canela y azúcar, y decoradas con pasitas, coco o nuez, los condoches que son gorditas de maíz tierno que se cuecen sobre las hojas de elote.

Zacatecas cuenta con la tradición de producción de mezcal y el estado es parte de la lista de entidades con denominación de origen para la producción de mezcal junto con otros 7 estados. Además esta región produce vinos de mesa reconocidos internacionalmente, excelentes vinos tintos, blancos y rosados, ideales como acompañamiento en las comidas, dentro de otras bebidas se deben mencionar el pulque y el aguamiel, junto con el aguardiente de caña y el colonche la bebida regional por excelencia, que se obtiene de la fermentación del jugo de la tuna cardona.

En lo referente a los postres destacan las cocadas jerezanas, melcochas, ates (pasta dulce) de guayaba y membrillo, jamoncillos de leche, miel de tuna, así como los tradicionales dulces de camote, biznaga, chilacayote y calabaza. En época de cuaresma se consume la capirotada como postre principal.

El Estado de Zacatecas puede considerarse que se encuentra bien comunicado, dado que se encuentra en la zona centro-norte. Existe una red ferroviaria que actualmente solo se utiliza para transporte de carga con una longitud de 671 km. Asimismo se cuenta con varias centrales de autobuses a lo largo del estado donde las más importantes son la de Zacatecas y la de Fresnillo, cuanta además con un Aeropuerto Internacional, que tiene servicio nacional e internacional con vuelos nacionales a Tijuana e internacionales a Chicago-Midway y Los Ángeles; cuenta con 5 aeródromos en los municipios de Mazapil, Tlaltenango y Huanusco.

En cuanto a caminos, la red carretera federal, estatal y rural tiene una longitud de 11 842 km y cuenta con una red de carreteras pavimentadas que cruzan su territorio, asimismo cuenta con una amplia red que comunica la capital con todos los .

Las principales rutas que cruzan el estado son: carreteras federales, la Carretera Federal 23 Fresnillo, ZAC - Chapala, JAL; la Carretera Federal 25, Aguascalientes, AGS - Loreto, ZAC; la Carretera Federal 44 Fresnillo - Valparaíso; la Carretera Federal 54 Saltillo - Zacatecas.

El estado cuenta con 188 438 líneas de teléfono fijas, 851 oficinas postales, 30 oficinas de telégrafos, operan en el estado 19 radiodifusoras (13 de amplitud modulada y 6 de frecuencia modulada) así como 16 estaciones televisoras. La cobertura de la radiodifusión de amplitud y frecuencia modulada.

En la entidad se publican cinco periódicos, Imagen, Página 24, El Sol de Zacatecas (fundado en 1965, el cual es uno de los diarios con mayor difusión y ventas en el estado con un tiraje de 17 217 ejemplares, de periodicidad diaria y miembro de la Organización Editorial Mexicana), La Jornada Zacatecas y el Diario NTR; dos canales de señal abierta de televisión con 12 repetidoras de Televisa y TV Azteca, así como una gran variedad de programas locales por cable y publicaciones impresas en municipios.

Debido a su ubicación geográfica, Zacatecas está conectado con los principales puertos y centros económicos del país. El Estado de Zacatecas tradicionalmente ha tenido muy pequeña su aportación al producto interno bruto (PIB). En la actualidad su participación en el total nacional es tan solo de.9 por ciento.

Zacatecas recibió 5.9 millones de dólares por concepto de inversión extranjera directa (IED) en 2011. La industria manufacturera fue el principal destino de la inversión extranjera directa recibida por el estado en el año de referencia.
Zacatecas forma parte del recién creado Corredor Económico del Norte de México, integrado por los estados de Chihuahua, Coahuila, Durango, Nuevo León, Sinaloa, Tamaulipas y Zacatecas.

La PEA de 12 años en adelante es de 358,449 personas, esto es el 37.5%, ubicando a Zacatecas 11.8 puntos por debajo de la media nacional, que es de 49.3%. Sobresale la población económicamente inactiva, donde 62 de cada cien personas no trabajan, lo que nos ubica 12 puntos arriba el parámetro nacional. La población ocupada, es de 353,628 personas (98.66%) de las cuales el 53.2% son empleados y obreros, el resto trabaja por su cuenta o está en otra situación. En esta misma categoría sobresalen las mujeres, donde 68 de cada cien trabajan como empleadas y obreras. La población ocupada se encuentra distribuida de la manera siguiente: 73,126 personas en el sector primario, 94,549 en el sector secundario y 174,981 en el sector terciario; en este último se concentra casi el 50% de la población ocupada.

Están clasificadas como primarias: la agricultura, la ganadería, la silvicultura, la pesca, la minería, etc.

Agricultura

Los productos agrícolas que se cosechan son cereales (la producción de los cuales depende de la intensidad de las precipitaciones), y el maguey, el cual depende de la irrigación de los valles bajos, y que se desenvuelve con facilidad en climas secos.

De acuerdo a las cifras del Censo del 2000, se sembraron en el Estado 1’303,564 hectáreas, de las cuales a los cultivos cíclicos corresponden 1’241,824 hectáreas, y a los perennes apenas 61,735. Dentro de los cíclicos la mayor superficie se siembra de fríjol 755,615 hectáreas pero dado la baja o depresión del precio de esta leguminosa se espera que se sembrara una menor superficie en el futuro próximo. Cabe mencionar que en la siembra de maíz fue de 356,166 hectáreas, 64,177 de avena forrajera; 34,150 de chile, 5,248 de cebada, 4,025 de avena, además de siembra de sorgo, cebolla, ajo, etc. Por lo que respecta a los perennes se siembran 22,012 hectáreas, de durazno, 14,181 de nopal, 8,605 de alfalfa, 5,246 de guayaba etc.

Los cultivos cíclicos que más se siembran son: fríjol, maíz, avena forrajera, chile y cebada. Además se siembran avena, sorgo, cebolla, ajo, durazno, nopal, alfalfa y guayaba. En total, se usan 1.303.564 hectáreas para la agricultura.

Ganadería y pesca

La ganadería es también prioritaria en la economía pues el estado de Zacatecas cuenta con grandes extensiones de agostadero 5’388,434 hectáreas susceptibles de aprovecharse en actividades ganaderas. De acuerdo al Censo del año 2000, se producen anualmente 1’037,287 cabezas de ganado bovino, entre: producción de carne, leche,245,762 cabezas de porcino, 310,023 de cabezas de ganado ovino; 546,414 caprinos; 209,707 equinos entre caballos, asnos y acémilas; 1’862,726 aves gallináceas y 30,442 guajolotes y 46,426 colmenas entre rústicas y modernas. Cabe destacar que a pesar que el Estado carece de litorales, el volumen de captura de productos acuícola es de 5,095 toneladas destacando la tilapias, la carpa el bagre y la lobina.

Silvicultura
Con respecto a la silvicultura se obtienen 58,344 m3 en rollo de productos forestales maderables, siendo los principales el encino y el pino.

Minería

La minería es por mucho una de las actividades más antiguas realizadas en Zacatecas, así como una de las más importantes, destacan principalmente la extracción de plata, oro, mercurio, hierro, zinc, plomo, bismuto, antimonio, sal, cobre, cuarzo, caolín, ónix, cantera, cadmio y Wollastonita. Las riquezas minerales del estado fueron descubiertas poco después de la conquista, y algunas de las minas (y las más famosas de México) datan de 1546. Las más productivas son las minas de plata de Alvarado. Sólo de esta mina se extrajeron más de 800 millones de dólares durante 1548 y 1867, según los registros que se han conservado. Hoy en día, México es el primer productor de plata del mundo.

Actualmente la zonas mineralizadas más importantes, contienen principalmente agregados minerales en forma de óxidos y sulfuros complejos de plomo, zinc y cobre, con pequeñas cantidades de plata y oro. Tales zonas están localizados principalmente en 13 distritos mineros, entre los que destacan por su importancia los de Fresnillo, Zacatecas, Concepción del Oro, Mazapil, Sombrerete y Chalchihuites, siendo más importante en estos últimos años Noria de Ángeles.

Existen 86 unidades económicas en la actividad minera. La manufactura es un sector de la economía en crecimiento, de estas la industria alimenticia y de bebidas es la más grande. Cabe destacar que en las minas citadas también existen importantes yacimientos de minerales no metálicos de uso industrial como: caolín, ónix, cantera, wallastonita y cuarzo, entre otros.

Este sector se refiere a las actividades industriales, aquellas que transforman los recursos del sector primario.

La manufactura es un sector de la economía en crecimiento, de estas la industria alimenticia y de bebidas es la más grande; de la industria manufacturera zacatecana sobresale la elaboración de cerveza, que además de aparecer como la actividad industrial más importante al aportar el 24.8% de la producción bruta total de la entidad y 26.6% de los activos fijos, también ocupa el primer lugar en la generación del valor agregado. asimismo destacan por su aportación al Producto, la actividad de Alimentos Bebidas y Tabaco con el 55.93%, seguido de Productos Metálicos, Maquinaria y Equipo con el 22.41%. Los productos que tuvieron mayor dinamismo en Zacatecas por su tasa de crecimiento anual fueron: los productos de minerales no metálicos, seguidos de los productos metálicos, maquinaria y equipo.

Actualmente se trata de consolidar el Parque Industrial de Fresnillo, al cual se dotó de infraestructura en su área de reserva territorial, con la urbanización de 15 hectáreas (ampliación del Parque Industrial Fresnillo), donde se obtuvo facilidad de acceso y conectividad para el establecimiento de nuevas empresas, también se han creado 10 nuevas naves industriales, de 500m2 cada una, totalmente acondicionadas para ese fin.

En el Parque Industrial Calera se realizan trabajos de nueva infraestructura, la cual se ejecuta en sus vialidades respecto a pavimentos, alumbrado público, cerco perimetral, módulos de acceso y parador de autobuses. La industria aeroespacial se instaló en Zacatecas. La empresa Grupo Everest, del sector de proyectos industriales de manufactura en México, ha consolidado la instalación de una empresa ancla, triumph group, dedicada a la manufactura de partes para aeronaves, ultimadamente como resultado de la inversión japonesa se inauguró otra empresa japonesa en la entidad llamada Koide Kokan México.

Comercio

Cuenta con 48 257 unidades económicas, el 1.3 % del país, en cuando a ocupación emplea 174 368 personas, el 0.9% del personal ocupado de México. Del total del personal ocupado en la entidad, el 60% (103 894) son hombres y el 40% (70 474) son mujeres, en promedio, las remuneraciones que recibe cada trabajador al año en Zacatecas son de 72 211 pesos mexicanos, el promedio nacional es de $99 114.

En la entidad, se registra la existencia de 265 hoteles y 6 815 habitaciones, 604 establecimientos de preparación en servicios de alimentos, bebidas, recorridos nocturnos de leyendas, tours en tranvías, servicio de guías por hora, entre otros servicios turísticos. El estado de Zacatecas posee comunidades coloniales con reconocimiento internacional como Zacatecas, capital del estado; Guadalupe, Fresnillo, Sombrerete y Jerez de García Salinas.

Las localidades más visitadas son:

Mercado y la Plaza de Toros San Pedro (hoy convertida en hotel, uno de los más bellos de Latinoamérica), también es famosa por su arquitectura barroca y churrigueresca, aunque en realidad el único edificio puramente barroco es la fachada principal de la catedral y sus dos torres, de las cuales la torre norte se terminó hasta 1904, así como los 8 retablos bañados en oro de los altares laterales del templo de Santo Domingo, existe una iglesia de estilo gótico que es el templo de Fátima, construido a mitad del siglo XX única construcción de su estilo en el estado. Ciudad declarada Patrimonio Cultural de la Humanidad por la UNESCO en 1993. En Zacatecas se puede admirar el pueblo desde un teleférico que cruza el centro desde el Cerro de la Bufa hasta el Cerro del Grillo.

Se realiza cada año durante la Semana Santa una semana Cultural, donde todas las expresiones del arte se manifiestan en teatros, y escenarios naturales como plazas, plazuelas, calles y callejones, Esta semana cultural recibe artistas de todas partes del mundo y es considerado uno de los festivales culturales más importantes de América, solo después de festival Cervantino.

El último jueves, viernes, sábado y domingo del mes de agosto se celebran las morismas de bracho una representación de la batalla de lepanto moros vs. cristianos sucedida en 1571, es una de las 3 representaciones del mundo junto con la de Granada y Alicante en España, siendo está la de Zacatecas la más monumental con cerca de 15.000 actores y que tiene más de 100 años de tradición.
Del último domingo de julio al primer domingo de agosto se realiza el festival zacateas del folclor internacional "Gustavo Vaquera Contreras" donde nos visitan grupos de danza de los cinco continentes, así como de los estados de México, es considerado uno de los mejores 5 festivales del mundo, siendo el más importante del continente americano, los bailables son en plazuelas, plazas y lugares especiales, las presentaciones de gala que se hacen en el Teatro Fernando Calderón y Ramón López Velarde.

En septiembre se realiza la Feria Nacional de Zacatecas (FENAZA) durante las tres primeras semanas del mes, con eventos como corridas de toros, peleas de gallos, exposiciones, variedades artísticas y diversiones, considerada una de las 3 ferias más importantes de México y es una de las pocas ferias que no cobran el acceso a sus instalaciones.

También cada año en el mes de octubre, la ciudad de Zacatecas mantiene vigente su cita internacional con el festival de teatro de calle, siendo el único festival en su género en todo México, abriendo las puertas de sus plazuelas y callejones a las compañías que han hecho del espacio abierto, el escenario de su creatividad artística, donde invaden todos los espacios posibles para adentrarse en los sentidos de cada uno de los espectadores, para eliminar la barrera que divide la vida cotidiana del mundo de los sueños.

De esta forma, en el transcurso de estos ocho años, el Festival Internacional de Teatro de Calle se ha consolidado como uno de los proyectos culturales que han logrado alcanzar un importante objetivo: movilizar e integrar al público con los actores, en el marco incomparable de un espacio urbano patrimonio de la humanidad.

Todo el año Zacatecas realiza con eventos culturales diversos, y se cuenta con una gran infraestructura hotelera, siendo una de las mejores opciones para el turismo nacional e internacional.

Es el lugar del famoso Santo Niño de Atocha una imagen romana comprada por México a España

Centro Histórico de Zacatecas

En 1972 se celebró la Convención del Patrimonio Mundial parte de la Organización de las Naciones Unidas para la Educación, la Ciencia y la Cultura (UNESCO) y la Organización de las Naciones Unidas (ONU) en la ciudad de París en la que el Centro histórico de Zacatecas fue declarado Patrimonio de la Humanidad por la UNESCO en el año de 1993. Notable por su arquitectura y sus numerosos museos, la Catedral Basílica de Zacatecas, construida entre el 1730 y 1760, el Teatro Fernando Calderón, el Acueducto El Cubo, el Palacio de Gobierno, La Plaza de Armas, o el Museo de la Toma de Zacatecas, son solo algunos ejemplos.

Camino Real de Tierra Adentro

En el marco de la 34.ª reunión del Comité de Patrimonio Mundial de la UNESCO que se efectúa del 25 de julio al 3 de agosto en la capital brasileña de Brasilia, el comité votó y declaró al Camino Real de Tierra Adentro, también conocido como El Camino de la Plata como patrimonio mundial.

Dicho Camino es el más antiguo de América abarca una extensión de 2 900 km que parte desde la Ciudad de México hasta Santa Fe, Nuevo México, Estados Unidos. Dicha ruta fue trazada en el siglo XVI por los conquistadores españoles para desarrollar el comercio, facilitar las campañas militares, apoyar la colonización y evangelización en la Nueva España. Representa además uno de los puentes culturales más relevantes que unen a ambas naciones.

En Zacatecas los sitios que recibieron esta distinción son el ex Colegio Apostólico de Nuestra Señora de Guadalupe, en Guadalupe, la Cueva de Ávalos en Ojocaliente; el Santuario de Plateros en Fresnillo; el templo de San Nicolás Tolentino y el Centro Histórico de Pinos; los templos de Nuestra Señora de los Ángeles y Nuestra Señora de los Dolores, en Noria de ángeles y Villa González Ortega, el templo de Noria de San Pantaleón, Conjunto Histórico y Sierra de Órganos en Sombrerete; Conjunto Histórico de Chalchihuites, así como el Camino Real de Palmillas en Ojocaliente.

La UNESCO determinó que una carreta que forma parte de los murales prehispánicos de la Cueva de Ávalos será considerada el emblema de los 60 sitios que conforman el itinerario cultural declarado Patrimonio de la Humanidad. En este lugar fue plasmado en pintura rupestre el contacto con los colonizadores.

Las características de estos pueblos incluyen el estar ubicados en zonas cercanas a sitios turísticos o grandes ciudades, tener accesos fáciles por carretera. El Estado de Zacatecas cuenta con cinco Pueblos Mágicos que son:

Son más de 500 zonas arqueológicas los que se encuentran en todo el territorio zacatecano, de los cuales destacan La Quemada y Altavista.

La Quemada

La Quemada es una zona arqueológica singular en el mosaico de los sitios mesoamericanos. En 1615, Fray Juan Torquemada la identificó como uno de los lugares visitados por los Aztecas en su migración hacia la cuenca de México.

Altavista

La zona arqueológica Altavista fue un centro ceremonial y astronómico producto de la rama súchil de la cultura Chalchihuites, cuya ocupación y desarrollo tuvo un período de aproximadamente 800 años. Esta zona es considerada como un importante centro ceremonial-astronómico de la cultura Chalchihuites, cuyos vestigios arqueológicos son: la Plaza de la Luna (o salón de columnas), la pirámide votiva, la escalera de Gamio y el Laberinto. En esta última se pueden apreciar con puntualidad y precisión los respectivos equinoccios de las estaciones.

El estado participa dentro del Sistema Nacional de Cultura Física y Deporte en la región II conformada por los estados de Chihuahua, Durango y Zacatecas.

Igualmente en el Consejo Nacional del Deporte de la Educación participa en la Región Norte, zona integrada por las universidades de los estados de Chihuahua, Durango y Zacatecas. Por Zacatecas compiten las siguientes universidades:
- Universidad Autónoma de Zacatecas
- Universidad Autónoma de Durango. Campus Zacatecas
- Escuela Normal Manuel Ávila Camacho
- Escuela Normal Rural Matias Ramos

El profesionalismo se encuentra representado por el equipo de fútbol Mineros de Zacatecas
que compite en el Ascenso MX, la segunda categoría más alta en el balompié mexicano
, y el equipo homónimo de Basquetbol de la Liga Nacional de Baloncesto Profesional de México.

En tiempos pasados el estado solía contar con equipos muy competitivos como los Mineros de Zacatecas del desaparecido Circuito Mexicano de Basquetbol del que fue campeón en 2003, los Tuzos de la UAZ equipo de béisbol profesional de los años ochenta, y la Real Sociedad de Zacatecas finalista en 1997 de la Liga de Ascenso de México.

Fundadores y conquistadores


Artistas

Política y Militar

Religiosos


Deportistas

Intelectuales




</doc>
<doc id="3015" url="https://es.wikipedia.org/wiki?curid=3015" title="Zootecnia">
Zootecnia

La zootecnia es una ciencia que estudia diversos parámetros para el mejor aprovechamiento de los animales domésticos y silvestres, pero siempre teniendo en cuenta el bienestar animal ante todo y si estos serán útiles al hombre con la finalidad de obtener el máximo rendimiento, administrando los recursos adecuadamente bajo criterios de sostenibilidad. 

Se ocupa del estudio de la producción de animales, así como de sus derivados (carne, huevo, leche, piel, etc.), teniendo en cuenta el bienestar animal; fijándose como objetivo la obtención del óptimo rendimiento de las explotaciones pecuarias existentes.

En la mayor parte de los países latinos existen dos palabras que en los anglosajones se confunden. En castellano se distingue perfectamente entre zootecnia y ganadería, arte u objeto práctico de esta ciencia. Lo mismo sucede en francés entre "zootechnie" y "elevage", y en italiano entre "zootecnica" y "allevamento". En inglés, por el contrario, el vocablo "zootechnics" constituye un neologismo técnico poco empleado, y está más generalizada la voz "animal breeding" para designar tanto a la ciencia como al arte aplicado a la cría animal.

Los zootecnistas son personas con capacidad de observar y analizar holísticamente todos los fenómenos involucrados con la producción animal,mejoramiento genético, pastos y forrajes, reproducción animal, sanidad preventiva, nutrición animal y economía animal, con vocación y gusto por el campo y las actividades que en él se desarrollan.



</doc>
<doc id="3019" url="https://es.wikipedia.org/wiki?curid=3019" title="Épica">
Épica

La épica (del adjetivo: "ἐπικός", "epikós"; de "ἔπος", "épos", "palabra, historia, poema") es un género narrativo en el que se presentan hechos legendarios o ficticios relativos a las hazañas de uno o más héroes y a las luchas reales o imaginarias en las que han participado.

Su forma de expresión más tradicional fue la narración en verso, bajo la forma de poemas épicos cuya finalidad última era exaltación o engrandecimiento de un pueblo. En algunos casos, la épica no tenía forma escrita, sino que era contada oralmente por los rapsodas. Con posterioridad la épica adoptó también la forma narrativa en prosa, incorporando elementos de descripción y diálogo y dando lugar, en primera instancia, a la novela de caballerías y posteriormente al género conocido como fantasía épica.

Alternancia de discursos que tiene como origen la observación aristotélica de la diferencia entre mímesis y diégesis, es decir, entre narración y descripción.


El género épico se encuentra en todas las literaturas, pues es un género esencial, y se puede dar y se ha dado históricamente en formas muy diferentes.
Los sumerios ("Epopeya de Gilgamesh"), griegos ("Ilíada", "Odisea"), romanos ("Eneida"), hinduistas ("Majabhárata") y persas ("Shahnameh") compusieron epopeyas en torno a las hazañas de un héroe arquetípico, que representaba los valores tradicionales colectivos de una nación, y otros personajes como dioses y hombres, incluyendo además elementos fantásticos. 

En la Edad Media la epopeya se denominó cantar de gesta, y en ella empezaron a escasear los elementos divinos y fantásticos. En Francia se compusieron la mayoría de ellas, y la más influyente fue la "Chanson de Roland" o "Cantar de Roldán". En España se compuso el "Cantar de Mío Cid", entre otros. Los alemanes compusieron el "Cantar de los Nibelungos", y los sajones el "Beowulf". En Inglaterra, no llegaron a reunirse leyendas dispersas en torno a Robin Hood, pero se escribieron en prosa historias sobre un hipotético rey llamado Artus o Arturo. En Islandia, las sagas, aunque tienen un marcado carácter histórico, se emparentan con esta tradición narrativa, sobre todo en las sagas arcaicas como la "Volsunga Saga". 

Con el paso a los tiempos modernos, la epopeya empezó a estar protagonizada no por héroes y dioses, sino únicamente por personas vulgares y corrientes, cuya única hazaña era la supervivencia o conseguir una mejor condición social; de igual manera, las hazañas fantásticas fueron sustituidas por una tendencia realista. Ésa fue la gran contribución de novelas como la anónima novela picaresca española "El lazarillo de Tormes" y, sobre todo, las dos partes del "El ingenioso hidalgo don Quijote de La Mancha" de Cervantes, que desacreditaron por completo los restos de epopeya que venían de la Edad Media, encarnados por los llamados libros de caballerías. El "Quijote" supone, pues, el nacimiento de la novela moderna realista y polifónica, escrita en prosa, y cuyos protagonistas son personas vulgares y corrientes que se mueven en ambiente realista, sin hechos sobrenaturales y sin que intervengan los dioses. Este tipo de novela se desarrolló extraordinariamente en el siglo XIX, cuando la burguesía lo tomó como modelo para exponer sus 
inquietudes y como espejo de su nueva ideología materialista. La novela realista del siglo XIX es la epopeya de la clase media o burguesía.

El poema épico intenta reactualizar en los tiempos modernos la epopeya griega y romana, sus antecedentes, en un estilo generalmente lleno de reminiscencias y en rima consonante. A este género pertenecen, por ejemplo:

El cuento tradicional es una narración anónima de carácter oral que sirve para pasar el tiempo y se suele contar a los niños. En el siglo XVIII y XIX empezaron a recogerse y estudiarse. Colecciones de cuentos populares son las de los hermanos Jacob y Wilhelm Grimm en Alemania, o Charles Perrault en Francia.

La leyenda, escrita en verso o en prosa, es característica del siglo XIX y narra hechos con alguna base histórica de verdad, pero fabulándose mucho en ellos libremente. Cabe destacar, por ejemplo, las Leyendas de Gustavo Adolfo Bécquer.

El mito es una narración corta que tiene una función cognoscitiva o explicativa, etiológica, frecuentemente de carácter alegórico. Es por esto que su estudio está más relacionado con la mitología. En ella se habla a cerca de personajes divinos o extraordinarios, que forman parte de las creencias de una cultura.

El relato es una narración escrita de autor conocido, con pocos personajes y sin la complicación y meandros de que hace gala la novela clásica.

El romance o, en los países nórdicos, balada, es una narración corta en verso, casi siempre de carácter anónimo, surgida en general de la descomposición de los cantares de gesta medievales, aunque pronto fueron compuestos algunos romances y baladas por nuevos autores imitando los romances viejos (La Balada del Caballo Blanco, de G. K. Chesterton).

La fantasía heroica o fantasía épica, es un subgénero del género fantástico, principalmente de la literatura, aunque también presente en la historieta, el cine fantástico y los juegos de rol, caracterizado por la presencia de seres mitológicos o fantásticos, la ambientación ficticia de carácter medieval, antiguo, indefinido o, en cualquier caso, sobre la base de sociedades tecnológicamente atrasadas, y un fuerte componente mágico y épico.


</doc>
<doc id="3020" url="https://es.wikipedia.org/wiki?curid=3020" title="Égloga">
Égloga

La égloga es un subgénero de la poesía lírica que se dialoga a veces como una pequeña pieza teatral en un acto. De tema amoroso, uno o varios pastores lo desarrollan contándolo en un ambiente campesino donde la naturaleza es paradisíaca y tiene un gran protagonismo la música. Como subgénero lírico se desarrolla a veces mediante un monólogo pastoril o, más frecuentemente, con un diálogo. 

La égloga es una composición en la que el poeta, encarnado en uno o varios pastores, expresa su amor en un marco idealizado, lleno de belleza y amor.

Las primeras églogas fueron los "Idilios" (en griego, "poemitas" o "pequeños cantos") de Teócrito; luego los escribieron Mosco, Bión de Esmirna y otros autores bajo su influencia. El escritor latino Virgilio (siglo I a. C.) con sus "Églogas" (en griego, "selecciones") o "Bucólicas" añadió elementos autobiográficos, haciendo de cada pastor un personaje imaginario que encubría a un personaje real: Cayo Cilnio Mecenas, Augusto, etc. Algunas de ellas llegaron a escenificarse en Roma. Otros autores latinos escribieron también églogas, como Nemesiano, Calpurnio Sículo o Ausonio.

Esta innovación pasó a la bucólica posterior, de forma que algunas veces los personajes de las églogas representaban personajes reales. A través de Giovanni Boccaccio y con el Renacimiento y la "Arcadia" de Jacopo Sannazaro el género se volvió a recuperar mezclándose las composiciones en verso en un marco narrativo en prosa, y se difundió por todo el mundo occidental, bien en verso, bien como églogas intercaladas en una novela pastoril cualquiera. En la literatura castellana, escribieron églogas Juan del Encina, Lucas Fernández, Garcilaso de la Vega, Juan Boscán, Lope de Vega, Pedro Soto de Rojas, Bernardo de Balbuena y Juan Meléndez Valdés.



</doc>
<doc id="3022" url="https://es.wikipedia.org/wiki?curid=3022" title="África">
África

África es el tercer continente por su extensión, tras Asia y América. Está situado entre los océanos Atlántico, al oeste, e Índico, al este. El mar Mediterráneo lo separa al norte del continente europeo; el punto en el que los dos continentes se hallan más cercanos es el estrecho de Gibraltar de 14,4 km de anchura. El mar Rojo lo separa al este de la península arábiga y queda unido a Asia a través del istmo de Suez, en territorio egipcio. Posee una superficie total de 30 272 922 km² (621 600 km² en masa insular), que representa el 20,4 % del total de las tierras emergidas del planeta. La población es de mil millones de habitantes, menos del 15 % del total mundial. El continente se divide en 54 países organizados en la Unión Africana, además de 2 territorios no reconocidos y 2 territorios dependientes.

Se cree que África es la cuna de la humanidad y que de allí proceden las sucesivas especies de homínidos y antropoides que dieron lugar a los seres humanos. La teoría explica que allí se originó el "Homo sapiens" hace cerca de 190 000 años para luego expandirse por el resto de los continentes.

Según el historiador griego Heródoto (484 a. C.), una expedición fenicia auspiciada por el faraón Necao II (616 a. C.) circunnavegó el continente africano por primera vez.

Los orígenes del tráfico comercial entre el oeste y el centro de África y la cuenca mediterránea se pierden en la prehistoria. Los primeros relatos históricos datan de la antigüedad y versan sobre los nómadas que organizaban el comercio entre Leptis Magna y el Chad. Este comercio vivió su primer auge en el siglo I a. C. con el ascenso del Imperio romano. Sobre todo se comerciaba con oro, esclavos, marfil y animales exóticos para los juegos de circo en Roma a cambio de bienes de lujo romanos. De hecho es en esta época en la que se gesta el propio nombre de África. Tras la derrota de Cartago por Roma en la tercera guerra púnica se establece la provincia romana de África que abarcaría aproximadamente el Túnez actual. Fue una generalización territorial de la provincia lo que dio nombre a todo el continente. Una importancia crucial tuvo también la mayor utilización del camello a partir del siglo I en el norte de África.

A partir del siglo VII los árabes invaden el África del norte. El comercio caravanero y la expansión islámica alimentan el establecimiento de nuevas relaciones entre las «dos Áfricas».

El Imperio Kanem-Bornu existió en África entre el siglo XIII y la década de 1840. En su momento de mayor esplendor abarcó el área de lo que actualmente es el sur de Libia, Chad, noreste de Nigeria, este de Níger y norte de Camerún.

El Reino del Congo fue un estado situado en lo que actualmente constituye la zona norte de Angola, el enclave de Cabinda, Congo-Brazzaville y la parte occidental de Congo-Kinsasa. Su área de influencia abarcaba también los estados vecinos.

La total repartición colonial de África por las potencias europeas, iniciada a partir del siglo XVII, tuvo lugar aproximadamente en 1885, con la conferencia de Berlín y el comienzo de la Primera Guerra Mundial, época en la que los imperios coloniales se extendieron más rápidamente en África que en cualquier otro lugar del mundo, si bien dos países, Liberia y Etiopía, consiguieron mantener su independencia. Es un ejemplo del Nuevo Imperialismo generado por la necesidad de los países europeos de obtener materias primas para el rápido crecimiento de su producción manufacturera después de la Revolución Industrial, iniciada en Inglaterra a fines del siglo XVIII.

Al final de la Segunda Guerra Mundial los aliados no logran ponerse de acuerdo sobre el futuro de la antigua colonia italiana de Libia. En ese momento era un territorio más de cinco veces mayor que la propia Italia. Sin embargo, la población no sobrepasaba el millón de habitantes, por lo que representaba un destino apropiado para la población desplazada de Italia por la guerra, que empezó a buscar lugares a donde emigrar. Los recelos entre Occidente y la Unión de Repúblicas Socialistas Soviéticas (URSS) hacen que finalmente la Organización de las Naciones Unidas (ONU) decida dar la independencia al país dejándolo en manos del rey Idris.

Aunque ya había cuatro países independientes en África (Liberia en 1847, Sudáfrica en 1910, Egipto en 1922 y Etiopía en 1941) Libia se convierte así en la primera colonia africana en lograr su independencia en 1951, a la que seguirá la de Ghana en 1957. Más adelante las potencias europeas lamentarían este hecho, pues contribuyó a desencadenar las diferentes luchas por la independencia africana.

En su mayor parte, África es una enorme y antigua plataforma continental maciza y compacta, elevada entre 600 y 800 msnm, surcada por grandes ríos (aunque pocos) y escasa en penínsulas. Destaca por su regularidad orográfica y considerable altitud media.

Tres franjas climáticas sucesivas se repiten al norte y al sur del ecuador, abarcando los climas mediterráneo, desértico, subtropical e intertropical lluvioso, este último, en sus dos tipos principales, tanto de sabana como de selva. África es el continente con mayor índice de insolación anual, lo cual podría haber dado origen a su nombre (África, del griego "a-phrike", ‘sin frío’).

Los suelos son excepcionalmente ricos en minerales y muy aptos para pastos. Debido al clima es allí donde evolucionó la mosca tsetsé y donde prolifera actualmente. Las principales áreas cultivadas se encuentran en las tierras altas orientales y la zona de los Grandes Lagos, algunos deltas y riberas e incluso en el Sahel.- Situación Astronómica Continental: Norte: Cabo Blanco, Túnez (37º20' Norte) Sur: Cabo de las agujas, Rep. Sudafricana (35º Sur) Este: Cabo Hafún, Somalia (51º24' Este) Oeste: Cabo Verde, Senegal (18º Oeste)



El continente africano está compuesto de 54 estados soberanos, 3 territorios dependientes y varios territorios integrados en estados no africanos como Francia, España o Portugal. Las entidades políticas africanas anteriores al colonialismo desaparecieron con la expansión europea por el continente a finales del siglo XIX, sólo Abisinia, que se mantuvo independiente gracias a su victoria sobre los italianos en 1896 en la batalla de Adua, y Liberia, que fundada por el gobierno estadounidense con esclavos liberados de su país en 1847, se mantuvieron independientes.
La mayoría de los países africanos lograron la independencia en el siglo XX a partir del proceso de descolonización tras la Segunda Guerra Mundial y que alcanzó su plenitud en los años 60. En 2011 surgió, hasta el momento, el último estado en el continente, Sudán del Sur tras conseguir la independencia de Sudán tras una larga guerra civil (1955-1972 y 1983-2005).

Todos los estados africanos soberanos son miembros de pleno derecho de la ONU contando entre ellos con cuatro estados fundadores como fueron Egipto, Sudáfrica, Liberia y Etiopía.

En materia económica 52 estados son miembros de la Organización Mundial del Comercio (8 son observadores) mientras que Sudán del Sur y Eritrea no pertenecen a ella. Así mismo, la totalidad del continente se incluye en el Fondo Monetario Internacional aunque 8 estados no cumple el artículo VIII de la organización. 

Es importante destacar la presencia del continente en la OPEP, ya que Argelia, Angola, Gabón, Libia y Nigeria son productores de petróleo. 

En materia de justicia y seguridad todos los países africanos están integrados en la INTERPOL, sin embargo en el caso de la Corte Penal Internacional nueve países no han firmado ni ratificado el Estatuto de Roma. Mientras son diez los firmantes que aún no lo han ratificado. En el resto de países acepta la jurisdicción del Corte Penal Internacional para juzgar casos de crímenes contra la humanidad. 

También esta presente la Liga Árabe que engloba a los países musulmanes del continente: Marruecos, Argelia, Túnez, Libia, Egipto, Sudán, Mauritania, Somalia y Yibuti. 

Los estados africanos (salvo Sudán del Sur) están adscritos al Movimiento de Países No Alineados. 

En cuanto a las organizaciones transcontinentales el continente africano esta presente en Asociación ribereña del Océano Índico para la cooperación regional (1995) de cooperación entre países asiáticos, Australia y 9 estados africanos (Somalia, Tanzania, Madagascar, Seychelles, Mauricio, Mozambique, Kenia, Sudáfrica y Comoras). Durante la Guerra Fría (1986) se creó la Zona de Paz y Cooperación del Atlántico Sur, bajo el auspicio de Naciones Unidas, con el objetivo de mantener la seguridad y la paz en el Atlántico Sur, así por iniciativa de Brasil se unieron 21 estados africanos más 3 sudamericanos. 

En 1975 se creó Estados de África, del Caribe y del Pacífico (ACP) para, a través de varios acuerdos (el más reciente Acuerdo de Cotonú del año 2000) luchar contra la pobreza junto a la Unión Europea que trabaja por medio del Fondo Europeo de Desarrollo. Forman parte de esta organización todos 47 estados africanos. La Unión Europea trabaja a través de la firma de acuerdos económicos con los cinco bloques regionales. 

La principal organización política regional del continente es la Unión Africana (U.A), heredera de varios intentos previos de unir políticamente al continente, a semejanza de la Unión Europea en Europa. Sus predecesoras son la Unión de Estados Africanos, creada por el ghanés Kwame Nkrumah en 1958, la Organización para la Unidad Africana de 1963. Desde 1984 hasta el año 2017 Marruecos no formó parte de la U.A como protesta por la admisión de la República Árabe Saharaui Democrática con la que mantiene un contencioso por el Sáhara Occidental.

La principal organización económica es la Comunidad Económica Africana (CEA) fundada en 1981. El objetivo de la C.E.A es fomentar la integración y el desarrollo a través de la cooperación entre los estados africanos. Para ello utiliza un sistema de agrupaciones regionales como pilares básicos:
Fuera del paraguas de la CEA existen otras organizaciones de tipo económico como la Comisión del Océano Índico, Autoridad de Liptako-Gourma, Unión del Río Mano o la Comunidad Económica de los Países de los Grandes Lagos. 

Datos de superficie y población consultados en actualizados 1 junio de 2016.

Dos territorios africanos aún tienen un reconocimiento limitado debido a conflictos territoriales. Así, la República Árabe Saharaui Democrática surgió como resultado del proceso de descolonización del Sáhara Español en 1976, siendo reconocido por 48 estados soberanos. El resto del territorio está ocupado por Marruecos que no reconoce la independencia y lo reclama como territorio propio.
Somalilandia se autoproclamó estado durante la crisis política somalí que desembocó en una guerra civil todavía latente. Somalilandia no es reconocida por ningún estado.
Datos de superficie y población consultados en actualizados 1 junio de 2016.

La dependencia francesa está compuesta por una serie de islas conocidas como Islas Dispersas del Océano Índico de las que la mayoría están deshabitadas y son reclamadas por países soberanos como Mauricio, Madagascar o Seychelles. Por su parte el territorio de ultramar de Santa Helena está incluido en el Comité de Descolonización de la ONU. Ambos territorios forman parte de los países y territorios de ultramar (o PTU) son las dependencias y territorios de ultramar de los Estados miembros de la Unión Europea que no forman parte de la Unión, sino que tiene un estatuto de asociados a los Estados miembros desde el Tratado de Lisboa.
Datos de superficie y población consultados en actualizados 1 junio de 2016.

Está formada por territorios integrados como parte de otros estados. Los territorios de las Islas Canarias, Reunión, Mayotte y Madeira forman parte de la Región Ultraperiférica de la Unión Europea, que, aún estando geográficamente alejados del continente europeo, forman parte indivisible de alguno de los veintiocho Estados miembros de la Unión.

Datos de superficie y población consultados en actualizados 1 junio de 2016.


En su condición de excolonias, la mayoría de los países africanos mantienen estrechas relaciones económicas con la Unión Europea (UE).

Existe una organización supranacional, tomando como referencia a la Unión Europea, llamada Unión Africana(UA), de la que forman parte todos los países del continente, incluida la República Árabe Saharaui Democrática. La mayor parte de los países africanos están subdesarrollados o en vías de desarrollo.

Más del 50 % de la población o 350 millones de personas viven con menos de un dólar al día. África paga cerca de 20 000 millones de dólares en pagos de deuda cada año, aun pese a las condonaciones de deuda de los años 90.

Durante el régimen colonial los europeos explotaron los productos más fáciles y más provechosos de extraer, como el oro, el marfil, maderas y fibras textiles. Tras la emancipación de las colonias los más codiciados pasaron a ser el petróleo, los diamantes y la minería en general, pero estos productos mencionados se hallan en pocos países.

La carencia de buena tecnología y de medios de comunicación eficientes dificultan la explotación de dichas materias primas. El 60 % de los trabajadores africanos realiza actividades rurales, y el 80 % de lo que África exporta son materias primas, siendo a su vez los productos industrializados los que representan la casi totalidad de sus importaciones. Solo el 15 % está empleado en el sector industrial, siendo Egipto, Sudáfrica, Túnez y Marruecos los que poseen casi el total de dicha actividad. El resultado es que África es el continente más pobre del planeta: su PBI representa tan solo el 2,6 % del total mundial.

La ayuda exterior llega a los cincuenta millones de dólares cada año, y en los últimos 60 años esa ayuda ha sido de al menos mil millones. Sin embargo, esto ha empobrecido más a los países, ha ralentizado el crecimiento, los ha endeudado más, los ha hecho más propensos a la inflación y vulnerables a los vaivenes de las divisas, ha reducido el atractivo para la inversión y ha aumentado el riesgo de conflictos civiles. La ayuda exterior se transforma en deuda, que se paga a expensas de la educación y los servicios médicos africanos. Aún cuando se termina de pagar una deuda, los países vuelven a pedir más ayuda. A fin de paliar este círculo vicioso, la tendencia actual consiste en condonar la deuda externa a los países que demuestran un compromiso con el sistema democrático y con el desarrollo.

La asistencia ha estado afectada por corrupción, y los flujos han acabado beneficiando a las burocracias gubernamentales y ciertas ONG financiadas por algunos gobiernos. La corrupción le cuesta a África 150 millones de dólares al año. No existen incentivos para que los gobiernos busquen formas más transparentes para recaudar fondos para el desarrollo, solo se les pide a las agencias de donación una infusión de capital.

En contraposición, en otros países la ayuda ha servido para resolver problemas como las epidemias que diezman la salud y las vidas de la población activa (sida, malaria), la falta de infraestructuras básicas, el rendimiento agrícola, el analfabetismo y la carencia de educación primaria universal. Existen ejemplos de países, como Ghana, que demuestran emplear correctamente la ayuda.

El flujo de capital ayuda a que los gobiernos ineficientes sigan en el poder, ya que el presidente no tiene que hacer nada pues la ayuda sigue llegando, siempre y cuando pague al ejército. No tiene que subir los impuestos, ni preocuparse del descontento de los ciudadanos ni de la representación de estos. Los choques civiles a menudo son motivados por el conocimiento de que al hacerse con el poder, el ganador obtiene un acceso virtualmente completo al paquete de ayuda.

La ayuda hace que la burocracia se vuelva clientelista y envuelva a los ciudadanos con trámites innecesarios. En Camerún se tardan 426 días en hacer un procedimiento comercial y 119 días en Angola.

La ayuda alimentaria que compra comida cultivada en Estados Unidos quiebra a los agricultores locales. Se ha hecho poco para ayudar a los agricultores y se gastan millones de dólares en el programa.

La gran cantidad de dinero crea la "enfermedad holandesa": los grandes flujos de dinero hacen que la moneda local se fortalezca incrementando además los precios internos. Esto crea además inflación, por lo que los países deben emitir bonos. Uganda fue obligada a emitirlos en 2005, pagando intereses de $110 millones anuales.

China está presente en países con grandes recursos, como petróleo, en Angola, que es su principal proveedor, y en otros países como son Guinea Ecuatorial, Nigeria, Chad, Sudán, Gabón, Zambia y República Democrática del Congo, estos dos últimos países productores de minerales.

Después de Estados Unidos y de la Unión Europea, China es el tercer socio más importante del continente, con inversiones en industrias de la construcción que están haciendo carreteras, embalses, viviendas, hospitales, y en la explotación de hidrocarburos y minerales. China tiene estrecha relación con Zimbabue, y Sudán, cuyos gobiernos son cuestionados.

Estados Unidos tiene interés en África por el petróleo.

Según el Programa de las Naciones Unidas para el Desarrollo (PNUD), África cuenta con ciudades subdesarrolladas, la mayoría de ciudades de África que poseen mayor desarrollo están en Sudáfrica y Egipto.
Hay aproximadamente 41 monedas oficiales distintas.La moneda oficial más extendida es el franco CFA en sus dos versiones: el Franco CFA de África Occidental moneda nacional en 8 países y el Franco CFA de África Central en 6. También esta presente el euro a través de los territorios españoles y franceses en el continente, así como la libra de Santa Elena, par a la libra esterlina, pero con sus propios modelos de monedas y billetes con respecto a la moneda británica.

Las estimaciones sobre la población no son precisas debido a lo obsoleto de gran número de censos nacionales. Se calcula sin embargo que viven en África no menos de 1000 millones de personas.

En África predomina la raza negra, cerca de un 80 % del total de la población, a excepción de la franja costera mediterránea donde son mayoritarios, aunque no exclusivos, tipos humanos arabo-bereberes y caucasoides-mediterráneos. Entre el trópico de Capricornio y el trópico de Cáncer la población es casi en su totalidad negra, y suele ser sub-dividida en cuatro grupos principales, aunque siempre han existido en las zonas limítrofes entre estos grandes grupos pueblos más o menos mixtos en todas sus combinaciones. Tales grupos principales son sudanés, (Sahel y países del golfo de Guinea), nilótico, (Nilo, desde Sudán hasta los Grandes Lagos), cusita (Macizo etíope y Cuerno de África) y bantú, siendo éste el más extendido, ya que ocupa toda el área a partir del cinturón selvático ecuatorial. Es además un tipo mixto relacionado con dos tipos antaño muy extendidos (hoy en día minoritarios), los twa y otros grupos mal denominados pigmeos, habitantes de los bosques, y los kung-san, mal denominados bosquimanos, de las zonas áridas del extremo sur.

Migrantes de origen francés se hallan establecidos en el Magreb y escasamente en las grandes ciudades de África Occidental, los de origen español habitan Marruecos y el Sáhara Occidental, mientras que en Angola y algunas ciudades costeras de África Occidental hay un número minoritario de grupos mixtos de origen africano-portugués. En el sur de África hay una significante cantidad (6 millones) de africanos blancos o afrikaaners, descendientes de holandeses e ingleses.

La mayoría de los africanos mantienen un estilo de vida rural, pero la urbanización aumenta ya que la gente abandona el campo para buscar trabajo en las ciudades. Las mayores densidades de población se encuentran donde el agua es más accesible, como en el valle del Nilo, las costas del norte y oeste, a lo largo del Níger, en las regiones montañosas del este y en Sudáfrica.

Aumento de la población desde el año 0 hasta el 2010 y estimaciones de población para los años 2050 y 2100.

En África las características de la población y su esperanza de vida varían según las condiciones. En África del Norte o en el desierto del Sahara, la mayor parte de sus habitantes son adultos y superan a la población juvenil, aunque no se da tampoco un envejecimiento progresivo. En el África subsahariana la mayor parte de sus habitantes son jóvenes, aunque en las últimas décadas se ha experimentado un crecimiento en la población adulta y un progresivo envejecimiento. Esto se da principalmente en países como Etiopía y Somalia, aunque en Sudáfrica también se experimenta un crecimiento de población adulta pero no es tan común el envejecimiento.

La población por sexo varia en el continente, al Sur del Sahara conocido también como el África negra predominan las personas de sexo femenino, excepto en países como Angola, Mozambique, Etiopía, Somalia y Yibuti, entre otros. En cambio, en la mayor parte de los países del África del Norte predominan las personas de sexo masculino, excepto Marruecos, Sáhara Occidental, Mauritania y Chad.

Además de lenguas alóctonas como el árabe, el francés o el inglés (entre otras) cuya presencia en África se debe a procesos de conquista y dominación política. Se estima que en África actualmente existen unas 1700 lenguas autóctonas.

Demográficamente el árabe y el francés son las lenguas con más hablantes potenciales y las más extendidas en el continente. Las lenguas autóctonas de África con el mayor número de hablantes son el suahili (90 millones de hablantes), el oromo (70 millones), el hausa (40 millones) y el amhárico, todas ellas con un buen número de hablantes para los cuales es su segunda lengua y no su lengua materna (estas cuatro lenguas se usan ampliamente como "lingua franca" en sus respectivas áreas de influencia). Las lenguas europeas más extendidas son el francés, el inglés y el portugués, generalmente utilizados por las administraciones postcoloniales y las clases urbanas. A continuación existe un grupo de cerca de 20 idiomas étnicos con entre 1 y 20 millones de hablantes como: (de norte a sur) el wólof, manding (mandé), ewe, fon, yoruba, igbo, lingala, shona, setsuana, xhosa, malgache, etc. Otros idiomas minoritarios son el afrikáans y el español, de origen europeo, y otros autóctonos como el bereber. Los idiomas africanos y oficiales en sus respectivos estados son: el amárico hablado en Etiopía, el somalí en Somalia, el suajili en Kenia y Tanzania, el setsuana en Botsuana, el afrikáans en Sudáfrica y Namibia (junto con el inglés), y el malgache en la República de Madagascar (junto con el francés).

Las lenguas africanas autóctonas pertenecen a cuatro grandes grupos:

La mayor parte del continente profesa religiones tradicionales africanas, englobadas dentro del impreciso grupo conocido como animista. Esto significa que creen que los espíritus habitan objetos animados o inanimados. Dicho así mismo suele persistir bajo la apariencia de religiones universalistas como el islam o el cristianismo. También hay creyentes del rastafarismo.

El islam tiene una presencia dominante en el norte y destacada en el Sáhara, el Sahel, África Occidental y África Oriental. El cristianismo monofisita, aunque más antiguo que el islam, quedó confinado a Etiopía. A partir del siglo XX adquirirán una creciente importancia el catolicismo y protestantismo.

Sin embargo tanto islam como el cristianismo se encuentran en África con sincretismos más o menos sectarizados como el kimbanguismo o la Iglesia "Cita con la Vida", que persisten y se reproducen gracias a la fortaleza implícita de los conceptos de las religiones tradicionales. Las religiones tradicionales africanas tienen una presencia destacada en América, especialmente el vudú en Haití, la religión yoruba y las religiones del antiguo Reino del Congo en el Caribe y en Brasil principalmente.

Existen asimismo minorías hinduistas.

Los antiguos la personificaban bajo la figura de una mujer y con la de un escorpión. En una medalla del emperador Adriano lleva por casco o morrión la cabeza de un elefante. En otras varias medallas se observa que tiene en la mano derecha un escorpión y en la izquierda el cuerno de la abundancia; a sus pies un cesto lleno de flores y de frutos. El caballo y la palma son los símbolos de la parte de África vecina a Cartago.

En una medalla de la reina Cristina se ve una alegoría menos conocida: Atlas cubierto con la piel de la cabeza de un elefante guarnecida con su trompa y sus colmillos, contemplando los signos del zodíaco, para indicar que este rey, considerado por algunos como el inventor de la astronomía, reinó en África. Los modernos aprovechando de todas estas ideas han representado el África bajo los rasgos de una mujer mora, casi desnuda teniendo los cabellos rizados, llevando por casco una cabeza de elefante, un collar de coral, un cuerno lleno de espigas en una mano, un escorpión en la otra o un colmillo de elefante y acompañada de un león y de varias serpientes. Lebrun la ha pintado bajo la figura de una mora desnuda hasta la cintura, sentada sobre un elefante y en la cabeza un parasol que la pone enteramente a la sombra. Sus cabellos son negros, cortos y rizados: lleva por pendientes dos grandes perlas y sus brazos adornados con ricos brazaletes.

El teatro africano, entre tradición e historia, se está encauzando actualmente por nuevas vías. Todo predispone en África al teatro. El sentido del ritmo y de la mímica, la afición por la palabra y la verborrea son cualidades que todos los africanos comparten en mayor o menor medida y que hacen de ellos actores natos. La vida cotidiana de los africanos transcurre al ritmo de variadas ceremonias, rituales o religiosas, concebidas y vividas generalmente como verdaderos espectáculos. No obstante, aunque África ha conocido desde siempre este tipo de ceremonias, cabe preguntarse si se trataba realmente de teatro; a los ojos de muchos, estos espectáculos están demasiado cargados de significado religioso para que puedan considerarse como tal. Otros estiman que los tipos de teatro africanos guardan cierto parecido, como en otros tiempos la tragedia griega, como un preteatro que nunca llegara totalmente a ser teatro si no se desacraliza. La fuerza y las posibilidades de supervivencia del teatro negro residirán, por lo tanto, en su capacidad para conservar su especificidad. en el África independiente está tomando forma un nuevo teatro.

Nuevo Teatro: Se trata de un teatro comprometido, incluso militante, concebido para defender la identidad de un pueblo que ha logrado su independencia

Teatro de Vanguardia: Se orienta actualmente hacia una investigación sobre el papel de actor, próxima a la de Jerzy Grotowski y su teatro laboratorio. Así, en Libreville (Gabón), se formó en 1970 un teatro vanguardista que realizó dos espectáculos que dejaron una huella perdurable en las jóvenes generaciones de comediantes. Otra vía de investigación es el teatro de silencio, creado por François Rosira, cuyo fin era realizar espectáculos en los que el canto, el recitado, la música y el baile se complementen en perfecta armonía.

Asociaciones como Ndjembé promovían el carácter teatral en África.



</doc>
<doc id="3026" url="https://es.wikipedia.org/wiki?curid=3026" title="2000">
2000

2000 (MM) fue un en el calendario gregoriano. Científicamente el siglo XX y el II milenio terminaron el 31 de diciembre de este año.
El año 2000 fue declarado:
La cultura popular sostiene el año 2000 como el primer año del siglo XXI y el tercer milenio debido a la tendencia de agrupar los años de acuerdo con valores decimales, como si se hubiera contado el año cero. Según el calendario gregoriano, estas distinciones caen en el año 2001, porque se dijo retroactivamente que el primer siglo comenzaba con el año 1. Como el calendario no tiene el año cero, su primer milenio abarcó desde el año 1 hasta el 1 000 inclusive y su segundo milenio milenio desde los años 1001 a 2000. Sin embargo, el siglo y el milenio que comenzaron en 2000 se conocen como los años 2000.

El año 2000 a veces se abrevia como "Y2K" (la "Y" significa "año", y la "K" significa "kilo", que significa "mil"). El año 2000 fue el tema del Problema del año 2000, que temían que las computadoras no cambien de 1999 a 2000 correctamente. Sin embargo, para fines de 1999, muchas empresas ya se habían convertido a software existente nuevo o mejorado. Algunos incluso obtuvieron la certificación Y2K. Como resultado de un esfuerzo masivo, ocurrieron relativamente pocos problemas.





























Todas las fechas pertenecen a los estrenos oficiales de sus países de origen, salvo que se indique lo contrario.







</doc>
<doc id="3028" url="https://es.wikipedia.org/wiki?curid=3028" title="1978">
1978

1978 (MCMLXXVIII) fue un año normal que comenzó en domingo en el calendario gregoriano.

Según el horóscopo chino, 1978 fue Año de Caballo.


21 de febrero: ciudad de México. Se hallan ruinas pertenecientes a Tenochtitlan en el zócalo capitalino tras excavación de la compañía de luz y fuerza. Por igual fue descubierto un monolito de la diosa lunar Coyolxuauhqui.


































10 de julio: Martina Navratilova llega a ser la número 1 del mundo.














</doc>
<doc id="3029" url="https://es.wikipedia.org/wiki?curid=3029" title="2002">
2002

2002 (MMII) fue un año común comenzado en martes según el calendario gregoriano.





26 de mayo: en Colombia, Álvaro Uribe es elegido presidente.






























Todas las fechas pertenecen a los estrenos oficiales de sus países de origen, salvo que se indique lo contrario.






</doc>
<doc id="3030" url="https://es.wikipedia.org/wiki?curid=3030" title="1 de enero">
1 de enero

El 1 de enero es el primer día del año en el calendario gregoriano. Quedan 364 días para finalizar el año y 365 en los años bisiestos. En el calendario juliano empezó a ser el primer día del año en el 153 a. C.
























</doc>
<doc id="3031" url="https://es.wikipedia.org/wiki?curid=3031" title="2001">
2001

2001 (MMI) fue un año común comenzado en lunes según el calendario gregoriano. Fue declarado:












Honduras consigue ser el número 20 en el ranking de la FIFA























Todas las fechas pertenecen a los estrenos oficiales de sus países de origen, salvo que se indique lo contrario.






</doc>
<doc id="3034" url="https://es.wikipedia.org/wiki?curid=3034" title="8 de enero">
8 de enero

El 8 de enero es el octavo día del año en el calendario gregoriano. Quedan 357 días para finalizar el año y 358 en los años bisiestos.


















</doc>
<doc id="3035" url="https://es.wikipedia.org/wiki?curid=3035" title="6 de enero">
6 de enero

El 6 de enero es el sexto día del año en el calendario gregoriano. Quedan 359 días para finalizar el año y 360 en los años bisiestos.










</doc>
<doc id="3036" url="https://es.wikipedia.org/wiki?curid=3036" title="9 de enero">
9 de enero

El 9 de enero es el noveno día del año en el calendario gregoriano. Quedan 356 días para finalizar el año y 357 en los años bisiestos.






















</doc>
<doc id="3037" url="https://es.wikipedia.org/wiki?curid=3037" title="10 de enero">
10 de enero

El 10 de enero es el 10.º (décimo) día del año en el calendario gregoriano. Quedan 355 días para finalizar el año y 356 en los años bisiestos.








</doc>
<doc id="3038" url="https://es.wikipedia.org/wiki?curid=3038" title="11 de enero">
11 de enero

El 11 de enero es el undécimo día del año en el calendario gregoriano. Quedan 354 días para finalizar el año y 355 en los años bisiestos.







</doc>
<doc id="3039" url="https://es.wikipedia.org/wiki?curid=3039" title="15 de enero">
15 de enero

El 15 de enero es el 15.º (decimoquinto) día del año en el calendario gregoriano. Quedan 350 días para finalizar el año y 351 en los años bisiestos.



















</doc>
<doc id="3040" url="https://es.wikipedia.org/wiki?curid=3040" title="14 de enero">
14 de enero

El 14 de enero es el 14.º (decimocuarto) día del año del calendario gregoriano.
Quedan 351 días para finalizar el año y 352 en los años bisiestos.







</doc>
<doc id="3041" url="https://es.wikipedia.org/wiki?curid=3041" title="16 de enero">
16 de enero

El 16 de enero es el 16.º (decimosexto) día del año en el calendario gregoriano. Quedan 349 días para finalizar el año y 350 en los años bisiestos.








</doc>
<doc id="3042" url="https://es.wikipedia.org/wiki?curid=3042" title="17 de enero">
17 de enero

El 17 de enero es el 17.º (decimoséptimo) día del año en el calendario gregoriano. Quedan 348 días para finalizar el año y 349 en los años bisiestos.









</doc>
<doc id="3044" url="https://es.wikipedia.org/wiki?curid=3044" title="18 de enero">
18 de enero

El 18 de enero es el decimoctavo día del año en el calendario gregoriano. Quedan 347 días para finalizar el año y 348 en los años bisiestos.








</doc>
<doc id="3045" url="https://es.wikipedia.org/wiki?curid=3045" title="21 de enero">
21 de enero

El 21 de enero es el 21.º (vigesimoprimer) día del año en el calendario gregoriano. Quedan 344 días para finalizar el año y 345 en los años bisiestos.










</doc>
<doc id="3046" url="https://es.wikipedia.org/wiki?curid=3046" title="22 de enero">
22 de enero

El 22 de enero es el 22.º (vigesimosegundo) día del año en el calendario gregoriano. Quedan 343 días para finalizar el año y 344 en los años bisiestos.










</doc>
<doc id="3049" url="https://es.wikipedia.org/wiki?curid=3049" title="Óptica">
Óptica

La óptica (del latín medieval "opticus, relativo a la visión", proveniente del griego clásico , "optikós") es la rama de la física que involucra el estudio del comportamiento y las propiedades de la luz, incluidas sus interacciones con la materia, así como la construcción de instrumentos que se sirven de ella o la detectan. La óptica generalmente describe el comportamiento de la luz visible, de la radiación ultravioleta y de la radiación infrarroja. Al ser una radiación electromagnética, otras formas de radiación del mismo tipo como los rayos X, las microondas y las ondas de radio muestran propiedades similares.

La mayoría de los fenómenos ópticos pueden explicarse utilizando la descripción electrodinámica clásica de la luz. Sin embargo, la óptica práctica generalmente utiliza modelos simplificados. El más común de estos modelos, la óptica geométrica, trata la luz como una colección de rayos que viajan en línea recta y se desvían cuando atraviesan o se reflejan en las superficies. La óptica física es un modelo de la luz más completo, que incluye efectos ondulatorios como la difracción y la interferencia, que no se pueden abordar mediante la óptica geométrica. 

Algunos fenómenos dependen del hecho de que la luz muestra indistintamente propiedades como onda y partícula. La explicación de estos efectos requiere acudir a la mecánica cuántica. Al considerar las propiedades de la luz similares a las de las partículas, se puede modelar como un conjunto de "fotones" individuales. La óptica cuántica se ocupa de la aplicación de la mecánica cuántica a los sistemas ópticos.

La óptica como ciencia es un campo muy relevante, y es estudiada en muchas disciplinas con las que está íntimamente relacionada, como la astronomía, varios campos de la ingeniería, la fotografía y la medicina (particularmente la oftalmología y la optometría). Las aplicaciones prácticas de la óptica se encuentran en una gran variedad de tecnologías, incluidos espejos, lentes, telescopios, microscopios, equipos láser y sistemas de fibra óptica.

Las primeras aplicaciones de la óptica muy probablemente comenzaron con el desarrollo de lentes en el antiguo Egipto y en Mesopotamia. Las primeras lentes conocidas, hechas de cristal pulido, a menudo cuarzo, datan ya del año 700 a.C., como la lente de Nimrud, descubierta en Asiria. También se conocen esferas de cristal rellenas de agua utilizadas como lentes en la antigua Roma y en la antigua Grecia. La invención de estos objetos fue seguida por la aparición de teorías sobre la luz y la visión planteadas por los antiguos filósofos griegos y de la India, y por el desarrollo de la óptica geométrica en el mundo grecorromano. La palabra "óptica" proviene de la palabra griega ("optikē"), que significa "aspecto, apariencia".

La filosofía griega sobre la óptica se dividió en dos ideas opuestas sobre cómo funcionaba la vista: la "teoría de la visión" y la "teoría de la emisión". Un enfoque consideraba que la visión provenía de los propios objetos, que emitían copias de sí mismos (llamadas "eidola") que eran captadas por el ojo. Con muchos propagadores, entre ellos Demócrito, Epicuro, Aristóteles y sus seguidores.

Platón fue el primero que articuló la teoría de la emisión, la idea de que la visión se logra mediante rayos emitidos por los ojos. También habló sobre la inversión en los espejos (de la paridad entre un objeto y su imagen reflejada) en el "Timaeus". Unos cien años después, Euclides escribió un tratado titulado "Óptica", donde vinculó la visión a la geometría, creando la "óptica geométrica". En su trabajo sobre la teoría de la emisión de Platón describió las reglas matemáticas de la perspectiva y describió los efectos de la refracción cualitativamente, aunque cuestionó que un rayo de luz emitido desde un ojo iluminara instantáneamente las estrellas cada vez que alguien parpadeaba. Claudio Ptolomeo, en su tratado sobre "Óptica", introdujo una teoría de la visión que combinaba las dos anteriores: los rayos (o el flujo emitido) del ojo formaban un cono, el vértice estaba dentro del ojo y la base definía el campo visual. Los rayos eran sensibles y transmitían información al intelecto del observador sobre la distancia y la orientación de las superficies. Resumió gran parte del trabajo de Euclides y describió una forma de medir los efectos de la ley de Snell, aunque no se dio cuenta de la relación empírica existente entre los ángulos.

Durante la Edad Media, las ideas griegas sobre la óptica fueron resucitadas y ampliadas por varios escritores en el mundo islámico. Uno de los primeros fue Al-Kindi (c 801-73), que escribió sobre los méritos de las ideas aristotélicas y euclidianas de la óptica, favoreciendo la teoría de la emisión, ya que podía cuantificar mejor los fenómenos ópticos. En 984, el matemático iraní Ibn Sahl escribió el tratado "Sobre espejos y lentes incendiarios", describiendo correctamente una ley de refracción equivalente a la ley de Snell. Utilizó esta ley para calcular formas óptimas para lentes y espejos curvos. A principios del siglo XI, Alhacén, considerado uno de los padres de la óptica, escribió el "Libro de Óptica" ("Kitab al-manazir") en el que exploró la reflexión y la refracción y propuso un nuevo sistema para explicar la visión y la luz basado en la observación y la experimentación. Rechazó la "teoría de emisión" de la óptica ptolemaica con sus rayos emitidos por el ojo, y planteó la idea de que la luz se refleja en todas las direcciones en líneas rectas desde todos los puntos de los objetos vistos y luego entra en el ojo, aunque no fue capaz de explicar correctamente cómo el ojo captaba los rayos. El trabajo de Alhacén fue ignorado en gran medida en el mundo árabe, pero fue traducido anónimamente al latín alrededor del año 1200 y más tarde resumido y expandido por el monje polaco Witelo, convirtiéndose en un texto estándar sobre óptica en Europa durante los 400 años siguientes.

En la Europa medieval del siglo XIII, el obispo inglés Roberto Grosseteste escribió sobre una amplia gama de temas científicos y discutió la luz desde cuatro perspectivas diferentes: una epistemología de la luz, una metafísica o cosmogonía de la luz, una etiología o física de la luz y un teología de la luz, basándose en las obras de Aristóteles y el platonismo. El discípulo más famoso de Grosseteste, Roger Bacon, escribió obras que citan una amplia gama de trabajos ópticos y filosóficos por entonces traducidos, incluidos los de Alhacén, Aristóteles, Avicena, Averroes, Euclides, al-Kindi, Ptolomeo, Tideus y Constantino el Africano. Bacon pudo usar partes de esferas de vidrio como lupas para demostrar que la luz se refleja en los objetos en lugar de liberarse de ellos.

Los primeros anteojos prácticos fueron inventados en Italia alrededor de 1286. Este fue el comienzo de la industria óptica del pulido de lentes para estos oculares, primero en Venecia y Florencia en el siglo XIII, y más tarde en los centros de fabricación de gafas en los Países Bajos y Alemania. Los fabricantes de gafas crearon tipos mejorados de lentes para la corrección de la visión, basados más en el conocimiento empírico obtenido al observar los efectos de las lentes que en utilizar la rudimentaria teoría óptica de la época (teoría que ni siquiera podía explicar adecuadamente cómo funcionaban las gafas). La práctica del desarrollo, el dominio y la experimentación con lentes condujo directamente a la invención del microscopio óptico compuesto alrededor de 1595 y del telescopio refractor en 1608. Ambos aparecieron en los centros de fabricación de gafas en los Países Bajos.

Hacia el año 1600, Galileo Galilei dirigió su primitivo telescopio refractor hacia el firmamento, dando origen a la astronomía moderna, que podía servirse de instrumentos de aumento para ver los detalles de los cuerpos celestes. Siguiendo su estela, a principios del siglo XVII Johannes Kepler amplió la óptica geométrica en sus escritos, cubriendo las lentes, los reflejos de espejos planos y curvos, los principios de la cámara estenopeica, las leyes de los cuadrados inversos que rigen la intensidad de la luz y las explicaciones ópticas de fenómenos astronómicos como los eclipses lunares y solares y el paralaje astronómico. También fue capaz de deducir correctamente el papel de la retina como el órgano real que percibe las imágenes, y finalmente fue capaz de cuantificar científicamente los efectos de los diferentes tipos de lentes que los fabricantes de gafas habían estado observando durante los últimos 300 años. Después de que se inventara el telescopio, Kepler estableció las bases teóricas sobre cómo funcionaba y describió una versión mejorada, conocida como "telescopio kepleriano", utilizando dos lentes convexas para producir una mayor ampliación.

La teoría óptica progresó a mediados del siglo XVII con los tratados escritos por el filósofo René Descartes, en los que explicaba una gran variedad de fenómenos ópticos, incluyendo la reflexión y la refracción al asumir que la luz era emitida por los objetos que la producían. Esta interpretación difería sustancialmente de la antigua teoría de emisión griega. A finales de la década de 1660 y principios de la de 1670, Isaac Newton expandió las ideas de Descartes en una teoría corpuscular de la luz, y determinó que la luz blanca era una mezcla de colores que se puede separar en sus partes componentes con un prisma. En 1690, Christiaan Huygens propuso una explicación ondulatoria para la luz, basándose en las sugerencias que había hecho Robert Hooke en 1664. El propio Hooke criticó públicamente las teorías de la luz de Newton y la disputa entre los dos duró hasta la muerte de Hooke. En 1704, Newton publicó "Opticks" y, en ese momento, en parte debido a su éxito en otras áreas de la física, generalmente se le consideraba el vencedor en el debate sobre la naturaleza de la luz.

Entretanto, los instrumentos ópticos empezaron a experimentar considerables mejoras técnicas, que permitieron a la ciencia adentrarse en campos hasta entonces inaccesibles, desde lo extremadamente pequeño (representado por el descubrimiento de los microbios) hasta lo inconcebiblemente grande (con un conocimiento cada vez mayor del sistema solar). El microscopio, considerablemente evolucionado desde el primitivo modelo de Anton van Leeuwenhoek (1650), permitió iniciar el estudio de las células gracias a los trabajos pioneros de Robert Hooke, recogidos en su tratado "Micrographia". Por otro lado, los telescopios refractores habían alcanzado su límite teórico de resolución, limitado por la aberración cromática, lo que en parte contribuyó al nacimiento de un nuevo tipo de instrumento: el telescopio reflector. Fue Isaac Newton quien construyó el primero de estos instrumentos en 1668. Este fue el inicio de una enconada carrera, que duró dos siglos y medio, entre los dos tipos de telescopios: refractores (lentes) y reflectores (espejos). La invención de las lentes acromáticas hacia 1750, permitió solucionar el problema de la aberración cromática, lo que dio inicialmente la primacía a los telescopios refractores sobre los primitivos telescopios reflectores, lastrados por la escasa luminancia y la poca durabilidad de los espejos de speculum, una aleación de bronce que se oxidaba con relativa facilidad. En esta época se sentaron las bases del desarrollo de los grandes refractores, que con Joseph von Fraunhofer adquirieron su madurez funcional a finales del siglo XVIII, convirtiéndose en la técnica dominante en el siglo XIX. También fue Fraunhofer quien sentaría las bases de una nueva ciencia que forma parte de la óptica: la espectroscopia. Los avances en la fabricación de lentes permitieron a su vez el desarrollo de los instrumentos utilizados en geodesia, permitiendo completar con una precisión hasta entonces impensable la medición del arco de meridiano de París en 1798, lo que permitiría establecer la unidad de longitud del sistema internacional: el metro.

La óptica newtoniana fue generalmente aceptada hasta principios del siglo XIX, cuando Thomas Young y Augustin Fresnel llevaron a cabo experimentos sobre la interferencia de la luz, que establecieron firmemente su naturaleza ondulatoria. El famoso experimento de la doble rendija de Young, con el que se hacía patente el fenómeno de la interferencia, demostró que la luz seguía el principio de la superposición de estratos, que es una propiedad ondulatoria no prevista por la teoría corpuscular de Newton. Este trabajo condujo a una teoría de la difracción de la luz y abrió un área completa de estudio en la óptica física. La óptica ondulatoria se unificó con éxito con el electromagnetismo gracias a James Clerk Maxwell en los años 1860.

La segunda mitad del siglo XIX contempló una serie de descubrimientos que sentarían las bases del desarrollo de instrumentos ópticos a lo largo del siglo XX. En el campo de los telescopios, la posibilidad de depositar una película de aluminio sobre una base de vidrio, decantó de forma ya definitiva la carrera entre los dos tipos de telescopios, decidiéndose a favor de los de espejos, que han seguido aumentando de tamaño sin cesar desde entonces. Así mismo, se descubrió la base de la fotografía con los trabajos de Niépce, que a su vez propiciaría la aparición del cine unas décadas después. Otro invento de finales del siglo XIX, el tubo de rayos catódicos, permitiría desarrollar unos años después las pantallas de televisión. En este período también vio la luz otro tipo de instrumento científico, el interferómetro, que sirvió para dar un inesperado soporte a la teoría de la relatividad y que con el paso del tiempo ha pasado a formar parte de equipos de medición de altísima precisión, como el LIGO, que ha permitido confirmar la existencia de ondas gravitatorias a comienzos del siglo XXI. 

La aparente confirmación de la naturaleza ondulatoria de la luz debido a su carácter de radiación electromagnética, llevó a un callejón sin salida, generando un intenso debate a lo largo de medio siglo acerca de la existencia del éter, un medio hipotético que se consideraba imprescindible para posibilitar la propagación de las ondas de luz. Se realizaron sin éxito numerosos experimentos para demostrar su existencia (como el famoso experimento de Michelson y Morley de 1887), y no sería hasta 1905 cuando Albert Einstein, con su Teoría de la relatividad especial, estableció el papel clave de la velocidad de la luz como una de las constantes fundamentales de la naturaleza, resolviendo de una vez por todas la cuestión del éter, descartando definitivamente su existencia.

El siguiente desarrollo en la teoría óptica llegó en 1899, cuando Max Planck modeló correctamente la radiación del cuerpo negro, al asumir que el intercambio de energía entre la luz y la materia solo ocurría en cantidades discretas que denominó "cuantos". En 1905 Albert Einstein publicó la teoría del efecto fotoeléctrico que estableció firmemente la cuantificación de la luz en sí misma. En 1913 Niels Bohr demostró que los átomos solo podían emitir cantidades discretas de energía, lo que explica las líneas discretas observadas en los espectros de emisión y de absorción. La comprensión de la interacción entre la luz y la materia que siguió a estos desarrollos no solo formó la base de la óptica cuántica, si no que también fue crucial para el desarrollo de la mecánica cuántica en su conjunto. La última culminación, la teoría electrodinámica cuántica, explica todos los procesos ópticos y electromagnéticos en general como resultado del intercambio de partículas reales y de fotones virtuales.

La óptica cuántica adquirió importancia práctica con las invenciones del máser en 1953 y del láser en 1960. Siguiendo el trabajo de Paul Dirac en la teoría cuántica de campos, George Sudarshan, Roy Jay Glauber y Leonard Mandel aplicaron la teoría cuántica al campo electromagnético en los años 1950 y 1960 para obtener una comprensión más detallada de la fotodetección y del comportamiento estadístico de la luz.

Otro hito importante en el campo de la aplicación práctica de dispositivos ópticos son los LED, cuyo principio de funcionamiento (la electroluminiscencia) fue descubierto en 1903. Se empezaron a producir industrialmente en la década de 1950, hasta hacerse omnipresentes en las pantallas de todo tipo de aparatos de consumo de masas, como teléfonos móviles o televisores.

La óptica clásica se divide en dos ramas principales: la óptica geométrica (o de rayos) y la óptica física (u ondulatoria). En la óptica geométrica, se considera que la luz viaja en línea recta, mientras que en la óptica física, la luz se considera como una onda electromagnética.

La óptica geométrica se puede ver como una aproximación a la óptica física que se aplica cuando la longitud de onda de la luz utilizada es mucho menor que el tamaño de los elementos ópticos en el sistema que se está analizando.

La "óptica geométrica", u "óptica de rayos", describe la propagación de la luz en términos de "rayos" que viajan en línea recta, y cuyos caminos se rigen por las leyes de la reflexión y la refracción en los cambios de fase entre diferentes medios. Estas leyes descubiertas empíricamente se han utilizado de forma generalizada en el diseño de componentes e instrumentos ópticos. 

Las leyes de reflexión y refracción pueden derivarse del principio de Fermat, que establece que "el camino recorrido entre dos puntos por un rayo de luz es el camino que se puede atravesar en el menor tiempo posible".

La óptica geométrica a menudo se simplifica haciendo una aproximación paraxial o "aproximación de ángulos pequeños". El comportamiento matemático se vuelve lineal, permitiendo que los componentes ópticos y los sistemas se describan mediante matrices simples. Esto lleva a las técnicas de la óptica gaussiana y del "trazado de rayos paraxial", que se utilizan para determinar las propiedades básicas de los sistemas ópticos, como las imágenes y posiciones aproximadas de objetos y el correspondiente aumento óptico.

La reflexión se puede dividir en dos tipos: imagen especular y reflexión difusa. La reflexión especular describe el brillo de superficies como los espejos, que reflejan la luz de una manera simple y predecible. Esto permite la producción de imágenes reflejadas que pueden asociarse con una ubicación real (real) o extrapolada (virtual) en el espacio. La reflexión difusa describe materiales no brillantes, como papel o las rocas. Los reflejos de estas superficies solo se pueden describir estadísticamente, con la distribución exacta de la luz reflejada dependiendo de la estructura microscópica del material. Muchos reflectores difusos se describen o se pueden aproximar mediante la ley de Lambert, que describe superficies que tienen igual luminancia cuando se ven desde cualquier ángulo. Las superficies brillantes pueden dar una reflexión tanto especular como difusa.

En la reflexión especular, la dirección del rayo reflejado está determinada por el ángulo que forma el rayo incidente con el vector normal, una línea perpendicular a la superficie en el punto donde incide el rayo. Los rayos incidentes y reflejados y la normal se encuentran en un solo plano, y el ángulo entre el rayo reflejado y la superficie normal es el mismo que entre el rayo incidente y la normal. Este fenómeno físico se conoce como imagen especular.

Para espejos planos, la ley de la reflexión implica que las imágenes de los objetos están en posición vertical y a la misma distancia detrás del espejo que los objetos frente al espejo. El tamaño de la imagen es el mismo que el tamaño del objeto. La ley también implica que las imágenes especulares presentan una paridad invertida, que se percibe como una inversión izquierda-derecha. Las imágenes formadas a partir de la reflexión en dos (o cualquier cantidad par de) espejos no presentan paridad invertida. Un reflector de esquina es un retrorreflector que produce rayos reflejados que viajan en la misma dirección (y distinto sentido) desde la que vinieron los rayos incidentes.

Los espejos curvos pueden ser modelizados utilizando el trazado de rayos y usando la ley de reflexión en cada punto de la superficie. En los espejos parabólicos, los rayos paralelos al eje incidentes en el espejo producen rayos reflejados que convergen en un foco común. Otras superficies curvas también pueden enfocar la luz, pero con aberraciones debidas a la forma divergente que hace que el foco se disperse en el espacio. En particular, los espejos esféricos exhiben aberración esférica. Los espejos curvados pueden formar imágenes con una ampliación mayor o menor que uno, y la ampliación puede ser negativa, lo que indica que la imagen está invertida. Una imagen vertical formada por reflejo en un espejo siempre es virtual, mientras que una imagen invertida es real y puede proyectarse en una pantalla.

La refracción se produce cuando la luz viaja a través de un área del espacio que tiene un índice de refracción cambiante; este principio permite construir lentes capaces de enfocar la luz. El caso más simple de refracción ocurre cuando se tiene una interfaz entre un medio uniforme con índice de refracción formula_1 y otro medio con índice de refracción formula_2. En tales situaciones, la ley de Snell describe la deflexión resultante del rayo de luz:

donde formula_4 y formula_5 son los ángulos entre la normal (a la interfaz) y los rayos incidentes y refractados, respectivamente.

El índice de refracción de un medio está relacionado con la velocidad, , de la luz en ese medio por
donde es la velocidad de la luz.

La ley de Snell se puede utilizar para predecir la deflexión de los rayos de luz a medida que pasan a través de medios lineales, siempre que se conozcan los índices de refracción y la geometría de los medios. Por ejemplo, la propagación de la luz a través de un prisma da como resultado que el rayo de luz se desvíe dependiendo de la forma y orientación del prisma. En la mayoría de los materiales, el índice de refracción varía con la frecuencia de la luz. Teniendo esto en cuenta, la ley de Snell se puede utilizar para predecir cómo un prisma dispersará la luz en un espectro.

Algunos medios tienen un índice de refracción que varía gradualmente con la posición y, por lo tanto, los rayos de luz en el medio son curvos. Este efecto es responsable de los espejismos vistos en días calurosos: un cambio en el índice de refracción del aire en altura hace que los rayos de luz se curven, creando la apariencia de reflejos especulares en la distancia (como si estuvieran en la superficie de una extensión de agua). Los materiales ópticos con índice de refracción variable se denominan materiales de índice de gradiente (GRIN). Dichos materiales se utilizan para hacer instrumentos de óptica de índice de gradiente.

Para los rayos de luz que viajan desde un material con un alto índice de refracción a un material con un índice de refracción bajo, la ley de Snell predice que formula_5 desaparece cuando formula_4 es grande. En este caso, no ocurre transmisión; toda la luz se refleja. Este fenómeno se llama reflexión interna total y permite la tecnología de la fibra óptica. A medida que la luz viaja por una fibra óptica, se somete a una reflexión interna total que permite que prácticamente no se pierda luz en el cable.

Un dispositivo que produce rayos de luz convergentes o divergentes debido a la refracción se conoce como "lente". Las lentes se caracterizan por su distancia focal: una lente convergente tiene una distancia focal positiva, mientras que una lente divergente tiene una distancia focal negativa. Una distancia focal más pequeña indica que la lente tiene un efecto convergente o divergente más fuerte. La distancia focal de una lente simple en el aire viene dada por la configuración de la propia lente.

El trazado de rayos se puede usar para mostrar cómo se forman las imágenes con una lente. Para una lente delgada en el aire, la ubicación de la imagen viene dada por la simple ecuación:

donde formula_10 es la distancia desde el objeto a la lente, formula_11 es la distancia desde la lente a la imagen, y formula_12 es la distancia focal de la lente. Con la convención de signos utilizada, las distancias entre el objeto y la imagen son positivas si el objeto y la imagen están en lados opuestos de la lente.

Los rayos paralelos entrantes se enfocan mediante una lente convergente en un punto a una distancia focal de la lente, en el lado más alejado de la lente. Esto se llama punto focal trasero de la lente. Los rayos de un objeto a distancia finita se enfocan más lejos de la lente que la distancia focal; cuanto más cerca esté el objeto de la lente, más lejos estará la imagen de la lente.

Con lentes divergentes, los rayos paralelos entrantes divergen después de atravesar la lente, de tal manera que parecen haberse originado en un punto a una distancia focal enfrente de la lente. Este es el punto focal frontal de la lente. Los rayos de un objeto a una distancia finita se asocian con una imagen virtual que está más cerca de la lente que el punto focal, y en el mismo lado de la lente que el objeto. Cuanto más cerca esté el objeto de la lente, más cerca estará la imagen virtual de la lente. Al igual que con los espejos, las imágenes verticales producidas por una sola lente son virtuales, mientras que las imágenes invertidas son reales.

Las lentes sufren de aberraciones que distorsionan las imágenes. Las "aberraciones monocromáticas" ocurren porque la geometría de la lente no dirige los rayos desde cada punto del objeto a un solo punto en la imagen, mientras que la aberración cromática ocurre porque el índice de refracción de la lente varía con la longitud de onda de la luz.

En óptica física, se considera que la luz se propaga como una onda. Este modelo predice fenómenos como la interferencia y la difracción, que no se explican por la óptica geométrica. Las ondas se propagan en la atmósfera terrestre casi a la misma velocidad de la luz en el vacío, aproximadamente a 3,0×10 m/s (exactamente 299,792,458 m/s en el vacío). La longitud de onda de las ondas de luz visible varía entre 400 y 700 nm, pero el término "luz" también se aplica con frecuencia a la radiación infrarroja (0.7-300 μm) y a la radiación ultravioleta (10-400 nm).
El modelo de onda se puede usar para hacer predicciones sobre cómo se comportará un sistema óptico sin requerir una explicación de sobre qué medio se están "agitando" las ondas. Hasta mediados del siglo XIX, la mayoría de los físicos creían en un medio "etéreo" en el que se propagaba la perturbación lumínica. La existencia de ondas electromagnéticas fue predicha en 1865 por las ecuaciones de Maxwell. Estas ondas se propagan a la velocidad de la luz y manifiestan campos eléctricos y magnéticos variables que son ortogonales entre sí, y también a la dirección de propagación de las ondas. Actualmente, las ondas de luz se tratan como ondas electromagnéticas, excepto cuando se deben considerar efectos de mecánica cuántica.

Muchas aproximaciones simplificadas están disponibles para analizar y diseñar sistemas ópticos. La mayoría usan una sola cantidad escalar para representar el campo eléctrico de la onda de luz, en lugar de usar un modelo vectorial con vectores eléctricos y magnéticos ortogonales.

La ecuación de Huygens–Fresnel es uno de esos modelos, deducido empíricamente por Fresnel en 1815, basándose en la hipótesis de Huygens de que cada punto en un frente de onda genera un frente de onda esférico secundario, que Fresnel combinaba con el principio de superposición de ondas. La fórmula de difracción de Kirchhoff, que se deduce a partir de las ecuaciones de Maxwell, coloca la ecuación de Huygens-Fresnel sobre una base física más firme. Los ejemplos de la aplicación del principio de Huygens-Fresnel se pueden encontrar en las secciones de "difracción" y "difracción de Fraunhofer".

Se requieren modelos más rigurosos, que impliquen el modelado de campos eléctricos y magnéticos de la onda de luz, cuando se trata de la interacción detallada de la luz con materiales en los que la interacción depende de sus propiedades eléctricas y magnéticas. Por ejemplo, el comportamiento de una onda de luz interactuando con una superficie de metal es bastante diferente de lo que sucede cuando interactúa con un material dieléctrico. También se debe usar un modelo vectorial para modelizar la luz polarizada.

Las técnicas de simulación numérica, como el método de los elementos finitos, el método de elementos de frontera y el método de transmisión lineal matricial, se pueden usar para modelar la propagación de la luz en sistemas que no se pueden resolver analíticamente. Dichos modelos son computacionalmente exigentes y normalmente solo se utilizan para resolver problemas de pequeña escala que requieren una precisión superior a la que se puede lograr con soluciones analíticas.

Todos los resultados de la óptica geométrica se pueden reproducir utilizando las técnicas de la óptica de Fourier, que aplican muchas de las mismas técnicas matemáticas y analíticas utilizadas en ingeniería acústica y procesamiento de señales.

El haz de propagación gaussiano es un modelo de óptica física paraxial simple para abordar la propagación de radiación coherente, como los rayos láser. Esta técnica explica parcialmente la difracción, permitiendo cálculos precisos de la velocidad a la que un rayo láser se expande con la distancia y el tamaño mínimo al que se puede enfocar el rayo. La propagación del haz gaussiano cierra la brecha entre la óptica geométrica y la física.

En ausencia de efectos no lineales, el principio de superposición puede usarse para predecir la configuración de las formas de onda que interactúan mediante la simple suma de las perturbaciones. Esta interacción de ondas para producir un patrón resultante generalmente se denomina "interferencia" y puede dar como resultado una gran variedad de resultados. Si dos ondas de la misma longitud de onda y frecuencia están "en fase", las crestas y los valles de las ondas se alinean. Esto da como resultado una interferencia, con un aumento en la amplitud de la onda, que para la luz se asocia con un brillo de la forma de onda en esa ubicación. Alternativamente, si las dos ondas de la misma longitud de onda y frecuencia están desfasadas, las crestas de onda se alinearán con los valles de cada onda y viceversa. Esto da como resultado una interferencia con una disminución en la amplitud de la onda, que para la luz se asocia con un oscurecimiento de la forma de onda en esa ubicación. Véase a continuación una ilustración de este efecto.

Como el Principio de Fresnel - Huygens establece que cada punto de un frente de onda está asociado con la producción de una nueva perturbación, es posible que un frente de onda se interfiera de manera constructiva o destructiva en diferentes ubicaciones, produciendo franjas brillantes y oscuras en patrones regulares y predecibles. La interferometría es la ciencia que mide estos patrones, generalmente como un medio para hacer determinaciones precisas de distancias o resoluciones ópticas. El interferómetro de Michelson es un instrumento famoso que usaba efectos de interferencia para medir con precisión la velocidad de la luz.

La apariencia de películas finas y revestimientos se ve directamente afectada por los efectos de interferencia. La supresión de reflejos utiliza la interferencia destructiva para reducir la reflectividad de las superficies que recubren, y se pueden usar para minimizar el deslumbramiento y los reflejos no deseados. El caso más simple es una sola capa con un grosor de un cuarto de la longitud de onda de la luz incidente. La onda reflejada desde la parte superior de la película y la onda reflejada desde la interfaz película/material están exactamente desfasadas 180°, lo que causa interferencia destructiva. Las ondas solo están exactamente desfasadas para una longitud de onda determinada, que normalmente se elige para estar cerca del centro del espectro visible, alrededor de 550 nm. Los diseños más complejos que utilizan capas múltiples pueden lograr una baja reflectividad en una banda ancha o una reflectividad extremadamente baja en una sola longitud de onda.

La interferencia constructiva en películas delgadas puede crear un fuerte reflejo de la luz en un rango de longitudes de onda, que puede ser estrecho o amplio dependiendo del diseño del recubrimiento. Estas películas se usan para hacer espejos dieléctricos, filtros de interferencia, reflectores de calor y filtros para separación de colores en cámaras de televisión en color. Este efecto de interferencia también es lo que causa los coloridos patrones del arco iris que se ven en las manchas de petróleo y en las pompas de jabón.

La difracción es el proceso por el que la interferencia de la luz se observa más comúnmente. El efecto fue descrito por primera vez en 1665 por Francesco Maria Grimaldi, quien también acuñó el término del latín "diffringere", para "romperse en trozos". Posteriormente en ese mismo siglo, Robert Hooke e Isaac Newton también describieron fenómenos que ahora se conocen como difracción en anillos de Newton, mientras que James Gregory registró sus observaciones sobre los patrones de difracción de las plumas de ave.

El primer modelo de la difracción utilizando la óptica física se basó en el principio de Fresnel - Huygens, y fue desarrollado en 1803 por Thomas Young mediante su experimento de la doble rendija, analizando los patrones de interferencia de dos ranuras estrechamente espaciadas. Demostró que sus resultados solo podían explicarse si las dos ranuras actuaban como dos únicas fuentes de ondas en lugar de corpúsculos. En 1815 y 1818, Augustin Fresnel estableció firmemente las matemáticas de cómo la interferencia de ondas puede explicar la difracción.

Los modelos físicos más simples de difracción usan ecuaciones que describen la separación angular de franjas claras y oscuras debido a la luz de una longitud de onda particular (λ). En general, la ecuación toma la forma

donde formula_14 es la separación entre dos fuentes de frente de onda (en el caso de los experimentos de Young, fueron dos ranuras), formula_15 es la separación angular entre la franja central y la franja de orden formula_16, donde el máximo central es formula_17.

Esta ecuación se modifica ligeramente para tener en cuenta una variedad de situaciones tales como la difracción a través de un espacio único, la difracción a través de rendijas múltiples o la difracción a través de una red de difracción que contiene un gran número de rendijas a igual espaciado. Los modelos de difracción más complicados requieren trabajar con las matemáticas de Fresnel o de Fraunhofer.

La cristalografía de rayos X hace uso del hecho de que los átomos en un cristal tienen un espaciado regular a distancias que están en el orden de un ångström. Para ver los patrones de difracción, se hacen pasar rayos X con longitudes de onda similares a ese espaciado a través del cristal. Dado que los cristales son objetos tridimensionales en lugar de rejillas bidimensionales, el patrón de difracción asociado varía en dos direcciones según la ley de Bragg, y los puntos brillantes asociados se producen en patrones únicos y formula_14 es el doble del espaciado entre átomos.

Los efectos de la difracción limitan la sensibilidad de un detector óptico a la separación entre dos fuentes de luz, determinando su resolución óptica. En general, la luz que pasa por una apertura experimentará difracción, y las mejores imágenes que se pueden crear a través de esta apertura (como se describe en un "sistema limitado por la difracción") aparecen como un punto central con anillos brillantes circundantes, separados por franjas oscuras; este patrón se conoce como disco de Airy. El tamaño de dicho disco viene dado por

donde "θ" es la resolución angular, "λ" es la longitud de onda de la luz, y "D" es el diámetro de la apertura del objetivo. Si la separación angular de los dos puntos es significativamente menor que el radio angular del disco de Airy, entonces los dos puntos no se pueden resolver en la imagen, pero si su separación angular es mucho mayor que esta, se forman imágenes distintas de los dos puntos y por lo tanto, se pueden resolver. Rayleigh definió la "resolución óptica" de forma arbitraria como los dos puntos cuya separación angular es igual al radio del disco de Airy (medido al primer anillo nulo, es decir, al primer lugar donde no se ve luz) que pueden considerarse resueltos. Se puede ver que cuanto mayor es el diámetro de la lente o su apertura, más fina es la resolución. La interferometría astronómica, con su capacidad para emular aperturas de línea de base extremadamente grandes, permite la mayor resolución angular posible.

Para imágenes astronómicas, la atmósfera impide que se logre una resolución óptima en el espectro visible debido a la dispersión atmosférica que provoca el titilado de las estrellas. Los astrónomos se refieren a este efecto como la calidad de visualización. Las técnicas conocidas como óptica adaptativa se han utilizado para eliminar la alteración atmosférica de las imágenes y lograr resultados que se acercan al límite de difracción.

Los procesos de refracción tienen lugar en el límite de la óptica física, donde la longitud de onda de la luz es similar a otras distancias, como en un fenómeno del tipo de la dispersión. El tipo más simple de dispersión es la dispersión de Thomson, que ocurre cuando las ondas electromagnéticas son desviadas por partículas individuales. En el límite de la dispersión de Thomson, en la que la naturaleza ondulatoria de la luz es evidente, la luz se dispersa independientemente de la frecuencia, en contraste con el efecto Compton, que depende de la frecuencia y es estrictamente un proceso de mecánica cuántica, que involucra la naturaleza de la luz como un haz de partículas. En un sentido estadístico, la dispersión elástica de la luz provocada por numerosas partículas mucho más pequeñas que la longitud de onda de la luz es un proceso conocido como dispersión de Rayleigh, mientras que el proceso similar para dispersar partículas similares o mayores que la longitud de onda se conoce como difusión de Mie siendo el efecto Tyndall un resultado comúnmente observado. Una pequeña proporción de la dispersión de la luz producida por átomos o moléculas puede sufrir efecto Raman, donde la frecuencia cambia debido a la excitación de los átomos y las moléculas. La dispersión de Brillouin se produce cuando la frecuencia de la luz varía debido a los cambios locales con el tiempo y los movimientos de un material denso.

La dispersión tiene lugar cuando diferentes frecuencias de luz tienen velocidades de fase diferentes, debido a las propiedades del material ("dispersión del material") o a la geometría de una guía de onda óptica ("dispersión de guía de onda"). La forma más familiar de dispersión es una disminución en el índice de refracción con el aumento de la longitud de onda, que se observa en la mayoría de los materiales transparentes. Esto se llama "dispersión normal". Ocurre en todos los materiales dieléctricos, en rangos de longitud de onda donde el material no absorbe la luz. En los rangos de longitud de onda donde un medio tiene una absorción significativa, el índice de refracción puede aumentar con la longitud de onda. Este fenómeno se denomina "dispersión anómala".

La separación de colores por un prisma es un ejemplo de dispersión normal. En las superficies del prisma, la ley de Snell predice que la luz incidente en un ángulo θ a la normal se refracta en un ángulo igual al [arco seno (sin (θ)/"n")]. Por lo tanto, la luz azul, con su índice de refracción más alto, se desvía con más fuerza que la luz roja, lo que da como resultado el conocido patrón del arcoíris.

La dispersión de un material a menudo se caracteriza por el número de Abbe, que proporciona una medida de la dispersión simple basada en el índice de refracción en tres longitudes de onda específicas. La dispersión de conducción de onda depende de la constante de propagación. Ambos tipos de dispersión provocan cambios en las características del grupo de la onda y en las características del paquete de onda, que cambian con la misma frecuencia que la amplitud de la propia onda electromagnética. La "dispersión de la velocidad de grupo" se manifiesta como una dispersión de la "envolvente" de la señal de la radiación y se puede cuantificar con un parámetro de retardo de la dispersión de grupo:

donde formula_21 es la velocidad de grupo. Para un medio uniforme, la velocidad del grupo es

donde "n" es el índice de refracción y "c" es la velocidad de la luz en el vacío. Esto proporciona una forma más simple para el parámetro de retardo de la dispersión:

Si "D" es menor que cero, se dice que el medio tiene "dispersión positiva" o dispersión normal. Si "D" es mayor que cero, el medio tiene "dispersión negativa". Si un pulso de luz se propaga a través de un medio normalmente dispersivo, el resultado es que los componentes de frecuencia más alta se ralentizan más que los componentes de frecuencia más baja. Por lo tanto, el pulso se convierte en "positivamente pulsante", aumentando su frecuencia con el tiempo. Esto hace que el espectro que sale de un prisma aparezca con la luz roja menos refractada y la luz azul/violeta más refractada. Por el contrario, si un pulso viaja a través de un medio dispersivo anómalo (negativo), los componentes de alta frecuencia viajan más rápido que los de baja frecuencia y el pulso se vuelve "negativamente pulsante", disminuyendo en frecuencia con el tiempo.

El resultado de la dispersión de la velocidad del grupo, ya sea negativa o positiva, es la dispersión temporal del pulso. Esto hace que la gestión de la dispersión sea extremadamente importante en los sistemas de comunicaciones ópticas basados en la fibra óptica, ya que si la dispersión es demasiado alta, un grupo de pulsos que codifican una información binaria se dispersarán en el tiempo y se fusionarán,lo que hará imposible extraer la señal.

La polarización es una propiedad general de las ondas que describe la orientación de sus oscilaciones. Para ondas transversales, como muchas ondas electromagnéticas, describe la orientación de las oscilaciones en el plano perpendicular a la dirección de desplazamiento de la onda. Las oscilaciones pueden orientarse en una sola dirección (polarización lineal), o la dirección de oscilación puede rotar a medida que la onda se desplaza (circular o elíptica). Las ondas polarizadas circularmente pueden girar hacia la derecha o hacia la izquierda respecto a la dirección de desplazamiento, y cuál de esas dos rotaciones está presente en una onda se denomina quiralidad de la onda.

La forma típica de considerar la polarización es realizar un seguimiento de la orientación del vector del campo eléctrico a medida que la onda electromagnética se propaga. El vector de campo eléctrico de una onda plana se puede dividir arbitrariamente en dos componentes perpendiculares con las denominaciones "x" e "y" (con z indicando la dirección de propagación). La forma proyectada en el plano xy por el vector del campo eléctrico es una figura de Lissajous que describe el "estado de polarización". Las figuras anteriores muestran algunos ejemplos de la evolución del vector del campo eléctrico (azul), con el tiempo (el eje vertical), en un punto particular en el espacio, junto con sus componentes "x" e "y" (rojo/izquierda y verde/derecha), y la ruta trazada por el vector en el plano (violeta): la misma evolución ocurriría si se observa el campo eléctrico en la dirección opuesta a la propagación en un momento particular mientras el punto evoluciona en el espacio.

En la figura anterior, los componentes "x" e "y" de la onda de luz están en fase. En este caso, la relación de sus amplitudes es constante, por lo que la dirección del vector eléctrico (el vector suma de estos dos componentes) es constante. Como la punta del vector traza una sola línea en el plano, este caso especial se llama polarización lineal. La dirección de esta línea depende de las amplitudes relativas de los dos componentes.

En la figura central, las dos componentes ortogonales tienen las mismas amplitudes y están desfasadas 90°. En este caso, un componente es cero cuando el otro componente está en amplitud máxima o mínima. Hay dos posibles relaciones de fase que satisfacen este requisito: el componente "x" puede estar 90° por delante del componente "y" o puede estar 90° por detrás del componente "y". En este caso especial, el vector eléctrico traza un círculo en el plano, por lo que esta polarización se denomina polarización circular. La dirección de rotación en el círculo depende de cuál de las dos relaciones de fase existe y corresponden a la "polarización circular dextrógira" y a la "polarización circular levógira".

En todos los demás casos, cuando los dos componentes no tienen las mismas amplitudes y/o su diferencia de fase no es cero ni múltiplo de 90°, la polarización se llama polarización elíptica porque el vector eléctrico traza una elipse en el plano (la "elipse de polarización"). Esto se muestra en la figura de arriba a la derecha. Las matemáticas detalladas de la polarización utilizan el cálculo de Jones y se caracterizan por los parámetros de Stokes.

Los medios que tienen diferentes índices de refracción para diferentes modos de polarización se llaman "birrefringentes". Manifestaciones bien conocidas de este efecto aparecen en láminas de onda/retardadores ópticos (modos lineales) y en el efecto Faraday/actividad óptica (modos circulares). Si la longitud de la ruta en el medio birrefringente es suficiente, las ondas de polarización plana saldrán del material con una dirección de propagación significativamente diferente, debido a la refracción. Por ejemplo, este es el caso de los cristales macroscópicos de calcita, que presentan al espectador dos imágenes desplazadas, ortogonalmente polarizadas, de lo que se ve a través de ellos. Fue este efecto el que proporcionó el primer descubrimiento de un fenómeno de polarización por Rasmus Bartholin en 1669. Además, el cambio de fase, y por lo tanto, el cambio en el estado de polarización, generalmente depende de la frecuencia, lo que, en combinación con el dicroísmo, a menudo da lugar a colores brillantes y efectos tipo arcoíris. En mineralogía, dichas propiedades, conocidas como pleocroísmo, se explotan con frecuencia con el fin de identificar minerales utilizando microscopios con luz polarizada. Además, muchos plásticos que normalmente no son birrefringentes llegan a serlo cuando están sujetos a tensión mecánica, un fenómeno que es la base de los métodos de fotoelasticidad. Para hacer rotar la polarización lineal de haces de luz, además del polarizador rotativo, existen prismáticos que usan la reflexión interna total en un conjunto de prismas diseñado para obtener una transmisión colineal eficiente.

Los medios que reducen la amplitud de ciertos modos de polarización se llaman "dicróicos", con dispositivos que bloquean casi toda la radiación en unos dispositivos conocidos como "filtros polarizadores" o simplemente "polarizadores". La ley de Malus, que lleva el nombre de Étienne-Louis Malus, dice que cuando se coloca un polarizador perfecto en un haz de luz polarizado lineal, la intensidad, "I", de la luz que lo atraviesa viene dada por

donde

Se puede pensar que un haz de luz no polarizada contiene una mezcla uniforme de polarizaciones lineales en todos los ángulos posibles. Dado que el valor promedio de formula_25 es 1/2, el coeficiente de transmisión se convierte en

En la práctica, se pierde algo de luz en el polarizador y la transmisión real de luz no polarizada será algo menor, alrededor del 38% para los polarizadores de tipo Polaroid pero considerablemente mayor (>49.9%) para algunos tipos de prismas birrefringentes.

Además de la birrefringencia y el dicroísmo en medios extensos, los efectos de la polarización también pueden ocurrir en la interfaz (reflectante) entre dos materiales de diferente índice de refracción. Estos efectos son tratados por las ecuaciones de Fresnel. Parte de la onda se transmite y parte se refleja, y la relación depende del ángulo de incidencia y del ángulo de refracción. De esta manera, la óptica física se relaciona con la física ondulatoria a través del parámetro denominado ángulo de Brewster. Cuando la luz se refleja desde una película delgada en una superficie, la interferencia entre las reflexiones de las superficies de la película puede producir polarización en la luz reflejada y en la transmitida.

La mayoría de las fuentes de radiación electromagnética contienen una gran cantidad de átomos o moléculas que emiten luz. La orientación de los campos eléctricos producidos por estos emisores puede no estar correlacionada, en cuyo caso se dice que la luz está "no polarizada". Si hay una correlación parcial entre los emisores, la luz está "parcialmente polarizada". Si la polarización es constante en todo el espectro de la fuente, la luz parcialmente polarizada se puede describir como una superposición de un componente completamente no polarizado, y uno completamente polarizado. La luz puede describirse en términos de su grado de polarización y según los parámetros de la elipse de polarización.

Cuando es reflejada por materiales transparentes y brillantes, está parcial o totalmente polarizada, excepto si la luz es normal (perpendicular) a la superficie. Fue este efecto el que permitió al matemático Étienne-Louis Malus realizar las mediciones que permitieron desarrollar los primeros modelos matemáticos de la luz polarizada. La polarización se produce cuando la luz se dispersa en la atmósfera terrestre. La luz dispersa produce el brillo y el color del cielo despejado. Esta polarización parcial de la luz dispersada se puede aprovechar al usar filtros polarizadores para oscurecer el cielo en determinadas fotografías. La polarización óptica es principalmente importante en química, debido al dicroísmo circular y a la actividad óptica (""birrefringencia circular"") exhibida por moléculas quirales ópticamente activas.

La "óptica moderna" abarca áreas de la ciencia óptica y de la ingeniería que se hicieron populares en el siglo XX. Estas áreas de la ciencia óptica se relacionan típicamente con las propiedades electromagnéticas o cuánticas de la luz, pero incluyen otros temas. Un importante subcampo de la óptica moderna, la óptica cuántica, trata específicamente de las propiedades de la luz según la mecánica cuántica. La óptica cuántica no es solo teórica; algunos dispositivos modernos, como los láseres, tienen principios de funcionamiento que describe la mecánica cuántica. Los detectores de luz, como fotomultiplicadores y canaltrones, responden a fotones individuales. Los sensores de imagen electrónicos, como los CCDs, exhiben un ruido de disparo correspondiente a las estadísticas de eventos de fotones individuales. Los Leds y las células fotoeléctricas tampoco se pueden entender sin la mecánica cuántica. En el estudio de estos dispositivos, la electrónica cuántica a menudo se superpone con la óptica cuántica.

Las áreas de especialidad de investigación óptica incluyen el estudio de cómo la luz interactúa con materiales específicos como en la óptica de cristales y en metamateriales. Otra línea de investigación se centra en los fenómenos asociados a las ondas electromagnéticas como en las singularidades ópticas, la óptica sin imagen, la óptica no lineal, la óptica estadística y la radiometría. Además, la ingeniería en computación se ha interesado en la óptica integrada, las máquinas de visión y las computadoras ópticas como posibles componentes de la "próxima generación" de ordenadores.

En la actualidad,la ciencia pura de la óptica se llama ciencia óptica o física óptica para distinguirla de las ciencias ópticas aplicadas, que se conocen como ingeniería óptica. Los subcampos destacados de la ingeniería óptica incluyen la ingeniería de la iluminación, la fotónica y la optoelectrónica, con aplicaciones prácticas como el diseño óptico de lentes, la fabricación y prueba de componentes ópticos y el procesamiento digital de imágenes. Algunos de estos campos se superponen, con límites nebulosos entre los términos que describen las respectivas disciplinas, que significan cosas ligeramente diferentes en diferentes partes del mundo y en diferentes áreas de la industria. En las últimas décadas se ha desarrollado una comunidad profesional de investigadores en óptica no lineal, gracias a los avances en tecnología láser.

Un láser es un dispositivo que emite luz (radiación electromagnética) a través de un proceso llamado "emisión estimulada". El término "láser" es un acrónimo de la expresión inglesa ""Light Amplification by Stimulated Emission of Radiation"" ("Amplificación de luz por emisión estimulada de radiación"). La luz láser es generalmente coherente, lo que significa que se emite en un estrecho haz de baja divergencia o que puede convertirse en uno de estos haces con la ayuda de componentes ópticos como las lentes. Debido a que el equivalente en microondas del láser, el "máser", se desarrolló primero, los dispositivos que emiten frecuencias de microondas y de radio generalmente se llaman "másers".

El primer láser en funcionamiento fue presentado el 16 de mayo de 1960 por Theodore Harold Maiman en los Hughes Research Laboratories. Cuando se inventaron por primera vez, se los llamó "una solución que busca un problema". Desde entonces, los láseres se han convertido en una industria multimillonaria, encontrando utilidad en miles de aplicaciones muy variadas. La primera aplicación de láser visible en la vida cotidiana de la población general fue el escáner de código de barras de los supermercados, introducido en 1974. El reproductor laserdisc, presentado en 1978, fue el primer producto de consumo exitoso en incluir un láser, pero el reproductor de disco compacto fue el primer dispositivo equipado con láser verdaderamente común en los hogares de los consumidores, comenzando en 1982. Estos dispositivos de almacenamiento óptico usan un diodo láser de menos de un milímetro de ancho con el que escanean la superficie del disco para la recuperación de datos. Las comunicaciones por fibra óptica dependen de los láseres para transmitir grandes cantidades de información a la velocidad de la luz. Otras aplicaciones comunes de los láseres incluyen las impresoras láser y los punteros láser. También se usan en medicina en áreas como la cirugía general, la cirugía refractiva y la microdisección láser; así como en aplicaciones militares como sistemas antimisil, contramedidas electro-ópticas y sistemas lIDAR. Los láseres también se usan en holografía, grabados 3D, pantallas láser y depilación láser.

En construcción, se utilizan como herramientas de corte de planchas metálicas; en geodesia y topografía los telémetros láser sirven para la medida precisa de distancias (como en el caso extremo de la medición de la distancia entre la Tierra y la Luna, utilizando los espejos situados en la superficie del satélite por distintas misiones espaciales); y en la navegación aeronáutica son la base de los giróscopos láser de anillo.

Así mismo, en algunos tipos de reactores de fusión nuclear se utilizan rayos láser de gran potencia para alcanzar las elevadas temperaturas que requieren este tipo de reacciones.

El efecto Kapitsa-Dirac hace que los haces de partículas se difracten como resultado de encontrarse con una onda estacionaria de luz. La luz se puede usar para manipular fragmentos atómicos o moleculares de materia, aprovechando las propiedades de este fenómeno (véase "pinza óptica").

La óptica es parte de la vida cotidiana. La ubicuidad de los sistemas visuales en biología indica el papel central que juega la óptica como ciencia de uno de los cinco sentidos. Muchas personas se benefician de gafas o lente de contacto, y la óptica es esencial para el funcionamiento de muchos bienes de consumo, incluidas cámaras fotográficas, de cine o de televisión. El arco iris y los espejismos son ejemplos de fenómenos ópticos. La fibra óptica proporciona la red troncal tanto para Internet como para la telefonía moderna.

El ojo humano funciona enfocando la luz sobre una capa de fotorreceptores llamada retina, que forma el revestimiento interior de la parte posterior del ojo. El enfoque se logra mediante una serie de medios transparentes. La luz que entra al ojo pasa primero a través del córnea, que proporciona gran parte de la potencia óptica del ojo. Luego continúa a través del fluido contenido justo detrás de la córnea, en la cámara anterior, y pasa a través de la pupila. A continuación atraviesa el cristalino, que enfoca más la luz y permite el ajuste del enfoque, y pasa a través del cuerpo principal de fluido interior del ojo, el humor vítreo, y alcanza la retina. Las células fotosensibles de la retina recubren la parte posterior del ojo, excepto donde sale el nervio óptico; esto da como resultado un punto ciego.

Hay dos tipos de células fotorreceptoras, bastones y conos, que son sensibles a diferentes aspectos de la luz. Las células Los conos son sensibles a la intensidad de la luz en un amplio rango de frecuencia, por lo tanto son responsables de la visión en blanco y negro. Los bastones no están presentes en la fóvea, el área de la retina responsable de la visión central, y no son tan sensibles como los conos a los cambios espaciales y temporales de la luz. Sin embargo, hay veinte veces más bastones que conos en la retina, porque los primeros están presentes en un área más amplia. Debido a su distribución más amplia, los bastones son responsables de la visión periférica.

Por el contrario, los conos son menos sensibles a la intensidad general de la luz, pero se presentan en tres variedades que son sensibles a diferentes rangos de frecuencia y, por lo tanto, se utilizan en la percepción del color y en la visión fotópica. Las células cónicas están altamente concentradas en la fóvea y tienen una agudeza visual alta, lo que significa que son mejores para la resolución espacial que las células bastón. Dado que los conos no son tan sensibles a la luz tenue como los bastones, la mayor parte de la visión nocturna se limita a los bastones. Del mismo modo, como los conos se encuentran en la fóvea, la visión central (incluida la visión necesaria para realizar la mayoría de las las tareas de detalle fino, como la lecturas, la costura o el examen cuidadoso de los objetos) se realiza mediante los células conos.

Los músculos ciliares alrededor del cristalino permiten ajustar el enfoque del ojo. Este proceso se conoce como acomodación. La presbicia y el punto remoto definen las distancias más cercana y más lejana al ojo en las que un objeto puede enfocarse con nitidez. Para una persona con visión normal, el punto lejano se encuentra en el infinito. La ubicación del punto cercano depende de cuánto pueden aumentar los músculos la curvatura del cristalino y de su pérdida de flexibidad con la edad. Optometristas, oftalmólogoss y ópticos generalmente consideran que un punto cercano apropiado está más cerca que la distancia de lectura normal, aproximadamente 25 cm.

Los defectos en la visión pueden explicarse utilizando principios ópticos. A medida que las personas envejecen, el cristalino se vuelve menos flexible y el punto cercano se aleja del ojo, una situación conocida como presbicia. Del mismo modo, las personas que sufren de hipermetropía no pueden disminuir la distancia focal de su cristalino lo suficiente como para permitir que los objetos cercanos se vean en su retina. Por el contrario, la miopía se produce cuando el punto lejano está considerablemente más cercano que el infinito. Un problema conocido como astigmatismo se produce cuando la córnea no es esférica, sino que es más curva en una determinada dirección. Esto hace que los objetos extendidos horizontalmente se enfoquen en la retina de diferente forma que los objetos extendidos verticalmente, y da como resultado imágenes distorsionadas.

Todas estas deficiecias funcionales se pueden corregir con lentes correctivas. Para la presbicia y la hipermetropía, una lente acerca el punto cercano al ojo, mientras que para la miopía, envia el punto lejano al infinito. El astigmatismo se corrige con una lente de superficie cilíndrica que se curva más fuertemente en una dirección que en otra, lo que compensa la falta de uniformidad de la córnea.

La potencia óptica de las lentes correctoras se mide en dioptrías, un valor igual al inverso de la distancia focal medida en metros. Una distancia focal positiva corresponde a una lente convergente y una distancia focal negativa correspondiente a una divergente. Para las lentes que también corrigen el astigmatismo, se dan tres números: uno para la potencia esférica, otro para la potencia cilíndrica y el tercero para el ángulo de orientación del astigmatismo.

Las ilusiones ópticas (también llamadas ilusiones visuales) se caracterizan por ser imágenes visualmente percibidas que difieren de la realidad objetiva. La información recopilada por los ojos se procesa en el cerebro para dar una percepción que difiere del objeto que se está observando. Las ilusiones ópticas pueden ser el resultado de variados fenómenos, que incluyen los efectos físicos que crean imágenes que son diferentes de los objetos que los producen, los efectos fisiológicos en los ojos y en el cerebro de una estimulación excesiva (por ejemplo, brillo, inclinación, color o movimiento) y las ilusiones cognitivas en las que el ojo y el cerebro producen inferencias subconscientes.

Las ilusiones cognitivas incluyen algunas que resultan de la mala aplicación inconsciente de ciertos principios ópticos. Por ejemplo, efectos como la habitación de Ames, la ilusión de Hering, las de Müller-Lyer, Orbison, Ponzo, Sander y de Wundt, se basan en crear la sensación de distancia mediante el uso de líneas convergentes y divergentes, de la misma manera que los rayos de luz paralelos (o de hecho, cualquier conjunto de líneas paralelas) parecen converger en un punto de fuga situado en el horizonte cuando se representa una perspectiva en dos dimensiones. Esta sugestión es también la responsable de la famosa "ilusión lunar", en la que la luna, a pesar de tener esencialmente el mismo diámetro angular, parece mucho más grande cerca del horizonte que en el cenit. Esta ilusión confundió a Ptolomeo, que incorrectamente la atribuyó a la refracción atmosférica cuando la describió en su tratado de óptica.

Otro tipo de ilusión óptica explota patrones descompuestos para engañar a la mente, de forma que perciba simetrías o asimetrías que no están realmente presentes. Los ejemplos incluyen las ilusiones de la pared de la cafetería, de Ehrenstein, de la espiral de Fraser, de Poggendorff y la ilusión de Zöllner. Relacionados, pero no siendo estrictamente ilusiones, están los patrones producidos por la superposición de estructuras periódicas. Por ejemplo, los tejidos transparentes con una estructura de cuadrícula producen formas conocidas como patrón de Moiré, mientras que la superposición de patrones transparentes periódicos que comprenden líneas o curvas opacas paralelas produce patrones líneales de Moiré.

Las lentes simples tienen una gran variedad de aplicaciones que incluyen objetivos fotográficos, lentes correctivas y lupas, mientras que los espejos simples se usan en reflectores parabólicos y espejos retrovisores. La combinación de varios espejos, prismas y lentes produce instrumentos ópticos compuestos que tienen diversos usos prácticos. Por ejemplo, un periscopio está formado simplemente por dos espejos planos alineados para permitir ver evitando un obstáculo. Los instrumentos ópticos compuestos más famosos de la ciencia son el microscopio y el telescopio, que fueron ideados por los holandeses a finales del siglo XVI.

Los microscopios se desarrollaron primero con solo dos lentes: un objetivo y un ocular. La lente del objetivo es esencialmente una lupa, y se diseñó con una distancia focal muy pequeña, mientras que el ocular generalmente tiene una distancia focal más larga. Esto tiene el efecto de producir imágenes ampliadas de objetos cercanos. En general, se utiliza una fuente de iluminación adicional, ya que las imágenes ampliadas son más débiles debido al principio de conservación de la energía y a la dispersión de los rayos de luz sobre un área de superficie más grande. Los microscopios modernos, conocidos como "microscopios compuestos" tienen muchas lentes (generalmente cuatro) para optimizar su funcionalidad y mejorar la estabilidad de la imagen. Una variedad ligeramente diferente de microscopio, el microscopio estereoscópico, permite obtener dos imágenes de las muestras examinadas, que se perciben en tres dimensiones gracias al uso de un sistema binocular.

Los primeros telescopios, los denominados "telescopios refractores" también se desarrollaron con un solo objetivo y una lente ocular. En contraste con el microscopio, la lente del objetivo del telescopio se diseñó con una gran distancia focal para evitar aberraciones ópticas. El objetivo enfoca una imagen de un objeto distante en su punto focal, que se ajusta para localizarse a su vez en el punto focal de un ocular con una distancia focal mucho más pequeña. El objetivo principal de un telescopio no es necesariamente la ampliación, sino más bien la recolección de luz, que viene determinada por el tamaño físico de la lente del objetivo. Por lo tanto, los telescopios se denominan normalmente por los diámetros de sus objetivos más que por la ampliación que se puede obtener cambiando los oculares. Debido a que la ampliación de un telescopio es igual a la distancia focal del objetivo dividida por la distancia focal del ocular, los oculares de longitud focal más pequeña producen una mayor ampliación.

Como fabricar lentes grandes es mucho más difícil que crear grandes espejos, la mayoría de los telescopios modernos son "telescopios reflectores", es decir, telescopios que usan un espejo primario en lugar de un objetivo. Las mismas consideraciones ópticas generales que se aplican a los telescopios reflectores, se aplican a los telescopios de refracción, a saber, que cuanto mayor es el espejo primario, más luz se recoge, y la ampliación es igual a la distancia focal del espejo primario dividido por la distancia focal del ocular. Los telescopios profesionales generalmente no tienen oculares y en su lugar se coloca un sistema de captación de imágenes electrónico (a menudo un dispositivo de carga acoplada) en el punto focal.

La óptica de la fotografía involucra tanto el uso de lentes como el medio en el que se registra la radiación electromagnética, ya sea una placa, una película o un dispositivo de carga acoplada. Los fotógrafos deben considerar la relación de reciprocidad entre la cámara y la toma, que se resume mediante la igualdad

En otras palabras, cuanto menor sea la abertura (proporcionando una mayor profundidad de enfoque), menor será la cantidad de luz que entrará, por lo que deberá aumentarse el tiempo de exposición (lo que puede provocar una imagen borrosa si se produce movimiento). Un ejemplo del uso de la ley de reciprocidad es la regla 16/f, que proporciona una referencia aproximada de la configuración necesaria para estimar la exposición adecuada durante el día.

La apertura de una cámara se mide con un número sin unidades denominado "f", f/#, a menudo anotado como formula_27, y dado por
donde formula_12 es la distancia focal y formula_30 es el diámetro del orificio del diafragma de entrada. Por convención, "f/#" se trata como un símbolo único, y los valores específicos de f/# se escriben reemplazando la almohadilla por un valor numérico. Las dos formas de aumentar el límite de la focal son disminuir el diámetro del obturador o cambiar a una distancia focal más larga (en el caso de un zum, esto se puede hacer simplemente ajustando la lente). Los números f más altos también tienen una profundidad de campo más grande debido a que el objetivo se acerca al límite de una cámara estenopeica que puede enfocar todas las imágenes perfectamente, independientemente de la distancia, pero requiere tiempos de exposición muy largos.

El campo de visión que proporcionará la lente cambia con la distancia focal de la lente. Hay tres clasificaciones básicas basadas en la relación con el tamaño diagonal de la película o el tamaño del sensor de la cámara con respecto a la distancia focal de la lente:


Los zum modernos pueden tener algunos o todos estos atributos.

El valor absoluto para el tiempo de exposición requerido depende de la sensibilidad lumínica del medio utilizado (medida según una escala de sensibilidad fotográfica o, para medios digitales, por su eficiencia cuántica). Las primeras fotografías usaban medios que tenían muy poca sensibilidad a la luz, y por lo tanto los tiempos de exposición tenían que ser largos, incluso para tomas muy brillantes. A medida que la tecnología ha mejorado, también lo ha hecho la sensibilidad gracias al desarrollo de películas cada vez más versátiles y de cámaras digitales con mejores prestaciones.

Otros resultados de la óptica física y geométrica se aplican a la óptica de la cámara. Por ejemplo, la capacidad de resolución máxima de una configuración particular de la cámara está determinada por el límite de difracción asociado con el tamaño del obturador y, aproximadamente, por el criterio de Rayleigh.

Las propiedades ópticas únicas de la atmósfera causan una amplia gama de fenómenos ópticos espectaculares. El color azul del cielo es un resultado directo de la dispersión de Rayleigh que redirige la luz solar de mayor frecuencia (azul) al campo de visión del observador. Debido a que la luz azul se dispersa más fácilmente que la luz roja, el sol adquiere un tono rojizo cuando se observa a través de una atmósfera espesa, como durante un orto u ocaso. Las partículas suspendidas en el cielo puede dispersar diferentes colores en diferentes ángulos creando coloridos cielos brillantes al anochecer y al amanecer. La dispersión de cristales de hielo y otras partículas en la atmósfera es responsable de los halos, arreboles, coronas, rayos crepusculares y parhelios. La variación en este tipo de fenómenos se debe a los diferentes tamaños y geometrías de las partículas.

Los espejismos son fenómenos ópticos en los que los rayos de luz se curvan debido a variaciones térmicas que modifican el índice de refracción del aire, produciendo imágenes desplazadas o muy distorsionadas de objetos distantes. Otros espectaculares fenómenos ópticos asociados con este efecto incluyen el efecto Nueva Zembla donde el sol parece elevarse antes de lo previsto con una forma distorsionada. Otra forma llamativa de refracción que se produce en condiciones de inversión térmica es el fenómeno llamado Fata Morgana, en el que los objetos en el horizonte o incluso más allá del horizonte, como islas, acantilados, barcos o icebergs, aparecen alargados y elevados, como "castillos de cuento de hadas".

Los arcoíris son el resultado de una combinación de reflexión interna y refracción dispersiva de la luz en las gotas de lluvia. Una sola reflexión en la parte posterior de una serie de gotas de lluvia produce un arcoíris con un tamaño angular en el cielo que varía de 40° a 42°, con el color rojo en el exterior. Los dos tipos de arcoíris dobles son producidos por dos reflejos internos con un tamaño angular de 50.5° a 54°, con el color violeta en el exterior. Debido a que los arcoíris se ven con el sol a 180° del centro del arcoíris, son más prominentes cuanto más cerca está el sol del horizonte.




</doc>
<doc id="3050" url="https://es.wikipedia.org/wiki?curid=3050" title="25 de enero">
25 de enero

El 25 de enero es el 25.º (vigesimoquinto) día del año del calendario gregoriano. Quedan 340 días para finalizar el año y 341 en los años bisiestos.








</doc>
<doc id="3053" url="https://es.wikipedia.org/wiki?curid=3053" title="7 de febrero">
7 de febrero

El 7 de febrero es el trigésimo octavo día del año en el calendario gregoriano. Quedan 327 días para finalizar el año y 328 en los años bisiestos.























</doc>
<doc id="3054" url="https://es.wikipedia.org/wiki?curid=3054" title="8 de febrero">
8 de febrero

El 8 de febrero es el 39.º (trigésimo noveno) día del año en el calendario gregoriano. Quedan 326 días para finalizar el año y 327 en los años bisiestos.








</doc>
<doc id="3055" url="https://es.wikipedia.org/wiki?curid=3055" title="Álgebra de Boole">
Álgebra de Boole

Álgebra de Boole también llamada álgebra booleana, en informática y matemática es una estructura algebraica que esquematiza las operaciones lógicas.

Se denomina así en honor a George Boole (2 de noviembre de 1815 a 8 de diciembre de 1864), matemático inglés autodidacta, que fue el primero en definirla como parte de un sistema lógico, inicialmente en un pequeño folleto, "The Mathematical Analysis of Logic", publicado en 1847, en respuesta a una controversia en curso entre Augustus De Morgan y "sir" William Rowan Hamilton. El álgebra de Boole fue un intento de utilizar las técnicas algebraicas para tratar expresiones de la lógica proposicional. Más tarde fue extendido como un libro más importante: "An Investigation of the Laws of Thought on Which are Founded the Mathematical Theories of Logic and Probabilities" (también conocido como "An Investigation of the Laws of Thought" o simplemente "The Laws of Thought"), publicado en 1854.

En la actualidad, el álgebra de Boole se aplica de forma generalizada en el ámbito del diseño electrónico. Claude Shannon fue el primero en aplicarla en el diseño de circuitos de conmutación eléctrica biestables, en 1948.
Esta lógica se puede aplicar a dos campos:


Dado un conjunto formula_1 en el que se han definido dos leyes de composición interna formula_2. La estructura formula_3 es un álgebra de Boole si y solo si formula_3 es un Retículo distributivo, esto es:



Basándose en esta definición se determina lo siguiente.

Dado un conjunto: formula_1 formado cuando menos por los elementos: formula_12 en el que se ha definido:


En esta operación definimos una aplicación que, a cada elemento a de B, le asigna un b de B.

Para todo elemento a en B, se cumple que existe un único b en B, tal que b es el complemento de a.


por la que definimos una aplicación que, a cada par ordenado (a, b) de B por B, le asigna un c de B.

Para todo par ordenado (a, b) en B por B, se cumple que existe un único c en B, tal que c es el resultado de sumar a con b.


Con lo que definimos una aplicación que, a cada par ordenado (a, b) de B por B, le asigna un c de B.

Para todo par ordenado (a, b) en B por B, se cumple que existe un único c en B, tal que c es el resultado del producto a y b.

Dada la definición del álgebra de Boole como una estructura algebraica genérica, según el caso concreto de que se trate, la simbología y los nombres de las operaciones pueden variar.

Diremos que este conjunto y las operaciones así definidas: formula_19 son un álgebra de boole, si cumple las siguientes axiomas:











Partiendo de los cinco axiomas anteriores, se pueden deducir y demostrar los siguientes teoremas fundamentales:










Sea: formula_19 un álgebra de Boole, sean a, b dos elementos del conjunto, podremos decir entonces que a antecede a b y lo denotamos:

si se cumple alguna de las siguientes condiciones:

Estas cuatro condiciones se consideran equivalentes y el cumplimiento de una de ellas implica necesariamente el cumplimiento de las demás. Definiendo un conjunto parcialmente ordenado.

Si se cumple que:

Para los valores a, b de formula_1, que cumplen que a antecede a b, o que b antecede a a, se dice que a y b son "comparables".

Si se cumple que:

Para los valores a, b de formula_1, que cumplen que a no antecede a b, y que b no antecede a a, se dice que a y b son "no comparables".

El concepto de dualidad permite formalizar este hecho: a toda relación o ley lógica le corresponderá su dual, formada mediante el intercambio de los operadores suma con los de producto, y de los formula_51 con los formula_52.

En Lógica binaria se suele emplear la notación formula_53, común en la tecnología digital, siendo la forma más usual y la más cómoda de representar.

Por ejemplo las leyes de De Morgan se representan así:

Empleando esta notación las leyes de De Morgan se representan:

Con la notación lógica las leyes de De Morgan serían así:

En el formato de Teoría de conjuntos el Álgebra de Boole toma el aspecto: formula_61

En esta notación las leyes de De Morgan serían así:

Otra forma en la álgebra de conjuntos del Álgebra de Boole, las leyes de De Morgan serían así:

Desde el punto de vista práctico existe una forma simplificada de representar expresiones booleanas. Se emplean apóstrofos (') para indicar la negación, la operación suma (+) se representa de la forma normal en álgebra, y para el producto no se emplea ningún signo, las variables se representan, normalmente con una letra mayúscula, la sucesión de dos variables indica el producto entre ellas, no una variable nombrada con dos letras.

La representación de las leyes de De Morgan con este sistema quedaría así, con letra minúsculas para las variables:

y así, empleando letras mayúsculas para representar las variables:

Todas estas formas de representación son correctas, se utilizan de hecho, y pueden verse al consultar bibliografía. La utilización de una u otra notación no modifica el álgebra de Boole, solo su aspecto, y depende de la rama de las matemáticas o la tecnología en la que se esté utilizando para emplear una u otra notación.

Hay numerosos casos de distintos análisis de estructuras algebraicas que corresponden al álgebra de Boole, aunque en apariencia son muy diferentes, su estructura es la misma. Vamos a ver algunos de ellos, con el propósito de hacer palpable las similitudes en la estructura y los distintos ámbitos de aplicación y distinta terminología para referirse a las operaciones o a las variables.

Una serie de temas, aparentemente tan distintos, tiene dos cosas en común, la lógica binaria basada en los ceros y los unos y el álgebra de Boole, posiblemente la forma más conocida de esta álgebra, que en ocasiones da lugar a la interpretación que el álgebra de Boole es la lógica binaria exclusivamente, así el conjunto formula_1 en este caso está formado por dos elementos {0,1}, o {F, V}, o {no, sí}, dos valores contrapuestos, que son las dos posibles alternativas entre dos situaciones posibles, aquí, sin pérdida de la generalidad, tomaremos el conjunto: {0,1} como ya hemos dicho:

Donde:


La operación unaria interna negación, definimos una aplicación que a cada elemento a de {0,1}, le asigna un b de {0,1}.

Para todo elemento a en {0.1}, se cumple que existe un único b en {0,1}, tal que b es la negación de a. Como se ve en la tabla.


Con la operación suma definimos una aplicación que, a cada par ordenado (a, b) de B por B, le asigna un c de B.

Para todo par ordenado (a,b) en B por B, se cumple que existe un único c en B, tal que c es el resultado de sumar a con b.


Con la operación producto definimos una aplicación que, a cada par ordenado (a, b) de B por B, le asigna un c de B.

Para todo par ordenado (a, b) en B por B, se cumple que existe un único c en B, tal que c es el resultado del producto a y b. Como se puede ver en la tabla.

Así formula_80 es un álgebra de Boole al cumplir los siguientes axiomas:











Luego formula_80 es álgebra de Boole.

Partiendo de estos axiomas se puede demostrar los siguientes teoremas:










Partiendo de formula_103 álgebra de Boole, dadas dos variables binarias: a, b, que cumplen alguna de estas condiciones:

entonces a es menor o igual que b. Dados los valores binarios 0 y 1, podemos ver:

Estas cuatro condiciones son equivalentes y el cumplimiento de una de ellas supone el cumplimiento de las otras, en este caso es sencillo comprobarlas todas. Luego podemos decir que 0 antecede a 1 y lo denotamos:

Si además sabemos que 0 y 1 son valores distintos:

El valor binario 0 es menor que el valor binario 1.

Dado cualquier conjunto U, se llama conjunto potencia de U, al conjunto de todos los subconjuntos posibles de U y lo denotamos formula_111.

A título de ejemplo podemos considerar:

Que tiene como conjunto potencia:

Donde podemos definir:

Y como es obvio:


En esta operación definimos una aplicación que, a cada elemento A de P(U), le asigna un B de P(U).

Para todo elemento A en P(U), se cumple que existe un único B en P(U), tal que B es el complemento A.

Definiendo el complemento de un conjunto así:

B es el complemento de A, si se cumple que para todo x que pertenezca a B, x pertenece a U y x no pertenece a A.


Con esta operación binaria interna definimos una aplicación que, a cada par ordenado (A, B) de P(U) por P(U), le asigna un C de P(U).

Para todo par ordenado (A,B) en P(U) por P(U), se cumple que existe un único C en P(U), tal que C es la unión A y B.

Definiendo la unión de dos conjuntos como:
El conjunto C es la unión de A y B, si para todo elemento x de C, se cumple que x es elemento de A o de B


Con lo que definimos una aplicación que, a cada par ordenado (A, B) de P(U) por P(U), le asigna un C de P(U).

Para todo par ordenado (A,B) en P(U) por P(U), se cumple que existe un único C en P(U), tal que C es la intersección A y B.

Definiendo la intersección de dos conjuntos como:

El conjunto C es la intersección de A y B, si para todo elemento x de C, se cumple que x es elemento de A y de B.

Con lo que podemos plantear: formula_126, para un U conocido, como álgebra de Boole si cumple las siguientes axiomas:











Concluyendo que formula_137 es un álgebra de boole.

Partiendo de estos axiomas se puede demostrar los siguientes teoremas:







 \forall A \in \mathcal{P}(U)


</doc>
<doc id="3057" url="https://es.wikipedia.org/wiki?curid=3057" title="9 de febrero">
9 de febrero

El 9 de febrero es el 40.º (cuadragésimo) día del año en el calendario gregoriano. Quedan 325 días para finalizar el año y 326 en los años bisiestos.








</doc>
<doc id="3059" url="https://es.wikipedia.org/wiki?curid=3059" title="1990">
1990

1990 (MCMXC) fue un año común comenzado en lunes según el calendario gregoriano, y fue designado como:

















 













En el Mundial disputado en Argentina, Yugoeslavia vence a la Unión Soviética en la final, iniciando la despedida de estas dos potencias europeas, de las competencias internacionales, puesto que ambos países se desintegrarían a la brevedad.

Campeonato Del Mundo de Velocidad:
125 c.c.: Loris Capirossi, ITA, Honda RS 125
250 c.c.: John Kocinski, USA. Yamaha YZR 250
500 c.c.: Waine Rainey, USA, Yamaha YZR 500











</doc>
<doc id="3062" url="https://es.wikipedia.org/wiki?curid=3062" title="Órgano">
Órgano

Órgano hace referencia a varios artículos:








</doc>
<doc id="3064" url="https://es.wikipedia.org/wiki?curid=3064" title="Ética">
Ética

<section begin=intro />
La ética (del "ethĭcus," y este del ἠθικός "ēthikós;" la forma , del tardío "ethĭca," y este del ἠθική "ēthikḗ") es la rama de la filosofía que estudia lo correcto o equivocado del comportamiento humano, la moral, la virtud, el deber, la felicidad y el buen vivir. Además, tiene como centro de atención las acciones humanas y aquellos aspectos de las mismas que se relacionan con el bien, la virtud, el deber, la felicidad y la vida realizada. El estudio de la ética se remonta a los orígenes mismos de la filosofía en la Antigua Grecia, y su desarrollo histórico ha sido amplio y variado. 

La ética estudia qué es un acto moral, cómo se justifica racionalmente un sistema moral, y cómo se ha de aplicar posteriormente a nivel individual y a nivel social. En la vida cotidiana constituye una reflexión sobre el hecho moral, es decir busca las razones que justifican la adopción de un sistema moral u otro.

Una doctrina ética elabora y verifica afirmaciones o juicios determinados. Una sentencia ética, juicio moral o declaración normativa es una afirmación que contendrá términos tales como «bueno», «malo», «correcto», «incorrecto», «obligatorio», «permitido», etc., referidos a una acción, a una decisión o incluso contendrá a las intenciones de quien actúa o decide algo. Cuando se emplean sentencias éticas se está valorando moralmente a personas, situaciones, o acciones. Se establecen juicios morales cuando, por ejemplo, se dice: «ese hombre es malo», «no se debe matar», etc. En estas declaraciones aparecen los términos «malo», «no se debe», etc., que implican valoraciones de tipo moral.<section end=intro />

La palabra ética proviene del latín "ethĭcus", y este del griego antiguo ἠθικός transliterado como "ēthikós". Según algunos autores, es correcto diferenciar "êthos", que significa «carácter», de "ethos", que significa «costumbre», pues «ética» se sigue de aquel sentido y no es éste.

Según una corriente «clásica», la ética tiene como objeto los actos que el ser humano realiza de modo consciente y libre (es decir, aquellos actos sobre los que ejerce de algún modo un control racional). No se limita solo a ver cómo se realizan esos actos, sino que busca emitir un juicio sobre estos, que permite determinar si un acto ha sido éticamente bueno o malo.

Fernando Savater, en el primer capítulo de su libro "Ética para Amador" («De qué va la ética»), define la ética como «el arte de vivir, el saber vivir, por lo tanto el arte de discernir lo que nos conviene (lo bueno) y lo que no nos conviene (lo malo)».

Ello implica establecer una distinción entre lo que sea bueno y lo que sea malo desde el punto de vista ético, y si el bien y el mal éticos coinciden o no con lo que serían el bien y el mal en sí.

La ética se relaciona con la antropología, el derecho, con la Ley, y con ciencias empíricas que estudian el comportamiento humano, como la sociología y la psicología.

Un autor define ética del siguiente modo: «Ética (del griego "ethika", de "ethos", «comportamiento», «costumbre»), principios o pautas de la conducta humana, a menudo y de forma impropia llamada moral (del latín "mores", «costumbre»)».

La palabra ética proviene del griego "êthikos" («carácter»). Se trata del estudio de la moral y del accionar humano para promover los comportamientos deseables. Una sentencia ética supone la elaboración de un juicio moral y una norma que señala cómo deberían actuar los integrantes de una sociedad. Por profesión se entiende una ocupación que se desarrolla con el fin de colaborar con el bienestar de una sociedad. Para realizar dicha labor es necesario que el profesional (persona que ejerce la misma) actúe con responsabilidad, siguiendo los requisitos que la ley vigente plantee para el desarrollo de esa actividad.

La ética profesional pretende regular las actividades que se realizan en el marco de una profesión. En este sentido, se trata de una disciplina que está incluida dentro de la ética aplicada ya que hace referencia a una parte específica de la realidad.

Cabe destacar que la ética, a nivel general, no es coactiva (no impone sanciones legales o normativas). Sin embargo, la ética profesional puede estar, en cierta forma, en los códigos deontológicos que regulan una actividad profesional. La deontología forma parte de lo que se conoce como ética normativa y presenta una serie de principios y reglas de cumplimiento obligatorio.

Se diferencia en que la ética es el estudio filosófico y científico de la moral y es teórica mientras que la moral es práctica. La ética trata sobre la razón y depende de la filosofía y en cambio la moral es el comportamiento en el que consiste nuestra vida. Etimológicamente «ética» y «moral» tienen el mismo significado. La palabra «moral» viene de latín "mos" que significa hábito o costumbre; y «ética» del griego "ethos" que significa lo mismo.

Dentro de la ética contemporánea se suelen distinguir tres áreas o niveles: la metaética, la ética normativa y la ética aplicada.

La metaética estudia el origen y el significado de los conceptos éticos, así como las cuestiones metafísicas acerca de la moralidad, en particular si los valores morales existen independientemente de los humanos, y si son relativos, convencionales o absolutos. Algunos problemas de la metaética son el problema del ser y el deber ser, el problema de la suerte moral, y la cuestión acerca de la existencia o no del libre albedrío.

La ética teleológica es generalmente una ética orientada hacia fines, hacia un "télos" (en griego, «fin u objetivo que perfecciona a quien lo alcanza»).

Hume ha planteado la objeción de que la transición de «lo que es» a «lo que debe ser» es problemática, y en general ilegítima. La falacia naturalista de George Edward Moore plantea cuestiones estrechamente relacionadas, pero no es estrictamente lo mismo.

Como se destaca más adelante, los positivistas, deben estar epistemológicamente entre las tasas y tarifas de destino, y se diferencian por su relación diferente a los sentidos. La distinción epistemológica entre es y debe se basa en la ciencia empírica moderna. Quien no acepta esta distinción, o bien debe postular a un ser que no es directamente o indirectamente detectable, o se debe considerar lo que debe ser perceptible.

Las normas éticas se derivan de supuestas declaraciones sobre los seres, con frecuencia pasan inadvertidos por el uso de la ambigüedad normativa y empírica de términos como «esencia», «naturaleza», «determinación», «función», «final», «sentido» u «objetivo alcanzado». Así, la palabra «objetivo» es incluso lo que en realidad busca una persona (Su meta es graduarse). La palabra objetivo puede referirse también a lo que debe perseguir un hombre (por ejemplo, cuando se dice de alguien que perdió el objetivo o la meta de su existencia humana).

La ambigüedad inadvertida empírico-normativa de ciertos términos conduce a falacias lógicas tales como: «La esencia de la sexualidad es la procreación. Por lo tanto, la anticoncepción no está permitida, porque no refleja la naturaleza de la sexualidad».

Tomando nota de que esto supone que se ha deducido lógicamente que el ser es una justificación de las normas aún no declaradas (Promulgadas). Porque, además de las declaraciones normativas y de los registros, hay voluntades. La expresión de la voluntad de una persona: «No quiero ser molestado en la siguiente hora por cualquier persona» incluye el deseo de que la norma «Nadie me debe molestar en la hora siguiente» sea capaz de ser acogida y respetada por otros. El punto central será saber si la expresión de la voluntad de esa persona sea o no sea éticamente correcta, o sea simplemente un acto arbitrario que no exige un respeto absoluto por parte de otros.

George Edward Moore, en su obra "Principia Ethica", acusa al naturalismo de cometer un error cuando infiere que algo tiene una propiedad moral a partir de que ese algo tiene tal o cual propiedad natural.
Por ejemplo, asumiendo que el placer es una propiedad natural, un naturalista podría sostener que las relaciones sexuales son buenas porque son placenteras.
Sin embargo, Moore señala que para afirmar esto, primero se necesita mostrar que todo lo placentero es bueno, y esto requiere de un argumento que parece difícil de proveer. Pese al nombre de la falacia, la misma parece poder extenderse más allá del naturalismo.Así, el desafío propuesto por Moore parece mostrar cómo es posible concluir legítimamente que una propiedad no moral puede identificarse o tener la misma extensión que una propiedad moral.

La ética normativa estudia los posibles criterios morales para determinar cuándo una acción es correcta y cuándo no lo es. Un ejemplo clásico de un criterio semejante es la regla de oro. Dentro de la ética normativa, existen tres posturas principales:


Las teorías de la filosofía ética o moral se pueden distinguir de acuerdo a los criterios de sus bases para la determinación del bien moral. El bien moral puede ser determinado por:


Las éticas teleológicas (Del gr. τέλος, fin) es un grupo de teorías éticas que emana deberes u obligaciones morales que buscan lograr un fin último, que presume bueno o deseable. También se le conoce como ética consecutiva, ya que se basa el juicio de los actos en sus consecuencias, y se opone a la éticas deontológicas (del griego δέον, deber), que sostienen que la moralidad de una acción es independiente del bien o mal generado a partir de ella. 

El consecuencialismo sostiene que la moralidad de una acción depende sólo de sus consecuencias (el fin justifica los medios).
El consecuencialismo no se aplica sólo a las acciones, pero éstas son el ejemplo más prominente.
Creer que la moralidad se trata sólo de generar la mayor cantidad de felicidad posible, o de aumentar la libertad lo más posible, o de promover la supervivencia de nuestra especie, es sostener una postura consecuencialista, porque aunque todas estas creencias difieren en cuanto a las consecuencias que importan, están de acuerdo en que lo que importa son las consecuencias.

Una manera de clasificar a los distintos tipos de consecuencialismos es a partir de los "agentes" que se deben tener en cuenta cuando se consideran las consecuencias de las acciones. Esto da lugar a tres tipos de consecuencialismo:


La deontología es la teoría normativa según la cual existen ciertas acciones que deben ser realizadas, y otras que no deben ser realizadas, más allá de las consecuencias positivas o negativas que puedan traer.
Es decir, hay ciertos deberes, u obligaciones, que deben ser cumplidos más allá de sus consecuencias.

La ética de virtud es una teoría que se remonta a Platón y, de modo más articulado, a Aristóteles, según la cual una acción es éticamente correcta si hacerla fuera propio de una persona virtuosa.
Por ejemplo, si para el utilitarismo hay que ayudar a los necesitados porque eso aumenta el bienestar general, y para la deontología hay que hacerlo porque es nuestro deber, para la ética de virtudes, hay que ayudar a los necesitados porque hacerlo sería caritativo y benevolente.

La ética aplicada estudia la aplicación de las teorías éticas a asuntos morales concretos y controversiales. Algunas de estas cuestiones son estudiadas por subdisciplinas. Por ejemplo, la bioética se ocupa de las cuestiones relacionadas con el avance de la biología y la medicina, como el aborto inducido, la eutanasia y la donación de órganos. La ética ambiental, por otra parte, estudia cuestiones como los derechos de los animales, la experimentación con animales y el control de la contaminación. Otras cuestiones estudiadas por la ética aplicada son la pena de muerte, la guerra nuclear, la homosexualidad, el racismo y el uso recreativo de drogas.

La ética aplicada es la parte de la ética que se ocupa de estudiar cuestiones morales concretas y controvertidas.
Por ejemplo, algunos objetos de estudio de la ética aplicada son el aborto inducido, la eutanasia y los derechos de los animales.
Algunas de estas cuestiones se agrupan por similitudes y son estudiadas por subdisciplinas:

En el primer sentido la deontología profesional es una disciplina normativa y filosófica. En el segundo sentido, se trata más bien de una disciplina descriptiva y por lo tanto científica.
La deontología profesional también cuenta con subdisciplinas como la ética médica, la ética de negocios y la ética de la ingeniería.
Quizás las dos preguntas fundamentales de esta disciplina sean: ¿qué deberes tienen los seres humanos hacia el medio ambiente, y por qué?En general, la respuesta a la primera pregunta es una consecuencia de la respuesta a la segunda.Distintas respuestas o aproximaciones a respuestas han dado lugar a distintas éticas ambientales.

Desde el inicio de la reflexión filosófica ha estado presente la consideración sobre la ética. Platón afronta la temática ética en diversos lugares y desde contextos diferentes. Así, por ejemplo, en el "Gorgias" busca superar el hedonismo y la ley del más fuerte. En el "Fedón" evidencia la importancia de lo que exista tras la muerte para regular el propio comportamiento. En "La República" aborda juntamente la ética individual (desde la perspectiva de una justicia dentro del alma) y la ética pública, con una compleja teoría del Estado, que encuentra complementos y puntos de vista diferentes en otras dos obras, el "Político" y las "Leyes". En la segunda mitad de la obra "Fedro", uno de los temas principales es la ética.

La "Ética nicomáquea", seguramente el más importante tratado de ética de Aristóteles, se basa en la premisa de que todo ser humano busca la felicidad (ética eudemónica). Para Aristóteles todos los seres naturales tienden a cumplir la función que les es propia y están orientados a realizar completamente sus potencialidades. El bien, que es lo mismo que la perfección de un ser o la realización de las capacidades es cumplir su función propia, aquello a que solo él puede realizar. También los seres humanos están orientados a la realización plena de la función que les es propia. El problema que se suscita, entonces, es cuál es la función propia del hombre. Y si acaso hay más de un bien propio del hombre, ¿cuál es el bien más alto y más perfecto de los que puede alcanzar el ser humano?

Como en otras de sus obras, Aristóteles releva las opiniones de sus contemporáneos al respecto y comprueba que todas parecen estar de acuerdo en que el objetivo supremo del hombre es vivir bien y ser feliz, aunque hay muchos desacuerdos respecto de en qué consiste la felicidad y el buen vivir. Para Aristóteles la vida feliz (plena) es la que permite realizar la actividad superior (contemplación), con una suficiente autonomía (bienes materiales, salud), y en compañía de un número suficiente de amigos (cf. "Ética nicomáquea" I).

Solo son morales las acciones en las que se puede elegir y decidir qué hacer. En cambio, no son morales ni inmorales las acciones padecidas, compulsivas o forzosas. Lo que es moral es la acción que depende de la voluntad, si se actúa de modo correcto. ¿Cuándo se actúa correctamente? La forma correcta de actuar depende del ámbito de acción (dianoético o intelectual, ético o moral) y en parte está pautada por las costumbres de la comunidad a la que se pertenece (si la comunidad es éticamente sana, algo que supone Aristóteles para el mundo griego quizá de modo acrítico) y se aprende con la educación. Cuando se actúa de acuerdo con estas pautas, se vive bien y se es virtuoso.

Por otra parte, los filósofos estoicos y epicúreos propusieron teorías morales basadas en principios opuestos: la virtud y la vida con moderación (estoicismo), y la búsqueda del placer (epicureísmo).

Es un momento en el que la ética asume elementos de las doctrinas clásicas de la felicidad (el fin del actuar humano consiste en obtener el bien que nos hace felices) y los une a la doctrina cristiana (vista como Revelación divina), especialmente según la normativa que recogen los mandamientos. El fin último del actuar humano es la caridad, que se consigue al vivir desde el Evangelio, y que permite al hombre acceder a la visión de Dios (en el cielo), donde el ser humano alcanza su máxima plenitud y el bien supremo.

Diversos autores hablan de ética y según perspectivas diferentes. Es oportuno recordar dos grandes nombres, san Agustín de Hipona y santo Tomás de Aquino (especialmente en la segunda parte de la "Suma de teología", en la que se recogen numerosos elementos de la ética de Aristóteles).

Posteriormente, y tras las huellas de las ideas de Tomás de Aquino, se desarrolla en el ámbito católico lo que luego será conocido como principio de doble efecto.

Los filósofos éticos modernos trabajan con la mirada puesta, sobre todo, en el mundo antiguo (estoicos, epicúreos, Platón, Aristóteles), si bien con algunos elementos heredados de la Escolástica medieval. Descartes tiene algunos elementos de ética en su famoso "Discurso del método". Dentro del racionalismo, es Baruch Spinoza quien elaboró de modo más amplio y sistemático una propuesta ética. En el ámbito del empirismo, David Hume trabajó en diversos momentos para comprender los motivos profundos de las acciones humanas.

La gran revolución ética moderna se realiza a través de Immanuel Kant, que rechaza una fundamentación de la ética en otra cosa que no sea imperativo moral mismo (deontologismo formal), pues si la moral se orienta a buscar la felicidad no podría dar ninguna norma categórica ni universal. Los filósofos idealistas desarrollaron esta moral del imperativo categórico. Hacen frente así al utilitarismo, al afirmar que el principio de utilidad no es el único criterio de corrección de las acciones.

La ética del siglo XX ha conocido aportes muy importantes por parte de numerosos autores: los vitalistas y existencialistas desarrollan el sentido de la opción y de la responsabilidad, Max Scheler elabora una fenomenología de los valores. Autores como Alain Badiou han intentado demostrar que esta principal tendencia (en las opiniones y en las instituciones), la cuestión de «la ética» en el siglo XX, es en realidad un «verdadero nihilismo» y «una amenazante denegación de todo pensamiento».

Recientemente, y desarrollando un análisis en profundidad de los orígenes y fundamentos de la ética, han aparecido diversos estudios sobre el papel de las emociones en el desarrollo de un pensamiento ético antifundacionalista, como ha indicado Richard Rorty. En las últimas dos décadas, el filósofo escocés MacIntyre establece nuevas herramientas de análisis histórico-filosófico de distintas versiones rivales de la ética.





</doc>
<doc id="3065" url="https://es.wikipedia.org/wiki?curid=3065" title="1969">
1969

1969 (MCMLXIX) fue un año normal según el calendario gregoriano.

































El XIV Festival se celebró en la ciudad de Madrid (España) el sábado 29 de marzo, en el Teatro Real.
La presentadora fue Laura Valenzuela y Ramón Díez, el director.
La dirección musical corrió a cargo de Augusto Algueró.
Concurrieron un total de 16 países.


En Estados Unidos se realiza el primer Festival de Woodstock al que asisten bandas de diversas categorías, como Canned Heat, The Who, Creedence Clearwater Revival, Grateful Dead y Jimi Hendrix, entre otras. Asiste una cantidad importante de personas. El Festival de Woodstock se realizó nuevamente en varias ocasiones.




</doc>
<doc id="3066" url="https://es.wikipedia.org/wiki?curid=3066" title="1975">
1975

1975 (MCMLXXV) fue un año normal y fue designado como:






























































</doc>
<doc id="3080" url="https://es.wikipedia.org/wiki?curid=3080" title="1985">
1985

1985 (MCMLXXXV) fue un año normal comenzado en martes en el calendario gregoriano.
Corresponde al año del Buey en el horóscopo chino. La Organización de las Naciones Unidas lo declaró "Año Internacional de la Juventud", en tanto la UNESCO lo declaró "Año Internacional de la Música".













































Campeón: Bolívar de La Paz; 
Subcampeón: Wilstermann de Cochabamba.




























</doc>
<doc id="3082" url="https://es.wikipedia.org/wiki?curid=3082" title="1616">
1616

1616 (MDCXVI) fue un año bisiesto comenzado en viernes.





</doc>
<doc id="3083" url="https://es.wikipedia.org/wiki?curid=3083" title="29 de agosto">
29 de agosto

El 29 de agosto es el 241.º (ducentésimo cuadragésimo primer) día del año en el calendario gregoriano y el 242.º en los años bisiestos. Quedan 124 días para finalizar el año.







krñogvn ldzgjnboknbfdln

</doc>
<doc id="3103" url="https://es.wikipedia.org/wiki?curid=3103" title="Ciencias humanas">
Ciencias humanas

Ciencias humanas es un concepto epistemológico que designa a un extenso grupo de ciencias y disciplinas cuyo objeto es el ser humano en el aspecto de sus manifestaciones inherentemente humanas, esto es el lenguaje verbal en primer término, el arte y el pensamiento y, en general, la cultura y sus formaciones históricas. El término de Ciencias humanas se opone y, por otra parte, complementa al de Ciencias naturales o físico-naturales. El término de Humanidades no es en realidad sino una abreviatura, de preferencia anglosajona, frente al uso más tradicional germánico y románico de Ciencias humanas, directamente establecido sobre la tradición humanística. 

Las modernamente denominadas Ciencias humanas constituyen una entidad fundada en la antigüedad clásica, con posterioridad humanísticamente delimitada, tras el régimen medieval del , mediante la designación secular de Studia humanitatis (es decir, característica y centralmente Gramática, Retórica, Dialéctica, Poética, Poesía o Literatura como disciplina y lectura del canon clásico, Historia, Filosofía, especialmente Ética o Filosofía moral). A finales del siglo XIX y comienzos del XX surgieron las denominaciones de Ciencia de la Cultura y Ciencias del Espíritu, esta última preconizada por Wilhelm Dilthey, el más importante teórico moderno sobre la materia, las cuales designan teorías fundamentales de la epistemología de las Ciencias humanas y, generalizada y permanentemente, han sido consideradas como términos equivalentes al de estas.

Entre las Ciencias humanas y las Ciencias naturales existe, a partir del siglo XIX, tras la crisis de la metafísica idealista y la irrupción de la Sociología, la serie intermedia ya estable designada Ciencias sociales, de definición sin duda menos nítida en virtud de su carácter interrelacionado. Fuera de los campos humanísticos, existe en nuestro tiempo la frecuente tendencia a omitir o aminorar la presencia de las Ciencias humanas en favor de una sobrexposición de las Ciencias sociales como consecuencia, entre otros factores, del incremento de la tendencia occidental, ahora también extendida a Asia, de predominio de las razones económicas de mercado frente a las clásicas y actualmente secundarias de cultura humanística, así como de la extraordinaria influencia desempeñada por los medios de comunicación y sus potentes capacidades de inserción política y social.

La historia de las ciencias humanas asienta en una antigüedad primigenia por principio fundada en saberes profundos pero indiferenciados cuya referencia indiscutible se encuentra en Pitágoras. Las Ciencias humanas se remontan evidentemente a época tan antigua como la de cualquier rama del conocimiento humano. En el pensamiento socrático y en el pensamiento más técnico de los sofistas queda constituido plenamente el saber de la ciencia humanística, ya en la "enciclopedia" aristotélica configurado en el orden más general de las ciencias, es decir, por ejemplo, Retórica y Poética, Ética y Política, o Biología. 

Dilthey, heredero de la hermenéutica de Friedrich Schleiermacher, asume el concepto de "comprensión" ("Verstehen") como principio cognoscitivo de las Ciencias humanas. Esto representa la oposición del par "explicación" / "comprensión", mantenido por Droysen, en tanto oposición Ciencia natural / Ciencia histórica o humana. Dice Dilthey: "La comprensión cae bajo el concepto general del conocer, entediéndose por "conocer", en el sentido más amplio, aquel proceso en el cual se busca un saber de validez universal". "Llamamos "comprender" al proceso en el cual se llega a conocer la vida psíquica partiendo de sus manifestaciones sensiblemente dadas". "Denominamos "interpretación" la comprensión técnica de manifestaciones de vida fijadas por escrito" . 

En la "Introducción a las Ciencias del Espíritu", afirma Dilthey que el estudio de las ciencias humanas o “ciencias del espíritu” es la interpretación de la experiencia personal en un entendimiento reflexivo de la experiencia y una expresión natural de los gestos, las palabras y el arte. También indica que todo saber debe analizarse a la luz de la historia. Sin esta lógica, el conocimiento sólo puede ser parcial. En "El mundo histórico", que ofrece el desarrollo epistemológico por antonomasia de la ciencia del espíritu como humanística, dice Dilthey a propósito de "los métodos en los que se nos presenta el mundo espiritual": "La conexión de las ciencias del espíritu se halla determinada por su fundamento en la vivencia y en la comprensión, y en ambas encontramos diferencias tajantes con respecto a las ciencias de la naturaleza, que prestan su carácter propio al edificio de las ciencias del espíritu".

El objeto de las Ciencias humanas, que se define, frente al de las físico-naturales, en virtud de su singularidad, irrepetibilidad e historicidad, estatuye una gama metodológica que alcanza desde el método filosófico y dialéctico, el hermenéutico y el histórico-crítico hasta el comparatista. 
Los métodos cuantitativos y estadísticos, si bien pueden ejercer subsidiariamente alguna función en la investigación científico-humanística, según en sana lógica cabe comprender, en ningún caso son susceptibles de desempeñar alguna función decisoria ni constante en Ciencias humanas, a diferencia de las Ciencias sociales, en las cuales desempeñan a menudo un procedimiento característico o imprescindible.

Existen taxonomías de criterio tanto en Ciencias humanas como naturales y sociales. Alguna de ellas incluso se quiere transversal entre humanas y sociales, pero de hecho, al presentar múltiples insuficiencias e indeterminaciones, ofrece resultados señaladamente antieconómicos. Existe una discriminación que divide en ontológicas, metodológicas y epistemológicas, pero cuyos solapamientos devienen insostenible desajuste. Como es evidente, la clasificación de las ciencias, humanas o cualesquiera otras es cambiante y responde a la cultura académica y epistemológica de cada época.

Suele afirmarse que la Filosofía es la primera de las Ciencias humanas por cuanto en origen fue matriz de parte de éstas y asimismo atañe de algún modo a la organización del conjunto. Esta relación, que es extensible a las ciencias en general, es de reconocer que modernamente se ha debilitado. En criterio asimismo general, se entiende con frecuencia la Filosofía como fundamento del conocimiento e incluso a veces como ciencia de ciencias. Sin embargo, ya no es frecuente considerar la Gnoseología o Teoría del Conocimiento, la específica disciplina filosófica de determinación cognoscitiva, como primera ciencia. Con todo, la Gnoseología, por una parte, y la Epistemología, que sin embargo actualmente ya cabe ser adscrita de manera sectorial a cada una de las disciplinas humanísticas por sí, puede decirse que continúan señalando los diferentes y respectivos límites de la actividad cognoscitiva y disciplinar. 

Desde su fundamento platónico, y tras el eje socrático, que decidió una filosofía del hombre frente a una filosofía de la naturaleza, es de notar la existencia secular de un doble lineamiento, el de una filosofía contemplativa, a veces neoplatónica, y una filosofía sectorialmente disciplinar y más característicamente aristotélica y académica.
Entre las tradicionales ramas disciplinares de la Filosofía se cuentan fundamentalmente la Metafísica, la Ontología, la Gnoseología, la Lógica, la Ética y la Axiología. 
Entre las delimitaciones modernas se encuentran la Antropología y la Estética, ya considerables con un alto grado de autonomía y vinculación a otras disciplinas contiguas. Por su parte, una disciplina como sobre todo la Psicología, ya se da por definitivamente escindida. 
La Filosofía, a lo largo de su desarrollo histórico y en función del avance del conocimiento, ha ido diversificándose en distintas ramas a fin de aproximarse de forma adecuada a su objeto. 
Existe establecida una serie de ramas especiales, así Filosofía del Lenguaje, Filosofía de la Historia, Filosofía de la Ciencia, Filosofía de la Religión, Filosofía del Derecho, Filosofía de la Educación. Por otro lado, al margen de dichas ramas y aparte ciertos usos más o menos justificables se ha fomentado la tendencia anglosajona a aducir distinciones que a veces se multiplican casi indiscriminadamente. En cualquier caso, se trata de distinciones que no constituyen disciplina y, referidas al saber o a la actividad que fuere, han de mantener cuando menos el sentido propio respecto de primeros principios, normas reguladoras y finalidades.

La gran serie científico-humanística configurada por la Filología delinea tanto la concatenación más extensa de campos de las Ciencias humanas como el trazado más técnico de su metodología. Según afirmaba Johan Huizinga, en Ciencias humanas casi todo es Filología. El marco extensísimo de la Filología permite discernir, siguiendo la "ciencia real", un ámbito general y un ámbito particular relativo al mundo concreto de las lenguas naturales y sus familias y culturas, en el bien entendido de que este último es requisito y objeto presupuesto en el primero.

El ámbito general de la Filología se encuentra organizado sobre la dicotomía de dos grandes dominios: Ciencia del lenguaje o Lingüística y su paralela Ciencia de la literatura o literaria. Ambos dominios han devenido, cada uno por su parte, un organismo disciplinar tripartito organizado sobre la base de tres criterios: histórico, teórico y aplicativo cuya disposición en tanto que ciencia real consiste en la subsiguiente doble serie de Lingüística histórica, Lingüística general o teórico-descriptiva y Lingüística aplicada, y por otra parte Historia de la literatura, Teoría de la literatura y Crítica literaria. Es preciso tener en cuenta que la Teoría de la literatura secularmente y desde la antigüedad configuró dos disciplinas clave de la enciclopedia aristotélica y actualmente vigentes y decisorias: la Retórica o ciencia del discurso general y la Poética o ciencia de la construcción de la obra literaria. Por demás, a estos campos disciplinares, tan autónomos como simétricamente interrelacionados, se suman otros verdaderos campos de naturaleza metodológica transversal de primer orden, así por ejemplo y eminentemente la Ecdótica o Crítica textual, la Traductología, la Dialectología, la Literatura comparada, la Lingüística comparada y, aún más allá, en su sentido completo pluridisciplinar y globalizador, la Comparatística, que en último término atañe al conjunto de las ciencias, sobre todo humanas, pero también sociales y naturales.

Bajo la denominación reciente de Biblioteconomía, o Biblioteconomía y Archivística y Documentación, tiende actualmente a discriminarse una disciplina auxiliar autónoma respecto de las tradicionales metodologías filológicas nacidas en la Escuela de Alejandría. No tanto sucede con las no menos tradicionales Paleografía y, la más general, Bibliografía. Ambas de hecho, pero sobre todo esta última, atañen instrumentalmente a todos los dominios del saber.

El ámbito particular de la Filología es relativo a las mútiples lenguas naturales concretas y sus mundos de cultura. Las áreas mayores de este ámbito a su vez se organizan escalonadamente en sucesivos dominios disciplinares cada vez más concretos y con más específica determinación, por tanto, de lengua concreta. Asimismo, la Filología general, sus series disciplinares, se realizan en las Filologías particulares. La clasificación de las áreas o campos disciplinares mayores de las Filologías particulares es muy nutrida; incluye, principalmente: Egiptología, Indología, Sinología, Niponología, Coreanología, así como Arabismo o Filología Árabe o Filología Semítica, Africanismo, Filología Bíblica o Escriturística, y Filología Clásica o griega y latina antiguas, fundamento de la cultura occidental. El ingente desarrollo de las filologías particulares hace prescindible la distinción de Filología Moderna frente a la referida Clásica y opta por la necesaria distinción sucesiva de particulares, entre ellas, sobre todo, Filología Alemana o Germanística, Filología Inglesa o Angloamericana, Filología Eslava o Eslavística, Filología Románica o Romanística, que a su vez incluye la completa familia neolatina: Filología Francesa, Filología italiana, Filología Rumana, Filología Portuguesa (y sus variantes brasileña y africana) o Galaicoportuguesa. 
Dentro de la Filología Románica posee especial dimensión la Filología Española en tanto que Filología Hispánica, de extraordinaria expansión americana, y sus múltiples subcampos, extremadamente desde la Iberística originaria hasta el sefardí o el Filipinismo asiático e incluso un dominio peninsular ibérico originalmente no románico como el vascuence, además de sus variantes románicas peninsulares como la gallega, la valenciana y la catalana.

La Hermenéutica es un método, característicamente humanístico, que devino gran campo disciplinario e incluso filosofía en sentido general. La Hermenéutica, esto es la teoría de la interpretación y la búsqueda del sentido, se refiere eminentemente a los textos, con preferencia a los textos importantes y difíciles, razón por la cual se encuentra ligada desde su origen a la Filología y a la Crítica literaria al igual que a la Filosofía y, asimismo, existe y ha existido secularmente su especialización también como Escriturística, relativa a los textos sagrados. El método hermenéutico, tras las escuelas antiguas, de Alejandría a Antioquía o Pérgamo, obtuvo un centramiento técnico con Flacius y posteriormente con Meier, para alcanzar en el pensamiento de Friedrich Schleiermacher su cima en tanto que método total dirigido a la "comprensión", es decir relativo tanto a la lógica y la gramática como a la retórica y a la dialéctica y a la historia, según explicó Dilthey. El vigor de la Hermenéutica durante el siglo XX puede ser calibrado simplemente con tomar en cuenta la dimensión e influencia, por otra parte muy problemáticas, de las obras de Martin Heidegger y su discípulo Hans-Georg Gadamer ("Verdad y método"), ambos tan importantes como discutibles.

Caído en desuso el término general de Ciencias Eclesiásticas (aun vigente a fines del XVIII en la obra de Juan Andrés), que englobaba Teología, Derecho canónico, Derecho eclesiástico, Historia de la Iglesia y Escriturística permaneciendo de éstas el marbete fundamental y distintivo de Teología, es Ciencia de las religiones actualmente el término subsiguiente de mayor vigencia y que por tanto, o al menos en parte, ha de englobar las restantes designaciones menores en uso vivo, empezando por el de Religiones comparadas. La denominada Teodicea designa a la Filosofía teológica.
Las ramas de la Teología son Teología natural o racional, Teología dogmática y revelada y Teología moral. A éstas se las puede calificar de "cristianas", o "católicas", "protestantes", "ortodoxas"...
Existe una Retórica sagrada o cristiana, el "Ars Praedicandi", de importante desarrollo medieval.
Ciertamente, gran parte de todas estas disciplinas posee profundo desarrollo en los ámbitos correspondientes a las distintas religiones, si bien es en las llamadas "religiones del libro", especialmente en la cristiana, donde disfrutan de mayor especificidad.

Dos distinciones contemporáneas muy extendidas, que designan meramente corrientes de pensamiento y no campos disciplinares, son Teología de la liberación y Teología negativa.

Se ha establecido el término general de Ciencias de la Educación, o Educación, que en general definiría el objeto, para englobar, sobre todo, Pedagogía y Didáctica, las dos ramas tradicionalmente definidas de este ámbito. La Educación y su gama disciplinaria, a veces lindante con la Psicología, sobre todo en su designación de Psicopedagogía, y permanentemente con el conjunto de materias que son objeto de "enseñanza", ha girado progresivamente durante la segunda mitad del siglo XX hacia la relación enseñanza-aprendizaje. La Educación es referida con frecuencia a sus aspectos de Filosofía, Economía, etc., pero discrimina con preferencia dos campos bien establecidos: Política de la Educación y Educación comparada. Por su parte, las instituciones educativas y su historia configuran un campo que aún no dispone del volumen de estudio que se aproxime a su evidente importancia. En nuestro tiempo se le suele achacar a las Ciencias de la Educación el haberse sometido a un proceso de identificación metodológica crecientemente burocratizado.

La Estética se refiere tanto a la Naturaleza, y a la vida en general, como al arte en tanto Filosofía del arte en concepto hegeliano. La Estética tiene dos grandes épocas o ciclos. La Estética antigua y clasicista define un saber entremezclado, como prototípicamente se observa en la obra de Platón; la Estética moderna, especialmente a partir del Empirismo inglés, Baumgarten y, sobre todo la "Crítica del Juicio" de Kant, se configura como disciplina autónoma desligada de la Ética, cosa esta última que fue discutida de inmediato, y reelaborada de hecho por Friedrich Schiller mediante sus argumentos estéticos acerca de la libertad antropológicamente fundados. A partir de Kant la Estética propiamente idealista pasa a desempeñar el lugar clave de resolución para el pensamiento moderno. Con posterioridad es de constatar, entre otras cosas, una estética tanto de la empatía o proyección como formalista en Alemania. A principios del siglo XX propuso Benedetto Croce un barrido de la "techne" epistemológicamente reordenador. La segunda mitad del siglo XX ha estado determinado especialmente por la teoría problemática de Theodor Adorno, en nuestro tiempo sometida a crítica.

El centro teórico de la disciplina está formado, principalmente, por la estimativa y la teoría del valor, por la teoría del efecto estético y, acaso en lo más característico, por las categorías estéticas, esto es fundamentalmente la Belleza y lo Sublime, pero también lo Humorístico y lo Trágico, por otra parte lo Feo, que son las distinciones mejor asentadas. Existe, cuando menos, otro tipo de categorías estéticas modernamente reconocido, las histórico-estilísticas, de inserción periodológica.

Las Ciencias del arte configuran una serie análoga a la general de la Filología, siguiendo los tres criterios de intervención histórica, teórica y aplicativa, esto es Historia o historiografía del Arte, Teoría del Arte y Crítica artística. Este régimen de la ciencia real se ha extendido con naturalidad y eficacia al conjunto de objetos que definen estos campos tradicionales pero también los de nueva creación contemporánea. Estos campos y objetos, definitoriamente, se refieren a artes plásticas, visuales y auditiva musical. Se trata de disciplinas que historizan, analizan y critican el arte. A esta serie disciplinar se ha de sumar la instrumentalización procurada por la Museografía. Se trata de campos y objetos que actualmente poseen importante proyección a través de los medios de comunicación. El crítico de arte analiza, observa y aprecia las obras de arte desde una perspectiva cuyo grado de objetividad constituye uno de los problemas básicos de estas especializaciones, particularmente en los medios de carácter publicístico o de actualidad.

Las tradicionalmente llamadas Bellas Artes han sido las plásticas, es decir Pintura, Escultura y Arquitectura, esto es las artes particulares en concepto hegeliano, pero también las subespecialidades como la de la estampación calcográfica y el grabado en sus distintas gamas, desde la xilografía hasta la serigrafía. Asimismo, la Numismática, y la serie de artes "menores", sea la cerámica y las artes decorativas, o la caligrafía. Por otra parte, son de distinguir campos de estudio, como especialmente la Iconología.

A ello se ha de sumar, ya sistemática y establemente desde la Poética aristotélica, la danza. El teatro es considerable tanto en su vertiente de arte literario o poesía como en tanto artes escénicas. A esta gran serie añadió el siglo XX géneros especiales híbridos como el de la instalación y, por otra parte, el cine sobre todo, también el vídeo. Pero antecede a éste la fotografía, quizás mejor incluible entre las plásticas tradicionales. Por su parte, la ópera, con frecuencia tenida por "arte total", en tanto compuesta, remite a las artes escénicas en conjunto, pero eminentemnete a la coral y la música, a un género de ésta. Todas estas artes son, pues, objeto de estudio histórico, teórico y crítico.

Se denomina en general Musicología, de manera paralela a la Filología y la serie dedicada a las Artes plásticas, a la triple distinción disciplinar de Historia de la música, Teoría de la música y Crítica musical. La tradición musicológica se funda al tiempo que soporta un intenso pasado que arranca eminentemente de Pitágoras, la filosofía clásica y la Física y Matemática antiguas. Modernamente, sobre todo a partir de fines del siglo XVIII y la filosofía sensista y empirista, en particular de la obra de Antonio Eximeno, la música abandona la doctrina físico-matemática para comenzar a instituir de manera definitiva un concepto expresivo, que es el que triunfará en la práctica musical y el pensamiento estético correspondiente en el siglo XIX, dando como fruto las músicas "nacionales".

La principal peculiaridad de la ciencia de la Música, a diferencia de otras artes, o en mayor grado que éstas, consiste en que su teoría técnica, en tanto que se ocupa de un lenguaje de forma autónoma constituido, establece un sólido organismo ténicamente propio al tiempo que una dificultad de transición conceptual, tanto intradisciplinaria como, sobre todo, exterior y crítica.

Es de entender, en primer lugar, siguiendo la clasificación hegeliana, desde una Historia inmediata que conceptualiza lo que ha sucedido y ha sido visto, una historia reflexionada y una historia por conceptos. Esta Historia por concepto es aquella que se refiere, y aquí es cuestión decisiva, a todas y cada una de las ciencias y disciplinas humanísticas. La historia como materia política es parcialmente una ciencia social que estudia el pasado de la humanidad. En su especialización, se centra en el desarrollo de ciertos sistemas (la sociedad, las poblaciones, etc.), a través del tiempo; en algunos casos insistiendo en su capacidad de cuantificación. Desde otro punto de vista, sistematiza y analiza las acciones humanas (para Habermas acción comunicativa) en periodos de tiempo definidos.

La Historiografía ha evolucionado con frecuencia durante el siglo XX desde los objetos generales civiles, políticos y socioeconómicos hacia preferencias de la vida privada, material y de las mentalidades.

La Historia de la cultura y la Historia de las ideas configuran dos ramas historiográficas modernas y especiales en virtud de la compleja historicidad de sus objetos. Ambas se refieren a objetos constitutivamente diferentes al tiempo que pueden ser reintegrados como parte. Característicamente definen formas del pensamiento contemporáneo, largamente maduradas y que culminan estableciéndose durante la segunda mitad del siglo XIX, sobre todo a manos, respectivamente, de Jacob Burckhardt y Marcelino Menéndez Pelayo. 

Mucha menor entidad han adquirido las delimitaciones de "Historia de las mentalidades" e "Historia intelectual", de perfiles menos nítidos o menos eficaces.
La Historia de las ideas, habitualmente relacionada con la Comparatística, ha tenido en los campos del pensamiento estético y político sus dos ámbitos mayores de desarrollo.

La Psicología estudia contenidos de conciencia y modos del comportamiento o la conducta, mediante la introspección y mediante la observación en participación. Existe una gran corriente psicológica denominada "conductista" y, a veces, se habla de la Psicología como la ciencia del comportamiento. Ésta se enfoca en definir a la conciencia en la medida en que ésta forma con el comportamiento al ente humano. El método seguido por la psicología implica que los distintos seres humanos observados no son diferentes hasta el punto de que las leyes sobre las relaciones entre los estímulos y las reacciones sean también diferentes. No siempre son claros los límites entre Psicología y Psicoanalítica, entre Psicología y Neurología.

Las áreas de estudio de la Psicología presentan relaciones de cierta complejidad. La psicología fisiológica, por ejemplo, estudia el funcionamiento del cerebro y del sistema nervioso, mientras que la psicología experimental aplica técnicas de laboratorio para estudiar, por ejemplo, la percepción o la memoria.

Existen fundamentalmente una Antropología filosófica y una Antropología etnográfica o Etnografía. Ha sido concebida una Antropología general pero también es un hecho que el objeto de estudio antropológico no ha sido especificado establemente. Es de asumir que la Antropología estudia el comportamiento humano desde un criterio parcial o bien holístico, así como las relaciones humanas, o sea los grupos humanos en tanto culturales y según qué relaciones interpersonales determinan, las jerarquías de estos grupos, sus conflictos y su evolución. El enfoque de esta disciplina, tradicionalmente, se ha aplicado al estudio de la evolución y el comportamiento de aspectos paradigmáticos del individuo, bien de grupos humanos, sean ágrafos (sin escritura) o aislados, pero también en convivencia con otras líneas de estudio relativas a la vida y las sociedades modernas y sus derivaciones (sobre todo las occidentales); así por ejemplo, las aplicaciones relativas a antropología de la empresa, ya posteriores a la relación de objeto establecida entre antropología rural y urbana, entre otras. Durante el siglo XX tanto el Funcionalismo como sobre todo el Estructuralismo, e incluso la Lingüística de esta inclinación, afectaron grandemente a los estudios antropológicos llegando a concebirse un antes y un después. Actualmente parece superado esa perspectiva de cosas.

Ciencias Jurídicas o Ciencias del Derecho es el marbete que engloba los diferentes campos disciplinarios establecidos en el ámbito jurídico, desde la híbrida Filosofía del Derecho, a su vez vinculada a la Filosofía Moral y Política, hasta la Jurisprudencia por su extremo particularizado, que alcanza a la documentaria y la taxonomía de los grandes repertorios, ya activos o meramente históricos. La gran serie disciplinar vigente consiste en Derecho civil, Derecho penal y procesal, Derecho político, Derecho constitucional, Derecho mercantil, Derecho tributario, Derecho laboral, Derecho administrativo, Derecho internacional. La Historia del Derecho occidental tiene como base el Derecho romano. El Derecho comparado define una metodología, y es consustancial especialmente a un campo como el del constitucionalismo. 

Existen distinciones no propiamente disciplinares sino sectoriales, bien inherentes, como Ética jurídica, o bien aplicadas, como Derecho ambiental, Derecho registral, Derecho del consumidor, Derecho informático.

La Geografía constituye la plural y paradigmática serie científica que atañe, según sus partes, tanto a las Ciencias humanas, así la tradicionalmente llamada Geografía humana, fronteriza con la Historia y la Antropología, como a las físico-naturales, esto es la Geografía física, y también sociales en el caso de la Geografía de la población.
Las ciencias auxiliares de la Geografía son múltiples, a partir de la Geografía de la Tierra y hasta concreciones aplicativas como las referentes a biología, cultura o turismo..., pero poseen especial estatus la Cartografía y la Paisajística, ambas de gran relieve humanístico.

Si la Geografía ofrece una entidad organizada por principio como pluralidad científica, las Ciencias jurídicas ofrecen por su parte un perfil de transición humana / social paradigmático. No puede olvidarse que el ámbito jurídico atiende desde una Filosofía del Derecho hasta una práctica puramente política o meramente administrativa aplicada en lo fundamental a métodos de trámite oficial o institucional en todas las diversas instancias. Puede decirse que la Sociología ocupa el centro definitorio de las Ciencias sociales, mientras que las ciencias políticas y la Economía o las Ciencias económicas configuran sus grandes dominios de expansión en el cuerpo social, ya con independencia del aspecto humano esencial. La Sociología estudia una parte específica o, más bien, una perspectiva específica de la totalidad de la vida social que incluye fundamentalmente el intercambio material, de actitudes y emociones, especialmente costumbres, comportamientos individuales e interpersonales de los miembros de una sociedad. Su mayor objetivo es entender al ser humano como parte de un grupo social y sus relaciones. Los estudios sociales son en gran medida, a diferencia de los humanísticos, de base cuantitativa y estadística. Existe, por otra parte, una tendencia llamada "filosofía social".

Si bien la Política nace de completa planta en la enciclopedia aristotélica como humana relación Ética y Retórica, modernamente representa, al menos en cierta medida relevante, el mejor ejemplo de desgajamiento del tradicional saber humanístico y aproximación al ámbito de la Sociología. Las ciencias políticas son el grupo de ciencias sociales dedicado, al menos en parte, a la toma decisiones. Estas son relativas a la teoría política.

Las Ciencias económicas son sociales y estudian las relaciones del individuo o el ciudadano regidas por instrumentos de cuantificación "naturales": precios, salario, cantidades de bienes producidos, ritmo de producción, etc. La Economía estudia con medios cuantitativos el funcionamiento que provee el hombre económico, el cual tiene a su disposición una gran diversidad de instrumentos y persigue una multiplicidad de fines. Este individuo es por tanto considerado en el marco del régimen social de producción, de la producción de bienes materiales. Más allá de la Contabilidad, en el marco de la Contaduría, existe una ciencia económica matematizada en tanto disciplina teorética y no sólo estadística. Aunque estas técnicas y saberes se relacionan con la actividad de la persona, muy poco tienen que ver con las humanidades. Es de saber que la teoría económica moderna fue creada por los filósofos de la Escuela de Salamanca.

Finalmente, cabe decir que la Biología Humana y, en general por otra parte, las actualmente denominadas Ciencias de la salud, ejercen una vinculación humana y humanitarista ajena al núcleo de los objetos humanísticos. La biología humana, fundándose en la anatomía y fisiología, explica el funcionamiento del cuerpo humano. Su enfoque primario recae en la descripción interna de los órganos que lo componen y las relaciones que mantienen entre sí. El término de Ciencias de la salud tiende a englobar la Medicina en sus diferentes especializaciones y niveles tanto técnicos y terapéuticos como asistenciales. Ha existido tradicionalmente una fuerte vinculación, aun epistemológicamente por completo externa, entre medicina y cultura humanística.

Entrado el siglo XXI, toda ciencia humanística queda referida al curso y al problema de la Globalización.
Por otra parte, este cuerpo de conocimiento y disciplinas, redefinido en el siglo XIX es fundamental para el desarrollo de las variantes epistemológicas que se pudieran producir en lo sucesivo. La Ciencias humanas han de desarrollar su propia epistemología con vigor e independencia frente a frecuentes e indisimuladas agresiones. El crecimiento de las Ciencias sociales, especialmente gracias al ingente aparato político organizativo de las sociedades occidentales, sobre todo como aplicación del procedimiento de la "encuesta" y la estadística, ha conducido a un retraimiento académico de las Ciencias humanas y la cultura humanística, que han de reubicar su programa y finalidades irrenunciables en un mundo regido, para bien y para mal, por el ámbito público. Misión actual de las Ciencias humanas es construir la relación entre civilizaciones, la cual ha de asentarse en el aspecto humano permanente, inherente y no sustituible, de pensamiento, lenguaje, religiones y artes, ahora integrado en un mundo abocado a la globalización y la multitud de problemas culturales, humanísticos y humanitarios que ésta suscita.





































</doc>
<doc id="3104" url="https://es.wikipedia.org/wiki?curid=3104" title="Ciencias naturales">
Ciencias naturales

Ciencias naturales, ciencias de la naturaleza, ciencias físico-naturales o ciencias experimentales son aquellas ciencias que tienen por objeto el estudio de la naturaleza, siguiendo la modalidad del método científico conocida como método experimental. Estudian los aspectos físicos e intentando no incluir aspectos relativos a las acciones humanas. Así, como grupo, las ciencias naturales se distinguen de las ciencias sociales o ciencias humanas (cuya identificación o diferenciación de las humanidades y artes y de otro tipo de saberes es un problema epistemológico diferente). 

Las ciencias naturales, por su parte, se apoyan en el razonamiento lógico y el aparato metodológico de las ciencias formales, especialmente de la matemática y la lógica, cuya relación con la realidad de la naturaleza es indirecta. A diferencia de las ciencias aplicadas, las ciencias naturales son parte de la ciencia básica, pero tienen en ellas sus desarrollos prácticos, e interactúan con ellas y con el sistema productivo en los sistemas denominados de "investigación y desarrollo" o "investigación, desarrollo e innovación" (I+D e I+D+I). 

No deben confundirse con el concepto más restringido de ciencias de la Tierra o geociencias.


En la segunda mitad del siglo XX, surgió un nuevo enfoque de las ciencias físico-naturales referido a las ciencias de la Tierra, en el que se elimina el estudio de la Astronomía y otras ciencias similares y, en cambio, se le añaden algunas ciencias a nivel detallado que permiten una mejor interpretación del mundo en que vivimos, es decir, del planeta Tierra. El enfoque unificador de las ciencias de la Tierra es el flujo de energía, por lo que la Termodinámica viene a ser la ciencia que sirve de base a este grupo de ciencias.

Esta disciplina es la ciencia de los objetos y fenómenos astronómicos originados fuera de la atmósfera terrestre. Su campo está relacionado con la Física, con la Química, con el movimiento y con la evolución de los objetos celestes, así como también con la formación y el desarrollo del Universo. La Astronomía incluye el examen, estudio y modelado de las estrellas, los planetas, los cometas, las galaxias y el cosmos. La mayoría de la información usada por los astrónomos es recogida por la observación remota, aunque se ha conseguido reproducir, en algunos casos, en laboratorio, la ejecución de fenómenos celestes, como, por ejemplo, la Química Molecular del medio interestelar.

Mientras los orígenes del estudio de los elementos y fenómenos celestes pueden ser rastreados hasta la antigüedad, la metodología científica de este campo empezó a desarrollarse a mediados del siglo XVII. Un factor clave fue la introducción del telescopio por Galileo Galilei, que permitió examinar el cielo de la noche más detalladamente. El tratamiento matemático de la Astronomía comenzó con el desarrollo de la mecánica celeste y con las leyes de gravitación por Isaac Newton, aunque ya había sido puesto en marcha por el trabajo anterior de astrónomos como Johannes Kepler. Hacia el siglo XIX, la Astronomía se había desarrollado como una ciencia formal, con la introducción de instrumentos tales como el espectroscopio y la fotografía, que permitieron la continua mejora de telescopios y la creación de observatorios profesionales.

Este campo, comprende un conjunto de disciplinas que examinan fenómenos relativos a organismos vivos. La escala de estudio va desde los subcomponentes biofísicos hasta los sistemas complejos. La Biología se ocupa de las características, la clasificación y la conducta de los organismos, así como de la formación y las interacciones de las especies entre sí y con el medio natural.

Los campos biológicos de la Botánica, la Zoología y la Medicina surgieron desde los primeros momentos de la civilización, mientras que la Microbiología fue introducida en el siglo XVII con el descubrimiento del microscopio. Sin embargo, no fue hasta el siglo XIX cuando la Biología se unificó, una vez que los científicos descubrieron coincidencias en todos los seres vivos y decidieron estudiarlos como un conjunto. Algunos desarrollos clave en la ciencia de la Biología fueron la genética, la Teoría de la Evolución de Charles Darwin con la llamada selección natural, la Teoría Microbiana de las Enfermedades Infecciosas y la aplicación de técnicas de Física y Química a nivel celular y molecular (Biofísica y Bioquímica, respectivamente).

La Biología moderna se divide en sub-disciplinas, según los tipos de organismo y la escala en el que se estudian. La Biología Molecular es el estudio de la Química fundamental de la vida, mientras que la Biología Celular tiene como objeto el examen de la célula, es decir, la unidad constructiva básica de toda la vida. A un nivel más elevado, está la Fisiología, que estudia la estructura interna del organismo.

La Física, incluye el estudio de los componentes fundamentales del Universo, las fuerzas e interacciones que ejercen entre sí y los resultados producidos por dichas interacciones. En general, la Física es considerada como una ciencia fundamental, estrechamente vinculada con la Matemática y la Lógica en la formulación y cuantificación de los principios.

El estudio de los principios del Universo tiene una larga historia y un gran trabajo deductivo, a partir de la observación y la experimentación. La formulación de las teorías sobre las leyes que gobiernan el Universo ha sido un objetivo central de la Física desde tiempos remotos, con la filosofía del empleo sistemático de experimentos cuantitativos de observación y prueba como fuente de verificación. La clave del desarrollo histórico de la Física incluye hitos como la Teoría de la Gravitación Universal y la mecánica clásica de Newton, la comprensión de la naturaleza de la electricidad y su relación con el magnetismo, la Teoría General de la Relatividad y la Teoría Especial de la Relatividad de Einstein, el desarrollo de la termodinámica y el modelo de la mecánica cuántica, a los niveles de la Física atómica y subatómica.

El campo de la Física es extraordinariamente amplio, y puede incluir estudios tan diversos como la Mecánica Cuántica, la Física Teórica o la Óptica. La Física moderna se orienta a una especialización creciente, donde los investigadores tienden a enfocar áreas particulares más que a ser universalistas, como lo fueron Albert Einstein o Lev Landau, que trabajaron en una multiplicidad de áreas.

La Geología es un término que engloba a las ciencias relacionadas con el planeta Tierra, que incluyen la Geofísica, la Tectónica, la Geología estructural, la Estratigrafía, la Geología histórica, la Hidrología, la Meteorología, la Geomorfología, la Oceanografía y la Edafología.

Aunque la minería y las piedras preciosas han sido objeto del interés humano a lo largo de la historia de la civilización, su desarrollo científico dentro de la ciencia de la Geología no ocurrió hasta el siglo XVIII. El estudio de la Tierra, en especial, la Paleontología, floreció en el siglo XIX, y el crecimiento de otras disciplinas, como la Geofísica, en el siglo XX, con la Teoría de las Placas Tectónicas, en los años 60, que tuvo un impacto sobre las ciencias de la Tierra similar a la Teoría de la Evolución sobre la Biología.

La Geología está, en la actualidad, estrechamente ligada a la investigación climática y a las industrias minera y petrolera.

La paleontología estudia e interpreta el pasado de la vida sobre la Tierra a través de los fósiles. Posee un cuerpo de doctrina propio y comparte fundamentos y métodos con la geología y la biología con las que se integra estrechamente. Se divide en tres campos de estudio: paleobiología, tafonomía y biocronología.

Entre sus objetivos están, además de la reconstrucción de los seres vivos que vivieron en el pasado, el estudio de su origen, de sus cambios en el tiempo (evolución y filogenia), de las relaciones entre ellos y con su entorno (paleoecología, evolución de la biosfera), de su distribución espacial y migraciones (paleobiogeografía), de las extinciones, de los procesos de fosilización (tafonomía) o de la correlación y datación de las rocas que los contienen (bioestratigrafía).

La Paleontología permite entender la actual composición (biodiversidad) y distribución de los seres vivos sobre la Tierra (biogeografía) —antes de la intervención humana—, ha aportado pruebas indispensables para la solución de dos de las más grandes controversias científicas del pasado siglo, la evolución de los seres vivos y la deriva de los continentes, y, de cara a nuestro futuro, ofrece herramientas para el análisis de cómo los cambios climáticos pueden afectar al conjunto de la biosfera.

Constituyendo el estudio científico de la materia a escala atómica y molecular, la Química se ocupa principalmente de las agrupaciones supraatómicas, como son los gases, las moléculas, los cristales y los metales, estudiando su composición, propiedades estadísticas, transformaciones y reacciones. La Química también incluye la comprensión de las propiedades e interacciones de la materia a escala atómica. La mayoría de los procesos químicos pueden ser estudiados directamente en el laboratorio, usando una serie de técnicas a menudo bien establecidas, tanto de manipulación de materiales como de comprensión de los procesos subyacentes. Una aproximación alternativa es la proporcionada por las técnicas de modelado molecular, que extraen conclusiones de modelos computacionales. La Química es llamada a menudo "ciencia central", por su papel de conexión con las otras Ciencias Naturales.

La experimentación química tuvo su origen en la Alquimia, un sistema de creencias que combinaba esoterismo y experimentación física. La ciencia de la Química comenzó a desarrollarse a finales del siglo XVIII, con el trabajo de científicos notables como Robert Boyle, el descubridor de los gases, o Antoine Lavoisier, que descubrió la Ley de Conservación de la Masa. La sistematización se hizo patente con la creación de la Tabla Periódica de los Elementos y la introducción de la Teoría Atómica, cuando los investigadores desarrollaron una comprensión fundamental de los estados de la materia, los iones, los enlaces químicos y las reacciones químicas. Desde la primera mitad del siglo XIX, el desarrollo de la Química lleva aparejado la aparición y expansión de una industria química de gran relevancia en la economía y la calidad de vida actuales.

Las diferencias entre las disciplinas de las Ciencias Naturales no siempre son marcadas, y estas «ciencias cruzadas» comparten un gran número de campos. La Física juega un papel significativo en las otras Ciencias Naturales, dando origen, por ejemplo, a la Astrofísica, la Geofísica, la Química Física y la Biofísica. Asimismo, la Química está representada por varios campos, como la Bioquímica, la Geoquímica y la Astroquímica.

Un ejemplo particular de disciplina científica que abarca múltiples Ciencias Naturales es la ciencia del medio ambiente. Esta materia estudia las interacciones de los componentes físicos, químicos y biológicos del medio, con particular atención a los efectos de la actividad humana y su impacto sobre la biodiversidad y la sostenibilidad. Esta ciencia también afecta a expertos de otros campos.

Una disciplina comparable a la anterior es la Oceanografía, que se relaciona con una amplia gama de disciplinas científicas. La Oceanografía se subdivide, a su vez, en otras disciplinas cruzadas, como la Biología Marina. Como el ecosistema marino es muy grande y diverso, la Biología Marina también se bifurca en muchas subdivisiones, incluyendo especializaciones en especies particulares.

Hay también un grupo de campos con disciplinas cruzadas en los que, por la naturaleza de los problemas que abarcan, hay fuertes corrientes contrarias a la especialización. Por otro lado, en algunos campos de aplicaciones integrales, los especialistas, en más de un campo, tienen un papel clave en el diálogo entre ellos. Tales campos integrales, por ejemplo, pueden incluir la Nanociencia, la Astrobiología y complejos sistemas informáticos.



</doc>
<doc id="3222" url="https://es.wikipedia.org/wiki?curid=3222" title="Teoría de la relatividad">
Teoría de la relatividad

La teoría de la relatividad incluye tanto a la teoría de la relatividad especial como la de relatividad general, formuladas por Albert Einstein a principios del siglo XX, que pretendían resolver la incompatibilidad existente entre la mecánica newtoniana y el electromagnetismo. 

La teoría de la relatividad especial, publicada en 1905, trata de la física del movimiento de los cuerpos en ausencia de fuerzas gravitatorias, en el que se hacían compatibles las ecuaciones de Maxwell del electromagnetismo con una reformulación de las leyes del movimiento.

La teoría de la relatividad general, publicada en 1915, es una teoría de la gravedad que reemplaza a la gravedad newtoniana, aunque coincide numéricamente con ella para campos gravitatorios débiles y "pequeñas" velocidades. La teoría general se reduce a la teoría especial en ausencia de campos gravitatorios.

El 7 de marzo de 2010, la Academia Israelí de Ciencias exhibió públicamente los manuscritos originales de Einstein (redactados en 1905). El documento, que contiene 46 páginas de textos y fórmulas matemáticas escritas a mano, fue donado por Einstein a la Universidad Hebrea de Jerusalén en 1925 con motivo de su inauguración.

El supuesto básico de la teoría de la relatividad es que la localización de los sucesos físicos, tanto en el tiempo como en el espacio, son relativos al estado de movimiento del observador: así, la longitud de un objeto en movimiento o el instante en que algo sucede, a diferencia de lo que sucede en mecánica newtoniana, no son invariantes absolutos, y diferentes observadores en movimiento relativo entre sí diferirán respecto a ellos (las longitudes y los intervalos temporales, en relatividad son relativos y no absolutos).

La teoría de la relatividad especial, también llamada teoría de la relatividad restringida, fue publicada por Albert Einstein en 1905 y describe la física del movimiento en el marco de un espacio-tiempo plano. Esta teoría describe correctamente el movimiento de los cuerpos incluso a grandes velocidades y sus interacciones electromagnéticas, se usa básicamente para estudiar sistemas de referencia inerciales (no es aplicable para problemas astrofísicos donde el campo gravitatorio desempeña un papel importante).

Estos conceptos fueron presentados anteriormente por Poincaré y Lorentz, que son considerados como precursores de la teoría. Si bien la teoría resolvía un buen número de problemas del electromagnetismo y daba una explicación del experimento de Michelson-Morley, no proporciona una descripción relativista adecuada del campo gravitatorio.

Tras la publicación del artículo de Einstein, la nueva teoría de la relatividad especial fue aceptada en unos pocos años por prácticamente la totalidad de los físicos y los matemáticos. De hecho, Poincaré o Lorentz habían estado muy cerca de llegar al mismo resultado que Einstein. La forma geométrica definitiva de la teoría se debe a Hermann Minkowski, antiguo profesor de Einstein en la Politécnica de Zürich; acuñó el término "espacio-tiempo" ("Raumzeit") y le dio la forma matemática adecuada. El espacio-tiempo de Minkowski es una variedad tetradimensional en la que se entrelazaban de una manera indisoluble las tres dimensiones espaciales y el tiempo. En este espacio-tiempo de Minkowski, el movimiento de una partícula se representa mediante su línea de universo ("Weltlinie"), una curva cuyos puntos vienen determinados por cuatro variables distintas: las tres dimensiones espaciales (formula_1,formula_2,formula_3) y el tiempo (formula_4). El nuevo esquema de Minkowski obligó a reinterpretar los conceptos de la métrica existentes hasta entonces. El concepto tridimensional de punto fue sustituido por el de suceso. La magnitud de distancia se reemplaza por la magnitud de intervalo.

La relatividad general fue publicada por Einstein en 1915, presentada como conferencia en la Academia de Ciencias Prusiana el 25 de noviembre. La teoría generaliza el principio de relatividad de Einstein para un observador arbitrario. Esto implica que las ecuaciones de la teoría deben tener una forma de covariancia más general que la covariancia de Lorentz usada en la teoría de la relatividad especial. Además de esto, la teoría de la relatividad general propone que la propia geometría del espacio-tiempo se ve afectada por la presencia de materia, de lo cual resulta una teoría relativista del campo gravitatorio. De hecho la teoría de la relatividad general predice que el espacio-tiempo no será plano en presencia de materia y que la curvatura del espacio-tiempo será percibida como un campo gravitatorio.

Debe notarse que el matemático alemán David Hilbert escribió e hizo públicas las ecuaciones de la covariancia antes que Einstein. Ello resultó en no pocas acusaciones de plagio contra Einstein, pero probablemente sea más, porque es una teoría (o perspectiva) geométrica. La misma postula que la presencia de masa o energía «curva» al espacio-tiempo, y esta curvatura afecta la trayectoria de los cuerpos móviles e incluso la trayectoria de la luz.

Einstein expresó el propósito de la teoría de la relatividad general para aplicar plenamente el programa de Ernst Mach de la relativización de todos los efectos de inercia, incluso añadiendo la llamada "constante cosmológica" a sus ecuaciones de campopara este propósito. Este punto de contacto real de la influencia de Ernst Mach fue claramente identificado en 1918, cuando Einstein distingue lo que él bautizó como el "principio de Mach" (los efectos inerciales se derivan de la interacción de los cuerpos) del principio de la relatividad general, que se interpreta ahora como el principio de covariancia general.

En la teoría de la relatividad una partícula puntual queda representada por un par formula_5, donde formula_6 es una curva diferenciable, llamada línea de universo de la partícula, y "m" es un escalar que representa la masa en reposo. El vector tangente a esta curva es un llamado cuadrivelocidad, el producto de este vector por la masa en reposo de la partícula es precisamente el cuadrimomento. Este cuadrimomento es un vector de cuatro componentes, tres de estas componentes se denominan espaciales y representan el análogo relativista del momento lineal de la mecánica clásica, la otra componente denominada componente temporal representa la generalización relativista de la energía cinética. Además, dada una curva arbitraria en el espacio-tiempo, puede definirse a lo largo de ella el llamado "intervalo relativista", que se obtiene a partir del tensor métrico. El intervalo relativista medido a lo largo de la trayectoria de una partícula es proporcional al intervalo de tiempo propio o intervalo de tiempo percibido por dicha partícula.

Cuando se consideran campos o distribuciones continuas de masa, se necesita algún tipo de generalización para la noción de partícula. Un campo físico posee momentum y energía distribuidos en el espacio-tiempo, el concepto de cuadrimomento se generaliza mediante el llamado tensor de energía-impulso que representa la distribución en el espacio-tiempo tanto de energía como de momento lineal. A su vez un campo dependiendo de su naturaleza puede representarse por un escalar, un vector o un tensor. Por ejemplo el campo electromagnético se representa por un tensor de segundo orden totalmente antisimétrico o 2-forma. Si se conoce la variación de un campo o una distribución de materia, en el espacio y en el tiempo entonces existen procedimientos para construir su tensor de energía-impulso.

En relatividad, estas magnitudes físicas son representadas por vectores 4-dimensionales o bien por objetos matemáticos llamados tensores, que generalizan los vectores, definidos sobre un espacio de cuatro dimensiones. Matemáticamente estos 4-vectores y 4-tensores son elementos definidos del espacio vectorial tangente al espacio-tiempo (y los tensores se definen y se construyen a partir del fibrado tangente o cotangente de la variedad que representa el espacio-tiempo).

Igualmente además de cuadrivectores, se definen cuadritensores (tensores ordinarios definidos sobre el fibrado tangente del espacio-tiempo concebido como variedad lorentziana). La curvatura del espacio-tiempo se representa por un 4-tensor (tensor de cuarto orden), mientras que la energía y el momento de un medio continuo o el campo electromagnético se representan mediante 2-tensores (simétrico el tensor energía-impulso, antisimétrico el de campo electromagnético). Los cuadrivectores son de hecho 1-tensores, en esta terminología. En este contexto se dice que una magnitud es un invariante relativista si tiene el mismo valor para todos los observadores, obviamente todos los invariantes relativistas son escalares (0-tensores), frecuentemente formados por la contracción de magnitudes tensoriales.

El intervalo relativista puede definirse en cualquier espacio-tiempo, sea este plano como en la relatividad especial, o curvo como en relatividad general. Sin embargo, por simplicidad, discutiremos inicialmente el concepto de intervalo para el caso de un espacio-tiempo plano. El tensor métrico del espacio-tiempo plano de Minkowski se designa con la letra formula_7, y en coordenadas galileanas o inerciales toma la siguiente forma:

El intervalo, la distancia tetradimensional, se representa mediante la expresión formula_8, que se calcula del siguiente modo:

Los intervalos pueden ser clasificados en tres categorías: Intervalos espaciales (cuando formula_9 es negativo), temporales (si formula_9 es positivo) y nulos (cuando formula_11). Como el lector habrá podido comprobar, los intervalos nulos son aquellos que corresponden a partículas que se mueven a la velocidad de la luz, como los fotones: La distancia formula_12 recorrida por el fotón es igual a su velocidad ("c") multiplicada por el tiempo formula_13 y por lo tanto el intervalo formula_14 se hace nulo.

Los intervalos nulos pueden ser representados en forma de cono de luz, popularizados por el celebérrimo libro de Stephen Hawking, "Historia del Tiempo". Sea un observador situado en el origen, el "futuro absoluto" (los sucesos que serán percibidos por el individuo) se despliega en la parte superior del eje de ordenadas, el "pasado absoluto" (los sucesos que ya han sido percibidos por el individuo) en la parte inferior, y el presente percibido por el observador en el punto 0. Los sucesos que están fuera del cono de luz no nos afectan, y por lo tanto se dice de ellos que están situados en zonas del espacio-tiempo que no tienen relación de causalidad con la nuestra.

Imaginemos, por un momento, que en la galaxia Andrómeda, situada a 2,5 millones de años luz de nosotros, sucedió un cataclismo cósmico hace 100 000 años. Dado que, primero: la luz de Andrómeda tarda dos millones de años en llegar hasta nosotros y segundo: nada puede viajar a una velocidad superior a la de los fotones, es evidente, que no tenemos manera de enterarnos de lo que sucedió en dicha Galaxia hace tan solo 100 000 años. Se dice por lo tanto que el intervalo existente entre dicha hipotética catástrofe cósmica y nosotros, observadores del presente, es un intervalo espacial (formula_15), y por lo tanto, no puede afectar a los individuos que en el presente viven en la Tierra: Es decir, no existe relación de causalidad entre ese evento y nosotros.

El único problema con esta hipótesis, es que al entrar en un agujero negro, se anula el espacio tiempo, y como ya sabemos, algo que contenga algún volumen o masa, debe tener como mínimo un espacio donde ubicarse, el tiempo en ese caso, no tiene mayor importancia, pero el espacio juega un rol muy importante en la ubicación de volúmenes, por lo que esto resulta muy improbable, pero no imposible para la tecnología.

Podemos escoger otro episodio histórico todavía más ilustrativo: El de la estrella de Belén, tal y como fue interpretada por Johannes Kepler. Este astrónomo alemán consideraba que dicha estrella se identificaba con una supernova que tuvo lugar el año 5 a. C., cuya luz fue observada por los astrónomos chinos contemporáneos, y que vino precedida en los años anteriores por varias conjunciones planetarias en la constelación de Piscis. Esa supernova probablemente estalló hace miles de años atrás, pero su luz no llegó a la tierra hasta el año 5 a. C. De ahí que el intervalo existente entre dicho evento y las observaciones de los astrónomos egipcios y megalíticos (que tuvieron lugar varios siglos antes de Cristo) sea un "intervalo espacial", pues la radiación de la supernova nunca pudo llegarles. Por el contrario, la explosión de la supernova por un lado, y las observaciones realizadas por los tres magos en Babilonia y por los astrónomos chinos en el año 5 a. C. por el otro, están unidas entre sí por un "intervalo temporal", ya que la luz sí pudo alcanzar a dichos observadores.

El tiempo propio y el intervalo se relacionan mediante la siguiente equivalencia: formula_16, es decir, el intervalo es igual al tiempo local multiplicado por la velocidad de la luz. Una de las características tanto del tiempo local como del intervalo es su invarianza ante las transformaciones de coordenadas. Sea cual sea nuestro punto de referencia, sea cual sea nuestra velocidad, el intervalo entre un determinado evento y nosotros permanece invariante.

Esta invarianza se expresa a través de la llamada geometría hiperbólica: La ecuación del intervalo formula_17 tiene la estructura de una hipérbola sobre cuatro dimensiones, cuyo "término independiente" coincide con el valor del cuadrado del intervalo (formula_18), que como se acaba de decir en el párrafo anterior, es constante. Las "asíntotas" de la hipérbola vendrían a coincidir con el cono de luz.

En el espacio-tiempo de Minkowski, las propiedades cinemáticas de las partículas se representan fundamentalmente por tres magnitudes: La cuadrivelocidad (o tetravelocidad) , la cuadriaceleración y el cuadrimomentum (o tetramomentum).

La cuadrivelocidad es un cuadrivector tangente a la línea de universo de la partícula, relacionada con la velocidad coordenada de un cuerpo medida por un observador en reposo cualquiera, esta velocidad coordenada se define con la expresión newtoniana formula_19, donde formula_20 son el tiempo coordenado y las coordenadas espaciales medidas por el observador, para el cual la velocidad newtoniana ampliada vendría dada por formula_21. Sin embargo, esta medida newtoniana de la velocidad no resulta útil en teoría de la relatividad, porque las velocidades newtonianas medidas por diferentes observadores no son fácilmente relacionables por no ser magnitudes covariantes. Así en relatividad se introduce una modificación en las expresiones que dan cuenta de la velocidad, introduciendo un invariante relativista. Este invariante es precisamente el tiempo propio de la partícula que es fácilmente relacionable con el tiempo coordenado de diferentes observadores. Usando la relación entre tiempo propio y tiempo coordenado: formula_22 se define la cuadrivelocidad [propia] multiplicando por formula_23 las de la velocidad coordenada: formula_24.

La velocidad coordenada de un cuerpo con masa depende caprichosamente del sistema de referencia que escojamos, mientras que la cuadrivelocidad propia es una magnitud que se transforma de acuerdo con el principio de covariancia y tiene un valor siempre constante equivalente al intervalo dividido entre el tiempo propio (formula_25), o lo que es lo mismo, a la velocidad de la luz "c". Para partículas sin masa, como los fotones, el procedimiento anterior no se puede aplicar, y la cuadrivelocidad puede definirse simplemente como vector tangente a la trayectoria seguida por los mismos.

La cuadriaceleración puede ser definida como la derivada temporal de la cuadrivelocidad (formula_26). Su magnitud es igual a cero en los sistemas inerciales, cuyas líneas del mundo son geodésicas, rectas en el espacio-tiempo llano de Minkowski. Por el contrario, las líneas del mundo curvadas corresponden a partículas con aceleración diferente de cero, a sistemas no inerciales.

Junto con los principios de invarianza del intervalo y la cuadrivelocidad, juega un papel fundamental la ley de conservación del cuadrimomentum. Es aplicable aquí la definición newtoniana del momentum (formula_27) como la masa (en este caso conservada, formula_28) multiplicada por la velocidad (en este caso, la cuadrivelocidad), y por lo tanto sus componentes son los siguientes: formula_29, teniendo en cuenta que formula_30. La cantidad de momentum conservado es definida como la raíz cuadrada de la norma del vector de cuadrimomentum. El momentum conservado, al igual que el intervalo y la cuadrivelocidad propia, "permanece invariante ante las transformaciones de coordenadas", aunque también aquí hay que distinguir entre los cuerpos con masa y los fotones. En los primeros, la magnitud del cuadriomentum es igual a la "masa multiplicada por la velocidad de la luz" (formula_31). Por el contrario, el cuadrimomentum conservado de los fotones es igual a la magnitud de su "momentum tridimensional" (formula_32).

Como tanto la velocidad de la luz como el cuadrimomentum son magnitudes conservadas, también lo es su producto, al que se le da el nombre de energía conservada (formula_33), que en los cuerpos con masa equivale a la masa multiplicada por la velocidad de la luz al cuadrado (formula_34, la famosa fórmula de Einstein) y en los fotones al momentum multiplicado por la velocidad de la luz (formula_35)
Componentes formula_36
Magnitud del cuadrimomentum formula_37

Magnitud en cuerpos con masa formula_38
Magnitud en fotones (masa = 0) formula_39
Energía formula_40

Energía en cuerpos con masa (cuerpos en reposo, p=0) formula_41
Energía en fotones (masa en reposo = 0) formula_42
La aparición de la Relatividad Especial puso fin a la secular disputa que mantenían en el seno de la mecánica clásica las escuelas de los mecanicistas y los energetistas. Los primeros sostenían, siguiendo a Descartes y Huygens, que la magnitud conservada en todo movimiento venía constituida por el momentum total del sistema, mientras que los energetistas -que tomaban por base los estudios de Leibniz- consideraban que la magnitud conservada venía conformada por la suma de dos cantidades: La "fuerza viva", equivalente a la mitad de la masa multiplicada por la velocidad al cuadrado (formula_43) a la que hoy denominaríamos "energía cinética", y la "fuerza muerta", equivalente a la altura por la constante "g" (formula_44), que correspondería a la "energía potencial". Fue el físico alemán Hermann von Helmholtz el que primero dio a la "fuerzas leibnizianas" la denominación genérica de energía y el que formuló la "Ley de conservación de la energía", que no se restringe a la mecánica, que se extiende también a otras disciplinas físicas como la termodinámica.

La mecánica newtoniana dio la razón a ambos postulados, afirmando que tanto el momentum como la energía son magnitudes conservadas en todo movimiento sometido a fuerzas conservativas. Sin embargo, la Relatividad Especial dio un paso más allá, por cuanto a partir de los trabajos de Einstein y Minkowski el momentum y la energía dejaron de ser considerados como entidades independientes y se les pasó a considerar como dos aspectos, dos facetas de una única magnitud conservada: el cuadrimomentum.

Tres son las ecuaciones fundamentales que en física newtoniana describen el fenómeno de la "gravitación universal": la primera, afirma que la fuerza gravitatoria entre dos cuerpos es proporcional al producto de sus masas e inversamente proporcional al cuadrado de su distancia (1); la segunda, que el potencial gravitatorio (formula_45) en un determinado punto es igual a la masa multiplicada por la constante "G" y dividida por la distancia "r" (2); y la tercera, finalmente, es la llamada ecuación de Poisson (3), que indica que el laplaciano del potencial gravitatorio es igual a formula_46, donde formula_47 es la densidad de masa en una determinada región esférica.

Sin embargo, estas ecuaciones no son compatibles con la Relatividad Especial por dos razones:

Por todo ello, resulta necesario prescindir del término formula_47, situado en el lado derecho de la fórmula de Poisson y sustituirlo por un objeto geométrico-matemático que permanezca invariante ante las transformaciones de Lorentz: Dicho objeto fue definido por Einstein en sus ecuaciones de universo y recibe el nombre de "tensor de energía-momentum" (formula_50). Sus coeficientes describen la cantidad de tetramomentum formula_51 que atraviesa una hipersuperficie formula_52, normal al vector unitario formula_53. De este modo, el tensor de energía momentum puede expresarse mediante la siguiente ecuación:

O lo que es lo mismo: El componente formula_51 del tetramomentum es igual a la integral de hipersuperficie formula_55 del tensor de tensión-energía. En un fluido ideal, del que están ausentes tanto la viscosidad como la conducción de calor, los componentes del tetramomentum se calculan de la siguiente forma:

donde formula_47 es la densidad de masa-energía (masa por unidad de volumen tridimensional), formula_57 es la presión hidrostática, formula_58 es la cuadrivelocidad del fluido, y formula_59 es la matriz inversa del tensor métrico de la variedad.

Además, si los componentes del tensor se miden por un observador en reposo relativo respecto al fluido, entonces, el tensor métrico viene constituido simplemente por la métrica de Minkowski:

Puesto que además la tetravelocidad del fluido respecto al observador en reposo es:

como consecuencia de ello, los coeficientes del tensor de tensión-energía son los siguientes:

Donde formula_47 es la densidad de masa, y formula_61 son los componentes tridimensionales de la presión hidrostática. Como vemos, el campo gravitatorio tiene dos fuentes diferentes: La masa y el momentum del fluido en cuestión. Los efectos gravitatorios originados por la masa se denominan efectos gravitoeléctricos, mientras que aquellos que se deben al momentum reciben el nombre de efectos gravitomagnéticos. Los primeros tienen una intensidad formula_62 superior a los segundos, que solo se manifiestan en aquellos casos en los que las partículas del fluido se mueven con una velocidad cercana a la de la luz (se habla entonces de "fluidos relativistas"): Es el caso de los chorros ("jets") que emanan del centro de la galaxia y que se propulsan en las dos direcciones marcadas por el eje de rotación de este cuerpo cósmico; de la materia que se precipita hacia un agujero negro; y del fluido estelar que se dirige hacia el centro de la estrella cuando esta entra en colapso. En este último caso, durante las fases finales del proceso de contracción de la estrella, la presión hidrostática puede llegar a ser tan fuerte como para llegar a acelerar el colapso, en lugar de ralentizarlo.

Podemos, a partir del tensor de tensión-energía, calcular cuánta masa contiene un determinado volumen del fluido: Retomando la definición de este tensor expuesta unas líneas más arriba, se puede definir al coeficiente formula_63 como la cantidad de momentum formula_64 (esto es, la masa) que atraviesa la hipersuperficie formula_65. En el espacio-tiempo de Minkowski, la hipersuperficie formula_65 es aquella región que se define por las tres bases vectoriales normales al vector formula_67: formula_68 es, por tanto, un volumen tridimensional, definido por los vectores base formula_69 (eje "x"), formula_70 (eje "y"), y formula_71 (eje "z"). Podemos por tanto escribir:

Del mismo modo, es posible deducir matemáticamente a partir del tensor de tensión-energía la definición newtoniana de presión, introduciendo en la mentada ecuación cualquier par de índices que sean diferentes de cero:

La hipersuperficie formula_75 es aquella región del espacio-tiempo definida por los tres vectores unitarios normales a formula_76 (se trata de los dos vectores espaciales, formula_70 y formula_78, correspondientes a los ejes "y" y "z"; y del vector temporal formula_79 —o formula_80, como se prefiera—). Esta definición nos permite descomponer la integral de hipersuperficie en una integral temporal (cuyo integrando viene definido por formula_80) y otra de superficie (esta vez bidimensional, formula_82):

Finalmente, derivamos parcialmente ambos miembros de la ecuación respecto al tiempo, y teniendo en cuenta que la fuerza no es más que la tasa de incremento temporal del momentum obtenemos el resultado siguiente:

Que contiene la definición newtoniana de la presión como fuerza ejercida por unidad de superficie.

Las ecuaciones deducidas por el físico escocés James Clerk Maxwell demostraron que electricidad y magnetismo no son más que dos manifestaciones de un mismo fenómeno físico: el campo electromagnético. Ahora bien, para describir las propiedades de este campo los físicos de finales del siglo XIX debían utilizar dos vectores diferentes, los correspondientes los campos eléctrico y magnético.

Fue la llegada de la Relatividad Especial la que permitió describir las propiedades del electromagnetismo con un solo objeto geométrico, el "vector cuadripotencial", cuyo componente temporal se correspondía con el potencial eléctrico, mientras que sus componentes espaciales eran los mismos que los del potencial magnético.

De este modo, el campo eléctrico puede ser entendido como la suma del gradiente del potencial eléctrico más la derivada temporal del potencial magnético:

y el campo magnético, como el rotacional del potencial magnético:

Las propiedades del campo electromagnético pueden también expresarse utilizando un tensor de segundo orden denominado tensor de Faraday y que se obtiene diferenciando exteriormente al vector cuadripotencial formula_88

formula_90

La fuerza de Lorentz puede deducirse a partir de la siguiente expresión:

Donde "q" es la carga y formula_93 la cuadrivelocidad de la partícula.





</doc>
<doc id="3246" url="https://es.wikipedia.org/wiki?curid=3246" title="Historia de la biología">
Historia de la biología

La historia de la biología remonta el estudio de los seres vivos desde la Antigüedad hasta la época actual. Aunque el concepto de biología como ciencia en si misma nace en el siglo XIX, las ciencias biológicas surgieron de tradiciones médicas e historia natural que se remontan a el "Āyurveda", la medicina en el Antiguo Egipto y los trabajos de Aristóteles y Galeno en el antiguo mundo grecorromano. Estos trabajos de la Antigüedad siguieron desarrollándose en la Edad Media por médicos y eruditos musulmanes como Avicena. Durante el Renacimiento europeo y a principios de la Edad Moderna el pensamiento biológico experimentó una revolución en Europa, con un renovado interés hacia el empirismo y por el descubrimiento de gran cantidad de nuevos organismos. Figuras prominentes de este movimiento fueron Vesalio y Harvey, que utilizaron la experimentación y la observación cuidadosa en la fisiología, y naturalistas como Linneo y Buffon que iniciaron la clasificación de la diversidad de la vida y el registro fósil, así como el desarrollo y el comportamiento de los organismos. La microscopía reveló el mundo, antes desconocido, de los microorganismos, sentando las bases de la teoría celular. La importancia creciente de la teología natural, en parte una respuesta al alza de la filosofía mecánica, y la pérdida de fuerza del argumento teleológico impulsó el crecimiento de la historia natural.

Durante los siglos XVIII y XIX, las ciencias biológicas, como la botánica y la zoología se convirtieron en disciplinas científicas cada vez más profesionales. Lavoisier y otros científicos físicos comenzaron a unir los mundos animados e inanimados a través de la física y química. Los exploradores-naturalistas, como Alexander von Humboldt investigaron la interacción entre organismos y su entorno, y los modos en que esta relación depende de la situación geográfica, iniciando así la biogeografía, la ecología y la etología. Los naturalistas comenzaron a rechazar el esencialismo y a considerar la importancia de la extinción y la mutabilidad de las especies. La teoría celular proporcionó una nueva perspectiva sobre los fundamentos de la vida. Estas investigaciones, así como los resultados obtenidos en los campos de la embriología y la paleontología, fueron sintetizados en la teoría de la evolución por selección natural de Charles Darwin. El final del siglo XIX vio la caída de la teoría de la generación espontánea y el nacimiento de la teoría microbiana de la enfermedad, aunque el mecanismo de la herencia genética fuera todavía un misterio.

A principios del siglo XX, el redescubrimiento del trabajo de Mendel condujo al rápido desarrollo de la genética por parte de Thomas Hunt Morgan y sus discípulos y la combinación de la genética de poblaciones y la selección natural en la síntesis evolutiva moderna durante los años 1930. Nuevas disciplinas se desarrollaron con rapidez, sobre todo después de que Watson y Crick descubrieron la estructura del ADN. Tras el establecimiento del dogma central de la biología molecular y el descifrado del código genético, la biología se dividió fundamentalmente entre la biología orgánica —los campos que trabajan con organismos completos y grupos de organismos— y los campos relacionados con la biología molecular y celular. A finales del siglo XX nuevos campos como la genómica y la proteómica invertían esta tendencia, con biólogos orgánicos que usan técnicas moleculares, y biólogos moleculares y celulares que investigan la interacción entre genes y el entorno, así como la genética de poblaciones naturales de organismos.

La palabra biología está formada por la combinación de los términos griegos βίος "bios", vida, y el sufijo -λογία , ciencia, tratado, estudio, basado en el verbo griego λέγειν ("legein"), seleccionar, reunir ("cf." el nombre λόγος "logos", palabra). El término "biología" en su sentido actual se cree que fue introducido de forma independiente por Karl Friedrich Burdach (en 1800), Gottfried Reinhold Treviranus ("Biologie oder Philosophie der lebenden Natur", 1802) y Jean-Baptiste Lamarck ("Hydrogéologie", 1802). La palabra en si misma ya aparece en el título del volumen 3 de "Philosophiae naturalis sive physicae dogmaticae": «Geologia, biologia, phytologia generalis et dendrologia», de Michael Christoph Hanow, publicado en 1766.

Con anterioridad se utilizaron distintos términos para el estudio de animales y plantas. "Historia natural" se utilizó para referirse a los aspectos descriptivos de la biología, aunque también incluía la mineralogía y otros campos no biológicos; de la Edad Media al Renacimiento, el marco de unificación de la historia natural era la "scala naturae" o cadena de los seres. Filosofía natural y teología natural englobaban la base conceptual y metafísica de planta y vida animal, tratando con problemas como por qué los organismos existen y se comportan del modo en que lo hacen, aunque estas materias también incluían lo que es en la actualidad la geología, la física, la química y la astronomía. La fisiología y la farmacología botánica eran de la incumbencia de la medicina. "Botánica", "zoología" y (en el caso de los fósiles) "geología" sustituyeron a la historia natural y a la filosofía natural en los siglos XVIII y XIX antes de que "biología" se adoptara mayoritariamente. En la actualidad "botánica" y "zoología" son términos utilizados de forma generalizada, aunque se les han añadido otras subdisciplinas de la biología, como la micología y la biología molecular.

Los primeros humanos deben haber tenido y transmitido el conocimiento sobre plantas y animales para aumentar sus posibilidades de supervivencia y probablemente tendrían también conocimientos sobre anatomía humana y animal y sobre algunos aspectos del comportamiento animal (como modelos de migración). Sin embargo, el primer paso decisivo en el conocimiento biológico vino con la revolución neolítica hace aproximadamente 10 000 años. Los humanos primero cultivaron plantas para la agricultura y posteriormente animales como ganado para acompañar a las sociedades sedentarias resultantes.

Las antiguas culturas de Mesopotamia, Egipto, el subcontinente indio y China, entre otras, dieron pie al nacimiento de renombrados cirujanos y estudiosos de las ciencias naturales como Sushruta o Zhang Zhong Jing, que reflejaron sofisticados sistemas independientes de la filosofía natural. Sin embargo, generalmente las raíces de la biología moderna se remontan a la tradición secular de la filosofía griega antigua.

Uno de los sistemas organizados más antiguos de la medicina se sitúa en el subcontinente indio en la forma del Āyurveda, proveniente del "Átharva Vedá" (uno de los cuatro libros más antiguos de conocimiento y cultura india) alrededor del 1500 a. C. Otros textos médicos antiguos surgen del Antiguo Egipto, como el papiro Edwin Smith; esta cultura también es conocida por desarrollar el proceso de embalsamamiento, que se utilizaba para la momificación, a fin de conservar el cuerpo humano y prevenir la descomposición. En la antigua China se pueden encontrar temas biológicos dispersos a través de varias disciplinas diferentes, como los trabajos de herbólogos, médicos, alquimistas y filósofos. La tradición taoísta de la alquimia china, por ejemplo, puede considerarse parte de las ciencias de la vida debido a su énfasis en la salud (con el objetivo último de obtener el «elixir de la vida»). El sistema de la medicina china clásica por lo general giraba en torno a la teoría del yin y yang y de los cinco elementos. Los filósofos taoístas, como Zhuangzi en el siglo IV a. C., también expresan ideas relacionadas con la evolución, como negar la persistencia o continuidad de las especies biológicas y especulando que las especies habían desarrollado atributos diferenciadores en respuesta a distintos ambientes.

La antigua tradición india del Ayurveda desarrolló independientemente el concepto de los tres humores, que se asemejaba al de los cuatro humores de la medicina en la Antigua Grecia, aunque el sistema ayurvédico incluía complejidades adicionales, como que el cuerpo estaba formado por cinco elementos y siete tejidos básicos. Los escritores de esta tradición también clasificaron a las criaturas en cuatro categorías basadas en el método utilizado para su nacimiento (útero, huevo, calor/humedad y semilla) y explicaron la concepción de un feto de forma detallada; también progresaron en el campo de cirugía, a menudo sin la utilización de la disección de humanos o la vivisección de animales. Uno de los tratados ayurvédicos más antiguos fue el "Sushruta Samhita", atribuido a Sushruta, en el siglo VI a. C., que también fue una temprana farmacopea y describía 700 plantas medicinales, 64 preparaciones de fuentes minerales y 57 preparaciones de origen animal.

Los filósofos presocráticos se hicieron muchas preguntas sobre la vida, si bien produjeron poco conocimiento sistemático en torno a temas específicamente biológicos; no obstante, los intentos de los atomistas para explicar la vida en términos puramente físicos aparecerán recurrentemente a lo largo de toda la historia de la biología. Sin embargo, las teorías médicas de Hipócrates y sus discípulos, especialmente el humorismo, tuvieron un gran impacto.

El filósofo Aristóteles fue el estudioso del mundo orgánico más influyente de la Antigüedad. Aunque sus primeros trabajos en la filosofía natural fueron especulativos, las escrituras biológicas posteriores de Aristóteles eran más empíricas, centrándose en la causalidad biológica y la diversidad de la vida. Hizo innumerables observaciones de la naturaleza, sobre todo sobre los hábitos y los atributos de las plantas y animales de su alrededor, con una especial atención a la categorización. En total Aristóteles clasificó 540 especies de animales y diseccionó al menos 50. Creía que los objetivos intelectuales y las causas formales dirigían todos los procesos naturales.

Aristóteles y casi todos los eruditos occidentales posteriores a él hasta el siglo XVIII, creían que las criaturas se organizaban en una escala graduada de perfección que se eleva desde las plantas hasta los humanos: la "scala naturae" (escala natural) o cadena de los seres. El sucesor de Aristóteles en el Liceo, Teofrasto, escribió una serie de libros sobre la botánica ("De historia plantarum"), que sobrevivió como la contribución más importante de la Antigüedad a la botánica hasta la Edad Media. Muchos de los nombres de Teofrasto sobreviven en la actualidad, como "carpos" para la fruta, y "pericarpio" para la parte del fruto que recubre su semilla. Dioscórides escribió una pionera farmacopea enciclopédica, "De materia medica", que incorporaba descripciones de unas 600 plantas y sus usos en la medicina. Plinio el Viejo también fue reconocido por su conocimiento de las plantas y la naturaleza con obras como "Naturalis historia", y fue un prolífico compilador de descripciones zoológicas.

Algunos eruditos del período helenístico bajo la Dinastía Ptolemaica (en especial Herófilo de Calcedonia y Erasístrato) corrigieron el trabajo fisiológico de Aristóteles, realizando incluso disecciones y vivisecciones. Galeno de Pérgamo se convirtió en la autoridad más importante en medicina y anatomía. Aunque algunos atomistas antiguos como Lucrecio desafiaran el punto de vista teleológico aristotélico de que todos los aspectos de la vida son el resultado de un diseño u objetivo, la teleología y la teología natural permanecerían en el centro del pensamiento biológico hasta los siglos XVIII y XIX. Ernst Mayr manifestó que «Nada realmente importante pasó en la biología después de Lucrecio y Galeno hasta el Renacimiento». Las ideas de las tradiciones griegas sobre la historia natural y la medicina sobrevivieron, y por lo general no fueron cuestionadas en la Europa medieval.

La decadencia del Imperio romano llevó a la desaparición o la destrucción de gran cantidad de conocimiento, aunque los médicos todavía incorporaban muchos aspectos de la tradición griega en formación y práctica. En Bizancio y el mundo islámico, muchos de los trabajos griegos fueron traducidos al árabe y muchos de los trabajos de Aristóteles fueron preservados.

Los médicos, los científicos y los filósofos musulmanes medievales hicieron contribuciones significativas al conocimiento biológico entre los siglos VIII y XIII, durante lo que se conoce como la «Edad de Oro del islam». En zoología, por ejemplo, el erudito afroárabe Al-Jahiz (781-869) describió algunas de las primeras ideas evolutivas, como la lucha por la existencia. También introdujo la idea de una cadena alimentaria, y fue un temprano partidario del determinismo geográfico. El biólogo kurdo Al-Dinawari (828–896) está considerado el fundador de la botánica árabe por su "Libro de las plantas", en el que describió al menos 637 especies y trató sobre el desarrollo de las plantas desde la germinación hasta la muerte, describiendo las fases de su crecimiento y la producción de flores y frutos. Al-Biruni describió el concepto de la selección artificial y sostuvo que la naturaleza trabaja más o menos de la misma forma, una idea que ha sido comparada con la selección natural.

En medicina experimental, el médico persa Avicena (980-1037) introdujo los ensayos clínicos y la farmacología clínica en su enciclopedia "El canon de medicina", que se utilizó como texto de referencia para la enseñanza médica europea hasta el siglo XVII. El médico andalusí Avenzoar (1091-1161) fue un temprano partidario de la disección experimental y la autopsia, que utilizó para demostrar que la enfermedad de la piel conocida como sarna era causada por un parásito, un descubrimiento que desestabilizaba la teoría del humorismo. También introdujo la cirugía experimental, y utilizó la experimentación con animales para probar técnicas quirúrgicas antes de su utilización con humanos. Durante una hambruna en Egipto en 1200, Abd al-Latif al-Baghdadi observó y examinó un gran número de esqueletos, y descubrió que Galeno había hecho una descripción incorrecta de la formación de los huesos de la mandíbula y el sacro.

A principios del siglo XIII el biólogo andalusí Abu al-Abbas al-Nabati fue uno de los primeros en utilizar el método científico en la botánica, introduciendo técnicas empíricas y experimentales en las pruebas, descripción e identificación de elementos de farmacopea, y separación de informes no verificados de aquellos apoyados por pruebas y observaciones. Su alumno Ibn al-Baitar (1190?-1248) escribió una enciclopedia farmacéutica que describía 1400 plantas, alimentos y medicinas, 300 de las cuales eran descubrimientos realizados por él mismo; una traducción al latín de su trabajo fue utilizada por biólogos y farmacéuticos europeos durante los siglos XVIII y XIX.

El médico árabe Ibn Nafis (1213-1288) fue otro de los primeros partidarios de la disección experimental y la autopsia, quien en 1242 descubrió la circulación pulmonar y la circulación coronaria, que forman la base del sistema circulatorio; también describió el concepto de metabolismo, pulso, huesos, músculos, intestinos, órganos sensoriales, bilis, esófago y estómago.

Durante la Alta Edad Media algunos eruditos europeos, como Hildegarda de Bingen, Alberto Magno y Federico II, ampliaron el catálogo de la historia natural. El nacimiento de las universidades europeas, aunque importante para el desarrollo de la física y la filosofía, tuvo poco impacto en el estudio de la biología.

El Renacimiento europeo trajo consigo un nuevo interés por la historia natural y la fisiología empíricas. En 1543 Andrés Vesalio iniciaba una nueva era en la medicina occidental con la publicación de su seminal tratado de anatomía humana "De humani corporis fabrica", que estaba basado en la disección de cadáveres. Vesalio fue el primero de una serie de anatomistas que gradualmente reemplazó la escolástica por el empirismo en la fisiología y la medicina, basándose en la experiencia propia y no en la autoridad y el razonamiento abstracto. A través del herbalismo, la medicina se convirtió en una fuente indirecta para el estudio empírico de las plantas. Otto Brunfels, Hieronymus Tragus y Leonhart Fuchs fueron prolíficos escritores sobre plantas silvestres, el principio de un acercamiento basado en la naturaleza a la gran variedad de la vida vegetal. Los bestiarios, un género que combinaba el conocimiento natural y figurativo sobre los animales, también se hicieron más sofisticados, especialmente gracias al trabajo de William Turner, Pierre Belon, Guillaume Rondelet, Conrad von Gesner y Ulisse Aldrovandi.

Artistas como Alberto Durero y Leonardo da Vinci, que a menudo trabajaron con naturalistas, también estuvieron interesados en el cuerpo de animales y humanos, estudiando la fisiología en detalle y contribuyendo así al progreso del conocimiento anatómico. La alquimia, especialmente en la obra de Paracelso, también contribuyó al conocimiento de los seres vivos; los alquimistas sometieron la materia orgánica al análisis químico y experimentaron profusamente tanto con la farmacología biológica como mineral. Estos estudios formaban parte de una transición más importante en la visión del mundo (el nacimiento de la filosofía mecánica) que continuó hasta el siglo XVII, cuando la metáfora tradicional de la «naturaleza como organismo» fue remplazada por la «naturaleza como máquina».

La sistematización, descripción y clasificación dominó la historia natural a lo largo de la mayor parte de los siglos XVII y XVIII. Carlos Linneo publicó una taxonomía básica para el mundo natural en 1736 (variaciones de la misma se han seguido utilizando hasta la actualidad), y en los años 1750 introdujo la nomenclatura binominal para todas sus especies. Mientras que Linneo concebía las especies como partes invariables de una jerarquía diseñada, el otro gran naturalista del siglo XVIII, Georges Louis Leclerc, conde de Buffon, trató a las especies como categorías artificiales y a las formas vivas como maleables (incluso la posibilidad de un origen común). Aunque estaba en contra de la evolución, Buffon fue una figura clave en la historia del pensamiento evolutivo; su trabajo influiría en las teorías evolutivas tanto de Lamarck como de Darwin.

El descubrimiento y la descripción de nuevas especies y la recogida de especímenes se convirtieron en una pasión de caballeros científicos y un lucrativo negocio para empresarios; muchos naturalistas viajaron por todo el mundo en busca de conocimiento científico y aventuras.
Ampliando el trabajo de Vesalio en experimentos en cuerpos todavía vivos (tanto de personas como de animales), William Harvey y otros filósofos naturales investigaron el papel de la sangre, las venas y las arterias. En 1628 el "Exercitatio anatomica de motu cordis et sanguinis in animalibus" (Ejercicio anatómico sobre el movimiento del corazón y de la sangre en animales) de Harvey fue el principio del fin para la teoría galénica, que junto a los estudios sobre el metabolismo de Santorio Santorio, sirvió como modelo de acercamiento cuantitativo a fisiología.

A principios del siglo XVII, el micromundo de la biología comenzaba a ampliarse. Algunos fabricantes de lentes y filósofos naturales habían estado creando rudimentarios microscopios desde finales del siglo XVI, y Robert Hooke publicó el seminal "Micrographia" basado en observaciones realizadas con su propio microscopio realizado en 1665. Pero no fue hasta las significativas mejoras en la fabricación de lentes introducidas por Anton van Leeuwenhoek a finales de los años 1670 (que consiguieron una ampliación de 200 aumentos de con una única lente), cuando los eruditos descubrieron los espermatozoides, las bacterias, los infusorios y la compleja diversidad de la vida microscópica. Investigaciones similares por parte de Jan Swammerdam conllevaron un nuevo interés hacia la entomología y establecieron las técnicas básicas de la disección microscópica y la tinción.
Mientras que el mundo microscópico se ampliaba, el mundo macroscópico se reducía. Botánicos como John Ray trabajaron para incluir la avalancha de nuevos organismos recién descubiertos provenientes de todo el globo en una taxonomía coherente y en una teología racional. El debate sobre el Diluvio universal catalizó el desarrollo de la paleontología; en 1669 Niels Stensen publicó un ensayo sobre como los restos de organismos vivos podrían quedar atrapados en capas de sedimento y mineralizarse para producir fósiles. Aunque las ideas de Stensen sobre la fosilización fueran conocidas y ampliamente debatidas entre filósofos naturales, un origen orgánico de los fósiles no sería aceptado por todos los naturalistas hasta finales del siglo XVIII debido al debate filosófico y teológico sobre cuestiones como la edad de la Tierra y la extinción.

Durante el siglo XIX, el ámbito de biología estaba dividido fundamentalmente entre la medicina, que investigaba sobre cuestiones de forma y función, e historia natural, que estudiaba la diversidad de la vida y las interacciones entre distintas formas de vida y entre la vida y la no vida. Hacia 1900, la mayor parte de estas áreas se superpuso, mientras la historia natural (y su equivalente filosofía natural) había cedido el paso en gran parte a disciplinas científicas especializadas, como la bacteriología, la morfología, la embriología, la geografía y la geología.

Los numerosos viajes emprendidos por naturalistas a principios y mediados del siglo XIX produjeron una gran cantidad de información novedosa sobre la diversidad y la distribución de los organismos vivos. De particular importancia fue el trabajo de Alexander von Humboldt, que analizó la relación entre organismos y su ambiente (el campo de la historia natural) utilizando los métodos cuantitativos de la filosofía natural (es decir, física y química). El trabajo de Humboldt estableció las bases de la biogeografía e inspiró a varias generaciones de científicos.

La emergente disciplina de la geología acercó a la historia natural y a la filosofía natural; el establecimiento de la columna estratigráfica unió la distribución espacial de los organismos a su distribución temporal, un precursor clave para la noción de la evolución. Georges Cuvier y otros dieron un gran paso en anatomía comparada y paleontología a finales de los años 1790 y principios de los años 1800. En una serie de conferencias y ensayos que hacían comparaciones detalladas entre mamíferos vivientes y fósiles, Cuvier fue capaz de establecer que los fósiles eran restos de especies que se habían extinguido, en lugar de corresponder a restos de especies todavía vivas en otras partes del mundo, tal como se creía por entonces. Los fósiles descubiertos y descritos por Gideon Mantell, William Buckland, Mary Anning y Richard Owen, entre otros, ayudaron a establecer que existió una «edad de los reptiles» y que éstos habían precedido incluso a los mamíferos prehistóricos. Estos descubrimientos captaron el interés público y dirigieron la atención hacia la historia de la vida en la Tierra. La mayor parte de estos geólogos sostenían la teoría del catastrofismo, pero el influyente "Principles of Geology" (1830) de Charles Lyell popularizó el uniformismo de Hutton, una teoría que explicaba en igualdad de términos el pasado y el presente geológico.

La teoría evolutiva más significativa antes de Darwin fue la de Jean-Baptiste Lamarck; basada en la transmisión de caracteres adquiridos (un mecanismo de herencia que fue ampliamente aceptado hasta el siglo XX), describió una cadena de desarrollo que se extiende desde el más ínfimo microbio hasta los seres humanos. El naturalista británico Charles Darwin, combinando la metodología de la biogeografía de Humboldt, la geología uniformista de Lyell, los trabajos de Thomas Malthus sobre el crecimiento demográfico y su propio conocimiento morfológico, crearon una teoría evolutiva más acertada basada en la selección natural; pruebas similares realizadas de forma independiente llevaron a Alfred Russel Wallace a alcanzar las mismas conclusiones.

La publicación en 1859 de la teoría de Darwin en "El origen de las especies" está considerado como el principal acontecimiento en la historia de la biología moderna. La credibilidad establecida de Darwin como naturalista, el tono sobrio del trabajo, y sobre todo la depurada fuerza y volumen de pruebas presentado, permitió a "El origen" tener éxito donde los trabajos evolutivos anteriores habían fallado, como el libro de Robert Chambers "Vestiges of the Natural History of Creation". La mayor parte de científicos aceptaron la evolución y el origen común hacia finales del siglo XIX, sin embargo, la selección natural no sería aceptada como el mecanismo primario de la evolución hasta bien entrado el siglo XX, cuando la mayoría de las teorías contemporáneas sobre la herencia parecieron incompatibles con la herencia de la variación aleatoria.

Wallace, siguiendo los trabajos anteriores de de Candolle, Humboldt y Darwin, realizó importantes contribuciones a la zoogeografía. Debido a su interés en la hipótesis de la transmutación, prestó particular atención a la distribución geográfica de las especies estrechamente relacionadas durante su trabajo de campo primero en América del Sur y después en el archipiélago malayo. Durante su estancia en el archipiélago identificó la llamada línea de Wallace, que discurre a través de las Molucas dividiendo la fauna del archipiélago entre una zona asiática y una zona nuevoguineana/australiana. Su pregunta clave, en cuanto a porqué la fauna de las islas con climas similares puede llegar a ser tan diferente, solo podía responderse considerando su origen. En 1876 escribió "The Geographical Distribution of Animals", que se convirtió en el trabajo de referencia estándar durante medio siglo, y una secuela, "Island Life", en 1880 que se centraba en la biogeografía insular. Amplió el sistema de seis regiones desarrollado por Philip Sclater para describir la distribución geográfica de las aves a los animales en general. Su método de tabular datos sobre los grupos animales en zonas geográficas destacó las discontinuidades y su apreciación sobre la evolución permitió que propusiera explicaciones racionales que no habían sido realizadas con anterioridad.

El estudio científico de la herencia genética creció rápidamente como consecuencia del "Origen de las especies" de Darwin con los trabajos de Francis Galton y los biométricos. El origen de la genética generalmente se asocia al trabajo de 1866 del monje agustino Gregor Mendel que sería conocido posteriormente como las Leyes de Mendel. Sin embargo, su trabajo no fue reconocido como significativo hasta 35 años después. Mientras tanto, una variedad de teorías de la herencia (basadas en la pangénesis, ortogénesis y otros mecanismos) fue debatida e investigada enérgicamente. La embriología y la ecología también se convirtieron en importantes campos biológicos, especialmente unidos a la evolución y popularizados por el trabajo de Ernst Haeckel. Sin embargo la mayor parte del trabajo del siglo XIX sobre la herencia no estaba en la esfera de la historia natural, sino en la de la fisiología experimental.

A lo largo del siglo XIX el alcance de fisiología se amplió en gran medida, de un campo fundamentalmente orientado a la medicina a una amplia investigación de los procesos físicos y químicos de la vida, incluidas plantas, animales e incluso microorganismo, además del hombre. "Seres vivos como máquinas" se convirtió en una metáfora dominante en el pensamiento biológico y social.

El desarrollo de la microscopía tuvo un profundo impacto en el pensamiento biológico. A principios del siglo, varios biólogos señalaron a la importancia fundamental de la célula. En 1838 y 1839, Schleiden y Schwann empezaron a promover la teoría según la cual (1) la unidad básica de los organismos es la célula, (2) las células individuales tienen todas las características de la vida, aunque se opusieran a la idea que (3) todas las células proceden de otras células. Gracias al trabajo de Robert Remak y Rudolf Virchow se aceptaron definitivamente entre la comunidad científica todas las tesis de la teoría celular.

La teoría celular obligó a los biólogos a volver a imaginar a los organismos individuales como conjuntos interdependientes de células individuales. Los científicos del emergente campo de la citología, armados con microscopios cada vez más potentes y con los nuevos métodos de tinción, pronto descubrieron que incluso las células individuales eran mucho más complejas que las cámaras llenas de fluido homogéneo descritas anteriormente por los microscopistas. Robert Brown había descrito el núcleo celular en 1831, y a finales del siglo XIX los citólogos ya habían identificado muchos de los componentes fundamentales de las células: cromosomas, centrosomas, mitocondrias, cloroplastos y otras estructuras se hacen visibles a través de la tinción. Entre 1874 y 1884 Walther Flemming describió las distintas fases de la mitosis, demostrando que no eran artefactos de la tinción, sino que ocurrían en las células vivas, y además que los cromosomas se duplicaban en número justo antes de la división celular y de la producción de una célula hija. Gran parte de la investigación sobre la reproducción celular se reunió en la teoría de August Weismann de la herencia: identificó el núcleo como el material hereditario, propuso la distinción entre células somáticas y células germinales (argumentando que el número de cromosomas se debe reducir a la mitad para las células germinales, un precursor del concepto de la meiosis), y adoptó la teoría de Hugo de Vries sobre la pangénesis. El weismannismo fue muy influyente, especialmente en el nuevo campo de la embriología experimental.

A mediados de 1850 la teoría miasmática de la enfermedad fue ampliamente superada por la teoría microbiana, creando un gran interés en los microorganismos y sus interacciones con otras formas de vida. En la década de 1880 la bacteriología se estaba convirtiendo en una disciplina coherente, especialmente a través de la obra de Robert Koch, quien introdujo métodos para el crecimiento de cultivos puros en placas de Petri con nutrientes específicos en gelatina de agar. La antigua idea de que los organismos vivos podrían originarse a partir de materia inanimada (generación espontánea) fue embestida por una serie de experimentos realizados por Louis Pasteur, mientras que los debates del vitalismo frente al mecanicismo (un tema perenne desde la época de Aristóteles y los atomistas griegos) continuaban con vehemencia.

En el campo de la química una cuestión fundamental era la distinción entre sustancias orgánicas e inorgánicas, sobre todo en el contexto de transformaciones orgánicas como la fermentación y la putrefacción. Desde Aristóteles, estos habían sido considerados procesos esencialmente biológicos ("vitales"), sin embargo, Friedrich Wöhler, Justus Liebig y otros pioneros del ascendente campo de la química orgánica (a partir de los trabajos de Lavoisier) demostraron que el mundo orgánico a menudo puede ser analizado por métodos físicos y químicos. En 1828 Wöhler demostró que una sustancia orgánica como la urea puede ser creada por medios químicos que no tienen que ver con la vida, poniendo en tela de juicio al vitalismo. Comenzando con la diastasa en 1833, se descubrieron extractos de célula («fermentos») que podría afectar las transformaciones químicas. A finales del siglo XIX se estableció el concepto de las enzimas, aunque las ecuaciones de la cinética química no se aplicarían a las reacciones enzimáticas hasta principios del siglo XX.

Fisiólogos como Claude Bernard exploraron (a través de la vivisección y otros métodos experimentales) las funciones físicas y químicas de los cuerpos vivos en un grado sin precedentes, sentando las bases para la endocrinología (un campo que se desarrolló rápidamente después del descubrimiento de la primera hormona, la secretina, en 1902), la biomecánica y el estudio de la nutrición y la digestión. La importancia y diversidad de los métodos de la fisiología experimental, en el seno de la medicina y la biología, creció de forma drástica durante la segunda mitad del siglo XIX. El control y la manipulación de los procesos de la vida se convirtió en una preocupación fundamental, y el experimento se situó en el centro de la educación biológica.

A principios del siglo XX la investigación biológica era en gran medida una tarea profesional. La mayor parte del trabajo todavía se realizaba al modo de la historia natural, que enfatizaba al análisis morfológico y filogenético por sobre las explicaciones causales basadas en experimentos. Sin embargo, los fisiólogos experimentales y embriólogos antivitalistas, especialmente en Europa, fueron cada vez más influyentes. El gran éxito de los enfoques experimentales hacia el desarrollo, la herencia y el metabolismo en las décadas de 1900 y 1910 demostró el poder de la experimentación en la biología. En las décadas siguientes, el trabajo experimental sustituyó a la historia natural como el método dominante de investigación.

A principios del siglo XX, los naturalistas se enfrentaron a una creciente presión para añadir rigor y preferentemente experimentación a sus métodos, tal como las nuevas y prominentes disciplinas biológicas basadas en el laboratorio habían hecho. La ecología había nacido como una combinación de la biogeografía con el ciclo biogeoquímico, concepto promovido por los químicos; los biólogos de campo desarrollaron métodos cuantitativos como el cuadrado de muestreo ("quadrat") y adaptaron instrumentos de laboratorio y cámaras para su utilización en el campo con tal de separar sus trabajos de la historia natural tradicional. Los zoólogos y botánicos hicieron lo posible para mitigar el carácter impredecible de los seres vivos, llevando a cabo experimentos de laboratorio y estudiando entornos naturales semicontrolados tales como jardines; nuevas instituciones como la Estación Carnegie para la Evolución Experimental y el Laboratorio de Biología Marina proporcionaron entornos más controlados para estudiar organismos a través de sus ciclos de vida completos.

El concepto de sucesión ecológica, promovido en las décadas de 1900 y 1910 por Henry Chandler Cowles y Frederic Clements, fue importante en los inicios de ecología de las plantas. Las ecuaciones presa-depredador de Alfred Lotka, los estudios de la biogeografía y la estructura bioquímica de los lagos y ríos (limnología) de G. Evelyn Hutchinson y los estudios sobre la cadena alimenticia animal de Charles Elton fueron pioneros entre la serie de métodos cuantitativos que colonizaron las especialidades ecológicas en desarrollo. La ecología se convirtió en una disciplina independiente en las décadas de 1940 y 1950 después de que Eugene P. Odum sintetizara muchos de los conceptos de la ecología de ecosistemas, poniendo a las relaciones entre grupos de organismos (especialmente relaciones de materia y energía) en el centro del campo.

En la década de 1960, debido a que los teóricos evolutivos exploraron la posibilidad de múltiples unidades de selección, los ecologistas se volvieron hacia enfoques evolutivos. En la ecología de poblaciones, el debate sobre la selección de grupos fue breve pero vigoroso; durante la década de 1970, la mayoría de los biólogos concordaban en que la selección natural era rara vez efectiva a nivel de organismos individuales. La evolución de los ecosistemas, sin embargo, se convirtió en un foco de investigación permanente. La ecología se expandió rápidamente con el aumento del movimiento ambientalista; el Programa Biológico Internacional trató de aplicar los métodos de la gran ciencia (que había tenido mucho éxito en las ciencias físicas) a la ecología de ecosistemas y a los problemas ambientales apremiantes, mientras que los esfuerzos independientes de menor escala, tales como la biogeografía de islas y el Bosque Experimental de Hubbard Brook ayudaron a redefinir el ámbito de una disciplina cada vez más diversa.

1900 marcó el llamado "redescubrimiento de Mendel": Hugo de Vries, Carl Correns y Erich von Tschermak llegaron independiente a las leyes de Mendel (que en realidad no están presentes en el trabajo de Mendel). Poco después, los citólogos (biólogos celulares) propusieron que los cromosomas eran el material hereditario. Entre 1910 y 1915, Thomas Hunt Morgan y los «drosofilistas» con su mosca de laboratorio forjaron estas dos ideas —ambas controversiales— dentro de la «teoría cromosómica mendeliana» de la herencia. Ellos cuantificaron el fenómeno de ligamiento genético y postularon que los genes residen en los cromosomas como las cuentas de una cadena; plantearon la hipótesis del entrecruzamiento cromosómico para explicar el ligamiento y la construcción de mapas genéticos de la mosca de la fruta "Drosophila melanogaster", que se convirtió en un organismo modelo ampliamente utilizado.

Hugo de Vries trató de vincular a la nueva genética con la evolución; basándose en su trabajo sobre la herencia y la hibridación, propuso una teoría de mutacionismo, que fue ampliamente aceptada en el siglo XX. El lamarckismo también tuvo muchos adeptos. El darwinismo era visto como incompatible con los rasgos continuamente variables estudiados por la biometría, que parecían sólo parcialmente hereditarios. En la década de 1920 y 1930 —tras la aceptación de la teoría cromosómica mendeliana— el surgimiento de la disciplina de la genética de poblaciones, con el trabajo de R. A. Fisher, J. B. S. Haldane y Sewall Wright, unificó la idea de la evolución por selección natural con la genética mendeliana, produciendo la síntesis moderna. La herencia de caracteres adquiridos fue rechazada, mientras que el mutacionismo dio lugar a la maduración de teorías genéticas.

En la segunda mitad del siglo, las ideas sobre genética de poblaciones comenzaron a aplicarse en las nuevas disciplinas de la genética del comportamiento, la sociobiología, y especialmente en seres humanos, la psicología evolutiva. En la década de 1960 W. D. Hamilton entre otros desarrollaron la teoría de juegos enfocada en explicar el altruismo desde una perspectiva evolutiva a través de la selección de parentesco. El posible origen de los organismos superiores a través de la endosimbiosis, en contrastante con los enfoques de la evolución molecular desde una visión centrada en los genes (que tiene a la selección como la causa predominante de la evolución) y la teoría neutralista (que hace de la deriva genética un factor clave) dio lugar a debates permanentes sobre el equilibrio adecuado entre adaptacionismo y contingencia en la teoría evolutiva.

En la década de 1970, Stephen Jay Gould y Niles Eldredge propusieron la teoría del equilibrio puntuado, que sostiene que la inmutabilidad es la característica más destacada del registro fósil, y que la mayoría de los cambios evolutivos se producen rápidamente durante periodos relativamente cortos de tiempo. En 1980, Luis Álvarez y Walter Álvarez propusieron la hipótesis de que un impacto astronómico fue el responsable de la extinción masiva del Cretácico-Terciario. También en la década de 1980, el análisis estadístico en los registros fósiles de organismos marinos publicado por Jack Sepkoski y David M. Raup, llevó a una mejor apreciación de la importancia de los eventos de extinción masiva en la historia de la vida en la Tierra.

A finales del siglo XIX todas las principales rutas en el metabolismo de fármacos habían sido descubiertas, gracias a la comprensión del metabolismo de proteínas y ácidos grasos y de la síntesis de urea. En las primeras décadas del siglo XX, los componentes menores en los alimentos de la nutrición humana, las vitaminas, comenzaron a ser aislados y sintetizados. Las mejoras en técnicas de laboratorio como la cromatografía y la electroforesis llevaron a los rápidos avances en la química fisiológica, que —como "bioquímica"— comenzó a adquirir independencia de sus orígenes médicos. En las décadas de 1920 y 1930, los bioquímicos —dirigidos por Hans Krebs y Carl y Gerty Cori— comenzaron a trazar muchas de las rutas metabólicas centrales para la vida: el ciclo del ácido cítrico, la glucogénesis, la glucólisis y la síntesis de esteroides y porfirinas. Entre los años 1930 y 1950, Fritz Lipmann entre otros establecieron el papel del ATP como el portador universal de energía en la célula, y de la mitocondria como el centro energético de la célula. Tales trabajos tradicionalmente bioquímicos, continuaron siendo activamente perseguidos durante todo el siglo XX y en el siglo XXI.

Tras el ascenso de la genética clásica, muchos biólogos, —incluyendo una nueva ola de físicos en la biología— persiguieron la interrogante del gen y su naturaleza física. Warren Weaver, jefe de la división científica de la Fundación Rockefeller, distribuyó subvenciones para promover la investigación que aplicara los métodos de la física y la química a los problemas biológicos básicos, acuñando el término de "biología molecular" para este enfoque en 1938, muchos de los avances biológicos significativos de las décadas de 1930 y 1940 fueron financiados por la Fundación Rockefeller.

Como en la bioquímica, la superposición de las disciplinas de la bacteriología y la virología (más tarde combinadas como "microbiología"), situadas entre la ciencia y la medicina, se desarrolló rápidamente en el siglo XX. El aislamiento del bacteriófago por Félix d'Herelle durante la Primera Guerra Mundial inició una larga línea de investigación que se centró en los virus bacteriófagos y las bacterias que infectan.

El desarrollo del estándar, organismos genéticamente uniformes que pudieran producir resultados experimentales repetibles, fue esencial para el desarrollo de la genética molecular. Después de los primeros trabajos con la mosca "Drosophila" y el maíz, la adopción de sistemas modelo más simples como el moho del pan "Neurospora crassa" hizo posible la conexión entre la genética y la bioquímica, y más importante, con la hipótesis «un gen, una enzima» de Beadle y Tatum en 1941. Experimentos genéticos en sistemas aún más simples como el virus del mosaico del tabaco y el bacteriófago, ayudado por las nuevas tecnologías de la microscopía electrónica y la ultracentrifugación, obligó a los científicos a volver a evaluar el significado literal de "vida"; la herencia del virus y la reproducción de las estructuras celulares nucleoproteicas fuera del núcleo («plasmagenes») complicaron la teoría cromosómica mendeliana aceptada.

Oswald Avery mostró en 1943 que el ADN era probablemente el material genético de los cromosomas, y no sus proteínas; la cuestión se resolvió decisivamente con el experimento de Hershey y Chase en 1952, una de las muchas contribuciones del llamado grupo del fago centrado en torno al físico y biólogo Max Delbrück. En 1953 James D. Watson y Francis Crick, basándose en el trabajo de Maurice Wilkins y Rosalind Franklin, sugirieron que la estructura del ADN era una doble hélice. En su famoso artículo «"Estructura molecular de los ácidos nucleicos"», Watson y Crick observaron tímidamente: «No se nos escapa que el emparejamiento específico que hemos postulado sugiere inmediatamente un posible mecanismo de copiado del material genético». Después de 1958 el experimento de Meselson-Stahl confirmó la replicación semiconservativa del ADN, con lo que era evidente para la mayoría de los biólogos que la secuencia de ácido nucleico de alguna manera debía determinar la secuencia de aminoácidos en las proteínas; el físico George Gamow propuso que un código genético fijo relacionaba las proteínas y el ADN. Entre 1953 y 1961, había pocos secuencias biológicas conocidas, —ni siquiera el ADN o las proteínas— pero sí una gran cantidad de sistemas de código propuestos, una situación aún más complicada por el incremento en el conocimiento de la función intermediaria del ARN. Para realmente descifrar el código, se realizaron una extensa serie de experimentos en la bioquímica y la genética bacteriana, entre 1961 y 1966 —muy importantemente el trabajo de Nirenberg y Khorana.

Además de la División de Biología en el Instituto de Tecnología de California (Caltech), el Laboratorio de Biología Molecular (y sus precursores) en Cambridge, y un puñado de otras instituciones, el Instituto Pasteur se convirtió en un importante centro de investigación de la biología molecular a finales de la década de 1950. Los científicos de Cambridge, dirigidos por Max Perutz y John Kendrew, se centraron en el campo de rápido desarrollo de la biología estructural, combinando la cristalografía de rayos X con el modelado molecular y las nuevas posibilidades de cálculo de la computación digital (ambos beneficiados directa e indirectamente con la financiación militar de la ciencia). Más tarde, un número de bioquímicos dirigidos por Fred Sanger se unió al laboratorio de Cambridge, reuniendo así el estudio de la estructura y función macromolecular. En el Instituto Pasteur, François Jacob y Jacques Monod continuaron el experimento PaJaMo de 1959 con una serie de publicaciones sobre el operón lac que estableció el concepto de regulación genética e identificaron lo que llegó a ser conocido como ARN mensajero. A mediados de la década de 1960, el núcleo intelectual de la biología molecular —un modelo para las bases moleculares del metabolismo y la reproducción— estuvo en gran parte completo.

Entre finales de la década de 1950 hasta principios de la década de 1970 fue un período de intensa investigación y expansión institucional para la biología molecular, que se ha convertido en una disciplina coherente sólo recientemente. Los métodos y profesionales en biología molecular crecen con rapidez en lo que el biólogo organísmico E. O. Wilson ha llamado «la guerra molecular», a menudo llegando a dominar departamentos e incluso disciplinas enteras. La molecularización fue particularmente importante para la genética, la inmunología, la embriología y la neurobiología, mientras que la idea de que la vida es controlada por un «programa genético» —una metáfora que Jacob y Monod introdujeron desde los campos emergentes de la cibernética y las ciencias de la computación— se convirtió en un punto de vista influyente en toda la biología. La inmunología en particular, se vinculó con la biología molecular, fluyendo la innovación en ambos sentidos: la teoría de la selección clonal desarrollada por Niels Kai Jerne y Frank Macfarlane Burnet a mediados de 1950 ayudó a arrojar luz sobre los mecanismos generales de la síntesis de proteínas.

La resistencia a la creciente influencia de la biología molecular fue especialmente evidente en la biología evolutiva. La secuenciación de proteínas tuvo un gran potencial para el estudio cuantitativo de la evolución (a través de la hipótesis del reloj molecular), pero importantes biólogos evolutivos cuestionaron la relevancia de la biología molecular para responder a las grandes preguntas de la causalidad evolutiva. Departamentos y disciplinas fracturadas, así como biólogos organicistas afirmaron su importancia e independencia: Theodosius Dobzhansky hizo la famosa declaración de que «nada en biología tiene sentido excepto a la luz de la evolución» como una respuesta al desafío molecular. El problema se hizo aún más crítico a partir de 1968; la teoría neutralista de la evolución molecular de Motoo Kimura sugiere que la selección natural no fue la causa de la evolución en todas partes, por lo menos a nivel molecular, y que la evolución molecular podría ser un proceso fundamentalmente diferente de la evolución morfológica. La resolución de esta «paradoja molecular/morfológica» ha sido un tema central de la investigación de la evolución molecular desde la década de 1960.

La biotecnología, en un sentido general ha sido una parte importante de la biología desde finales del siglo XIX. Con la industrialización en la elaboración de cerveza y la agricultura, los químicos y biólogos se dieron cuenta del gran potencial de los procesos biológicos controlados por humanos. En particular, la fermentación resultó ser de gran ayuda para las industrias químicas. Para inicios de la década de 1970, una amplia gama de biotecnologías fueron desarrolladas, desde drogas como la penicilina y los esteroides, hasta alimentos como "Chlorella" y proteína de origen unicelular para gasohol, así como una amplia gama de cultivos de alto rendimiento híbridos y tecnologías agrícolas, la base de la Revolución Verde.
La biotecnología en el sentido moderno de la ingeniería genética comenzó en la década de 1970 con la invención de técnicas de ADN recombinante. Las enzimas de restricción fueron descubiertas y caracterizadas a finales de la década de 1960, siguiendo los pasos de aislamiento, luego duplicación y luego síntesis de genes virales. Comenzando con el laboratorio de Paul Berg en 1972 (ayudado por la "Eco"RI del laboratorio Herbert Boyer basándose en el trabajo con la ligasa del laboratoria Arthur Kornberg), los biólogos moleculares pusieron todas estas piezas juntas para producir el primer organismo transgénico. Poco después, otros comenzaron a usar vectores plásmidos y a añadir genes para la resistencia a antibióticos, incrementando considerablemente el alcance de las técnicas de recombinación.

Cautelosa de los peligros potenciales (particularmente la posibilidad de una bacteria prolífica con un gen viral causante de cáncer), la comunidad científica, así como una amplia gama de científicos independientes reaccionaron hacia estos desarrollos tanto con entusiasmo como con reservas temerosas. Prominentes biólogos moleculares conducidos por Berg, sugirieron una moratoria temporal sobre las investigaciones con ADN recombinante hasta que los peligros pudiesen ser juzgados y las políticas pudiesen ser creadas. Esta moratoria fue largamente respetada, hasta que los participantes de la Conferencia de Asilomar sobre ADN Recombinante crearon recomendaciones políticas y concluyeron que la tecnología podía ser utilizada con seguridad.

Después de Asilomar, nuevas técnicas y aplicaciones de la ingeniería genética se desarrollaron rápidamente. Los métodos de secuenciación de ADN mejoraron mucho (iniciados por Fred Sanger y Walter Gilbert), al igual que la síntesis de oligonucleótidos y las técnicas de transfección. Los investigadores aprendieron a controlar la expresión de los transgenes, y pronto fueron conducidos —tanto en el contexto académico como en el industrial— a crear organismos capaces de expresar genes humanos para la producción de hormonas humanas. Sin embargo, esta fue una tarea de mayores proporciones de las que los biólogos moleculares habían esperado; los desarrollos entre 1977 y 1980 mostraron que, debido a los fenómenos de división y empalme de los genes, los organismos superiores tienen un sistema de expresión genética mucho más complejo que el de las bacterias modelo usadas en estudios anteriores. El primer puesto en la carrera por la síntesis de la insulina humana fue ganado por Genentech. Esto marcó el inicio de la explosión biotecnológica (y con ella, la era de las patentes genéticas) con un nivel de solapamiento sin precedentes entre la biotecnología, la industria y la ley.

Durante la década de 1980, la secuenciación de proteínas había ya transformado los métodos de clasificación científica de los organismos (especialmente la cladística) pero los biólogos pronto comenzaron a usar las secuencias de ARN y ADN como caracteres; esto incrementó la significatividad de la evolución molecular dentro de la biología evolutiva, como resultado la sistemática molecular podría ser comparada con los árboles evolutivos tradicionales basados en la morfología. Siguiendo las ideas pioneras de Lynn Margulis sobre la teoría endosimbiótica, que sostiene que algunos de los orgánulos de las células eucariotas se originaron a partir de organismos procariotas sin vida a través de relaciones simbióticas, incluso la división global del árbol de la vida ha sido revisado. En la década de 1990, los cinco dominios (plantas, animales, hongos, protistas, y moneras) se convirtieron en tres (Archaea, Bacteria, y Eukarya) con base en el trabajo pionero sobre sistemática molecular de Carl Woese con la secuenciación del ARN ribosomal 16S.

El desarrollo y la popularización de la reacción en cadena de la polimerasa (PCR) a mediados de 1980 (por Kary Mullis y otros científicos de Cetus Corporation) marcó otro hito en la historia de la biotecnología moderna, incrementando considerablemente la facilidad y rapidez del análisis genético. Junto con el uso de los marcadores de secuencia expresada, la PCR condujo al descubrimiento de muchos más genes que pueden encontrarse a través de bioquímicos tradicionales o métodos genéticos y abrió la posibilidad de secuenciar genomas completos.

La unidad de gran parte de la morfogénesis de los organismos desde el huevo fertilizado hasta el adulto, empezó a ser descifrada tras el descubrimiento de los genes homeobox, primero en moscas de la fruta y luego en otros insectos y animales, incluyendo a seres humanos. Estos desarrollos dieron lugar a avances en el campo de la biología evolutiva del desarrollo hacia la comprensión de cómo los diversos planes corporales de los filos animales han evolucionado y cómo se relacionan entre sí.

El Proyecto Genoma Humano —el más grande y más costoso estudio biológico único jamás realizado— se inició en 1988 bajo la dirección de James D. Watson, después del trabajo preliminar con organismos modelo genéticamente más simples, tales como "E. coli", "S. cerevisiae" y "C. elegans". La secuenciación aleatoria y los métodos de descubrimiento de genes iniciados por Craig Venter —y alimentados por la promesa financiera de las patentes genéticas con Celera Genomics—, condujo a un concurso de secuenciación en los sectores público y privado, que terminó en un compromiso con el primer borrador de la secuencia del ADN humano anunciado en el año 2000.

A principios del siglo XXI, las ciencias biológicas convergieron con disciplinas nuevas y clásicas anteriormente diferenciadas como la física en campos de investigación como la biofísica. Se hicieron avances en química analítica e instrumentación física, incluidas las mejoras en sensores, componentes ópticos, marcadores, instrumentación, procesamiento de señales, redes, robots, satélites y poder de cómputo para la recopilación, almacenamiento, análisis, modelado, visualización y simulación de datos. Estos avances tecnológicos permitieron la investigación teórica y experimental, incluida la publicación en Internet de la bioquímica molecular, los sistemas biológicos y la ciencia de ecosistemas. Esto hizo posible el acceso mundial para mejorar las mediciones, los modelos teóricos, las simulaciones complejas, la teoría de experimentación con modelos predictivos, el análisis, el reporte observacional de datos por Internet, la libre revisión por pares, la colaboración y la publicación en Internet. Nuevos campos de investigación en ciencias biológicas surgieron como la bioinformática, la biología teórica, la genómica computacional, la astrobiología y la biología sintética.





</doc>
<doc id="3251" url="https://es.wikipedia.org/wiki?curid=3251" title="Protoplasma">
Protoplasma

El protoplasma es el material viviente de la célula, es decir, todo el interior de la célula (también el núcleo y el citoplasma).

Está formado por los elementos y sustancias químicas que se encuentran en la naturaleza, formando los cuerpos o estructuras no vivientes. 

En estado coloidal el protoplasma está formado por las siguientes sustancias:

El protoplasma tiene 3 propiedades fisiológicas fundamentales, "la irritabilidad, el metabolismo y la reproducción".


</doc>
<doc id="3252" url="https://es.wikipedia.org/wiki?curid=3252" title="Eukaryota">
Eukaryota

En biología y taxonomía, Eukaryota, Eukarya o Eucaria (palabras con etimología del griego: εὖ "eu" —‘bueno’, ‘bien’— y κάρυον "karyon" —‘nuez’, ‘carozo’, ‘núcleo’—) es el dominio (o imperio) que incluye los organismos formados por células con núcleo verdadero. La castellanización adecuada del término es eucariota o eucarionte. Estos organismos constan de una o más células eucariotas, abarcando desde organismos unicelulares hasta verdaderos pluricelulares en los que las diferentes células se especializan para diferentes tareas y que, en general, no pueden sobrevivir de forma aislada. 

Pertenecen al dominio o imperio eucariota los reinos de los animales, plantas y hongos, así como varios grupos incluidos en el parafilético reino Protista. Todos ellos presentan semejanzas a nivel molecular (estructura de los lípidos, proteínas y genoma), comparten un origen común, y principalmente, comparten el plan corporal de los eucariotas, muy diferente del de procariotas.

Con excepción de algunos organismos unicelulares, el ciclo de vida eucariota alterna una fase haplonte y otra diplonte, que se consigue mediante la alternancia de meiosis y fecundación, procesos que dan células haplontes y diplontes respectivamente.

Las células eucariotas son generalmente mucho más grandes que las procariotas y están mucho más compartimentadas. Poseen una gran variedad de membranas con núcleo rodeado de la envoltura nuclear, retículo endoplasmático y aparato de Golgi, además de mecanismos para la gemación y fusión de vesículas, incluida la exocitosis y endocitosis. Estructuras internas llamadas orgánulos se encargan de realizar funciones especializadas dentro de la célula. Presencia de lisosomas, peroxisomas y mitocondrias.

También caracteriza a todos los eucariotas un esqueleto interno o endoesqueleto, en este caso llamado citoesqueleto, formado por dos entramados de proteínas: el sistema de microtúbulos y el sistema contráctil de actina/miosina, que desempeñan un papel importante en la definición de la organización y forma de la célula, en el tráfico intracelular (por ejemplo, los movimientos de vesículas y orgánulos) y en la división celular. El característico flagelo eucariota y sus motores moleculares asociados se encuentran anclados al citoesqueleto.

El ADN de las células eucariotas está contenido en un núcleo celular separado del resto de la célula por una doble membrana permeable. El material genético se divide en varios bloques lineales llamados cromosomas, que son separados por un huso microtubular durante la división nuclear. Los cromosomas contienen histonas, varios replicones, centrómeros y telómeros. Hay un característico ciclo celular con segregación mitótica y reproducción sexual por meiosis. Se incluye un complejo de poros nucleares, transporte trans-membranal de ARN y proteínas a través de la envoltura nuclear, intrones y nuevos patrones de procesamiento del ARN utilizando espliceosomas. 

La célula eucariota debe en gran parte su forma y capacidad de movimiento al citoesqueleto, ya que le otorga rigidez y flexibilidad. En los organismos flagelados ancla los flagelos al resto de la célula y permite su batido durante la locomoción o para la creación de corrientes de agua que le lleven el alimento. En los organismos ameboides permite la extensión de "pies" o seudópodos para la locomoción o la alimentación. También fija los surcos de alimentación de los excavados y el complejo apical que permite a los apicomplejos entrar en las células parasitadas.

Sólo después de desarrollar su citoesqueleto pudo el eucariota ancestral realizar la fagocitosis, ya que es este el que, mediante crecimiento diferencial de sus fibras, logra que la célula se deforme para que la fagocitosis ocurra. La fagocitosis es también una propiedad ancestral de los eucariotas, si bien se ha perdido en grupos que se adaptaron a otras formas de alimentación. Hongos y plantas perdieron esta capacidad al desarrollar una pared celular rígida externa a la célula, pero ya contaban con otros modos de nutrición, la saprotrofia o el parasitismo en hongos y la fotosíntesis en plantas.

La mitocondria, derivada de la fagocitosis y posterior simbiogénesis de una proteobacteria, permitió al eucariota ancestral la respiración aerobia y con ello aprovechar al máximo la energía contenida en la materia orgánica. Como no es sorprendente en la evolución de un carácter tan antiguo, en varios grupos la mitocondria ha perdido esa capacidad ancestral y a cambio se ha modificado para cumplir otras funciones. También proceden de un evento de endosimbiosis los cloroplastos, en este caso con una cianobacteria, que permiten a las plantas realizar la fotosíntesis. Posteriormente otros grupos de eucariotas consiguieron sus cloroplastos mediante la endosimbiosis secundaria con un alga verde o roja.

Además de la división asexual de las células (mitosis), la mayoría de los eucariontes tiene algún proceso de reproducción sexual basado en la meiosis que no se encuentra entre los procariontes. La reproducción de los eucariontes típicamente implica la existencia de una fase haploide, donde está presente solamente una copia de cada cromosoma en las células, y diploide, donde están presentes dos. Las células diploides surgen por fusión nuclear (fecundación) y las haploides, por meiosis. En los organismos multicelulares, se distinguen tres tipos de ciclos biológicos:

Los organismos unicelulares pueden reproducirse asexualmente por bipartición, gemación o esporulación y sexualmente mediante gametos o por conjugación.

En los eucariontes, la relación de superficie frente a volumen es más pequeña que los procariontes, y así tienen tasas metabólicas más bajas y tiempos de generación más largos.

El origen de la célula eucariota es el proceso biológico más revolucionario desde el origen de la vida desde varios puntos de vista, como es el caso de la morfología, desarrollo evolutivo, estructura genética, relaciones simbióticas y ecología. Todas las células complejas son de este tipo y constituyen la base de casi todos los organismos pluricelulares. Aunque no hay acuerdo sobre cuándo se han originado los eucariotas, en general se supone hace unos 2000 millones de años. 

Eukarya se relaciona con Archaea desde el punto de vista del ADN nuclear y de la maquinaria genética, y ambos grupos son clasificados a veces juntos en el clado Neomura. Desde otros puntos de vista, tales como por la composición de la membrana, se asemejan más a Bacteria. Se han propuesto para ello tres posibles explicaciones principales:


Cada vez son mayores las evidencias que parecen demostrar que el origen eucariota es producto de la fusión de una arquea y una bacteria. Mientras el núcleo celular tiene elementos genéticos relacionados con las arqueas, las mitocondrias y la membrana celular tienen características bacterianas. La fusión genética es más evidente al constatar que los genes informativos parecen de origen arqueano y los genes operacionales de origen bacteriano. En todo caso también es cierto que un cierto número de rasgos presentes exclusivamente en los eucariontes son difíciles de explicar por medio de un evento de fusión.

Tampoco está claro el retraso de mil millones de años entre el origen de los eucariotas y su diversificación, pues las bacterias dominaron la biosfera hasta hace unos 800 millones de años. Este intervalo de estabilidad ambiental, litosférica y evolutiva se conoce con el nombre de "boring billion" (aburridos mil millones de años). Este retraso quizá se debiera simplemente a la dificultad de introducción en una biosfera ocupada enteramente por los procariotas. Otras explicaciones están relacionadas con la lenta evolución de los eucariotas o con el aumento del oxígeno, que no alcanzó los niveles ideales para los eucariotas hasta el final de dicho período. El diseño compartimentado de la célula eucariota es el más adecuado para el metabolismo aerobio y es comúnmente aceptado que todos los eucariotas actuales, incluidos los anaerobios, descienden de antecesores aerobios con mitocondrias.

Algún tiempo después de que surgiera la primera célula eucariota se produjo una radiación explosiva que las llevó a ocupar la mayoría de los nichos ecológicos disponibles.

La primera célula eucariota era probablemente flagelada aunque con tendencias ameboides al no tener una cubierta rígida. Desde el antecesor flagelado, algunos grupos perdieron ulteriormente los flagelos, mientras que otros se convirtieron en multiflagelados o ciliados. Cilios y flagelos (incluidos los que tienen los espermatozoides) son estructuras homólogas con nueve dobletes de microtúbulos que se originan a partir de los centriolos.

El carácter ameboide surgió varias veces a lo largo de la evolución de los protistas dando lugar a los diversos tipos de seudópodos de los distintos grupos. El que los ameboides procedan de los flagelados y no al revés, como se pensaba en el pasado, tiene como base estudios moleculares (fusión, partición o duplicación de genes, inserción o borrado de intrones, etc.).
Está generalmente aceptado que los cloroplastos se originaron por endosimbiosis de una
cianobacteria y que todas las algas eucariotas evolucionaron en última instancia de antepasados heterótrofos. Se piensa que la diversificación primaria de la célula eucariota tuvo lugar entre los zooflagelados: células predadoras no fotosintéticas con uno o más flagelos para nadar, y a menudo también para generar corrientes de agua con las que capturar a las presas.

En la actualidad hay discrepancia en dónde debe ponerse la raíz del árbol de Eukarya. La posibilidad más aceptada es situarlo entre o próximo a los excavados, que serían el grupo basal de los eucariontes.

Durante la primera parte de su historia los eucariontes permanecieron unicelulares. A partir del período Ediacárico los pluricelulares comienzan a profilerar, aunque el proceso con seguridad comenzó bastante antes. Los organismos unicelulares de vida colonial comenzaron a cumplir funciones específicas en una zona del colectivo. Se formaron así los primeros tejidos y órganos. La pluricelularidad se desarrolló independientemente en varios grupos de eucariontes: plantas, hongos, animales, algas pardas y algas rojas. A pesar de su pluricelularidad, estos dos últimos grupos se siguen clasificando en el reino Protista. 

Las algas verdes, las primeras plantas, se desarrollaron para formar las primeras hojas. En el Silúrico surgen las primeras plantas terrestres y de ellas las plantas vasculares o cormófitas.

Los hongos unicelulares constituyeron filas de células o hifas que agrupadas se convirtieron en organismos pluricelulares absortivos con un marcado micelio. Inicialmente, los hongos fueron acuáticos y probablemente en el período Silúrico apareció el primer hongo terrestre, justo después de la aparición de las primeras plantas terrestres. Estudios moleculares sugieren que los hongos están más relacionados con los animales que con las plantas.

El reino animal comenzó con organismos similares a los actuales poríferos que carecen de verdaderos tejidos. Posteriormente se diversifican para dar lugar a los distintos grupos de invertebrados y vertebrados.

Los eucariontes se dividen tradicionalmente en cuatro reinos: Protista, Plantae, Animalia y Fungi (aunque Cavalier-Smith 2004, 2015 reemplaza Protista por dos nuevos reinos, Protozoa y Chromista). Esta clasificación es el punto de vista generalmente aceptado en actualidad, aunque ha de tenerse en cuenta que el reino Protista, definido como los eucariontes que no encajan en ninguno de los otros tres grupos, es parafilético. Por esta razón, la diversidad de los protistas coincide con la diversidad fundamental de los eucariontes.

La reciente clasificación de Adl "et al." (2005, 2012, 2014) evita la clasificación en reinos, sustituyéndola por una acorde con la filogenia actualmente conocida, en la que por otra parte a los clados o taxones no se les atribuye ya categoría alguna, para evitar los inconvenientes que suponen éstas para su posterior actualización. El primer nivel de esta clasificación (equivalente a reinos en clasificaciones anteriores) es como sigue:

Algunos grupos de protistas tienen una clasificación dudosa, en particular Hacrobia y Sulcozoa. El primero agrupa a Haptophyta (haptofitas), Cryptophyta (criptofitas) y Centrohelida (heliozoos), mientras que el segundo agrupa a Apusomonadida y Ancyromonadida, entre otros.

Adicionalmente se reconocen dos agrupaciones más grandes. Diaphoretickes (o corticados) engloba a Archaeplastida y al Supergrupo SAR, mientras que Amorphea (o podiados) agrupa a Amoebozoa y Opisthokonta. Nótese que una forma ameboide o flagelar no indica la pertenencia a un grupo taxonómico concreto, como se creía en clasificaciones tradicionales, creando grupos artificiales desde el punto de visto evolutivo (ver polifilia).

El siguiente árbol filogenético muestras las relaciones entre los principales grupos de Eukarya de acuerdo con Adl "et al." 2012 y su comparación con Cavalier.Smith "et al." 2015:

En general, se estima el origen de Eukarya hace unos 2000 Ma (millones de años). Los fósiles más tempranos que a veces se asignan a eucariotas son difíciles de interpretar y podrían pertenecer a bacterias o a colonias de cianobacterias. Los fósiles más antiguos que pueden asignarse razonablemente a Eukarya corresponden a los acritarcos acantomorfos de hace 1630 Ma, mientras que el primer fósil pluricelular es del alga roja "Bangiomorpha" de hace 1200 Ma. Existen biomarcadores y fósiles como "Grypania" y la biota francevillian más antiguos, pero son dudosos. Fósiles de la mayoría de los grupos modernos de Eukarya se conocen desde el Ediacárico o el Cámbrico, hace unos 600-500 Ma.

En la siguiente tabla se indican los fósiles conocidos más antiguos para los diferentes grupos eucariotas, junto con una estimación de la aparición de cada grupo usando relojes moleculares calibrados usando estos fósiles (en Ma):

Los relojes moleculares calculan la antigüedad del último ancestro común eucariota (LECA) en 1679-1866 Ma.



</doc>
<doc id="3253" url="https://es.wikipedia.org/wiki?curid=3253" title="Pinocitosis">
Pinocitosis

La pinocitosis (del griego: πίνειν gr. 'beber', kyto- κύτος gr. cient. 'célula' y -ō-sis gr. 'proceso') es un tipo de endocitosis que consiste en la captación de material del espacio extracelular por invaginación de la membrana citoplasmática. Con desprendimiento hacia el interior celular de una vesícula que contiene líquido con posibles moléculas disueltas o partículas sólidas en suspensión.

La pinocitosis puede describirse como la fagocitosis de moléculas solubles. En esta la membrana se repliega creando una "vesícula pinocítica" y es de esta manera como las grasas, que son insolubles, pasan de la luz del intestino al torrente sanguíneo.

Un tipo de célula en la cual se la ha observado frecuentemente es el óvulo humano. Cuando el óvulo madura en el ovario de la mujer, se rodea de "células nodrizas". Aparentemente, estas células ceden alimentos disueltos al óvulo, que los incorpora por pinocitosis.

En la pinocitosis, la membrana celular se invagina formando una vesícula alrededor del líquido del medio externo que será incorporado a la célula

Junto con la fagocitosis, constituyen los dos tipos principales de endocitosis. A diferencia de la fagocitosis, la pinocitosis consiste en el ingreso de fluidos a través de la membrana celular mediante la formación de vesículas especiales, que se denominan pinosomas o vesículas pinocíticas. En el ser humano, este fenómeno se observa en células de la mucosa intestinal, cuando éstas permiten el ingreso de vesículas de grasa durante la absorción de nutrientes.

Las vesículas se originan en la superficie descubierta por clatrina, la vesícula revestida de clatrina pasa al citoplasma mediante invaginación, una vez que la vesícula esta en el citoplasma el revestimiento de clatrina desaparece, los trisqueliones (formados por moléculas de clatrina) quedan libres en el citoplasma. 

Dicha vesícula se fusiona con el endosoma temprano en la que intervienen dos proteínas:
1D
a) V -SNAREs 
b) T - SNARES

Posteriormente entra en juego el endosoma tardío y la formación del lisosoma en la que se produce la digestión.

En la pinocitosis se produce una acidificación gradual desde el endosoma temprano con un pH de 6, endosoma tardío pH 5,5 y el lisosoma pH 3,5.

Hay que recalcar que hay células en las que la pinocitosis no precisa moléculas de clatrina ni trisqueliones, sino que están revestidas de otras proteínas llamadas caveolinas.


</doc>
<doc id="3255" url="https://es.wikipedia.org/wiki?curid=3255" title="Nutrición autótrofa">
Nutrición autótrofa

La nutrición autótrofa es la capacidad de ciertos organismos de sintetizar todas las sustancias esenciales para su metabolismo a partir de sustancias inorgánicas, de manera que para su nutrición no necesitan de otros seres vivos.

Se denominan autótrofos porque generan sus propios alimentos, a través de sustancias inorgánicas para su metabolismo. Los organismos autótrofos producen su masa celular y materia orgánica a partir del dióxido de carbono, que es inorgánico, como única fuente de carbono, usando la luz o sustancias químicas como fuente de energía.

Los seres autótrofos pueden clasificarse en fotosintéticos y quimiosintéticos. Las plantas y otros organismos que usan la fotosíntesis se llaman fotolitoautótrofos; las bacterias que utilizan la oxidación de compuestos inorgánicos, como el anhídrido sulfuroso o compuestos ferrosos, para producir energía se llaman quimiolitotróficos.

Los seres heterótrofos, como los animales, los hongos y la mayoría de bacterias y protozoos, dependen de los autótrofos, ya que aprovechan la materia que estos contienen para fabricar moléculas orgánicas complejas.

Los heterótrofos obtienen la energía rompiendo las moléculas de los seres autótrofos que han comido. Incluso los animales carnívoros dependen de los seres autótrofos, porque la energía obtenida de sus presas procede en última instancia de los seres autótrofos que sus presas comieron.

Los seres vivos basan su composición en compuestos en los que el elemento químico definitorio es el carbono (compuestos orgánicos), y los autótrofos obtienen todo el carbono a través de un proceso metabólico de fijación del carbono llamado ciclo de Calvin.

Los organismos autótrofos forman el primer eslabón en las cadenas tróficas como productores primarios de la materia orgánica que circula a través de ellas. Son necesariamente los organismos más abundantes, ya que —dada la eficiencia limitada de los procesos metabólicos— cada eslabón está mucho menos representado que los anteriores.

Los seres autótrofos son una parte esencial en la cadena alimenticia, ya que benefician a otros seres vivos, llamados heterótrofos, que utilizan a los autótrofos como alimento. Los autótrofos obtienen los átomos y la energía que necesitan de fuentes abióticas, como la luz solar (por medio de la fotosíntesis) o las reacciones químicas entre sustancias minerales (por medio de la quimiosíntesis), así como de fuentes inorgánicas, como el dióxido de carbono, y los convierten en moléculas orgánicas que utilizan para desarrollar funciones biológicas, como su propio crecimiento celular, además de servir de alimento a los heterótrofos.



</doc>

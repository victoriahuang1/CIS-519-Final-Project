<doc id="8789" url="https://es.wikipedia.org/wiki?curid=8789" title="Amoníaco">
Amoníaco

El amoníaco, amoniaco, azano, espíritu de Hartshorn o gas de amonio es un compuesto químico de nitrógeno con la fórmula química NH. Es un gas incoloro con un característico olor repulsivo. El amoníaco contribuye significativamente a las necesidades nutricionales de los organismos terrestres por ser un precursor de fertilizantes. Directa o indirectamente, el amoníaco es también un elemento importante para la síntesis de muchos fármacos y es usado en diversos productos comerciales de limpieza. Pese a su gran uso, el amoníaco es cáustico y peligroso. La producción industrial del amoníaco en 2012 fue de 198 000 000 toneladas, lo que equivale a un 35 % de incremento con respecto al año 2006, con 146 500 000 toneladas.

El NH hierve a los –33.34 °C a una presión de una atmósfera, esto ayuda a que pueda conservarse en estado líquido, bajo presión a temperaturas bajas. Sin embargo, a temperaturas mayores a 405.5 K (temperatura crítica) ningún aumento en la presión producirá la condensación de este gas. Si la presión aumenta por encima del valor crítico de 111.5 atm, cualquier aumento por encima de este valor aumenta la compresión de las moléculas del gas, pero no se forma una fase líquida definida. El amoníaco casero o hidróxido de amonio es una solución de NH en agua. La concentración de dicha solución es medida en unidades de la Escala Baumé, con 26 grados baumé (cerca del 30 % por peso de amoníaco) como concentración típica del producto comercial.

Según la teoría de repulsión entre pares de electrones de la capa de valencia, los pares electrónicos de valencia del nitrógeno en la molécula se orientan hacia los vértices de un tetraedro, distribución característica cuando existe hibridación sp³. Existe un par solitario, por lo que la geometría de la molécula es piramidal trigonal (grupo puntual de simetría C). En disolución acuosa se puede comportar como una base y formarse el ion amonio, NH, con un átomo de hidrógeno en cada vértice de un tetraedro.

El amoníaco, a temperatura ambiente, es un gas incoloro de olor muy penetrante y nauseabundo. Se produce naturalmente por descomposición de la materia orgánica y también se fabrica industrialmente. Es fácilmente soluble y se evapora rápidamente. Generalmente se vende en forma líquida.

La cantidad de amoníaco producido industrialmente cada año es casi igual a la producida por la naturaleza. El amoníaco es producido naturalmente en el suelo por bacterias, por plantas y animales en descomposición y por desechos animales. El amoníaco es esencial para muchos procesos biológicos.

La mayor parte (más del 80 %) del amoníaco producido en plantas químicas es usado para fabricar abonos y para su aplicación directa como abono. El resto es usado en textiles, plásticos, explosivos, en la producción de pulpa y papel, alimentos y bebidas, productos de limpieza domésticos, refrigerantes y otros productos. También se usa en sales aromáticas.

Por su pH alcalino, es capaz de reaccionar con ácidos produciendo sales de amonio.

Su nombre fue dado por el químico sueco Torbern Bergman al gas obtenido en los depósitos de sal cerca del templo de Amón, en Libia y viene del griego, "ammōniakón", que significa "lo perteneciente a Amón".

El amoníaco se encuentra en pequeñas cantidades en la atmósfera, siendo producido por la putrefacción de la materia nitrogenada proveniente de plantas y animales. El amoníaco y sales de amonio también se encuentran en pequeñas cantidades en el agua de lluvia, donde el cloruro de amonio y sulfato de amonio se encuentran en zonas volcánicas; los cristales de bicarbonato de amonio se han encontrado en la Patagonia, en el guano. El riñón segrega amoníaco para neutralizar el exceso de ácido. Las sales de amoníaco se encuentran distribuidas a través de suelo fértil y en el océano. El amoníaco también se encuentra en otras partes del sistema solar: en Marte, Júpiter, Saturno, Urano, Neptuno y Plutón. Las sustancias que contienen amoníaco, o aquellas que son similar a él, se llaman " amoniacales".

El amoníaco es un gas incoloro con un olor desagradable. Es más ligero que el aire, su densidad es 0.589 veces la del aire de la atmósfera. Se condensa fácilmente por sus fuertes puentes de hidrógeno entre las moléculas; el líquido hierve a –33.3 °C y se congela a los –77.7 °C en cristales blancos.

El amoníaco se puede desodorizar fácilmente reaccionando con bicarbonato de sodio o ácido acético. Ambas reacciones forman sales de amoníaco sin olor.


La simetría del cristal es cúbico, su símbolo pearson es CP16, grupo espacial P 213 No.198, constante de red 0.5125 nm.


El amoníaco líquido posee fuertes fuerzas ionizantes reflejando su alta constante dieléctrica de 22. El amoníaco líquido tiene una muy alta entalpía de vaporización (23.35 kJ/mol, cf. agua 40.65 kJ/mol, metano 8.19 kJ/mol, fosfina 14.6 kJ/mol) y puede ser usado en laboratorios en vasos no aislados sin refrigeración.


El amoníaco es miscible con agua. En una solución acuosa puede separarse por ebullición. La solución acuosa de amoníaco es una base. La concentración máxima de amoníaco en agua tiene una densidad de 0.880 g/cm (880kg/m) y es frecuentemente conocido como 'amoníaco 0.880'.

El amoníaco casero o hidróxido de amonio es una solución de NH en agua. La concentración de dicha solución es medida en unidades de la Escala Baumé, con 26 grados baumé (cerca del 30% por peso de amoníaco) como concentración típica del producto comercial.

El amoníaco no se quema ni sostiene la combustión por sí mismo, excepto en mezclas de combustible con el 15 a 25 % de aire.
Cuando se mezcla con oxígeno se quema con una llama de color verde amarillento pálido. A alta temperatura y en la presencia de un catalizador, el amoníaco se descompone en sus elementos constituyentes. La combustión ocurre cuando la clorina pasa a amonio, formando nitrógeno y cloruro de hidrógeno; si la clorina esta en exceso, se forma el explosivo tricloruro de nitrógeno (NCl).

La molécula de amoníaco tiene una forma trigonal piramidal, como lo predice la teoría de repulsión de los pares de electrones de la capa de valencia, con un ángulo de enlace determinado de 106.7º. El átomo central de nitrógeno tiene cinco electrones externos con un electrón adicional de cada átomo de hidrógeno. Esto da un total de ocho electrones, o cuatro pares de electrones que son acomodan tetraédricamente. Tres de esos pares de electrones se usan como enlaces pares, lo que deja un par de electrones libres. Este par repele más fuertemente los pares de enlaces, entonces el ángulo del enlace no es de 109.5º como se esperaría por un acomodo tetraedral, sino de 106.7º. El átomo de nitrógeno en la molécula tiene un par de electrones libres, lo cual provoca que el amoníaco sea una base, aceptador de protones. Esta forma le da a la molécula un momento dipolo y lo hace una molécula polar. La polaridad de la molécula y, especialmente, su habilidad para formar puentes de hidrógeno, hace que el amoníaco sea altamente miscible en agua. El amoníaco es moderadamente básico, una solución acuosa a 1M tiene un pH de 11.6 y si un ácido fuerte es agregado a la solución hasta que la solución alcance un pH neutral de (pH=7), 99.4 %, las moléculas amoníaco se protonan. La temperatura y la salinidad también afectan la proporción de NH. Lo resultante tiene una forma regular y es isoelectrónico con metano.

La molécula de amoníaco fácilmente experimenta inversión del nitrógeno a temperatura ambiente; una analogía útil es que cuando una sombrilla gira al revés en un fuerte viento. La barrera de energía a esta inversión es de 24.7 kJ/mol, y la frecuencia resonante es de 23.79 Hz, correspondiente a la radiación de un microondas con una longitud de onda de 1.260 cm. La absorción a esta frecuencia fue la primera espectroscopia de microondas observada.

Una de las características más importantes del amoníaco es su basicidad. El amoníaco es considerado una base débil. Se combina con ácidos para formar sales; con ácido clorhídrico forma cloruro de amonio; con ácido nítrico, nitrato de amonio, etc. De cualquier modo, el amoníaco perfectamente seco no se combina con cloruro de hidrógeno completamente seco; el agua es necesaria para que se lleve a cabo la reacción Como una demostración del experimento, las botellas abiertas con amoníaco concentrado y ácido clorhídrico producen nubes de cloruro de amonio, que parecen salir "de la nada" mientras las sales forman donde se encuentran las dos nubes de difusión de las moléculas, entre las dos botellas.

Las sales producidas por acción del amoníaco en ácidos son conocidos como compuesto de amonio, los cuales contienen el ion de amonio. (NH).

Aunque el amoníaco es conocido como base débil, también funciona como un ácido muy débil. Es una sustancia prótica si es capaz de formar aminas (las cuales contienen el ion NH ). Por ejemplo, el litio disuelto en amoníaco líquido para dar una solución de litio amina.

Así como el agua, el amoníaco sufre autoionización molecular para formar sus bases y ácidos conjugados.

En presión y temperatura estándar, K=[][] = 10

La combustión del amoníaco da nitrógeno y agua es una reacción exotérmica:

El poder calorífico, Δ"H"°, expresado por mol de amoníaco y formando condensación de agua, es -382.81 kJ/mol. El dinitrógeno es el producto termodinámico de una combustión del amoníaco: todos los óxidos de nitrógeno son inestables con respecto a N and O, el cual es el principio que está detrás del convertido catalítico. Como sea, los óxidos de nitrógeno pueden ser formados por productos cinéticos en la presencia de una catálisis apropiada, una reacción de gran importancia industrial en la producción de ácido nítrico:

Una reacción subsecuente lleva NO

La combustión de amoníaco en aire es muy difícil en la ausencia de un catalizador, pues la temperatura de la llama es normalmente menor que la de encendido de la mezcla de amoníaco con aire. El rango inflamable del amoníaco en el aire es del 16 % al 25 %.

En química orgánica, el amoníaco puede actuar como nucleófilo en reacciones de sustitución nucleofílica. Se pueden formar aminas por la reacción del amoníaco con haloalcanos, aunque el -NH resultando es también nucleofílico y aminas secundarias y terciarias se forman como subproductos. Un exceso de amoníaco ayuda a minimizar sustituciones múltiples, y neutraliza a los haluros de hidrógenos formados. La metilamina es preparada comercialmente por la reacción de amoníaco con clorometano, y la reacción de amoníaco con ácido 2-bromopropanoico se ha usado para preparar alanina racémica en un rendimiento al 70 %. La etanolamina se prepara por una reacción de oxidación con etileno: la reacción a veces permite producir dietalonamina y trietalonamina.

Las amidas pueden ser preparas por una reacción de amoníaco con derivados ácidos carboxílicos. El cloruro de acilo es el más reactivo, pero el amoníaco debe estar presente en al menos el doble para neutralizar el ácido clorhídrico formado. Los ésteres y anhídridos también reaccionan con el amoníaco para formar amidas. Las sales amonio de ácidos carboxílicos pueden estar deshidratadas en amidas tanto tiempo que no haya térmicamente grupos sensitivos presentes: temperaturas de 150 °C a 200 °C son requeridas.

El hidrógeno en el amoníaco puede ser reemplazado por metales, así el magnesio quema en el gas con la formación de nitrato de magnesio MgN, y cuando el gas pasa sobre sodio o potasio sobre calentado, sodamida, NaNH, y potasamida, KNH, se forman. Donde necesariamente en nomenclatura IUPAC, se prefiere "azano" como nombre del anomíaco: por lo tanto la cloramina se llamaría "cloroazano", no cloroamoníaco.

El amoníaco pentavalente es conocido como λ-amino, o más comúnmente como hídrico de amonio. Es sólido cristalino es estable solamente en altas presiones, y se descompone de nuevo en amoníaco trivalente y gas hidrógeno en condiciones normales. Se ha investigado esta sustancia como posible combustible en 1966.

El amoníaco puede funcionar como un ligando en complejo de metales de transición. Es un donor-σ, en medio de las series espectroquímicas, y muestra un comportamiento intermedio en duro-blando. Algunos complejos de amoníaco incluyen 'tetraamminediaquacopper'([Cu(NH)(HO)]), ([Ag(NH)]) que es la especie activa del agente de Tollen's. La formación de este complejo también puede distinguirse entre precipitados de diferentes haluros de plata: cloruro de plata (AgCl) es soluble en una solución de amoníaco diluido (2M), el bromuro de plata (AgBr) es soluble solo en una solución de amoníaco concentrado, donde el yoduro de plata (AgI) es insoluble en amoníaco líquido.

Los complejos de cromo se conocieron en el siglo XIX, y formaron la base de la teoría revolucionaria de Alfred Werner, sobre la estructura de la coordinación de compuestos. Werner notó solamente dos isómeros ("fac"- y "mer"-) del complejo [CrCl(NH)] puede formarse, y concluyó que los ligados deben estar acomodados alrededor de los iones de metales en los vértices de un octahedron. Esta propuesta ha disido confirmada desde entonces por la cristalografía de rayos X.

Un amoníaco ligando unido a un ion metal es marcadamente más ácido que una molécula libre de amoníaco, aunque la desprotonación en una solución acuosa aún es raro. Un ejemplo es la reacción Calomel:

El grupo puntual para amoníaco es C cuando el eje mayor va a través del nitrógeno vertical. Cuando el eje mayor esta hilado ya sea en sentido de las manecillas del reloj o al contrarió, en 120º, cada hidrógeno se mueve en la locación previa de otro hidrógeno. Otro aspecto del grupo puntal de C incluye tres planos verticales de simetrías que intercepta al nitrógeno y a uno de los hidrógenos permitiendo a los otros dos que se reflejen el uno con otro.

La matriz matemática para este subgrupo particular es complicado desde que las matrices producidas por las rotaciones o las reflexiones son reducibles comparadas a otras matrices que son irreducibles. Por eso, una transformación similar debe ser hecha en cada porción de la matriz que es reducible. La transformación similar para el amoníaco viene de los cálculos del diagrama orbital molecular de simetría adaptado a una combinación lineal para la contribución de enlaces de cada uno de los hidrógenos.

El amoníaco y las sales de amonio pueden ser fácilmente detectadas, en rastros minutos, añadiendo la solución de Nessler, la cual le da una coloración amarilla en presencia del menor rastro amoníaco o sales de amonio. La cantidad de amoníaco en sales amonio puede ser estimado cuantitativamente por destilación de sales con hidróxido de sodio o hidróxido de potasio, el amoníaco evoluciona estando absorbido en un volumen conocido en ácido sulfúrico estándar y el exceso de ácido después determina el análisis volumétrico; o el amoníaco puede ser absorbido en ácido clorhídrico y el cloruro de amonio se forma tan precipitado como el hexacloroplatinato de amonio, (NH)PtCl.

Los palos de azufre se queman para detectar pequeñas fugas en los sistemas de refrigeración de amoníaco. Cantidades más grandes pueden ser detectadas calentando las sales con un álcali cáustico o con óxido de calcio, cuando el olor característico del amoníaco sea aparente. El amoníaco es un irritante y la irritación aumenta con la concentración; el límite permitido de exposición es 25 ppm, y letal arriba 500 ppm. Concentraciones más altas son difícilmente detectadas por detectores convencionales, el tipo de defectos se escoge de acuerdo a la sensibilidad requerida (semiconductor, catalítico, electroquímico). Se han propuesto sensores holográficos para detectar concentraciones arriba del 12.5 % en volumen.

El nitrógeno amoniacal (NH-N) es una medida comúnmente usada para examinar la cantidad de iones amonio, derivados naturalmente del amoníaco, y regresado a amoníaco por procesos orgánicos, en agua o desechos líquidos. Es una medida usada principalmente para cuantificar valores en el tratamiento de desperdicio y sistemas de purificación de agua, así como medida de la salud reservas de agua natural y hecha por el hombre. Se mide en unidades de mg/L (miligramos/Litros).

Los Romanos le dieron el nombre de 'sal ammoniacus' (sal de amoníaco) a los depósitos de cloruro de amoníaco colectados cerca del templo de Amun (en griego Ἄμμων" Ammon) en la antigua Libia por la proximidad del templo. Las sales de amoníaco se conocen desde tiempo antiguos; así es que el término de "Hammoniacus sal" aparece en las escrituras de Plinio, aunque no se sabe si el término es idéntico al término moderno de "sal-ammoniac" (cloruro de amonio).

En la forma de sal de amoníaco, el amoníaco fue importante en la alquimia medieval, como en el sigo XVIII, se mencionó por químico Persa Jābir ibn Hayyān, y los alquimistas desde el siglo XIII, siendo mencionado por Albertus Magnus. También fue utilizado por tintoreros en la Edad media en forma de orina fermentada para alterar el color de los tintes vegetales. En el siglo XV, Basilius Valentinus demostró que el amoníaco podía ser obtenido por la acción de álcalis en sal de amoníaco. En un periodo posterior, cuando las sales de amoníaco se obtenían de la destilación de pezuñas y cuernos de los bueyes y neutralizando el carbonato resultante con ácido clorhídrico.

El amoníaco gaseoso fue aislado por primera vez por Joseph Priestley en 1774 y fue nombrado por él como "aire alcalino". Once años después en 1785, Claude Louis Berthollet encontró su composición.

El proceso de Haber-Bosch produce amoníaco desde el nitrógeno en el aire, el cual fue desarrollado Fritz Haber y Carlo Bosch en 1909 y la patentaron en 1910. Su primer uso fue en una escala industrial en Alemania durante Primera Guerra Mundial. El amoníaco fue usado para producir explosivos para sostener refuerzos de guerra. Sin embargo, a pesar de su origen con fines bélicos, el proceso ha llegado a ser la principal fuente de nitrógeno fijado en el mundo, mejorando el rendimiento de los cultivos y mitigando el hambre a millones de personas en el planeta.

Anterior a la disponibilidad del gas natural barato, el hidrógeno como precursor de la producción de amoníaco se llevaba a cabo con electrolisis del agua o usando el proceso de cloro-álcali.

Aproximadamente el 83 % del amoníaco en el 2004 se utilizaba como fertilizantes o sales, soluciones o anhídridos. Cuando se aplicaba en suelo, ayudaba a incrementar el rendimiento de los cultivos como el maíz y el trigo. El 30 % del nitrógeno agricultural usado en los Estados Unidos es en forma de anhídrido y en el mundo, 110 millones de toneladas se usan cada año.

El amoníaco es directa o indirectamente el precursor de la mayor parte de los compuestos que contienen nitrógeno. Virtualmente, todos los compuestos sintéticos de nitrógeno son derivados del amoníaco. Uno de los derivados más importantes el ácido nítrico. El material clave se genera gracias al proceso de Ostwald por oxidación de amoníaco con aire sobre un catalizador de platino, con una temperatura entre 700 °C y 800 °C y 9 atmósferas. El óxido nítrico es un intermediario en esta reacción.
El ácido nítrico es usado para la producción de fertilizantes, explosivos, etc.

El amoníaco casero es una solución de NH en agua (hidróxido de amoníaco) usado con el propósito de limpiar superficies. Uno de sus usos más comunes es limpiar cristal, porcelana y acero inoxidable. También se usa frecuentemente para limpiar hornos y absorbiendo elementos para ablandar en la suciedad. El amoníaco casero tiene una concentración peso de 5 a 10 % de amoníaco.

Soluciones de amoníaco de entre el 16 % y el 25 % se usan en fermentaciones industriales como fuente de nitrógeno para microorganismos y ajustar su pH durante la fermentación.

En 1895, se sabía que el amoníaco era un antiséptico fuerte. Por esto se requiere 1.4 gramos por litro para preservar el caldo. En un estudio, amoníaco anhídrido destruyó 99.999 % de bacterias zoonóticas (Zoonosis) en tres tipos de alimento para animales. El anhídrido de amoníaco se usa actualmente para eliminar contaminación microbiana. La baba rosa esta hecha de recortes de carne grasosos removiéndoles la grasa con calor y centrifugación, luego tratándolo con amoníaco para matar a las "E. coli" en niveles indetectables. Ha habido preocupaciones sobre la seguridad del proceso, así como las quejas de los consumidores sobre el sabor y su mal olor.

En forma de amoníaco anhidro tiene un uso como fertilizante aumentando los niveles de nitrógeno del suelo.

Gracias a las propiedades de vaporización del amoníaco, es útil como un refrigerante. Era usado comúnmente antes de la popularización del empleo de los compuestos clorofluorocarbonados. El amoníaco anhídrido es usado incansablemente en la industria de la refrigeración y para los pistas de hockey por su alta eficiencia de conversión de energía y bajo costo. No obstante, tiene la desventaja de ser tóxico, lo que le restringe su uso doméstico y a pequeña escala. Junto con su uso moderno de refrigeración por compresión de vapor, se utilizó junto con hidrógeno y agua en refrigeración de absorción. El ciclo de Kalina, depende ampliamente del rango de ebullición de la mezcla de amoníaco y agua.

El amoníaco es usado para depurar SO de combustibles calientes, y el producto resultante es convertido sulfato de amoníaco para usarse como fertilizando. El amoníaco neutraliza la contaminación de los óxidos de nitrógenos (NO) emitidos por los motores diésel. Esta tecnología, llamada SCR (Reducción Selectiva Catalítica), se basa en un catalizador a base de vanadio.

El amoníaco puede ser usado para mitigar derrames de gases de fosgeno.

El amoníaco fue usado durante la Segunda guerra mundial como combustible para camiones en Bélgica, y su motor y energía solar mayor a 1900. El amoníaco líquido también fue combustible de los motores de reacción XLR99 que prenderían X-15. Aunque no es un combustible fuerte, no deja duda en el uso del motor del cohete y su densidad aproximada coincide con la densidad del oxidante, oxígeno líquido, lo cual simplificó el diseño de la aeronave.

El amoníaco se ha propuesto como una alternativa práctica al combustible fósil para la combustión de motores internos. El valor calorífico del amoníaco es 22.5 Mj/kg, que es aproximadamente la mitad del diésel. Un motor normal, en el cual el vapor del agua no se condensa, el valor calorífico el amoníaco sería menos del 21 %.

El amoníaco no puede ser usado fácilmente en el ciclo de Otto por sus bajos niveles de octano. aunque con la menor de las modificaciones a los carburadores y una reducción drástica en el radio de compresión, lo cual requeriría nuevos pistones, un motor de gasolina puede funcionar exclusivamente con amoníaco, en una fracción baja de su poder antes de la conversión del consumo de combustibles más potentes.

El tanque de un automóvil puede almacenar amoníaco líquido siempre y cuando el tanque este presurizado apropiadamente, dependiendo de la temperatura. Dependiendo de las propiedades termodinámicas del amoníaco son tales que a -30 °C, la presión del tanque tendría que ser 27.5 psi, aproximadamente lo mismo que un neumático de un carro. A 30 °C tendría que ser 170 psi para mantener el amoníaco líquido. Si la presión del tanque fuera liberada, el amoníaco líquido se tornaría gaseoso y alzar la presión a ese nivel. Los compresores comunes de aire de neumáticos operan a esta presión, para que la presión del tanque no sea una barrera para el uso del combustible.

Como sea, hay otras barreras para extender su uso. En términos de suministros primos de amoníaco, las plantas tendrían que ser construidas para incrementar los niveles de producción, lo cual requeriría una inversión mayor monetaria y energética. Aun cuando es el segundo compuesto químico más producido, la escala de producción de amoníaco es una pequeña fracción del petróleo usado en el mundo. Puede ser manufacturo de energías renovables, así como la energía nuclear. Noruega produjo amoníaco con electrólisis de agua por muchos años desde 1913, produciendo fertilizando por Europa. Si se produce del carbono, el CO puede ser embargado, pero la captura y almacenamiento de las plantas de carbono aún no llegan ni a la fase de prototipo.

En 1981, una compañía Canadiense convirtió un Chevrolet Impala 1981 para que funcionara con amoníaco.
Desde Detroit hasta San Francisco con una sola recarga de amoníaco.

Los motores de amoníaco que usan el amoníaco como un fluido de trabajo, se han propuesto y usado ocasionalmente. El principio es similar a el que se usa como locomoción, pero con el amoníaco se usa un fluido, en lugar de aire comprimido. Los motores de amoníaco se han usado experimentalmente en el sigo XIX por Goldsworthy Gurney en el Reino Unido y en Nueva Orleans.

El amoníaco, como vapor liberado por sales aromáticas, tiene un uso importante como estimulante respiratorio. El amoníaco es comúnmente usado en la manufactura ilegal de metanfetaminas a través de una reducción de Birch. El método Birch para hacer metanfetamina es peligroso porque son extremadamente reactivas los metales alcalinos y el amoníaco líquido, y la temperatura líquida del amoníaco lo hace susceptible a explotar cuando los reactivas son añadidos.

El amoníaco líquido es usado para el tratamiento de materiales de algodón, dándole propiedades como mercerización, usando metales álcalis, se usa para prelavar lana.

En una presión y temperatura estándar, el amoníaco es menos denso que una atmósfera, y tiene aproximadamente 60 % de poder de levantamiento del hidrógeno o helio. El amoníaco se usa a veces para llenar globos meteorológicos como un gas de levantamiento, por su relativo alto punto de ebullición (comparado al helio o hidrógeno), el amoníaco potencialmente pude ser refrigerado y licuado en una nave para reducir la elevación y añadir lastre (y regresarlo como gas con el mismo fin).

El amoníaco también se usa para oscurecer madera del roble blanco en muebles. Los vapores de amoníaco reaccionan con los taninos naturales en la madera, provocando un cambio de color.

La Administración de Seguridad y Salud Ocupacional de los Estados Unidos, ha delimitado un máximo de 15 minutos a la exposición del amoníaco líquido en 35 ppm por volumen en el aire del ambiente y 8 horas a 25 ppm por volumen. El Instituto Nacional para la Seguridad y Salud Ocupacional (NIOSH), recientemente redujo el riesgo inmediato de vida y salud de 500 a 300 basados en interpretaciones conservativas más recientes de la investigación original en 1943. El riesgo inmediato a la vida y salud es el nivel en el que un trabajador sano pueda ser expuesto 30 minutos sin sufrir daños irreversibles a la salud. Otras organizaciones tienen niveles de exposición variante. Los estándares de la Marina estadounidense permitía concentraciones máximas de: Exposición continua (60 días) a 25 ppm, o una hora con 400 ppm. El vapor de amoníaco tiene un hedor agudo, irritante y desagradable que actúa como prevención de potencial exposición peligrosa. El hedor promedio en el ambiente son 5 ppm, un nivel muy abajo de cualquier peligro o daño. La exposición a concentraciones muy altas de gas de amoníaco puede dar como resultado daño en los pulmones, y posiblemente la muerte. Aún cuando el amoníaco es regulado en los Estados Unidos como un gas inflamable, aún entra en la defunción de un gas tóxico al inhalarlo y requiere seguridad específica al transportarlo en cantidades más grandes a 3,500 galones.

La toxicidad de las soluciones de amoníaco no suelen causar problemas a los humanos o a otros mamíferos, por un mecanismo específico que existe para prevenir su almacenamiento en la sangre. El amoníaco se convierte en fosfato de carbonilo por la enzima carbonil fosfato sintetasa, y luego entra al Ciclo de la Urea para ser incorporado a los aminoácidos o excretado en la orina. De cualquier modo, los peces y anfibios no tienen este mecanismo, pues normalmente eliminan el amoníaco de sus cuerpos con excreción directa. El amoníaco incluso en concentraciones diluidas es altamente tóxico para animales acuáticos, y por esa razón es clasificado "peligroso para el ambiente".

Se cree que la toxicidad del amoníaco es una causa de la aún inexplorada pérdida de peces en criaderos. El exceso de amoníaco puede acumularse y causar alteraciones metabólicas o incrementar el pH en el cuerpo del organismo expuesto. La tolerancia varía entre las diferentes especies de peces. En concentraciones menores, alrededor de 0.05 mg/L, el amoníaco no-ionizado es dañino para los peces y puede provocar un descenso en el crecimiento, reducir la fertilidad y fecundidad e incrementar el estrés y la susceptibilidad a infecciones por bacterias y enfermedades. En exposición a un exceso de amoníaco, los peces pueden sufrir pérdidas en el equilibrio, hiperexcitabilidad, incremento de la actividad respiratoria, absorción de oxígeno y aumento del ritmo cardiaco. En concentraciones que excedan los 2.00 mg/L, el amoníaco provoca daños en los tejidos, letargo extremo, convulsiones, coma y la muerte. Con experimentos se ha demostrado que la concentración letal para los peces en general varía de 0.2 a 2.00 mg/L

Durante el invierno, cuando se reduce la alimentación administrada a las acuícolas, los niveles de amoníaco pueden incrementarse. Las temperaturas menores a la del ambiente reducen la tarifa de fotosíntesis de las algas, por lo que menos amoníaco es removido por algas. En este tipo de ambiente, especialmente a larga escala, no hay remedios rápidos para tratar altos niveles de amoníaco. Se recomienda prevención antes que corrección para reducir el daños a los peces y en sistemas de agua abierta, el circundante ambiente.

Similar al propano, el amoníaco anhídrido ebulle con una temperatura menor a la del ambiente con una presión estándar. Un recipiente para almacenar amoníaco capaz de contenerlo en 250 psi, es apropiado para su almacenamiento. Los compuestos de amoníaco no deben entrar en contacto con bases (salvo que se busque realizar una reacción química específica), pues se pueden liberar cantidades peligrosas de amoníaco.

Soluciones de amoníaco (5-10 % por peso) se usan en limpiadores caseros, particularmente para cristal. Estas soluciones son irritante para los ojos y la membrana mucosa, y en una menor proporción para la piel. Se debe tener precaución en nunca mezclar estas soluciones con cualquier líquido que contenga blanqueadores, pues puede provocar un gas peligroso. Mezclarlo con productos que contengan clorina u oxidantes poderosos, como blanqueadores caseros, pueden llevar a producir gases peligrosos como la cloramina.

Los peligros de soluciones de amoníaco dependen en su concentración: Soluciones con amoníaco "diluido" tienen usualmente entre 5-10 % por peso (5.62 mol/l); las soluciones concentradas usualmente están preparadas con un aproximado del 25 % por peso. Con un 25 % en peso, la solución tiene una densidad de 0.907 g/cm (907 kg/m), y una solución que tiene una densidad menor estará más concentrada. La siguiente tabla clasifica las soluciones de amoníaco.

El vapor de amoníaco de soluciones concentradas de amoníaco son severamente irritantes a los ojos y el tracto respiratorio, y estas soluciones solo pueden ser manejadas en una campana de humo. Soluciones saturadas ("0.880") pueden desarrollar una presión significativa en una botella cerrada en un clima cálido, y la botella deberá ser abierta con cuidado; esto no es normalmente un problema para soluciones al 25 % ("0.900").

Las soluciones de amoníaco no deben mezclarse con halógenos, pues productos tóxico y/o explosivos se forman. El contacto prolongado de soluciones de amoníaco con sales de plata, mercurio o yodo pueden también resultar en productos explosivos: dichas mezclas son formadas normalmente en análisis cualitativos inorgánicos, y deben ser ligeramente acidificadas pero no concentradas antes de que el análisis sea completado.

El anhídrido de amoníaco es clasificado como tóxico (""T"") y peligroso para el medio ambiente (""N""). El gas es inflamable (Temperatura de autoignición: 651 °C) y puede formar mezclas explosivas con el aire (16-25 %). La exposición permitida en los Estados Unidos es de 50 ppm, mientras que la concentración para el daño inmediato a la vida y salud se estima que esta en 300ppm. La exposición repetidas a niveles más bajos de amoníaco, disminuye la sensibilidad al olor del gas: normalmente el hedor es detectable en concentraciones menores a 50 ppm, pero individuos insensibilizados no podrías detectarlo inclusive a concentraciones de 100ppm. El anhídrido de amoníaco corroe aleaciones de cobre y zinc, así como el latón no debe ser usado para almacenar el gas. El amoníaco líquido también puede atacar al caucho y a ciertos plásticos.

El amoníaco reacciona violentamente con halógenos. El triyoduro de nitrógeno, un explosivo primario, el cual se forma cuando el amoníaco entra en contacto con el yodo. El amoníaco provoca la polimerización explosiva del óxido de etileno. También forma compuestos fulminantes explosivos con el oro, plata, mercurio, germanio o telurio. También se han reportado reacciones violentas con soluciones de acetaldehído, hipoclorito, peróxidos y ferricianuro potásico.

El NH se obtiene por el método denominado proceso Haber-Bosch (Fritz Haber y Carl Bosch recibieron el Premio Nobel de Química en los años 1918 y 1931). El proceso consiste en la reacción directa entre el nitrógeno y el hidrógeno gaseosos

N + 3H → 2NH ΔH° = -46,2 kJ/mol
ΔS° < 0

25 °C K = 6,8.10¬5 atm
850 °C K = 7,8.10¬-2 atm
Estos valores se obtienen por medio de la "ecuación de van't Hoff". Es una reacción muy lenta, puesto que tiene una elevada energía de activación, consecuencia de la estabilidad del N. La solución al problema fue utilizar un catalizador (óxido de hierro que se reduce a Fe en la atmósfera de H). Debido a que la reacción cuenta con más moles del lado de los reactivos (4 moles) que del lado de los productos (2 moles), al aumentar la presión se favorece la formación del producto. Aunque termodinámicamente la reacción transcurre mejor a bajas temperaturas, esta síntesis se realiza a altas temperaturas para favorecer la energía cinética de las moléculas y aumentar así la velocidad de reacción. Además se va retirando el amoníaco a medida que se va produciendo para favorecer todavía más la síntesis de productos.

El método de Haber-Bosch utiliza dos materias primas: hidrógeno y nitrógeno. El hidrógeno necesario es producido a partir de la reformación de gas natural, de GLP o de nafta con vapor de agua, siendo el gas natural la alimentación más usual. La planta se divide en dos secciones: el "front-end" o parte frontal de la planta, donde se genera la mezcla de hidrógeno y nitrógeno necesaria para sintetizar el amoníaco, y el "back-end" o parte posterior de la planta, que es la sección donde se convierte el producto a partir de ambos reactivos.

La planta suele constar de las siguientes unidades:

Front-end:









Back-end:

La sección de back-end consta principalmente de un reactor (el convertidor de amoníaco) de una serie de equipos auxiliares para enfriar la corriente de salida del reactor. También cuenta con un compresor el cual eleva la presión de la corriente proveniente del "front-end" hasta la presión de trabajo requerida para la síntesis de amoníaco. Existen diversos diseños de reactor ofrecidas por distintos tecnólogos (ej. Haldor Topsoe, Casale, Kellogg, reactores tipo TVA, etc.) y cada convertidor tiene sus características propias, sus ventajas y sus desventajas. Algunos reactores son verticales, otros horizontales; algunos tienen flujo radial, otros flujo axial, y otros flujo mixto; algunos tienen un lecho, otros tienen dos lechos, etc. De todas formas una característica común a todos ellos es que requieren un catalizador para que la reacción tenga lugar, siendo en la actualidad el utilizado en casi todas las plantas a base de hierro y promovido con otros compuestos. Los reactores suelen operar en un rango de temperatura de 400-500 °C y a presiones muy elevadas, en el orden de los 200-350 kgf/cm²g. Como se explicó anteriormente, la reacción está fuertemente balanceada hacia el lado de los reactivos, y por lo tanto la concentración de amoníaco no es demasiado alta: en una planta operando en condiciones estables la concentración de entrada al convertidor suele rondar el 5 % y a la salida el 15 %.


El amoníaco líquido es conocido y comúnmente estudiado como un solvente ionizante no-acuoso. Su propiedad más conspicua es la habilidad de disolver metales álcali para formar soluciones altamente coloradas y eléctricamente conductivas conteniendo electrones solvatados. Además de estas soluciones, mucha de la química en el amoníaco líquido puede ser clasificado por analogía con reacciones relacionadas en soluciones acuosas. La comparación de las propiedades físicas del NH con aquellas del agua, muestra que el NH tiene un menor punto de ebullición, de fusión, densidad, viscosidad y constante dieléctrica; esto es al menos en parte por los enlaces débiles del hidrógeno en NH y porque dicho enlace no puede formar redes reticuladas, pues cada molécula de NH tiene un solo par de electrones libres comparado con dos por cada molécula de HO. La constante de disociación del NH a -50 °C es 10 mol·l.

El amoníaco líquido es un solvente ionizante, aunque menos que el agua, disuelve bastantes compuestos, incluyendo nitratos, nitritos, cianuros y tiocianatos. La mayoría de las sales amonio son solubles y actúan como ácidos en soluciones de amoníaco líquido. La solubilidad de las sales de halógeno incrementa desde el fluor hasta el yodo. Una solución saturada de nitrato de amonio coticen 0.083 moles de solito por mol de amoníaco y tiene una presión de vapor de menos de 1 bar en 25 °C.

El amoníaco líquido disuelve a los metales álcali y a otros metales electropositivos como el magnesio, calcio, estroncio, bario, europio e iterbio. En bajas concentraciones, se forman soluciones de tono azul oscuro: estas contienen cationes y electrones solvatados, electrones libres que están rodeados por una jaula de moléculas de amoníaco.

Estas soluciones son muy útiles y fuertes agentes reductores. En concentraciones altas, las soluciones son metálicas en apariencias y en conductividad eléctrica. En temperaturas bajas, los dos tipos de solución pueden coexistir como fases inmiscibles.

El rango de estabilidad termodinámica de soluciones con amoníaco líquido es muy estrecho, como el potencial de oxidación a dinitrógeno, potencial normal de electrodo(N + 6NH + 6e 8NH), es solo +0.04 V. En práctica, la oxidación a dinitrógeno y reducción de dihidrógeno son lentas. Esto es especialmente verdad en soluciones reductoras: las soluciones de los metales álcali mencionadas anteriormente son estables por varios días, descomponiendo lentamente la amiba metal y dihidrógeno. La mayor parte de los estudios sobre el amoníaco líquido en soluciones son hechos en condiciones reductoras; sin embargo, la oxidación del líquido de amoníaco es lenta y hay riesgo de explosión.

En concentraciones elevadas irrita la garganta, inflama los pulmones, daña las vías respiratorias y los ojos. Según aumenta la concentración puede llegar a producir edema pulmonar ( El edema pulmonar a menudo es causado por insuficiencia cardíaca congestiva. Cuando el corazón no es capaz de bombear sangre al cuerpo de manera eficiente, ésta se puede represar en las venas que llevan sangre a través de los pulmones hasta el lado izquierdo del corazón.

A medida que la presión en estos vasos sanguíneos se incrementa, el líquido es empujado hacia los espacios de aire (alvéolos) en los pulmones. Este líquido reduce el movimiento normal del oxígeno a través de los pulmones. Esto y el aumento de la presión pueden llevar a dificultad para respirar) o la muerte cuando supera las 5000 ppm.
Si la persona inhaló el tóxico, trasládela inmediatamente a un sitio donde pueda tomar aire fresco, e inmediatamente después acudir rápidamente al médico.

El amoníaco puede producir irritación de la piel, sobre todo si la piel se encuentra húmeda. Además, puede llegar a quemar y ampollar la piel al cabo de unos pocos segundos de exposición con concentraciones atmosféricas superiores a 300 ppm.
Si el químico está en la piel o en los ojos, enjuague con agua abundante al menos por 15 minutos.

Este compuesto es gaseoso en condiciones atmosféricas normales siendo poco probable su ingestión. Sin embargo, en caso de producirse, puede destruir la mucosa gástrica, provocando severas patologías e incluso la muerte.

Si la persona ingirió el químico, suminístrele agua o leche inmediatamente, a menos que el médico haya dado otras instrucciones. No suministre leche ni agua si el paciente presenta síntomas que dificulten la deglución (problemas al tragar), tales como vómitos, convulsiones o disminución de la lucidez mental.


El amoníaco es una importante fuente de nitrógeno para sistemas vivos. Aunque el nitrógeno atmosférico abunda (más del 75 %), algunas criaturas vivas son capaces de usar el nitrógeno atmosférico en su forma diatómica, N gas. Entonces, la fijación de nitrógeno es requerida para la síntesis de aminoácidos, los cuales son la base de la proteína. Algunas plantas usan el amoníaco del nitrógeno atmosférico.

En determinados organismos, el amoníaco es producido por el nitrógeno atmosférico por la enzima llamada nitrogenasa. El proceso general se llama fijación de nitrógeno. Aunque difícilmente los métodos biomiméticos sean competitivos con el "Proceso de Haber", un esfuerzo intenso se ha direccionado hacia el mecanismo biológico de fijación de nitrógeno. El interés científico en este problema es motivado por una estructura inusual del sitio activo de la enzima, que consiste de un conjunto FeMoS.

El amoníaco también es un producto metabólico de los aminoácidos, catalizador por enzimas como la Glutamato deshidrogenasa. La excreción de amoníaco es común en animales marinos. En humanos, es fácilmente convertido a urea, que es menos tóxica, así como menos básica. La urea es el mayor componente del peso seco de la orina. La mayoría de los reptiles, aves, insectos y caracoles secretan ácido úrico como desechos nitrogenados.

El amoníaco también juega un papel en las fisiologías normal y anormal. Es biosintetizado a través del metabolismo normal de los aminoácidos y es tóxico en altas concentraciones. El hígado convierte amoníaco a urea a través de varias series de reacciones conocidas como el ciclo de la urea. Disfunciones en el hígado, como la cirrosis, pueden llevar tener una cantidad alta de amoníaco en la sangre (hiperamonemia). Del mismo modo, defectos en la enzima responsable del ciclo de la urea, como la ornitina transcarbamilasa, pueden provocar hiperamonemia. La hiperamonemia lleva a la confusión y a un estado de coma de encefalopatía hepática, así como las enfermedades comunes en personas con problemas en el ciclo de la urea.

El amoníaco es importante para el balance normal ácido/base. Después de la formación de amonio de glutamina, α-cetoglutarato puede ser degradado para producir dos moléculas de bicarbonato, los cuales funcionan posteriormente como buffers para ácidos dietéticos. El amoníaco es excretado en la orina, perdiendo ácidos. El amoníaco puede difundirse a través de sí mismo los túmulos renales, combinado con iones de hidrógeno, para permitir futura excreción de ácidos.

Los iones de amoníaco son productos tóxicos desechados del metabolismo de animales. En peces e invertebrados acuáticos, se excreta directamente al agua. En mamífero, tiburones y anfibios, se convierte en el ciclo de la urea a urea, porque es menos tóxico y es un proceso eficiente. En aves, reptiles y caracoles terrestres, el amonio metabólico es convertido en ácido úrico, que es sólido, y por tanto es excretado con una pérdida mínima de agua

El amoníaco se ha detectado en la atmósfera de los planetas de gas gigante, incluyendo Júpiter, junto con otros gases como el metano, hidrógeno y helio. En el interior de Saturno hay cristales congelados de amoníaco. Se encuentra naturalmente en las lunas Deimos y Phobos, las dos lunas de Marte.

El amoníaco fue detectado en un principio en el espacio en 1968, basado en emisiones de microondas desde la dirección del núcleo galáctico. Esta fue la primera molécula poliatómica detectada. La sensibilidad de la molécula en un amplio rango de excitaciones y la facilidad con que se puede observar un número de regiones ha hecho al amoníaco una de las moléculas más importantes para los estudios de las nubes moleculares. La relativa intensidad de líneas de amoníaco pueden ser usadas para medir la temperatura del medio emisor.

Las siguientes especies isotópicas de amoníaco se han detectado:

La detección del triple deuterio amoníaco fue considerado una sorpresa fue el deuterio es relativamente escaso. Se cree que las bajas temperaturas permiten a esa molécula subsistir y acumularse.

Desde su descubrimiento interestelar, NH ha probado ser una herramienta espectroscopia invaluable en el estudio del medio interestelar. Con un largo número de transiciones, es sensible a un amplio rango de condiciones de excitación, NH ha sido ampliamente detectado astronómicamente -su detección ha sido reportada en cientos de artículos-.

El estudio del amoníaco interestelar ha sido importante para diversas áreas de investigación en las últimas décadas.

La abundancia interestelar para el amoníaco ha sido medida en varios ambientes. El radio de [NH]/[H] ha sido estimado desde 10 en pequeñas nubes oscuras hasta 10 en el denso núcleo del complejo de nube molecular de Orión. Aunque un total de producción total de 18 rutas han sido propuestas, el principal mecanismo para formar NH interestelar es la siguiente reacción:

La constante de cambio "k" en esta reacción depende de la temperatura del ambiente, con un valor de 5.2×10 a 10 K. La constante fue calculada de la fórmula "k = a(T/300)". Para la reacción de formación primaria, "a" = 1.05×10 y "B" = −0.47. Asumiendo una abundancia de NH de 3×10 y una abundancia de electrones de 10 típico de nubes moleculares, la formación procede a un cambio de 1.6×10 cms en una nube molecular con una densidad total de 10 cm.

Todas las demás propuestas de reacción de formación tienen constantes con valores entre 2 y 13 órdenes de magnitud menores, haciendo que las contribuciones a la abundancia del amoníaco sean relativamente insignificantes. Como ejemplo de una de las contribuciones mencionadas está:

Tiene un cambio constante de 2.2×10. Asumiendo que las densidades de 10 and NH/H ratio of 10 para H, esta reacción procede en con un cambio de 2.2×10, más de 3 órdenes de magnitud más lentos que la reacción primaria anterior.

Algunas otras posibles reacciones de formación son:

Hay 113 reacciones propuestas que llevan a la destrucción del NH. De estas, 39 fueron tabuladas en extensas tablas de química junto con compuestos de carbono, nitrógeno y oxígeno. Una revisión del amoníaco interestelar cita las siguientes reacciones como los principales mecanismos de disociación:

Con cambios constantes de 4.39×10 y 2.2×10, respectivamente. Las ecuaciones (1,2) corren con un cambio de 8.8×10 and 4.4×10, respectivamente. Estos cálculos asumen el cambio dado de constantes y abundancias de [NH]/[H] = 10, [H]/[H] = 2×10, [HCO]/[H] = 2×10, y densidades totales de n = 10, típicas de frías y densas, nubes moleculares. Claramente, entre estas dos reacciones primarias, la ecuación (1) es la reacción dominante de destrucción, con un cambio de ~10,000 veces más rápido que la ecuación (2). Esto se debe a la relativa alta abundancia de H.

Observaciones de radio de NH del Radiotelescopio de Effelsberg reveló que la línea de amoníaco está separada en dos componentes –un fondo rígido y núcleo sin forma–. El fondo corresponde bien con la localización previamente detectada de CO.El telescopio 25 m Chilbolton en Inglaterra detectó señales de radio de amoníano en regiones H II, HNHO, objetos H-H y otros objetos asociados con las formación de estrellas. Una comparación con la line de emisión indica que velocidades turbulentas o sistemáticas no incrementan en el centro del núcleo de las nubes moleculares

La radiación de microondas del amoníaco fue observada en diversos objetos galácticos incluyendo W3(OH), Orión (constelación), W43, W51, y cinco fuentes en el centro galáctico. La alta detección de el cambio indica que es una molécula común en el medio interestelar y que las regiones de alta densidad son comunes en la galaxia

Observaciones VLA en siete regiones con flujos de alta velocidad gaseosos revelaron condensaciones de menos de 0.1 pc en L1551, S140 y Cefeo. Tres condensaciones individuales fueron detectadas en Cefeo, una de ellas era una figura muy alongada. Pueden jugar un rol importante en crear flojos bipolares en la región.

Amoníaco extragaláctico fue imaginado usando VLA en IC 342. La temperatura del gas caliente está arriba de los 70 K, lo cual fue inferido de las líneas del radio de amoníaco y parece estar asociado con porciones más internas de la barra nuclear vista en CO. NH fue también monitoreada por VLA hacia la muestra de cuatro regiones galácticas ultracompactadas HII: G9.62+0.19, G10.47+0.03, G29.96−0.02, y G31.41+0.31. Basándose en diagnósticos de temperatura y densidad, se concluye que en general que tales cúmulos son probablemente los lugares de formación de estrella en una fase evolutiva temprana antes del desarrollo de una región HII ultracompacta.

Absorciones a 2.91 micrómetros de amoníaco sólido fueron grabados de granos interestelares en el Objeto Becklin-Neugebauer y probablemente en NGC 2264-IR. Esta detección ayudó a explicar la forma física de los previamente poco entendidos líneas de absorción de hielo.

Un espectro del anillo de Júpiter fue obtenido del observatorio Kuiper Airborne, cubriendo los 100 a 300 cm del rango de espectro. Análisis del espectro provee información de propiedades globales de amoníaco en gas y la neblina de hielo de amoníaco.

Un total de 149 posiciones de nubes negras fueron revisadas para evidencia de "núcleos densos" usando la inversión de línea de (J,K) = (1,1) deNH. En general, los núcleos no tienen figura de esfera, con radios rondando entre 1.1 a 4.4. También se encontró que núcleos con estrellas tienen líneas más amplias que núcleos sin estrellas.

El amoníaco se detectó también en Nebula de Draco y en un una o quizá dos nubes moleculares, que están asociadas con el cirrus infrarrojo.

Balanceando y estimulando una emisión con una emisión espontánea, es posible construir una relación entre las temperatura de excitación y la densidad. Más sin embargo, desde los niveles transitorios del amoníaco, se puede aproximar a un nivel 2 en un sistema de bajas temperaturas, este cálculo es sencillo. Esta premisa puede ser aplicada a nubes negras, regiones que se sospecha tienen extremadamente bajas temperaturas y posibles sitios para la futura formación de estrellas. Detecciones de amoníaco en nubes negras muestra líneas estrechas —indicando que no solo son bajas temperaturas, pero también un nivel bajo de turbulencia en la nube–. La línea de cálculos del radio provee una medida de la temperatura de la nube que es independiente de previas observaciones de CO. Las observaciones del amoníaco fueron consistentes con las medidas de CO de rotación de temperaturas de ~10 K. Con esto, las densidades pueden ser determinadas, y han sido calculadas en un rango de entre 10 y 10 cm en nubes negras. Trazando el mapa de NH, se concluye que tiene medidas cotidianas de las nubes de 0.1 pc y masas cercanas a una masa solas. Estos sitios fríos con núcleos densos son sitios donde se formará una estrella.

Regiones HII ultra compactadas están entre los mejores trazadores de formación de estrellas de gran masa. EL material denso alrededor de regiones UCHII es primariamente molecular. Desde un completo estudio de formación de estrellas masivas, necesariamente involucra la nube de donde se formó la estrella, el amoníaco es una herramienta invaluable para comprender este material molecular que rodea. Puesto que este material molecular puede ser resuelto espacialmente, es posible constreñir los recursos de calor/ionizantes, temperaturas, masas y tamaño de las regiones. Los componentes de la velocidad Doppler desplazada permite la separación de distintas regiones de gas molecular que puede trazar flujos y núcleos calientes originados de la formación de estrellas.

El amoníaco ha sido detectado en galaxias externas, y por simultáneamente medir varías líneas, es posible directamente medir la temperatura del gas en estas galaxias. Las líneas del radio implican que las temperaturas son calientes (~ 50 K), originadas de nubes densas con tamaños de decenas de pc. Esta imagen es consistente con la imagen de nuestra Vía Láctea—núcleos moleculares densos y calientes se forman alrededor de estrellas que se están formando incrustadas en nubes con material molecular en la escala de varios cientos de pc (nubes moleculares gigantes).




</doc>
<doc id="8790" url="https://es.wikipedia.org/wiki?curid=8790" title="Anafilaxia">
Anafilaxia

La anafilaxia consiste en una reacción inmunitaria severa, generalizada, de rápida instalación y potencialmente fatal ante el contacto del organismo con una sustancia que provoca alergia.

Con mayor frecuencia, es el resultado de reacciones inmunológicas a los alimentos, medicamentos y picaduras de insectos, pero la puede inducir un agente capaz de producir una degranulación espontánea, sistémica de mastocitos o basófilos.

La anafilaxia se diferencia de la alergia por la extensión de la reacción inmunitaria que compromete particularmente al sistema respiratorio y el cardiovascular. Cuando en las manifestaciones de la anafilaxia se pone en riesgo inmediato y repentino la vida del paciente, se utiliza el término choque anafiláctico. 
El riesgo de choque anafiláctico puede ocurrir cuando se utilizan medios de contraste en radiología, o se aplican algunos fármacos terapéuticos que se comportan como alérgenos al cual el sujeto está sensibilizado.

El término anafilaxia fue acuñado por el Premio Nobel de Medicina Charles Robert Richet.

Las reacciones anafilácticas forman parte de las reacciones de hipersensibilidad tipo 1 en la clasificación de Coombs y Gell, mientras que en la clasificación de hipersensibilidad de Sell, las reacciones anafilácticas y anafilactoides ocupan el tipo 4.

Algunos autores reservan el término anafilaxis exclusivamente para los mecanismos dependientes de IgE, mientras que el término anafilactoide queda designado a las reacciones independientes de IgE.

La posibilidad de que ciertas sustancias agudicen la sensibilidad en lugar de proteger fue reconocida hacia 1902, por el francés Charles Richet y su colaborador Paul Portier quien intentó inmunizar a perros contra las toxinas de un tipo de medusa "Physalia".

El experimento consistió en que los perros fuesen expuestos a dosis subletales de la toxina y se observó que reaccionaron casi de manera instantánea y letal a un contacto ulterior con cantidades diminutas de la toxina.

Richert concluyó que una inmunización o vacunación exitosa creaba filaxis, o protección, y que podía observarse un resultado opuesto, anafilaxis, en el cual una exposición al antígeno podía precipitar una sensibilidad potencialmente letal a él si se repetía la exposición. Este hallazgo le valió el premio nobel en 1913.

Está aumentando el número de casos de anafilaxia en el mundo, no obstante no existen todavía fuentes fiables para determinar la incidencia global de esta entidad ya que la mayoría de los estudios hacen referencia a casos fatales, quedando relegados los casos leves.

Es más común que se desarrolle anafilaxis en adultos con respecto a los niños, siendo la anafilaxia por alimentos la excepción. En mujeres se ha demostrado una mayor susceptibilidad a la reacción anafiláctica por látex y para relajantes musculares. La reacción es más frecuente (y violenta) si el antígeno tiene contacto en la administración parenteral, seguida del contacto con mucosas y por último la piel. Los sujetos sometidos a tratamiento con betabloqueadores no presentan mayor incidencia anafiláctica, pero cuando aparece la anafilaxis el cuadro es más grave.

Algunos alérgenos son responsables de la mayoría de los casos de choque anafiláctico:










Existen otros fármacos que también son capaces de inducir anafilaxia, aunque con menor frecuencia, tales como la vitamina B1, derivados del dextrán, y el glucagón, entre otros. Cabe mencionar que teóricamente cualquier fármaco tiene potencial alergénico, aunque en la práctica pocos de ellos han mostrado relacionarse con relativa frecuencia a la anafilaxia.

La anafilaxia es el resultado de la liberación dependiente de IgE de mediadores químicos de los mastocitos y basófilos.

En esta fase, los alergenos inducen una reacción de hipersensibilidad tipo I, donde son secretadas cantidades enormes de IgE por células plasmáticas activadas por linfocitos T2 específicas de alergeno. El anticuerpo de esta clase se fija con gran afinidad a receptores Fc en la membrana de mastocitos y basófilos. Cuando un mastocito o basófilo está cubierto por IgE se dice que está "sensibilizado". Algunas personas pueden presentar Atopia, una predisposición genética al desarrollo de reacciones de hipersensibilidad tipo I contra antígenos ambientales orginarios.

La reacción se inicia cuando un alérgeno se combina con un anticuerpo IgE unido a membrana de los mastocitos y basófilos, lo que lleva a una cascada de transducción de señal (mediada por cAMP para derivados araquidónicos y mediada por Ca++ Para sustancias secuestradas en vesículas, por ejemplo, histamina) que se traduce en la degranulación y la liberación de mediadores de la anafilaxia. La histamina, puede causar broncoconstricción, la vasodilatación, aumento en la secreción de moco y una mayor permeabilidad vascular. Otros mediadores preformados de los mastocitos incluyen proteasas neutras como triptasa, hidrolasa ácida, enzimas oxidativas, factores quimiotácticos y proteoglicanos.

Los leucotrienos B4, C4 y E4, la prostaglandina D2, y el factor activador de plaquetas se derivan de la membrana de los mediadores formados a partir del metabolismo del ácido araquidónico. La liberación de estos mediadores puede causar broncoconstricción, secreción de moco, y alteraciones en la permeabilidad vascular. El factor activador de plaquetario puede disminuir la presión arterial, además, y activar la coagulación. Estos mediadores, además de la activación de otros vías pro inflamatorias y la síntesis de óxido nítrico, pueden conducir a una reacción sistémica grave con el potencial de resultados desastrosos.

Varios mediadores bioquímicos y sustancias quimiotácticas son liberados sistémicamente durante la degranulación de los mastocitos y basófilos. Estos incluyen sustancias preformadas granulares, tales como la histamina, triptasa, quimasa, y la heparina, factor de liberación de histamina y otras citocinas, y mediadores derivados de lípidos recién generados, tales como PGD2, leucotrienos LT-B4, factor activador de plaquetas, y los cisteinil leucotrienos LTC4, LTD4 y LTE4. Los eosinófilos podrían jugar un papel ya sea pro-inflamatorio (por ejemplo, la liberación de proteínas granulares citotóxicas) o anti-inflamatorio (por ejemplo, el metabolismo de los mediadores vasoactivos).

La histamina activa los receptores H1 y H2. La rinorrea, prurito, taquicardia y broncoespasmo son causadas por la activación de los receptores H1, mientras que tanto los receptores H1 y H2 median dolor de cabeza, rubor e hipotensión. Los niveles de histamina en suero se correlacionan con la gravedad y la persistencia de las manifestaciones cardiopulmonares, pero no con la formación de urticaria. Los signos y síntomas gastrointestinales están asociados con la histamina más que con los niveles de triptasa. Parece ser que los receptores H3 son inhibidores presinápticos que modulan la liberación de noradrenalina endógena de las fibras simpáticas que inervan el sistema cardiovascular, por ende, tiende a deprimir la TA.

La triptasa es la única proteína que se concentra selectivamente en los gránulos secretores de los mastocitos humanos. Los niveles plasmáticos de triptasa se correlacionan con la gravedad clínica de la anafilaxia. Debido a que la β-triptasa se almacena en los gránulos secretores de los mastocitos, su liberación podría ser más específica para detectar la activación de estos que la α-protriptasa, que parece ser secretada constitutivamente. Las mediciones post-mortem de triptasa sérica pueden ser útiles en el establecimiento de anafilaxia como causa de muerte en los sujetos que experimentan la muerte súbita de causa incierta.

La unión de histamina a los receptores H1 durante la anafilaxia también estimula las células endoteliales para activar a la enzima óxido nítrico sintasa convertir el aminoácido L-arginina en óxido nítrico (NO), un vasodilatador autacoide potente. El NO activa la guanilato ciclasa, lo que lleva a la vasodilatación y la producción de cGMP. Fisiológicamente, el NO ayuda a modular el tono vascular y la presión arterial regional. Una mayor producción de NO disminuye el retorno venoso, lo que contribuye a la vasodilatación que se produce durante la anafilaxia. Los inhibidores de la Óxido nítrico sintasa causan depresión miocárdica, facilitando la liberación de histamina, la producción de leucotrienos, y la vasoconstricción coronaria. Los inhibidores de NO durante la anafilaxia también promueven broncoespasmo, lo que sugiere que el NO puede disminuir los signos y síntomas de la anafilaxia, pero exacerban la vasodilatación asociada.

Los metabolitos del ácido araquidónico incluyen los productos de las vías de la lipoxigenasa y ciclooxigenasa. Cabe destacar que LTB4 es un agente quimiotáctico y por lo tanto teóricamente podrían contribuir a la fase tardía de la anafilaxia y las reacciones prolongadas. Hay otras vías inflamatorias que son probablemente importantes en la prolongación y ampliación de la anafilaxia. Durante los episodios serios de anafilaxia, hay una activación simultánea de complemento, las vías de la coagulación y el sistema de contacto calicreína-quinina. La disminución en C4 y C3 y la generación de C3a han sido observadas en la anafilaxia. La activación de la vía de coagulación incluye disminución en el factor V, factor VIII y fibrinógeno.
La disminución del quininógeno de alto peso molecular y la formación de los complejos inhibidores calicreína-C1 y factor XIIa-C1 indica la activación del sistema por contacto. La plasmina puede activar el complemento. Algunos mediadores pueden tener efectos que limiten la anafilaxis, un claro ejemplo es la quimasa que puede activar a la angiotensina II, que puede modular la hipotensión. La heparina inhibe la coagulación, la calicreína y plasmina. También se opone a la formación de complementar y modula la actividad de triptasa.

El factor de activación plaquetario promueve la liberación por parte de las plaquetas de factores quimiotácticos para eosinófilos, y el de los factores quimiotácticos como LTB, ECF-A(Factor quimiotáctico anafiláctico eosinófilo) y NCF-A (Factor quimiotáctico anafiláctico neutrófilo) lo que se creé tiene impacto en la fase tardía.

El compromiso de los órganos del sistema varía de especie a especie y determina la evolución clínica de anafilaxia de cualquier causa. Entre los factores que determinan el choque de un órgano específico se incluyen las variaciones en la respuesta inmune, la ubicación del músculo liso y características del metabolismo de mediadores químicos, tales como la distribución y las tasas de degradación y de respuesta de los mismos. Se han desarrollado varios modelos animales en los que se observan los siguientes resultados, en el conejillo de indias hay es la constricción del músculo liso bronquial, lo que lleva a broncoespasmo, hipoxia y la muerte. La anafilaxia en los conejos produce vasoconstricción arterial pulmonar fatal con fallo ventricular derecho. El órgano de choque principal en el perro es el sistema venoso del hígado, que se contrae y produce congestión hepática grave. En el ser humano de los órganos de choque que predominan son el pulmón y el corazón, con manifestaciones clínicas comunes de edema laríngeo, insuficiencia respiratoria y colapso circulatorio.

Los mediadores químicos de la anafilaxia afectan directamente al miocardio. Los receptores H1 median la vasoconstricción coronaria y aumentan la permeabilidad vascular, mientras que los receptores H2 aumentan la fuerza de contracción ventricular. La estimulación conjunta de receptores H1 y H2 parecen mediar la disminución de la presión diastólica y el aumento de la presión pulsátil.

La anafilaxia recurrente o bifásica se produce 8 a 12 horas después del ataque inicial hasta en un 20% de los sujetos que experimentan anafilaxia.

Esta reacción se desarrolla muy rápidamente actuando en segundos o minutos, incluso puede durar más de unas horas, como consecuencia de la liberación de mediadores. La rapidez se correlaciona con la gravedad del ataque.

Las manifestaciones clínicas de la anafilaxis varían, no obstante los hallazgos más frecuentes son cutáneos, usualmente urticaria, eritema, prurito y angioedema, seguidas por las respiratorias como resultado de la broncoconstricción y pueden ser congestión nasal, rinorrea, estornudo, edema laríngeo, broncoespasmo; y en tercer lugar las cardiovasculares por el aumento de la permeabilidad vascular como hipotensión y arritmias. Además se incluyen manifestaciones gastrointestinales como náuseas, vómitos, diarrea y dolor abdominal; y neurológicas, como la cefalea (no migrañosa), acúfenos, vértigos, relajación de esfínteres y pérdida de la conciencia.

Clínicamente, la anafilaxia se considera probable si uno de los siguientes tres criterios se cumplen en cuestión de minutos a horas:




Si no se diagnostica y trata la anafilaxia puede producirse una obstrucción respiratoria secundaria al edema laríngeo o un colapso circulatorio con un desenlace fatal.

Las respuestas compensatorias intrínsecas a la anafilaxia (es decir, las catecolaminas endógenas, la angiotensina II y endotelina-1) también influyen en el grado de manifestaciones clínicas y, cuando son adecuadas, pueden salvar vidas independientemente de la intervención médica.

Debido a que los mastocitos se acumulan en los sitios de las placas ateroscleróticas coronarias, algunos investigadores han sugerido que la anafilaxia puede promover la ruptura de la placa. La estimulación de los receptores de la histamina H1 también puede producir vasoespasmo coronario.

El diagnóstico de anafilaxis permanece como un diagnóstico clínico basado en el reconocimiento de patrones. La relación causa-efecto a menudo es confirmada mediante la historia clínica.
El primer elemento del diagnóstico es la demostración de la exposición al alérgeno. Generalmente esto es evidente, como en el caso de una picadura de insecto o la administración de un antibiótico o medio de contraste radiológico. Sin embargo, en los casos en lo que no es evidente el disparador de la reacción anafiláctica, no es recomendable demorar el tratamiento, debido a la rápida progresión de los síntomas, que en muchos casos pueden terminar fatalmente en cuestión de minutos.

Después de la exposición, los signos y síntomas aparecen en cuestión de segundos o minutos, aunque raramente pueden demorar algunas horas. Generalmente se circunscriben a la piel, el sistema cardiovascular, el respiratorio, el digestivo y el nervioso, y pueden predominar en alguno de ellos o presentarse simultáneamente.

Síntomas generales: Malestar difuso o generalizado, ansiedad, sensación de muerte inminente.






En el trazo electrocardiográfico son signos frecuentes: trastornos de la excitabilidad, de la conducción y sobre todo de la repolarización. En ocasiones se registran imágenes de lesión isquémica del miocardio.
En mujeres, un intenso dolor del útero es un posible síntoma de shock anafiláctico.
Dos características confieren al choque anafiláctico un particular interés:


Las pruebas de laboratorio por lo general son de poca ayuda en el diagnóstico ya que principalmente es clínico. La gasometría arterial puede ser útil para excluir embolia pulmonar, estatus asmático y aspiración de cuerpo extraño.Los niveles de histamina séricos y en orina pueden ser de gran utilidad en el diagnóstico de anafilaxis, pero estos exámenes son poco comunes.

No obstante, las pruebas de laboratorio pueden confirmar la presencia de alérgenos específicos de IgE o ayudar a reducir los diagnósticos diferenciales.

La triptasa sértica elevada refleja la degranulación de mastocitos y podría ser útil en la confirmación de la anafilaxia. Los niveles de triptasa sérica total presentan un pico entre 60 y 90 minutos después del inicio de la reacción anafiláctica y pueden persistir hasta 5 horas después del inicio de los síntomas, sin embargo, los niveles de triptasa pueden ser normales en una reacción. Las determinaciones seriadas de triptasa podría mejorar la sensibilidad diagnóstica.

Otras pruebas complementarias por lo general son de poca ayuda en el diagnóstico, ya que principalmente es clínico. La radiografía de tórax y la evaluación de la epiglotitis están indicadas en pacientes con compromiso respiratorio agudo. Un electrocardiograma (ECG) debe ser considerado en todos los pacientes con pérdida súbita de la conciencia, que refieran dolor de pecho o presenten disnea, y en cualquier paciente de edad avanzada.

El diagnóstico diferencial comprende, principalmente:

El síncope vasovagal (reacción vasodepresora) es el diagnóstico más susceptible de ser confundido con la anafilaxis. En las reacciones vasodepresoras generalmente no se encuentra urticaria, la disnea casi siempre está ausente, la presión arterial se encuentra en rangos normales, e incluso elevados, y la piel está típicamente fría y pálida. A pesar de que la taquicardia es característica de la anafilaxis, la bradicardia puede presentarse en su lugar, por eso la bradicardia no debería usarse para distinguir entre estos dos diagnósticos diferenciales. Asimismo, los defectos de conducción y tratamientos simpatolíticos pueden producir bradicardia.

Dos nociones fisiopatológicas deben tenerse en cuenta para establecer un tratamiento eficaz:



Sin tratamiento adecuado, la vasodilatación se generaliza con grave disminución del retorno venoso, de las presiones de llenado de los ventrículos, del volumen sistólico y del débito cardíaco.

El compromiso hemodinámico del choque anafiláctico se relaciona con un choque hipovolémico, que puede evolucionar hacia fibrilación ventricular y asistolia.


La epinefrina o adrenalina intramuscular es el medicamento de primera elección. Sus propiedades corrigen las anomalías del choque.


El choque grave requiere del uso de clorhidrato de adrenalina por vía intravenosa a una dosis de 0,25 a 1 mg, diluido en 10 ml de suero fisiológico, aplicado muy lentamente. El medicamento suele tolerarse bien. El riesgo de una arritmia cardíaca es mínimo, comparado con el peligro que conllevaría el no administrarlo. En las formas menos serias se prefiere la vía intramuscular, a la misma dosis, eventualmente repetida a los 15 minutos si la mejoría no es muy notoria.

En el enfermo coronario, el riesgo de una arritmia ventricular puede hacer que se prefiera una amina de acción alfa predominante, como la dopamina o la fenilefrina. La administración de oxígeno por cánula es útil para atenuar la hipoxia tisular concomitante al choque. Los antihistamínicos no tienen una real utilización, por su poca eficacia de acción. Los corticoides son muy útiles para prevenir las reacciones tardías. Es aconsejable la hidrocortisona, 200 mg IV cada 6 horas.

El choque prolongado necesita el servicio de cuidados intensivos. La fuga líquida transcapilar justifica el recurso de líquidos expansores bajo una vigilancia estrecha de la presión venosa central o de la presión capilar pulmonar, puesto que el margen de seguridad entre la hipovolemia y la sobrecarga pulmonar es bastante estrecha. Muchas veces es necesaria la ventilación artificial y, en caso de insuficiencia renal, la hemodiálisis.

Es necesario conocer los factores que favorecen el desarrollo de un choque anafiláctico: el terreno atópico y los antecedentes de alergia medicamentosa. Es importante tener en cuenta también la ansiedad del enfermo, frecuentemente espasmógena.

Ciertas medidas deben aplicarse en caso de una cirugía programada en un paciente con antecedentes atópicos.




</doc>
<doc id="8791" url="https://es.wikipedia.org/wiki?curid=8791" title="Cianosis">
Cianosis

Cianosis es la coloración azulada de la piel, mucosas y lechos ungueales, usualmente debida a la presencia de concentraciones iguales o mayores a 5 g/dL de hemoglobina sin oxígeno en los vasos sanguíneos cerca de la superficie de la piel,o de pigmentos hemoglobínicos anómalos (metahemoglobina o sulfohemoglobina) en los hematíes o glóbulos rojos. Debido a que la cianosis depende de la cantidad y no de un porcentaje de hemoglobina desoxigenada, es mucho más fácil hallarla en estados con aumento en el volumen de glóbulos rojos (policitemia) que en aquellos casos con disminución en la masa eritrocitaria (anemia). Puede ser difícil de detectar en los pacientes con piel muy pigmentada.

Aunque la sangre humana siempre tiene una coloración rojiza (excepto en raros casos de enfermedades relacionadas con la hemoglobina), las propiedades ópticas de la piel distorsionan el color rojo oscuro de la sangre no oxigenada para que parezca azulada.

El principio elemental detrás de la cianosis es que la hemoglobina desoxigenada es más propensa a la coloración azulada óptica, y también produce vasoconstricción que hace que sea más evidente. La dispersión de color que produce el color azul de las venas y la cianosis es similar al proceso que hace que el cielo se vea azul: algunos colores se refractan y se absorben más que otros. Durante la cianosis, los tejidos tienen concentraciones inusualmente bajas de oxígeno, y por lo tanto los tejidos que normalmente están llenos de sangre oxigenada brillante se llenan de sangre desoxigenada, oscura. La sangre oscura es mucho más propensa a los efectos ópticos,y por lo tanto la deficiencia de oxígeno (hipoxia) conduce a la coloración azulada de los labios y otras mucosas.

El nombre se deriva del color cian, que viene del griego κυάνωσις, (cyanōsis), que significa "azul".

La cianosis se divide en dos tipos principales: la central (alrededor del corazón y los labios) y la periférica (solo afecta las extremidades). La cianosis puede ocurrir en los dedos, incluyendo las uñas, además de otras extremidades (cianosis periférica) o en los labios y la lengua (cianosis central), pudiendo conducir también a una inflamación del tejido conectivo en los dedos de las manos, patología conocida como dedos hipocráticos.

La hipoxemia arterial con frecuencia es causada por la alteración de la función pulmonar (hipoventilación alveolar, alteraciones de la ventilación-perfusión, trastornos de difusión de oxígeno) o por la existencia de cortocircuitos o "shunt" intracardíacos derecha-izquierda (defectos septales cardíacos), entre los grandes vasos (conducto arterioso) o en los pulmones. Esto reduce el contenido de oxígeno en los pulmones sangre o una gran pérdida de oxígeno por un descenso de la circulación de la sangre en los vasos sanguíneos de la piel. También puede observarse en la policitemia vera en ausencia de desaturación arterial de oxígeno, debido al incremento de hemoglobina reducida en la sangre.En la cianosis central tanto la piel como las mucosas tienen el color azulado.

Algunas de las causas de cianosis central son las siguientes:

1. En el sistema nervioso central:


2. En el sistema respiratorio:


3.Trastornos cardíacos:


4.Sangre:


5.Otras:


Aparece como resultado de la disminución del flujo sanguíneo periférico y de vasoconstricción. El flujo sanguíneo lento permite que cada hematíe esté en contacto con los tejidos durante más tiempo; en consecuencia, se extrae más oxígeno de la sangre arterial con el posterior incremento de hemoglobina reducida en la sangre venosa. Se observa habitualmente en los tejidos periféricos (manos, orejas, nariz y pies), pudiendo ser generalizada o localizada. Las causas que la originan son múltiples, entre las que se encuentran las mismas que en el caso de la cianosis central, excepto que la periférica puede observarse sin que existan problemas cardíacos o pulmonares. Los pequeños vasos sanguíneos pueden ser restringidos y se puede tratar mediante el aumento del nivel de oxigenación normal de la sangre, por el calentamiento de la zona o por elevación del miembro afectados.

Las causas de la cianosis periférica pueden ser:










</doc>
<doc id="8792" url="https://es.wikipedia.org/wiki?curid=8792" title="Vuelta a España">
Vuelta a España

La Vuelta Ciclista a España, también conocida como la Vuelta a España o simplemente La Vuelta, es una vuelta por etapas profesional de ciclismo en ruta disputada a lo largo de la geografía española. Se celebra entre finales de agosto y principios de septiembre y pertenece al calendario UCI WorldTour, máxima categoría de las carreras profesionales.

La Vuelta se disputó por primera vez en 1935. Desde su creación, la carrera ha sido suspendida la carrera en cuatro ocasiones: desde 1937 hasta 1940 debido a la Guerra Civil, desde 1943 hasta 1944 debido a la Segunda Guerra Mundial y a la mala situación económica de España, en 1949 y desde 1951 hasta 1954.

Es la última y más joven de las conocidas "Grandes Vueltas" del ciclismo, junto al Tour de Francia y el Giro de Italia.

El ganador de la Vuelta a España obtiene 720 puntos para el Salón de la Fama del Ciclismo (Cycling Hall of Fame).

Con cuatro triunfos (2000, 2002, 2003 y 2004), Roberto Heras es el corredor más laureado de la carrera.

Se disputó el G. P. República y las primeras carreras que se corrieron a nivel nacional fueron promovidas por los fabricantes de bicicletas de Éibar. Así pues, se hizo el recorrido Éibar-Madrid-Éibar recibiendo el nombre de "Gran Premio de la República".

A comienzos de 1935, Clemente López Dóriga, en colaboración con Juan Pujol, director del diario "Informaciones", organizó la I Vuelta ciclista a España con un recorrido de 14 etapas y 3431 km en total. La primera etapa se disputó entre Madrid y Valladolid. Aquel año se vivió el primer gran duelo de la historia de la Vuelta entre el belga Gustaaf Deloor, a la postre vencedor, y el español Mariano Cañardo, subcampeón. La II edición de la Vuelta, que se celebró a pesar de la delicada situación política del país, significó la revalidación del título de Deloor que mantuvo el liderato desde el primero hasta el último día. Tras estas dos primeras ediciones, la ronda española sufrió un parón a causa de la Guerra Civil Española.

En 1941 se reanudó la prueba con una participación casi totalmente española y con muy poca representación extranjera. Aquel año tuvo lugar la disputa de la primera etapa contrarreloj de la Vuelta. Julián Berrendero se proclamó vencedor de la ronda española, título que revalidó un año después. Asimismo, Berrendero se convirtió en rey de la montaña durante tres ediciones consecutivas.

A causa de la Segunda Guerra Mundial y la precaria situación económica del país, volvió a producirse otro parón en la disputa de la Vuelta ciclista.

En 1945, el "Diario Ya" se hizo cargo de la organización y volvió a disputarse la competición, aunque de nuevo con una pobre participación extranjera. En esta ocasión fue Delio Rodríguez quien se hizo con la victoria final. Aquel año también se instauró por primera vez la clasificación por puntos, aunque no fue estable hasta 1955. Se disputaron cuatro ediciones más hasta 1950 momento en el cual el «Diario Ya» renunció a organizar definitivamente la Vuelta a España.

Hasta 1955 no se volvió a celebrar la ronda española y a partir de ese año se hizo cargo de ella el diario "El Correo Español/El Pueblo Vasco". Desde entonces, la Vuelta a España se ha disputado anualmente. Además, la Vuelta pasó a celebrarse de manera estable entre los meses de abril y mayo, mientras que anteriormente su celebración había oscilado entre los meses de abril y agosto. Otra variación fue el número de participantes, hasta entonces muy bajo, que se vio duplicado, así como una asistencia mayor de grandes figuras extranjeras y nacionales.

El prestigio de la Vuelta fue creciendo y, cada vez más, se contaba con la presencia de estrellas ciclistas del panorama internacional. Durante finales de los años 1950 se produjeron los primeros triunfos en la clasificación general de ciclistas italianos y franceses. En los años 1960 lo hicieron también ciclistas alemanes y neerlandeses. En 1963, Jacques Anquetil logró liderar la clasificación general y se erigió como el primer ciclista que ganaba las tres Grandes Vueltas. Cinco años más tarde, en 1968, Felice Gimondi haría lo mismo. Solo Eddy Merckx, en 1973, Bernard Hinault que ganó en 1978 y 1983, Alberto Contador que ganó en 2008 y Vincenzo Nibali que ganó en 2010 consiguieron repetir la gesta.

Antonio Karmany dominó la clasificación de la montaña durante tres años consecutivos y lo relevó Julio Jiménez, que la ganó durante otros tres años.

La formación Kas de aquellos años con Karmany, Angelino Soler, Julio Jiménez y Gabica inicio la andadura de un equipazo que nació en 1958 y continuó hasta mediados de los años 80.

Rik van Looy se convirtió en el primer ciclista en repetir triunfo en la clasificación por puntos en 1965. Jan Janssen en 1968 y Domingo Perurena en 1974 hicieron lo mismo y lideraron dicha clasificación en dos ocasiones.

A mediados de los 60 el organizador de la Vuelta, "El Correo Español/El Pueblo Vasco", pasó por algunos apuros económicos que pusieron en peligro la disputa de la competición. Sin embargo terminaron disputándose todas las ediciones de forma normal. En 1968 la Vuelta se vio afectada por un atentado terrorista y otras manifestaciones y se tuvo que anular la decimoquinta etapa. Por fortuna no hubo que lamentar víctimas.

La década de los 70 comenzó con el triunfo de Luis Ocaña, que ya estaba consolidado en el pelotón internacional como una de las grandes figuras del ciclismo.

José Manuel Fuente, con sus triunfos en 1972 y 1974, se convirtió en el tercer ciclista que lograba ganar dos Vueltas a España. Unos años más tarde, Bernard Hinault repetió la hazaña, algo que también logró Pedro Delgado durante los años 1980.

El Equipo Kas en los años 70 innovó en el ciclismo moderno con su forma de correr. Ciclistas como Domingo Perurena, Miguel María Lasa,Vicente López Carril, Pérez Francés, Pesarrodona y muchos otros alegraron con sus amarillos y azules la carrera.

El Super Ser fue otro equipo español de aquella época. Agustín Tamames y Luis Ocaña lideraron el equipo.

En 1973 Eddy Merckx consiguió vencer en la Vuelta y lo hizo de una forma arrolladora ganando seis etapas y todas las clasificaciones individuales con excepción de la montaña, en la que terminó segundo.

Freddy Maertens repitió en 1977 un dominio similar al demostrado por Merckx unos años antes, ganando trece etapas y el resto de clasificaciones individuales exceptuando la de la montaña. Andrés Oliva conseguía también ganar la clasificación de la montaña en tres ediciones de la Vuelta a mediados de los años 1970.

España vio nacer en 1978 a Bernard Hinault como estrella del ciclismo internacional. Hinault ganó ese mismo año su primer Tour de Francia. También hubo que suspender la última etapa de aquella edición, por causa de revueltas y barricadas que impidieron el transcurso normal de la misma.

En 1979 "El Correo Español/El Pueblo Vasco" dejó de patrocinar la ronda española, que corrió una vez más peligro de desaparición. Sin embargo, la empresa Unipublic (que sigue organizando las distintas ediciones de la Vuelta en la actualidad) con el apoyo de la ciudad de Jerez de la Frontera, se hizo cargo de la competición. Este hecho, unido a un aumento de la publicidad y a las retransmisiones por televisión, hicieron aumentar aún más el nivel de la Vuelta.

Fue la época de más popularidad de la Vuelta a España. A principios de los años 1980 destacaron dos nombres en las clasificaciones suplementarias: José Luis Laguía, que lograría imponerse cinco veces en la clasificación de la montaña, y Sean Kelly que venció en cuatro ocasiones en la clasificación por puntos y la general en 1988.

En 1982 se produjo el primer caso de desposesión del título por dopaje. Dos días después del término de la competición, Ángel Arroyo —junto a otros cuantos ciclistas— fue descalificado y perdió su victoria en favor de Marino Lejarreta. A pesar de la solicitud del contraanálisis, este volvió a dar positivo.

La edición del año siguiente supuso la primera aparición de los Lagos de Covadonga como final de etapa, una ascensión que se convertiría, con el paso de los años, en la subida más emblemática de la Vuelta a España. En 1984 se disputó la edición que terminó con la diferencia más pequeña entre el primer y el segundo clasificado. Eric Caritoux, un completo desconocido hasta entonces, logró adjudicarse la Vuelta con tan solo seis segundos de ventaja sobre Alberto Fernández, segundo clasificado, quien moriría en diciembre de ese mismo año en un accidente de transito y en cuyo honor la organización de la Vuelta decidió bautizar a partir de la siguiente edición a la cima de la carrera como la "Cima Alberto Fernández" en homenaje a este gran ciclista.

A partir de 1985 y hasta un poco después del final de la década de los 80 se observó un auge del ciclismo colombiano, que presentó un fuerte dominio sobre todo en las etapas de montaña. Nombres como Francisco Rodríguez (tercero en el 85) u Oscar de Jesús Vargas, tercero en el 89, empiezan a tomar fuerza. Sin embargo, los líderes del ciclismo colombiano fueron Lucho Herrera, vencedor de la carrera, de la clasificación de la montaña y de varias parciales en 1987, y Fabio Parra (2º en 1989), vencedor de la clasificación de novatos en el 85, 5º en otras cuatro ocasiones y ganador de algunas etapas. A pesar del destacado papel de los colombianos, el principal dominador de la vuelta en esa época fue Pedro Delgado con dos victorias (1985 y 1989), un segundo puesto y dos terceros.

Reseñar que durante esta década hubo buenos gregarios y buenos corredores para el recuerdo, como Federico Echave, Iñaqui Gastón, Julián Gorospe, Vicente Belda, Alberto Fernández, Raymon Dietzen, Blanco Villar, Pepe Recio, Eduardo Chozas, Marino Lejarreta, y Enrique Aja; y equipos como El Teka, El Dormilón, El Huesitos, el Reynolds, Bh, y Orbea, que durante aquellos años corrían por las carreteras españolas.

La primera mitad de los años 1990 estuvo marcada por el dominio del suizo Tony Rominger, el primer ciclista que consiguió ganar tres veces la carrera de forma consecutiva, entre 1992 y 1994. En el año 93 Tony Rominger ganó las clasificaciones individuales. En aquellos años 1990 La Vuelta podía contar con el potencial de dos de los mejores equipos españoles que ha habido: la Once y el Banesto con destacados planteles de buenos corredores nacionales e internacionales. También puede destacarse el Clas Cajastur, que luego fue absorbido y terminó siendo el Mapei.

La edición quincuagésima de la Vuelta, disputada en 1995, coincidió con el cambio de fechas. La Vuelta a España pasó a disputarse en septiembre, cerca ya del final de temporada. Aquel año, Laurent Jalabert logró vencer en todas las clasificaciones, algo que nadie más ha logrado en la ronda española. El francés fue también vencedor en cuatro ocasiones de la clasificación por puntos, igualando el récord conseguido por Kelly en los 80.

Los dos años siguientes estarían dominados por otro suizo, Alex Zülle, que aún hoy en día posee el récord de haberse enfundado más maillots amarillos hasta la fecha (48).

En 1997, la Vuelta comenzó por primera vez en un país extranjero. Lo hizo en Lisboa, con motivo de la Expo '98.

La ascensión al Alto de l'Angliru forma parte de una etapa por primera vez en 1999, con el triunfo de José María Jiménez, cuatro veces ganador de la clasificación de la montaña. La fama del puerto creció rápidamente a causa de su dureza y la espectacularidad de la subida. A partir de esta edición se introdujo el maillot oro para identificar al líder de la general. El vencedor, el alemán Jan Ullrich, fue el primero en ganarlo.

Las primeras ediciones de los años 2000 estuvieron marcadas por el dominio de Roberto Heras, que logró alcanzar la victoria también en tres ocasiones, e incluso en 2005 lo consiguió por cuarta vez. Sin embargo, y como ya sucediera con Ángel Arroyo en 1982, fue descalificado días después del término de la competición al dar positivo en un control antidopaje, esta vez por consumo de EPO. Dicho positivo fue ratificado días después por el contraanálisis y Roberto Heras fue desposeído de su cuarto título, en beneficio del ruso Denís Menshov.

El 24 de junio de 2011 el Tribunal Superior de Justicia de Castilla y León anula la sanción por dopaje impuesta el 7 de febrero de 2006 y dictada por el Comité Nacional de Competición y Disciplina Deportiva de la Real Federación Española de Ciclismo, dejando abierta la puerta a la recuperación del título de la Vuelta a España 2005 a Roberto Heras.

El 21 de diciembre de 2012 el Tribunal Supremo de Justicia confirma la anulación de la sanción al corredor bejarano por una serie de irregularidades en la práctica de los análisis, entre ellas que las muestras se entregaron casi 40 horas después, a temperatura ambiente, por persona o empresa que se desconocía, con lo que Roberto Heras vuelve a aparecer en el palmarés de la Vuelta a España como ganador de la edición 2005, siendo además el único ciclista que ha ganado esta carrera en cuatro ediciones.

En el 2006 y tras una dura pugna con el entonces líder del UCI Pro Tour, el murciano Alejandro Valverde, resultó vencedor el kazajo Alexandre Vinokourov. En la edición de 2007 Denís Menshov volvió a alzarse con el triunfo en la general seguido, a más de tres minutos, por los españoles Carlos Sastre y Samuel Sánchez. En 2008, el ganador fue el madrileño Alberto Contador, vencedor aquel mismo año del Giro de Italia, y que conseguía así convertirse en el primer español en ganar las tres Grandes Vueltas.
En 2009, la Vuelta comenzó en Drenthe, Holanda, pasando por Bélgica y Alemania. El vencedor final fue el murciano Alejandro Valverde, quien adoptó una postura conservadora sin ganar ninguna etapa y esprintando en los metros finales para lograr bonificaciones. Sus principales rivales fueron Samuel Sánchez (segundo clasificado), Cadel Evans (tercero), Ivan Basso, Robert Gesink y Ezequiel Mosquera. Todos ellos sufrieron caídas decisivas o pinchazos inoportunos como el de Evans en Monachil, que allanaron a Valverde su conquista del último jersey oro.

La edición de 2010 presenció la reimplantación del jersey rojo, que fue para Vincenzo Nibali tras imponerse en su duelo con Ezequiel Mosquera tras la caída de Igor Antón, en un año en que se subió por primera vez la Bola del Mundo.

Las ediciones posteriores al año 2010 trajeron un cambio en el modelo de la Vuelta. Se incrementaron los finales con altos o puertos explosivos que elevaron el interés de los espectadores a costa de reducir la dureza intermedia que no era tanta hasta ese momento.
Así, figuras como Christopher Froome, Vincenzo Nibali, Alberto Contador, Alejandro Valverde, Joaquim Rodríguez, Nairo Quintana, Fabio Aru y Bradley Wiggins contribuyeron a internacionalizar la Vuelta. A pesar de no ascenderse tantos grandes puertos como en el país transalpino la Vuelta se sitúa como la segunda carrera en participación de primeras figuras de las tres grandes. Fue debido a que se propició que los corredores pudiesen correr el Tour y la Vuelta al ser de los mismos organizadores: Amaury Sport Organisation, y ofrecer un recorrido favorable para ello: poca dureza global pero muchos finales en alto. Hay que destacar que la organización mejoró los recorridos buscando finales nuevos con rutas por carreteras secundarias muy interesantes. Gracias a ese deseo de mejorar los recorridos, se incluyeron nuevos puertos o altos como La Camperona, Ancares, Santuario de la Virgen de Alba, Jitu de Escarandi, Mas de Costas, o la Zubia, todos finales en alto excepto Ancares.

Aparte del éxito de audiencias, esta década trajo la desaparición de equipos de ciclismo como el Euskatel. El panorama de patrocinadores españoles se complicó notablemente y apenas hay equipos locales. Muchos ciclistas no encuentran equipo en España.








Desde el año 2009, las llegadas a Madrid se hacen en la plaza de Cibeles, y no en la plaza de Lima como era el caso justo antes.

El club es reducido Jacques Anquetil, Jan Janssen, Felice Gimondi, Roger Pingeon, Luis Ocaña, Eddy Merckx , Bernard Hinault, Joop Zoetemelk, Pedro Delgado, Jan Ulrich, Alberto Contador, Vincenzo Nibali y Christopher Froome.

Para facilitar el reconocimiento del líder en carrera, este suele portar un maillot con un color determinado, como sucede en el Tour de Francia (maillot amarillo) y en el Giro de Italia (maglia rosa). El maillot de líder de la Vuelta a España no ha sido siempre del mismo color. Hubo varias suspensiones de la carrera y los distintos organizadores que la rescataron eligieron sus colores. Empezó siendo naranja (1935 y 36), luego blanco (1941), otra vez naranja (1942), incluso fue rojo cuando la carrera la cogió el Diario Ya en 1945, aunque luego cambió a blanco con una franja horizontal hasta 1950. En 1955, El Correo resucitó la Vuelta y eligió el amarillo como distinción para el primer clasificado de la prueba, a semejanza del utilizado en el Tour de Francia. Exceptuando el año 1977, en el que el color fue naranja, el maillot amarillo se mantuvo hasta 1999 en el que pasó a ser de color oro. Siendo así hasta la edición de 2010, desde entonces, y hasta la fecha, el maillot de líder, así como el pantalón y el casco, son de color rojo.

Los líderes de las distintas clasificaciones suplementarias también llevan maillots identificativos desde 1950 (con anterioridad, aún existiendo una clasificación de la montaña oficial, no se distinguía al líder de la misma). El color azul estuvo durante bastante tiempo asociado al líder de la clasificación por puntos y el color verde al líder de la clasificación de la montaña (en algunas ediciones su coloración fue roja o granate). El color rojo se asoció durante bastantes años al liderato en la clasificación de las metas volantes. Otras clasificaciones que han existido durante la disputa de la ronda española, como los sprints especiales o la combinada, tuvieron diferentes maillots dependiendo de la edición. Desde 1994 y hasta el 2009 (ambos inclusive), el maillot de la regularidad (o por puntos) pasó a tener una tonalidad granate, mientras que el maillot de la montaña, pasó a ser blanco.

Los demás líderes de las demás clasificaciones a partir de la edición 2010 son exactamente iguales que los del Tour de Francia (salvo el de la montaña, que tiene los lunares azules en vez de rojos) para evitar confusiones a los aficionados no expertos en ciclismo.


Otra característica diferenciadora sobre otras Grandes Vueltas es que nunca ha existido clasificación de los jóvenes y en su lugar se utiliza la clasificación de la combinada que tiene en cuenta los puestos de los corredores en la clasificación general, la clasificación por puntos y la de la montaña.

Las alabanzas al modelo de recorridos de la Vuelta se han centrado en que "«todos los días pasa algo»", debido a la elaboración de recorridos de etapas con algún tipo de aliciente, de forma que no pasen demasiados días seguidos sin que se produzca algún tipo de actividad entre los favoritos de la carrera.

Las críticas se centran en la proliferación de finales en alto, que se argumenta que impiden los ataques lejanos porque con atacar unos pocos kilómetros cada día sin riesgo vale para sacar tiempo; así como la falta de contrarreloj (tendencia que también están adoptando las otras Grandes Vueltas), que fuerce a los escaladores a atacar en montaña; y la falta de inclusión de puertos de montaña de similar dureza a los de Giro de Italia y Tour de Francia, hacen de la vuelta una carrera menor. Se critica especialmente la ausencia de puertos de paso de categoría especial que puedan romper la carrera lejos de meta, ya que en la Vuelta 2012 hubo uno (San Lorenzo) y en el Giro de Italia 2012 y el Tour de Francia 2012 hubo siete y seis, respectivamente, en cada uno. De hecho son conocidos los "«especiales de paso»" del Gavia y Mortirolo (en el Giro de Italia) o el Aubisque, Tourmalet y Galibier (en el Tour de Francia) mientras en la Vuelta no hay ningún puerto conocido de esas características debido a que casi siempre se utilizan como "«final en alto»", y el puerto más alto de la carrera casi siempre suele ser "«final en alto»". Además, la "«etapa reina»" se sitúa en las últimas jornadas con lo que no hay necesidad de arriesgar con ataques lejanos hasta después de esa etapa debido a las escasas diferencias y ya después apenas hay oportunidades para hacerlo. Todo esto provoca que todas las etapas tengan un desarrollo parecido y que las diferencias sean mínimas equiparando su desarrollo a muchas carreras de una semana.

Es significativo que históricamente la Vuelta a España sea la Gran Vuelta con menos dureza cuando en España se encuentra la (pico Veleta a 3367 msnm, con múltiples vertientes y puertos para acceder a ella) y sea uno de los países más altos de Europa con 650 metros de media solo por detrás de Suiza, Andorra, Austria y Liechtenstein con la diferencia que en ellos no hay costa (habitualmente a 0 msnm). El ejemplo más significativo es que puede pasar de los 0 msnm de la Costa de Almería hasta más de 2000 msnm de la Sierra de Los Filabres en menos de 50 km.

Debido a que el criterio para catalogar un puerto de categoría especial es subjetivo en este cuadro se muestran los puertos/altos con más de 2000 msnm que hay en la península ibérica de España -se omiten las islas por motivos logísticos- y las veces que se han pasado en la Vuelta a España para demostrar el escaso uso de puertos de estas características, normalmente con más de 1 hora de ascension, en la ronda española.
En 1996 el asturiano y director de información de la ONCE, Miguel Prieto, después de visitar El Gamonal se puso en contacto con la empresa organizadora de la Vuelta Ciclista a España (Unipublic) proponiendo dicha ascensión como final de etapa. Esta propuesta no cayó en saco roto, estando como estaba en ese momento La Vuelta buscando un final de etapa del mismo renombre, resonancia y dureza como era la ascensión a los Lagos de Covadonga. En 1997 el Ayuntamiento de Riosa arregló la carretera y en 1999 fue por primera vez final de etapa.

"Para saber más sobre esta etapa, véase Alto de l'Angliru"

"Para los ganadores de las clasificaciones secundarias, véase "

"Para los datos estadísticos, véase "

Actualizado a 10.09.17

"Para más datos, véase "





</doc>
<doc id="8793" url="https://es.wikipedia.org/wiki?curid=8793" title="Tour de Francia">
Tour de Francia

El Tour de Francia (oficialmente Le Tour de France), también conocido simplemente como el Tour, es una vuelta por etapas profesional de ciclismo en ruta disputada a lo largo de la geografía francesa. Se celebra en julio y pertenece al calendario UCI WorldTour, máxima categoría de las carreras profesionales

Considerada la carrera más importante del mundo, el Tour se disputó por primera vez en 1903. Desde su creación, la carrera se ha visto interrumpida en dos ocasiones debido a las dos guerras mundiales: desde 1915 hasta 1918 y desde 1940 hasta 1946.

Es la segunda y más antigua de las conocidas "Grandes Vueltas" del ciclismo, junto al Giro de Italia y la Vuelta a España.

El ganador del Tour de Francia obtiene 1800 puntos para el Salón de la Fama del Ciclismo (Cycling Hall of Fame), siendo la prueba ciclista que más puntos otorga al ganador.

Con cinco triunfos, son cuatro los ciclistas que poseen el récord de victorias en "La Grande Boucle": Jacques Anquetil (1957, 1961, 1962, 1963 y 1964), Eddy Merckx (1969, 1970, 1971, 1972 y 1974), Bernard Hinault (1978, 1979, 1981, 1982 y 1985) y Miguel Indurain (1991, 1992, 1993, 1994 y 1995).

El Tour de Francia fue galardonado con el Premio Príncipe de Asturias de los Deportes en el año 2003.

Existió un Tour de Francia femenino que comenzó en 1955 (desde 1984 disputándose con regularidad) hasta 2009, siendo de las pocas carreras femeninas con una duración superior a una semana junto al Giro de Italia Femenino y el Tour de l'Aude Femenino (este también ya desaparecido), aunque durante sus últimos 15 años sin relación con la de hombres.

El Tour de Francia de 1903 fue la primera competición ciclista por etapas de la historia. Anteriormente se habían realizado competiciones que cubrían enormes distancias, como el recorrido París-Brest-París de 1200 km en 1891 o Burdeos-París de 576 km también en 1891. Sin embargo, fue el periodista francés Géo Lefèvre quién desarrolló la idea de crear una competición por etapas que transcurriera por parte del territorio francés. Lefèvre propuso al director del periódico deportivo "L'Auto", Henri Desgrange, crear una competición ciclista para promocionar el diario. Así, el 19 de julio de 1903 el primer Tour de Francia comenzó en Montgeron, cerca de París, donde tomaron la salida 60 ciclistas que cubrieron la etapa inaugural de 467 km hasta Lyon. El recorrido constaba de seis etapas con un total de 2428 km. El francés Maurice Garin fue el vencedor del primer Tour de la historia, completando la prueba a una velocidad de 25 km/h. Recibió un premio de 6 075 francos.

Las siguientes ediciones del Tour de Francia estuvieron marcadas por una serie de escándalos que culminaron en la exclusión de los cuatro primeros de la clasificación general del Tour de Francia 1904, en parte como resultado del uso no autorizado de la vía férrea. El período anterior a la Primera Guerra Mundial se ve en retrospectiva como una época heroica, ya que en ese momento se cubrieron regularmente distancias diarias de 400 kilómetros. Desde la perspectiva actual, parece asombroso si se tiene en cuenta el modesto equipo técnico de aquella época y la mala calidad de las carreteras, que solían ser de adoquines. Posteriormente entraron en escena las etapas de montaña. Así, en 1905 se produjo la primera subida al Ballon d'Alsace en los Vosgos. Más tarde, en 1910, se ascendió por primera vez el Tourmalet, en los Pirineos y en 1911 se iniciaron los ascensos a los Alpes. De esta época destacan ciclistas como el belga Philippe Thijs, quien fue el primero en lograr tres victorias en el Tour. Lamentablemente su carrera, como la de muchos ciclistas profesionales en Europa, se vio interrumpida por el estallido de la Primera Guerra Mundial en 1914, que provocó la suspensión de la competición durante cuatro ediciones. Después de la guerra, el Tour regresó en 1919 con la novedad del maillot amarillo para distinguir al líder de la carrera, en honor del color de las páginas del periódico "L'Auto". El ciclista francés Eugène Christophe fue el primer corredor que lució la prenda.

Originalmente, el Tour de Francia se disputaba de manera individual, y estaba prohibido el trabajo en equipo. Los ciclistas podían optar por contar con patrocinador o no. En 1930 se legalizaron los equipos nacionales.

El número de etapas se incrementó gradualmente a once (1905), quince (1910), dieciocho (1925) y, finalmente, a veinticuatro etapas (1931). La longitud total del Tour continuó aumentando hasta los 5500 kilómetros. Ya en las primeras ediciones el Tour pasó por otros países vecinos de Francia. Así, desde 1905 se empezaron a disputar etapas en Alemania, y en 1906 transcurrió por primera vez por España e Italia. Con el tiempo, se fueron incluyendo etapas de manera regular en todos los vecinos actuales de Francia como Suiza (primera vez en 1913), Bélgica (desde 1947), Luxemburgo (1947), Mónaco (1952) y Andorra (1964). También se han disputado etapas en países no fronterizos con Francia tales como los Países Bajos, Gran Bretaña e Irlanda. En 1933 se introdujo la distinción al mejor escalador y se otorgaron bonificaciones a los ciclistas que alcanzasen los puertos en primer lugar.

En 1936 Jacques Goddet sustituyó a Desgrange en la dirección del Tour de Francia, cargo que ocuparía hasta 1987. Goddet siempre fue favorable a las innovaciones técnicas en las competiciones e introdujo la clasificación por puntos, así como el prólogo al principio de la carrera.

El italiano Gino Bartali ganó con autoridad la edición de 1938,la última antes de la Segunda Guerra Mundial. En la reanudación de 1948 repitió triunfo el veterano escalador de la Toscana y apareció en escena su compatriota Fausto Coppi, Il Campionissimo, quien ganó en 1949 y 1952 siendo un precursor de las técnicas de trabajo en equipo, el entrenamiento y la dieta del ciclista de carretera. Hubo victorias intercaladas de los suizos Kübler y Koblet en 1950 y 51. Tres serían las victorias consecutivas del gran campeón galo Louison Bobet entre el 53 y el 55 como antesala a la época dorada del primer quíntuple campeón de la Ronda Francesa, Jacques Anquetil.

En el año 1957 se produce el primer reportaje televisivo en directo y al año siguiente, comienzan a transmitirse fragmentos de etapa. En ese mismo año, el joven francés Jacques Anquetil, de 23 años, logra su primer Tour con una gran superioridad. Anquetil fue el primer ciclista que ganó cinco veces el Tour y que lo ganó cuatro veces consecutivas al imponerse entre 1961 y 1964, gracias a su habilidad como contrarrelojista y a su progresiva adaptación a la montaña. Uno de los grandes rivales de Anquetil fue Federico Martín Bahamontes, que destacaba por ser un gran especialista en montaña, consiguiendo el Tour en 1959 y siendo el primer español en ganarlo. Durante la década de los sesenta destacan también ciclistas como el italiano Felice Gimondi, ganador del Tour de 1965, o el francés Raymond Poulidor, apodado el "eterno segundón".

En 1962 se abandonó la composición de los equipos por países y se adoptó definitivamente por equipos profesionales patrocinados por empresas. Se volvieron a prohibir los equipos patrocinados en 1967 y 1968, reestableciéndose los equipos nacionales, para luego legalizarse definitivamente en 1969.

Más adelante, irrumpe en escena el considerado para muchos como el mejor ciclista de la historia, el belga Eddy Merckx, quien en su primera aparición en el Tour de 1969 consigue la victoria. Merckx iniciaría un espectacular dominio de la ronda gala proclamándose también vencedor en las ediciones de 1970, 1971, 1972 y 1974, igualando así los logros de Jaques Anquetil. Eddy Mercx aún posee el récord de triunfos de etapa del Tour con un total de 34 victorias y fue apodado "El Caníbal" debido a su insaciable sed de victorias. El reinado de Merckx solo se vio interrumpido por la victoria del español Luis Ocaña en la edición de 1973, en la que el belga no participó.

Tras la era de Merckx y las victorias de Bernard Thevenet y Lucien Van Impe se iniciaría el dominio del mítico Bernard Hinault quien igualaría las cinco victorias de Anquetil y Merckx, dominando desde finales de los setenta hasta mediados de los ochenta. De la época de Hinault destacan ciclistas como el holandés Joop Zoetemelk, vencedor en 1980 y segundo en otras seis ocasiones, o el francés Laurent Fignon ganador de las ediciones de 1983 y 1984. En 1986 Greg LeMond se convirtió en el primer ciclista no europeo en proclamarse vencedor del Tour, quien repetiría victoria en 1989 y 1990. De finales de los ochenta destacan también las victorias del irlandés Stephen Roche en 1987 y del español Pedro Delgado en 1988. En la década de los ochenta se da la masiva participación en el Tour de ciclistas procentes de todo el mundo, especialmente de América, destacando a los colombianos como notables escaladores, particularmente Luis Herrera y Fabio Enrique Parra.

En 1991 se inicia el dominio del español Miguel Indurain, quien fue el primer ciclista en lograr cinco victorias consecutivas al vencer de 1991 hasta 1995. El gran dominio de Indurain dejó a la sombra a otros grandes ciclistas de la década de los noventa como Richard Virenque o Tony Rominger entre otros. En 1996 el danés Bjarne Riis ganó el Tour de Francia, terminando con la era de Miguel Indurain. Sin embargo, Riis confesó años más tarde haberse dopado con EPO en el período 1993-1998 aunque oficialmente no se le ha retirado el Tour. En 1997 el joven alemán Jan Ullrich se hizo con la victoria en el Tour. Ullrich, que había sido segundo en 1996, destacaba como contrarrelojista y se defendía bien en todos los terrenos, lo que le permitió lograr una victoria con superioridad. En 1998 se esperaba de Ullrich una gran victoria. Sin embargo, el escalador Marco Pantani le sacó ocho minutos en una etapa de montaña en la que se pasaba el Col du Galibier y se llegaba a meta en la cima de Les Deux Alpes y se puso el maillot amarillo. Pantani mantuvo dicho maillot hasta el final del Tour, pese a los intentos de Ullrich de quitárselo, que de nuevo volvía a ser el segundo clasificado. Además en este año salto a la luz el Caso Festina, en el que se vieron implicados corredores de gran importancia como Richard Virenque.

Desde 1999 a 2005 el ganador fue el estadounidense Lance Armstrong. Sin embargo, el 23 de agosto de 2012 la Agencia Antidopaje estadounidense (USADA) decidió retirarle sus siete títulos del Tour de Francia por dopaje, además de suspenderlo de por vida. La decisión de desposeer al estadounidense de sus victorias fue ratificada posteriormente por la Unión Ciclista Internacional (UCI), que decidió además declarar desierto el título correspondiente a esas ediciones.

En 2006 el ganador fue Óscar Pereiro, tras la descalificación del estadounidense Floyd Landis. En 2007 Alberto Contador se impuso en un Tour claramente marcado por el dopaje, que dejó fuera a Alexandre Vinokourov y a su equipo, el Astana. También se vio forzado a retirarse el danés Michael Rasmussen, cuando faltaban cuatro etapas para el final del Tour y siendo líder de la clasificación general.

El Tour de 2008 estuvo marcado por la ausencia del equipo Astana, donde figuraba el entonces vigente ganador. La organización castigó de esta forma el positivo del kazajo Alexandre Vinokourov de la anterior edición. A pesar de esta ausencia, otro español, Carlos Sastre, se adjudicó una victoria forjada en la montaña, especialmente con un ataque en Alpe d'Huez que le sirvió para sacar más de dos minutos a Cadel Evans, segundo clasificado.

A estas tres victorias españolas hay que sumarle una segunda lograda por Alberto Contador en el Tour 2009, junto al equipo Astana. Tour marcado por la supremacía marcada por el español en las etapas de montaña, como el Arcalis y Verbier, y la mostrada en la contrarreloj de Annecy (victoria en las dos últimas), por la vuelta a la carretera de Lance Armstrong a sus 38 años y por la tensión vivida en el equipo Astana entre los dos líderes. Y posteriormente la de 2010, con la que el corredor nacido en Pinto conseguía su tercer Tour. Tras una apretada contrarreloj final se impondría por tan solo 39 segundos a su immediato rival, Andy Schleck. De todos modos, Contador dio positivo por 50 picogramos de clembuterol. El de Pinto arguyó que el positivo se debía a una ingesta de carne contaminada y le fue permitido seguir compitiendo hasta que se juzgase su caso. En ese periodo de tiempo ganó el Giro de Italia 2011 y corrió el Tour de Francia 2011 en el que fue quinto. Finalmente el corredor fue desposeído de su Tour de 2010, que pasaría a las manos de Andy Schleck.

En el Tour de Francia 2011, Cadel Evans se adjudicó la victoria tras una contrarreloj final, superando a Andy Schleck en Grenoble. En 2012 el ganador fue el británico Bradley Wiggins en un Tour en el que su compañero de equipo, gregario y compatriota Chris Froome fue segundo. Fue la primera vez que un ciclista del Reino Unido se proclamaba campeón. Al año siguiente, fue Froome el vencedor.

En enero de 2013 Lance Armstrong, después de sufrir una gran presión por sus excompañeros, confiesa y admite en una esperada entrevista con Oprah Winfrey, que los siete Tours ganados entre los años 1999 a 2005 son fruto del dopaje, dejando frases que pasarán a la posteridad como: "Todo se ha tratado de una gran mentira que resultó bastante perfecta durante mucho tiempo", "Sí, me dopé; el cuento de hadas no era cierto" o "Hijo no me defiendas más, lo siento". Una vez hecha la confesión de Armstrong, la UCI acepta la demanda puesta por la USADA y anula todas sus victorias.

En la prueba de 2014, ciclistas como Chris Froome y Alberto Contador se caerían y se retirarían debido a la dura climatología de los primeros días del Tour. Ante esto, solo quedaría un favorito, Vincenzo Nibali que ganaría la prueba con mucha superioridad ante sus rivales más inmediatos, Thibaut Pinot y Jean-Christophe Peraud. Gracias a su victoria en el Tour Nibali se convertiría en el sexto ciclista en ganar las tres Grandes Vueltas obteniendo la Triple Corona del Ciclismo, al ganar la Vuelta de 2010, el Giro de 2013 y el Tour de 2014.

 maillots actuales.
Durante cada etapa, los líderes de las clasificaciones deben portar una camiseta (maillot) que los distinga del resto de los corredores.

El "ciclista más combativo" tiene derecho a llevar en la etapa siguiente un dorsal con los números blancos sobre fondo rojo y los integrantes del equipo que marcha primero llevan el dorsal con números negros sobre fondo amarillo.

"Para los ganadores de las clasificaciones secundarias, véase "

"Para los datos estadísticos, véase "

Estos son los ciclistas que han ganado más etapas:
Ciclistas de 33 países han logrado obtener una o más victorias de etapa:












"Para más datos, véase "

En España, los derechos de emisión de la ronda ciclista gala los tiene desde mediados de los años 60 del siglo XX la cadena pública TVE. Hasta 2003 se emitían todas las etapas en directo por La 1, excepto cuando coincidían con el "Telediario" o alguna otra retransmisión especial; entonces se televisaban por La 2. De 2004 a 2011, el Tour de Francia se emitió, primero, por La 2, y más tarde por Teledeporte, y únicamente se solía emitir alguna etapa muy destacada por el primer canal de TVE. En 2012 el Tour volvió a emitirse diariamente por La 1, pero a partir de 2013 volvió a Teledeporte, con solo las etapas más destacadas por La 1.

Desde la muerte de Pedro González en enero de 2000, los comentaristas del Tour en TVE son Carlos de Andrés y Pedro Delgado.

Por otra parte, en algunas ediciones desde la década de 1990, e ininterrumpidamente desde 2009, TVE comparte con ETB 1 la emisión en abierto de las etapas para la comunidad autónoma del País Vasco y la Comunidad Foral de Navarra, tras largas negociaciones que comenzaron en el Giro de Italia 2008.

En cuanto a canales de pago, es Eurosport quien emite la ronda gala para España, con Antonio Alix y Eduardo Chozas como comentaristas.




</doc>
<doc id="8794" url="https://es.wikipedia.org/wiki?curid=8794" title="Giro de Italia">
Giro de Italia

El Giro de Italia () es una competición ciclista por etapas de tres semanas de duración, disputada en el mes de mayo en Italia con un recorrido diferente cada año. En ocasiones también se disputa alguna etapa en los países colindantes. Es una de las tres Grandes Vueltas, la segunda en aparecer históricamente. Dejó de formar parte del UCI ProTour, como las otras dos grandes vueltas, para posteriormente integrarse en el UCI World Ranking y UCI WorldTour.

El primer Giro de Italia comenzó el 13 de mayo de 1909 en Milán con un total de 8 etapas y 2.448 kilómetros.

Tres ciclistas comparten el récord de victorias en esta competición con cinco triunfos: Alfredo Binda (entre 1925 y 1933), Fausto Coppi (entre 1940 y 1953) y Eddy Merckx (entre 1968 y 1974).

El corredor con mayor número de victorias de etapas es Mario Cipollini, que en la edición de 2003, superó el récord de 41 victorias que poseía Alfredo Binda desde los años treinta.
Desde 1988 existe un Giro de Italia Femenino, siendo de las pocas carreras femeninas de más de una semana junto a la Grande Boucle y el Tour de l'Aude Femenino (estas ya desaparecidas), aunque sin relación con la de hombres.

Al igual que el Tour de Francia con el periódico L'Auto, la creación del Giro de Italia está ligada a un periódico deportivo, La Gazzetta dello Sport. 

La rivalidad con otro periódico italiano (el Corriere della Sera) que organizaba el Giro de Italia en Automóvil y planeaba organizar un Giro de Italia en bicicleta, llevaron al periodista Tullio Morgagni a plantearle a su director Eugenio Camillo Costamagna la creación de una carrera ciclista por etapas inspirándose en el Tour de Francia. La Gazzetta, previamente ya había comenzado a organizar carreras ciclistas como el Giro de Lombardía en 1905 y la Milán-San Remo en 1907. 

El 7 de agosto de 1908, bajo el liderazgo de Eugenio Camillo Costamagna, Armando Cougnet y Tullio Morgagni, el periódico anunció el nacimiento del Giro de Italia, cuya primera edición sería en 1909, anticipándose al Corriere della Sera.

Con la participación de 127 corredores, el 13 de mayo de 1909 a las 2:53 a. m. se largó la primera edición en la plaza de Loreto en Milán rumbo a Bolonia. Fueron 8 etapas para un total de 2.448 kilómetros, corriéndose una etapa cada dos o tres días, ya que La Gazzetta dello Sport era una publicación trisemanal. El reglamento utilizado fue el mismo que se usaba en el Tour de Francia, con una clasificación por puntos según el orden de llegada en las etapas y no una clasificación por tiempos. Cuarenta y nueve ciclistas lograron completar el recorrido y el ganador de esa primera edición fue Luigi Ganna que sumó 27 puntos. Su gran rival Giovanni Rossignoli, finalizó tercero con 40 puntos, pero si se hubiera tomado en cuenta los tiempos, Rossignoli habría ganado por más de 37 minutos.

Las primeras ediciones fueron sufriendo varias modificaciones: las etapas variaron de ocho a doce, en 1911 la carrera empezó y terminó en Roma, en 1912 se corrió por equipos y en 1914 se dejó de usar el sistema de puntos para pasar a la clasificación por tiempo.

Carlo Galetti fue el primero en ganar dos veces la carrera (1910 y 1911). En 1912, Galletti fue quien menos tiempo invirtió en realizar el recorrido, pero como se corrió en forma de equipos, ganó el equipo Atala (del cual era integrante), en lo podría haber sido su tercera victoria.

En 1913, el Tour de Francia pasó a utilizar la clasificación por tiempos. El Giro lo hizo una año después, en 1914, última edición antes de la suspensión debida a la Primera Guerra Mundial. Alfonso Calzolari fue el ganador de esa edición, superando por casi dos horas al segundo.

Tras la guerra, en 1919 volvió la disputa de la carrera. El piamontés Costante Girardengo obtuvo la victoria en siete de las diez etapas, logrando el primero de sus dos Giros (en 1923 repitió la victoria y ganó ocho etapas). En esa edición de 1919 se produjo el primer podio extranjero con el belga Marcel Buysse que finalizó tercero. Giovanni Brunero fue otro de los destacados de la década de 1920, al ganar tres Giros (1921, 1922 y 1926). Un hecho particular e inédito hasta el día de hoy, ocurrió en 1924 cuando participó una mujer, Alfonsina Strada. En una época en que no estaba bien visto que una mujer compitiera, Strada había corrido el Giro de Lombardía en 1917 y 1918 y en 1924 se inscribió en el Giro. Aunque no figuró en las primeras posiciones, tampoco estaba entre las últimas, hasta la octava etapa cuando fue descalificada por llegar fuera de tiempo, aunque se especuló que ante las críticas que se hicieron, la organización decidió sacarla de la carrera. Igualmente, le permitieron seguir en carrera pero sin tiempo y llegó al final en Milán.

Los años 1920 vieron surgir a uno de los ciclistas más grandes de todos los tiempos, Alfredo Binda, quién ganó cinco Giros; 1925, 1927, 1928, 1929 y 1933. En total logró 41 victorias de etapa, ganando 12 de las 15 en 1927 y 8 consecutivas en 1929. La supremacía de Binda era tal que La Gazzetta dello Sport en 1930 le pagó 22.500 liras para que no corriera el Giro, con el fin de mantener el interés de la carrera.

Armando Cougnet, director del Giro, decidió en 1931 otorgar un símbolo que hiciera reconocible a simple vista al líder de la carrera. Así nació la maglia rosa, tomando el color de las páginas de La Gazzetta dello Sport. El primer maglia rosa fue Learco Guerra, ganador de la primera etapa del Giro 1931 entre Milán y Mantova. En 1933, se le hicieron a la carrera algunas modificaciones: por primera vez se corrió una etapa contrarreloj entre Bolonia y Ferrara y además coincidiendo con la primera incursión por los Alpes se comenzó a disputar el Gran Premio de la montaña.

En los albores de la Segunda Guerra Mundial, Gino Bartali ya era famoso, habiendo ganado en 1936 y 1937. En 1940 la participación extranjera fue escasa, debido a que ya había comenzado la Segunda Guerra Mundial. El equipo Legnano del cual Bartali era integrante, contrató al joven Fausto Coppi de 20 años como gregario. Todo el equipo debía trabajar para Bartali, que tenía como rival a Giovanni Valetti del equipo Bianchi, ganador de las ediciones de 1938 y 1939. En las primeras etapas, Bartali perdió casi 15 minutos, mientras que Coppi fue segundo en dos etapas y estaba en los primeros lugares de la clasificación. El técnico de la Legnano, decidió que Coppi no fuera más gregario y fuera por la carrera, debiendo convencer a Bartali de que fuera el gregario a la vez que el "maestro" del joven Coppi. En la 11ª etapa con final en Módena, Coppi ganó en solitario colocándose la maglia rosa que mantuvo hasta el final en Milán. Con 20 años, 8 meses y 25 días, Fausto Coppi se convirtió en el ciclista más joven en ganar el Giro, récord que aún se mantiene. Al día siguiente, Italia declaró la guerra a Francia y el Giro sufrió la segunda interrupción en su historia.

Tras el paréntesis de la guerra, el Giro volvió en 1946. Bartali continuaba en el equipo Legnano y Coppi corría por el Bianchi. Las diferencias políticas y religiosas entre ambos dividieron a Italia. La Democracia Cristiana y los católicos estaban a favor de Bartali y la izquierda y los laicos a favor de Coppi, aunque ambos tuvieron una relación cordial pese a la rivalidad. El duelo de 1946 fue ganado por Bartali, logrando su tercer Giro. Coppi se tomó revancha en 1947 y en los años siguientes ganó 3 veces más, igualando el récord de Binda. La bipolarización Coppi-Bartali fue rota por Fiorenzo Magni que en ese período ganó 3 Giros y por el suizo Hugo Koblet, primer extranjero en subirse a lo más alto del podio en 1950.

Tras el triunfo de Koblet en 1950, los extranjeros lograron dominar varias ediciones. El luxemburgués Charly Gaul gracias a sus condiciones de escalador lo hizo en dos oportunidades (1956 y 1959) y el francés Jacques Anquetil (quíntuple ganador del Tour de Francia) también en dos (1960 y 1964). Entre las victorias de Anquetil, el italiano Franco Balmanion logró los Giros de 1962 y 63 aunque no ganó ninguna etapa. 

En 1966, Gianni Motta, ganó la clasificación general y la clasificación por puntos, que por primera vez se comenzó a disputar. A partir de 1967 se otorgó maillot identificatorio al líder de esa clasificación, siendo los primeros dos años la maglia rossa y luego la maglia ciclamino. Esta última fue usada hasta la edición de 2009, volviendo en 2010 a la roja.

A finales de los años 1960 Felice Gimondi era el gran ciclista italiano del momento. Había ganado el Tour de Francia en 1965, el Giro en 1967 y la Vuelta a España en 1968. En 1967 un joven Eddy Merckx fue noveno en el Giro y ganó 2 etapas, preámbulo de su época dorada. De las siete ediciones corridas entre 1968 y 1974, Merckx ganó 5, alcanzando el récord de Binda y Coppi. Solo vio cortada su hegemonía por Gimondi en 1969 cuando fue descalificado por un control antidopaje positivo y por el sueco Gösta Pettersson en 1971. 

En 1976 Gimondi ganó su tercer Giro. Con ese triunfo se subió el podio por novena vez, siendo el ciclista con más podios en la historia. En 1974, se comenzó a entregar al líder de la clasificación de la montaña la "maglia verde". Este distintivo fue usado hasta 2012, cuando fue cambiada por la azul.

A fines de la década de 1970 y principios de los 80, los duelos entre Giuseppe Saronni y Francesco Moser reavivaron el Giro. Saronni logró los triunfos en 1979 y 1983, mientras que Moser en 1984. Ambos lograron la clasificación por puntos en cuatro oportunidades y se mantienen al frente como los más ganadores de esta clasificación. En medio de este duelo Bernard Hinault se llevó tres ediciones: las de 1980, 1982 y 1985.

La edición de 1988, estuvo marcada por dos hechos: el triunfo del estadounidense Andrew Hampsten, primer no europeo en ganar la carrera y el ascenso al Passo di Gavia y posterior descenso hasta Bormio en condiciones climatológicas muy adversas. La lluvia y una tormenta de nieve obligó a varios ciclistas a descender en automóvil ya que les era imposible hacerlo en bicicleta.

En los primeros años de la década de 1990 Gianni Bugno, Claudio Chiappucci y Franco Chioccioli eran los grandes animadores de la ronda italiana. Bugno ganó en 1990 y Chioccioli en 1991, pero todos se vieron eclipsados por el español Miguel Indurain. El navarro dominó la carrera italiana en 1992 y 1993 y cuando se esperaba su tercer Giro en 1994, el ruso Eugeni Berzin dio la sorpresa adjudicándose la maglia rosa en la cuarta etapa y manteniéndola hasta el final, ganándole las dos etapas contrarreloj a Indurain. Tras el éxito del suizo Tony Rominger en 1995 y el ruso Pavel Tonkov en 1996, comenzó un ciclo de once ediciones donde los italianos dominaron el Giro. Marco Pantani ganó en 1998 y partió como favorito en 1999, llevando la maglia rosa hasta que faltando dos etapas fue excluido de la carrera por tener altos niveles de hematocrito. Ivan Gotti ganó ese año, siendo su segunda victoria tras su triunfo en 1997.

En esa misma década y principios de los 2000, brilló en las llegadas masivas el esprínter Mario Cipollini. En el período 1989-2003 logró 42 triunfos de etapa y batió el récord de Binda que estaba vigente desde los años 1930.

En el período 1997-2007, dos victorias para Ivan Gotti, dos de Gilberto Simoni, dos de Paolo Savoldelli, más los triunfos de Garzelli, Cunego, Basso y Di Luca, marcaron la hegemonía italiana que no se daba desde que el primer extranjero ganó en 1950. El ciclo italiano fue cortado por Alberto Contador en 2008.

En 2003, en el ocaso deportivo de Cipollini hizo aparición otro esprínter italiano, Alessandro Petacchi. Petacchi ganó seis etapas en 2003 y en 2004 nueve.

Denis Menchov en 2009 fue el tercer ruso en ganar la carrera y en 2010 Basso ganó su segundo Giro. Contador ganó su segundo Giro en la ruta en 2011, pero lo perdió en las oficinas del TAS tras la sanción por el caso Contador y la victoria final fue para Michele Scarponi. 

La edición de 2012 fue para Ryder Hesjedal, primer canadiense y segundo ciclista del otro lado del Atlántico en ganar el Giro. 
En 2013 el italiano Vincenzo Nibali gana su primer giro después de dos podios anteriormente (2do y 3ro respectivamente).
En 2014 el colombiano Nairo Quintana, en su primera participación en la carrera, se proclamó como el primer latinoamericano en ser campeón del Giro, siendo además el mejor joven de la carrera. Ese mismo año el también colombiano Rigoberto Urán resultó subcampeón del Giro por segunda vez consecutiva. 
En el 2015 el experimentado Alberto Contador gana su segundo giro ante una joven promesa del ciclismo italiano Fabio Aru.

Vincenzo Nibali se hizo con su segundo triunfo en la ronda italiana del 2016, en una edición de gran dureza en la alta montaña.

 maglias actuales.

El líder de la clasificación general se distingue por llevar una "maglia rosa" (maillot del color del diario deportivo milanés La Gazzetta dello Sport que organiza la carrera), el líder de la clasificación de la montaña, desde 2012 lleva la "maglia azul", (anteriormente fue la "maglia verde"). El líder de la clasificación por puntos o de la regularidad lucía la "maglia rosso passione" o "rossa" (actualmente es "ciclamino") y el líder de la clasificación para menores de 25 años lleva la "maglia blanca".
En el Giro 100, no se usa la "maglia" roja, fue sustituida por la original "maglia ciclamino".

El Giro se caracteriza también por tener multitud de clasificaciones secundarias, la mayoría de ellas sin malla distintiva debido a la limitación de la UCI de solo poder haber hasta 4 maillots de clasificaciones. Entre estas destacan las del Intergiro renombrado por "Expo Milano 2015" (que tradicionalmente sí ha tenido maillot azul que lo identificaba) y la de por equipos. Otras han sido por ejemplo "Trofeo Super Team", "Traguardo Volante" (metas volantes), "Trofeo Fuga Cervelo" (más kilómetros en fuga), "Fair Play", "Maglia Nera" (último de la clasificación con el dorsal en negro), "Azzurri d'Italia", "Most Combative" (combatividad)...

Al contrario que en el resto de las Grandes Vueltas e incluso de muchas vueltas por etapas, en el Giro no ha existido la catalogación habitual de los puertos (de mayor a menor dificultad: Especial, 1ª, 2ª, 3ª y 4ª) sino por colores (azul -puerto más alto-; verde -puerto final de etapa-; y el resto de mayor a menor dificultad: rojo, amarillo y gris) por lo que a veces ha habido cierta confusión respecto a la dificultad real de los puertos. Por ello a partir del 2011 se introdujo la catalogación por números aunque sin categoría Especial, con lo que la mayoría de puertos están numerados una categoría por debajo de lo que estarían en otras carreras que utilizan este tipo de catalogación.

En cada edición, el puerto de más altitud que deban afrontar los ciclistas se le denomina Cima Coppi y otorga más puntos que los puertos de 1ª categoría.

Desde sus inicios hasta la actualidad, el Giro de Italia ha tenido seis directores generales:

<nowiki>*</nowiki> El Giro de Italia de 1912 se disputó por equipos. El equipo "Atala" estaba formado por Carlo Galetti, Giovanni Michelotto, Eberardo Pavesi y Luigi Ganna.


Actualizado a 2017/05/28

Clasificación por puntos
Clasificación de la montaña
Más etapas ganadas
Más etapas ganadas en una edición
Más etapas consecutivas ganadas
Giro más largo
Giro más corto
Etapa más larga
Etapa más corta (excluidas las contrarreloj)
Ganador más joven
Ganador de más edad
Mayor diferencia del 1º al 2º
Menor diferencia del 1º al 2º
"Para más datos, véase "


En España, el Giro se emite todos los años por Eurosport. En la televisión en abierto las emisoras que lo han emitido desde el año 1998 son:

En Latinoamérica transmite:




</doc>
<doc id="8796" url="https://es.wikipedia.org/wiki?curid=8796" title="John Dalton">
John Dalton

John Dalton (ʤɒn ˈdɔːltən) (Eaglesfield, Cumberland (Reino Unido), 6 de septiembre de 1766-Mánchester, 27 de julio de 1844) fue un naturalista, químico, matemático y meteorólogo británico. Son especialmente relevantes su modelo atómico y su tabla de pesos relativos de los elementos, que contribuyeron a sentar las bases de la química moderna.

También es conocido por haber descrito el daltonismo, defecto visual relativo a la percepción de los colores que padecía y que lleva su nombre.

John Dalton nació el 6 de septiembre de 1766 en una familia cuáquera de la población de Eaglesfield, en Cumberland, Inglaterra. Hijo de un tejedor, sabemos que tuvo cinco hermanos, de los cuales sobrevivieron dos: Jonathan, mayor que Dalton, y Mary, cuya fecha de nacimiento se desconoce. Dalton fue enviado a una escuela donde aprendió matemática y destacó lo suficiente para que, a la edad de 12 años, pudo contribuir con la economía familiar dando clases a otros niños, primero en su casa y después en el templo cuáquero. Los ingresos eran modestos por lo que se dedicó a trabajos agrícolas hasta que en 1781 se asoció con su hermano Jonathan, que ayudaba a uno de sus primos a llevar una escuela cuáquera en la cercana Kendal. 

Alrededor de 1790 Dalton consideró la posibilidad de estudiar derecho o medicina, pero no encontró apoyo de su familia para sus proyectos —a los disidentes religiosos de la época se les impedía asistir o enseñar en universidades inglesas— por lo que permaneció en Kendal hasta que en la primavera de 1793 se trasladó a Mánchester. Gracias a la influencia de John Gough, un filósofo ciego y erudito a cuya instrucción informal Dalton debía en gran parte sus conocimientos científicos, fue nombrado profesor de Matemáticas y Filosofía Natural en la «Nueva Escuela» de Mánchester, una academia de disidentes religiosos. Conservó el puesto hasta 1800, cuando la academia enfrentó la peor situación financiera obligándolo a renunciar a su cargo y comenzar una nueva carrera en Mánchester como profesor particular.

En su juventud Dalton estuvo muy influenciado por un prominente cuáquero de Eaglesfield llamado Elihu Robinson, competente meteorólogo además de fabricante de instrumental, que fue quien despertó su interés por las Matemáticas y la Meteorología. Durante sus años en Kendal, Dalton colaboró en el almanaque "Gentlemen's and Ladies' Diaries" remitiendo soluciones a problemas y preguntas y en 1787, comenzó a redactar un diario meteorológico en el que, durante los siguientes 57 años, anotó más de 200 000 observaciones. En esta época también redescubrió la teoría de circulación atmosférica ahora conocida como la célula de Hadley. La primera publicación de Dalton fue "Observaciones y ensayos meteorológicos" (1793), que contenía los gérmenes de varios de sus descubrimientos posteriores, aunque a pesar de ello y de la originalidad de su tratamiento recibió escasa atención por parte de otros estudiosos. Una segunda obra de Dalton, "Elementos de la gramática inglesa", se publicó en 1802.

En 1794, poco después de su llegada a Mánchester, Dalton fue elegido miembro de la Sociedad Filosófica y Literaria de Mánchester, informalmente conocida como «"Lit & Phil"», ante la que unas semanas más tarde presentó su primer trabajo, "Hechos extraordinarios relacionados con la visión de los colores", en el que postulaba que las deficiencias en la percepción del color se deben a anomalías del humor vítreo. Era la primera vez en la que no solo se describía el hecho de la falta de percepción del color en algunas personas, sino que también se daba una explicación causal al fenómeno. Aunque su teoría fue desacreditada estando él mismo en vida, la investigación profunda y metódica que realizó sobre su propio problema visual causó una impresión tal que su nombre se convirtió en el término común para designar la ceguera al color, el daltonismo. 

Dalton dejó instrucciones de que sus ojos fueran conservados, lo que ha permitido que los análisis de ADN publicados en 1995 demostraran que en realidad padecía un tipo menos común de ceguera al color, la deuteranopia, en la que faltan los conos sensibles a longitudes de onda medianas, en lugar de funcionar con una forma mutada de su pigmento, como en el tipo más común de ceguera al color. Además de los azul y púrpura del espectro, Dalton era capaz de reconocer un solo color, el amarillo, o como él mismo dice en su publicación:

Este trabajo fue seguido por muchos otros sobre temas diversos: acerca de la lluvia y el rocío y el origen de manantiales; sobre el calor; el color del cielo; el vapor; los verbos auxiliares y participios del idioma Inglés; y sobre la reflexión y la refracción de la luz.

Esta ceguera a ciertos colores dificultó en ocasiones su trabajo científico, especialmente en el laboratorio, donde confundía los frascos de reactivos. Sin embargo, esto no le impedía defender ideas con firmeza en sus escritos.

Otra muestra de esta ceguera que le acompañó toda su vida ocurrió en 1832, cuando fue a conocer al rey Guillermo IV y lució una vestimenta académica escarlata (rojo), un color nada habitual para un hombre de su discreción. La razón: Dalton la veía de color gris oscuro, por lo que poco le importó la sorpresa que ese día causó entre sus conocidos. El daltonismo fue descrito por primera vez por el propio John Dalton en 1808. Al igual que su hermano, sufría de esta alteración genética que en términos simples le impedía percibir colores como el rojo y el verde.

En 1800, Dalton se convirtió en secretario de la Sociedad Filosófica y Literaria de Mánchester, y al año siguiente dio una serie de conferencias, bajo el título "Ensayos experimentales", sobre la constitución de las mezclas de gases; sobre la presión de vapor de agua y otros vapores a diferentes temperaturas, tanto en el vacío como en aire; sobre la evaporación, y acerca de la expansión térmica de los gases. Estos cuatro artículos fueron publicados en las "Memorias" de la «"Lit & Phil"» correspondientes a 1802.

El segundo de estos ensayos comienza con una observación sorprendente:

Después de describir estos experimentos para determinar la presión de vapor de agua en varios puntos entre 0 y 100 °C (32 y 212 °F), Dalton llegó a la conclusión a partir de las observaciones de la presión de vapor de seis líquidos diferentes, que la variación de la presión de vapor para todos los líquidos es equivalente, para la misma variación de la temperatura, determinados a partir de vapor a cualquier presión.

En el cuarto ensayo, Dalton anota:

La más importante de todas las investigaciones de Dalton fue la teoría atómica, que está indisolublemente asociada a su nombre. Se ha propuesto que esta teoría se la sugirieron, o bien sus investigaciones sobre el etileno («gas oleificante») y el metano (hidrógeno carburado) o los análisis que realizó del óxido nitroso (protóxido de nitrógeno) y del dióxido de nitrógeno (dióxido de ázoe), puntos de vista que descansan en la autoridad de Thomas Thomson. Sin embargo, un estudio de los cuadernos de laboratorio de Dalton, descubiertos en la sede de la «"Lit & Phil"», llegó a la conclusión de que, lejos de haber sido guiado a la idea de que la combinación química consiste en la interacción de los átomos de peso definido y característico por su búsqueda de una explicación de la ley de las proporciones múltiples; la idea de los átomos surgió en su mente como un concepto puramente físico, inducido por el estudio de las propiedades físicas de la atmósfera y de otros gases. Los primeros indicios de esta idea se encuentran al final de su nota ya mencionada sobre la absorción de gases, que fue leída el 21 de octubre de 1803, aunque no se publicó hasta 1805. Aquí dice:

Dalton fue el primero en publicar una tabla de pesos atómicos relativos. Seis elementos aparecen en esta tabla: hidrógeno, oxígeno, nitrógeno, carbono, azufre y fósforo, atribuyendo convencionalmente al átomo de hidrógeno el peso de una unidad. Dalton no proporciona ninguna indicación en este primer artículo de cómo había realizado sus cálculos. Sin embargo, en una entrada de su cuaderno de laboratorio fechada el 6 de septiembre 1803, aparece una lista en la que se establecen los pesos relativos de los átomos de una serie de elementos, que se derivan del análisis del agua, amoniaco, dióxido de carbono y otros compuestos ya realizados por los químicos de la época.

Parece, entonces, que al enfrentarse con el problema de calcular el diámetro relativo de los átomos, que tenía la convicción de que eran los componentes básicos de todos los gases, utilizó los resultados de análisis químicos. A partir de la suposición de que la combinación se realiza siempre en la forma más sencilla posible, llegó a la idea de que la combinación química se lleva a cabo entre partículas de diferentes pesos, y es este enfoque experimental lo que diferencia su teoría de las especulaciones de los filósofos atomistas de la antigüedad, como Demócrito y Lucrecio.

La extensión de esta idea a las sustancias en general necesariamente lo llevó a formular la ley de las proporciones múltiples, que fue brillantemente confirmada de forma experimental. Cabe señalar que en un documento sobre la proporción de los gases o fluidos elásticos que constituyen la atmósfera, que leyó en noviembre de 1802, la ley de las proporciones múltiples parece ser anticipada en las palabras siguientes: «Los elementos de oxígeno pueden combinarse con una cierta proporción de gas nitroso o con el doble de esa parte, pero no por cantidad intermedia», pero hay razones para sospechar que esta frase fue añadida algún tiempo después de la lectura del documento, que no fue publicado hasta 1805.

En su obra "Un nuevo sistema de filosofía química" (1808) los compuestos fueron enumerados como binarios, ternarios, cuaternarios, etc, en función del número de átomos que el compuesto tenía en su forma más simple, la forma empírica.

Planteó la hipótesis de que la estructura de los compuestos siempre responde a proporciones que se pueden expresar con números enteros. Por lo tanto, un átomo del elemento X con la combinación de un átomo del elemento Y es un compuesto binario. Por otra parte, un átomo del elemento X con la combinación de dos elementos de Y o viceversa, es un compuesto ternario. Aunque no siempre, muchas de los primeras formulaciones de compuestos realizadas por Dalton en "Un nuevo sistema de filosofía química" resultaron exactas y son las que se usan en la actualidad.

Dalton utiliza sus propios símbolos para representar visualmente la estructura atómica de los compuestos. Así lo hizo en "Un nuevo sistema de filosofía química" donde usó esa simbología para listar los elementos y compuestos más comunes.


Dalton propuso adicionalmente un «principio de máxima simplicidad» que encontró resistencia para ser aceptado, ya que no podía ser confirmado de forma independiente:

Esto no era más que una suposición derivada de la fe en la simplicidad de la naturaleza. No había pruebas a disposición de los científicos para deducir cuántos átomos de cada elemento se combinan para formar moléculas de compuestos. Pero esta o alguna regla de cualquier otro tipo era absolutamente necesaria para el desarrollo cualquier teoría incipiente, ya que era necesario presuponer una fórmula molecular para calcular los pesos atómicos relativos. En cualquier caso, a Dalton este «principio de máxima simplicidad» le hizo suponer equivocadamente que la fórmula del agua era OH y la del amoniaco NH.

A pesar de la incertidumbre en el corazón de la teoría atómica de Dalton, los principios de su teoría sobrevivieron. Sin duda, la convicción de que los átomos no se pueden subdividir, crear, o dividirse en partículas más pequeñas cuando se combinan, separan o reorganizan en las reacciones químicas es incompatible con la existencia de la fusión nuclear y la fisión nuclear, pero estos procesos son reacciones nucleares y no reacciones químicas. Además, la idea de que todos los átomos de un elemento son idénticos en sus propiedades físicas y químicas no es exacta: como ahora sabemos los diferentes isótopos de un elemento tienen diferentes pesos. A pesar de todo, Dalton había creado una teoría enormemente potente y fructífera. De hecho, la innovación de Dalton fue tan importante para el futuro de la ciencia como lo sería la misma experiencia de la química moderna realizada por Lavoisier.

Dalton tomó como punto de partida una serie de evidencias experimentales conocidas en su época:

Para explicar estos hechos propuso las siguientes hipótesis:

La contribución de Dalton no fue proponer una idea asombrosamente original, sino formular claramente una serie de hipótesis sobre la naturaleza de los átomos que señalaban la masa como una de sus propiedades fundamentales, y preocuparse por probar tales ideas mediante experimentos cuantitativos.




</doc>
<doc id="8797" url="https://es.wikipedia.org/wiki?curid=8797" title="Dorothy Dandridge">
Dorothy Dandridge

Dorothy Dandridge (Cleveland, 9 de noviembre de 1922 - Hollywood, 8 de septiembre de 1965) fue una cantante y actriz estadounidense, la segunda afroamericana nominada a un Óscar (en 1954, por su papel en la película "Carmen Jones" de Otto Preminger). A pesar de sus puntuales éxitos el racismo imperaba en esa época, y Dorothy Dandridge falleció prematuramente, tras varios reveses profesionales y personales. Estrellas posteriores la han elogiado como una pionera, entre ellas Halle Berry, quien encarnó su personaje en una película biográfica. 

Dorothy Dandridge nació en Cleveland (estado de Ohio), hija de un ebanista, Cyril Dandridge, y de una aspirante a artista, Ruby Dandridge. El matrimonio tuvo dos niñas (Vivian y Dorothy) y se separó pronto, y ambas quedaron al cuidado de la madre, quien las introdujo a edad muy temprana en el mundo del espectáculo.

Llamadas artísticamente "The Wonder Children", Vivian y Dorothy trabajaron durante cinco años en el sur de los Estados Unidos, principalmente en locales de variedades de modesta categoría, los únicos no sometidos a la segregación racial. Mientras, su madre siguió actuando en Cleveland.

La gran depresión posterior al Crack del 29 sumió en una seria crisis al mundo del espectáculo, y Ruby Dandridge optó por ir a Hollywood, donde obtuvo pequeños papeles en radio y televisión, mientras sus dos hijas se rebautizaban como "The Dandridge Sisters" y actuaban en Nueva York, en locales del barrio de Harlem, como "Cotton Club", local al que dedicaría una película Francis Ford Coppola.

En 1935, Dorothy Dandridge hizo su incursión en el cine participando en un cortometraje de la serie infantil "Our Gang", y dos años después tuvo un pequeño papel en el clásico "Un día en las carreras", de los Hermanos Marx. En 1940 rodó la película "Four Shall Die", típica del género llamado entonces "cine racial" o "cine negro", enteramente producido y dirigido al público afroestadounidense, que tenía prohibido el acceso a los cines de público blanco.

Aunque los papeles que le ofrecían eran limitados e insistían en los tópicos sobre la población negra, Dorothy Dandridge se labró una creciente popularidad como artista de variedades, por su calidad como cantante y su dominio del escenario. A esta fama contribuyeron varios "soundies" de éxito, antecesores del actual video musical, que Dorothy grabó para los tocadiscos de monedas ("jukebox").

El gran éxito que dio a Dorothy la fama definitiva fue "Carmen Jones" (1954), adaptación al cine del musical de Broadway de 1943, inspirado a su vez en la ópera "Carmen" de Georges Bizet. El director Otto Preminger incluyó a Dorothy en un amplio reparto de actores negros, junto con Harry Belafonte y Diahann Carroll.

"Carmen Jones" fue un éxito de taquilla, y Dorothy Dandridge alcanzó el hito de ser nominada al Oscar a la mejor actriz; hasta entonces, sólo dos actores afroestadounidenses habían sido nominados, pero ambos en categoría de actores secundarios. Uno de ellos fue Hattie McDaniel, la entrañable "Mammie" de "Lo que el viento se llevó". Pero, al contrario que Hattie, Dorothy no consiguió ganar la estatuilla, pues fue derrotada por Grace Kelly.

En 1959, Dorothy volvió a rodar bajo las órdenes de Otto Preminger: en esta ocasión, la adaptación al cine de "Porgy & Bess", trabajo por el cual ella fue nominada al Globo de Oro. Colaboraron Sydney Poitier, Sammy Davis Jr. y Diahann Carroll.

Como nominada al "Óscar", Dorothy Dandridge se erigió en un personaje de referencia para la población afroestadounidense, pero sufrió vaivenes tanto en su vida profesional como en la personal, y terminaría falleciendo prematuramente.

Un serio revés en su carrera se produjo cuando fue apartada de la superproducción "Cleopatra", que preparaba Rouben Mamoulian. Cuando éste fue reemplazado por Joseph L. Mankiewicz, todos los actores principales fueron también sustituidos, y se eligió a Elizabeth Taylor y a Richard Burton, dos estrellas con mayor atracción comercial, como protagonistas. Dado que esta medida se produjo con el rodaje ya empezado, los productores tuvieron que desechar todo el material rodado y repetirlo con el nuevo reparto, una de las razones del descomunal desfase presupuestario de esta película.

Dorothy Dandridge vio declinar su estrellato en el cine y tuvo que subsistir como cantante en locales de variedades. 

Una anécdota acerca de los problemas raciales que sufrió fue representada en una película sobre su vida, "Introducing Dorothy Dandridge", rodada para el canal HBO por Halle Berry en 1999. Una vez Dandridge fue contratada para actuar como cantante en un hotel de categoría cinco estrellas, y fue la primera mujer negra en hospedarse en tal hotel. Después de estar en su habitación, decide bajar hasta la piscina del hotel para bañarse en ella, y un encargado se dirigió hacia ella para indicarle que no podía nadar allí por ser negra. Ella lo miró e introdujo la punta de su pie para salpicarle y señaló que era la gente blanca como él la que no era digna de nadar allí y se marchó. Horas más tarde, la piscina fue desocupada y "desinfectada", pero Dandridge supo erguirse con dignidad ante este hecho.

En 1965, a la edad de 42 años, falleció en su casa en West Hollywood, debido a una sobredosis de imipramina, un antidepresivo tricíclico.

Halle Berry ganó un Premio Emmy en su papel de Dorothy Dandridge, y volvió a recordarla cuando recogió su Óscar por "Monster's Ball".


</doc>
<doc id="8798" url="https://es.wikipedia.org/wiki?curid=8798" title="Guillermo Dañino Ribatto">
Guillermo Dañino Ribatto

Guillermo Alejandro Dañino Ribatto (chino: 吉叶墨; n. Trujillo, 2 de diciembre de 1929 - ) es un religioso peruano perteneciente a la congregación de los Hermanos de las Escuelas Cristianas, escritor, traductor y reconocido entre los principales actores extranjeros del cine chino. Actualmente tiene .
Guillermo fue el menor de los catorce hijos de Francisco Solano José Dañino de la Torre-Ugarte y de Rosa Ribatto Carranza. 

Ingresó a la Congregación de los Hermanos de las Escuelas Cristianas - La Salle, y fue enviado a Arequipa para realizar su proceso de formación como religioso educador. Graduado en la Pontificia Universidad Católica del Perú, cursó estudios de lingüística en la Escuela de Estudios Superiores en Ciencias Sociales de París. 

Residente en China desde mediados de la década de 1970. Se ha desempeñado como profesor de literatura y lingüística en la Universidad de Beijing y Nanjing. Prolífico escritor y traductor del chino al español, también se ha desempeñado como actor cinematográfico en China.

Artículos suyos han sido publicados por la prestigiosa revista de poesía Arquitrave, de Colombia; China Hoy, de Beijing y en programas especiales de Radio "Sol Armonía", de Lima.

La editorial Hiperión ha publicado en España, varias de sus traducciones de poesía china clásica. Sus ediciones bilingües tienen la inusual particularidad de presentar el original chino "traducido" a caracteres modernos y con la lectura en pinyin al pie.

Actualmente es catedrático de la Universidad Nacional Mayor de San Marcos, Pontificia Universidad Católica del Perú y Universidad Antonio Ruiz de Montoya.

Debido a la transcripción de su nombre al mandarín, (Jiyemo), también es conocido como "Lao Ji".

Por error en la transcripción del chino al inglés, su nombre aparece como H. De Niro, en los créditos de la película "Negociaciones en Chongqing" ("Chongqing tan pan", 1993), cuyo reparto encabeza, en itle/tt0340626/ Chongqing tan pan en Imdb.] Consultado el 1 de febrero de 2014.</ref>

Como actor cinematográfico ha participado en casi treinta películas, cortometrajes, y documentales relacionados con la historia y la cultura de China, así como en programas y series de la televisión china, entre las que se destacan, "Te regalo un poco de ternura", "Una joven moderna" con la actriz Ban Hong, "Operación Plutón" con Li Baolin y la estrella de la NBA Yao Ming.

Personajes históricos representados

John Leighton Stuart. Embajador de Estados Unidos en China), en la superproducción "Da jue Zhan" ("La campaña decisiva"), exhaustiva descripción de la primera gran campaña de los comunistas chinos en Manchuria en la cual obtuvieron la victoria, pero, irónicamente, el logro del comandante fue ignorado y negado hasta 1990, simplemente porque el comandante era el Mariscal de Campo Lin Biao, quien posteriormente intentó (sin éxito) asesinar al Presidente Mao Zedong en los comienzos de la Gran Revolución Cultural.
Patrick Hurley. Embajador de Estados Unidos. En esta película, ambientada en China poco después de la Segunda Guerra Mundial, Dañino ofrece un retrato del papel desempeñado por ese diplomático, en las negociaciones de paz entre los nacionalistas y los comunistas chinos. Esta parte de la historia china fue prohibida durante casi medio siglo en la República Popular y se mantiene olvidada en la isla de Taiwán. El lanzamiento de esta película simbolizó el reconocimiento final del pasado por el gobierno comunista de China.
Mateo Ricci, misionero jesuita, en un documental sobre el Palacio Imperial de Pekín para la Discovery Channel.




En prensa




</doc>
<doc id="8799" url="https://es.wikipedia.org/wiki?curid=8799" title="Erich von Däniken">
Erich von Däniken

Erich Anton Paul von Däniken (Zofingen, 14 de abril de 1935) es un escritor suizo en lengua alemana. Es conocido por haber sido una de las primeras personas que han difundido la hipótesis de que la Tierra pudo haber sido visitada por extraterrestres en el pasado.

Es un prolífico escritor; se estima que ha vendido más de 63 millones de ejemplares de sus 26 libros, que han sido traducidos a 32 idiomas.
Populariza sus hipótesis a través de sus numerosos libros, vídeos y programas de televisión. Su influencia se ha dejado sentir también en el campo de la ciencia ficción y en el movimiento New age.

El escritor ha sido acusado de plagio por su obra "El oro de los dioses," donde utilizó fotografías sin permiso y se adjudicó una expedición a la Cueva de los Tayos, la cual se demostraría que fue falsa.

Von Däniken nació en Zofingen (en Suiza). Educado como católico estricto, asistió a la escuela católica internacional de Saint-Michel, en Friburgo (Suiza). Durante su tiempo en la escuela rechazó las interpretaciones de la iglesia de la "Biblia", y desarrolló su interés por la astronomía y el fenómeno de los platillos volantes.

A la edad de 19 años, Von Däniken fue condenado a cuatro meses de prisión por robo.
Von Däniken dejó la escuela y trabajó como aprendiz de un hotelero suizo.
Trabajó como empleado en un hotel en Egipto, donde fue declarado culpable de fraude y malversación de fondos.

Más adelante trabajó como empleado del Rosenhügel Hotel en Davos (Suiza), periodo durante el que escribió "Recuerdos del futuro".
En diciembre de 1964, von Däniken escribió un artículo titulado «Hatten unsere Vorfahren Besuch aus dem Weltraum?» (‘¿nuestros antepasados fueron visitados desde el espacio?’), para el periódico alemán-canadiense "Der Nordwesten". El manuscrito de "Recuerdos del futuro" fue aceptado por un editor a principios de 1967, e impreso en marzo de 1968.

En noviembre de 1968 von Däniken fue arrestado por fraude, falsificación de documentos de hoteles y falsas referencias de crédito, para obtener préstamos por valor de 130 000 dólares estadounidenses durante un período de doce años.
Dos años más tarde, Von Däniken fue condenado por «repetida y mantenida» malversación de fondos, fraude y falsificación; el fallo del tribunal alegaba que el escritor había estado viviendo una vida al estilo de un "playboy".
Von Däniken entregó un alegato en favor de la nulidad de la sentencia, sobre la base de que sus intenciones no eran maliciosas y que las entidades de crédito habían tenido la culpa por no haber investigado adecuadamente sus referencias fraudulentas.
El 13 de febrero de 1970, Von Däniken fue condenado a tres años y medio de prisión y una multa de 3000 CHF (francos suizos).
Cumplió un año de condena antes de ser puesto en libertad.

Su primer libro, "Recuerdos del futuro", había sido publicado en la fecha de su juicio, y sus ventas le permitieron pagar sus deudas y dejar el negocio hotelero. Von Däniken escribió su segundo libro, "Regreso a las estrellas", mientras estaba en prisión.

Los estudios de Von Däniken han sido calificados como pseudociencia o pseudohistoria y
recibido un gran número de críticas desde los sectores serios de la ciencia y la arqueología, al presentar como misterios atribuibles a visitantes extraterrestres numerosos vestigios arqueológicos de todo el mundo, dando su explicación sin contar con pruebas de ningún tipo.

Básicamente, Von Däniken da explicaciones inusuales a determinadas características de piezas arqueológicas, cuyo origen, según él, no estaría suficientemente documentado por la arqueología académica. Dichas explicaciones se basan en premisas no demostradas por la ciencia, como es la existencia de vida extraterrestre inteligente (hipótesis aceptada por la ciencia), que pudiera en algún momento del pasado haber viajado por el espacio hasta nuestro planeta (hipótesis no aceptada al no existir ninguna prueba fidedigna). Al no seguir ningún método científico apoyado en pruebas reales verificables por otras personas, no se le puede considerar un científico.

Un ejemplo típico serían unas figuras sudamericanas preincaicas de 3000 años de antigüedad, grabadas en oro, que representan lo que para los científicos sería arte plástico inspirado en formas de insectos. Sin embargo, para Von Däniken estos «insectos» serían lo que él ve como sillas de piloto y estabilizadores verticales y horizontales, con lo cual a él le parece más lógico deducir que se trataría no de adornos, sino de aviones similares a los modernos, que los antiguos artistas debieron de conocer a través de su interacción con una cultura tecnológicamente mucho más avanzada de origen presumiblemente alienígena, a pesar de no haberse encontrado ningún rastro de ellos ni de otras tecnologías avanzadas en la zona. El sarcófago está considerado como un falso "oopart"; en realidad los arqueólogos están convencidos de que el sarcófago del rey K'inich Janaab' Pakal representa el comienzo del viaje del difunto rey al inframundo.

Debido a sus hipótesis, en 1991 se le otorgó el premio Ig Nobel de literatura (otorgados por una revista humorística) por su libro "Recuerdos del futuro", en el que explicaba que la civilización pudo haber sido influida por astronautas extraterrestres. Aquí queda claro que Von Däniken defiende la postura científica del creacionismo alienígena.

En realidad, el texto "Samara-angana-sútradhara" contiene seis versos (95 a 100 del capítulo 31) que apenas mencionan los "vímanas", sin mencionar nada acerca de fuego, mercurio o una cola, pero es sencillo acomodar las observaciones a los intereses, lo que es justo lo contrario que hacen los científicos. Mucho más detallados (y antiguos) son el "Majábharata" y el "Ramaiana" (ambos del siglo III a. C. aproximadamente).

En mayo de 2003 abrió un parque temático basado en sus hipótesis sobre los «dioses astronautas» en Suiza, el cual ha sido catalogado por científicos como Antoine Wasserfallen, de la Academia Suiza de Ciencias Técnicas, como un «Chernobil cultural».

En "Recuerdos del futuro", Däniken escribe sobre el pilar de hierro de Delhi, en la India, como una evidencia de la influencia extraterrestre al no estar oxidada. En una entrevista en la revista "Playboy", al preguntarle por qué la columna tiene rastro de oxidación y sobre las pruebas que respaldaban su sistema de construcción medieval y su resistencia a la corrosión, Däniken dijo que posteriores investigaciones le llevaban a concluir otras cosas, por lo que descartaba que el pilar fuera un misterio.

En "El oro de los dioses", Von Däniken afirmó que había sido guiado a través de una serie de túneles artificiales a un lugar dentro de la Cueva de los Tayos (en Ecuador) que contenía extrañas estatuas de oro y una biblioteca en tablillas de metal que él consideraba como una evidencia de antiguos visitantes del espacio. El hombre que Von Däniken dijo que le había enseñado estos túneles, Juan Móricz, dijo a la revista "Der Spiegel" que las descripciones de Von Däniken provenían de una larga conversación, y que las fotos del libro habían sido «un enredo». Von Däniken dijo a la revista "Playboy" que, aunque había visto la biblioteca y otros lugares que había descrito, había fabricado algunos de los hechos para agregar interés a su libro. Sin embargo, en 1978 dijo que nunca había estado en la parte de la cueva que se ilustra en su libro, solo en una «sala lateral», y que había inventado todo el descenso a la cueva. Los geólogos que han examinado la zona no han encontrado ningún sistema de cuevas oculto. Von Däniken también escribió acerca de una colección de objetos de oro en poder de sacerdote local, Crespi, que tenía un permiso especial del Vaticano para su investigación arqueológica. Sin embargo, un arqueólogo informó a "Der Spiegel" que, aunque había algunas piezas de oro, muchas eran solo imitaciones locales para los turistas. Juan Moricz demostraría los engaños de Däniken

Von Däniken afirmó que el sarcófago de Palenque representa un astronauta sentado en una nave espacial propulsada por cohetes, vestido con un traje espacial. Sin embargo, los arqueólogos no ven nada especial en la figura de un monarca maya muerto, que lleva peinado tradicional y las joyas mayas, rodeado de símbolos mayas que pueden observarse en otros dibujos mayas. La mano derecha no está manejando los controles de un cohete, sino simplemente haciendo un gesto tradicional maya, que otras figuras en los laterales del sarcófago también hacen. La forma de los cohetes de propulsión corresponden en realidad a dos serpientes que unen sus cabezas en la parte inferior y las llamas que producen los cohetes a las barbas de las serpientes. El motor del cohete bajo la figura es la cara de un monstruo, símbolo del inframundo.

Estas son las obras de Von Däniken que han sido publicadas en español (entre paréntesis se especifica el año de la primera edición):







</doc>
<doc id="8800" url="https://es.wikipedia.org/wiki?curid=8800" title="Edgar Degas">
Edgar Degas

Hilaire-Germain-Edgar de Gas, más conocido como Edgar Degas (París, 19 de julio de 1834-ibídem, 27 de septiembre de 1917), fue un pintor y escultor francés. 

Considerado uno de los fundadores del Impresionismo, aunque él mismo rechazaba el nombre y prefería llamarlo realismo o "arte realista," Degas fue uno de los grandes dibujantes de la historia por su magistral captación de las sensaciones de vida y movimiento, especialmente en sus obras de bailarinas, carreras de caballos y desnudos. Sus retratos son muy apreciados por la complejidad psicológica y sensación de verdad que transmiten.

Hilaire-Germain-Edgar De Gas nació en París el 19 de julio de 1834, siendo el mayor de los cinco hijos de Célestine Musson De Gas, hija de una familia criolla de Nueva Orleans y de Augustin De Gas, un banquero francés. Años más tarde, adoptaría una forma mucho más simplificada de su apellido a "Degas", con el cual sería conocido por el resto de su vida. Comenzó su educación a la edad de once años en el Lycée Louis-le-Grand; dos años más tarde su madre fallece convirtiendo a su padre y a su abuelo en sus principales influencias el resto de su juventud.

Degas comenzó a pintar a una edad temprana. Después de graduarse en el liceo en bachillerato en literatura a los 18 años en 1853, instaló un estudio de arte en su casa. Al graduarse, Degas se registra como copista de arte en el Museo de Louvre. Sin embargo, terminó ingresando en la Facultad de leyes en la Universidad de París a petición de su padre en noviembre de 1853; no obstante dedica muy poco esfuerzo a sus estudios. 

En 1855, Degas conoce a Jean Auguste Dominique Ingres, de quien era gran admirador, y cuyo consejo nunca olvidó: ""Dibuje líneas y más líneas, joven, tomadas de la realidad y de la memoria, así se convertirá en un buen artista"." En abril de ese mismo año, Degas es admitido a la Escuela de Bellas Artes (“École des Beaux-Arts”), comenzando a estudiar dibujo bajo la guía de Louis Lamothe, tomando como inspiración principal el estilo de Ingres. En julio de 1856, Degas viaja a Italia, donde se establecería los siguientes tres años, comenzando los estudios para su primera obra: "La familia Bellelli". Al mismo tiempo hacía copias de obras renacentistas, de Miguel Ángel, Rafael o Tiziano, entre otros. Sin embargo, contrariamente a las prácticas convencionales, Degas seleccionaba detalles de los cuadros que atraían su atención, usualmente figuras secundarias o alguna cabeza, los cuales trataba como un retrato.

En 1859, Degas se instala en un estudio en París lo suficientemente grande como para trabajar en el retrato de "La familia Bellelli", obra que había pensado exhibir en el Salón de París; sin embargo la pintura se mantuvo incompleta hasta 1867. Alrededor de esa época, Degas comienza a trabajar sobre varias de sus pinturas de historia: "Alejandro y Bucéfalo" y "La hija de Jephthah" en 1859-60; "Semíramis construyendo Babilonia" en 1860; y "Jóvenes espartanos" alrededor de 1860. 

En 1861 comenzó sus primeros estudios sobre caballos durante la visita que realizó a su amigo Paul Valpincon en Normandía. Su primera exposición en el Salón parisino fue en 1865, cuando el jurado acepta su pintura "Escena de Guerra en la Edad Media" que no atrajo gran atención. Degas siguió exponiendo sus obras en el Salón de manera anual; sin embargo durante los cinco años siguientes ninguna de las obras presentadas fueron de tema histórico, y así su "Steeplechase, el jinete caído" (Salón de 1866) marcó su distanciamiento con el arte tradicional. El cambio en el arte de Degas, que pasó de las grandilocuentes pinturas de historia a retratar temas contemporáneos, se debió principalmente a Édouard Manet, a quien Degas había conocido en 1864 cuando ambos artistas copiaban el mismo cuadro de Velázquez en el Museo del Louvre.

Al inicio de la Guerra franco-prusiana de 1870, Degas se une a la Guardia Nacional en la defensa de París, por lo que se ve obligado a dejar de pintar por un tiempo. Durante una práctica de tiro con rifle le fue detectado un problema en la vista. Durante el resto de su vida, la mala visión fue una preocupación constante para él.

Al terminar la guerra en 1872, Degas decide establecerse en Nueva Orleans, Luisiana, donde vivían varios de sus familiares, incluyendo a su hermano René. Durante varios años vivió en casa de un tío criollo en la avenida Esplanade. Durante esos años en Nueva Orleans, Degas produjo varias obras, algunas de retratos familiares; una de ellas, "Una oficina en la lonja del algodón en Nueva Orleans", llamó la atención en Francia. Fue la única de sus obras comprada por un museo durante su vida. 

Degas regresó a París en 1873 y su padre falleció al año siguiente; fue entonces cuando el pintor descubrió que su hermano René había acumulado una enorme cantidad de deudas de negocios. Para salvar la reputación de su familia, Degas terminó vendiendo su casa, al igual que varias colecciones que había heredado, para pagar las deudas de su hermano, lo que motivó que por primera vez en su vida tuviera que vivir de su arte como único ingreso. Durante estos años creó muchas de sus grandes obras.

Decepcionado con el Salón oficial, Degas se unió a un grupo de jóvenes artistas que comenzaban a organizar un grupo independiente con el fin de exponer sus pinturas, rechazadas casi siempre por los Salones. Dicho grupo fue conocido más tarde como "Los impresionistas". Entre 1874 y 1886, el grupo había montado alrededor de ocho muestras, conocidas como "Exposiciones impresionistas". Degas lideró el grupo, organizando las exposiciones donde presentó muchos de sus trabajos, a pesar de sus constantes desacuerdos con los demás integrantes. Su desacuerdo con las pinturas al aire libre hizo que tuviera enfrentamiento con los paisajistas del grupo, incluyendo a Claude Monet. Conservador en cuanto a sus actitudes sociales, Degas aborrecía el escándalo que creaban las exhibiciones, al igual que el nivel de publicidad y notoriedad que buscaban sus colegas. El mismo término "Impresionismo" era rechazado por Degas y le disgustaba que lo asociasen con él. Insistía que se incluyeran obras no impresionistas dentro de las exposiciones, como las de Jean-Louis Forain y Jean-François Raffaëlli. Estos conflictos llevaron a la disolución del grupo en 1886.

A la vez que mejoraba su situación financiera, Degas fue adquiriendo obras de artistas que admiraba, como El Greco, Manet, Pissarro, Cézanne, Gauguin e incluso Vincent van Gogh. Hubo tres artistas en particular idolatrados por Degas cuyas obras formaron el núcleo de su colección personal: Ingres, Delacroix y Daumier. En el inventario final de la colección particular del artista se documentaron 20 pinturas y 88 dibujos de Ingres y 13 pinturas y alrededor de 200 dibujos de Honoré Daumier; sin embargo no había ningún Monet.

A finales de la década de 1880, Degas comenzó a desarrollar una gran pasión por la fotografía. A lo largo de su vida, Degas retrató a varios amigos, muchas veces bajo la luz de lámparas, como en su retrato doble de Renoir y Mallarmé. También fotografió bailarinas y desnudos, los cuales fueron utilizados como referencia para varias de sus pinturas y dibujos.
Conforme pasaban los años, Degas comenzó a aislarse, siendo fiel a su creencia de que un pintor no debería tener vida personal. La controversia del Caso Dreyfus llevó el lado antisemita de Degas al extremo de romper toda conexión con sus amigos judíos. Estas creencias e ideales conservadores llevaron a Degas a vivir un tanto aislado de la sociedad.

Degas se centró en sus obras en pastel hacia 1907 y se cree que se dedicó principalmente a sus esculturas hasta finales de 1910. Su carrera artística terminó prácticamente en 1912, cuando deja de trabajar y es obligado a mudarse debido a la demolición de su residencia en Victor Masse. Debido a su aislamiento social, Degas nunca estuvo casado y vivió las últimas décadas de su vida solo, casi ciego debido a la enfermedad ocular que se le había desarrollado a través de los años, paseando solitario por las calles de París hasta su muerte en septiembre de 1917.

Degas es clasificado usualmente como un artista impresionista, descripción sin embargo insuficiente. El Impresionismo se originó entre 1860-1870 partiendo del realismo de pintores como Courbet y Corot. Los impresionistas pintaban la realidad usando colores brillantes y saturados, centrándose principalmente en los efectos de la luz y la atmósfera, buscando transmitir una sensación de inmediatez.

Técnicamente, Degas difiere de los impresionistas ya que él “nunca adoptó la técnica de mancha y color", y menospreciaba la práctica de la pintura "al aire libre". Según la historiadora de arte Carol Armstrong, Degas era más bien un anti-impresionista que criticaba muchas de las obras que seguían dicho movimiento. Tal y como el mismo Degas llegó a explicar “"Ningún arte fue nunca menos espontáneo que el mío. Lo que hago es el resultado del estudio de grandes maestros; de inspiración, espontaneidad o temperamento, yo no sé nada."" Sin embargo, el tipo de arte de Degas se apega más al Impresionismo que a cualquier otro movimiento artístico. Sus escenas sobre la vida parisina, la composición y sus experimentos con el color y la forma; sin mencionar su cercanía con varios notables artistas impresionistas como Cassatt o Manet, relacionan indudablemente a Degas con el movimiento impresionista.

El estilo de Degas refleja el respeto que sentía hacia los maestros clásicos (fue un copista entusiasta durante gran parte de su vida) en especial hacia sus admirados Jean Auguste Dominique Ingres y Eugène Delacroix. Degas era un gran coleccionista de grabados japoneses, cuyos principios compositivos influyeron fuertemente en sus obras, al igual que el realismo de ilustradores como Daumier y Gavarni. Aunque Degas es reconocido principalmente por sus caballos y bailarinas, inició su carrera con pinturas históricas convencionales, como "La hija de Jephthah" (1859-61) y "Los jóvenes espartanos" (1860-62), en los cuales sin embargo el tratamiento de las figuras es notablemente anticonvencional. 

Durante el inicio de su carrera, Degas dedicó gran parte de su tiempo a retratos tanto individuales como grupales, como "La familia Bellelli" (1858-67) que retrata a su tía con su esposo e hijos. En varias de estas obras Degas refleja la tensión que sentía presente entre hombres y mujeres. Dentro de sus primeras obras, Degas ya mostraba las primeras evidencias del estilo que más tarde desarrollaría, modificando el formato del cuadro y eligiendo puntos de vista inusuales.

A finales de la década de 1860, Degas abandona la temática historicista de sus pinturas y comienza a dedicarse a la observación de la vida contemporánea. Las escenas de carreras de caballos le permitieron estudiar el movimiento de caballos y jinetes dentro de un contexto moderno. También comenzó a pintar mujeres trabajando como modistas o lavanderas. "Mlle. Fiocre en el Ballet La Souce", exhibida en el Salón de 1868, fue su primera gran obra centrada en un nuevo tema con el que se le identificaría particularmente: las bailarinas.

En varias de sus pinturas posteriores, Degas retrató bailarinas ensayando o preparándose tras el escenario, enfatizando su estatus de profesionales haciendo su trabajo. A partir de 1870, las obras de bailarinas aumentaron considerablemente, en parte por la necesidad de pagar las deudas de su hermano, que habían dejado a la familia en bancarrota, ya que las pinturas de esta temática se vendían bien. Degas comenzó de igual manera a retratar escenas en cafés, como en sus obras "La absenta" y "Cantante con un guante". A menudo sus pinturas insinuaban un contenido narrativo de una manera altamente ambigua; por ejemplo "Interior" (también llamada “La violación”), en cuya temática los historiadores han buscado infructuosamente una referencia literaria detrás.; aunque puede representar una simple escena de prostitución.

Al cambiar el tema en sus obras, Degas cambió de igual manera su técnica y estilo. La paleta de colores opacos, que surgieron por influencia de la pintura holandesa, fue remplazada por colores mucho más vivos y pinceladas mucho más marcadas. La falta de color en "Ensayos de ballet" (1876) e "Instructor de ballet" señala un posible cambio en su técnica motivado por su interés en la fotografía.

El estilo de Degas se distingue por sus zonas inacabadas, incluso en pinturas altamente detalladas. Frecuentemente culpaba dichas faltas a sus problemas en la vista, argumento que fue refutado por varios colegas y coleccionistas que creían poco probable que "alguien con problemas de vista pudiera pintar de tal manera". 

Su interés en los retratos llevó a Degas a estudiar el comportamiento de las personas y la manera en que estas revelan parte de su estatus a través de su fisionomía, postura, prendas y otros atributos. En su obra "Retratos en la bolsa de valores", Degas retrató un grupo de trabajadores judíos con ciertos indicios de antisemitismo. En 1881 exhibe dos obras al pastel: "Criminal Physiognomies", donde retrataba a un grupo de jóvenes, miembros de una pandilla, que habían sido condenados recientemente por asesinato en el llamado "Caso de la Abadía". Degas asistió al juicio llevando consigo su cuaderno de bocetos; sus numerosos dibujos revelan el interés del pintor sobre las ideas atávicas defendidas por los científicos del siglo XIX como evidencia de la criminalidad innata. 

En sus pinturas de bailarinas y lavanderas, Degas refleja las ocupaciones de ambas no solo por su atuendo sino por su estructura física: sus bailarinas exhiben un físico atlético y grácil, mientras que las lavanderas poseen un cuerpo pesado y sólido.

A finales de la década de 1870, Degas dominaba tanto el óleo sobre lienzo como el pastel. El medio seco, el cual aplicó en diversas y complejas capas y texturas, le permitía trazar las formas con más facilidad y explotar libremente el uso de los colores. Alrededor de esos años, Degas retoma la técnica del grabado, la cual había abandonado diez años atrás. Al principio aprendió junto a su amigo Ludovic-Napoléon Lepic, un innovador de la técnica del grabado, y comenzó a experimentar con la litografía y monotipia. Degas tenía una fascinación particular por los efectos que producía la monotipia y modificaba las imágenes impresas utilizando pastel. Hacia 1880 la escultura se había convertido en otro apartado más de la búsqueda de Degas por encontrar nuevos y diferentes medios expresivos, aunque el artista únicamente mostró en público una escultura a lo largo de su vida.

En la evolución que sufrió el estilo del arte de Degas, algunas características se mantuvieron inmutables a lo largo de toda su vida. Primeramente, sus pinturas se realizaban en interiores (usualmente en su estudio) utilizando como referencia la memoria, fotografías o modelos; sus pocos paisajes fueron compuestos a través de la imaginación o incluso de memoria. Era usual en Degas el repetir un mismo tema en varias ocasiones, sin embargo siempre con alguna variación dentro de la composición o el tratamiento. Fue un artista cuyas obras eran, como apuntó Andrew Forge, ""bien preparadas, calculadas, practicadas, y desarrolladas en etapas. Estaban compuestas de partes. El ajuste de cada parte para crear un todo, el arreglo lineal, era la ocasión para reflexión infinita y experimentación"". El mismo Degas explicó: ""En arte, nada debería parecer casual"".

A lo largo de su vida, Degas solo exhibió una escultura: "Pequeña bailarina de catorce años" (1881). Era una escultura de cera casi a tamaño real con cabello auténtico y vestida con un tutú verdadero, la cual provocó una fuerte reacción de críticas negativas. La mayor parte encontraban el realismo de la escultura extraordinario, pero criticaban la fealdad de la niña. En una crítica, J.K. Huysmans escribió: ""La terrible realidad de esta estatuilla evidentemente produce inquietud en los espectadores; todas sus nociones acerca de la escultura están aquí alteradas. El hecho es que este primer intento de Monsieur Degas ha revolucionado las tradiciones de la escultura, de igual manera que lo hizo con la pintura.""

Degas creó un considerable número de esculturas en un periodo de 4 décadas; sin embargo, se mantuvieron ocultas al público hasta 1918. Ninguna de ellas, ni siquiera la "Pequeña bailarina de catorce años" fueron fundidas en bronce mientras vivió el artista. Los especialistas en Degas concuerdan que sus esculturas no fueron creadas como auxiliares de su pintura. Degas asignó el mismo significado a sus esculturas que a sus dibujos: ""Dibujar es una forma de pensar, esculpir es otra.""

Tras la muerte del artista, sus herederos encontraron en su estudio alrededor de 150 esculturas de cera, varias en mal estado de conservación. Consultaron al fundidor Adrien Hébrard, quien tras analizar las piezas concluyó que sólo 74 de ellas podían ser fundidas en bronce. Se asume que, a excepción de la “Pequeña bailarina de catorce años”, fueron fundidas al “surmoulage”. Un “surmoulage” de bronce es un tanto más pequeño y muestra menos detalle en la superficie que su molde original. La Fundición Hébrard fundió bronces de 1919 a 1936, y cerró en 1937, poco antes de la muerte de Hébrard.

Degas sostenía con ahínco que las artes deben estar apartadas de la vida personal, y él mismo vivía una vida sin aparentes complicaciones. Entre la gente era reconocido por su ingenio, considerado cruel en algunas ocasiones. En una ocasión, el novelista George Moore se refirió a Degas como un “viejo avaro”, lo que llevó a que se le viera como un solterón misántropo. 

Siendo profundamente conservador en cuanto a temas políticos, se oponía a todas las reformas sociales y mostraba un total desinterés por los avances tecnológicos como el teléfono. Su intransigencia llegaba a tales extremos que en una ocasión llegó a despedir a una modelo al enterarse que ésta era protestante. Aunque varias obras que pintó entre 1865 y 1870 muestran temas relacionados con judíos, su antisemitismo se manifestó en 1870 en el cuadro "En el Bourse", al pintar el rostro de un banquero con un gran parecido a una caricatura francesa antisemita de la época.

El Caso Dreyfus, que dividió París de 1890 hasta inicios de 1900, hizo que se intensificara su antisemitismo. Hacia mediados de 1890, Degas había cortado toda relación con judíos, rechazando públicamente a sus amigos judíos y negándose a contratar a personas que pudieran ser judías. Se mantuvo como miembro del grupo antisemita “Anti-Dreyfusards” hasta su muerte.

Durante su vida, la recepción pública de las obras de Degas osciló entre la admiración y el rechazo. En sus inicios, durante su fase "clasicista" Degas llevó muchas obras al Salón de París entre 1865 y 1870. Dichos trabajos fueron elogiados por Jules Antoine Castagnary. Poco después, Degas se une a los impresionistas y rechaza las reglas rígidas y el elitismo del Salón; del mismo modo, el Salón y el público en general rechazaron su viraje al Impresionismo.

La escultura "Pequeña bailarina de catorce años", exhibida en las sexta presentación impresionista en 1881, fue probablemente su pieza más controvertida; algunos críticos describieron su “fealdad atractiva”. La originalidad de Degas consistió en descartar las superficies lisas de la escultura clásica y añadir cabello humano y ropaje, otorgando a la escultura cierta imagen de muñeca. Las adiciones aumentaban la ilusión, pero llevaron a cuestionamientos en cuanto a la “realidad” dentro del arte. 

Por otro lado, el conjunto de dibujos al pastel exhibidos en la octava exhibición impresionista de 1886, produjo “"La mayor cantidad de opiniones críticas al artista durante el curso de su vida… la reacción global fue positiva y elogiosa."”

Reconocido como uno de los artistas más importantes de la época, Degas es considerado al presente uno de los fundadores del Impresionismo. Aunque en muchos aspectos, sobre todo en la teoría y en ciertas técnicas, la pintura de Degas chocaba con el grupo impresionista, su sentido del color, de las formas en movimiento, de la luz y las composiciones dinámicas no solamente inspiraron a otros miembros del grupo, sino que de algún modo fueron una de las cimas de este movimiento pictórico.

Las obras de Degas se conservan hoy en día en museos y colecciones privadas de todo el mundo, y es uno de los artistas más cotizados en el mercado internacional de arte.





</doc>
<doc id="8802" url="https://es.wikipedia.org/wiki?curid=8802" title="Óscar Domínguez">
Óscar Domínguez

Óscar Domínguez (San Cristóbal de La Laguna, 3 de enero de 1906 – París, 31 de diciembre de 1957) fue un pintor surrealista español perteneciente a la generación del 27.

Óscar Manuel Domínguez Palazón nació el 3 de enero de 1906 en la casa marcada entonces con el número 64 de la calle Herradores de La Laguna, en Tenerife, Islas Canarias. Fue el único hijo varón del matrimonio formado por Antonio Andrés Domínguez, un terrateniente poseedor de extensas propiedades agrícolas de Tacoronte, y María Palazón Riquelme, lagunera de familia de procedencia murciana; «"fruto, según se murmujeaba, del reencuentro de la joven pareja tras algunas desavenencias por líos de faldas. Andando los años, el propio Óscar se vanagloriaría de ser fruto del amor renacido, e incluso pondría en labios de la madre moribunda una hipotética conversación con su padre, con la que [...] quiso proteger para siempre a "l'enfant de notre reconciliation", que "ne sois jamais de chagrin"."»; de esta manera y de acuerdo con el pintor, su madre, en su lecho de muerte, le habría hecho jurar a su padre que el niño "no lloraría jamás". Su bautizo se celebró en la Parroquia Matriz de Nuestra Señora de la Concepción de la misma ciudad el 26 de abril. Fueron padrinos su hermana Julia Domínguez Palazón, que contaba entonces con doce años de edad, y José Izquierdo Domínguez, primo hermano y estrecho amigo de Antonio Domínguez.

«"Al hecho de ser hijo del amor le atribuyó Domínguez un valor premonitorio. Su madre -tal como le confesaría en París a su amigo Marcel Jean- había sufrido un intento de envenenamiento a manos de una mujer con quien su padre mantenía relaciones. Fruto de la reconciliación entre sus progenitores fue su nacimiento"». En relación al episodio sobre la tentativa de intoxicación de María Palazón, el también pintor Marcel Jean recupera textualmente información oral proporcionada por el mismo Óscar Domínguez:

Investigaciones subsecuentes desmentirían la versión de Óscar Domínguez con respecto a estos hechos, «"ya que la que vertió el veneno en la copa de la madre de Óscar Domínguez no fue una propietaria de una finca vecina, sino una sirviente de la casa"». En cuanto a la solicitud de juramento que Domínguez pone en boca de su madre, también recogido por Jean:

Todo parece indicar que esto último es falso, «"ya que sus hermanas así lo afirman. Tal juramento es producto de la fantasía de Domínguez"».

Cuando Óscar Domínguez cuenta con un año de edad, su madre, de veintinueve, queda nuevamente embarazada. María Palazón Riquelme daría a luz una niña en la misma casa donde naciera Óscar el 21 de diciembre de 1907. La recién nacida, de nombre María Demetria, fallece sin embargo a los dos días, de acuerdo con el acta parroquial de defunción "«de vicio de conformidad»"; es decir, a causa de una malformación física o, más concretamente, por su deformación craneal. Tres semanas después fallecería asimismo María Palazón a causa de una septicemia puerperal. A sus tres años, Domínguez contrae a su vez la corea de Sydenham, conocida popularmente como «mal de San Vito», tras el susto ocasionado por el encuentro con un perro. Los síntomas fueron la pérdida del habla y una parálisis que lo tuvo postrado durante dos años. En este tiempo sólo podría realizar movimientos involuntarios. No llegará a recuperarse totalmente hasta los cinco años de edad.

Al quedar huérfano de madre, atrae Domínguez los cuidados de sus dos hermanas mayores y de su abuela paterna Federica. Ya entonces, «"se podía adivinar su vocación de pintor, pero lo que había causado mayor extrañeza durante su infancia era la arbitrariedad de su carácter, su rebeldía natural, su falta de prejuicios o hábitos morales. De una manera natural se desarrollaba su individualismo frente a toda norma y a toda conducta establecida"». El trato con las sirvientas de la casa habría implicado asimismo un factor de consideración en la configuración temprana de su personalidad. Inicialmente, se resaltará la posible influencia «"de una fiel sirvienta de gran hermosura campesina e inclinación a tradiciones orales de brujería, llamada Concha la "Corre-Corre""»; sin embargo, posteriormente se señala como «"falsa la importancia que le concede Eduardo Westerdahl a una de las criadas de la casa, llamada Concha "la corre-corre". Westerdahl ha difundido la leyenda de que era una hechicera que echaba las cartas y que introdujo a Óscar en el mundo de las artes mágicas y la superchería popular. Según la información "[...]" de sus dos hermanas esto es completamente falso"».

El padre de Óscar Domínguez, Antonio Andrés Domínguez de Mesa, a quien el pintor habría admirado con vehemencia, ha sido retratado por diferentes crónicas o testimonios como "«refinado»", "«elegante»", "«culto»", "«solitario»", "«buen conversador»", "«de elegante atuendo»", así como "«algo mujeriego»". Emprendía frecuentes viajes a Europa, donde se proveería de instrumentación; un telescopio, cámaras fotográficas, prismáticos, etc. Tendente al coleccionismo, complementan su residencia libros de tricomías, restos óseos de aborígenes, mariposas disecadas, cerámicas y paisajes de su autoría. Poseedor de una habilidad manual extraordinaria, de acuerdo con el pintor, «"Il était agronome, ingénieur, peintre, mécanicien. C'est lui d'ailleurs qui m'a appris 'a peindre"»; los paisajes de amateur al óleo que realizara Antonio Andrés suponen las primeras lecciones pictóricas de Óscar Domínguez. Por otra parte, Domínguez de Mesa, de ideales liberales y vocación política, resulta investido alcalde de Tacoronte durante la Segunda República. El liberalismo «"condicionó en cierto modo la educación de Óscar, ya que su padre permitía dar rienda suelta a su personalidad infantil y no recurría a castigos y reprimendas"».

Hasta la edad de ocho años, Óscar Domínguez y su familia residen en la calle Herradores de La Laguna, pasando alguna temporada en Santa Cruz de Tenerife y veraneando en la casa paterna de Tacoronte, en el barrio de El Calvario. Con el fallecimiento de María Palazón, el padre de Óscar intensificará su presencia en Tacoronte para mejorar el control de la explotación de las tierras de su propiedad; es entre los años 1913 y 1914 cuando se estima que la familia Domínguez establece su residencia definitiva en la casa de El Calvario.

En su niñez, impresiona vivamente al futuro artista la muerte de su prima Mariquita, compañera de juegos de su misma edad; ya en su adultez, rememorará cómo en su sepelio la niña fue enterrada en una caja de tapa de cristal. Finalmente, «"el anecdotario de sus travesuras infantiles es inacabable. En cierta ocasión se cuenta que en la playa de Guayonge salió a remar con un pequeño bote que se había construido [...] y se salvó gracias a unos pescadores que se tiraron para rescatarle. En otra ocasión le arrancó una muela de oro a su primo Juanito Domínguez porque no tenía dinero para ir al cine"».

Los juegos infantiles de Óscar Domínguez en las playas de arena negra como las de Guayonje, los escarpados barrancos de la región o el jardín de su casa de El Calvario, junto con otras particularidades características de Tacoronte, proveerán al futuro artista de una imaginería personal que habría de verse reflejada posteriormente como parte integrante de su obra.

En Tacoronte, el padre de Óscar Domínguez prestaría especial atención a su finca de Guayonje, provista de varias series de bancales al pie de un acantiliado de unos 700 m.s.m.; Antonio Domínguez habría diseñado para el terreno un sistema de riego propio, así como un modelo de teleférico a motor mediante el cual se valdría para el transporte de los productos, el ascenso de la cosecha de plataneras y el descenso de provisiones. Posteriormente, se hace construir adjunta una casa de veraneo en un pequeño altozano rodeado de cañaverales y cercano al mar, caracterizada por su pequeño torreón coronado de almenas, que le daría el aspecto de un castillete.

La casa de Óscar Domínguez en Tacoronte se halla en un enclave dominado entonces por una serie de alhóndigas. Años después de su muerte, el político, periodista y amigo personal del pintor, Elfidio Alonso Rodríguez, recuerda:

La imagen de la prostituta empalada puede econtrarse en uno de sus óleos de 1934, "Recuerdo de mi isla" o "Paisaje de Canarias".

En el huerto de la misma casa de El Calvario, crecía además un antiquísimo ejemplar de drago, el cual, antes de caer derribado por un temporal, habría de convertirse en uno de los iconos más sugestivos de su pintura.

A los doce años de edad, Óscar Domínguez es enviado por su padre como interno a un pensionado de estudiantes anexo al Instituto de Segunda Enseñanza de La Laguna. Del año 1924, cuando aún no había conluido sus estudios en el Instituto, data un testimonio fotográfico que evidencia su temprana vocación pictórica; en una terraza de su casa de Guayonge posa ante un cuadro suyo que representa un desnudo femenino y cuyo estilo recuerda a la estética del "art decó". Asimismo, de 1924 y 1925 se conservan algunas primeras ilustraciones de Domínguez a lápiz y acuarela, tales como "Mujer con mantón y fondo urbano", "Retrato de mujer", "Cabeza de mujer" y "Mujer con pañuelo".

Sus estudios en el Instituto de La Laguna no fueron nada brillantes; el mismo día que expiraba el plazo de matrícula para el curso 1921-1922, Antonio A. Domínguez acude al centro para formalizar el ingreso de su hijo como repetidor en las asignaturas de primero de bachillerato; al mismo tiempo, inscrito a todos los efectos en el centro oficial de estudios de La Laguna, Óscar Domínguez figurará en las listas de alumnado del Establecimiento de Segunda Enseñanza de Santa Cruz de Tenerife. En el curso académico 1922-1923, Domínguez es matriculado de nuevo por su padre en seis asignaturas, de las que sólo aprobará la pendiente e "Historia de España", beneficiándose además de un suficiente general en "Gimnasia"; no se presentaría al examen de las tres restantes asignaturas en las dos convocatorias de ese mismo año. Por último, es inscrito en el curso 1923-1924 del centro, sin que acuda a las pruebas finales. Es en este último periodo cuando el pintor renuncia definitivamente a sus estudios.

Óscar Domínguez habría abandonado la isla de Tenerife a los diecinueve años y marchado a París donde, según su propio testimonio, «"se encontraba desde enero de 1925"»; su padre habría enviado al joven pintor a la capital francesa para que desempeñara las funciones de oficinista junto a su representante en el envío de frutas, no ejerciendo para la empresa ninguna función de responsabilidad económica destacable. Por su parte, «"Óscar Domínguez estaba interesado en hacerse pasar por representante de una juventud despreocupada y alegre, de una "jeunesse dorée"".» Esta razón podría haber motivado parte de sus semblanzas de la época: 

En esta primera estancia parisiense, Óscar estuvo acompañado por su hermana Antonia y por su cuñado, el también pintor Álvaro Fariña, quienes se hallaban instalados en un apartamento de la ciudad. El mismo año, Domínguez es llamado a filas para el ejercicio del servicio militar obligatorio. Sin embargo, no se incorpora en su momento a la milicia al solicitarse formalmente la reducción del servicio, para cuya concesión se requerirá el abono de una cuota total de cinco mil pesetas; de esta forma, Óscar Domínguez ingresaría posteriormente en la milicia bajo la denominación popular de «recluta de cuota».

Próxima la fecha de su ingreso en el cuartel para el cumplimiento del periodo de formación castrense, es depositada en el consulado de España de la capital francesa una instancia dirigida al capitán general de Canarias en Santa Cruz de Tenerife, con firma de Óscar Domínguez Palazón en París el 24 de septiembre de 1927, por la que se solicita la prórroga de la fecha de incorporación del signatario al Ejército. Justifica esta petición la razón de encontrarse el artista por entonces en París «"dedicado al comercio de frutas y que abandonarlo le acarrearía grandes perjuicios que ocasionarían casi totalmente la pérdida del negocio con el que gana el sustento de la vida"»; esto es, que «"no podría estar en las islas en el momento de ser citado, no queriendo caer en falta alguna cuando a su llamada no se encuentre en ésa, teniendo fielísima intención de cumplir sus deberes militares tan pronto le sea factible"». El escrito de petición de prórroga es enviado por el consulado español al Ministerio de Estado de Madrid para su remisión por conducto reglamentario a la Capitanía General de Canarias, donde tendría entrada en las oficinas de Estado Mayor el 28 de octubre. Sin embargo, la instancia no se incluye en la Caja de Reclutas de Tenerife para su informe preceptivo hasta el 7 de enero de 1928. La prensa local sin embargo daría constancia de la presencia de Domínguez en su tierra natal a finales de marzo de 1927, al informar sobre el banquete de reconocimiento que el día veinte ofrecieran «los socios, amigos y admiradores» del joven pintor en los salones del Círculo Minerva de Tacoronte, cuyas paredes él mismo habría decorado. Entre los oradores intervinientes en el acto figurará el periodista tacorontero Antonio Dorta. La solicitud de prórroga de ingreso resultaría finalmente denegada.

En Tenerife permaneció Óscar Domínguez el tiempo que duró el periodo de instrucción en el acuartelamiento de costa del Regimiento Mixto de Artillería, que concluyó en octubre del mismo 1928. De esa etapa de su vida se conserva al menos un testimonio gráfico de su ejercicio en el servicio militar. Durante su instrucción en el Ateneo, Domínguez tendría noticias del colectivo multidisciplinar de artistas "Pajaritas de Papel" y el desarrollo de sus actividad teatral en torno al Círculo de Bellas Artes de Tenerife por mediación del poeta y compañero en el servicio militar, Domingo López Torres. Inmediatamente después de haber completado el tiempo de servicio en filas, jurar bandera y obtener liciencia ilimitada como soldado de cuota, Domínguez eleva un nuevo escrito a la autoridad militar del archipiélago, el 8 de noviembre inmediato, en solicitud de autorización para «"regresar a París, siéndole de absoluta necesidad el volver a la citada capital de Francia, por el tiempo de un año, para atender a sus intereses, que se encuentran en la actualidad en estado de abandono, a la vez que se compromete a presentarse en todo momento en que la Nación lo exija"». En el informe correspondiente, firmado el día 13, el coronel del regimiento mixto de Artillería de Tenerife «"estima que pudiera accederse a lo solicitado"» de acuerdo a la anotación mecanografiada al margen del escrito de referencia. En razón del requerimiento de su persona para el otorgamiento de poder frente a notario mediante firma, Domínguez habría permanecido, al menos, hasta el 30 de enero de 1929 en Tenerife.

A finales de 1928 Óscar Domínguez expone sus primeros lienzos en el Círculo de Bellas Artes de Tenerife junto con la pintora francesa Lily Guett. El catálogo de la exposición recoge sus siguientes obras: "La violinista de "Fetiche"", "Vendedores ambulantes", "En casa de la modista", "La partida de naipes", "Formas", "Arlequín y su mujer", "Las muchachas que van por agua", "Hombre pintando", "Mujer con bufanda roja", "La Exposición", así como tres trabajos sobre "Naturaleza muerta". Las obras son por entonces duramente criticadas, en particular por el escritor Ernesto Pestana Ramos, uno de los redactores de la revista "La Rosa de los Vientos":

El 10 de septiembre e 1931 Óscar Domínguez pierde a su padre Antonio Domínguez de Mesa, quien fallece acosado por los acreedores y dejando a su familia en una situación económica comprometedora; la familia perdería el derecho sobre la mayoría de los bienes de Antonio A. Domínguez ante las deudas contraídas por su propietario. En este momento, Óscar decide regresar a Tenerife para intervenir en la gestión administrativa de la herencia de su padre, con escasos logros. En enero de 1932 retorna a París. Hasta entonces Óscar Domínguez se habría mantenido gracias a una asignación mensual que recibía de Canarias. El joven pintor decide entonces ofertar sus servicios para la industria publicitaria. A raíz de los acontecimientos, Domínguez iniciará en estas fechas sus trayectoria como profesional del diseño, «"oficio que -según Jean- ejerció con imaginación, facilidad y desgana"». 

En este tiempo el pintor asiste a exposiciones de pintura moderna y frecuenta academias en las que se dibuja del natural con modelos a fin de mejorar sus dotes profesionales. La mayor parte de su trabajo como publicista, de la cual apenas se conservan obras, datan de 1934; entre ellas se halla un anuncio para la marca de caramelos de mantequilla Kréma encargado por la Office International de Publicité et d'Edition, así como un cartel turístico para el Patronato de Turismo del Cabildo Insular de Tenerife. Por otra parte, Óscar Domínguez recurrirá durante estos años a la falsificación de obras consagradas de autores como Monet o Sisley para la obtención de una mayor ganancia económica.

En 1932 Óscar Domínguez presenta sus primeros lienzos surrealistas en la exposición anual del Círculo de Bellas Artes de Tenerife entre el 18 y el 31 de diciembre junto a la obra de Robert Gumbricht, Servando del Pilar, Álvaro Fariña, Pedro de Guezala, Francisco Borges y Francisco Bonnin. Domingo López Torres dedicará un elogioso artículo a la muestra del pintor en La Prensa el 21 de diciembre. Otro reportaje, firmado por ≪Leonardo≫, recogerá positivamente la propuesta pictórica de Óscar Domínguez en el reportaje "Exposición del Círculo de Bellas Artes" el 29 de diciembre en el periódico Hoy.

En 1933 Domínguez envía desde París al equipo de redacción de "Gaceta de Arte" el texto "Revista internacional de exposiciones", el cual resulta publicado en marzo del mismo año por la propia gaceta en su número 13. Poco después Domínguez realiza un nuevo viaje a Tenerife acompañado por Roma, una consumada pianista polaca, hija de un médico y de una traductora de consignatarios de buques, de amplia cultura y un gran dominio de los idiomas. Aunque los padres de Roma tenían una posición económica elevada, su hija vivía separada de ellos sin recibir asignaciones económicas. Óscar y Roma convivirán durante este tiempo llevando una vida bohemia, con algunas penalidades en el barrio de Montmartre.

La primera exposición indivicual de Óscar Domínguez es organizada por el grupo de "Gaceta de Arte" y celebrada en el Círculo de Bellas Artes de Tenerife entre el 4 y el 15 de mayo. "Gaceta de Arte" incluirá paralelamente un breve artículo sobre la muestra en su número 15 del mismo mes. Para esta ocasión se exhibirán los lienzos del surrealista tinerfeño "Mujer invisible", "Niveles del deseo", "«La France»", "Edipo", "El enigma de la inspiración", "Estudio para autorretrato", "«Extase»", "Efecto surrealista", "Muerte de un buey", "Tarde de amor nº 1", "Tarde de amor nº 2", "El deseo", "Asunto decorativo", "Mi país natal", así como un segundo "Efecto surrealista". Eduardo Westerdahl publica en La Tarde un primer reportaje el 9 de mayo, "Círculo de Bellas Artes: La exposición surrealista del pintor Óscar Domínguez", mientras Domingo López Torres le dedica dos artículos: "Expresión de "Gaceta de Arte": ¿Qué es el surrealismo?" y "Expresión de "Gaceta de Arte": el surrealismo".

De la redacción publicada por Eduardo Westerdahl a tenor de la muestra de 1933 se extrae el siguiente comentario:

Tres meses después de la exposición en el Círculo de Bellas Artes de Tenerife, Óscar Domínguez participará junto con Servando del Pilar y Robert Gumbricht en una exposición colectiva organizada nuevamente por "Gaceta de Arte" en el Círculo Mercantil de Las Palmas, en Gran Canaria. En esta ocasión serán cuatro los artículos publicados sobre la muestra: "Visión: Óscar Domínguez", "Pasen, señores, pasen!...", "g.a. en Las Palmas: Óscar Domínguez", y "La exposición de 'Gaceta de Arte' en el Círculo Mercantil".

Agustín Espinosa, residente en estos años de la ciudad de Las Palmas de Gran Canaria, expresaría sus impresiones sobre la obra del pintor en los siguientes términos:

Por su parte, José Mateo Díaz refiere en el periódico La Provincia:

Gracias a la mediación de Óscar Domínguez, el equipo de redacción de "Gaceta de Arte" entra en contacto con el círculo surrealista de París; como consecuencia de este encuentro, se celebra en 1935 la "II Exposición Internacional de Surrealismo" en el Ateneo de Santa Cruz de Tenerife; la primera muestra de estas características celebrada en España y la segunda dentro de su género con una pretendida proyección internacional. El evento, encabezado por Agustín Espinosa como presidente del Ateneo y secundado por el resto de componentes de la gaceta contaría con la presencia de los franceses André Breton, su mujer Jacqueline Lamba y el escritor Benjamin Péret; la asistencia del poeta surrealista Paul Éluard habría sido asimismo inicialmente prevista. La exhibición, abierta al público entre los días 11 y 21 de mayo, conlleva la presentación de un conjunto de 76 obras conformado por óleos, acuarelas, aguafuertes o fotografías de artistas como Pablo Picasso, Joan Miró, Salvador Dalí, Max Ernst, Alberto Giacometti, Hans Arp, Man Ray, René Magritte, Marcel Duchamp, Yves Tanguy, Victor Brauner o el propio Óscar Domínguez, entre otros. Paralelamente, la proyección de la película de 1930 "La edad de oro" del director Luis Buñuel habría sido programada para la clausura de la muestra; sin embargo, diversas movilizaciones institucionales y mediáticas promovidas por colectivos católicos insulares propician la prohibición desde la Gobernación civil de sus diversos pases cinematográficos.

En 1936, tras el padecimiento de una fiebre paratifoidea, Domínguez regresa a Tenerife para su participación presencial en la "Exposición de Arte Contemporáneo" celebrada entre el 10 y el 20 de junio en el Círculo de Bellas Artes; una iniciativa organizada a instancias del grupo de "Gaceta de Arte" en colaboración con la asociación ADLAN. La exposición presenta al público sus lienzos "Máquina de coser electro-sexual", "Recuerdo de mi isla", "Cueva de guanches", "Tengo razón" y "Mariposas perdidas en la montaña", así como cuatro dibujos. El 20 de junio, durante el acto de clausura de la exposición, Domínguez pronuncia una conferencia en la que evoca «"la significación del objeto surrealista, los trabajos de Giacometti y explica una de las leyendas de uno de los cuadros que más expectación había causado en los visitantes de la exposición. Al llegar a este punto se advierten protestas en la sala que son sofocadas por el mismo público, quien prorrumpió en un largo aplauso al obtener del propio Domínguez la explicación del cuadro. Seguidamente continúa la conferencia en un tono delirante"». Eduardo Westerdahl publicaría una nota dedicada a Domínguez en el número 38 de "Gaceta de Arte", así como un artículo en "La Prensa", el 13 de junio. 

Tras el alzamiento militar de julio, el pintor encuentra refugio en Puerto de la Cruz, en casa de su hermana Julia. El día 13 de agosto escribe a Marcel Jean:

Con la adquisición de un pasaporte falso, Domínguez escapa del país a través de un barco frutero para fijar su estancia definitivamente en Francia.

En el número 28 de "Gaceta de Arte" de julio de 1934, Óscar Domínguez publica "Carta de París. Conversación con Salvador Dalí"; desde al menos esta fecha dataría el encuentro del pintor tinerfeño con el grupo surrealista parisiense. El encuentro entre Domínguez y André Breton habría tenido lugar entre septiembre y octubre de 1934 en el café de la Place Blanche, donde el grupo surrealista se reunía habitualmente; antes de ingresar en el grupo «"tuvo que enseñarle a Breton "dibujos y el catálogo de las exposiciones de 1933""». Por entonces «"Domínguez inaugura su técnica de la decalcomanía. Un trabajo que gaceta de arte publica por vez primera en la cubierta de la monograría que Westerdahl escribe sobre Willi Baumeister. Óscar hace sus primeras "decalcomanías sin objeto preconcebido" o "decalcomanías automáticas con interpretación premeditada" abriendo así un nuevo campo de experiencias para el surrealismo"».

El procedimiento para la realización de decalcomanías ha sido descrito de la siguiente manera:

Hasta finales del decenio, la participación de Domínguez en las actividades del colectivo surrealista francés se torna habitual. En 1935 toma parte en la exposición "International Kunststudstilling Kubisme-surrealisme", organizada por Vilhelm Bjerke-Petersen para su apertura al público entre los días 15 y 28 de enero en Copenhague; Domínguez contribuye en esta ocasión con la muestra de los lienzos "Désir d'eté" y "Papillons perdus dans la montaigne". Entre julio y agosto, en compañía de Marcel Jean, Esteban Francés y Remedios Varo, renueva el procedimiento del "cadáver exquisito" durante una estancia en Barcelona, asociándole la técnica del fotomontaje. En 1935, se halla asimismo entre los firmantes del "Ciclo sistemático de conferencias sobre las más recientes posiciones del surrealismo", así como de la octavilla colectiva "Du temps que les surréalistes avaient raison", que escenificará la ruptura del movimiento surrealista de París con el Partido Comunista. En diciembre del mismo año forma parte de la exposición "Dessins surréalistes" en la Galería Aux quatre chemins del boulevard Raspail de París junto con Hans Bellmer, De Chirico, Hans Arp, Victor Brauner, Salvador Dalí, y Pablo Picasso, entre otros.

En este tiempo, Óscar Domínguez confecciona de manera alterna objetos surrealistas como "Las peregrinaciones de Georges Hugnet", "Exacte sensibilité", "Spectre du silicum", "conversión de la force" y "«Viaje al infinito» o «La llegada de la Belle Epoque»", con los que participa junto a Marcel Jean, Arp, Giacometti, Tanguy, Breton, Picasso y Miró, en la "Exposition surréaliste d'objects" celebrada en mayo de 1936 en la Galería Charles Ratton de París. Paralelamente a su participación presencial en la "Exposición de Arte Contemporáneo" del Círculo de Bellas Artes de Tenerife, Domínguez se sumará a "The International Surrealist Exhibition" de las New Burlington Galleries de Londres, entre el 11 de junio y el 4 de julio. La "décalcomanie sans objet préconçu" de Domínguez es presentada por André Breton en el número 8 de la revista de orientación surrealista "Minotaure", correspondiente a junio de 1936; «"Breton muestra la decalcomanía como uno de los procedimientos que ha de ser incorporado a los ""Secrets de l'art magique surréaliste""." [...] "Yves Tanguy, André Breton, Jacqueline Lamba, G. Hugnet y Marcel Jean, sus compañeros parisinos, aprenden el procedimiento a finales de 1935, y participan con sus decalcomanías en el número de "Minotaure" donde también aparece un relato de Benjamín Péret inspirado en estas imágenes."» Asimismo, en 1936, colabora con una ilustración en aguafuerte, a través de la casa Editions G.L.M., en el libro de Georges Hugnet "La Hampe de l'imaginaire".

Desde 1937, Óscar Domínguez traslada su residencia al barrio de Montparnasse, donde mantendrá una estancia fija hasta el final de su vida. Inicialmente se instala en un "atelier" situado en el nº 83 del Boulevard de Montparnasse; allí realiza diversos muebles surrealistas, entre los que destacan una gran mesa en forma de piano y un sillón obtenido tras cubrir con satén rojo el interior de una carretilla. Esta última obra, titulada "La brouette capitonnée", aparece retratada en el número 10 de "Minotaure" sirviendo como asiento para un maniquí ataviado por Lucien Lelong en una fotografía de Man Ray. Posteriormente, Domínguez se domiciliaría en un estudio ubicado en el nº 23 de la rue Campagne Première, perteneciente al escritor César González Ruano y cedido por este al pintor en 1942.

En 1937 aparecen además las primeras "decalcomanías del deseo", cuya técnica «"consiste en aprovechar los paisajes inconscientemente como escenografía para una nueva creación. El artista interpreta la decalcomanía y de este modo "cierra" la obra."».En este género destacará la serie de decalcomanías titulada "Grisou", cuya publicación anunciara el editor Guy Lévis Mano para el mismo año; sin embargo, el libro no se llegaría a editar por falta de suscriptores. "Grisou" se presentará finalmente al público en 1990 gracias a la iniciativa de Jean-Luc Mercié, quien edita la serie respetando en lo posible el proyecto inicial de G.L.M. a partir de las 16 planchas originales reproducidas en fototipia. En los meses de marzo y abril, Domínguez participa en la exposición "First British Artist Congress", organizada en Londres por la sección cultural del Partido Comunista inglés. En mayo intervendrá en la exposición inaugural de la Galerie Gradiva de París, dirigida por André Breton. El pintor tomará parte asimismo en las muestras "Surrealism", impulsada por Shuzo Takiguchi para el mes de junio en Tokio, en la exposición surrealista organizada en la Gordon Fraser's Gallery por la oficina de la Sociedad artística de la Universidad de Cambridge entre octubre y noviembre, y en "Surrealist Objects & Poems" de la London Gallery, entre el 24 de noviembre y el 22 de diciembre.

En 1938 Óscar Domínguez participa en la "Exposition internationale du surréalisme", organizada por Breton y Éluard en la Galerie des Beaux-Arts de París, del 7 de enero a finales de febrero. En esta ocasión presenta las obras "Les Guanches", "Le salon-plage", "Madame", "Le brouette capitonnée", "Le Tireur", "Larme" y "Jamais"; estos dos últimos objetos se reproducen en el "Dictionnaire abregé du surréalisme" de la galería, adjunto a un catálogo de la exposición. El éxito de la muestra propiciará su ampliación entre los meses de marzo y junio con una versión reducida desarrollada por Hugnet y el pintor Kristians Tonny para la Galería Robert de Amsterdam; Domínguez colaborará entonces con la cesión de "Jamais", "Madame", "Les Pyrénées" y "L'Imprévu". Durante la "64th Autumn Exhibition" celebrada entre el 12 de octubre y el 23 de noviembre en la Walker Art Gallery de Liverpool se expondrá el cuadro del autor "The Four Seasons". Por estas fechas, su objeto "Ouverture" figurará además entre las «"tarjetas postales surrealistas con certificado de autenticidad"» editadas por Georges Hugnet.
1938 es también el año en el que el Doctor en Ciencias Físicas y Matemáticas por la Universidad de La Plata Ernesto Sabato inicia, con una beca postdoctoral, sus investigaciones sobre las radiaciones atómicas del actinio en el Laboratorio Curie de París. En los ambientes nocturnos parisinos, frecuentados tanto por Breton como por el propio Sabato, el futuro Premio Cervantes de 1984 habría entrado en contacto con Domínguez gracias a la mediación del periodista argentino Ernesto Bonasso. La relación de amistad entre Sabato y Domínguez resultaría «"más que intensa: congeniaron desde el primer instante y enseguida comenzaron a trabajar juntos en el "atelier" del pintor. Allí pasaban largas horas delirando entre el olor a trementina y botellas de coñac o de vino que no cesaban de correr por sus manos. Domínguez cada vez que acababan una botella le instigaba al suicidio, sugerencia que le repetía, una y otra vez, y a lo que Sabato respondía siempre negativamente"». La colaboración creativa de Domínguez y Sabato se vería reflejada en contribuciones como la nota expositiva y conjunta relativa a la teoría de inspiración científica «"superficies litocrónicas"», incluida en el artículo de Breton de mayo de 1939 "Des tendances les plus récentes de la peinture surréaliste". Sabato permanecería en París hasta el estallido de la Segunda Guerra Mundial, momento en el que retornaría a la Argentina. El escritor integrará posteriormente la figura de Óscar Domínguez en sus novelas "Sobre héroes y tumbas", como personaje, y en "Abaddón el exterminador". 

Óscar Domínguez realizó ilustraciones para tres ediciones de la casa denominada GLM, iniciales del poeta, tipógrafo y librero francés de origen turco Guy Lévis Mano. En 1935, el colectivo surrealista, cuyos editores predilectos habrían desaparecido o se hallaban en dificultades, iniciará una fructífera línea de colaboraciones con Lévis Mano, cuyas ediciones pasarían de una media anual de seis publicaciones a cuarenta. 

La impresión del boletín de suscripción de junio de 1935 para el "Cycle systématique de conférences sur les plus récentes positions du surréalisme", incluirá diversas ilustraciones de Domínguez, consideradas como la continuación de su trabajo para el número de diciembre de 1933 de "Gaceta de Arte". En julio de 1935 aparece a través de esta editorial la colección "Repères", concebida, al igual que los precedentes "Cahiers des Douze", como una asociación individualizada entre la labor del artista plástico y el texto breve del poeta. El 12º cuaderno de "Repères" reunirá el texto de Georges Hugnet "La hampe de l'imaginaire" con el primer grabado documentado de Domínguez. A falta de suscriptores necesarios, quedará inédito "Grisou" de Domínguez y Jean, que reproducía en fototipia dieciséis decalcomanías automáticas de interpretación premeditada, acompañadas con un texto de Breton; el boletín de suscripción de GLM anunciaría su aparición para octubre de 1937. Breton reúne ilustraciones y textos, entre otros su sueño del 7 de febrero de 1934 sobre la pintura de Domínguez que titula "Accomplissement onirique et genése d'un tableau animé", para el séptimo de los Cahiers GLM, impreso el 30 de marzo de 1938; el mismo número incluirá el diseño del pintor canario "Le souvenir de l'avenir". El 10 de agosto de 1938 se publican las "Oeuvres complètes" de Lautréamont; un dibujo de Domínguez ilustra el tercero de "Los cantos de Maldoror". En 1939 el proyecto de reedición de "Los cantos de Maldoror" quedará sin realizarse. La guerra mundial pondría fin a las relaciones de GLM con el grupo surrealista.

En el curso del año 1938 destacará significativamente el accidente que Domínguez provocaría durante el desarrollo de una disputa y que le valdría al pintor Victor Brauner la pérdida de la visión en un ojo, el 27 de agosto. De acuerdo con la interpretación de numerosos miembros del colectivo surrealista parisino, Victor Brauner habría presagiado su propia lesión con la producción de obras como "Autorretrato con el ojo extirpado", de 1931. El incidente, que implicaría a los autores Remedios Varo y Esteban Francés, ha sido descrito de la siguiente manera:

El accidente «"contribuye a la fragmentación del grupo surrealista, iniciada en 1936 a raíz de la marcha de Éluard, de la excomunión de Parisot, de las rupturas con Arp, Dalí, Ernst, Hugnet, Man Ray..."». El sentimiento de culpa que despierta este incidente en Óscar Domínguez habría resultado para él «"mucho más insoportable que el sufrimiento que debió de padecer su amigo Brauner, víctima de la agresión. Todo parece indicar que en Domínguez la consecuencia del sacrificio fue tan devastadora en un plano psicológico como benéfica para Brauner desde el punto de vista profesional. La mutilación devino para él algo así como un rito de paso: sometido a tal prueba alcanzó un grado de consciencia más elevado. En cambio, el sufrimiento de Domínguez no tenía compensación posible. Durante meses evitó asistir a las reuniones de grupo. No deseaba encontrarse con Brauner, del que, sin embargo seguía recibiendo noticias por sus amigos comunes, Jacques Hérold y Tanguy."» En los anales del surrealismo, el "affaire" del ojo de Brauner habría sido asumido «"como un caso, tal vez el más relevante, de la regla que proclama el valor absoluto que las premoniciones ostentan en la vida del hombre. Pierre Babille y Juan Larrea así lo interpretaron. El segundo vio en este suceso trágico algo que afectaba no sólo a Brauner, sino también a Domínguez, víctima y verdugo respectivamente, y en última instancia a todo el grupo surrealista, que accedería de este modo a un estado de videncia por medio de la pérdida del ojo de uno de sus miembros."».

En 1939 Óscar Domínguez participa en la exposición "Le Rève dans l'art", organizada por Frédéric Delanglade, y en el "50º Salon des Indépendents", poco antes del estallido de la guerra. Su cuadro "Lancelot 28º, 7º" se verá reproducido en el número 12-13 de la revista "Minotaure".

Al comienzo de la guerra Domínguez pasa algún tiempo en Perpignan en compañía de Jacques Hérold, Victor Brauner y Remedios Varo, trasladándose posteriormente a Marsella con sus compañeros por motivos de seguridad. En Marsella, se encontrará con Breton, Ernst, Péret, Marcel Duchamp, Hans Bellmer, Wilfredo Lam, André Masson y René Char; el colectivo desarrolla en torno a la vivienda provisional de Breton, la villa de Air-Bel de La Pomme, diversas actividades lúdicas en espera de un visado para salir de Francia. Refugiados bajo el auspicio del comité estadounidense para la ayuda a los intelectuales, el grupo genera diversas creaciones, entre las que destacan la serie de dibujos colectivos de Marsella, así como un juego de cartas cuya baraja elaborarían los propios artistas; «"inspirado en el famoso tarot medieval de Marsella, reconstruido por Paul Marteau en el siglo XVIIII, aunque no era propiamente un tarot, sino un juego de naipes organizado de acuerdo con la escala de valores del surrealismo.»

Entre enero y febrero de 1940 Domínguez participa en la "Exposición internacional del surrealismo" de la Galería de Arte Mexicano, organizada por los pintores Wolfgang Paalen y César Moro. Durante el mes de junio se imprimirá en la capital francesa la edición de lujo nº15 de la "Anthologie de l'humour noir" de Breton, cubierta con una decalcomanía intervenida del tinerfeño. En la primavera de 1941, André Breton, secundado en lo sucesivo por diversos miembros del colectivo surrealista francés, embarca junto a su familia hacia los Estados Unidos. Por su parte, Domínguez retornaría en la misma primavera de 1941 a la ciudad de París.

Años más tarde, en una misiva dirigida al crítico de arte Eduardo Westerdahl, el autor revelaría:

Tras su regreso a la ciudad de París en el contexto de la ocupación de Francia por las fuerzas del Eje, el pintor se reunirá asiduamente con los artistas Hugnet, Lucien Coutaud, André Thirion o Apel·les Fenosa en el restaurante "Le Select" o en el "Café de Flore". En torno al poeta Hugnet se organizan alternativamente una serie de tertulias celebradas por recomendación de Pablo Picasso en el restaurante "Le Catalan", donde no se tienen en cuenta los vales de racionamiento; entre los asistente habituales a estas tertulias se encuentran Hugnet, Picasso, Paul Éluard, Charles Ratton, Robert Desnos, Michel Leiris o el propio Domínguez, entre otros. En este tiempo el pintor canario estrecharía sus lazos de amistad con Éluard y Picasso.
Durante los primeros años de la ocupación, el pintor surrealista demuestra un notable interés por las actividades del colectivo "La Main à plume", el único grupo filial al movimiento de Breton constituido entonces en Europa. Desde su primer año de vida, "La Main à plume" intentará reunir en su seno a los diversos artistas dispersados por la guerra: Picasso, Éluard, Brauner, Hérold, Maurice Henry, Jean Ferry y Léo Malet. Los autores logran imprimir una decena de publicaciones colectivas y una treintena de "plaquettes" individuales, así como una serie de octavillas dirigidas a los escritores comprometidos con la prensa colaboracionista o a aquellos otros que pretendían situarse al margen de la situación sociopolítica del momento. Domínguez participa en las experiencias de pinturas colectivas impulsadas por el poeta y periodista Jean-François Chabrun para las páginas de "Géographie Nocturne". En diciembre de 1941 aparecen sus dibujos para "Transfusion du verbe", además del frontispicio para un largo poema de Christian Dotremont, "Noués comme une cravate". En octubre de 1942 aporta una viñeta para una antología de Laurence Iché titulada "Au fil du vent", que servirá posteriormente como motivo de cubierta para la serie de Páginas Libres de "La Main à plume". En enero de 1943 un aguafuerte de Domínguez integra los ejemplares de "Aux absents qui n'ont pas toujours tort", de Noël Arnaud. En "La Conquête du Monde par l'Image" publicaría asimismo un destacado artículo relativo a las superficies litocrónicas titulado "La pétrification du temps".

Del 1 al 14 de diciembre de 1943 presenta en la Galerie Louis Carré su primera exposición individual en París. En el mismo año figura entre los signatarios de la declaración colectiva "La Parole est à Péret", publicada el 28 de mayo en Nueva York, al tiempo que ilustra los textos de Hugnet y Thirion, "Le Feu au cul" y "Le Grand ordinaire". Del 30 de noviembre al 21 de diciembre participa en la exposición colectiva "Un groupe d'artistes de l'école de Paris" en la Galerie des Trois Quartiers. En este tiempo Óscar Domínguez se distancia de Roma, al tiempo que conoce a la estudiante de la Universidad de Limoges Maud Bonneaud, con la que se casaría años más tarde.

En el verano de 1944 «"se inica una fase nueva con la liberación de París"». Para Domínguez, la inmediata posguerra se presentará como «"uno de los periodos más equilibrados de su vida. Su arte refleja claramente la estabilidad y el optimismo con el que empezó a vivir después de la guerra"»; a este hecho habría de contribuir su matrimonio con Maud Bonneaud en 1945. Participa en el "Salon d'Automne" de octubre de 1944 donde se ofrece un gran homenaje a Picasso, viaja a Alemania con motivo de la primera gira de teatro francés como autor de los decorados y vestuario de la obra de Jean-Paul Sartre "Les mouches", hallándose además presente en todas las exposiciones de los «Españoles de París» que tienen lugar en Europa.

En 1945 la labor expositiva de Domínguez junto a su intervención en las revistas de moda resultarán prolíficas; colabora en el "Salon de Mai", el "Salon de Tuileries", el "Salon des Surindépendants" y en las exposiciones "Quelques peintres et sculpteurs espagnols" de "l'Ecole de Paris". Al tiempo, presenta su primera y única exposición individual de Nueva York, en la Hugo Gallery. Destaca en este año su contribución en la elaboración de un gran fresco para la sala de espera del hospital psiquiátrico Saint-Anne de París junto a Marcel Jean, Jacques Hérold, Maurice Henry, Frédéric Delanglade, Louis Fernández, Francis Bott, Honorio García Condoy, Baltasar Lobo y Manuel Viola. En 1946 viaja a Checoslovaquia con motivo de la muestra celebrada a instancias de la Asociación de Artistas Plásticos "Arte de la España Republicana", en la Sala Mánes de Praga, del 30 de enero al 23 de febrero; la obra de Domínguez «"obtiene una respuesta más particular, lo que explica la invitación a exponer individualmente durante los tres años posteriores en Olomouc, Bratislava y Praga"». En 1947 aparece su relato poético "Les deux qui se croisent" a través de la editorial Fontaine para la colección "L'Age d'or", al tiempo que ilustra una reedición de "Poésie et verité" de Éluard con 32 aguafuertes.

Tras obtener la nacionalidad francesa en 1948, el pintor canario culmina la década bajo una intensa actividad. En julio de 1949 escribe nuevamente a Westerdahl:

Durante todo este periodo «"hay, naturalmente, una importante evolución en Domínguez. La influencia de Picasso es ciertamente visible tanto en la manera como en los temas, y con ello pierde, no sin algunos fulgores tardíos, la soltura estremecedora e incluso desordenada que hacía apreciar su modo de proceder"». Para Domínguez, «"la admiración que consagra a Picasso va a transformar efectivamente su visión de un modo radical. Muchas de las cartas que envía a Eduardo Westerdahl lo demuestran"»:

De igual forma, en la misiva citada del 27 de abril de 1949:

Domínguez, en efecto, se había alejado del surrealismo ortodoxo correspondiente al círculo de Breton desde la guerra; durante una estancia en Londres escribirá a Westerdahl:

Prueba de ello es su exclusión de la muestra "El surrealismo de 1947" organizada por Breton en la Galería Maeght de París.

A comienzos de la década de los 50 Óscar Domínguez presentaba claros síntomas de declive. «"Había apurado la existencia hasta el límite. A los estragos producidos en su organismo por la vida bohemia que llevaba desde que llegó a París se sumaban ahora los signos de la acromegalia, que ya empezaban a manifestarse en la apariencia monstruosa que iban tomando su rostro y sus manos."» En 1950 realiza una exposición individual en la Galerie Apollo de Bruselas, con prólogo de Christian Dotremont, del 4 al 16 de marzo. En la capital belga conoce a la escultora Nadine Effront, «"mujer inteligente, activa, dominante y con cierto talento artístico, con quien inicia una relación sentimental"», poniendo así fin a la que ya mantenía mediante su único enlace matrimonial con Maud Bonneaud. En el mismo año escribe:

En este tiempo el pintor «"empezó a frecuentar los salones de la "rive droite". La vizcondesa de Noailles acababa de entrar en su vida, aunque Domínguez tardó todavía un año en abandonar definitivamente a Nadine Effront. Fue entonces cuando se introdujo en los círculos de la alta sociedad parisina"». En 1951 expone individualmente en la Galerie de France de París, donde obtiene su primer gran éxito, en la Galerie Georges Moos de Zurich en el mes de mayo, nuevamente en la Galerie Apollo de Bruselas del 23 de noviembre al 7 de diciembre, y en la Galerie Le Touquet de Paris-Plague.

En 1952 participa en la "Exposition de peinture surréaliste en Europe", organizada por el pintor Edgar Jené en el Saarland Museum de Saarbrücken. Inicia en estos momentos su romance con la vizcondesa de Noailles. La vizcondesa, de nombre Marie-Laure, «"era cuatro años mayor que él (había nacido en París el 31 de octubre de 1902) y frisaba los cincuenta años cuando conoció a Domínguez. Aunque nunca fue una gran belleza, tenía lo que los franceses llaman "charme"." [...] "El título de Noailles le correspondía a su marido. Ella era hija única del matrimonio de Maurice de Bischofsheim y Marie Thérèse Anne Germaine de Chevigné" [...] "Domínguez se había convertido en un personaje mundano al lado de la vizcondesa. Pasaba temporadas en el palacio situado en el número 11 de la place des États Unis de París, mansión construida por el abuelo paterno de Marie-Laure que albergaba una magníica galería de obras de arte."»

En 1953 se celebra en la Galerie La Demeure de París una exposición individual de cartones de tapices del autor. Del 5 de julio al 31 de agosto el pintor canario participa en la exposición Art fantastique en la Kunsthalle de Ostende. En junio de 1954 expone individualmente en la Galerie Drouant-David de París, al tiempo que participa en el mes de julio en la muestra "Surrealismo" organizada por Sophie Babet en Lima. El apoyo de la vizcondesa «"le había ayudado a conseguir un éxito rotundo en la exposición celebrada el 3 de junio de ese año en Drouant-David "[...]" Domínguez formaba parte ya del "gran mundo" de París, pues en los últimos cinco años de su vida se había convertido en el animador divertido e ingenioso de la alta sociedad parisina, abandonando casi completamente la vida bohemia de Montparnasse. La influencia de la vizcondesa había pesado para que se le abrieran las puertas del mundo artístico de la capital de Francia. Mientras tanto, Maud había partido nuevamente a Tenerife y esperaba tan sólo que Domínguez arreglara definitivamente los papeles del divorcio para poder casarse con Eduardo Westerdahl."» Durante su primera estancia en Tenerife, Maud Bonneaud habría tenido noticias del precario estado de salud de Domínguez a partir de su correspondencia con Westerdahl:

Del 19 al 30 de noviembre de 1955 Domínguez expone individualmente en el Palacio de Bellas Artes de Bruselas, del 11 de mayo al 2 de junio en la Galerie Diderot de París, y en 1957, del 12 de noviembre al 2 de diciembre, en la Galerie Rive Gauche. Esta muestra final, la última en vida del autor, habría sido reconocida tanto por el artista como por la crítica como un destacado fracaso profesional. Un año antes, Óscar Domínguez testimoniaría su situación personal en una epístola dirigida a Eduardo Westerdahl y Maud Bonneaud, en Tenerife:

Sabemos que Óscar Domínguez «"fue internado en un sanatorio psiquiátrico en tres ocasiones, por causa de la bebida. La primera vez fue conducido por Maud y la vizcondesa. A los pocos días se escapó de la clínica privada en que le habían recluido "[...]" La tercera y última vez fue, indudablemente, la que supuso unas consecuencias más dolorosas para él, que fue conducido por el chófer de la vizcondesa al Sanatorio Siquiátrico de París de Sainte-Anne (donde en 1945 él había realizado un mural colectivo) Este sanatorio no era una clínica privada y sus métodos eran muchos más violentos. Pero, en aquella ocasión, Domínguez estaba en un estado agresivo y peligroso. La policía le perseguía porque había disparado su revólver en la calle."» 

Antes de cometer suicidio, Óscar Domínguez había pasado varios días en coma etílico. «"En esta situación se encontraba cuando fue invitado al "reveillon" de Noche Vieja en casa de su amiga Ninette. Asistían también Man Ray, Patrick Waldberg, Félix Labisse y Max Ernst. Domínguez no asistió. Al día siguiente, Ninette, asustada, mandó a su chófer para que investigara lo sucedido [...] Al entrar lo encontraron tendido en el suelo del cuarto de baño, desnudo. Tenía cortes en las muñecas y los tobillos. El médico, Dr. Degême, afirmaba que no murió desangrado, sino por causa de un golpe recibido en la nuca al resbalar probablemente en su propia sangre."» 

Fue llevado a L'Institut Médico-Legal y depositado en la iglesia de Notre-dame-des-Champs.

El miércoles día 3 de enero de 1958 fue enterrado en el cementerio de Montparnasse, en el panteón de los Noailles.

Se ha incidido en numerosas ocasiones sobre las dificultades y limitaciones que el acercamiento a la obra de Óscar Domínguez presenta sin el estudio previo de su componente biográfico. Resulta asimismo recurrente la observación generalizada de una cierta sensación inicial de extrañamiento o «"de pérdida, del extravío ante una obra de múltiples facetas y que a pesar de todo se percibe como unitaria"». Así, el examen anticipado de la persona y su devenir histórico se trasluciría en este caso como una de las principales vías de acceso hacia el conocimiento en dirección a su creación. La personalidad del artista parece revelarse de hecho como el elemento unitario decisivo en un rendimiento plástico marcado por la diversidad de tendencias; de cualquier forma, y a pesar de sus muchos cambios de estilo, «"a lo largo de toda la producción de Domínguez se detecta un mismo espíritu imaginativo, violento y atormentado, en el que el humor y la poesía vienen a poner un punto de alivio a su vértigo interior"». Existe por lo demás cierto consenso en el ámbito académico respecto a la excepcional confluencia identitaria advertida entre Óscar Domínguez y el movimiento artístico del Surrealismo, al que se le asocia con mayor frecuencia. La expresión del subconsciente en Domínguez no parecía responder a un ejercicio intelectual o a una técnica aprendida; por el contrario, las manifestaciones conscientes e inconscientes se daban espontáneamente en su persona como una sola unidad. El carácter inconsciente resultaba de hecho propio de la personalidad del pintor. Domínguez habría reflejado una prominente incapacidad para la represión de sus propios instintos. La satisfacción inmediata de sus deseos «"le impulsaba a la búsqueda incesante de la libertad y el más absoluto desprecio por la ley"»; de su temperamento subversivo se extraería este afán proactivo y rebelde de ruptura con toda norma, de desafío a la autoridad, de rebasamiento allá donde imperara o se impusiera la censura. Domínguez parecía mostrar además indicios de una desmesurada apetencia sexual, así como una patente pulsión de muerte. El surrealismo en Domínguez podría apreciarse entonces no solo en su obra, sino en su comportamiento. «"Su vida en París fue la prolongación del deseo de su infancia"». 

Técnicamente Óscar Domínguez se distingue por la particular soltura de su trazo, cuya fluidez se emparentará naturalmente con el ejercicio del automatismo pretendido por los prosélitos del surrealismo y encomendado por el mismo Breton. Este último realizará en su artículo "Des tendances le plus récentes de la peinture surréaliste" una semblanza del procedimiento del tinerfeño, «"d'un mouvement de bras aussi peu dirigé et aussi rapide que celui du nettoyer de vitre ou de l'ouvrier".» Marcel Jean, por su parte, apuntaría a propósito de la incursión del pintor en su posteriormente denominada etapa cósmica cómo su método se daría «"par pure paresse, au fond, parce qu'il trouvait moins fatigant de se laisser aller "au fil du pinceau"," [...] "Dans son atelier du boulevard du Montparnasse en 1937, des amies, des visiteurs entraient et sortaient, Dominguez pouvait fort bien écouter leurs conversations et y prendre part sans que son pinceau s'arrête pour autant de glisser et tourner sur la toile."» Este estilo torrencial, asociado con la noción pictórica del "fa presto", se aliará con una factura a menudo amortiguada, veraz o primaria para ofrecer por conjunción la fijación de impresiones de una psicología neta. Desde el punto de vista temático, destaca en el repertorio artístico de Domínguez la ilustración reiterada de determinados elementos figurativos, acompañados por otros más ocasionales. Habitual en Óscar Domínguez es la representación del abrelatas, el sifón, la bicicleta, la máquina de coser, el revólver o el piano; la máquina de escribir, el imperdible, el fonógrafo o el huevo; el toro, la mariposa, el caballo, el león o el drago, así como diversidad de restos óseos, residuos vegetales y articulaciones más o menos escabrosas. Las representaciones humanas más comunes se corresponden con las de la mujer del rostro oculto y el torero. La mayoría de las veces, «"en los cuadros de Domínguez, estos objetos o seres vivos se pliegan a las leyes del simbolismo freudiano".» Sin embargo, la figuración en Domínguez difícilmente puede calificarse de definitiva; en sus imágenes los elementos parecen responder a los principios de la alteración, la asociación o la dinámica. La abstracción se hace patente por contraposición en su concepto del paisaje, frecuentemente alusivo a su isla natal o inspirado en ella. 

El legado cultural del autor tinerfeño evidenciaría finalmente una predilección profesional por las artes plásticas prácticamente restrictiva; un libro de poemas, "Les deux qui se croisent", constituye el caso más significativo fuera de este ámbito de creación. La exploración de posibilidades en su género, sin embargo, se revelará profusa. El arte preferente en Óscar Domínguez es el de la pintura. Cincuenta años después del fallecimiento del pintor no menos de 300 óleos se conservan mayoritariamente en los fondos del Instituto Óscar Domínguez anexo al complejo cultural (TEA) Tenerife Espacio de las Artes de Santa Cruz de Tenerife, en el Museo Nacional Centro de Arte Reina Sofía, así como en una amplia diversificación de fondos de exposición privada o colecciones de carácter particular. El óleo sobre tela o lienzo u otros soportes ocasionales como la madera, el gouache sobre papel o lienzo, o la tinta sobre papel constituyen el cuerpo principal de esta producción. Además, dentro del catálogo de obras de Óscar Domínguez destaca la variante del objeto artesanal como propuesta o prototipo del objeto surrealista, para cuya industria el tinerfeño se mostraría especialmente capacitado; los formatos logrados son los del simple objeto, el objeto-cuadro o el objeto-escultura. Completan este modelo de producción relieves en madera o en bronce, la cerámica pintada o el esmalte sobre plancha de cobre. Abundante resultará también la aportación del autor en el campo de la ilustración de ediciones de textos impresos, libros de poesía, boletines o revistas, a modo de gráficos de acompañamiento para su reproducción seriada, diseños en portadas o en cartelería. Impresiones de estos trabajos han sido registradas en la modalidad del aguafuerte, la litografía y otras formas de grabado. Existe por lo demás un conjunto preservado de numerosos dibujos a lápiz del pintor sobre papel, cartas, viñetas o simples bocetos. Puntualmente Óscar Domínguez habría incursionado en el género del fotomontaje, el mural, la escultura de hierro forjado, en el armazón de piedra o en la escenografía. 

La obra de Óscar Domínguez puede clasificarse de acuerdo a un criterio de distinción entre componentes estéticos comunes en su pintura, según etapas, o a los formatos o soportes correspondientes a la diversas modalidades de su práctica artística.

El catedrático de Historia del Arte por la Universidad de La Laguna, docente e investigador Fernando Castro Borrego ha distinguido en la trayectoria pictórica de Óscar Domínguez un conjunto de etapas diferenciales en el tiempo, distinguibles por la coherencia de las relaciones contenidas. Pinturas de Óscar Domínguez asociadas a determinadas etapas pueden ser localizadas en ocasiones dentro de contextos sin aparente correspondencia. Por otra parte, algunas obras compartirían características propias de varios de estos intervalos en función de guía, especialmente durante la sucesión de etapas contiguas o momentos de transición. 
El periodo inicial de la obra de Domínguez abarca desde la temprana pintura vocacional efectuada durante su juventud en Tenerife hasta la labor y muestra de 1928, concluyendo definitivamente con su segundo traslado y establecimiento en París en 1929. El mundo de la moda es por entonces una afición que comparte con Selina Calzadilla, amiga de sus hermanas, quien durante los veranos alterna temporadas con la familia en Tacoronte. De 1924 data un testimonio fotográfico atribuido a Calzadilla en el que el joven autor posa ante un desnudo femenino suyo, cuyo estilo recuerda al arte de Amedeo Modigliani o a «"la estética del art déco. Tales imágenes pudo haberlas contemplado en las revistas de la época".» Se conservan muy pocos lienzos de esta etapa inicial; entre los más destacados se hallan el "Autorretrato" de 1926 y el "Retrato de Celina Calzadilla" de 1927. En este último año Domínguez emprende su primer viaje a París, en donde habría entrado en contacto directo con las tendencias de la vanguardia en la pintura. Una inmediata aproximación al cubismo puede apreciarse tras su primer encuentro con los ambiente parisinos. Tal influencia se contempla en "Bodegón" o en "Los músicos", de 1928. El conocimiento del círculo de actividad surrealista podría remontarse a su vez al año 1927, cuando comparte el ejercicio del servicio militar con el seguidor del movimiento Domingo López Torres. Sus trabajos son expuestos ya en 1926, obteniendo una respuesta negativa de crítica y público, especialmente incidente con la exposición en el Círculo de Bellas Artes de 1928. A este mismo periodo pertenecen "Jardín" y "Mujer con mantón y peineta", fechados en 1926.

Esta etapa comienza en 1929 con el segundo traslado del pintor a París y se prolonga hasta 1938. Las primeras telas manifiestamente surrealistas de Domínguez se producen con anterioridad a su ingreso efectivo en el grupo surrealista, acaecido en 1934. Ello se explica por el supuesto de que el autor se habría formado inicialmente de manera autónoma en esta práctica cuando el arte surreal ya había alcanzado el espacio público con su difusión impresa y en exposiciones. Por otra parte se atribuye este hecho a la natural predisposición en el carácter específico del tinerfeño. Sobresalen en un principio "La media", de 1929, y "Souvenir de Paris" de 1932. Los cuadros comprendidos entre 1933 y 1937 se manifiestan formalmente deudores del estilo que por entonces hace famoso a Dalí. Ambos coinciden en la predilección por los cuerpos sustraídos, ímprobos y prolongados, flácidos y lacerantes, en la negación del espacio como realidad física y el interés por el objeto simbólico. En su contenido en cambio, el surrealismo de Óscar Domínguez recuerda a las creaciones de Max Ernst, ya que «"tanto Ernst como Domínguez tuvieron en común un alto sentido poético y una utilización ampliamente imaginativa de la metáfora".» En 1933 el pintor celebra su primera exposición individual, que es también la tercera en el Círculo de Bellas Artes de Tenerife, mediada en esta ocasión por el equipo de "Gaceta de Arte". El evento es empatado con la colectiva del mismo año organizada por la revista en el Círculo Mercantil de Las Palmas. La constitución previa de un colectivo propenso a la recepción de las vanguardias en las islas facilita la promoción de Domínguez en el circuito de la crítica del arte. Despuntará aquí "Los niveles del deseo". A este intervalo se deben también "Autorretrato", "La Boule rouge", "Drago", "Retrato de Roma" y "Le Chasseur", realizados en 1933. 

Cuando Óscar Domínguez participa en la "Exposición Internacional del Surrealismo" de Santa Cruz de Tenerife en 1935 ya forma parte integrante del grupo surrealista y es él de hecho quien media en su realización. La "Exposición de Arte Contemporáneo" de 1936 será la última muestra en vida en su isla natal, donde parece preludiar la evocación regional con sus "Mariposas perdidas en la montaña", "Recuerdo de mi isla", "Cueva de guanches" y la "Máquina de coser electro-sexual". "Cueva de guanches" secciona el paisaje, que evidencia su contenido subterráneo; semeja una gruta abandonada, primitiva y mental. Su articulación es mórbida, se compone de residuos, vestigios o remanentes. La sección supone tres cuartas partes del escenario. En la superficie, diáfana y serena, un pescador desvestido permanece de espaldas, impasible sobre el subsuelo. Tras de sí se sitúa un abrelatas. "Máquina de coser electro-sexual" llama la atención por su carácter icónico y por su ingenio y equilibrio compositivos. Las asociaciones recorren aquí con naturalidad y fluidez un simbolismo lírico de trazado cíclico. A esta labor se suman "Le Dimanche" y "Exacte sensibilité" de 1935, "La apisonadora y la rosa" de 1937, o "Les siphons", de 1938. El contenido erótico de los cuadros en este tiempo se intensifica, adoptando a menudo una estética del sensualismo cuyas características derivan en una vertiente propia. "L'Ouvre-Boîte" de 1936, "Le couple", "Homenaje al Greco" y "Deux couples "o" Femmes aux boîtez de sardines" de 1937 se plantean como un manierismo desmedido. "Mujer con cabeza de rosas" y "La Girafe en feu" de Dalí parecen haber inspirado "Personages Surréalistes" y "Madamme" de 1937 o "l'Hirondelle de l'eau" de 1938 en la misma línea.

La técnica de la decalcomanía permite a Domínguez producir impresiones de gouache carentes de contenido figurativo cuyas propiedades incitan al observador a la extracción subjetiva de motivos por pareidolia, propiciando al mismo tiempo el ejercicio de la asociación libre. La decalcomanía como técnica surrealista favorece además la intervención del azar como agente indicador de los designios, o el retorno al estado de transmisión libre de las esencias mediante la práctica del juego infantil. En ocasiones el pintor interviene sobre la decalcomanía ya impresa para revelar al espectador las formas por él halladas; son los casos de "decalcomanía interpretada" y "decalcomanía interpretada con abrelatas" de 1936. A partir de 1937 aparecen las primeras "decalcomanías del deseo", método consistente en el aprovechamiento de los paisajes concebidos por el inconsciente en la decalcomanía como escenografía para una nueva creación; en otros casos, se selecciona un cuerpo u objeto simbólico concreto para realizar la impresión. Con esta progresión, la decalcomanía se incorpora paulatinamente como un recurso técnico más en la elaboración de los cuadros de Domínguez. Algunos ejemplos de ello son "Decalcomanía con río y puente" y "La Fumeuse I" de 1937, el ya citado "Les siphons", o "Nus avec le profil de Breton" elaborado en 1938. Óscar Domínguez es entonces reconocido como el introductor de la técnica surrealista de la decalcomanía en el grupo parisino; también lo será como uno de los máximos exponentes del automatismo pictórico, procedimiento consistente en la realización de piezas mediante la ejecución irreflexiva, ininterrumpida e instintiva, contraria a la confección meditada o por fases. La destreza en el automatismo absoluto de Domínguez rendirá su expresión más prevalente y desarrollada durante la denominada etapa cósmica. Mientras tanto su posición entre los surrealistas se refuerza con la participación en colectivas como la "Internationale" de la Galerie des Beaux-Arts. El surrealismo no dejará de manifestarse en lo sucesivo, aún intrínsecamente, en toda la pintura del isleño. Algunos cuadros de un cierto surrealismo clásico se darán todavía en lo venidero; son los casos de "Los caracoles", en 1940, "Tête de taureau", de 1941, o "Portrait de Miss Ruth", de 1942.

Hacia 1938 Domínguez incursiona en la que será su más profunda implicación y decidida exploración en el arte del automatismo pictórico. "La vague", con su compleja y poderosa sugestión, se enmarca aún entre las obras del surrealismo de la narrativa y de los objetos, metafóricos y poéticos. El despliegue acaparador de trazos continuados, espontáneos y seriados entronca sin embargo con los próximos paisajes de 1938 y 1939, que conforman la colección de su etapa cósmica. La supeditación del proceso pictórico a la ejecución de pinceladas persistentes, prolongadas y desenvueltas, junto con la elección súbita y arbitraria de las tonalidades, conducen en estos años al pintor a la realización de escenarios impensados y reveladores. La naturaleza insólita y extraña de estas creaciones son interpretadas como panorámicas de otros mundos: Domínguez las titula comúnmente "Paysage cosmique" o "Composition cosmique". Guardan en común el flujo desatado de ráfagas y turbulencias, la concentración de ciclones o espirales, paredes y caídas abruptas, relieves quebrados, y formas similares a las de organismos primigenios o presuntamente geográficas en posibles simbiosis, de condición casi siempre ignota. En la mayoría de los casos los colores son fríos, oscuros y monótonos, las atmósferas inquietantes, inhóspitas y declinantes; se hace gravitar «"extraños sistemas estelares, e incluso evoca precipicios y abismos de astros muertos, en la transparencia de una luz congelada."» El concepto de geofísica, liberado, plantea alternativas confusas y contradictorias, «"arrastrando al espectador, vertiginosamente, a contemplar unas profundidades incomprensibles para el entendimiento humano."» Pertenecen a la etapa cósmica "Composition et squelette", "Les soucoupes volantes", el particularmente enigmático "Souvenir de l'avenir", y "Lancelot 28º 33"', donde se ha querido ver una alusión involuntaria a la ceguera de Victor Brauner provocada por el autor: «"No hay lugar para la confusión: la cuenca cavada en el centro, de la que se escurre una materia despedazada, es el ojo enucleado de Brauner"». "Le bois" y su despliegue obsesivo de aristas o trama de espinar anuncia en 1938 la estética próxima de la etapa de las redes.

A finales de 1937 se incorpora al grupo surrealista el arquitecto e ilustrador chileno Roberto Matta, quien abandona entonces la práctica de la arquitectura para centrarse en la pintura al óleo. Ese mismo año el oficial de la marina inglesa Gordon Onslow Ford marcha a París con la intención de progresar en su carrera artística. Onslow Ford no formaliza su ingreso en el colectivo surrealista hasta 1938, tiempo en el que el físico y matemático argentino Ernesto Sabato se persona en la capital francesa para emprender su labor como investigador en el Laboratorio Curie. Por su parte Óscar Domínguez atraviesa el momento creativo de la etapa cósmica. Sabato y Domínguez entablan en estos meses una pronta amistad. Matta y Onslow Ford simpatizan al tiempo, mostrando en común el interés por la representación efectiva de la cuarta dimensión en sus trabajos. Matta habría presentado sus dibujos a Breton, quien le animaría a proseguir con la experimentación en este sentido, que ya secundaba Onslow Ford; las premisas de estos estudios resultan entonces expuestas teóricamente por el artista americano en las redacciones "Inscape" y "Morfología sicológica", correspondientes al verano de 1938. Domínguez, con noticia de esta indagación, visita junto con Ernesto Sabato el estudio de Matta; sin embargo, en las pinturas allí presentes el físico «"no reconoció lo que los artistas consideraban entonces la cuarta dimensión".» Óscar Domínguez habría generado para este mismo propósito su método de las superficies litocrónicas, solidificación del tiempo o crónicas del tiempo petrificado. Este procedimiento le permitiría registrar los sucesos o el desenvolvimiento de uno o varios cuerpos escogidos al azar a partir de dos instantes dados trazando una superficie envolvente por todos sus puntos. Su desarrollo teórico es publicado en mayo de 1939, con coautoría de Sabato, en el artículo "Des tendances les plus récentes de la peinture surréaliste" de la revista "Minotaure", donde el conjunto de las innovaciones de Matta, Onslow Ford y Domínguez se analizan junto a las de otros creadores. Resultantes del método de las superficies litocrónicas son "Sans titre [Époque lithocronique]", "Personnages [Lithocronique]" o "L'estocade lithocronique", de 1939. Lienzos de la etapa cósmica como "Lancelot 28º 33"' son considerados a su vez productos de este sistema. Las observaciones en torno a las litocrónicas tendrán además su repercusión en los trazados de la etapa de las redes.

La etapa pictórica de las redes se desarrolla entre 1939 y 1940. Se conservan muy pocas obras de esté género. El conjunto de imágenes que las compone se caracteriza por el abarrotamiento de rectas conexas y punzantes, constituyente de un complejo de poliedros irregulares extenso a toda profundidad que usualmente acapara todo el cuadro. Su angulosidad y retraimiento hacia el primer término se oponen a la curvatura y permisividad de los óleos inmediatamente anteriores. Persisten la tonalidad fría, insistente y limitada, el planteamiento gélido y desapacible, la expresión inconsciente y obsesiva. Las estructuras de las redes transmiten compulsión y paranoia. A veces el cielo está rasgado por meteoros que cruzan dejando una estela amarilla; «"El efecto producido es el de un extraordinario dramatismo"». Las redes de Óscar Domínguez no carecen sin embargo de belleza. Las formaciones recuerdan a configuraciones minerales, vegetales y otros tipos de cristalización natural. Las consecuencias de este horror vacui, la opresión o asfixia del volumen o la angustia de la dimensión propuesta traen habitualmente a la memoria los campos de concentración y, en términos generales, las secuelas ocasionadas por la Segunda Guerra Mundial. Conviene señalar «"que Domínguez, poco antes del comienzo de la Segunda Guerra Mundial, fue internado en un campo de concentración."» Pertenecen a esta etapa "Paisaje de Redes" o "Paysage abyssal", ambos de 1939, "La solitude" y "Cuadro cósmico IV", de 1940. Las redes remitirán aún ocasionalmente en épocas posteriores, por ejemplo con "Cálculo" o en "El quinqué y la paloma", de 1942, y con "Les cocottes en papier" y "Teléfono y revólver", de 1943.

Entre 1940 y 1942 la producción pictórica del autor es casi inexistente. A partir de 1942 Domínguez torna drásticamente su trayectoria con una proyección plástica muy alejada de lo antecesor. Sensiblemente, Picasso es ahora el principal referente; concretamente su periodo surrealista, con la serie de "bañistas" y "figures au bord de la mer", los retratos "Femme au fauteuil rouge" o "Jeune fille jetant un rocher". Parcialmente aparece también el uso libre y lúdico de los planos del cubismo sintético. El principal motivo aquí es la mujer, sujeta a la morfología, objetualizada y comprendida anatómicamente. El cuerpo femenino se transfigura por una geometría más próxima a lo elemental, con curvas, torsiones o volúmenes simples, limpios y definidos. Las partes se desestiman o exageran en deformaciones acentuadas, alzándose hacia un nuevo sentido del equilibrio clásico en la desproporción. La intuición de la geodesia se plantea a menudo como constituyente compositivo o decorativo. Se percibe en estos contornos la magnificación de las zonas erógenas, incluidos pies y manos, reubicados o multiplicados. Las mujeres desmontables, dispuestas a modo de estudios, enfatizan las nociones del sensualismo y la monstruosidad con un atractivo turbador. Sus distintas piezas parecen efectivamente intercambiables; así, «"el más voluptuoso erotismo se enreda con un sentimiento dramático de la existencia".» El carácter escultórico de estos diseños coincide con el ejercicio de la talla en madera seriada por el artista, de igual temática. Domínguez estaría manifestando además en este intervalo síntomas evidentes de acromegalia. Corresponden a esta etapa "Mujer", de 1941, "La main passe II", "Femme sur divan", "Celle que l'on voyait" y "Crepúsculo", de 1942.

En 1943 Domínguez debuta con su exposición individual en la Galerie Louis Carré como artista independiente en París. El muestrario en exhibición, claramente diferenciado en su estilo respecto a etapas anteriores, supone una nueva trama en el ciclo de búsquedas hacia un lenguaje propio recorrido por el pintor. Recursos característicos de otros periodos contribuyen a delinear estas nuevas composiciones. El paso efectuado por otras influencias puede intuirse aún en estas telas, pero es Giorgio de Chirico quien predomina, inmediatamente reconocible en términos generales, o por vínculos aparentemente directos en este caso con cuadros como "The scholar's playthings", "The uncertainty of the poet", "Le secret du château", "Interieur métaphysique avec profil de statue" y, a grandes rasgos, la serie de los "Intérieurs métaphysiques" más tempranos. Óscar Domínguez se adentra por tanto en la pintura metafísica, anticipada durante la etapa de las mujeres desmontables, aunque su mayor contribución al género sea quizás la introducción del complejo poliédrico y de las redes, opuestas en esta ocasión al brote desbocado de 1939, mesuradas, ajenas y estables. En los escenarios de la etapa metafísica no se reconoce el espacio de lo cotidiano, pero tampoco el del sueño. El sentido poético y la insinuación de la metáfora reaparecen en el primer plano; no tanto por la relación dinámica y transitoria entre los objetos simbólicos como por el efecto de tensión generado por la disposición estática del conjunto y el sentimiento de accesibilidad. No hay más personificaciones en estas propuestas que la del solo espectador, al que se integra con la observación de un orden inclinado al impulso comprometedor. La inclusión provocada por cada planteamiento incrementa la impresión de soledad o de tensión inherente, en consonancia con la de una toma de conciencia forzosa en cuanto a la realidad presente. Destacan en esta etapa "Nature morte au prisme", "Le plus clair du temps I", "El mapamundi", "La fin du voyage II" y "Bodegón" o "Composición con frutero y escorpión".

Esta etapa, contenida entre los años 1944 y 1946, puede ser dividida a su vez en dos periodos: un primer periodo comprendido entre 1944 y 1946 vinculado a París y el incremento de muestras, y un segundo periodo determinado esencialmente por la relación profesional y personal de Óscar Domínguez con la escena de las artes plásticas en Checoslovaquia, entre 1946 y 1948. Domínguez se ha desembarazado del surrealismo canónico, asume ostensiblemente el modo de Picasso, concretamente aquel que desde finales de los años 20 se dirige estilísticamente hacia su consagración en el Guernica. La estética asociada con mayor familiaridad a Picasso, asentada en la primera mitad de los años 30, constituye una figuración «"con contornos y envoltura lineal, pero sin estar vinculada a la forma de la figura natural "[...]" La disociación cubista, la figuración y el simbolismo infantil son los tres fundamentos sobre los que descansa el lenguaje formal del "Estilo Picasso""»; las formas plásticas son la razón de ser de sí mismas, sujetas tan sólo a sus propios requerimientos, y como tales expresan lo que son. El color, la sustancia, el dibujo o la contextura entrelazada de líneas y planos ocupan el lugar de la representación, funcionando en su lugar. Se advierte un optimismo y una vitalidad inusual en el tinerfeño; desaparecen los contenidos tortuosos. Los contornos son serenos y firmes, los cuerpos animados, los tonos apacibles, gratos y luminosos. Las mujeres de la etapa picassiana de Domínguez ya no ocultan su rostro; protagonizan la expresión de un frescor romántico. Su espíritu es proactivo, lozano y sensible; su actitud íntima o resuelta. "Mujer", de 1942, "Fémme á l'ecritoire", "Tres mujeres" o "Silent listener (La Venus de L'Ebre)" de 1943 muestran esta transición en el retrato femenino. Les siguen "Petite fille sautant á corde" y "Femme peignant" de 1945, "La costurera", "La douce chatte" y la serie de "mujeres en balcón" de 1946, "Mujeres con pájaro y pez" de 1947 o "Mujeres en el balcón" de 1948, entre otros. Motivos significativos en estos años son también el quinqué, como en "Naturaleza muerta con lámpara", el gato, en "El gato azul", y el revólver, en "La liberación de España", todos ellos en 1947. 

Al seguimiento estético se une el temático. La figura del Minotauro emerge en la imaginería de Picasso con el óleo "Minotauro y mujer dormida" de 1927. En junio de 1933 el mismo motivo ilustra la portada del primer número de la revista "Minotaure", parcialmente conducida por los surrealistas. El minotauro deviene prontamente en un tratamiento liberador o personificación del encauce instintivo y artístico para el malagueño, que asume su idea hasta la propia identificación. Para los surrealistas, «"el minotauro era equiparado a sus propias aspiraciones, la violencia, la rebelión absoluta, la libertad sin freno, tal y como Freud había traspasado el mito al insconciente." [...] "A Picasso le interesa el mito, pero más bien le interesa los aspectos del mito que pueden compararse con la vida cotidiana, con el ser humano."» Picasso reconoce la condición trágica del Minotauro, consciente de su condición de monstruo o semidios, incapaz de realizarse sin eximir por completo la ferocidad de sus apetencias o el hambre de embestida, en constante deseo, violentamente creativo o voluptuoso, sensible a la belleza, carnal, devorador de vírgenes. El Minotauro permite a Picasso comprenderse en la encrucijada personal y profesional, abrirse paso u orientarse. Óscar Domínguez recoge para sí esta mitología en términos muy similares tras su encuentro con Maud Bonneaud, dado en 1943. La pareja contrae matrimonio en 1945 y viven juntos aproximadamente cinco años; durante toda esta relación, en las cartas de amor que Óscar Domínguez escribe a Maud en sus ausencias, el pintor se identifica a sí mismo como minotauro. El asunto del Asterión se hallará por esta época más presente en la documentación de carácter privada, en ilustraciones, en adornos o en relieves que en la producción estrictamente pictórica, con casos inusuales como "Composición con toro (y minotauro)" de 1949. La efigie de la cabeza de toro a la manera de la máscara africana se transforma en una suerte de sello o firma personal, que en la praxis vale tanto para la bestia como para el antropomorfo, y resulta ya visible en la serie de "toros moribundos" de 1944. A diferencia de Picasso, la figura del Minotauro seguirá rindiendo imágenes en las facturas de Domínguez hasta el final de sus días, multiplicándose de hecho durante la década de los 50. Con todo, el minotauro conjurado por Domínguez gozaría de particularidades diferenciadas:

Ambos pintores se ven aliados complementariamente en la exteriorización de su españolidad. Picasso habría perpetuado la tradición del bodegón español de los siglos XVII y XVIII, hecho perceptible en la elección deliberada de objetos, colores, u otras cualidades: «"el contraste extremo de luz y sombra "[...]" objetos inanimados de una individualidad obstinada "[...]" dura materialidad "[...]" volumen básico."» La tauromaquia surge en sus obras iniciales, durante el último lustro del siglo XIX, y aún será frecuente hasta emprender el cubismo. Los toros reaparecen en 1917 principalmente en papel, y con notable asiduidad en 1920, en contienda con el caballo. El torero recostado sobre el dorso del toro, figurante en "Minotauromaquia" de 1935, emerge en la iconografía picassiana en 1922, y es con la recurrencia del Minotauro en los años 30 que el tándem del toro y el caballo alcanza su mayor auge. El Minotauro de Picasso seguirá estando presente todavía en los gouaches de 1936 para desparecer, como motivo iconográfico, en "La fin d'un monstre" de 1937. El bodegón en Óscar Domínguez, de por sí relegado en sus etapas anteriores y ahora recuperado, se debe al de Picasso: lo frecuentan el gallo, como en "Galleto", la guitarra, en "Guitarra española" de 1947, y el constante revólver, con la culata igualmente encañonada. Las tauromaquias del pintor canario se prodigan aproximadamente a partir de 1948 hasta alcanzar junto con el toro el estatus de tema predilecto en 1951, para mantenerse todavía con regularidad hasta su etapa final. Las tauromaquias de Domínguez son siempre festivas; más cómicas que drámáticas en su etapa picassiana. Las escenas de esta etapa, de tirantez expectante, se corresponden con el instante previo al de la lidia o con la estocada consumada. El torero porta a menudo la misma cabeza que el toro; el toro mismo parecer ser comprendido por el autor con conmiseración. "Tauromaquia" y "Toro arquero" de 1948 son dos ejemplos. La producción picassiana es a su vez una pugna esforzada en la conciliación de los valores tradicionales de la cultura clásica, identificados esencialmente con la antigüedad mediterránea, y la intencionalidad rupturista y desarticuladora de las vanguardias del s. XX, que adopta patrones discordantes como el del arte africano. Se ha propuesto que la obra realizada por Domínguez en este tiempo es una manifestación genuina del magicismo surrealista, próxima a Paul Éluard y a Valentine Penrose; la metamorfosis de la mujer «"que remiten al mito de la diosa-madre, v.g., la mujer-hoja, constituyen imágenes medulares tanto de la poesía de Valentine como de la pintura de Domínguez."» Otra referencia clave para la interpretación de su mitología se encontraría en el texto "Die Hochzeit" de Johannes Valentinus Andreae, relativo a la figura de Christian Rosenkreuz, fundador de la Rosacruz. "Pastor negro" de 1946, "Fecundidad" de 1947 o "Chevalier" de 1949 parecen recurrir al medievo. La vidente hace acto de presencia en dos versiones de "La voyante" de 1944 y en "Deux voyantes" de 1945. La ilustración bidimensional de carros direccionados como "El caballo de Troya" de 1947, "Estudio II" de 1948, o "Composición con fondo azul" de 1949, o de siluetas femeninas orientalizadas o africanizadas como "Dos figuras" de 1948, junto con las tauromaquias del momento, inspiran el ideal de una actualización de las artes decorativas prototípicas del arte antiguo.

En febrero de 1946 se celebra en la sala Mánes de Praga la colectiva "Umění replublikánského Španělska. Španělští umělci pařížské školy." Los españoles causan un hondo impacto; la exposición es reproducida en abril en Brno, capital de Moravia. El que más atrae entre todos es Óscar Domínguez, «"no sólo por la cantidad de cuadros que lo representaban, sino también por la intesidad de los mismos. Había conseguido un efecto coherente y original."» En noviembre acude por segunda vez a Praga para "Tři Španělé. Výstava obrazů a kreseb", en la Alšova síň Umělecké besedy, con Julio González e Ismael González de la Serna. La especial acogida de Domínguez en Checoslovaquia conlleva la organización de varias individuales: en diciembre de 1947 en Olomouc por la Ústrední osvětová rada Hlavního města Olomouce a Skupina výtvarníků v Olomouci, con 20 obras; en diciembre de 1948 en la SVU Mánes, con 44 obras, y en enero de 1949 por la Výtvarný odbor Umeleckej besedy slovenskej de Bratislava con 57 obras expuestas. Desde la primera exhibición de los españoles de la Escuela de París en la capital, el pintor se trasladará a Checoslovaquia al menos en tres ocasiones, residiendo por temporadas en Praga, Olomouc y Bratislava. Sus cuadros habrían llamado la atención de la escena regional durante los años 30, mientras que el contacto con los surrealistas checos se remontaba a su relación con Jindřich Štyrský, con Toyen o con Vítězslav Nezval en París. Este último, director del impreso "Surrealismus" de 1936, había declarado ya con anterioridad «"que "escribía infinitamente excitado" sobre la decalcomanía de Domínguez."» En Olomouc es acogido por el escultor Jaromír Šolc, que intermedia en su individual de 1947 y pone a su disposición su propio estudio. La estancia de Domínguez en la ciudad, una de las mejores documentadas a nivel gráfico de toda la andadura checoslovaca, es registrada fotográficamente por el pintor y escenógrafo Oldřich Šimáček. El crítico de arte Miroslav Míčko o el poeta y pintor Jiří Kolář atenderán a estos eventos. Los poetas Stanislav Křupka y Rudolf Fabry contribuyen a la redacción de los catálogos para Olomouc y Bratislava, respectivamente. Numerosas elaboraciones surgirán a raíz de la celebración de estos actos y la interacción con el entorno; parte de estas piezas permanecerán en el país durante largo tiempo como adquisiciones privadas, regalos personales o para la exposición pública en entidades institucionales. Es el caso del "Retrato de Jaroslav Solç", de 1948.

Se trata de una de las etapas de mayor personalidad estilística del artista canario. Domínguez ha hecho acopio con su progresión de una amplia provisión de recursos técnicos y lingüísticos, que imprime con simultaneidad de acuerdo al despliegue de asuntos visiblemente objetivados a través de una iconografía enriquecida y privada, ligada a su propio devenir. La influencia de Picasso se distancia, en especial con la cohesión armónica de los planos. La idea de composición fundamenta estos trabajos de 1949 a 1953. El propósito embellecedor o funcional, de tipo complemento, el carácter de ilustración o la integridad de los registros unificados conectan con la modalidad del diseño gráfico, incidental hasta la fecha. La solución para la profundidad es básica en extremo. Una sección dentro de esta colección se enmarca en el dibujo sobre tarima, esbozada por simples diagonales o mediante la gradación de la tonalidad dirigida hacia el fondo, como en "Minotauro", "Le bateau", "Toro y caballo" o "Amazona", de 1950. Buena parte, en cambio, carece de profundidad preceptiva o deliberada; las composiciones se construyen a partir de un plano primero y totalizador según la vertical del soporte. El juego de líneas, el peso de las superficies acotadas y monocromas, y, ante todo, el conocimiento previo en el observador de la naturaleza de los objetos figurados posibilitan la percepción de profundidades subjetivas y variables dentro del plano único. Cierto virtuosismo en la estructuración es de hecho apreciable en estas piezas. Este método es combinado usualmente con la técnica de triple trazo, consistente en el perfilado fino a tinta china y la coloración delimitada por márgenes diferenciales. Se conciben así "Pájaros", "El teléfono", "La table noire" o "Le siphon" de 1950, "El pirata" o el "Florero rojo" de 1951, y "Le revolver" de 1952. Los temas más frecuentes son el "atelier" y la tauromaquia. A pesar de todo, el grueso de las obras de esta etapa se desprende de la concepción rigurosa del plano único o del triple trazo, proporcionando una colección ecléctica y heterogéna: "Femmes aux boïtes de sardines" y "Papillon" de 1949, "Taureu et femme" y "Le fromage de Hollande" de 1950, "L'acrobate", "La table" y la serie "L'atelier" de 1952, "Ritmo sobre una sonata de Chopin" de 1953, o los distintos "fruteros come-frutas" de estos años muestran una disposición más libre. Los bocetos de carros para "Rapto de Europa" o "La Diosa Hélice" de 1953 o "Teléfono y revolver" de 1949 aparentan pertenecer a etapas anteriores, mientras que "Atelier" de 1952 o "Decalcomanie interpretée" de 1953 se ajustan más a su etapa final. Algunos lienzos cobran una difícil clasificación, como "Batalla" o "la vole lion", de 1950, y el desconsolador "Autoportrait" de 1949.

La etapa final de Óscar Domínguez se sitúa entre los años 1953 y 1957. La pintura localizada en este último tramo radia a grandes rasgos una predisposición indeterminada, el presentimiento de una experimentación insatisfactoria, o de impronta desnaturalizada, sin dejar por ello de brindar telas de un logro completo. La conclusión en estos momentos aparenta ser la de la aproximación al arte abstracto por un pintor contrario en sus instintos a la abstracción. Retornan los tonos lúgubres, crepusculares, cerrados, de marismas o terrosos. Las proposiciones son oníricas y rigurosas, a veces enigmáticas. La atmósfera decadente supone paradójicamente uno de los valores más atractivos de estos trabajos. Regresa la decalcomanía, manejada con una habilidad y un planeamiento muy superior al de cualquiera de las etapas anteriores. La técnica es además coordinada con la práctica del automatismo pictórico, también recurrente. Los motivos más frecuentes varían desde los paisajes urbanos de "La maison hantée" de 1955, o "Ville" de 1957, los paisajes de inspiración clásica como "Templo" de 1954 o "Delphes" de 1957, los laberintos de "Paisaje de Hyères" de 1956 o "Mon modèle" de 1957, la pura abstracción de las "compositions", la abstracción paisajística de "Mars" en 1954, "Apocalipsis" de 1956 y "La patience" de 1957, o la abstracción del figurativismo lírico propio de "Cabeza de toro y paisaje" de 1954 o "Pájaro abstracto" de 1957. Una tentativa de coloración viva y de integración entre las tendencias en boga se detecta en las batallas de "La cavalerie double", "Batalla" y "Batalla sobre tema de Uccello", de 1955. "Le clown" y "Liberté" concluirían en 1957, con sendas propuestas de sentimientos encontrados y lecturas abiertas, un legado pictórico caracterizado por la emoción, la contradicción, el deseo, la búsqueda y la autenticidad.

Se conocen aproximadamente 450 trabajos catalogados de Óscar Domínguez en el ámbito de la pintura y de la ilustración, de entre los cuales al menos 300 consistirían en obras al óleo principalmente sobre lienzo o tabla; se estima a su vez que de la producción al óleo realizada por el tinerfeño 120 cuadros corresponderían al periodo inscrito entre los años 1946 y 1949. Entre las instituciones o centros museísticos que han albergado las más numerosas colecciones de pinturas de Domínguez se encuentran el Museo Nacional Centro de Arte Reina Sofía, Tenerife Espacio de las Artes con el fondo propiedad del Cabildo de Tenerife, la Slovenská národná galéria de Bratislava, la Galerie výtvarného umění v Ostravě, en Ostrava, y el Centro Atlántico de Arte Moderno de Las Palmas de Gran Canaria; de entre las colecciones dadas a exposición en galerías o particulares destacan por el número de piezas reunidas del autor la colección Stéphane Janssen en Arizona o la perteneciente a la Galería Guillermo de Osma en Madrid. Hacia mayo de 2008 se estimaría que el precio monetario de las obras de Óscar Domínguez se habría triplicado desde el 2005. La casa de subastas Christie's adjudica tras puja en la Subasta de Arte Impresionista y Moderno de 2006 en Londres "Personajes surrealistas" por £904.000; "Mujeres", de 1942, en la anual Subasta de Arte Español de Madrid por €908.000 en octubre del 2008; en su sede de Londres en 2014 el "Retrato de Roma" por £902.500. En 2008, en la sede de Christie's en Londres, la "Máquina de coser electro-sexual" se transfiere tras puja por valor de €1.890.000; el comprador, diputado autonómico en el Parlamento de Canarias y en consecuente régimen de aforamiento, tras causa abierta en el Ministerio Fiscal por posible delito contra la Hacienda Pública, es investigado por el Tribunal Superior de Justicia de Canarias en 2014, no hallándose justificado el origen de los fondos para la realización de la compraventa. La pintura de Óscar Domínguez formará parte del material incautado con motivo del proceso promovido por la Fiscalía contra la Corrupción y la Criminalidad Organizada, en marzo de 2006, durante la intervención en el entramado de asociaciones responsables de numerosas actividades delictivas designado como caso Malaya. "Chevalier", tasado en €160.000, es localizado entre las obras de artes depositadas en Suiza y destinadas al blanqueamiento de dinero a partir de la actuación efectuada por la Guardia Civil para la desarticulación de la trama de corrupción política y empresarial demarcada por la Operación Púnica.

El año 1933 es la data de la primera contribución de Domínguez para la ilustración en tinta de una obra literaria: el poemario "Romanticismo y cuenta nueva" de Emeterio Gutiérrez Albelo, en su primera edición de "Gaceta de Arte", decorada su cubierta con una viñeta de tres tintas planas y sintéticas, azul, roja y gris. Ostenta el relato "Crimen", de Agustín Espinosa, un diseño exclusivo de Domínguez en portada para su primera edición de 1934 a través del mismo equipo editorial, de tres tintas planas, azul, negro y gris, en posible alusión simbólica al destacado pasaje "La mano muerta". En la línea de la serie de cuadernos "Repères" de 1935, en los que se asocian planteamientos gráficos y textos poéticos a la par, los "Cahiers GLM" de la empresa parisina Guy Lévis Mano conllevan la impresión en 1938 del número especial dedicado al sueño impulsado por André Breton "Trajectoire du rêve", de marzo de 1938, con la tinta de Domínguez "Souvenir de l'avenir" vinculada a las líneas de Georges Hugnet "Les revenants futurs". GLM asume para el mismo año la propuesta de las "Œuvres complètes" de Lautréamont, que finaliza en agosto; Domínguez ilustra el tercero de "Los Cantos de Maldoror", compaginado entre una docena de artistas plásticos involucrados en el proyecto. Integrado en el colectivo cultural próximo a la resistencia francesa "La Main à Plume", el isleño se inicia en 1941 en el seno de la actividad editorial del grupo con su idea de frontispicio para el libro "Noués comme une cravate" de Christian Dotremont, consistente en un dibujo reversible. "Transfusion du verbe", con textos y poemas de autores varios, presenta, también con impresión de "La Main à Plume" de 1941, un diseño próximo a las «mujeres desmontables». El tratamiento gráfico del poema erótico de versos libres de la compañera en el colectivo Laurence Iché, "Au fil du vent", de 16 páginas y tirada de 300 ejemplares, resulta completado por el isleño en 1942 con 5 ilustraciones, frontispicio y viñeta para la portada; esta última servirá además como imagen de las restantes portadas para la colección de 12 cuadernillos llevados adelante por los promotores de "Les Pages Libres de la Main à Plume", con la sola excepción de la última cubierta, de Pablo Picasso. 

Las ilustraciones para el texto "Le grand ordinaire" de André Thirion habrían de merecer especial atención en tanto pueden considerarse el primer brote de un ramal autónomo en el arte de Domínguez, a medio camino entre las «mujeres desmontables» y su particular esquematismo de inspiración picassiana. Se establece con este encargo el precedente de la exteriorización del sexo veraz, retratado en plena práctica, sustentado por una actitud sin ambages, desafiante, soez, resuelta, hostil al decoro, a la moralina u otras formas de la hipocresía. Las aportaciones de Domínguez al círculo contestatario de "La Main à Plume" y sus allegados se adecuarían con progresiva pertinencia en el contexto de la Francia de Vichy. Thirion, que en la agitación de la entreguerra se habría sentido muy inferior a André Breton o Louis Aragon, decidiría, «"al sentirse mucho más en su elemento cuando se trata de analizar lo social desde un punto de vista marxista, posponer toda ambición o trabajo literarios "[...]" La nueva situación histórica invita a utilizar "del mejor modo las aproximaciones y las licencias del lenguaje poético. El lema Trabajo, Familia, Patria era una buena fuente de inspiración""». El escrito no se valdrá de una clasificación literaria de género específica; se publica en la primavera de 1943, con fecha falsa de 1934, sin relación informativa de autoría o editorial. La primera tirada, de menos de 150 ejemplares, contiene 6 ilustraciones de Domínguez. Una edición de autor de 5 ejemplares presenta los añadidos del aguafuerte "León con sifón", 2 aguafuertes más sobre un mismo estado, y un dibujo a tinta china del Mariscal Pétain portando un voluminoso bigote, espuela, y condecoraciones sobre el torso poligonal y el pene, más esmeradamente configurado. La salida de Paul Éluard de "La Main à Plume" en el meridiano de 1943 será secundada por la de Óscar Domínguez. Georges Hugnet, espíritu afín al tinerfeño, se separa también del movimiento; prepara entonces una edición propia con la ayuda del pintor, que acentúa y extiende a su vez la explicitud sexual invertida al tiempo en "Le grand ordinaire". El resultado, "Le feu au cul", provocación nihilista cercana al dadaísmo burlón y perverso, es una obra ya no erótica «"sino pornográfica, donde se exaltan una serie de juegos sexuales absolutamente depravados que llegan incluso a lo escatológico. En contraposición a esta crudeza, las ilustraciones de Domínguez son muy elegantes y armoniosas, y el texto de Hugnet posee una calidad lírica de primer orden"». El número de aportes de Domínguez para esta obra de 1943 es de 17. Descolla en lo venidero la labor para "Vrille, La peinture et la littérature libres" de 1945, de autores varios, con diseño de portada del artista.

Se considera al aguarfuerte preparado para la unificación con el texto de Georges Hugnet "La hampe de l'imaginaire" como contenido del 12º cuaderno de la serie "Repères" de la editorial GLM el primer grabado profesional de Domínguez; también la primera ilustración del artista para un libro en París. El año figurante en esta lámina es 1935. Los 8 aguafuertes destinados al cuaderno "Domaine" del poeta venezolano Robert Ganzo, de complejidad ligera, astucia en el diseño y sensible exquisitez, conducen a la consideración del potencial alcance de Óscar Domínguez en su faceta como ilustrador. Ideas de la etapa de las redes, las «mujeres desmontables» y del esquematismo figurativo de la etapa picassiana confluyen aquí en geometrías vivas y en humanidad lírica con límpida naturalidad. Los 74 primeros ejemplares de "Domaine" se imprimirán en los talleres Duran de París en el mes de junio de 1942. Para el primer número de los cuadernos de "Les Pages Libres de la Main à Plume", constituido con el texto "Aux absents qui n'ont pas toujours" de Noël Arnaud, Domínguez cede el aguafuerte "Figura femenina" de 1942, que reproducirá independientemente en el mismo año con coloración en gouache. "Sombre est noir" de Amy Bakaloff, en Editions Minuit, de 1945, contiene un aguafuerte reconocible en su plena identidad con el ánima de la etapa picassiana del autor, además de dos dibujos a tinta firmados y fechados.

El valor sobresaliente del hito que en múltiples sentidos supone la ilustración de "Poésie et vérité" de Paul Éluard en 1947 respecto a la figura de Óscar Domínguez puede inferirse desde varios frentes: esta colaboración en particular se comprende en un contexto de índole definitoria para el fenómeno de "La Main à Plume", para la génesis y devenir de un poemario considerado capital en el ámbito de la contracultura francesa durante la ocupación, luego preeminente en el estudio de la literatura surrealista en lengua francesa, la literatura de la resistencia, la Francia Libre y la liberación, y para la biografía al fin de Paul Éluard y el conocimiento de su relación profesional y personal con el artista canario. En mayo de 1942, Noël Arnaud toma la responsabilidad de la primera edición de "Poésie et vérité" con "La Main à Plume" y ya en los siguientes meses el texto de Éluard tiene el reclamo dentro y fuera del país de avances aliados, con enérgico interés por sus versos: 

Óscar Domínguez es emplazado por Éluard para la ilustración de la 8ª edición del poemario, que maquetará Ed. Librairie Les Nourritures Terrestres en 1947. Se trata por otra parte de la obra literaria con mayor número de aportes exclusivos del pintor. La 8ª edición del poemario, de 17 composiciones, no incorporará más ampliaciones en relación al original que el tratamiento gráfico de Domínguez: 32 aguafuertes y una litrografía a color ideada para el frontispicio. La tirada asciende a 221 ejemplares, en venta a comienzos de 1948. El libro, tanto en lo textual como en lo iconográfico, habría de ser entendido como una unidad; el artista plástico «"enseguida comprendió la profunda carga humanista, repleta de matices y aristas, que tenía el conjunto de poemas reunidos por Éluard. Para su plasmación se sirvió de algunos de los motivos sobre los que su obra había venido reflexionando en los últimos años"». Enmarca a la primera pieza "Liberté" los símbolos frecuentados del quinqué, la mujer con antorcha, el pájaro, el arco y la flecha. Un ejemplar personal de Domínguez dedicado a Maud Bonneaud tras su separación se conserva con la numeración 213; el volumen incluye un poema inédito de Éluard manuscrito en la portadilla, la dedicatoria del pintor y 30 dibujos originales a tinta china en los reversos de las hojas. La práctica del aguafuerte por Óscar Domínguez se reducirá drásticamente tras los estampados de 1947 hasta el final de su trayectoria.

En su nº 17 de julio de 1933 "Gaceta de Arte" anuncia la preparación de una monografía desde su editorial dedicada al artista plástico alemán Willi Baumeister. Eduardo Westerdahl, director del proyecto, busca conocer la impresión de Baumeister sobre los diseños para la cubierta y contracubierta seleccionados en una carta del 26 de septiembre de 1934. Constituyen la ilustración de las tapas del estudio "Willi Baumeister" dos trabajos de la decalcomanía de Domínguez caracterizados por su inusual sobriedad o aridez, de simplicidad minuciosa. El libro es finalmente publicado en 1934 con los dos ejercicios de decalcomanía sin intervenir, atribuidos a la práctica temprana de esta técnica por el autor. La falta de suscriptores requeridos por la casa Guy Lévis Mano para la publicación "Grisou", consistente en la reproducción en fototipa de 16 decalcomanías automáticas de interpretación premeditada, elaboradas por Óscar Domínguez y Marcel Jean, con prefacio de André Breton, deja el material gráfico ya completado sin edición. El boletín de suscripción precedente anunciaría la aparición de la obra para octubre de 1937, ideada para su distribución con tirada de 25 ejemplares acompañados por una decalcomanía original. Se conserva la serie manufacturada, formada con plantillas para la representación de una ventana o un león, lo que hace posible una primera edición con la recuperación del proyecto por Jean-Luc Mercié en 1990. La cancelación de "Grisou" frustrará la mayor colección de decalcomanías surrealistas preparadas para su difusión impresa planeada durante la vigencia del movimiento vanguardista francés. Diversos historiadores del surrealismo coincidirían en señalar «"que Domínguez no supo sacar partido de su descubrimiento, abandonándolo muy pronto por otros estilos."» El autor acudirá nuevamente al uso de la decalcomanía de interpretación premeditada para ilustrar la cubierta de "Anthologie de l'humor noir" de Breton en su edición de lujo; sirve de modelo para la imagen el dibujo a tinta del tinerfeño "Révolver et téléphone", de 1940. La censura afectará a la distribución de los ejemplares para el mercado, retenidos hasta 1945, si bien las variadas decalcomanías para tapa se sitúan en 1946.

La ilustración para revistas en el bloque de la producción del artista se muestra eventual, aunque se opte por su recurrencia a lo largo de sus distintos periodos, tempranos o tardíos. Entre los años 1933 y 1934 Domínguez destina al menos 2 dibujos en tinta para "Gaceta de Arte". En junio de 1935 tiene parte en el tratamiento gráfico del boletín de suscripción bajo la edición de GLM para el "Cycle systématique de conférences sur les plus récentes positions du surréalisme" con Hans Arp, Salvador Dalí, Marcel Duchamp, Max Ernst, Alberto Giacometti, Valentine Hugo, Marcel Jean, Yves Tanguy y Man Ray, profundizando en los contornos adelantados para la "Gaceta". "La conquête du monde par l'image" de "La Main à Plume" en 1942 incluye el nombre de Óscar Domínguez en la plana de sus colaboradores, así como el aporte teórico del pintor sobre la «"Pétrification du temps"», ligada a su etapa litocrónica. Para la revista parisina "Le Potomak", de dos únicas entregas y periodicidad anual, cede Domínguez un aguafuerte y un poema, completados entre 1948 y 1949. En su número especial 73-74 de 1951, "La Nef", por Éditions du Sagittaire, se valdrá de la composición "Figura musical", en tinta y gouache sobre papel, para la maquetación de su portada.

La posibilidad excepcional de la autoedición de catálogo para exposición queda patente en la preparación con Ginés Parra de los aproximadamente 400 ejemplares para la muestra doble de la Galerie Breteau de París de 1948, con 5 dibujos originales de sendos artistas. El catálogo de la exposición "Óscar Domínguez", París-Zürich de 1951, por la Galerie Georges Moos ejemplariza la adición de originales a los folletos generados con la exhibición de arte. El trabajo en cartelería de Domínguez se remontaría al menos hasta 1933, tiempo en que se emplea como publicista; las escasas unidades registradas de este ciclo profesional formarán parte del depósito del Museo de Historia de Tenerife. Los planos acreditados informarían de una predisposición en el autor al dominio estructural y la disposición dinámica, perceptible aún en los lienzos del automatismo pictórico y rentabilizada profusamente con la pericia organizativa de la etapa del triple trazo. Se deberán a Óscar Domínguez carteles con reproducciones de dos ilustraciones propias correspondientes a los poemarios de Paul Éluard "Poésie et vérité" y "Voir", destinados a su gestión por el ayuntamiento de la localidad natal de poeta, Saint-Denis, el anuncio en litografía para su individual en la Galerie de France y la litografía para la exhibición de noviembre de 1957 en la Galerie Rive Gauche. Se recogen además entre el legado de Domínguez múltiples tarjetas de promoción profesional, de envío postal o ideadas para la celebración de efemérides como el fin de año.

El dibujo a tinta o lápiz sobre papel se sucede de manera regular en el itinerario creativo de Domínguez, si bien este género de producción raras veces se destinaría a galería. Las obras en este formato adolecen de una amplia y reiterada dispersión; el simple hábito de Domínguez de regalar o entregar libremente sus ilustraciones, incluidas a menudo como motivos decorativos en sus cartas, dificultan su localización y catalogación. Son tintas conocidas del autor "Figuras mitológicas" de 1938, "La douce chatte I", coloreada con gouache, de 1941, "Les 4 sphères obeissantes" de 1943, "La terre n'est pas une vallée de larmes" de 1945, "Cuatro variaciones de minotauro" de 1948, "Les menines" de 1950, "Taureau assis" de 1951 o "Revólveres" de 1956. Se mantienen de las realizaciones a grafito o carboncillo "Souvenir d'une promenade en automobile" de 1935, "Bougie-arc" de 1936, "Edifice" de 1937, "Femme nue" de 1942, "Retrato" de 1947, "Les trois assiettes" de 1953, o "Baiser Lintain" de 1957. A pesar de la posterior separación de sus unidades, los 14 dibujos de tinta sobre papel dedicados en portada «"A Lucio y Jeannette" [...] "París, 1950."» dejan constancia de la noción en Domínguez del álbum como variable de interés entre las facetas de la industria gráfica. Prueba de participaciones en la elaboración de "cadáveres exquisitos" son los dibujos depositados en colección "Cadavre exquis" de 1935 con Remedios Varo, "Cadavre exquis" de 1936 con Esteban Francés, Marcel Jean y Gerardo Lizárraga, "Cadavre exquis" de 1937, con Georges Hugnet, o los numerados "Cadavre exquis I", con Sophie Taeuber-Arp y Jean Arp, y "Cadavre exquis II" con Raoul Hausmann, Sophie y Jean Arp y Marcel Jean, de 1937, a los que se les asocia la técnica del fotocollage.

Algunos casos especialmente relevantes de ilustración colaborativa se darán en 1940 con la residencia provisional de artistas del surrealismo parisino en Marsella, en vistas a la obtención de amparo en territorio francés libre o acceso a una salida portuaria hacia el extranjero, motivados por el desarrollo de la Batalla de Francia. Los surrealistas paliarán su suspensión en la villa de Air-Bel elaborando un álbum viñeteado de "dessin collectif", con lápices de colores, tinta y collage sobre papel, además de un juego de cartas rebautizado como "Jeu de Marseille" o "Juego de Marsella". Conformado entre finales de 1940 y principios de 1941, "Jue de Marseille" funcionará con una baraja específica de cuatro colores o palos: el Amor, el Sueño, la Revolución y el Conocimiento, con los respectivos emblemas de la Llama, la Estrella negra, la Rueda Sangrante y la Cerradura. El proceso de adjudicación del diseño de estampas se realiza por sorteo en marzo de 1941. Domínguez forma parte de estas propuestas lúdicas, y con el reparto de encargos de ilustraciones para el juego de cartas se le encomiendan los naipes del As del Sueño y el Mago Freud. La carta del Mago Freud, custodiada en los fondos del Museo Cantini, se forma con acuarelas, lápices de colores y tinta china sobre papel. Se compone de tres partes; cabeza, busto y una tira de imágenes: la cabeza «"está completamente vendada o envuelta, con la excepción de una boca en forma de cerradura y unos bigotes blancos que se revelan como dos magníficas bacantes desnudas. "[...]" una insólita corbata que despliega ante nuestros ojos un cuerpo femenino desnudo, mientras que en la solapa apreciamos el perfil del seno y la cadera de una mujer. "[...]" una tabla sujeta por una mano de mujer alinea siete objetos que podrían estar catalogados en un tratado de interpretación de los sueños"». Bocetos de la etapa surrealista del autor se conservan asimismo en el Museo Municipal de Bellas Artes de Santa Cruz de Tenerife.

Por sus funciones como publicista y el material preservado de esta modalidad, se tiene conocimiento de la práctica entre 1933 y 1934 de la litografía por Óscar Domínguez, valiéndose de esta técnica para la elaboración de un cartel comercial ofertado a la marca de golosinas Kréma. El registro de los grabados en la obra de Domínguez llevan a suponer que su producción desde la etapa inicial hasta el estallido de la Guerra Mundial es menor o anecdótica, relevante durante la ocupación de Francia con la implicación del artista en la actividad literaria allegada a la resistencia, eminentemente a través del aguafuerte, y ocasional con el viraje hacia la revalorización de las cualidades plásticas como aspecto autónomo del arte tras la travesía de la etapa picassiana. La forma de grabado preeminente desde finales de los años 40 será la litografía. Se refieren los títulos de ejemplares de series litográficas catalogadas: "Petite fille à la pelle sur la plage", "Silhouette au palmier", "El niño de Cádiz" de 1947, "Mujeres en el balcón" de 1948, "Toro arquero" de 1949, "Toro y caballo" y "La puerta del toril", de 1950, "Bonne Anné 1951", "Pareja y perfil" y "Toro con rueda" de 1951, "Tres mujeres" y "Hommage à Manolete" de 1955. El linograbado se presenta además como uno de los distintos modos frecuentados por el artista, combinado mayoritariamente con el ejercicio de la decalcomanía.

La portada para la monografía de Willi Baumeister por Eduardo Westerdahl con decalcomanía de Óscar Domínguez prueba que el pintor dominaba esta técnica al menos en su modalidad no intervenida ya en 1934. Hacia 1935, Domínguez muestra a sus amigos las planchas obtenidas «"selon un procédé qu'il venail, par hasard, de découvrir: c'étaient les premières" décalcomanies-sans-objet. [...] "D'autres antécédents auraient pu être trouvés, par exemple les "dentrites" que George Sand et son entourage s'amusaient à fabriquer aux soirèes de Nohant. Dans un" Portrait de Newton "par William Blake (à la Tate Gallery de Londres), la même méthode semble avoir été uilisée"». La primera mención de la decalcomanía como técnica surrealista se produce en el artículo de la revista "Minotaure", "D'une décalcomanie sans objet préconçu" por André Breton , en su nº8 de 15 de junio de 1936; se imprimen con este número 10 ejemplos de decalcomanías: 3 atribuidas al propio Breton, 2 de Óscar Domínguez, 2 de Marcel Jean, 1 de Georges Hugnet, 1 de Ives Tanguy y 1 de Jacqueline Lamba, además de un relato inspirado en ellas de Benjamin Péret. Para Breton, el descubrimiento de Óscar Domínguez, que demanda ser incorprado a los «Secrets de l'art magique surréaliste», entra en relación con el método a seguir para la obtención de campos de interpretación ideales; aquí es encontrado «"à l'état le plus pur le charme sous lequel nous tenaient, au sortir de l'enfance, les rochers et les saules d'Arthur Rackham. "[...]" le vieux mur paranoïaque de Vinci, mais c'est mur porté à sa perfection."» El "Dictionnaire abrégé du surréalisme", publicado por la galerie Beaux-Arts en 1938 bajo supervisión de Éluard y Breton, incorporará una entrada dedicada a este procedimiento, cuyos pasos se describen, distinguiéndose a su vez las versiones de decalcomanías "sans objet préconçu" y "du désir"; la decalcomanía de Marcel Jean fijada por tipografía magnética para la portada de la serie "Grisou" del año anterior es reproducida con este suplemento. En los números 12-13 de mayo de 1939 de "Minotaure", Breton refiere nuevamente con su balance "Des tendances les plus récentes de la peinture surréaliste" que el automatismo absoluto en la pintura «"n'a commencé à en être autrement qu'avec la "décalcomanie sans objet" de Dominguez et le "fumage" de Paalen."» Incorporan en adelante la decalcomanía en su repertorio técnico los artistas plásticos Salvador Dalí, Hans Bellmer, Max Bucaille, Remedios Varo y Max Ernst, quien dejará patente el resultado de la particular estética del procedimiento en tres obras de especial consideración en su carrera, "Arbre solitaire et arbres conjugaux", "L'Europe après la pluie" y "L’œil du silence", de 1940. En generaciones venideras, recurrirán aún a este modo pictórico profesionales de las artes como el egipcio Ramsés Younan, el italiano Enrico Donati, y, con sobresaliente abundancia y trascendencia, el japonés Shūzō Takiguchi.

Las decalcomanías sin objeto preconcebido serán las menos en la obra de Óscar Domínguez; priman en ella las decalcomanías del deseo, preparadas de antemano para su adaptación al lenguaje figurativo mediante el uso de plantillas adecuadas a la representación, o bien, las decalcomanías intervenidas, es decir, aquellas sobre las que se superpone pintura a partir del resultado del impregnado inicial mediante presión, generalmente interpretado como fondo de la composición. Comúnmente, las decalcomanías de Domínguez forman en sí mismas diseños de tinta o gouache en papel, intervenidos o no, o sirven complementariamente en la elaboración de óleos sobre tela. Las decalcomanías en óleos sobre tela tienen su mayor frecuencia durante la etapa surrealista y la etapa final; el primer óleo conocido del pintor con los rasgos propios de esta técnica es "Recuerdo de mi isla" de 1933. Le siguen, "Les siphons" y "Nuc avec le prefil de Breton" de 1938, "Mars" y "Decalcomanías II" y "III" de 1954, "La maison hantée" de 1955, "Apocalipsis" de 1956, "La Vérité", "Fleur", "Élements mécaniques", "La licorne double" y "Ville", de 1957, entre otros. Son decalcomanías sobre papel las "decalcomanías sin objeto preconcebido I", "II" y "III", "Decalcomanía interpretada", "Decalcomanía interpretada con abrelatas", "Le lion-éclair", "Le lion, la neige", o "Lion-bicyclette" de 1936, "Le pont", "L'Europe" y la serie "Nicolas" de 1937, las series "Femme au chandelier", "Téléphone", "Bicyclette", "Le peintre" y "Decalcomanía interpretada" de los años 40, "Composición", "Templo I" y "Templo II" de 1954. Otras combinaciones de materiales se contemplan en ocasiones, como en la decalcomanía sobre base de linoleum "Le grand surréaliste à son travail" de 1944 o el gouache sobre lienzo "Gato azul" de 1947.

La alienación mental contribuye a desacreditar la realidad; lejos de experimentar pasivamente las visiones de su imaginación, el paranoico «"las utiliza para interpretar los objetos del mundo exterior y otorgarles un sentido diferente al habitual "[...]" uno se identifica con su visión, mientras que el otro recobra su comportamiento normal después de sus incursiones en la zona prohibida."» El objeto surrealista persigue esta irrupción en el orden de la interpretación común de las cosas, a la espera de que el despertar de las psiquis cohibida responda consecuentemente con su otro orden de la realidad en el mismo espacio de lo objetivo. Se ha considerado la producción de objetos significantes no utilizables y no decorativos, aunque llenos de poder onírico y libidinal, como las más original contribución del surrealismo a la sensibilidad de su tiempo. La relación del tratamiento de objetos surrealistas presenta un ineludible punto de inflexión con la exposición de la Galerie Charles Ratton de mayo de 1936 y la adicional introducción de Breton "Crise de l'objet". La muestra se organiza a partir de un sistema de clasificación arbitrario de la producción dispuesto por los propios surrealistas; Óscar Domínguez se distinguiría como creador en el género del "objet trouvé interpreté". Su contribución en esta ocasión es el cuerpo de obras formado por "Pérégrinations de Georges Hugnet", "Arrivée de la belle époque", "Exacte sensibilité", "Conversion de la force", "Spectre du silicium" y por "Le tireur", el primer objeto surrealista catalogado del autor. Objetos surrealistas de Domínguez figurarán luego en la convocatoria internacional "Fantastic Art, Dada, Surrealism" de diciembre de 1936 en el Museum of Modern Art de Nueva York; en junio de 1937 en la "Exposition Internationale du Surréalisme" del Nippon Salon de Ginza, en Tokio; de igual manera, en 1937 en el evento "Surrealist Object & Poems", preparado por la London Gallery LTD de E.L.T. Mesens. El 17 de enero de 1938 se inaugura en la Galerie Beaux-Arts de París la "Exposition International de Surréalisme", abierta hasta el 22 de febrero; la muestra tendrá una gran repercusión mediática «"y supuso una de las aportaciones más intensas hasta la fecha, "[...]" hecho este que se evidencia en la propia concepción de la exposición y en la escenografía del montaje."» Óscar Domínguez entrega tres cuadros, una decalcomanía colectiva, un maniquí reinterpretado y los objetos "Le tireur", "Larme", "Brouette" y "Jamais" para la propuesta, asumiendo además parcialmente la pintura del portón del recinto; Marcel Duchamp supervisa la ambientación y decorado del complejo, con una mudanza especialmente ambiciosa, heterodoxa y radical. Llaman fuertemente la atención de los asistentes el trabajo "Brouette", consistente en una carretilla de madera acolchada y tapizada con satén carmesí a modo de lujoso asiento, y el gramófono "Jamais", de cuya corneta emergen las piernas de una mujer aparentemente contraída por el efecto de la mano que, en lugar de una aguja, se dispone a registrar las zonas erógenas situadas en su plato giratorio. En la primavera de 1938 la "Exposition Internationale du Surréalisme" se reproduce en la Galerie Robert de Ámsterdam y, a continuación, en la Koninklijke Kunstzaal Kleykamp de La Haya. Tras la "Internationale du Surréalisme", Domínguez no volverá a presentar objetos para exposición en vida, aún confeccionándolos hasta sus últimos años, como pertenencias personales o para obsequiar a sus amigos.

El pasaje de Óscar Domínguez por el tramo álgido de los objetos surrealistas es en parte documentado por sus compañeros, lo que permitirá la constancia de elaboraciones en paraderos desconocidos. Georges Hugnet publica en mayo de 1935 en "Cahiers d'Art" el artículo , con imágenes de "Le tireur" y "Arrivée de la belle époque"; los números 1-2 de "Cahiers d'Art" de junio de 1936, monográfico con el título "Pour l'objet" sobre la materia, tiene consigo un registro de "Exacte sensibilité" en fase de construcción; se conocen dos tomas de la sesión fotográfica realizada por Man Ray sobre la pieza "Brouette", con modelo femenino exhibiendo vestido de Lucien Lelong, presente a su vez en el número 10 de "Minotaure" de diciembre de 1937; "Jamais" aparecerá en las publicaciones "Voilà", "Paris-Midi", "Marianne", "Le Rire" y "Life" entre enero y febrero de 1938; el maniquí nº 11 de la , instalado durante la "International" de 1938 en la Galerie Beaux-Arts es fotografiado por Raoul Ubac, Josef Breitenbach, Denise Bellon, Robert Valencey y Man Ray, valiéndose este último de una toma para la ilustración de su álbum "Les Mannequins. Résurrection des mannequins" editado en 1966. Atendiendo a su formato, los objetos surrealista de Óscar Domínguez pueden distinguirse entre objetos de cuerpo independiente, como los citados "Le tireur", "Conversion de la force" y "Jamais", así como también, "Jeux", "Overture" o "Parole d'honneur" de 1937, las cajas-objetos como "Caja con piano y toro" de 1936 o "Boïte de cigares", "La estrella acuática" o "L'improviste" de 1938, y los cuadros-objetos como "Exacte sensibilité", o "Pérégrinations de Georges Hugnet". Los objetos de formato independiente, exentos de la atmósfera en la que se imbuyen las entidades del surrealismo pictórico de Domínguez, se identifican con estas, por lo demás, en lo mayor de sus aspectos conceptuales; se trata de la materialización de ideas simbólicas y elementales, a menudo diluidas en su interrelación, capturadas en aquel instante de mutación en el que pueden ser interpretadas en su fórmula como un símbolo nuevo y único, representante de una voluntad singular y poética. Objetos como "Overture" o "Brouette" se aproximan más a una combinación poética de los símbolos definida, cerrada, deliberada y coherente. En contraste con sus formas pictóricas, las obras de este género del autor evocan con menor intensidad sentimientos de angustia o desasosiego en tanto se enfatizan los atributos asexuados, cómicos y ocurrentes, dispuestos a la consideración, la curiosidad o la paradoja. Se reitera en cualquier caso el cruce de ideas en sentidos opuestos hasta su imposibilidad o el plano vertical diferenciador. Son también objetos surrealistas conocidos de Óscar Domínguez los titulados "Le Dactylographe" y "L'après-midi" de 1935, "El encuentro de materias exactas", "Fin de un día sin aventuras" y "Violin et tête de poulain", de 1936, "Le calculateur" de 1943, "Minotaure" de 1955 y "Le trophée de Marie-Laure" de 1957.

Numerosas razones de sólida consistencia confluirían en torno a 1940 para propiciar en Óscar Domínguez un viraje desde la manufactura de objetos surrealistas hacia la labor escultórica. Un caso elocuente de la transición entre los dos géneros de producción se contempla con "Tête de taureau", de 1936, construcción simple y audaz de madera pintada, enmarcada al modo del cuadro-objeto, cercana a la desproporción y al animismo de la máscara tallada, en representación directa y primitiva del concepto individualizado de la cabeza de toro. El volumen de las construcciones imaginadas por Domínguez durante la denominada etapa pictórica de las mujeres desmontables se extiende en 1942 a su perfilado en madera. El rendimiento en este tipo de industria resultará escaso comparado con el de la pintura, y relevante por su calidad y por la riqueza de sus connotaciones; son de este año las esculturas "Mujer desmontable I", "Mujer desmontable II" y la mujer acostada "Escultura transformable". Las tallas en madera de las "mujeres desmontables", de antropomorfismo hipertrofiado, cosificadas o fetiches, se articulan en piezas de ajuste, y son susceptibles de tornarse armadas y desarmadas según el deseo del observador. La facultad de la combinación de las partes concede a las figuras la cualidad de una corporeidad indeterminada, elusiva, nunca concluyente. Acorde a los sentimientos creativos del arte indígena o el arte prehistórico, las "mujeres desmontables" de Domínguez apelarían a los instintos primarios del individuo, favoreciendo la fertilidad, infundiendo temor o respeto, o facilitando la alucinación. Con la "Escultura transformable" se sustituye el formato de la talla de piezas desmontables por el de las piezas móviles, con similares resultados. El mismo planteamiento se dará con "Poupée flechée", igualmente fechada en 1942.

El autor mantiene una relación sentimental con la escultora Nadine Effront en 1950 cuando conoce a la promotora de las artes Marie-Laure Bischoffsheim, vizcondesa de Noailles, quien le sostiente con el desarrollo de la nueva década en el propósito de impulsar su carrera profesional en Europa; la amistad con Marie-Laure de Noailles deviene en una nueva relación sentimental y afianza la ruptura de su matrimonio con Maud Bonneaud. Es esta una nueva oportunidad para Domínguez en el empeño de perfeccionar sus dotes como escultor. En enero de 1951, el tinerfeño dispone de un taller para la forja de hierro, así como un espacio de retiro con Marie-Laure en la Villa Noailles de la localidad de Hyères. Los jardines de la villa serán decorados con los trabajos en hierro de 1951 "Gato", "Petit fille sautant à la corde" y "L'oiseau", imitaciones de los dibujos bidimensionales y filiformes practicados hasta entonces en papel, sustituido el trazo por la vara de metal, en planos estructurados de hasta tres metros de longitud, carentes de dintornos; "El pirata", muy semejante, se vale de su tridimensionalidad para apoyarse sobre el techo del apartamento anexo, desde el que escudriña como un minotauro con catalejo la lejanía, silueteado en blanco los planos mayores con velamen de barco acordonado. En un ángulo del jardín triangular del recinto el artista erigirá complementariamente el "Monumento al gato", formado por cuatro pilares y un dintel asentados como mampostería de piedra y hormigón; el dintel es decorado con vidrio verde, varillas de acero y remates de prismas triangulares, con el aspecto esquemático de un gato. Una réplica de esta construcción, de aproximadamente tres metros de alto y cuatro de ancho, se introducirá entre el complejo escultórico del Parque García Sanabria en Santa Cruz de Tenerife, en 1973.

En el transcurso de sus estancias en la Costa Azul, Pablo Picasso habría conocido la fabricación cerámica tradicional en el contexto de la industria alfarera típica de la localidad de Vallauris. Durante una de las exposiciones organizadas por los alfareros del lugar, los esposos Georges y Suzanne Ramié animan en 1946 al artista malagueño a iniciarse personalmente en esta práctica con algunas creaciones. Picasso tendrá un espacio en sus comienzos como ceramista en el taller del matrimonio Ramié, "Madoura", hasta su mudanza en 1948 a la Villa La Galloise, en la misma población. En 1949 Picasso habría organizado ya su propio taller de cerámica; en pocos años «"creó más de 2.000 piezas, número que atestigua claramente la desacostumbrada energía con la cual se precipitó sobre esa nueva técnica. La fase más intensa "[...]" se produjo en Vallauris, en el período que va desde 1947/1948 hasta 1954."» Óscar Domínguez probaría también a hacer algunas piezas de cerámica en la misma fábrica donde trabajaba Picasso en 1949, pintadas y decoradas de acuerdo a su personal estilo, con elementos procedentes de sus cuadros.

Por otra parte, Domínguez y Maud Bonneaud se interesarían por la vitrificación de objetos. Ambos se comprarían «"un pequeño horno en el que probaron distintos métodos "[...]" Juntos realizaron mediante esta técnica unas pequeñas placas vitrificadas que reproducían ciertos elementos de su obra pictórica"». Son cerámicas o esmaltes conocidos del pintor canario el "Plato de sandía con moscas", los diseños sobre cobre del autor y Bonneaud "Cabeza", "Pareja", y "Pez y frutero", un broche sin título de técnica mixta o su "Botijo" de 1949.

Al pintor se debe la decoración con murales realizados de manera colectiva o individual. En 1945 interviene en la realización de dos frescos para la sala de guardia del Centre Hospitalier Sainte-Anne de París, en compañía de Marcel Jean, Jacques Hérold, Maurice Henry, Frédéric Delanglade, Louis Fernández, Francis Bott, Honorio García Condoy, Baltasar Lobo y Manuel Viola. En 1953 estará con Maurice Van Moppes, Lucien Coutaud, Marcel Vertes, Vieira Da Silva, Felix Labisse, Georges Wakhevitch y Marie-Luise de Noailles en la decoración del restaurante cabaret "La Castagnette" de la capital francesa. Individualmente, Domínguez ornamenta con un fresco de unos diez metros parte de los muros de la Villa Noailles, mientras que en la pared de las instalaciones correspondientes a la piscina del recinto pintaría una tauromaquia de inspiración micénica. El relieve en piedra de un toro en un muro del castillo de St. Bernard en Hyères del autor se halla datado en 1952.

Domínguez habría intervenido además en el diseño del vestuario y los decorados y en la realización de estos últimos para la representación de la obra de teatro de Jean-Paul Sartre, "Le Mouches", llevada a los escenarios de Alemania. Sin embargo, tanto «"los dibujos preparatorios del decorado como los diseños de los trajes se han perdido "[...]" ésta fue la única incursión de Domínguez en el campo de la escenografía"».

El complejo arquitectónico Tenerife Espacio de las Artes|TEA, ubicado en Santa Cruz de Tenerife, integra desde octubre de 2008 el museo contemporáneo Instituto Oscar Domínguez de Arte y Cultura Contemporánea, (IODACC); el diseño del recinto cultural, dirigido por el tinerfeño Virgilio Gutiérrez Herreros, se debe al estudio de arquitectos suizos Herzog & de Meuron. En julio de 2017 se anuncia la constitución de la Comisión Consultiva de Expertos y Defensa de la Obra de Óscar Domínguez (CEDOOC), con localización en TEA, destinada a «"emitir consideraciones sobre la autenticidad de las obras del pintor "[...]" un seguimiento continuado de las diferentes obras de Domínguez que habitualmente salen a subasta, así como de aquellas otras presentadas en galerías y ferias de arte de las exhibidas en diferentes contextos expositivos, con el objetivo de obtener la información necesaria sobre la circulación de sus obras y atribuciones en el mercado de arte"».







</doc>
<doc id="8803" url="https://es.wikipedia.org/wiki?curid=8803" title="Rudi Dornbusch">
Rudi Dornbusch

Rudi Dornbusch (8 de junio de 1942 - 25 de julio de 2002) fue un economista alemán nacido en Krefeld.

Cursó sus estudios en la Universidad de Ginebra y se doctoró en Universidad de Chicago en 1971. Enseñó en la Universidad de Rochester, en la de Chicago y por último en el Instituto Tecnológico de Massachusetts.

Poseía gran talento para extraer la esencia de un problema y hacerlo comprensible en términos sencillos. Explicó, por ejemplo, las fluctuaciones de precios y tipos de cambio con gran claridad en lo que se conoce como 'resultado de Dornbusch'.

Murió el 25 de julio de 2002 aquejado de un cáncer.



</doc>
<doc id="8806" url="https://es.wikipedia.org/wiki?curid=8806" title="Computadora de escritorio">
Computadora de escritorio

La "computadora de escritorio" (en Hispanoamérica) u "ordenador de sobremesa" (en España) es un tipo de computadora personal, diseñada y fabricada para ser instalada en una ubicación fija, como un escritorio o mesa, a diferencia de otras computadoras similares, como la computadora portátil.
Puede referirse a dos tipos de computadoras:

Antes de la popularización de los microprocesadores, el rumor quería que una computadora que cupiese en un escritorio debería ser como un juguete. De hecho, las primeras computadoras (en el sentido moderno) ocupaban una habitación completa. Las minicomputadoras, de uso común, tenían el tamaño de un escritorio, y casi siempre estaban montadas en los estantes.
El Programma 101 italiano, comercializado en 1965, fue la primera «calculadora/computadora programable» ("programmable calculator/computer") que podía caber en un escritorio. Otros equipos de oficina siguieron en 1971. En 1972, se vendieron los primeros dispositivos que podían programarse en BASIC; eran versiones reducidas de minicomputadoras que incluían una ROM y una pantalla alfanumérica de una línea de LED; podían trazar figuras 2D gracias a un trazador.
Las computadoras de uso doméstico suelen estar dedicadas al entretenimiento (multimedia, videojuegos, etc.) y a tareas domésticas (contabilidad casera, escritos, etc.). Estas computadoras carecen de gestión y mantenimiento ya que estas tareas son de poca importancia para un particular; sin embargo, la situación es bien distinta en el ámbito empresarial, en el cual la computadora de escritorio es la herramienta de trabajo por excelencia; se trata de un elemento muy importante para la marcha de un negocio. El uso que se hace de las computadoras de escritorio está relacionado normalmente con las tareas productivas y administrativas de los empleados: creación de informes, presentaciones, memorandos (véase paquete de oficina), comunicación con otras empresas, contabilidad, gestión de tareas, etc.; por este motivo, la computadora de escritorio debe ser adecuadamente gestionada en el ámbito empresarial.

Obsérvese que mientras un particular debe preocuparse normalmente de una o dos computadoras únicamente, una empresa puede tener como activo un parque de cientos o miles de computadoras personales. En este sentido existen dos actuaciones complementarias:

La computadora de escritorio, como cualquier máquina, está sujeta a defectos y averías. La incidencia de una avería en un usuario doméstico suele reducirse a una mera molestia.

En el ámbito empresarial el impacto de una avería supone como poco, la pérdida de tiempo de trabajo de un empleado. Pero existen casos donde hay pérdida monetaria y de imagen. Generalmente, en puestos de trabajo de atención al público; por ejemplo, en las ventanillas de una oficina bancaria, o en el puesto de trabajo de un "broker" o agente de bolsa.

La garantía de mantenimiento de una computadora de escritorio suele durar de dos a cinco años; esto obliga a las empresas a renovar su parque de computadoras muy frecuentemente.

Todas las computadoras necesitan software para funcionar. La instalación de software en miles de equipos repartidos por una oficina o diversas sedes no es nada trivial. Además, esta actividad es prácticamente obligatoria en la empresa. Las actualizaciones de software y los parches de seguridad son necesarios para evitar las mismas consecuencias que tendría una avería del hardware. Los problemas típicos de una empresa respecto al software de escritorio son:

Un error típico de las empresas es desarrollar (o comprar) software sin saber qué características tienen las computadoras personales donde debe ser usado; por ejemplo, es frecuente desarrollar software para Windows 8 y una versión concreta de Mozilla Firefox; cuando llega el momento de la respectiva instalación resulta que parte del parque de computadoras aún tiene instalado Windows XP o una versión antigua del navegador.

Existen tres enfoques:

Generalmente, las organizaciones suelen estar "a caballo" entre dos de estos enfoques, ya que se trata de una cuestión de madurez organizativa.

Generalmente, las grandes organizaciones disponen de un departamento organizado de atención a las incidencias en los puestos de usuario. Se suele denominar "help desk" o ‘gestión de incidencias’, y puede estar externalizado. Este departamento suele estructurarse en dos niveles de soporte.

El papel fundamental de esta unidad es la centralización de las incidencias, generalmente mediante un número único de teléfono. Esto es esencial de cara al usuario de la computadora. De otra manera se perdería mucho tiempo solamente en localizar a un técnico (como ocurre en la "gestión reactiva"). Cada incidencia es registrada y documentada, lo que ofrece cierta garantía al usuario de que el problema se resolverá. Además, esto proporciona información a los mandos directivos para la toma de decisiones y la localización de ineficiencias.

La mayoría de las incidencias corresponden a "problemas conocidos" que pueden ser resueltos en esta unidad gracias a "argumentarios". Un argumentario describe un problema conocido y cómo resolverlo.

En caso de que no sea posible resolver una incidencia en este nivel, es dirigida al siguiente. Cuando se trata de incidencias responsabilidad de terceros, esta unidad es responsable de movilizar y coordinar los recursos necesarios con total transparencia para el usuario; por ejemplo: dar aviso a un servicio técnico o reclamar la garantía de un fabricante.

Esta unidad tiene el cometido de resolver las incidencias que no se hayan producido con anterioridad, y documentarlas para que el soporte de primer nivel pueda solucionarlas en el futuro.

Existen diversas herramientas software que facilitan la gestión de un parque de computadoras de escritorio; cabe destacar:

Una computadora de escritorio todo-en-uno (en inglés, "All-in-One": AIO) es un equipo que integra en una carcasa de monitor (mayoritariamente ampliada) todos los componentes del sistema, bien por detrás de la pantalla (en los casos de pantallas TFT) bien en uno o varios de los lados del tubo de rayos catódicos (pantallas de tubo), o en el pie del equipo. Esto permite ocupar un espacio menor, pero sin los beneficios de portabilidad de un netbook, pues debe añadirse el teclado y ratón independientes y carecen del beneficio de la batería de este.

El factor de forma del todo-en-uno fue popular durante la década de 1980 para los computadores destinados al uso profesional como el Kaypro II, Osborne 1, TRS-80 Model II y Compaq Portable, y la computadora doméstica Commodore SX-64 incluyen en una carcasa de sobremesa la placa base, disqueteras y un monitor CTR de pequeño tamaño, cerrando el frontal un teclado que puede separarse y a la vez protege el monitor y las disqueteras. Todos presentan un conector para un monitor externo y un asa de transporte. Forman la subclase de computadora portable, que no portátil pues su gran peso (hasta 17kg) y el requisito de un conector mural de potencia los separan claramente.

En 1984, Apple Computer lanza el Macintosh original y sus sucesores con el mismo factor de forma que abandona en 1993, para recuperar el "todo en uno" con la gama iMac en 1998, verdadero motor de este tipo de equipos que habían caído en desuso.

La competencia del iMac original comienza con equipos que integran todo en la carcasa de monitor (incluso clonan su forma al 100%) y se crea además un «engendro» consistente en fusionar una carcasa de monitor de 14 ó 15 pulgadas con una caja SFF o incluso de sobremesa completa.

A principios de la década de 2000, muchos diseños all-in-one estaban usando las pantallas planas, y muchos de ellos, como el iMac G4 han utilizado componentes de la computadora portátil con el fin de reducir el tamaño de la carcasa del sistema. La fuente de alimentación mayoritariamente se externaliza, aunque no siempre se sigue el criterio de sencillez en el conector. Esto redunda en equipos cada vez más reducidos en cuanto a componentes, aunque el tamaño de la pantalla ha ido creciendo.

Para finales de 2012 algunos modelos todo-en-uno también incluyen pantalla táctil para dar cabida a Windows 8 y Windows 10.

Al igual que los computadores portátiles, algunos todo-en-uno se caracterizan por la incapacidad de personalizar o actualizar los componentes internos, porque el chasis del sistema no proporcionan fácil acceso, excepto a través de paneles que sólo exponen conectores de RAM o un dispositivo de almacenamiento. Una avería de ciertos componentes puede significar que todo el equipo deba ser sustituido, independientemente del estado de sus componentes restantes. Hay excepciones a estos casos; por ejemplo, la porción de monitor de la estación de trabajo HP Z1 G2 puede estar en ángulo plano, y abrirse como un capó de automóvil para acceder al hardware interno.



</doc>
<doc id="8809" url="https://es.wikipedia.org/wiki?curid=8809" title="Hemoglobina">
Hemoglobina

La hemoglobina es una hemoproteína de la sangre, de masa molecular de 64 000 g/mol (64 kDa), de color rojo característico, que transporta el dioxígeno (antiguamente llamado oxígeno), O, desde los órganos respiratorios hasta los tejidos, el dióxido de carbono, CO, desde los tejidos hasta los pulmones que lo eliminan y también participa en la regulación de pH de la sangre, en vertebrados y algunos invertebrados.

La hemoglobina es una proteína de estructura cuaternaria, que consta de cuatro subunidades. Esta proteína forma parte de la familia de las hemoproteínas, ya que posee 1 grupo hemo en cada subunidad.

En 1825 J.F. Engelhard descubrió que la relación del hierro con la proteína era idéntica en las hemoglobinas de varias especies. A partir de la masa atómica conocida del hierro calculó la masa molecular de la hemoglobina como "n" × 16 000 ("n" = número de átomos de hierro por molécula de hemoglobina, que ahora se sabe que es 4), la primera determinación de la masa molecular de una proteína. Esta "conclusión apresurada" causó mucha burla en el momento entre los científicos que no podían creer que ninguna molécula pudiera ser tan grande. Gilbert Smithson Adair confirmó los resultados de Engelhard en 1925 midiendo la presión osmótica de las disoluciones de hemoglobina.

La proteína hemoglobina que transporta el dioxígeno fue descubierta en 1840 por el médico y químico alemán Friedrich Ludwig Hünefeld (1799-1882). En 1851, el fisiólogo alemán Otto Funke publicó una serie de artículos en los que describía el crecimiento de los cristales de hemoglobina mediante la dilución sucesiva de glóbulos rojos con un disolvente —agua pura, alcohol o éter—, seguida de la evaporación lenta del disolvente de la solución. La oxigenación reversible de la hemoglobina fue descrita unos años más tarde por Felix Hoppe-Seyler.

En 1959, Max Perutz determinó la estructura molecular de la mioglobina (similar a la hemoglobina) por cristalografía de rayos X. Este trabajo le supuso la concesión del Premio Nobel de Química de 1962, compartido con John Kendrew.

El papel de la hemoglobina en la sangre fue dilucidado por el fisiólogo francés Claude Bernard.

El nombre de la hemoglobina deriva de las palabras "heme" y globina, lo que refleja el hecho de que cada subunidad de hemoglobina es una proteína globular con un grupo hemo incrustado. Cada grupo hemo contiene un átomo de hierro, que puede unirse a una molécula de dioxígeno a través de fuerzas dipolares iónicas inducidas. El tipo más común de la hemoglobina en los mamíferos contiene cuatro de tales subunidades.

La forman cuatro cadenas polipeptídicas (globinas) a cada una de las cuales se une un grupo hemo, cuyo átomo de hierro es capaz de unir de forma reversible una molécula de dioxígeno. El grupo hemo está formado por:

La hemoglobina es una proteína tetrámera, que consta de cuatro cadenas polipeptídicas con estructuras primarias diferentes. La hemoglobina presente en los adultos (HbA) tiene dos cadenas α y dos cadenas β. La cadena α consta de 141 aminoácidos y una secuencia específica, mientras que la cadena β consiste de 146 aminoácidos con una estructura primaria diferente. Estas cadenas son codificadas por genes diferentes y tienen estructuras primarias diferentes. En el caso de las cadenas δ y γ de otros tipos de hemoglobina humana, como la hemoglobina fetal (HbF) es muy similar a la cadena β. La estructura tetrámera de los tipos comunes de hemoglobina humana son las siguientes: HbA1 tiene α2β2, HbF tiene α2γ2 y HbA2 (tipo menos común en los adultos) tiene α2δ2.

Las cadenas α y β de la hemoglobina tienen un 75 % de hélices alfa como estructura secundaria, con 7 y 8 segmentos respectivamente. Cada cadena polipeptídica de la hemoglobina está unida a un grupo hemo para formar una subunidad. Las cuatro subunidades de la hemoglobina en su estructura cuaternaria forman un tetraedro. Y sus subunidades se unen entre ellas por puentes de sal, que estabilizan su estructura.

El grupo hemo está localizado en una cavidad entre dos hélices de la cadena de la globina y a su vez está protegido por un residuo de valina. Los grupos vinilo no polares del grupo hemo se encuentran en el interior hidrofóbico de la cavidad, mientras que los grupos porfirina polares cargados se encuentran orientados hacia la superficie hidrofílica de la subunidad.

También se encuentran residuos de histidina de las cadenas polipeptídicas, que se enlazan al átomo de hierro y se designan como histidinas proximales, ya que están presentes cerca del grupo hemo, mientras que la histidina distal se encuentra lejos del grupo hemo.

El átomo de hierro se encuentra en el centro del anillo de porfirina y tiene seis valencias. El hierro está unido al nitrógeno de los cuatro anillos de pirol por cuatro de sus valencias, su quinta valencia se une al nitrógeno de la histidina proximal y la sexta está ocupada por la hisitidina distal o por dioxígeno.

Se puede estudiar las propiedades del enlace entre el dioxígeno y la hemoglobina a partir de la curva de enlace del dioxígeno, la cual presenta la saturación fraccional, respecto a la concentración del mismo. La saturación fraccional, Y, se define como el número de sitios de enlace saturados con dioxígeno respecto al número total de sitios de enlace posibles en una molécula de hemoglobina. El valor de Y puede ir desde 0 (todos los sitios de enlace están sin dioxígeno) hasta 1 (todos los sitios de enlace están enlazados con dioxígeno). La concentración de dioxígeno se mide en presión parcial, pO.

La curva de enlace de la hemoglobina es sigmoidea. Esta forma de la curva sugiere que el enlace del dioxígeno a un sitio de enlace, aumenta la probabilidad de que se enlace otro dioxígeno a un sitio de enlace vacío. Asimismo, la liberación de dioxígeno de un sitio de enlace facilita la liberación de dioxígeno de otros sitios de enlace. A este comportamiento se le llama cooperativo, porque las reacciones de enlace en sitios de enlace individuales en cada molécula de hemoglobina están relacionadas e influyen directamente en las reacciones de enlace de los otros sitios de enlace de cada molécula.

El comportamiento cooperativo de la hemoglobina es indispensable para un transporte eficiente del dioxígeno dentro del cuerpo. En los pulmones, la hemoglobina se satura en un 98 % de dioxígeno. Esto quiere decir que un 98 % de los sitios de enlace de cada molécula de hemoglobina están enlazados a una molécula de dioxígeno. Al movilizarse la hemoglobina por la sangre, libera el dioxígeno a las células, y su nivel de saturación se reduce a un 32 %. Esto quiere decir que un 66 % (98 % − 32 % = 66 %) de los sitios de enlace de la hemoglobina contribuyen al transporte y descarga de dioxígeno. Si una proteína que no presenta un comportamiento de enlace cooperativo, realiza el mismo trabajo que la hemoglobina, su eficiencia se verá reducida notablemente, por ejemplo la mioglobina tiene una eficiencia del 7 %.

La presión a la cual la hemoglobina se encuentra saturada en un 50 % (p50) muestra la afinidad de distintos tipos de hemoglobina respecto al dioxígeno. En la HbA (hemoglobina adulta), la p50 es a 26 mm de Hg, mientras que la HbF (hemoglobina fetal) tiene el p50 a 20 mm de Hg. Esta diferencia en la afinidad relativa por el O permite a la HbF extraer dioxígeno de la HbA de la sangre placentaria de la madre para que el feto la utilice. Después del nacimiento, la HbF es reemplazada por la HbA.

El comportamiento de enlace cooperativo de la hemoglobina con el O requiere que el enlace del dioxígeno en un sitio de enlace en el tetrámero de la hemoglobina influya en los otros sitios de enlace dentro de la misma molécula. Estos cambios se evidencian en su estructura cuaternaria. Los dímeros α1β1 y α2β2 rotan aproximadamente 15 grados el uno respecto al otro.

La estructura cuaternaria observada en el estado desoxigenado de la hemoglobina se conoce como el estado T (tenso), ya que las interacciones entre sus subunidades son fuertes. Mientras que la estructura de la hemoglobina completamente oxigenada, oxihemoglobina, es conocida como el estado R (relajado), ya que las interacciones entre sus subunidades se encuentran debilitadas (o relajadas). Al desencadenar el paso del estado T al estado R, el enlace de una molécula de dioxígeno aumenta la afinidad de otros sitios de enlace.

Se puede explicar la cooperatividad de la hemoglobina a partir de distintos modelos. Se han desarrollado 2 modelos diferentes. El modelo concertado (Modelo MWC) explica que la hemoglobina tiene únicamente 2 formas: el estado T y el estado R. Al enlazarse con un ligando, el equilibrio cambia entre estos 2 estados. La desoxihemoglobina se considera en estado T. Pero al enlazarse una molécula de dioxígeno, el estado R está muy favorecido. En este estado se favorece fuertemente el enlace de más moléculas de dioxígeno. En este modelo, cada tetrámero puede existir exclusivamente en dos estados (T o R). En cambio el modelo secuencial explica que la unión de un dioxígeno a la hemoglobina favorece la unión de más dioxígenos, pero no significa un cambio total del estado T al estado R.

Cuando la hemoglobina tiene unido dioxígeno se denomina oxihemoglobina o hemoglobina oxigenada, dando el aspecto rojo o escarlata intenso característico de la sangre arterial. Cuando pierde el oxígeno, se denomina desoxihemoglobina o hemoglobina reducida, y presenta el color rojo oscuro de la sangre venosa (se manifiesta clínicamente por cianosis).


En las células rojas es crucial la presencia del 2,3-bisfosfoglicerato, ya que éste determina la afinidad de la hemoglobina con el dioxígeno. Para que la hemoglobina funcione eficientemente, el estado T debe permanecer estable hasta que la unión de suficientes dioxígenos permita la transición al estado R. El estado T es muy inestable; esto desplaza el equilibrio hacia el estado R. Esto resultaría en una baja liberación de dioxígeno en los tejidos. Por lo tanto es necesario un mecanismo adicional para estabilizar el estado T. Este mecanismo se descubrió a partir de la comparación de la afinidad con el dioxígeno de la hemoglobina en un estado puro y en las células rojas. Se encontró que en un estado puro la hemoglobina se enlaza fuertemente al dioxígeno, dificultando su liberación, mientras que en las células rojas su afinidad es menor. Esta diferencia se debe a la presencia del 2,3-bifosfoglicerato (2,3-BPG). Este compuesto está presente en las células rojas de la sangre a una concentración igual a la concentración de hemoglobina (~ 2 mM).

El 2,3-BPG se enlaza al centro del tetrámero, en un ‘bolsillo’ que solo está presente en el estado T. En la transición de T a R, este ‘bolsillo’ colapsa y se libera el 2,3-BPG. Para que esto suceda los enlaces entre la hemoglobina y el 2,3-BPG se deben romper, y a su vez cuando este compuesto está presente se necesitan mayores enlaces de dioxígeno con la hemoglobina, para que ésta cambie su forma de T a R. Por lo tanto la hemoglobina se mantiene en su estado T de baja afinidad hasta que alcance un medio con altas concentraciones de dioxígeno. Debido a esto se le llama al compuesto 2,3-BPG efector alostérico. La regulación por parte de una molécula estructuralmente diferente al O es posible gracias al enlace del efector alostérico en un sitio completamente distinto a los sitios de enlace del dioxígeno en la hemoglobina.

El aumento de oxigenación de la hemoglobina en los pulmones y la rápida liberación de dioxígeno en los tejidos gracias a los efectos del pH y la presión parcial del dióxido de carbono, pCO, es conocido como el efecto Bohr. Los tejidos de metabolización rápida, como el músculo durante la contracción, generan grandes cantidades de iones de hidrógeno y dióxido de carbono. Para liberar dioxígeno donde es más necesario, la hemoglobina ha evolucionado para responder a las concentraciones de estas dos sustancias. Al igual que el 2,3-BPG, los iones de hidrógeno y el dióxido de carbono son efectores alostéricos de la hemoglobina que se unen a sitios distintos a los sitios de enlace del dioxígeno.

La afinidad de la hemoglobina respecto al dioxígeno disminuye, al bajar el valor del pH de 7,4. Por lo tanto, cuando la hemoglobina se mueve hacia la región con menor pH, tiende a liberar más dioxígeno. Por ejemplo, el transporte desde los pulmones, con pH de 7,4 y un pO de 100 torr, hacia el músculo activo, con un pH de 7,2 y una presión de dioxígeno parcial del 20 torr, resulta en la liberación de un 77 % del total de dioxígeno que lleva una molécula de hemoglobina. Mientras que en el caso de no encontrarse ningún cambio de pH o de la pO se liberaría solo un 66 % del total de dioxígeno de la hemoglobina.

En el metabolismo aeróbico en los tejidos, el CO liberado y la pCO aumentan con un incremento simultáneo de iones H. El pH de los tejidos se reduce debido a la formación de ácidos metabólicos como el ácido carbónico y láctico. A pH bajo la afinidad de la hemoglobina hacia el dioxígeno se reduce y aumenta la velocidad de disociación del dioxígeno de la oxihemoglobina en los tejidos. La condición inversa prevalece en los pulmones, donde pCO es baja y el pH es alto, por lo tanto la afinidad de la hemoglobina para unirse con O aumenta. Al enlazarse con el primer dioxígeno, la hemoglobina sufre cambios conformacionales y pasa del estado T al R, aumentando así la cooperatividad.

La hemoglobina se enlaza al dióxido de carbono en los tejidos después de liberar dioxígeno, y transporta el 15 % del total de CO transportado en la sangre. También tiene una función como tampón químico, ya que se enlaza a dos protones por cada cuatro moléculas de dioxígeno liberadas, por lo tanto contribuye al mantenimiento de un pH constante en la sangre.

Todas las condiciones fisiológicas y clínicas asociadas con falta de dioxígeno estimulan la producción de 2,3-DPG en los eritrocitos, lo cual resulta en un aumento de liberación de dioxígeno de la hemoglobina.
Hipoxia: en un estado hipóxico, la concentración de 2,3-DPG en las células rojas es elevada, debido a un aumento en la glucólisis. Este es un ejemplo de la adaptación a la hipoxia por parte del cuerpo.
Anemia: es una condición clínica asociada con una disminución del nivel de hemoglobina en la sangre. Esto genera un suministro pobre de dioxígeno a los tejidos. En la anemia, la concentración de 2,3-DPG en las células rojas es elevada, aumentando la liberación de dioxígeno.
Adaptación a altura: las personas que viven en regiones gran altitud, donde la concentración de dioxígeno es baja, el cuerpo realiza varios cambios fisiológicos para adaptarse a estas condiciones. Estos cambios incluyen hiperventilación, policitemia y un aumento en la producción de 2,3-DPG en los eritrocitos.


También hay hemoglobinas de los tipos: Gower 1, Gower 2 y Portland. Estas solo están presentes en el embrión.

Los valores de referencia varían de acuerdo a cada laboratorio clínico y por eso se especifican al solicitar la prueba. Dependen de la ubicación del mismo, específicamente de la altitud, de la calidad de las técnicas usadas, etc.




</doc>
<doc id="8810" url="https://es.wikipedia.org/wiki?curid=8810" title="Lance Armstrong">
Lance Armstrong

Lance Edward Armstrong (nacido Lance Edward Gunderson, en Austin, Texas, 18 de septiembre de 1971) es un exciclista profesional estadounidense. Se retiró definitivamente del ciclismo profesional al inicio de la temporada 2011 tras participar en el Tour Down Under.

En 1996 se le detectó un cáncer de testículo, del cual se recuperó hasta volver a las rutas dos años después. Logró siete triunfos consecutivos del Tour de Francia entre 1999 y 2005, así como una medalla de bronce en los Juegos Olímpicos de Sidney 2000. Sin embargo, el 13 de junio de 2012 fue acusado de dopaje sistemático por la Agencia Antidopaje de Estados Unidos (USADA). El 23 de agosto de 2012 la USADA decidió finalmente retirarle las siete victorias por dopaje, además de suspenderlo de por vida. El 22 de octubre de 2012 la UCI ratificó la decisión de la USADA, y anuló su palmarés ciclístico a partir de 1998. Armstrong admitió haber usado EPO, testosterona y transfusiones de sangre para mejorar el rendimiento durante su carrera de ciclismo.

Su caso provocó reacciones en su contra por parte del mundo del deporte en general.

Armstrong nació el 18 de septiembre de 1971 en Plano, Texas, en el norte de Dallas. Con doce años, empezó su carrera deportiva en el equipo de natación de la ciudad de Plano (City of Plan Swim Club). Uno de sus primeros logros fue terminar cuarto en el campeonato de Texas de los 1.500 libres. El futuro ciclista cambió de disciplina cuando vio un anuncio para participar en un triatlón. Se apuntó y ganó fácilmente.

En la temporada 1987-1988, Armstrong consiguió terminar en la primera posición del calendario estadounidense de triatlón en la categoría de 19 o menos años. La segunda plaza fue para Chann McRae, con quien más tarde coincidió en el equipo ciclista US Postal Service. A los 16, Armstrong se convirtió en triatleta profesional. Los logros no tardaron en llegar, con campeonatos nacionales de la modalidad de sprint en 1989 y 1990, cuando solo tenía 18 y 19 años, respectivamente.

Pronto quedó claro que su gran talento, la disciplina en la que marcaba las diferencias, era la carrera en bicicleta. Sobre todo después de ganar en 1991 el campeonato de aficionados además de ganar la prueba profesional de la Semana Lombarda. Esos logros le sirvieron para participar en la modalidad de carretera de los , donde acabó en decimocuarta posición con la ayuda de su compañero Bob Mionske. Tras esta actuación, Armstrong firmó su primer contrato profesional como ciclista con el Motorola, equipo con el que ganó su primera carrera, el Trofeo Laigueglia en Italia, donde superó al favorito, Moreno Argentin.
En 1993, Armstrong ganó diez carreras de un día y etapas de vueltas por etapas. Además, se convirtió en uno de los corredores más jóvenes en ganar el Campeonato del Mundo en ruta, celebrado en Oslo, donde bajo la lluvia dio la sorpresa ante los favoritos. Miguel Indurain llegó segundo. Al año siguiente, ya con el maillot arcoíris, acabó segundo en la Clásica de San Sebastián y en la Liège-Bastogne-Liège.

El estadounidense aumentó su prestigio como corredor de clásicas tras ganar la Clásica de San Sebastián en 1995 (donde tres años antes fue último). Además, esa temporada consiguió la etapa con final en Limoges en el Tour de Francia. De ese día es muy recordada entre los aficionados la imagen de Armstrong llegando a meta señalando el cielo para dedicar la victoria a su compañero de equipo fallecido en ese mismo Tour, Fabio Casartelli.

La siguiente temporada, la de 1996, marcó un antes y después en la vida de Armstrong. En la primera parte del calendario consiguió ganar la Flecha Valona, hito que hasta ese momento ningún estadounidense había conseguido. Sin embargo, a partir de esa actuación, el rendimiento del norteamericano empezó a bajar. Solo corrió cinco días en el Tour de Francia y defraudó en su participación en los , donde finalizó sexto en la contrarreloj y decimosegundo en la carrera en ruta.

En octubre de 1996, a la edad de 25 años, se le detectó un cáncer testicular con metástasis pulmonares y cerebrales. En su primera visita al urólogo en Austin, Texas, Armstrong presentaba distintos síntomas, entre ellos dolor testicular y sangre en la tos. Inmediatamente, el ciclista se sometió de urgencia a una operación quirúrgica en la que le extirparon un testículo y a ciclos de quimioterapia. Tras la cirugía, su doctor le informó que tenía menos de un 40% de probabilidades de sobrevivir.

Armstrong eligió una quimioterapia que a priori no disminuiría su capacidad pulmonar en caso de supervivencia. Esta elección fue considerada a la postre como vital para salvar su carrera deportiva. El ciclista recibió sus primeros tratamientos en el centro médico de la Universidad de Indiana, donde el doctor Lawrence Einhorn había desarrollado una técnica pionera en el tratamiento del cáncer testicular. Su primer oncólogo fue el doctor Craig Nichols. Además, en ese mismo centro, sus tumores cerebrales fueron extirpados quirúrgicamente. Finalmente, se sometió a su último ciclo de quimioterapia el 13 de diciembre de 1996.

Lance pudo recuperarse progresivamente hasta regresar en la París-Niza de 1998, enrolado en las filas del equipo US Postal. Tras el prólogo, abandonó la carrera y pensó en retirarse definitivamente de las competiciones deportivas, pero tras fuertes reflexiones y con el apoyo de su entrenador Chris Carmichael decidió seguir; planteándose como principal objetivo el Campeonato del Mundo que se celebraba en Valkenburg (Holanda).

Ese verano ganó el Tour de Luxemburgo y se puso a punto para la Vuelta a España, donde rindió a un gran nivel clasificándose en la cuarta posición. Si bien no ganó ninguna etapa, estuvo con los mejores tanto en la montaña como en las etapas contrarreloj, recuperando su autoestima y encontrándose en un gran estado de forma de cara al Campeonato del Mundo, en el que finalmente fue cuarto, con victoria para Oscar Camenzind.

De cara a 1999, Armstrong se encontraba pletórico de moral y su director le convenció que era posible incluso llegar a una meta más alta: vencer el Tour de Francia. Lance se presentó en la salida como un favorito de segunda fila y al final arrasó en la clasificación general por delante del suizo Alex Zülle, si bien éste se vio inmensamente perjudicado por una caída en la segunda etapa donde perdió más de seis minutos.

Entre 1999 y 2002, tanto Armstrong como su equipo iban a más (año tras año, Johan Bruyneel armaba un equipo cada vez mejor al servicio del estadounidense que incluyó a ciclistas como Viatcheslav Ekimov, George Hincapie, Tyler Hamilton, José Luis "Chechu" Rubiera y Roberto Heras), mientras que sus principales rivales, el alemán Jan Ullrich y el italiano Marco Pantani, ganadores de los Tours anteriores, apenas le hicieron sombra. Ullrich fue segundo dos veces por detrás de Armstrong (en 2000 y 2001), y Pantani consiguió arrancarle un par de victorias de etapa en el Tour 2000 antes de abandonar dicha competición, justo cuando ya caía en picado tanto en su carrera deportiva como en su vida personal. Pero ninguno de los dos inquietaron a Armstrong en esos primeros 4 Tours. Ni siquiera el corredor revelación de aquellos años, el español Joseba Beloki, podio durante 3 años seguidos, pudo seguir la estela de un Armstrong sobrehumano. Solamente el francés Richard Virenque y el español Roberto Heras consiguieron darle un pequeño susto en la 15ª etapa del Tour 2000, donde Armstrong tuvo el único momento malo en esos 4 años. Tanto le impresionó Heras en aquel Tour, que, tirando de talonario, lo fichó para su propio equipo al acabar la temporada.

Y en 2003, el texano llegó a la edición del Tour del Centenario con la mira puesta en igualar la marca de cinco Tours consecutivos de Miguel Indurain. Y pese a que un resucitado Jan Ullrich llegó a ponerle contra las cuerdas, lo logró. Aquel año fue el único en el que Armstrong mostró cierta debilidad, una debilidad que sus principales rivales aprovecharon para acercársele. Ullrich quedó 2º en la general a sólo un minuto, además de ganarle una etapa contrarreloj por más de 1:30 minutos; Joseba Beloki también le atacó como nunca antes, hasta que llegó el desgraciado accidente que sufrió en la etapa con final en Gap, que ganó el kazajo Alexandre Vinokourov, a la postre, tercero en París. Incluso su anterior gregario, su compatriota Tyler Hamilton, le atacó en alguna etapa.

En 2004 Armstrong aspiraba a hacer más grande su leyenda, y lo hacía más cuestionado que nunca, tras el bajo rendimiento que había mostrado durante su preparación para el Tour. Pero el ciclista estadounidense volvió a sorprender con una autoridad aplastante, ganando 5 etapas y acabando a más de seis minutos del segundo, el alemán Andreas Klöden. Jan Ullrich sólo pudo ser cuarto, mientras que el resto de los favoritos sobre el papel abandonaron.

Y en 2005 consiguió lo inimaginable, vencer por séptima vez la ronda gala. Con su nuevo equipo, el Discovery Channel, heredero del US Postal, se impuso en el Tour de Francia 2005, esta vez sobre el italiano Ivan Basso, mientras que Ullrich, que tuvo un accidente el día antes de empezar, fue tercero.

El 18 de abril de 2005 había anunciado en rueda de prensa en Georgia que se retiraba en julio, tras el Tour, pese a tener un año más de contrato con su equipo, el Discovery Channel. Tras su séptima victoria consecutiva en la ronda gala, Armstrong se retiraba como campeón invicto.

Entre los numerosos premios que se le han concedido hay que destacar el Premio Príncipe de Asturias de los Deportes en el año 2000.
En 2006 Armstrong corrió la maratón de Nueva York con el fin de conseguir fondos para la lucha contra el cáncer, con un tiempo de 2 horas 59 minutos 37 segundos, a unos 50 minutos del ganador, el brasileño Marilson Gomes dos Santos (2h 09:58).

El 8 de septiembre de 2008 Armstrong anunció su vuelta al ciclismo profesional en el año 2009 con el objetivo de potenciar la lucha contra el cáncer. El 24 de septiembre de 2008, anunció la fecha de su vuelta, el 18 de enero de 2009 en la Down-Under de Australia, con el equipo Astaná, que dirigía Johann Bruyneel, y en el que militaba Alberto Contador. Posteriormente, el 1 de diciembre, en la concentración de su equipo, el Astaná, anunció que correría el Tour de Francia 2009. En marzo de 2009, como preludio al Giro de Italia, Armstrong participó en la Vuelta a Castilla y León, en donde tuvo que retirarse en la primera etapa tras sufrir una caída a su paso por Antigüedad (Palencia), fracturándose la clavícula. Con motivo del accidente, se erigió un monumento en la pequeña localidad palentina.
Finalizó en el puesto número 11º el Giro de Italia, que corre como preparación para el Tour de Francia, en el que finalizaría tercero y en el que ganó junto con sus compañeros del equipo Astaná la contrarreloj por equipos de la cuarta etapa y la clasificación general final por equipos.

Antes de la finalización del Tour 2009, anunció que crearía en 2010 junto con Johan Bruyneel un nuevo equipo patrocinado por RadioShack Corporation, el Team RadioShack.

Sin embargo, su actuación en el Tour de Francia 2010 con este nuevo equipo no fue como se esperaba. Mostró gran debilidad en la alta montaña y acabó 23º en la general. Chris Horner, Levi Leipheimer y Andreas Klöden, fichados como gregarios de lujo para ayudarle a conseguir su 8º Tour, acabaron por delante de él en la clasificación general.

Poco después, Armstrong anunció que ése sería su último Tour de Francia. Y a comienzos de 2011, manifestó que la última carrera de prestigio en la que participaría sería el Tour Down Under 2011, ya que después se dedicaría exclusivamente a carreras de segundo nivel en los Estados Unidos.

En 2011 anuncia su retirada definitiva.

Armstrong fue acusado de prácticas dopantes en varias ocasiones. En 2001, fue criticado por trabajar con el controvertido preparador y médico Michele Ferrari. Uno de sus críticos fue Greg Lemond (compatriota de Armstrong y triple campeón del Tour), quién se declaró "desolado" tras conocer la noticia de que el propio Armstrong había admitido trabajar con Ferrari. Esto llevó a una enemistad entre ambos con acusaciones de uno y otro lado. Mientras, el por entonces organizador del Tour Jean-Marie Leblanc confesó que "no estaba feliz" de que los nombres de Armstrong y Ferrari "estuvieran mezclados". Cuando el doctor Ferrari fue condenado por "fraude deportivo", Armstrong rompió su relación profesional con el médico ya que, según comentó, sentía "cero tolerancia por alguien condenado por usar o facilitar drogas dopantes" a pesar de que durante su colaboración "nunca le había sugerido, recetado o facilitado ninguna droga". Ferrari fue posteriormente absuelto de todos los cargos por un tribunal italiano de apelación.
En 2004, los periodistas Pierre Ballester y David Walsh publicaron en un libro que Armstrong había utilizado sustancias dopantes ("L. A. Confidentiel - Les secrets de Lance Armstrong"). La obra contenía el testimonio de la masajista del corredor Emma O'Reilly, quién aseguró que el ciclista le pidió que tirara jeringuillas usadas y le solicitó maquillaje para ocultar las marcas de las agujas en los brazos. Otro entrevistado en el libro, Steve Swarts, comentó que él y otros corredores del equipo Motorola, incluyendo a Armstrong, habían empezado a recurrir al dopaje a partir de 1995, acusación desmetida por otros miembros de la escuadra. Estas partes del libro fueron a su vez reproducidas en junio en la versión dominical del Times británico, The Sunday Times. Armstrong demandó por difamación al periódico. El caso se resolvió con un acuerdo extrajudicial que determinó que el artículo "contenía acusaciones de culpabilidad pero sin dar motivos que justificaran" las afirmaciones. Finalmente, los abogados del periódico declararon que "The Sunday Times ha confirmado a Mr. Armstrong que nunca había sido su intención tratar de acusarle de haber consumido sustancias dopantes y que, por tanto, le presentaba sus sinceras disculpas".

En marzo de 2005, Mike Anderson presentó una denuncia en la Corte del Distrito del Condado de Travis, en Texas, tras dejar de trabajar para Armstrong en noviembre de 2004. Anderson trabajó para el ciclista dos años como asistente personal. En su escrito, el ex empleado afirmó haber encontrado en febrero de 2004, en el cuarto de baño del domicilio en España de Lance Armstrong, en Gerona, una caja con el título "androsterina', un esteroide anabolizante.El ciclista rechazó las acusaciones. Finalmente, el deportista y su ex asistente resolvieron el caso con acuerdo extrajudicial. Los términos del acuerdo no se hicieron públicos.

Por otra parte, estuvo enfrentado en multitud de ocasiones con el diario L'Equipe. El primer enfrentamiento surgió a raíz de unas acusaciones de dopaje, en su reaparición tras el cáncer, en el Tour de 1999 que Armstrong terminó ganando. Este y otros enfrentamientos posteriores hicieron correr ríos de tinta en Francia, pero sobre todo una investigación posterior a la retirada de Lance, que fue llevada a cabo en 2006, por el diario L'Equipe, en la que un periodista de dicho diario aseguraba que en las muestras de orina de Lance Armstrong tomadas en 1999 fueron supuestamente encontrados restos de la sustancia EPO, que eran imposible de ser detectados en aquella época. Dichos restos eran supuestos ya que no se había dado a conocer el número del código de la muestra B, la cual contenía la muestra de orina. Armstrong se defendió achacando la noticia a cierta envidia por parte de los franceses, y en particular del diario L'Equipe.
En junio de 2006, la Unión Ciclista Internacional (UCI) reiteró la validez del "informe Vrijman", que exculpaba a Lance Armstrong, sosteniendo que "los análisis que se realizaron a las muestras de orina fueron llevados a cabo de manera incorrecta y muy alejada de los criterios científicos", y que era "totalmente irresponsable" sostener que "constituyen una prueba de algo". La UCI, además de dar por bueno el informe Vrijman, confirmando la inocencia de Armstrong, acusó a la AMA de haber facilitado informaciones confidenciales al diario deportivo L'Équipe.

En junio de 2006, el periódico francés Le Monde publicó las acusaciones del ex compañero de Armstrong Frankie Andreu y su mujer, Betsy. El matrimonio aseguró que el ciclista había admitido consumir sustancias dopantes justo después de pasar por el quirófano durante su tratamiento para vencer al cáncer, en 1996. Las declaraciones de la pareja fueron hechas bajo juramento durante el procedimiento judicial de arbitraje iniciado por Lance Armstrong contra la compañía de seguros SCA Promotions, que se negaba a pagarle la prima de cinco millones de dólares por su victoria en el Tour de 2004 al considerar que existían sospechas fundadas de dopaje. La mujer de Andreu relató: "El médico comenzó a hacerle preguntas banales y, de pronto, le soltó: ¿Ha tomado productos dopantes?. Y respondió que sí. Le preguntó cuáles y Lance respondió: EPO, hormonas del crecimiento, cortisona, esteroides y testosterona". Su marido confirmó el testimonio. "No sé cómo el doctor hizo la pregunta, pero la respuesta fue que había tomado EPO, testosterona...". Armstrong sostuvo que Betsy Andreu podría haberse confundido por una posible mención al tratamiento posterior a la operación, que incluía esteroides y EPO para contrarrestar parte de los efectos de la quimioterapia. El relato de los Andreu no fue respaldado por ninguna de las otras ocho personas presentes, incluyendo el doctor de Armstrong Craig Nichols, Sin embargo, y de acuerdo a Greg Lemond, enfrentado con Armstrong, existe una conversación grabada con Stephanie McIlvain, el contacto del ciclista con Oakley, uno de sus patrocinadores, que dice: "Estuve en esa habitación y lo escuché". McIlvain negó todo en su testimonio bajo juramento.

A principios de 2012 comenzó un proceso que culminó con la anulación de todos los resultados conseguidos por Armstrong desde el 1 de agosto de 1998 en adelante, incluyendo una sanción de por vida para la práctica del deporte profesional:







Luego de que la entrevista con Oprah saliera al aire, el mundo del deporte y el espectáculo reaccionó casi de manera inmediata lanzando fuertes críticas al ciclista estadounidense.





1992

1993

1995

1996

1998

No corrió entre 2006 y 2008 (se había retirado a finales de 2005).








</doc>
<doc id="8817" url="https://es.wikipedia.org/wiki?curid=8817" title="Persea americana">
Persea americana

Persea americana, conocida comúnmente como aguacate o palta es una especie arbórea del género "Persea" perteneciente a la familia Lauraceae,originaria de Mesoamérica y domesticada por los pueblos indígenas de esa zona hace unos 7,000 años, pero con evidencias de uso en Coaxcatlán (México) hace unos 10.000 años.
La especie se cultiva en lugares con climas tropical y mediterráneo.

En estado silvestre, el árbol puede alcanzar alturas de alrededor de 20m, más comúnmente entre 8 y 12m, y un diámetro a la altura del pecho de 30-60cm, con tronco erecto o torcido. Los árboles en plantación, generalmente derivados de injertos y sujetos a podas de formación, muestran una apariencia muy distinta. Copa: extendida, globulosa o acampanulada, con ramas bajas, ramas jóvenes al principio, de color verde amarillento, que después se tornan opacas y con cicatrices prominentes dejadas por las hojas. Corteza: áspera, a veces surcada longitudinalmente. 

El tronco posee una corteza gris-verdosa con fisuras longitudinales. Las hojas, alternas con peciolo de 2 a 5 cm y limbo generalmente glauco por el envés. Estrechamente elípticos, ovados u obovados de 8 a 20 cm por 5 a 12 cm y son coriáceos, de color verde y escasamente pubescentes en el haz, aunque muy densamente por el envés, que es de color marrón amarillento y, donde resalta el nervio central. Tiene base cuneiforme y ápice agudo. Los márgenes enteros y más o menos ondulados.

Las inflorescencias son panículas de 8 a 4 cm de largo, con flores de 5 a 6 mm, con perianto densamente pubescente, de tubo muy corto y con seis tépalos oblongos, de medio centímetro, siendo los tres exteriores más cortos. Tienen nueve estambres fértiles de unos 4 mm, con filamentos pubescentes, organizados en tres círculos concéntricos. El ovario es ovoide, de aproximadamente 1,5 mm, densamente pubescente, con estilo también pubescente de 2,5 mm, terminado por un estigma discoidal algo dilatado.

El fruto es tipo baya, oval o piriforme, según la variedad, de tamaño muy variado (7 a 33cm de largo y hasta 15cm de ancho), cáscara de color verde a púrpura oscuro, pudiendo ser delgada, gruesa, lisa o ligeramente rugosa, a veces con una apariencia como la del cuero. Pulpa firme, oleíca, de un color que varía desde el amarillo al verde claro. Contiene una semilla grande (5 a 6,4cm), dura y pesada, redonda o puntuda, de color marfil. Tiene dos envolturas papelosas de color café, muy delgadas, que a menudo se quedan adheridas a la pulpa. El fruto es generalmente en forma de pera, a veces ovoide o globoso, de 8 a 18 cm, con epicarpio corchoso más o menos tuberculado y mesocarpio carnoso y comestible. Este último rodea íntimamente una semilla globular de episperma (tegumento) papiráceo, sin endosperma, de unos 5 a 6 cm

Los requerimientos de clima y suelo varían con las diferentes razas. La raza antillana prefiere clima tropical húmedo, y se cultiva desde el nivel del mar hasta los 800m de altura, con temperaturas promedio de 24 a 26ºC . Es muy susceptible a las heladas. La raza guatemalteca crece entre 500 y , con temperaturas medias de 22 a 25ºC y puede  tolerar temperaturas no inferiores a 4,5ºC. La raza mexicana puede crecer hasta los , con temperaturas medias de 20ºC y puede tolerar heladas hasta de –4ºC. "P. americana" crece en climas secos a húmedos, con precipitaciones de 800 a 2000mm, con estaciones secas bien definidas de hasta 6 meses, aunque crece mejor con estaciones secas más cortas. Por otro lado, requiere más de tres meses secos para buena producción de frutas. Los periodos de calor y sequía pueden provocar la caída de los frutos, especialmente en las variedades de montaña. Los sitios demasiado húmedos no son apropiados, por la mayor posibilidad de ocurrencia de ciertas enfermedades del suelo, a las cuales la especie es altamente susceptible. Se adapta a gran variedad de suelos, desde arenosos hasta arcillosos, limos volcánicos, lateríticos y calizos, pero crece mejor en suelos francos, bien drenados, ligeramente ácidos y ricos en materia orgánica. La raza antillana tolera suelos calizos y ligeramente salinos. Ninguna variedad tolera suelos pesados, con drenaje deficiente y no debe plantarse cuando la capa freática esté a menos de 1m de la superficie. El rango óptimo de pH se considera entre 6 y 7, aunque algunos cultivares en Florida crecen bien en suelos con pH de 7,2 a 8,3.

La evidencia genética permite sostener que el proceso de domesticación de "P. americana" ha ocurrido más de una  vez y que la variedad mexicana "P. americana" var. "dryimifolia" es una de las variedades que ha aportado material genético a los cultivares modernos de aguacate. La diversidad genética al interior de "P.  americana" es elevada. La mayor parte de los estudios están basados en el análisis de poblaciones o colectas correspondientes a  las maneras de cultivar el aguacate y se ha encontrado mayor diversidad genética en los cultivares de Mesoamérica, con relación a lugares donde no hay poblaciones silvestres del género "Persea". Por otra parte, hasta la fecha no se han estudiado suficientemente las poblaciones silvestres y existe un déficit, en el conocimiento de la variación genética en las variedades nativas.

Los antecesores del género "Persea" surgieron en la parte septentrional de América del Norte, pero entre el Mioceno y el Plioceno emigraron hacia Mesoamérica. Se piensa que la especiación que dio lugar a la "Persea americana," puede haber tenido como factor principal los procesos geológicos ocurridos en México. La evidencia fósil, sugiere que especies similares se extendieron aún más, hasta el norte de California (EE.UU.), hace millones de años , en un momento en que el clima de esa región era más propicio.

Existen evidencias de su consumo en el valle de Tehuacán (Puebla, México), que tienen entre 9000 y 10000 años de antigüedad. Su domesticación ocurrió en la región mesoamericana, alrededor del año 5000 a. C. y alrededor del año 3000 a. C., se consumía en Caral, en el actual Perú.
Se divide en tres variedades: "mexicana, guatemalteca" y "antillana".

Los ejemplares de "P. americana" originarios de las zonas altas del centro y del este de México, generan la variedad "mexicana".

Los árboles originarios de las zonas altas de Guatemala generan la variedad "guatemalteca".

La variedad "antillana" proviene de la zona de las Antillas y se cree que fue la primera variedad encontrada por los europeos, al ser esa la primera zona a la que llegaron.

Existen discrepancias con respecto al origen de la raza antillana, puesto que también cabe la posibilidad de que los primeros ejemplares de aguacate hubieran sido introducidos en las Antillas desde México por los españoles o los ingleses durante la colonización.

Las tres variedades de "P. americana" se fueron mezclando naturalmente entre ellas por medio de su propio sistema de reproducción. El resultado de estas fusiones, producidas por medio de la polinización cruzada, dieron origen a incontables variedades híbridas naturales indefinidas.

A partir de la década de 1900, se comenzaron a seleccionar los ejemplares de esta especie, con mejores atributos para ganar consumidores en los mercados, dando origen a los distintos cultivares que encabezaron los mercados mundiales hasta los años 1930. Las nuevas variedades se comercializaron bien, hasta que en 1935 se patentó en Estados Unidos una nueva variedad llamada "Hass", de progenitores desconocidos, originada en La Habrá, un lugar de California, donde Rudolph Hass la detectó entre los árboles de su huerto.

La palabra "aguacate" proviene del náhuatl "ahuacatl" [aːwakat͡ɬ], que se remonta a la proto-azteca * PA:WA. que también significaba "aguacate" A veces la palabra náhuatl se utiliza con el significado «testículo», probablemente debido a la semejanza entre la fruta y la parte del cuerpo.
Se conoce con este nombre, y sus derivados, al fruto de "Persea americana" en México, Paraguay, Venezuela, Colombia, Estados Unidos, Centroamérica, el Caribe, España y los países anglosajones y lusófonos.

La palabra guacamole proviene del náhuatl "ahuacamolli", ‘salsa de aguacate’. También es conocida como aguaco o ahuaca.

Con este nombre se le conoce principalmente en Argentina, Bolivia, Chile, Perú y Uruguay.

La palabra «palta» proviene del quechua, siendo el nombre con el que se conoce a una etnia amerindia, los paltas, que habitaron en la provincia ecuatoriana de Loja y al norte de Perú. Probablemente esta sea la región descrita como la «provincia de Palta» por el Inca Garcilaso de la Vega en su obra "Comentarios Reales de los Incas" de 1601.

La región de los paltas fue conquistada por Tupac Inca Yupanqui durante su marcha para conquistar la Provincia de Cañar. Ese sería el origen del nombre con que los incas bautizaron al fruto de esta especie, traído de la zona norte de su imperio. También el tiempo aproximado en que el árbol llegó de Ecuador a Perú, ya que se sabe, que la conquista de las provincias norteñas por Tupac Yupanqui ocurrió entre 1450 y 1475.

Los escritos españoles mencionaron este fruto por primera vez en 1519.

Actualmente, "P. americana" tiene una amplia distribución y mercado.













En 2014, se produjo 5 millones de toneladas, con México sola tiene el 30 % (1,52 millones de t) del total (tabla). Otros grandes productores son: República Dominicana, Perú, Indonesia, Colombia, totalizando 1,38 millones t o el 27 % de la producción mundial (tabla).

En EE. UU., el consumo per cápita ha crecido de 1 kg en 2001; a 3 kg en 2016.

Algunos sujetos presentan reacciones alérgica al aguacate. Existen dos formas principales de alergia: las personas con alergia al polen de los árboles desarrollan síntomas locales en la boca y la garganta poco después de comer aguacate; el segundo, conocido como síndrome de la fruta del látex, se relaciona a la y los síntomas incluyen urticaria generalizada, dolor abdominal y vómito, y a veces es potencialmente fatal.

El árbol del aguacate requiere para su mejor sanidad y desarrollo radicular, un suelo permeable y profundo, franco-arenoso, en lo posible sin presencia de calcáreos ni cloruros. La siembra se debe realizar en zonas no inundables ni propensas a encharcamientos puesto que el exceso de humedad le afecta negativamente. Con respecto al clima, se deben evitar zonas de heladas ya que estas afectan la floración y si son muy intensas pueden llegar a perjudicar a las plantas.

La principal y más importante enfermedad de "P. americana" es la "podredumbre de la raíz", producida por el hongo "Phytophthora cinnamomi". Esta enfermedad está presente en casi todas las zonas productoras del mundo.

Los sistemas actuales para controlar esta afección incluyen por una parte lograr una "resistencia genética" a través del uso de portainjertos tolerantes a "Phytophthora" y, por otra, incorporar un programa de tratamientos fitosanitarios con la aplicación de distintos fungicidas en aplicaciones foliares y al suelo, combinadas con pinturas al tronco cuando la planta es joven. De ahí en adelante se requiere del uso de distintas mezclas químicas ácidas que, mediante inyecciones al tronco, se incorporan a los vasos internos de conducción. El conjunto de estas técnicas bien aplicadas está permitiendo un buen nivel de control de esta enfermedad.

Posee un alto contenido en aceites vegetales, por lo que se le considera un excelente alimento en cuanto a nutrición en proporciones moderadas, ya que posee un gran contenido calórico y graso. Además se ha descubierto que el aceite de aguacate posee propiedades antioxidantes. Es rico en grasa vegetal que aporta beneficios al organismo y en vitaminas E, A, B1, B2, B3, ácidos grasos, proteínas, minerales.

El fruto de "P. americana" ha sido utilizado principalmente como alimento.

En México y Centroamérica, el aguacate es importante y tradicional en la dieta diaria desde antes de la llegada de los europeos. Se utiliza como acompañamiento para el pan, como ingrediente de ensaladas, como guarnición y para preparar guacamole, entre muchos otros usos. En Tocumbo, Michoacán, es utilizado además para preparar "patatitas francesas".

Se cultivan distintas variedades con diferentes características, como el color y grosor de la piel o el tamaño del fruto. Sus hojas frescas o secas se emplean como condimento de varios platos, como la barbacoa, los mixiotes y las enfrijoladas.

En Chile, existe un tipo de aguacate de piel negra llamado a veces "palta chilena". Es un alimento bastante consumido y utilizado de variadas formas en la gastronomía de este país. El aguacate se utiliza como acompañamiento en comidas, como ingrediente de ensaladas, o incluso como acompañamiento para el pan, consumiéndose generalmente en recetas saladas. Es muy común uso en los llamados completos (nombre dado en este país a los perros calientes).

En el Perú, la palta que se produce mayoritariamente es una variedad verde que es originaria del propio país. El tamaño del fruto puede llegar hasta los 15 centímetros, según la zona de producción. Se usa para preparar palta rellena, palta rellena con camarones, sopa de palta y como acompañamiento en diversos platos de la gastronomía peruana.

En Venezuela se utiliza principalmente como acompañante sazonándolo con sal y en ensaladas, así como en la elaboración de la arepa Reina Pepiada y la guasacaca.

En Colombia el aguacate más extendido es de piel verde y carne amarilla. Se usa en ensaladas (con tomate, cebolla y cilantro), en guacamole, solo o con un poco de sal al gusto, como acompañante de comidas como el seco y para sancochos, bandeja paisa y ajiaco santafereño. Es muy apreciado la variedad de aguacate carmero, de la región del municipio de Carmen de Bolívar.

En Argentina, el aguacate que se consume es por lo general, del mismo tipo que el chileno. De cáscara negra en su madurez, es un alimento que suele comerse mucho en el litoral del país, usándose como aderezo. También se emplea para hacer "dulce de palta", que se logra pelando la fruta, sacando la pulpa, haciéndola puré y agregando azúcar. Posee un sabor agridulce y se usa acompañar la comida o bien para su consumo directo con pan.

Un uso tradicional de "P. americana", menos popular, es el de planta medicinal. Su fruto y sus aceites son ampliamente utilizados como productos de belleza, tanto para la piel como para el cabello, y sus hojas para la elaboración de expectorantes.

El aguacate se produce aproximadamente en 46 países. La superficie total cosechada en el mundo alcanzó las 436.3 millones de hectáreas en 2009, siendo, en orden de importancia, México, Indonesia, República Dominicana, Estados Unidos, Colombia, Perú, Kenia los principales productores. Particularmente México el principal productor, superando el millón de toneladas anuales (1 millón 316 mil 104 toneladas en 2012), seguido por Indonesia y República Dominicana. Asimismo, México es considerado el más importante "distribuidor" a nivel mundial, participando con el 51.4 % del mercado de exportaciones abasteciendo así a gran parte de la población mundial. América concentra el 60 % de las plantaciones mundiales. Tan sólo en México, se produce en 28 entidades federativas, siendo Michoacán la más importante de ellas, con un 85.9 % de la producción total en 2009. El 95 % de la producción nacional se concentra en los estados de Michoacán, Jalisco, Nayarit, Edo. de México y Morelos. Los cultivos se realizan en sierras muy fértiles, semihúmedas. En estas zonas los inviernos son fríos y durante el verano la temperatura raramente rebasa los 32 °C. El frío abajo de 4 °C daña la flor y por lo tanto la producción, en este microclima se produce el aguacate de mejor calidad.

Por su parte, entre los principales países exportadores de aguacate se encuentra México, con el 51.4 % del mercado, le siguen en menor medida Israel (11.6 %), Perú (15 %) y Sudáfrica (8.0 %). En 2010, los principales países importadores de aguacate fueron los Estados Unidos(47.1 %), Francia (12.8 %), Japón (6.1 %) y Canadá (4.9 %), los cuales concentran 70.8 % de la importación total. La comarca de la Axarquía, perteneciente a la provincia de Málaga es la principal zona productora de aguacates en el país, además se considera la reserva tropical europea. En el estado de Michoacán, la región que comprende los municipios de Tancítaro, Uruapan y Peribán, es la número uno a nivel nacional e internacional en producción de esta fruta, conociéndose como la capital mundial del Aguacate. Cabe destacar que dicha región es la propicia para la producción de aguacate ¨Hass¨ debido a su clima cálido-húmedo en verano y frío en invierno sin rebasar las temperaturas de 4 grados centígrados, la variedad ¨Méndez¨ se da en un clima más cálido aún y a una altura menor, pero la fruta es de menor calidad, en cuanto a tamaño, pulpa y sabor.

Los líderes del comercio internacional son Israel, Sudáfrica y España, países que fueron los principales exportadores desde 1993. El comercio mundial de aguacate se ha incrementado significativamente desde 1980, y en el caso de México se ha limitado a los EE. UU. y Europa. Japón ha comenzado a importar grandes volúmenes del fruto, siendo el principal importador en Asia.

Los principales abastecedores de Europa son Israel, Chile, Perú y Sudáfrica. México exporta a 21 países, principalmente Estados Unidos, Japón, Canadá, América Central y Europa. Cabe destacar que en países fríos como Dinamarca o Rusia no se produce por las condiciones polares, (aunque hay zonas que no son polares y con frío) igual se exporta desde otros países para crear recetas con este producto.



</doc>
<doc id="8822" url="https://es.wikipedia.org/wiki?curid=8822" title="PCMCIA">
PCMCIA

PCMCIA es el acrónimo de Personal Computer Memory Card International Association: "Asociación Internacional de Tarjetas de Memoria para Computadoras Personales".

Existen muchos tipos de dispositivos disponibles con formato de tarjeta PCMCIA: módems, tarjetas de sonido, tarjetas de red.

La proliferación de los diferentes equipos portátiles, como las computadoras portátiles o las PDA (agendas electrónicas), hizo necesario el desarrollo de dispositivos requeridos para todo tipo de equipos portátiles. Se dio prioridad al desarrollo de tarjetas estándar, diseñadas para la compatibilidad entre dispositivos periféricos, como complementos de memoria y módems, que eran propiedad única y exclusiva de dicha marca, excluyendo el uso de dispositivos similares hechos por otros fabricantes. Generalmente, estos dispositivos periféricos no fueron diseñados para ser intercambiados con otras computadoras.

Como resultado de ello hubo una expansión en la industria, a fin de que los fabricantes de computadoras pudieran normalizar cada máquina y su capacidad. Este método permitiría a los usuarios seleccionar a sus proveedores y también compartir periféricos con otras computadoras. Originalmente estos dispositivos eran principalmente tarjetas de memoria. Estas tarjetas de memoria se utilizan a veces en lugar de disquetes para intercambiar datos o ampliar la memoria del sistema informático. La funcionalidad de estas tarjetas se ha ampliado más allá de las tarjetas de memoria, que sumado a su facilidad de uso, tamaño compacto, compatibilidad de plataformas, y aplicaciones ha derivado en una creciente popularidad en los últimos años.

PCMCIA (Personal Computer Memory Card International Association) se fundó en 1989 con el objetivo de establecer estándares para circuitos integrados y promover la compatibilidad dentro de las computadoras portátiles, donde solidez, bajo consumo eléctrico y tamaño pequeño son los factores más importantes. Sus miembros incluyeron Dell, Hewlett-Packard, IBM, Intel, Lexar Media, Microsoft, SCM Microsystems y Texas Instruments.

A medida que las necesidades de los usuarios de computadoras portátiles han cambiado, también el estándar de las tarjetas de PC. Ya en 1991 PCMCIA había definido una interfaz I/O (entrada/salida) para el mismo conector de 68 pines que inicialmente se usaba en las tarjetas de memoria. A medida que los diseñadores se daban cuenta de la necesidad de un software común para aumentar la compatibilidad, se fueron añadiendo primero las especificaciones de servicios de "socket", seguidos de los servicios de especificación de tarjeta.

La PCMCIA ha disuelto 2009/2010. Las especificaciones han sido adoptadas por el USB Implementers Forum (USB-IF).


estamos



</doc>
<doc id="8826" url="https://es.wikipedia.org/wiki?curid=8826" title="Ronald Reagan">
Ronald Reagan

Ronald Wilson Reagan (pronunciado como /ˈrɑnəld ˈwɪlsən ˈreɪgən/; Tampico, 6 de febrero de 1911-Bel-Air, Los Ángeles, 5 de junio de 2004) fue un actor y político estadounidense, el cuadragésimo presidente de los Estados Unidos entre 1981 y 1989 y el trigésimo tercer gobernador del estado de California entre 1967 y 1975.

Nacido en Illinois, Reagan se mudó a Los Ángeles, California, en 1928, donde trabajó como actor, fue presidente del "Screen Actors Guild" ("Sindicato de Actores de Pantalla" o SAG por sus siglas en inglés) y portavoz de la compañía multinacional de infraestructuras, servicios financieros y medios de comunicación, "General Electric" (GE). Su inicio en la política ocurrió durante su trabajo en GE; originalmente miembro del Partido Demócrata, se cambió al Partido Republicano en 1962 a la edad de 51 años. Después de pronunciar un entusiasta discurso en apoyo a la candidatura presidencial de 1964 de Barry Goldwater, fue persuadido para lograr la gobernación de California, ganándola dos años después y por segunda vez en 1970. Fue vencido en su carrera por la nominación republicana presidencial en 1968 y en 1976, pero ganó tanto la nominación como las elecciones en 1980, convirtiéndose en Presidente de los Estados Unidos de América.

Como presidente introdujo nuevas y osadas iniciativas políticas y económicas. Su política económica, entroncada en la llamada "economía de la oferta", se haría famosa bajo el nombre de "reaganomics", caracterizada por la desregularización del sistema financiero y por las rebajas substanciales de impuestos implementadas en 1981. En su primer período, sobrevivió a un intento de asesinato, marcó una línea dura con los sindicatos y además ordenó acciones militares en la independiente isla caribeña de Granada, próxima a la costa de Venezuela. Fue reelegido con una gran mayoría en las elecciones de 1984. El segundo período de Reagan estuvo marcado principalmente por asuntos extranjeros, siendo los más importantes el fin de la Guerra Fría, el bombardeo de Libia y la revelación del escándalo Irán-Contras. Previamente el presidente había ordenado un masivo incremento militar para la lucha estrecha contra la Unión de Repúblicas Socialistas Soviéticas (URSS), dejando atrás la estrategia de la "détente". Describió públicamente a la URSS como el "imperio del mal" y apoyó movimientos anticomunistas en todo el mundo a través de la denominada Doctrina Reagan. Negoció el Tratado INF para el desarme nuclear con el secretario general soviético Mijaíl Gorbachov, logrando la reducción de los arsenales nucleares de ambos países.

Reagan dejó el cargo en 1989. En 1994 el expresidente reveló que le habían diagnosticado la enfermedad de Alzheimer a comienzos de ese año. Falleció diez años después a la edad de noventa y tres, siendo uno de los expresidentes más longevos del país.

Ronald Reagan nació en el edificio del banco local en Tampico, Illinois, el 6 de febrero de 1911. Sus padres fueron John "Jack" Reagan y Nelle Wilson Reagan. Cuando era pequeño, su padre le puso el apodo de "Dutch", debido a su parecido a un "gordito Dutchman", y su corte de pelo "Dutchboy"; el sobrenombre siguió a Ronald durante su juventud. La familia de Reagan vivió brevemente en diferentes pueblos y ciudades en Illinois, incluyendo Monmouth, Galesburg y Chicago, hasta que en 1919 regresaron a Tampico y vivieron encima de la H.C. Pitney Variety Store. Después de su elección como presidente, cuando residía en el segundo piso de la Casa Blanca, Reagan diría que estaba "nuevamente viviendo encima de la tienda".
De acuerdo a Paul Kengor, escritor de "God and Ronald Reagan", Reagan tenía particularmente una profunda fe en la bondad de las personas, que venía de la fe optimista de su madre, Nelle, y de la fe de los Discípulos de Cristo, en la que fue bautizado en 1922. Cuando Reagan tenía 11 años, su madre le regaló el libro "That Printer of Udell's" y él dijo que ese libro le inspiró para convertirse en un cristiano evangélico; a la edad de 66, Reagan afirmó que el libro "le dejó una inmutable creencia en el triunfo del bien sobre el mal". Para su época, Reagan tenía un comportamiento inusual en su oposición a la discriminación racial, y recuerda una ocasión en que en una hostería local en Dixon no se permitió a un grupo de personas negras que pasaran la noche. Reagan les llevó a su casa, donde su madre les invitó a quedarse por la noche y les dio desayuno en la mañana siguiente.

Tras la clausura de la Tienda Pitney a fines de 1920, los Reagan se mudaron a Dixon, donde el "pequeño universo" del centroeste dejó una gran impresión en Ronald. Estudió en el Dixon High School, donde desarrolló un interés en la actuación, los deportes y en contar historias. Su primer trabajo fue como salvavidas en Rock River en Lowell Park, cerca de Dixon, en 1926. "Salvé 77 vidas," manifestó Reagan en una entrevista, y dijo que dejaba una marca en un trozo de madera cada vez que salvaba una vida. Después del High School, Reagan asistió al Eureka College, donde fue un miembro de la fraternidad Tau Kappa Epsilon, sacando su mención principal en economía y sociología. Fue muy activo en los deportes, incluyendo el fútbol americano.

Después de graduarse de Eureka en 1932, Reagan se trasladó a Iowa, donde trabajó en varias estaciones de radio locales. Le contrataron para transmitir los juegos locales de fútbol americano del equipo Hawkeyes de la University of Iowa. Le pagaban $10 por partido. Al poco tiempo se abrió un puesto de trabajo para miembros del equipo de anunciantes de la estación de radio WOC en Davenport, y contrataron a Reagan, ganando ahora $100 al mes. Debido a su voz persuasiva, se cambió a la radio WHO en Des Moines como locutor de los partidos de béisbol de los Chicago Cubs. Su especialidad era recrear jugada por jugada las noticias que la estación recibía por cable.

Mientras viajaba con los Cubs en California, Reagan realizó una prueba de cámara en 1937 que le proporcionó un contrato de siete años con el estudio Warner Brothers. Su primer crédito en una película fue con el rol protagónico de la película de 1937 "Love Is on the Air", y para finales de 1939 ya había trabajado en 19 películas. Antes de la película "Camino Santa Fe" de 1940, interpretó el rol de George "The Gipper" Gipp en la película "Knute Rockne, All American"; de ella obtuvo el sobrenombre "the Gipper," que le acompañaría durante su vida La interpretación favorita de Reagan la realizó en la película "Kings Row" de 1942, pero su interpretación no obtuvo una aprobación universal: un crítico sintió que Reagan había "estado solo casualmente en contacto con su personaje". Reagan también actuó en "Tennessee's Partner", "Hellcats of the Navy", "This Is the Army", "Dark Victory", "Bedtime for Bonzo", "Cattle Queen of Montana" y "The Killers" (su última película) en un remake de 1964.

Pasó la mayor parte de su carrera en la división de "películas B", donde Reagan bromeaba diciendo que los productores "no las querían buenas, las querían para el jueves". Si bien fue muchas veces eclipsado por actores más famosos, las películas de Reagan recibieron muchas críticas favorables.A finales de 1950, cuando las propuestas de cine son pocas, Ronald Reagan vuelve a la televisión. Se convirtió en presidente del sindicato de actores donde sus ingresos llegaron a 125,000 dólares por año a finales de 1950 (el equivalente al precio de una casa).Sus actividades como presidente del Screen Actors Guild (Sindicato de actores de cine) lo llevaron a la política mediante la alineación con el senador Joseph McCarthy y cooperando con el Comité Parlamentario de Actividades Antiamericanas (famosa caza de brujas ) donde denunció a varios de sus colegas supuestamente comunistas ante el senador Joseph McCarthy (1908-1957) quién desencadenó un extendido proceso de declaraciones, acusaciones infundadas, denuncias, interrogatorios, procesos irregulares pérdida del trabajo, negación del pasaporte, y listas negras contra personas sospechosas de ser comunistas. Décadas después según el historiador Richard Rovere, Reagan utilizó estas acusaciones para resolver disputas personales contra actores que lo habían opacado o que habían cuestionado su manejo opaco de las cuentas como presidente del Screen Actors Guild.

Después de completar catorce Cursos de Extensión del Ejército que se realizan en la casa, Reagan se alistó en el Ejército de Reserva el 29 de abril de 1937 y fue asignado en calidad de soldado a la Troop B, 322º de Caballería en Des Moines, Iowa. Fue nombrado Teniente Segundo en el Cuerpo de Oficiales de Reserva de Caballería el 25 de mayo de 1937, y el 18 de junio fue destinado al 323º Caballería.

Reagan fue ordenado al servicio activo por primera vez el 18 de abril de 1942. Debido a su miopía, fue clasificado para realizar solamente servicios limitados, lo que le excluía de servir fuera de Estados Unidos. Su primera misión fue en el San Francisco Port of Embarkation en Fort Mason, California, como oficial de enlace de la Port and Transportation Office. Tras la aprobación de la Fuerza Aérea del Ejército (AAF), solicitó que le transfirieran de la Caballería a la AAF el 15 de mayo de 1942, y fue asignado a las Relaciones Públicas de la AAF y seguidamente al 1st Motion Picture Unit (oficialmente, el "18º AAF Base Unit") en Culver City, California. El 14 de enero de 1943 fue promovido a Teniente Primero y enviado al Provisional Task Force Show Unit of "This Is The Army" en Burbank, California. Regresó al 1st Motion Picture Unit después de completar su servicio y fue ascendido a Capitán el 22 de julio de 1943.

En enero de 1944, el capitán Reagan fue llamado a servicio temporalmente en la Ciudad de Nueva York para participar en la inauguración del sexto War Loan Drive. Fue reasignado al 18º AAF Base Unit el 14 de noviembre de 1944, donde permaneció hasta el final de la Segunda Guerra Mundial. Fue recomendado para ser promovido a Mayor el 2 de febrero de 1945, pero la recomendación fue rechazada el 17 de julio de ese año. Regresó a Fort MacArthur, California, donde se licenció del servicio activo el 9 de diciembre de 1945. Al final de la guerra, su unidad había producido alrededor de 400 películas de entrenamiento para la AAF.

Reagan fue elegido por primera vez a la Junta de Directores del Screen Actors Guild en 1941, sirviendo como un alterno. Después de la Segunda Guerra Mundial, regresó a su puesto y en 1946 se convirtió en 3. Vicepresidente. La adopción de leyes de conflictos de intereses de 1947 llevaron a que el presidente y seis miembros de la Junta renunciaran; Reagan fue nominado en una elección especial para el cargo de presidente y resultó electo. Desde entonces sería escogido por los miembros para siete períodos de un año adicionales, desde 1947 a 1952 y después nuevamente en 1959. Reagan guió al SAG durante años marcados por grandes eventos, como las disputas entre los trabajadores y los patronos, la Ley Taft-Hartley, las audiencias de la Comité de Actividades Antiestadounidenses (HUAC) y la era de la lista negra de Hollywood.Participó durante el Macarthismo denunciando a actores, directores e intelectuales sospechosos de tener ideas de izquierda, que fueron perseguidos por sospechas, con acusaciones infundadas,interrogatorios,pérdida del trabajo y negación del pasaporte a los sospechosos de comunismo,o encarcelados,".

En 1947, como presidente del SAG, Reagan testificó, ante el Comité de la Cámara de Representantes sobre Actividades No Estadounidenses, respecto a la influencia comunista en la industria del espectáculo. Totalmente contrario al comunismo, reafirmó su compromiso con los principios democráticos, declarando: "Como ciudadano, vacilaría al ver a cualquier partido político proscrito por una ley que se basa en su ideología política. Sin embargo, si se prueba que una organización es un agente de poderes extranjeros, o en cualquier caso no es un partido político legítimo —y yo creo que el gobierno es capaz de probarlo- entonces estamos en otro tema... Pero al mismo tiempo yo nunca, como ciudadano, quiero ver a nuestro país urgido, por miedo o resentimiento hacia ese grupo, que nosotros no podemos comprometer ningún principio democrático por miedo o resentimiento

Aunque al principio tenía una postura crítica respecto a la televisión, Reagan obtuvo algunos papeles en el cine a fines de la década de 1950, por lo que se decidió a unirse al medio. Fue contratado como el anfitrión del "General Electric Theater", una serie semanal de dramas que se volvió muy popular. Su contrato le exigía que recorriera las plantas de GE por diez semanas al año, y usualmente le demandaban catorce discursos por día. Ganaba aproximadamente $125.000 al año (cerca de $1.000.000 en dólares de 2008) por su rol. Su trabajo final como actor profesional fue como anfitrión y actor de la serie de televisión "Death Valley Days", entre 1964 y 1965.

En 1938, Reagan coprotagonizó la película "Brother Rat" junto a la actriz Jane Wyman (1914–2007). Se comprometieron en el Chicago Theatre y contrajeron matrimonio el 26 de enero de 1940, en la Wee Kirk o' the Heather church en Glendale, California. Tuvieron dos hijos, Maureen (1941–2001) y Christine (26 de junio de 1947-27 de junio de 1947), y adoptaron a un tercer hijo, Michael (nacido en 1945). En 1948 Wyman solicitó el divorcio argumentando que se debía a las ambiciones políticas de Reagan, citando que era una distracción debida a las obligaciones de su marido con el sindicato SAG; el divorcio finalizó en 1949. Siendo así que Reagan fue el primer presidente de Estados Unidos divorciado.

Reagan conoció a la actriz Nancy Davis (1921-2016) en 1949 después de que ella le contactara en su calidad de presidente del Screen Actors Guild, para que la ayudara con un asunto relacionado con la aparición de su nombre en una lista negra comunista en Hollywood (había sido confundida por otra Nancy Davis). Ella describió el encuentro diciendo: "No sé si habrá sido exactamente amor a primera vista, pero fue muy cercano a ello". Se comprometieron en el restaurante Chasen's en Los Ángeles y contrajeron matrimonio el 4 de marzo de 1952 en la Little Brown Church en el San Fernando Valley. El actor William Holden fue el padrino en la ceremonia. Tuvieron dos hijos: Patti (nacida en 1952) y Ron (nacido en 1958).

Quienes les observaron, describen la relación de los Reagan como cercana e íntima. Durante su presidencia, se reportó que frecuentemente mostraban su cariño el uno por el otro; una secretaria de prensa dijo, "Nunca se tomaron el uno al otro por sentado. Nunca dejaron de cortejarse". La solía llamar "Mommy"; ella le llamaba "Ronnie". Él le escribió una vez, "Todo lo que atesoro y disfruto... no tendría sentido si no te tuviera a ti". Cuando él estaba en el hospital después del intento de asesinato de 1981, ella dormía con una de sus camisas para confortarse con su aroma. En una carta a los ciudadanos de EE.UU. escrita en 1994, Reagan escribió: "Recientemente se me ha dicho que yo voy a ser uno entre los millones de estadounidenses que se va a ver afectado por la enfermedad de Alzheimer... Solo desearía que hubiera alguna forma de evitar que Nancy pase por esta dolorosa experiencia," y en 1998, cuando estaba seriamente afectado por el Alzheimer, Nancy contó a "Vanity Fair": "Nuestra relación es muy especial. Estuvimos muy enamorados y todavía lo estamos. Cuando dije que mi vida comenzó con Ronnie, bueno, es verdad. Lo hizo. No puedo imaginar la vida sin él".

Se unió a numerosos comités políticos con una orientación de izquierda, como el Comité de Veteranos de Estados Unidos. Era entonces en sentía que los comunistas eran una poderosa influencia entre bastidores en esos grupos, lo que le llevó a reunir a sus amigos en contra de ellos.

Registrado como Demócrata y admirador de Franklin D. Roosevelt, Reagan apoyaba el Nuevo trato. A comienzos de la década de 1950, Reagan comenzó a girar a la derecha debido a su deseo de un gobierno federal más limitado, apoyando las candidaturas presidenciales de Dwight D. Eisenhower en 1952 y 1956 como también la de Richard Nixon en 1960. En su posición con General Electric, a Reagan se le exigía recorrer las plantas de GE y dar discursos. Usualmente, estos discursos tenían contenidos políticos y entregaban un mensaje conservador y pro empresa. Aunque después, en la Casa Blanca, tuvo escritores de discursos, Reagan los continuaba editando, y algunas veces escribiendo, muchos de sus discursos. Eventualmente los discursos se volvieron muy controvertidos para el gusto de la empresa, y Reagan fue despedido de General Electric en 1962. Reagan se cambió formalmente al Partido Republicano ese mismo año, diciendo "Yo no abandoné al Partido Demócrata. El partido me abandonó a mí".

Dos años antes de cambiar de partido, Reagan se unió a la campaña del candidato conservador a la presidencia Barry Goldwater. Hablando en nombre de Goldwater, Reagan recalcó su creencia en la importancia de un gobierno pequeño. Reveló su motivación ideológica en un famoso discurso dado el 27 de octubre de 1964: "Los Padres Fundadores sabían que un gobierno no puede controlar la economía sin controlar a la gente. Y ellos sabían que cuando un gobierno se propone hacer eso, debe usar la fuerza y la coacción para lograr sus propósitos. Así que hemos llegado a un tiempo para elegir". El discurso recaudó $1 millón para la campaña de Goldwater, y pronto se conoció como el discurso "Tiempo para elegir". Es considerado el evento que dio partida a la carrera política de Reagan.

En 1966 fue electo 33° Gobernador de California, derrotando al dos veces gobernador Pat Brown; fue reelegido en 1970, derrotando a Jesse Unruh, pero decidió no postularse para un tercer mandato. Durante las protestas en People's Park, envió 2200 soldados de la Guardia Nacional al campus de la Universidad de California. Lo hizo aduciendo que su administración no estaría bajo la influencia de las agitaciones estudiantiles. Cuando terroristas del grupo izquierdista People's Park secuestraron a Patty Hearst en Berkeley, y presentó una lista de demandas que incluían la distribución gratuita de alimento a los pobres, Reagan sugirió que era un buen tiempo para un brote de botulismo. Después de que los medios de comunicación recogieran y difundieran este comentario, pidió perdón. De gobernador intentó sofocar las protestas del Partido Pantera Negra y el movimiento juvenil de Berkeley.

En mayo de 1969 la policía se lanzó contra "Peoples Park", un parque comunal de Berkeley. La policía abrió fuego y mató a James Rector, lo que desencadeno nuevas protestas Reagan mandó a la Guardia Nacional a ocupar la universidad los soldados avanzaron con bayonetas caladas y los helicópteros lanzaban gas lacrimógeno. 

Se opuso a la construcción de una represa federal, el Dos Rios, que habría inundado un valle de rancheros indios. Más tarde, Reagan y su familia realizaron un viaje de verano en Sierra Nevada al lugar donde sería construida una carretera. Una vez allí, declaró que no sería construida. Una de las mayores frustraciones de Reagan fue que no logró la restitución de la pena capital. Había hecho campaña como uno de sus partidarios más fuertes; sin embargo, frustraron sus esfuerzos para hacer cumplir las leyes del estado en esta área cuando el Tribunal Supremo de California emitió el veredicto del juicio del Pueblo versus Anderson. La decisión de Anderson, que invalidó todas las penas de muerte en California condenadas antes de 1972. Aunque la decisión fue rápidamente revertida por una enmienda constitucional, no habría otra ejecución en California hasta 1992.

Reagan promovió el desmontaje del sistema público de hospitales psiquiátricos, proponiendo un alojamiento y tratamiento por decisión de la comunidad, substituyendo la hospitalización involuntaria, en la que vio como una violación de las libertades civiles. Según algunos críticos de Reagan, las instalaciones de reemplazo de comunidad nunca fueron suficientemente financiadas por Reagan o por sus sucesores.

La primera tentativa de Reagan de obtener la nominación como candidato republicano para la presidencia fue en 1968 contra Richard Nixon pero fracasó, lo intentó nuevamente en 1976 contra Gerald Ford, pero fue derrotado en la Convención Republicana por una pequeña diferencia.

En 1980 Reagan obtuvo finalmente el nombramiento del Partido Republicano para la candidatura presidencial después de derrotar a sus rivales en las elecciones primarias internas de la mayoría de los Estados. Durante la Convención Nacional Republicana celebrada desde el 14 de julio hasta el 17 de julio de 1980, en Detroit, Míchigan, Reagan habló de la posibilidad de que el expresidente Gerald Ford fuera como candidato a la vicepresidencia, pero en última instancia seleccionó a George H. W. Bush. Como opositor de Reagan durante las elecciones primarias del partido, Bush había declarado que nunca sería su vicepresidente. Bush era muchas cosas que Reagan no: un Republicano de toda la vida, veterano de guerra y con una posición de acercamiento a las Naciones Unidas, asimismo la filosofía económica y política de Bush era mucho más moderada que la de Reagan.
Un republicano liberal (del ala izquierdista del partido) que había sido derrotado en las primarias internas por Reagan, el entonces representante del Congreso de los Estados Unidos, John B. Anderson, decidió presentar su candidatura presidencial como independiente, llevándose consigo a una parte del electorado republicano tradicional y a muchos independientes que deseaban una tercera opción diferente a la de los dos grandes partidos tradicionales.

Después de la Convención Republicana, Ronald Reagan dio un discurso de campaña en una feria anual en las afueras de Filadelfia, Misisipi, en el lugar de los asesinatos de trabajadores defensores de los derechos civiles de Misisipi en 1964.

Durante el discurso, Reagan declaró «"creo en los derechos de los estados"» y «"creo que hemos distorsionado el equilibrio de nuestro gobierno por entregar poderes que nunca se creyó fueran dadas en la Constitución al gobierno federal"». Continuó prometiendo «"restaurar a los estados y administraciones locales el poder que por derecho les pertenece"».

La campaña, liderada por William J. Casey, se desarrolló a la sombra de la crisis de los rehenes de Irán; algunos analistas creen que la incapacidad del entonces presidente y aspirante a la reelección, Jimmy Carter, de solucionar la crisis, desempeñó un papel fundamental en la victoria de Reagan. Por otra parte, la incapacidad de Carter de solucionar una inflación de dos dígitos y el nivel de desempleo, la paralización del crecimiento económico, la inestabilidad en el mercado del petróleo, y la debilidad percibida de la defensa nacional, podrían haber tenido un mayor impacto sobre el electorado. En lo que concierne a la economía, Reagan dijo una de sus frases famosas: ""Una recesión es cuando tu vecino pierde su empleo. Una depresión es cuando tú pierdes el tuyo. Y una recuperación es cuando Jimmy Carter pierde el suyo"".

En la elección presidencial celebrada el 4 de noviembre de 1980, Reagan obtuvo 43.903.230 votos populares equivalentes al 50,75% del total de los sufragios emitidos; Carter obtuvo 35.480.115 votos populares, que equivalían al 41,01% de los sufragios; Anderson obtuvo 5.719.850 votos populares, equivalentes al 6,61% de los votos; el candidato del Partido Libertario, Ed Clark, obtuvo 921.128 votos, que equivalían al 1,06%; y el resto se repartió entre otros candidatos más pequeños. Reagan ganó en 44 estados y Carter en apenas 6 estados y el Distrito de Columbia, por lo que en el Colegio Electoral Reagan obtuvo 489 Electores contra 49 de Carter.Por lo tanto Reagan se convirtió en Presidente electo.

La elección presidencial fue acompañada con el cambio de 12 escaños del Senado de manos Demócratas a Republicanas, dando a los Republicanos una mayoría en el Senado por primera vez en 28 años. Al tomar posesión, el 20 de enero de 1981, se convirtió en el presidente de más edad en tomar el cargo, con 69 años, una de las frases que pronunció en su posesión fue: «"Government is the problem"» ("El Gobierno es el problema").

Reagan se retrataba a sí mismo como un defensor del liberalismo económico, a favor de fuertes recortes fiscales, y la reducción del Estado protector. El economista y premio Nobel Milton Friedman lo elogia ""de estar dispuesto a causar una severa recesión para terminar con la inflación"". 

El punto alto de la presidencia de Reagan fueron sus primeros 100 días en la presidencia, iniciado con el fin de la crisis de los rehenes en Irán, tras ser liberados minutos después de su toma de mando. El primer acto oficial de Reagan fue terminar con el control de precios del petróleo, con la esperanza de aumentar la producción doméstica del combustible y fomentar su exploración, el precio de venta al público de un galón de gasolina pasó de un promedio de 28,5 centavos en mayo de 1981 a 65,1 centavos en junio de 1982. 

Uno de los focos de mayor preocupación del primer periodo de Reagan era la reactivación de la economía, se combinaba el estancamiento económico con una gran inflación, esta política causó una recesión de corto plazo (1981-1982), que bajó temporalmente el apoyo público de Reagan. Otros lo elogiaron por tomar aquella estrategia. La campaña electoral de Reagan se llevó a cabo con un programa económico que prometía reducir los impuestos. Este tipo de plan económico ha conducido a un enorme déficit y a una profunda recesión. El desempleo había alcanzado su punto más alto desde la gran depresión: 11,6 millones de norteamericanos estaban nsin trabajo (10,4% de la población económica activa). De ellos, 3,7 millones han perdido su empleo desde la llegada de Reagan a la Casa Blanca. Sectores enteros de la producción habían sido desmantelados en los primeros veinte meses. La crisis afectó a las industrias tradicionales, automóviles, electrodomésticos, textiles y siderurgia. Para fines de 1982 la situación económica era cada vez más dramática, porque los subsidios en todas las áreas de los servicios sociales fueron drásticamente disminuidos. Con un déficit presupuestario que Wall Street estima entre 140 y 160 billones de dólares, uno de los más altos de la historia a pesar de los recortes.Realizó una estrategia de combinar esta política de contraer la emisión con cortes fiscales generales diseñados para aumentar la inversión en los negocios (en palabras de Reagan: "la escuela de Chicago, ofertismo fiscal, bajar los impuestos a las grandes empresas, reducir los controles, privatizar los servicios públicos, disminuir el poder de las organizaciones obreras y la negociación colectiva. Mientras esta política era ridiculizada por sus opositores como el "vudú" y "Reaganomics". La disminución de los gastos sociales acarreó un aumento de la pobreza, las minorías étnicas quedaron excluidas y los sistemas de protección social y educativa fueron fuertemente afectados, la política de Reagan registró un déficit presupuestario y comercial récord.

Debió enfrentar a poco de comenzar su gobierno un paro de controladores aéreos, para frenarlo se envió miles de cartas de despido a parte de los 12.172 controladores (el colectivo reúne a 13.000) que llevaban a cabo la huelga, la reducción a la mitad de los vuelos regulares durante el próximo mes en veintidós aeropuertos de las principales ciudades del país y el cierre de 58 torres de control en distintos puntos del país.Durante su presidencia, la Liga Anticomunista Mundial (WACL) se convierte en herramienta para las operaciones secretas de la CIA. Paralelamente, Reagan crea en 1984 la National Endowment for Democracy (NED) para proseguir las actividades de injerencia política y sindical de la CIA. Esta reforma provoca una multiplicación de las operaciones secretas, sobre todo en América Latina. Uno de los primeros blancos es Guatemala, cuyo presidente saliente, el general Romeo Lucas García, había sin embargo financiado la campaña presidencial de Reagan con fondos del orden de los 500.000 dólares. Su brazo derecho, Mario Sandoval Alarcón, «el ligado a los llamados escuadrones de la muerte en América Centra» había sido invitado a la ceremonia de investidura del nuevo presidente de los Estados Unidos en enero de 1981. En 1982, decide reemplazarlo mediante un golpe de Estado lleva al general Efraín Ríos Mont al poder. En 1983, el presidente izquierdista de Granada, Maurice Bishop, es derrocado por un golpe de Estado con apoyo de la CIA. En Honduras, donde John Negroponte dirige la guerra de baja intensidad. Sus actividades se extienden rápidamente a Nicaragua. En libia la administración Reagan ordena el bombardeo del palacio del presidente libio el 14 de abril de 1986 y causa la muerte de cerca de 80 personas. Reagan envía a uno de los jefes de la industria farmacéutica, Donald Rumsfeld, a vender armas químicas a Sadam Husein incumpliendo los acuerdos internacionales. En los momentos más intensos de la guerra Irak-Irán, el 3 de julio de 1988, ordena al USS Vincennes que derribe el vuelo Iran Air 655 en las aguas territoriales iraníes, lo que causa 290 muertos.

Nancy Reagan, influyó en que se produjera la disculpa pública del presidente por la venta de armamento a Irán, un escándalo que estuvo a punto de tumbar el mandato de este. Al mismo tiempo la primera dama Nancy Reagan tuvo un papel destacado por las grandes cenas de estado, la vajilla cara y, el buen vestir, le supuso un sinfín de críticas, dada la recesión en la que estaba sumido el país. Llegó a ser investigada, dado su uso continuado de un conjunto de piezas de joyería y diseño que ella consideraba o bien regalos o 'préstamos' que se tasaron en un millón de dólares. 

Reagan impuso recortes fiscales por el congreso en 1981. Al mismo tiempo, la administración redujo los gastos sociales, con las protestas demócratas incluidas. En el crac de 1987, el índice bursátil de Wall Street pierde un 22 % en un día. El crac del 87 anuló en una sola sesión bursátil buena parte de las ganancias que se habían acumulado durante cinco años de subidas continuadas en la Bolsa de Nueva York. Hizo que los cimientos de la mayor bolsa del mundo, Wall Street, se tambaleasen por el pánico de millones de inversores que se lanzaron en masa a vender sus acciones. Había temores inflacionistas y la inflación era muy alta. Los tipos de interés estaban subiendo.Aquel 19 de octubre, los inversores perdieron más de 500.000 millones de dólares en un solo día en la bolsa neoyorquina.

Tras la recesión de 1981-1982, la economía experimentó una notable recuperación que comenzó en 1983. Reagan movilizó a la "mayoría moral" y predicó el regreso a los "valores morales tradicionales", entre ellos prohibir el aborto, imponer la teoría creacionista en escuelas estatales y otorgar mayor participación en la vida pública al cristianismo.
Lanzó un programa de re militarización y el lanzamiento de un programa de armamento espacial intitulado Strategic Defense Initiative, que modificaría profundamente el equilibrio de fuerzas a escala planetaria y estimularía la militarización del espacio, gastando miles de millones de dólares para su realización. Económicamente la administración Reagan había convertido a Estados Unidos en un país con un gran déficit comercial y la mayor deuda externa del mundo, que compromete más y más sus recursos en empresas militares y que vive por encima de sus posibilidades gracias a la aportación de capitalesjunto a partidas presupuestarias para armar y entrenar a grupos anti-comunistas como los Contras y los muyahidines en Afganistán. La política económica aplicada fue pronto a ser llamada Reaganomics, término usado por los críticos del presidente y sus partidarios, por las reducciones de impuestos al decil más rico de la población, combinada con un fuerte aumento de los gastos militares llevaron a un enorme déficit presupuestario y el aumento de la deuda pública, que aumento en casi un 200% Como contraparte, la disminución de los gastos sociales acarreó un aumento de la pobreza, que afectó sobre todo a las minorías étnicas, los sistemas de protección social y educativo fueron fuertemente afectados por la falta de presupuesto del Estado. Además, la política de Reagan registró un déficit presupuestario y comercial récord, pesada herencia para los presidentes venideros.

La política exterior de Reagan estuvo marcada por su intento de contener la influencia soviética en muchas regiones. Carter había considerado la influencia soviética como un proceso inevitable, pero Reagan pasó a una política de mayor confrontación contra la Unión Soviética. Durante la era Reagan la Guerra Fría pasó por su fase final, y de alguna manera los Estados Unidos emergieron de ese período como la única superpotencia mundial incontestada, frente al bloque soviético que acabó por disolverse. La administración Reagan convirtió la carrera armamentista en una pugna tecnológica, donde la mayor capacidad industrial, militar y económica de Estados Unidos se impuso sobre el bloque soviético. Durante la administración Reagan, aparecieron los submarinos nucleares "Trident" (clase Ohio) y se aprovecharon los avances en la computación. La política de Reagan y la superioridad estadounidense se impusieron a la estrategia de su rival Yuri Andrópov. Este era el concepto de la Guerra de las Galaxias.En África, formó ejércitos para luchar contra los gobiernos en Angola y Mozambique apoyó al movimiento UNITA de Jonas Savimbi en Angola, lo que precipitó una guerra civil que duró años. En 1982, Reagan despachó a más de 1.200 marines a la capital del Líbano, Beirut, pero se vio obligado a retirarlos en 1983 cuando 241 murieron en una explosión. 

En Centroamérica apoyó a los gobiernos, en general autoritarios, favorables a los intereses de Estados Unidos (El Salvador y Guatemala) y que eventualmente podrían haberse convertido en aliados del bloque soviético y otros países de inspiración comunista, como Cuba o Nicaragua. También mantuvo el equilibrio de las Fuerzas Armadas en la región ya que hasta en la actualidad ninguna tiene superioridad sobre otra. Con la caída de la URSS el apoyo militar se vio sustancialmente reducido, lo que trajo como resultado la firma de los acuerdos de paz. Tras una serie de negociaciones y presiones internacionales se firma la paz en El Salvador el 16 de enero de 1992, reconociendo como fuerza política al FMLN, el cual tan solo había obtenido un escaso reconocimiento internacional como fuerza militar beligerante, aparte de países en general alineados o pertenecientes al campo socialista.
Desde 1983 Cuba denunció la introducción de una cepa de dengue en su territorio por parte de EE.UU. durante el gobierno de Reagan, y en agosto de 2015 la revista "Archives of Virology", órgano oficial de la sección de Virología de la Unión Internacional de Sociedades Microbiológicas determinó que, efectivamente, el virus fue responsable de la muerte de 158 personas (101 de ellos niños) como consecuencia de la introducción de una cepa de laboratorio de dengue hemorrágico en Cuba.

Igualmente emprendió una campaña contra el gobierno sandinista de Nicaragua, que accedió al poder tras derrocar al régimen dictatorial de Anastasio Somoza Debayle, otro cliente político de Estados Unidos. Financió a la contra nicaragüense llegando a la financiación no autorizada por el Congreso, caso conocido como el escándalo de Irán-Contra esta operación consistía en la venta de armas a Irán (por más de 47 millones de dólares) mediante gestiones de Oliver North con cuentas bancarias en Suiza. El gobierno nicaragüense acusó a Estados Unidos de violaciones al derecho internacional ante la Corte Internacional de Justicia por este apoyo a los contras y por minar los puertos del país. La Corte dictaminó en favor de Nicaragua, pero los Estados Unidos se negaron a respetar la decisión de la Corte, argumentando que ésta no tenía jurisdicción sobre el caso.

A petición de los gobiernos de Barbados y Dominica invadió Granada, cuyo gobierno había sido acusado de alinearse con la Unión Soviética y con Cuba y de promover la militarización del país, con la construcción de un aeropuerto de gran capacidad con la ayuda cubana.

En Oriente Medio, ordenó el bombardeo de Beirut, tras el asesinato de 241 marines estadounidenses y 58 paracaidistas franceses, y apoyó decisivamente a Saddam Husein, al igual que lo hizo Francia. Donald Rumsfeld llegó a visitar a Hussein y cerró varios acuerdos de venta de armamento, entre las que se habrían encontrado parte del arsenal químico que en un tiempo poseyó Irak y fue usado como argumento para la Invasión de Irak.

Durante la administración Reagan, la economía pasó de un crecimiento del PIB del –0.3 % anual en 1980 al 4.1 % en 1988 (en dólares constantes de 2005), lo que permitió que se crearan 20 millones de nuevos empleos haciendo descender la tasa de desempleo en más de dos puntos, pasando del 7.5 % a principios de 1981 al 5.2 % en 1989, aunque con máximos del 9.5 % en 1982 y 1983. La tasa de inflación, del 13.5 % en 1980, pasó al 4.1 % en 1988, lo que se consiguió aplicando altas tasas de interés por parte de la Autoridad Monetaria (llegaron a un máximo del 20 % en junio de 1981). Esto último causó una breve recesión en 1982, con una caída del PIB del –1.9 y el alto desempleo antes reseñado. Además durante la administración de Reagan el Producto Nacional Bruto creció en términos reales un 26 %, el patrimonio neto de las familias que ganan entre 20.000 dólares y 50.000 dólares al año creció un 27 % y la tasa de interés preferencial se redujo más de la mitad de un 21.5 % en enero de 1981 al 10 % en agosto de 1988.Sin embargo, algunas previsiones del plan económico trazado en 1981 resultaron fallidas: el déficit fiscal, que se preveía cercano al cero para 1986, no dejó de crecer y alcanzó en ese año el mayor dato desde el fin de la Segunda Guerra Mundial, la inflación no dejó de crecer a partir de 1986, a pesar de la fuerte restricción de la política monetaria. La brecha de la desigualdad económica aumentó significativamente: en 1966, el salario de un directivo era 41 veces superior al de un obrero; en 1988, era 92 veces superior.

Reagan aumentó de forma muy importante el gasto público, básicamente el militar, que pasó (en dólares constantes de 2000) de 267.100 millones de dólares en 1980 (4.9 % del PIB y 22.7 % de los gastos públicos) a 393.100 millones en 1988 (5.8 % del PIB y 27.3 de los gastos públicos); la mayoría de esos años el gasto militar estuvo alrededor del 6 % del PIB, superando esta cifra en 4 años distintos. Estas cifras no se veían desde el fin de la participación estadounidense en la Guerra de Vietnam, en 1973. En 1981 redujo de manera importante el tipo impositivo máximo, que afectaba a los muy ricos, y que pasó de una tasa nominal del 70 % al 50 %; en 1986 volvió a reducir el tipo al 28 %.Como consecuencia de todo esto, el déficit presupuestario y la deuda pública crecieron en gran medida: la deuda pasó del 33,3 % del PIB en 1980 al 51.9 % a finales de 1988 y el déficit pasó del 2.7% en 1980 a más del doble en 1983, cuando alcanzó el 6 %; en 1984, 1985 y 1986 estuvo alrededor del 5 %.

El número de estadounidenses por debajo del nivel de pobreza aumentó de 29.272 millones en 1980 a 31.745 en 1988, lo que significa que, en porcentaje del conjunto de la población, se mantuvo casi estacionario, desde el 12.95 % en 1980 hasta el 13 % en 1988. Los menores de 18 años por debajo del nivel de pobreza pasaron de 11.543 millones en 1980 (18.3 % del total de niños) a 12.455 (19.5 %) en 1988. Además, la situación de los grupos de renta baja se vio perjudicada por la reducción del gasto social. También aumentó la desigualdad. La participación en el ingreso total del 5 % de los hogares de mayores ingresos pasó del 16.5 % en 1980 al 18.3 % en 1988 y la del quinto de mayores ingresos pasó del 44.2 % al 46.3 % en los mismos años. En cambio, la participación en el ingreso total del quinto más pobre pasó del 4.1 % al 3.8 % y la del segundo quinto más pobre del 10.2 % al 9.6 %. En el primer año de Reagan se produjo una huelga ilegal de los controladores federales del tráfico aéreo. Su respuesta fue cesarlos y reemplazarlos con personal militar hasta conseguir reemplazos permanentes.

Durante la presidencia de Ronald Reagan se produjo la crisis de Crisis de ahorros y préstamos (asociaciones de ahorro y préstamo, un tipo especial de instituciones financieras). El coste final de la crisis se estima que ha supuesto en torno al 160,1 mil millones de dólares, alrededor de 124.600 millones de dólares de los cuales fue entregada directamente por el gobierno de los Estados Unidos a través de un rescate financiero, a partir de 1986. En 1987 se produjo el lunes negro, el mayor derrumbe porcentual sucedido en un mismo día en la historia de los mercados de valores. Lo que llevó a un pánico financiero que desato una corrida bancaria a nivel internacional y llevó al quiebre de decenas de firmas inversoras estadounidenses. 

En el año 1984 Reagan se presentó para la reelección con el apoyo incondicional de su partido (ningún dirigente republicano compitió contra él para obtener la candidatura presidencial del partido); en la Convención Nacional Republicana celebrada desde el 20 de agosto hasta el 23 de agosto de 1984 en la ciudad de Dallas, en Texas, los delegados aclamaron por unanimidad a Reagan.

Mientras tanto, el que había sido Vicepresidente de los Estados Unidos en el Gobierno de Carter, Walter Mondale, ganó las elecciones primarias internas del Partido Demócrata y en la Convención Nacional Demócrata celebrada entre el 16 de julio y el 19 de julio de 1984 en San Francisco, California, fue confirmado como el candidato de ese partido para luchar contra Reagan por la presidencia.

En la elección presidencial celebrada el 6 de noviembre de 1984, Reagan obtuvo 54 455 472 votos populares, equivalentes al 58.77 % de los sufragios emitidos; Mondale obtuvo 37 577 352 votos populares, equivalentes al 40.56 % de los sufragios; David Bergland, del Partido Libertario, obtuvo 228 111 votos, que equivalían al 0.3 % de los votos, y el resto se repartió en otras pequeñas candidaturas. Reagan ganó en 49 estados, y Mondale en apenas uno (su Minnesota natal, por escaso margen) y en el Distrito de Columbia; Reagan tuvo 525 Electores en el Colegio Electoral contra los escasos 13 de Mondale.Fue una de las victorias más aplastantes en la historia de las elecciones presidenciales estadounidenses.

En la mañana del 5 de junio de 2004 había reporteros que indicaban que la salud del expresidente Ronald Reagan se había deteriorado significativamente, tras 10 años de sufrir la enfermedad de Alzheimer. De acuerdo a la hija de Reagan, Patti Davis, "En el último momento, cuando su respiración nos dijo que era el fin, abrió sus ojos y miró directamente a mi madre. Se abrieron esos ojos que no se habían abierto por días, y no estaban opacos ni vagos. Estaban claros y azules y llenos de amor. Si una muerte puede ser hermosa, la suya lo fue". Su esposa, la ex Primera Dama Nancy Reagan, le dijo que el momento era "el mejor regalo que me podrías haber dado". El Presidente Reagan murió por neumonía en su hogar a las 13:09 PDT (20:09 UTC, o 16:09 EDT). A su lado estaban su esposa y dos de sus hijos, Ron y Patti. Su hijo mayor, vivo, Michael, había estado con él el día anterior.



Según uno de sus biógrafos, John Patrick Diggins, Reagan mantenía una creencia "emersoniana" en cuanto a la confianza personal y poseía una fe optimista en la bondad de las personas. La madre de Reagan pertenecía a la Iglesia de los Discípulos de Cristo, quienes mantenían un punto de vista optimista respecto a la naturaleza humana, las responsabilidades personales, la sobriedad y la tolerancia cristiana.

Reagan recordaba en su autobiografía: "Mi madre siempre nos enseñó: «Trata a tu vecino como te gustaría que tu vecino te tratase a ti, y juzga a todo el mundo por como actúan, no por lo que son»". Le espantaba la discriminación, y decía: "Mis padres constantemente me explicaban la importancia de juzgar a las personas como «individuos». Todo individuo es único, pero todos queremos libertad, paz, amor y seguridad, un buen hogar y una oportunidad de poder alabar a Dios a nuestra propia manera. Todos queremos la posibilidad de adelantarnos y lograr que nuestros hijos tengan una mejor vida que la nuestra", Reagan escribió en "An American Life".

Es un hecho documentado que Ronald y Nancy Reagan recurrían a astrólogos para planificar los eventos importantes de su agenda:

Aunque muchas de las decisiones de Reagan estuvieron influidas por astrólogos, no está claro hasta qué punto creía en esa disciplina; al parecer la influencia procedía sobre todo de su esposa Nancy Reagan.

Como consecuencia de la Gran recesión de 2008, diversos economistas han rastreado las causas de esta violenta crisis económica hasta encontrarlas en las políticas desreguladoras impulsadas por Ronald Reagan y Margaret Thatcher, seguidores de Milton Friedman y la Escuela de Chicago, e imitadas por la gran mayoría de los estadistas de su época. Según Teodoro Petkoff, la crisis financiera acabó con la “ideología neoconservadora” que llegó y se apoderó de los pensamientos de aquellos círculos más poderosos en Estados Unidos, desde “Ronald Reagan para acá”. Dichas políticas desreguladoras y neoliberales terminaron por suprimir la acción del Estado en la economía, dejándola en manos del sector financiero que, libre de regulaciones, generó toda una serie de instrumentos sin una base sólida para sostenerse.




</doc>
<doc id="8827" url="https://es.wikipedia.org/wiki?curid=8827" title="Polígono">
Polígono

En geometría, un polígono es una figura geométrica plana compuesta por una secuencia finita de segmentos rectos consecutivos que encierran una región en el plano. Estos segmentos son llamados lados, y los puntos en que se intersecan se llaman vértices. El polígono es el caso bidimensional del politopo.

La palabra "polígono" deriva del griego antiguo πολύγωνος ("polúgonos"), a su vez formado por πολύ ("polú") ‘muchos’ y γωνία ("gōnía") ‘ángulo’, aunque hoy en día los polígonos son usualmente entendidos por el número de sus lados.

La noción geométrica elemental ha sido adaptada de distintas maneras para servir a propósitos específicos. A los matemáticos a menudo les interesan sólo las líneas poligonales cerradas y los polígonos simples (aquellos en los cuales sus lados sólo se intersecan en los vértices), y pueden definir un polígono de acuerdo a ello. Es requisito geométrico que dos lados que se intersecan en un vértice formen un ángulo no llano (distinto a 180°), ya que de otra manera los segmentos se considerarían partes de un único lado; sin embargo, esos vértices podrían permitirse algunas veces por cuestiones prácticas. En el ámbito de la computación, la definición de polígono ha sido ligeramente alterada debido a la manera en que las figuras son almacenadas y manipuladas en la computación gráfica para la generación de imágenes.

La definición del polígono depende del uso que se le quiera dar, así por ejemplo para hacer referencia a una región del plano se tiene:


Para hacer referencia al estudio euclidiano de las longitudes de los lados de un polígono, se tiene:


Para desarrollar un concepto didáctico del polígono, se tiene:


En esta última definición se suele evitar los puntos consecutivos alineados. 

Se denomina línea poligonal o línea quebrada al conjunto de segmentos, formula_1, unidos sucesivamente por sus extremos donde el extremo de cada uno es origen del siguiente, tal que dos segmentos sucesivos no están alineados, en tal caso se considera ambos como un único segmento.


Ejemplo de una línea poligonal de seis segmentos:

Véase también

La definición y su aplicación del concepto de Grafo de la teoría de grafos.

La definición de símplex usada en topología algebraica.


En un polígono se distinguen los siguientes elementos geométricos:

En un polígono regular se puede distinguir, además:


Existen varias clasificaciones posibles de los polígonos. Para ver una clasificación basada en su número de lados, vea la tabla inferior.

Según las propiedades que cumpla el contorno del polígono, es posible realizar las siguientes clasificaciones.


Los polígonos tienen un nombre especial para designar el número de lados del mismo. Los nombres más comunes están en la siguiente tabla:



</doc>
<doc id="8828" url="https://es.wikipedia.org/wiki?curid=8828" title="Cuadrilátero">
Cuadrilátero

En geometría plana, un cuadrilátero o tetrágono es un polígono de cuatro lados y cuatro vértices

La palabra "cuadrilátero" procede de dos palabras latinas "quadri", que significa cuatro, y "latus", que significa lado.

Los cuadriláteros según su forma se dividen en complejos y simples, y estos a su vez se dividen en cóncavos y convexos, y estos a su vez pueden estar o no inscritos o circunscritos.




Los elementos de un cuadrilátero son los siguientes:






Los cuadriláteros se clasifican según el paralelismo de sus lados, sus longitudes y sus ángulos interiores:


En el gráfico ilustrativo de la taxonomía de los cuadriláteros se pasa de las definiciones más generales a las más específicas siguiendo el sentido de las flechas.

Así se parte de un cuadrilátero definido como un polígono cerrado de cuatro lados, sin más restricciones, para diferenciar los cuadriláteros compuestos de los simples.

En un cuadrilátero complejo, dos de sus lados se cortan. En uno simple los lados no se cruzan.

Los cuadriláteros simples se dividen en:

A un cuadrilátero que al mismo tiempo sea cíclico y tangencial se le denomina cuadrilátero bicéntrico. El deltoide es tangencial con dos pares de lados iguales.

Un caso particular de trapecio isósceles es cuando la longitud de una de las bases es igual que la de sus lados, por lo cual se configura un trapecio de tres lados iguales.

Se llama así cualquier cuadrilátero en el cual una de sus diagonales sirve de eje de simetría. Por ejemplo: el rombo, el deltoide, el cuadrado. 

El rectángulo es un cuadrilátero que simultáneamente cumple las características de:

Del mismo modo se puede verificar que el rombo es un deltoide paralelogramo, pues cumple las características de ambos.

Por último, el cuadrado puede considerarse rombo, rectángulo, con lados iguales y bicéntrico.

formula_12

formula_13

formula_14
formula_15
formula_16
formula_17
formula_18
formula_19 (para un cuadrilátero con concavidad en C cambiar el primer signo + por -).

Son aquellos cuyos vértices están en una circunferencia y sus lados son cuerdas. Se establecen las siguientes fórmulas siendo 

Sea el cuadrilátero inscrito de lados a,b,c,d; de diagonales perpendiculares que al intersecarse determinan los segmentos m,n en uno de ellos y p, q en el otro, R radio de la circunferencia circunscrita. En tal caso son válidas las igualdades:




</doc>
<doc id="8830" url="https://es.wikipedia.org/wiki?curid=8830" title="Mira (estrella)">
Mira (estrella)

Mira (Ómicron Ceti / ο Cet / 68 Ceti) es una estrella variable de la constelación de Cetus, «la ballena».
Una de las estrellas más notables del cielo nocturno, su magnitud aparente varía entre +2,0 —siendo en ese momento la estrella más brillante de la constelación— y +10,1 —cuando no es visible a simple vista— con un período de 332 días.
Ello ha dado origen a su nombre, "Mira", procedente del latín "mira", «maravillosa, asombrosa».
La distancia a la que se encuentra es incierta; mientras que las mediciones realizadas antes del satélite Hipparcos la situaban a 220 años luz del Sistema Solar, los datos de Hipparcos indican una distancia de 418 años luz, con un margen del error del 14%.

Puede ser que la variabilidad de Mira fuera ya conocida en la antigua China, Babilonia y Grecia.
Lo que es seguro es que la variabilidad de Mira fue registrada por el astrónomo David Fabricius desde el 3 de agosto de 1596. Al observar el planeta Mercurio, Fabricius necesitaba una estrella de referencia para comparar posiciones, escogiendo una estrella de tercera magnitud cercana antes inadvertida —Mira—. Sin embargo, hacia el 21 de agosto el brillo de la estrella había aumentado una magnitud, mientras que para octubre de ese mismo año no era ya visible. Fabricius supuso que era una nova, hasta que la vio de nuevo el 16 de febrero de 1609.

En 1638, Johann Holwarda determinó el período de las reapariciones de la estrella en once meses; a menudo se atribuye a este astrónomo frisio el descubrimiento de la variabilidad de Mira. En la misma época, Johannes Hevelius observó la peculiar estrella, denominándola «Mira» —en el sentido de «maravillosa» o «asombrosa»— en la "Historiola Mirae Stellae" de 1662, pues su comportamiento se apartaba del de cualquier otra estrella conocida. Ismail Bouillaud estimó su período en 333 días, lo que supone menos de un día de diferencia respecto al período actualmente aceptado de 332 días.

Hay una considerable especulación sobre si Mira había sido ya observada antes de Fabricius. La historia de Algol (β Persei) —con seguridad conocida como variable en 1667, aunque distintas leyendas muestran que había sido observada desde milenios con recelo— sugiere que Mira pudiera haber sido conocida en la antigüedad. Karl Manitius, traductor del "Comentario en Aratus" de Hiparco de Nicea, sugiere que ciertas líneas de aquel texto del siglo II a. C. pueden versar sobre Mira. Otros catálogos, como los de Ptolemeo, Al-Sufi, Ulugh Beg y Tycho Brahe no la mencionan, ni siquiera como estrella «normal». Existen tres observaciones de archivos chinos y coreanos, de 1596, 1070 y 134 a. C. —el mismo año que Hiparco de Nicea habría hecho sus observaciones— que sugieren que la estrella podría ser ya conocida en aquellas épocas.

Actualmente, Mira es el prototipo de una clase de variables que llevan su nombre, las variables Mira.

Mira es una gigante roja de tipo espectral medio M7IIIe; éste varía entre M5 y M9 —momento en el que su temperatura y brillo son menores—. Como consecuencia de su variabilidad, es problemático definir su temperatura y tamaño, ya que estos parámetros dependen del momento del ciclo en el cual se realice la medida y de la longitud de onda utilizada; consecuentemente, su luminosidad tampoco es inequívoca.
La relativa cercanía de Mira permite, sin embargo, medir su diámetro angular. Éste permite calcular su radio, que varía desde 2 UA en luz visible, hasta aproximadamente el doble en luz infrarroja.
Considerando una temperatura superficial de 3000 K, su luminosidad se puede estimar en aproximadamente 8.500 veces la luminosidad solar —incluyendo una gran cantidad de energía emitida como radiación infrarroja—.

Mira se encuentra en las últimas fases de su evolución estelar.
Hace miles de millones de años era una estrella similar al Sol, pero, una vez agotado su combustible de hidrógeno y helio, se ha transformado en una estrella muy distendida y luminosa. Su variabilidad proviene de pulsaciones en su superficie, cambios en el tamaño de la estrella —que pueden suponer un 15% en cada pulsación— que afectan también a su temperatura y luminosidad.

Observaciones llevadas a cabo con el telescopio espacial GALEX en la región ultravioleta han puesto de manifiesto que Mira deja un rastro de materia proveniente de sus capas externas, creando una estela de 13 años luz de longitud —unas tres veces la distancia que separa el Sol de la estrella más cercana, Próxima Centauri—, formada a lo largo de 30.000 años o más.
Se piensa que una onda de choque de plasma o gas comprimido genera la estela; dicha onda de choque resulta de la interacción entre el viento estelar de Mira y el gas en el espacio interestelar, a través del cual la estrella se mueve a gran velocidad —130 km/s—.
La masa del «rastro» de Mira se estima en unas 3.000 veces la de la Tierra.

En última instancia, el material perdido constituirá una nebulosa planetaria, mientras que el remanente estelar se condensará en una enana blanca de un tamaño similar al de nuestro planeta.

Mira forma un sistema binario con una acompañante, Mira B, resuelta en 1995 por el Telescopio Espacial Hubble. Distante 70 UA de la primaria, imágenes en el ultravioleta y rayos X muestran una espiral de gas procedente de Mira en dirección a Mira B. El período orbital de esta compañera es de ~ 400 años.

Mira B se halla rodeada por un disco protoplanetario, formado a partir del material procedente del viento solar de Mira. Se piensa que probablemente Mira B es una enana naranja de tipo K con una masa aproximada de 0,7 masas solares, y no una enana blanca como se creyó inicialmente.




</doc>
<doc id="8831" url="https://es.wikipedia.org/wiki?curid=8831" title="Vega (estrella)">
Vega (estrella)

Vega (Alfa Lyrae / α Lyr) es una estrella de primera magnitud (en la clasificación de Ptolomeo) de la constelación de la Lira y la principal de la misma. Es la quinta estrella más brillante del cielo nocturno y la tercera del hemisferio norte celeste tras Sirio y Arturo. Se considera una estrella relativamente cercana, a solo 25 años-luz de la Tierra, siendo una de las más brillantes cercanas al sistema solar. Vega ha sido muy estudiada por los astrónomos, llegando a ser catalogada como la estrella más importante en el cielo después del Sol. Vega fue la estrella polar alrededor del año 12000 a. C. y volverá a serlo alrededor del año 13720 d. C. cuando la declinación sea de +86°14’. Vega fue la primera estrella, después del Sol, en ser fotografiada y a la primera que se le realizó un registro espectral.

Esta estrella solo posee una décima parte de la edad del Sol, pero al ser 2.1 veces más masiva su ciclo de vida es también una décima parte comparada con el Sol; ambos astros, en el presente, se encuentran muy cerca de alcanzar el punto intermedio en sus ciclos de vida. Vega es inusualmente pobre en elementos con número atómico mayor que el del helio. Vega es una supuesta estrella variable que varía ligeramente en magnitud de manera periódica. Esta rota rápidamente con una velocidad de 274 km/s en el ecuador. Esto provoca un abultamiento externo en el ecuador provocado por los efectos de la aceleración centrífuga y, como resultado, existe una variación de la temperatura sobre la fotosfera de la estrella, alcanzado su valor máximo en los polos. Desde la Tierra, Vega es observada desde la dirección de uno de sus polos.

La astrofotografía, la fotografía de objetos celestes, comenzó en 1840 cuando John William Draper tomó una imagen de la Luna utilizando el proceso del daguerrotipo. El 17 de julio de 1850, Vega se convirtió en la primera estrella (después del Sol) en ser fotografiada por William Bond y John Adams Whiple en el Observatorio del Colegio de Harvard, también empleando un daguerrotipo. Henry Draper tomó la primera fotografía del espectro estelar en agosto de 1872 mientras le realizaba una fotografía a Vega, convirtiéndose así en la primera persona en mostrar las líneas de absorción en el espectro de una estrella. Líneas similares han sido identificadas en el espectro del Sol. 
Los astrónomos profesionales han utilizado a Vega para fijar los baremos absolutos de brillo fotométrico, lo que supone que la magnitud visual de Vega es aproximadamente cero en todas las longitudes de onda. La intención original era que el valor fuera exactamente cero, pero en la práctica no resultó así. Por ejemplo, en el filtro "V" de Johnson (el más usado por los astrónomos en el rango visible), la magnitud de Vega es 0,026 ± 0,008, y en otros filtros hay también desviaciones de unas pocas centésimas.

Vega puede observarse frecuentemente cerca del cenit en latitudes medias-septentrionales durante las noches de verano en el hemisferio norte. Puede observarse sobre el horizonte, al norte, durante el invierno en latitudes medias del hemisferio sur. Como Vega tiene una declinación de +38.78°..., solo puede verse en latitudes al norte de los 51°S. Esta estrella no es apreciable desde la Antártida o desde las regiones más australes de América del Sur, incluyendo Punta Arenas, Chile (53° S). En latitudes de +51° N, Vega permanece continuamente sobre el horizonte como una estrella circumpolar.

Esta estrella permanece como un vértice del llamado Triángulo estival constituido, además, por Altair en la constelación Águila y Deneb en la del Cisne. El Triángulo del Verano es muy reconocible en los cielos septentrionales puesto que en su vecindad existen pocas estrellas brillantes.

Las Líridas son una fuerte lluvia de meteoros cuyo pico es alcanzado durante el 21-22 de abril. Cuando un pequeño meteoro ingresa a la atmósfera terrestre a una gran velocidad, produce un rastro de luz mientras el objeto es vaporizado. Durante una lluvia, una gran cantidad de meteoros arriban desde una misma dirección, y, desde la perspectiva del observador, los rastros brillantes parecen ser irradiados desde un solo punto en el espacio. En el caso de las Líridas, estos meteoros son irradiados desde la constelación de la Lira. Sin embargo, actualmente se conoce que estos son restos emitidos por el cometa C/1861 G1 Thatcher y no tiene nada que ver con la estrella.

La clase espectral de Vega es A0V, convirtiéndola en una estrella de secuencia principal blanca con matices azulados, la cual se encuentra fusionando hidrógeno y formando helio en su núcleo. La edad actual de la estrella es de alrededor de 455 millones de años. Vega se convertirá, en el futuro de su ciclo de vida, en una gigante roja clase-M y perderá mucha de su masa, convirtiéndose finalmente en una enana blanca. En el presente, Vega posee más que el doble de la masa del Sol y su luminosidad es, aproximadamente, 37 veces el valor de la del Sol. Sin embargo, debido a su alto ritmo de rotación, el polo es considerablemente más brillante que el ecuador. Al ser observada desde la Tierra, su brillo asciende hasta 57 la luminosidad del Sol, ya que se observa desde uno de sus polos. La mayoría de la energía producida en el núcleo de Vega es producida por el ciclo carbono-nitrógeno-oxígeno (ciclo CNO). Este es un proceso de fusión nuclear que requiere temperaturas de alrededor de 15 millones K, lo cual es muy superior a la temperatura existente en el núcleo del Sol, pero es mucho más eficiente que la reacción de fusión del Sol.

Vega posee un disco de polvo y gas a su alrededor que fue descubierto por el satélite IRAS a mediados de la década de los 80. Esto puede significar que o bien tiene planetas, o que se podrían formar relativamente pronto.
Tiene también un espectro relativamente plano en la región visual (un campo de longitud de onda que va desde los 350 a los 850 nanómetros, la mayoría de las cuales son visibles al ojo humano), de modo que las densidades de flujo son aproximadamente iguales, 2000-4000 Jy. La densidad de flujo de Vega se reduce rápidamente en el infrarrojo y se acerca a los 100 Jy en 5 micrómetros.

Cuando el radio de Vega fue medido con gran precisión con un interferómetro, arrojó un inesperado valor estimado de 2.73 ± 0.01 veces el radio del Sol. Esto es un 60 % más grande que el radio de la estrella Sirio, mientras que los modelos estelares indicaban que solo debía ser un 12 % mayor. Sin embargo, esta discrepancia puede ser explicada si Vega es una estrella de rápida rotación que es vista desde la dirección de su polos de rotación. Observaciones realizadas en 2005-06 confirmaron esta deducción. 
Vega es un ejemplo de estrella que rota a gran velocidad, como Altair (α Aquilae) o Regulus (α Leonis), por lo que su radio ecuatorial es significativamente mayor que su radio polar. Una velocidad de rotación alta también genera diferencias de temperatura superficial entre el ecuador y los polos. La velocidad de rotación en el ecuador de Vega es de 275 km/s, lo que hace que los polos estén a una temperatura superficial de 10 150 K y el ecuador a una temperatura de 7900 K.

El nombre Vega (posteriormente Vega) proviene de una transliteración de la palabra árabe wāqi‘ que significa “cayendo” o “aterrizando”. El término “Al Nesr al Waki” apareció en el catálogo estelar de Al Achsasi Al Mouakket. El nombre árabe posteriormente apareció en el mundo occidental en las Tablas Alfonsíes, las cuales fueron dibujadas entre 1215 y 1270 por orden de Alfonso X.

En el presente la estrella polar es Polaris, pero alrededor del año 12000 a. C. la estrella polar era Vega (a solo cinco grados de ella). A causa del movimiento de precesión, la estrella polar estará cerca de Vega alrededor del año 14000 d. C. Vega es la más brillante de todas las estrellas polares sucesivas. Entre los habitantes del norte de la Polinesia, Vega era conocida como whetu o te tau, la estrella del año. Durante un período histórico esta estrella marcaba el comienzo de un nuevo año, cuando el suelo debía ser preparado para ser plantado. Con el paso del tiempo esta función pasó a las Pléyades.

Los asirios nombraron a esta estrella polar Dayan-same, el “Juicio del Cielo”, mientras que por los acadios fue nombrada Tir-anna, “Vida de los Cielos”. En la astronomía babilónica, Vega debió de ser una de las estrellas llamadas Dilgan, “Mensajero de Luz”. Para los antiguos griegos, la constelación de Lira estaba formada por el arpa de Orfeo, con Vega como su mango. En la mitología China, existe la historia de amor de Qi Xi (七夕) en la cual Niu Lang (牛郎, Altair) y sus dos hijos (β Aquilae y γ Aquilae) son separados de su madre Zhi Nü (織女, Vega) quien se encuentra en el lado más alejado del río, la Vía Láctea. Sin embargo, una vez por año en el séptimo día del séptimo mes del calendario lunisolar chino, se hace un puente y así Niu Lang y Zhi Nü pueden estar juntos de nuevo durante un breve tiempo. El festival japonés Tanabata, en el cual Vega es conocida como orihime (織姫), se basa en esta leyenda. En el zoroastrismo, Vega es asociada en ocasiones con Vanant, una divinidad menor cuyo nombre significa “conquistador”.



</doc>
<doc id="8834" url="https://es.wikipedia.org/wiki?curid=8834" title="Francisco Ibáñez">
Francisco Ibáñez

Francisco Ibáñez Talavera (Barcelona, 15 de marzo de 1936), es un historietista español, perteneciente a la segunda generación o generación del 57 de la Escuela Bruguera, junto a autores como Figueras, Gin, Nadal, Raf, Segura o Martz Schmidt. Creador de multitud de series humorísticas, entre las que destaca Mortadelo y Filemón, muchas de ellas se perciben en España como un icono esencial de varias generaciones y muchos otros dibujantes de cómic posteriores reconocen su gran influencia.

Está casado y tiene dos hijas.

Francisco Ibáñez Talavera nació en Barcelona el 15 de marzo de 1936 cuatro meses antes de que estallara la Guerra Civil Española, en el seno de una familia de clase media baja, compuesta por el padre, de origen alicantino y contable de profesión; la madre, de origen andaluz, y tres hermanos. Desde muy pequeño, desarrolló una gran afición por los tebeos y el cine cómico estadounidense. En octubre de 1947, con once años, se publicó su primer dibujo en la revista "Chicos".

Tras finalizar la enseñanza primaria en las Escuelas Guimerá, Ibáñez empezó a estudiar Contabilidad, Banca y Peritaje Mercantil y en 1950 entró a trabajar como botones en el Banco Español de Crédito, labor esta que dos años después empezó a compaginar con colaboraciones en las revistas "Nicolás", "Chicolino", "La hora del recreo", "Alex", "Liliput", "El Barbas" y sobre todo en las dos cabeceras humorísticas de la Editorial Marco: "La Risa" e "Hipo, Monito y Fifí". En ellas creó portadas y series como "Kokolo" (1952), "Melenas" (1954), "Don Usura" (1955) y "Haciendo el indio" (1955), la primera de cierto éxito del autor, al ser reproducida también en el suplemento semanal de "La Prensa" de Barcelona. Destacaba además entre todos los autores de la editorial por una violencia que anticipaba la de sus futuras creaciones.

En el verano de 1957, Ibáñez, que ganaba ya más como dibujante que como ayudante de cartera y riesgos en la banca, decidió dedicarse por completo a la historieta y, además de seguir colaborando con las publicaciones de la Editorial Marco, entró a formar parte de la plantilla de "Paseo infantil", que desapareció al poco tiempo y donde creó series como "Pepe Roña" y continuó la serie "Loony" de Alfons Figueras.

Simultáneamente, Ibáñez empezó en agosto a colaborar con la potente Bruguera, que entonces necesitaba imperiosamente nuevos dibujantes tras la marcha de sus principales artistas a "Tío Vivo". En Bruguera Ibáñez aportó inicialmente páginas de chistes sobre un tema determinado o un deporte para "Pulgarcito" y las centrales de "El DDT" y "Selecciones de Humor de El DDT", ya que como explicaba Armando Matías Guiu, ""el chiste era el primer paso para conseguir un personaje de las revistas"".

El 20 de enero de 1958, trabajando ya en exclusiva para Bruguera, y tras la aprobación del director artístico de la misma, Rafael González, Ibáñez publicó la primera entrega de Mortadelo y Filemón en la revista "Pulgarcito". Desde entonces y durante la década de los años 1960, Ibáñez fue creando y adaptando algunos de sus mejores series para diferentes revistas de la editorial: "La familia Trapisonda" ("Pulgarcito", 7/07/1958), la originalísima "13, Rue del Percebe" ("Tío Vivo", 6/03/1961), "El botones Sacarino" ("El DDT", 1963), "Rompetechos" (Tío Vivo, 1964) y Pepe Gotera y Otilio ("Tío Vivo", 1966).

Influido por el cómic franco-belga, Ibáñez publicó en 1969 "El sulfato atómico", la primera historieta de Mortadelo y Filemón concebida como parodia del mundo de los espías y de larga extensión. El nuevo modelo triunfó tanto a nivel nacional como internacional y Bruguera lo explotó sacando cabeceras como "Mortadelo" (1970), "Super Mortadelo" (1972), "Mortadelo Gigante" (1974) o "Mortadelo Especial" (1975), a veces sin respetar sus derechos laborales. El recrudecimiento de la censura también contribuyó al abandono de los referentes sociales locales.

Ibáñez, que en el terreno personal se convierte en esos años en padre de dos hijas, sufrió entonces la mercantilización e industrialización progresiva de sus personajes estrella, que le obligaba a trabajar a destajo (hasta 40 páginas semanales), abandonar sus otros personajes y recurrir a colaboradores. El teórico Jesús Cuadrado lo resumió así:
En esta época, solo un personaje nuevo logró sus propias historietas: "Tete Cohete" (1981).

En 1985 Ibáñez abandonó la editorial Bruguera, que se había quedado con los derechos de sus personajes, por lo que todas las historietas protagonizadas por los mismos (y no solo parte) empezaron a ser desarrolladas completamente por otros autores, integrados en lo que se denominó Bruguera Equip. Mientras tanto, Ibáñez comenzó a trabajar para otra editorial, Grijalbo, donde en 1986 creó nuevos personajes para la revista Guai!: así nacieron "Chicha, Tato y Clodoveo, de profesión sin empleo" y "7, Rebolling Street".

Tras la publicación en 1987 de la Ley 22/1987, de 11 de noviembre, de Propiedad intelectual, que confirmaba la propiedad de las obras por parte de los autores, Ibáñez entró a formar parte de Ediciones B y desde entonces realiza 6 nuevos álbumes de Mortadelo y Filemón por año donde aparecen abundantes elementos de la actualidad y de las modas del momento en que los creó.

En 1994 ayudó junto a un equipo de Ediciones B y de la BRB Internacional a realizar la serie de animación Mortadelo y Filemón.

Tanto Vázquez como Ibáñez se caracterizan por presentar una sucesión continua de gags desde el principio hasta el final de la historia, de tal manera que en una viñeta se prepara el gag que se va a producir en la otra. Como continúa explicando Armando Matías Guiu 

Configuran así un tipo de humor mucho más directo y explosivo, más propenso a la carcajada, que el de sus predecesores, como Peñarroya o Escobar.

El propio Ibáñez se ha caricaturizado numerosas veces en sus historietas, llegando a ser un personaje más e incluso el principal en algunas. Se presenta en estas ocasiones como un individuo engreído que cobra muchos millones por dibujar y también (esto se acerca más a la realidad) que trabaja mucho, aunque sus propios personajes hacen burla de su capacidad para dibujar bien. También era habitual en sus tiempos en la Editorial Bruguera que, en números especiales, la redacción fuera caricaturizada y los trabajadores y dibujantes más conocidos ejercieran de personajes en una trama historietística.

A lo largo de su dilatada carrera, Ibáñez ha dibujado las siguientes series:






</doc>
<doc id="8835" url="https://es.wikipedia.org/wiki?curid=8835" title="A buen fin no hay mal tiempo">
A buen fin no hay mal tiempo

A buen fin no hay mal principio, también traducida como Bien está lo que bien acaba (inglés: "All's Well That Ends Well") es una obra de teatro de William Shakespeare.

Se piensa que Shakespeare escribió esta obra aproximadamente entre 1601 y 1605, junto con la titulada "Medida por medida". A estas dos obras se las llamó "comedias oscuras" por el hecho de no entrar en ninguna categoría y tener un final que se pueda decir inteligible. La acción de esta obra está situada en lugares que al autor le parecían exóticos: París, Florencia y el Rosellón.

Los temas que se abordan son el amor y el poder. Los dos personajes centrales, Elena y Beltrán, tratan de encontrar el mecanismo apropiado para llegar al objeto de su deseo. Se da a conocer al espectador una exposición de los medios que el hombre puede llegar a utilizar para conseguir sus fines.

Rosellón, París, Florencia, Marsella.


</doc>
<doc id="8837" url="https://es.wikipedia.org/wiki?curid=8837" title="Pablo de Tarso">
Pablo de Tarso

Pablo de Tarso, originalmente Saulo de Tarso o Saulo Pablo, también llamado san Pablo, nacido entre los años 5 y 10 d. C., en Tarso de Cilicia (actual Turquía centro-meridional) y muerto martirizado bajo el gobierno de Nerón entre los años 58 y 67 en Roma, es conocido como el "Apóstol de los gentiles", el "Apóstol de las naciones", o simplemente "el Apóstol", y constituye una de las personalidades señeras del cristianismo primitivo.

De sus epístolas auténticas surge que Pablo de Tarso reunía en su personalidad sus raíces judías, la gran influencia que sobre él tuvo la cultura helénica y su reconocida interacción con el Imperio romano, cuya ciudadanía —en el decir del libro de los Hechos de los Apóstoles— ejerció. Pablo no cambió su nombre al abrazar la fe en Jesucristo como Mesías de Israel y Salvador de los gentiles, ya que, como todo romano de la época, tenía un "praenomen" relacionado con una característica familiar (Saulo, su nombre judío, que etimológicamente significa ‘invocado’, ‘llamado’), y un "cognomen", el único usado en sus epístolas ("Paulus", su nombre romano, que etimológicamente significa ‘pequeño’ o ‘poco’).

Su conocimiento de la cultura helénica —hablaba fluidamente tanto el griego como el arameo— le permitió predicar el Evangelio con ejemplos y comparaciones comunes de esta cultura, por lo que su mensaje cosechó un pronto éxito en territorio griego. Pero esta característica también dificultó por momentos la exacta comprensión de sus palabras, ya que Pablo recurría en ocasiones a nociones helenísticas alejadas del judaísmo, mientras que otras veces hablaba como un judío estricto y observante de la Ley. De ahí que en la Antigüedad algunas de sus afirmaciones fueran calificadas como «"τινα δυσνοητα"» (transliterado, "tina dysnoēta", que significa puntos ‘difíciles de entender’; y que hasta hoy se susciten polémicas en la interpretación de ciertos pasajes y temas de las cartas paulinas, como, por ejemplo, la relación entre judíos y gentiles, entre gracia y Ley, etc. Por otra parte, es claro que sus epístolas fueron escritos de ocasión, respuestas a situaciones concretas. Por ello el análisis exegético moderno, más que esperar de cada una de ellas una formulación sistemática del pensamiento del Apóstol, examina las dificultades y particularidades que él presenta, analiza su evolución y debate sobre su integridad.

Sin haber pertenecido al círculo inicial de los Doce Apóstoles, y recorriendo caminos jalonados de incomprensiones y adversidades, Pablo se constituyó en artífice de primer orden en la construcción y expansión del cristianismo en el Imperio romano, merced a su talento, a su convicción y a su carácter indiscutiblemente misionero. Su pensamiento conformó el llamado "cristianismo paulino", una de las cuatro corrientes básicas del cristianismo primitivo que terminaron por integrar el canon bíblico.

De las llamadas "epístolas paulinas", la Epístola a los romanos, la Primera y la Segunda epístola a los corintios, la Epístola a los gálatas, la Epístola a los filipenses, la Primera epístola a los tesalonicenses (probablemente la más antigua) y la Epístola a Filemón tienen en Pablo de Tarso su autor prácticamente indiscutido. Ellas son, junto con el libro de los Hechos de los Apóstoles, las fuentes primarias independientes cuyo exhaustivo estudio científico-literario permitió fijar algunas fechas de su vida, establecer una cronología relativamente precisa de su actividad, y una semblanza bastante acabada de su apasionada personalidad. Sus escritos, de los que han llegado a la actualidad copias tan antiguas como el papiro formula_1 datado de los años 175-225, fueron aceptados unánimemente por todas las Iglesias cristianas. Su figura, asociada con la cumbre de la mística experimental cristiana, resultó inspiradora en artes tan diversas como la arquitectura, la escultura, la pintura, la literatura, y la cinematografía y es para el cristianismo, ya desde sus primeros tiempos, una fuente ineludible de doctrina y de espiritualidad.

El Apóstol se llamaba a sí mismo "Παῦλος" ("Paulos") en sus cartas escritas en griego koiné. Este nombre aparece también en la Segunda epístola de Pedro 3:15 y en los Hechos de los Apóstoles a partir de 13, 9.

Antes de ese versículo, el libro de los Hechos lo llama con la forma griega "Σαούλ" ("Saoul") o "Σαῦλος" ("Saulos") (; en hebreo moderno "Sha'ul", y en hebreo tiberiano "Šāʼûl"). El nombre, expresado en hebreo antiguo, equivaldría al del primer rey del Antiguo Israel, un benjaminita igual que Pablo. Ese nombre significa «invocado», «llamado» o «pedido» (de Dios o de Yahveh).

También se utiliza su nombre "Σαῦλος" ("Saulos") en los relatos de su «conversión». El libro de los Hechos de los Apóstoles señala además el paso de «Saulo» a «Pablo», al emplear la expresión «"Σαυλος, ο και Παυλος"», «Saulo, también [llamado] Pablo» o «Saulo, [conocido] también [por] Pablo», lo que no significa un cambio de nombre. En el judaísmo helenista, era relativamente frecuente portar un doble nombre: uno griego y otro hebreo.

El nombre "Paulos" es la forma griega del conocido "cognomen" romano "Paulus", utilizado por la gens Emilia.
Solo se puede conjeturar respecto de la forma en que Pablo obtuvo este nombre romano. Es posible que tuviera relación con la ciudadanía romana que su familia poseía por habitar en Tarso.
También es posible que algún antepasado de Pablo adoptara ese nombre por ser el de un romano que lo manumitió.
Si bien "paulus" significa en latín ‘pequeño’ o ‘exiguo’, no se relaciona con su contextura física o con su carácter.

Con todo, Pablo pudo dar otro significado al uso del nombre Paulos. Giorgio Agamben recuerda que cuando un señor romano dueño de esclavos compraba un nuevo siervo, le cambiaba el nombre como signo de su cambio de estado o de situación. Agamben señala ejemplos de ello: «Januarius qui et Asellus (Asnillo); Lucius qui et Porcellus (Cochinillo); Ildebrandus qui et Pecora (Ganado); Manlius qui et Longus (Largo); Aemilia Maura qui et Minima (La menor)».
El nombre de la persona aparecía en primer lugar; el nuevo nombre se señalaba al final; ambos nombres se unían por la fórmula «qüi et», que significa ‘el cual también [se llama]’. En el libro de los Hechos de los Apóstoles aparece la frase: «Σαυλος, ο και Παυλος» (‘Saulo, también [llamado] Pablo’), donde «ο και» es el equivalente griego de la expresión latina «qüi et». Agamben propone que Saulo cambió su nombre por el de Pablo cuando mudó de estado, de libre a siervo/esclavo, siendo que se consideraba servidor de Dios o de su Mesías. Siguiendo esa línea de pensamiento, Pablo se habría considerado un instrumento humano pequeño ("paulus", ‘pequeño’; san Agustín de Hipona señala lo mismo en el "Comm. in Psalm." 72,4: "«Paulum […] minimum est»"), de poco valor, escogido, sin embargo, por Dios, su Señor, para desempeñar una misión.

Se conoce a Pablo de Tarso principalmente por dos tipos de documentación, que se pueden clasificar según su nivel de importancia:

Existe otro tipo de obras, las llamadas «epístolas pseudoepigráficas o deuteropaulinas», que fueron escritas con el nombre de Pablo, quizá por algunos discípulos suyos después de su muerte. Incluyen la Segunda epístola a los tesalonicenses, la Epístola a los colosenses, Epístola a los efesios, y tres «cartas pastorales», la Primera y la Segunda epístola a Timoteo y la Epístola a Tito. Desde el siglo XIX, distintos autores han negado la paternidad paulina directa de estas cartas, atribuyéndolas a varias figuras de discípulos posteriores. Con todo, otros autores sostienen la autoría paulina de estas cartas, en particular de Colosenses, argumentando que las variaciones en el estilo y en la temática se pueden justificar por el cambio del marco histórico en que se escribieron. (Ver sección sobre las epístolas pseudoepigráficas).

Saulo Pablo nació entre el año 5 y el año 10 en Tarso (en la actual Turquía), por entonces ciudad capital de la provincia romana de Cilicia, en la costa sur del Asia Menor.

En la Epístola a Filemón, Pablo se declaró ya anciano ("presbytés"). La escribió estando preso, bien a mediados de la década del año 50 en Éfeso, o bien a principios de la década del año 60 en Roma o Cesarea. Se supone que en aquella época se alcanzaba la ancianidad hacia los cincuenta o sesenta años. A partir de este dato, se estima que Pablo nació a comienzos del siglo I, hacia el año 10.
Por lo tanto, fue contemporáneo de Jesús de Nazaret.

Lucas afirma que Pablo era oriundo de Tarso, ciudad situada en la provincia de Cilicia, información considerada digna de crédito. Corrobora esta tradición que la lengua materna de Pablo era el griego desde su nacimiento, y que no se observan semitismos en su uso de este idioma.

Además, Pablo utilizó la Septuaginta, traducción al griego de los textos bíblicos, empleada por las comunidades judías del mundo antiguo más allá de Judea. Este conjunto concuerda con el perfil de un judío de la diáspora nacido en una ciudad helenística. A esto se suma la inexistencia de tradiciones alternativas que mencionen otros posibles lugares de nacimiento, con excepción de una noticia tardía de Jerónimo de Estridón que consigna el rumor de que la familia de Pablo procedía de Giscala, ciudad de Galilea ("De viris illustribus" 5 —Comentario a Filemón—; fines del siglo IV), noticia considerada en general carente de respaldo.

Por entonces, Tarso era una ciudad próspera, de no poca importancia (Hechos 21, 39). Capital de la provincia romana de Cilicia desde el año 64 a. C., estaba enclavada a los pies de los montes Tauro y a orillas del río Cidno, cuya desembocadura en el mar Mediterráneo servía a Tarso de puerto. Tarso poseía importancia comercial, ya que formaba parte de la ruta que unía Siria y Anatolia. Además era el centro de una escuela de filosofía estoica. Se trataba, pues, de una ciudad conocida como centro de cultura, filosofía y enseñanza. La ciudad de Tarso tenía concedida la ciudadanía romana por nacimiento. Como se explicó anteriormente, esta situación constituye una explicación posible de que Pablo fuera ciudadano romano pese a ser hijo de judíos.

La información sobre la ciudadanía romana de Pablo solo es presentada por los Hechos de los Apóstoles, y no encuentra paralelismos en las cartas de Pablo, lo que aún hoy resulta motivo de debate. Contra esta noticia, Vidal García aduce que un ciudadano romano no hubiese sido apaleado, tal como asegura Pablo que le ocurrió a él en 2 Corintios 11, 24-25, ya que estaba prohibido. A favor, Bornkamm señala que el nombre "Paulus" era romano. Y, si no hubiera sido romano, Pablo no habría sido trasladado a Roma tras su detención en Jerusalén. Sin embargo, hay excepciones a ambos supuestos. Peter van Minnen, papirólogo e investigador especializado en documentos griegos del período helenístico y romano incluyendo los del cristianismo primitivo, defendió enérgicamente la historicidad de la ciudadanía romana de Pablo, sosteniendo que Pablo era descendiente de uno o más libertos, de quienes habría heredado la ciudadanía.

Hijo de hebreos y descendiente de la tribu de Benjamín, el libro de los Hechos de los Apóstoles señala además otros tres puntos respecto de Pablo: (1) que fue educado en Jerusalén; (2) que fue instruido a los pies del famoso rabino Gamaliel; y (3) que era fariseo.

La educación de Pablo es objeto de muchas especulaciones. La opinión mayoritaria de los especialistas señala que recibió la educación inicial en la misma ciudad de Tarso. Asimismo, se sugiere que se habría mudado a Jerusalén posteriormente, siendo adolescente, o ya un joven. Algunos estudiosos, que mantienen una actitud de gran reserva respecto de la información brindada por los Hechos, objetan estos datos. Otros no encuentran razón suficiente para descartar los datos del libro de los Hechos 22, 3 referidos a su educación a los pies de Gamaliel I el Viejo, autoridad de mente abierta. Según Du Toi, los Hechos y las cartas paulinas auténticas respaldan como más probable que Pablo fuera a Jerusalén en sus años de adolescencia. Más importante aún, este estudioso remarca que la dicotomía Tarso–Jerusalén debería superarse mediante el reconocimiento de que la persona de Pablo fue un punto de encuentro e integración de una variedad de influencias. La educación de Pablo a los pies de Gamaliel sugiere su preparación para ser rabino.

Que Pablo fuera fariseo es un dato que llegó a nosotros a partir del pasaje autobiográfico de la Epístola a los filipenses:

Sin embargo, estos versículos forman parte de un fragmento de la carta que algunos autores consideran un escrito independiente posterior al año 70. Hyam Maccoby cuestionó que Pablo fuese fariseo al afirmar que no se observa ningún rasgo rabínico en las cartas paulinas.

Con todo, el carácter fariseo de Saulo Pablo en su juventud suele ser aceptado sin reticencias por otros autores, a lo que se suman las palabras puestas en boca del Apóstol por el libro de los Hechos:

En resumen, Saulo Pablo sería un judío de profundas convicciones, estricto seguidor de la Ley mosaica.

Un tema discutido en la investigación del «Pablo histórico» es su estado civil, del cual no existe constancia clara. Los textos de 1 Corintios 7, 8 y 1 Corintios 9, 5 sugieren que, cuando escribió esa carta en la primera mitad de la década del año 50, no estaba casado, pero eso no aclara si nunca se había casado, si se había divorciado o si había enviudado.

En general, los investigadores suelen optar por dos posiciones mayoritarias:


Romano Penna y Rinaldo Fabris señalan otra posición posible: que Pablo y su presunta mujer se hubiesen separado. Ese supuesto podría vincularse con el llamado privilegio paulino establecido por el Apóstol, lo cual consiste en el derecho que tiene a romper el vínculo matrimonial la parte cristiana cuando la otra parte es infiel y no se aviene a vivir con ella pacíficamente.

Cabe plantearse si, habiendo estado Saulo Pablo en Jerusalén «a los pies de Gamaliel», conoció personalmente a Jesús de Nazaret durante su ministerio o al momento de su muerte. Las posiciones de los estudiosos son diversas, pero en general se presume que no fue así, ya que no hay mención de ello en sus epístolas. Resulta razonable pensar que, si hubiera sucedido un encuentro semejante, Pablo lo habría consignado en algún momento por escrito.

Siendo este el caso, cabría también cuestionar la presencia permanente de Saulo Pablo en Jerusalén en sus años de adolescencia o juventud. A partir de Hechos 26, 4-5, Raymond E. Brown sugiere que Saulo Pablo era fariseo desde su juventud. Dado que resultaría infrecuente la presencia de maestros fariseos fuera de Palestina y que, además del griego, Pablo conocía el hebreo, el arameo o ambos, la suma de toda esa información da pie a pensar que al iniciarse la década del año 30, Saulo Pablo se trasladó a Jerusalén con el fin de estudiar más profundamente la Torá.

Según los Hechos de los Apóstoles, el primer contacto fidedigno con los seguidores de Jesús lo tuvo en Jerusalén, con el grupo judeo-helenístico de Esteban y sus compañeros. Saulo Pablo aprobó la lapidación de Esteban el protomártir, ejecución datada de la primera mitad de la década del año 30.

En su análisis, Vidal García limita la participación de Saulo Pablo en el martirio de Esteban al señalar que la noticia sobre la presencia de Pablo en esa lapidación no pertenecería a la tradición original utilizada por Hechos. Bornkamm argumenta sobre la dificultad de suponer que Pablo haya estado siquiera presente en la lapidación de Esteban.

Con todo, otros autores (por ejemplo, Brown, Fitzmyer, Penna, Murphy O'Connor, etc.) no encuentran razones suficientes para dudar sobre la presencia de Pablo en el martirio de Esteban. Siempre según los Hechos, los testigos de la ejecución de Esteban pusieron sus vestidos a los pies del «joven Saulo» (Hechos 7, 58). Martin Hengel considera que Pablo podría tener en aquellos momentos unos 25 años.

El capítulo 8 de los Hechos de los Apóstoles muestra en los primeros versículos un cuadro panorámico de la primera persecución cristiana en Jerusalén, en el que Saulo Pablo se presenta como el alma de esa persecución. Sin respetar ni a las mujeres, llevaba a los cristianos a la cárcel.

No se habla de matanzas pero, en un discurso posterior en el templo (Hechos 22, 19-21), Pablo señaló que andaba por las sinagogas encarcelando y azotando a los que creían en Jesús de Nazaret. En Hechos 9,1 se indica que las intenciones y propósitos de Saulo eran amedrentar de muerte a los fieles. Y en Hechos 22, 4 se coloca en boca de Pablo su persecución «hasta la muerte», encadenando y encarcelando a hombres y mujeres.

Vidal García y Bornkamm manifiestan su desconfianza respecto de los alcances reales de esa persecución, tanto desde el punto de vista de su extensión geográfica cuanto de su grado de violencia. Barbaglio señala que los Hechos hacen aparecer a Pablo, «no como el perseguidor sino como la persecución personificada», por lo que no se los puede considerar una crónica neutra. Sanders sostiene que esa persecución se debió al celo de Saulo Pablo, y no a su condición de fariseo. Más allá de los alcances precisos de su carácter persecutorio, se podría resumir —en palabras de Gerd Theissen— que la vida del "Pablo precristiano" se caracterizó por «el orgullo y el celo ostentoso por la Ley».

Según el libro de los Hechos de los Apóstoles, luego del martirio de Esteban, Saulo Pablo se dirigió a Damasco, hecho que los biblistas tienden a situar en el término del año subsiguiente a la lapidación de Esteban, según se comenta en la sección anterior (ver además el análisis de V. M. Fernández y bibliografía allí citada).

Pablo mismo presentó esta experiencia como una «visión» (1 Corintios 9, 1), como una «aparición» de Jesucristo resucitado (1 Corintios 15, 8) o como una «revelación» de Jesucristo y su Evangelio (Gálatas 1, 12-16; 1Corintios 2, 10). Pero nunca presentó esta experiencia como una «conversión», porque para los judíos «convertirse» significaba abandonar a los ídolos para creer en el Dios verdadero, y Pablo nunca había adorado a ídolos paganos, ni había llevado una vida disoluta. Los biblistas tienden a acotar a un marco muy preciso el significado del término «conversión» aplicado a Pablo. En realidad, cabe que Pablo interpretara que tal experiencia no lo hacía menos judío, sino que le permitía llegar a la esencia más profunda de la fe judía. Por entonces, el cristianismo aún no existía como religión independiente.

Existen varios puntos sin resolver respecto de este relato. Por ejemplo, en 1 Corintios 9, 1 Pablo señaló que «vio» a Jesús, pero en ningún pasaje de los Hechos (Hechos 9, 3-7; 22, 6-9; 26, 13-18) ocurre tal cosa. Más aún, los tres pasajes de Hechos no coinciden en los detalles: si los acompañantes quedaron en pie sin poder hablar o si cayeron por tierra; si oyeron o no la voz; asimismo, el hecho de que Jesús hablara a Pablo «en idioma "hebreo"», pero citando un proverbio griego (Hechos 26,14). Sin embargo, el núcleo central del relato coincide siempre:

Las epístolas paulinas guardan silencio sobre los detalles de este episodio, aunque el comportamiento previo y posterior de Pablo es señalado por él mismo en una de ellas.

En otra de sus epístolas afirmó:

Como resultado de esa «experiencia» vivida en el camino a Damasco, Saulo de Tarso, hasta entonces dedicado a «perseguir encarnizadamente» y «asolar» con «celo» a la «Iglesia de Dios» según sus propias palabras, transformó su pensamiento y su comportamiento. Pablo siempre habló de su condición judía en tiempo presente, y señaló que él mismo debía cumplir las normas dictaminadas por las autoridades judías. Probablemente nunca abandonó sus raíces judías, pero permaneció fiel a aquella experiencia vivida, considerada uno de los principales acontecimientos en la historia de la Iglesia.

Después del suceso vivido por Pablo en el camino de Damasco, Ananías lo curó de su ceguera imponiéndole las manos. Pablo fue bautizado y permaneció en Damasco «durante algunos días».

Desde la década de 1950 se presentaron trabajos científicos que sugirieron la presunta epilepsia de Pablo de Tarso, y se postuló que su visión y experiencias extáticas pudieron ser manifestaciones de epilepsia del lóbulo temporal. También se propuso un escotoma central como dolencia de Pablo, y que esa condición podría haber sido causada por retinitis solar en el camino de Jerusalén a Damasco. Bullock sugirió hasta seis posibles causas de la ceguera de Pablo en el camino a Damasco: oclusión de la arteria vertebrobasilar, contusión occipital, hemorragia vítrea secundaria/desgarro de retina, lesión causada por un rayo, intoxicación por "Digitalis", o ulceraciones (quemaduras) de la córnea. Con todo, el estado de salud física de Pablo de Tarso permanece desconocido.

Pablo de Tarso comenzó su ministerio en Damasco y Arabia, nombre con el cual se hacía referencia al reino nabateo. Fue perseguido por el etnarca Aretas IV, hecho que se suele datar de los años 38-39, o eventualmente de antes del año 36.

Pablo huyó a Jerusalén donde, según la Epístola a los gálatas (1, 18-19), visitó y conversó con Pedro y con Santiago. Según los Hechos (9, 26-28), fue Bernabé quien lo llevó ante los apóstoles. Podría interpretarse que fue entonces cuando le transmitieron a Pablo lo que más tarde mencionó en sus cartas haber recibido "por tradición" sobre Jesús (1 Corintios 11, 23; 1Corintios 15, 3). La estancia en Jerusalén fue breve: se habría visto obligado a huir de Jerusalén para escapar de los judíos de habla griega. Fue conducido a Cesarea Marítima y enviado a refugiarse en Tarso de Cilicia. Raymond Brown señala que no se conoce con exactitud cuanto tiempo permaneció allí, pero pudieron ser varios años.

Bernabé acudió a Tarso y fue con Pablo a Antioquía, donde surgió por primera vez la denominación de «cristianos» para los discípulos de Jesús. Pablo habría pasado un año evangelizando allí, antes de ser enviado a Jerusalén con ayuda para aquellos que sufrían hambruna (Hechos 11,25-30). Antioquía se convertiría en el centro de los cristianos convertidos desde el paganismo.

A partir del año 46 comienzan los tres grandes viajes misioneros de Pablo, que el revisionismo moderno interpreta se iniciaron con anterioridad, después del año 37. Los tres viajes son en realidad una clasificación con fines didácticos.

Pablo hacía generalmente sus viajes a pie (2 Corintios 11, 26). El esfuerzo realizado por Pablo de Tarso en sus viajes es digno de mención. Si se cuenta únicamente el número de kilómetros de los tres viajes por Asia Menor, se puede dar el siguiente resultado, según Josef Holzner:

A lo anterior habría que añadir los viajes por tierras de Europa y por mar, los caminos difíciles, las diferencias de altitud, etc. De una forma muy vívida, Pablo mismo describió en el pasaje siguiente lo que estos viajes implicaron:

En efecto, como viajero desprotegido de toda escolta, sería víctima fácil de bandidos, en particular en zonas rurales poco frecuentadas. Los viajes marítimos no eran más seguros: los vientos podían ser de ayuda proa al este, pero era peligroso poner rumbo a poniente y los naufragios eran frecuentes en cualquier sentido. Aun en las grandes ciudades greco-romanas como Éfeso, Pablo no dejaba de ser un judío, posiblemente con un zurrón al hombro, queriendo cuestionar toda la cultura en nombre de quien había sido considerado un criminal crucificado. Ni aun los «suyos» (los de su «clase», «raza» o «estirpe», es decir, los judíos) dejaban de sancionarlo. Finalmente, su labor ni siquiera finalizaba luego de predicar el evangelio de Jesucristo o conformar una comunidad.

El teólogo protestante alemán Gustav Adolf Deissmann enfatizó el punto al comentar que sentía «indecible admiración» a vista del esfuerzo puramente físico de Pablo, que con toda razón podía decir de sí mismo que «azotaba su cuerpo y lo domaba como a un esclavo» (1 Corintios 9, 27).

Enviados por la Iglesia antioquena, Bernabé y Pablo partieron en el primer viaje misional (Hechos 13-14), acompañados por Juan Marcos, primo de Bernabé que oficiaba de auxiliar. Del relato surge que Bernabé habría dirigido la misión en sus inicios. Zarparon de Seleucia, puerto de Antioquía ubicado a 25 km de la ciudad, hacia la isla de Chipre, patria de Bernabé.
Atravesaron la isla desde Salamina en la costa oriental de Chipre, hasta Pafos en la costa occidental.

En Pafos, Pablo logró un converso ilustre en la persona del procónsul romano Sergio Paulo. En su séquito se hallaba el mago Elimas, que procuró apartar al procónsul de la fe. Pablo lo llamó «repleto de todo engaño y de toda maldad, hijo del Diablo y enemigo de toda justicia», y dejó a Elimas ciego. Viendo lo ocurrido, el procónsul creyó. Desde Pafos los misioneros navegaron hacia Perge, en la región de Panfilia, en la costa sur del Asia Menor central. Es aquí donde el relato de los Hechos de los Apóstoles comienza a llamar a Saulo con su nombre romano Pablo, quien en adelante encabeza la misión. En esta etapa los dejó Juan Marcos para regresar a Jerusalén, con gran disgusto de Pablo como se indica más adelante.

Pablo y Bernabé continuaron viaje tierra adentro, hacia la Anatolia centro-meridional, tocando las ciudades del sur de Galacia: Antioquía de Pisidia, Iconio, Listra y Derbe. La norma constante en Pablo, tal como la presenta los Hechos, era la de predicar primero a los judíos a quienes suponía más preparados para recibir el mensaje. El relato de los Hechos muestra también la oposición activa que hacían «los de su raza» al anuncio evangélico. Ante la resistencia abierta que le opusieron manifestó su intención de dirigirse en adelante a los gentiles. Los paganos comenzaron a acogerlo gozosamente. Pablo y Bernabé deshicieron el camino desde Derbe, por Listra, Iconio y Antioquía de Pisidia, hasta Perge; embarcaron en Atalía con dirección a Antioquía de Siria, donde Pablo pasó algún tiempo con los cristianos.

Si bien las epístolas auténticas de Pablo no brindan ninguna información sobre este primer viaje, mencionan en cambio que predicó a los gentiles con antelación al concilio de Jerusalén y que sufrió una lapidación, la cual tendría correspondencia con la que padeció en Listra, según los Hechos.

Después de la primera misión paulina y durante la breve estadía del Apóstol en Antioquía, arribaron algunos judaizantes, cuya prédica señalaba la necesidad de la circuncisión para salvarse, por lo que desencadenaron un conflicto no menor con Pablo y Bernabé. La Iglesia de Antioquía envió a Pablo, Bernabé y algunos otros (entre ellos Tito, según Gálatas 2, 1) a Jerusalén para consultar a los apóstoles y ancianos. Según las palabras del propio Pablo, esta sería su segunda visita a Jerusalén después de su conversión («una vez más en catorce años»). Este acontecimiento se data tradicionalmente del año 49, en tanto que las posturas revisionistas varían en la datación, entre los años 47 y 51. Según Thiessen, este conflicto activó en Pablo su propia conversión, llevándola a debate público como argumento para instruir acerca del riesgo que implicaba admitir la circuncisión.

Si bien con algunos matices, este hecho aparece tanto en la Epístola a los gálatas como en el libro de los Hechos, y dio lugar a un conciliábulo conocido como el Concilio de Jerusalén, en el que triunfó la postura de Pablo sobre no imponer el ritual judío de la circuncisión a los conversos gentiles.

La decisión adoptada en el concilio implicó un avance en la liberación del cristianismo primitivo de sus raíces judías para abrirse al apostolado universal. La cuestión resuelta allí parece haber sido puntual, aunque con implicaciones doctrinales que excederían el problema planteado. En efecto, Pablo denunciaría más tarde la inutilidad de las prácticas cultuales propias del judaísmo, que incluían no solo la circuncisión (Gálatas 6, 12) sino además las observancias (Gálatas 4,10), para desembocar finalmente en la concepción de que no es el hombre el que "logra su propia justificación" como resultado de la observancia de la Ley divina, sino que es el sacrificio de Cristo el que lo justifica gratuitamente, es decir, que la salvación es un don gratuito de Dios (Romanos 3, 21-30).

Tras el concilio de Jerusalén, Pablo y Bernabé retornaron a Antioquía donde tendría lugar una disputa de importancia. Según Gálatas 2, 12-14, habiendo Simón Pedro comido con los gentiles, abandonó esta práctica ante la llegada de hombres de Santiago que presentaron objeciones a esa praxis.

Pablo reconocía la posición de Pedro, a quien consideraba uno de los pilares de la Iglesia de Jerusalén, pero se sintió obligado a protestar y «le resistió en el rostro». Le advirtió a Pedro que estaba violando sus propios principios y que no caminaba rectamente de acuerdo con la verdad del evangelio. No se trataba, pues, de una mera diferencia de opinión. Según Bornkamm, Pablo veía en la actitud de Pedro una recaída en el legalismo, que volvía la espalda al evangelio y a lo acordado anteriormente en Jerusalén, minimizando la importancia de la fe en Cristo como superior a la ley.

Es dudoso el resultado final de este incidente respecto de la prevalencia de una opinión u otra. En cualquier caso, el conflicto tuvo consecuencias. Según la Epístola a los gálatas, Bernabé también tomó posición a favor de los hombres de Santiago, y esta podría ser una razón adicional de la separación de Pablo y Bernabé, y de la salida de Pablo de Antioquía en compañía de Silas.

En el segundo viaje misionero Pablo se hizo acompañar por Silas. Partieron de Antioquía y, atravesando las tierras de Siria y Cilicia, alcanzaron Derbe y Listra, ciudades del sur de Galacia. En Listra se les unió Timoteo (Hechos 16, 1-3). Luego, a través de Frigia, se encaminó hacia el norte de Galacia, donde fundó nuevas comunidades. Por la Epístola a los gálatas se sabe que Pablo enfermó mientras atravesaba Galacia y que, durante esa estadía no planificada, gracias a su predicación surgieron allí las comunidades gálatas. No pudiendo proseguir hacia Bitinia, partió de Galacia hacia Misia y Tróade, donde se presume se le unió Lucas.

Decidió ir a Europa, y en Macedonia fundó la primera Iglesia cristiana europea: la comunidad de Filipos. Después de sufrir azotes con varas y prisión a manos de pretores romanos en Filipos, Pablo pasó a Tesalónica, donde tuvo una corta estadía destinada a la evangelización, matizada por sus controversias con los judíos.

La hostilidad de Tesalónica parece haber torcido la idea inicial de Pablo que, según los autores, sería la de dirigirse a Roma, capital del Imperio. Así lo indicaría el hecho de que Pablo transitó la reconocida "Vía Egnatia" hasta que, luego de Tesalónica, cambió el rumbo para adentrarse más en Grecia. En efecto, la estancia en Tesalónica finalizó con la huida de Pablo a Berea, y su posterior viaje a Atenas, donde intentó infructuosamente atrapar la atención de los atenienses, famosos por su avidez de novedades, con un discurso en el Areópago sobre el evangelio de Jesús resucitado. De allí se dirigió a Corinto, donde permaneció durante un año y medio, acogido por Aquila y Priscila, un matrimonio judeocristiano que había sido expulsado de Roma debido al edicto del emperador Claudio, y que se convertirían en amigos entrañables de Pablo. Durante su estadía en Éfeso, Pablo fue conducido ante el tribunal de Galión, procónsul de Acaya. Se trata de Lucio Junio Anneo Galión, hermano mayor del filósofo Séneca, cuyo mandato se menciona en la llamada inscripción de Delfos, una evidencia epigráfica que originalmente se hallaba en el templo de Apolo, descubierta en Delfos (Grecia) en el año 1905. Desde el punto de vista histórico, esta prueba es considerada clave y segura, y permite datar de los años 50 a 51 la presencia de Pablo en Corinto. En el año 51, Pablo redactó la Primera epístola a los tesalonicenses, el documento más antiguo del Nuevo Testamento. Al año siguiente volvió a Antioquía.

El tercer viaje de Pablo fue sin dudas complejo, y enmarcó su misión más sufrida, por varias razones. Esta etapa incluyó la experiencia de una muy fuerte oposición (en su propio decir, «fieras» y «muchos adversarios») y de tribulaciones (con probable prisión) que llegaron a «abrumar» al Apóstol, además de verse jalonada por las crisis que sacudieron las comunidades de Galacia y de Corinto y que motivaron la intervención de Pablo y de su equipo, por medio de sendas epístolas suyas y de visitas personales. Sin embargo, a la postre fue una de las misiones más fecundas. Tradicionalmente esta etapa se data de los años 54 a 57, en tanto que las posturas revisionistas tienden a ubicarla entre los años 51 y 54. En esa etapa de su vida, Pablo escribió buena parte de su obra epistolar.

Desde Antioquía, Pablo pasó por el norte de Galacia y Frigia «para confirmar a todos los discípulos» que había allí, y siguió hasta Éfeso, capital de Asia Menor, donde fijó su nueva sede de misión, y desde donde evangelizó toda el área de influencia acompañado por el equipo que dirigía.
Primero se dirigió a los judíos en la sinagoga pero, como luego de tres meses seguían manifestándose incrédulos, comenzó a impartir sus enseñanzas en la «escuela de Tirano». No se dispone de más información sobre esta «escuela». Sin embargo, esta breve noticia se considera verídica, aun por parte de quienes asumen una actitud de desconfianza ante el libro de los Hechos de los Apóstoles (por ejemplo Helmut Köester, discípulo de Bultmann, Bornkamm y Käsemann). Algunos conjeturan que se trataría de una escuela de retórica que alquilaba el local a Pablo en las horas libres. El texto occidental (códice de Beza) indica que Pablo enseñaba allí desde las 11 de la mañana hasta las 4 de la tarde («desde la hora quinta hasta la décima»). Si esta noticia es cierta, podría tratarse de una forma temprana de catequesis, efectuada de modo regular. Pero según Vidal, es posible que la enseñanza diaria de Pablo en «la escuela de Tirano» apuntara a una especie de escuela teológica paulina en esa ciudad, lugar de estudio de temas relacionados con la interpretación de la Escritura.

Poco después de llegar a Éfeso, Pablo habría escrito su carta a las iglesias de Galacia, motivada por las pretensiones de unos misioneros judaizantes opositores del Apóstol, que exigían la circuncisión a los cristianos gálatas de origen gentil. Tanto la carta, un manifiesto de la libertad cristiana para oponerse a la tentativa de judaización de aquellas Iglesias, como su portador Tito, tuvieron éxito al lograr la conservación de la identidad paulina de las comunidades de Galacia.

También en esta etapa llegaron a oídos de Pablo noticias sobre graves problemas surgidos en la Iglesia de Corinto: formación de facciones dentro de la comunidad, animadversión contra el propio Pablo, escándalos, y problemas doctrinales diversos, de todo lo cual se tiene noticia únicamente por sus cartas. Pablo les escribió por lo menos cuatro epístolas (según Vidal García, "op.cit.", hasta seis). De ellas se conservaron hasta hoy las dos conocidas, probables resultantes de la fusión por parte de un recopilador, quizá a fines del siglo I, de los originales fragmentados de cuatro. Las primeras dos cartas, hoy probablemente fusionadas en la que conocemos como Primera epístola a los corintios, constituyeron serias advertencias a esa comunidad contra las dramáticas divisiones dentro de ella, al igual que contra algunos casos escandalosos, como el de la unión conyugal incestuosa, y la práctica de la prostitución. Los problemas con esta comunidad continuaron, fomentados por unos misioneros enfrentados con el equipo paulino. Esto dio ocasión a la tercera carta, representada hoy por el fragmento de 2 Corintios 2, 14 - 7, 4. Entre la tercera y la cuarta carta, Pablo se dirigió a Corinto en la que constituyó una visita dolorosa: se encontró con una Iglesia levantada contra él, que incluso lo agravió públicamente. A su vuelta a Éfeso, Pablo escribió la cuarta carta a la comunidad corintia (2 Corintios 10, 1-13, 13), conocida como la "Carta de las lágrimas". No se trataba solo de un mensaje apologético de defensa frente a sus adversarios, sino que estaba cargado de emotividad.
Se considera segura la estadía de Pablo en Éfeso durante 2 o 3 años. Entre los sucesos narrados por los Hechos se cuentan el enfrentamiento de Pablo con los siete hijos exorcistas de un sacerdote judío y la llamada «revuelta de los plateros», una sublevación hostil provocada por un tal Demetrio y secundada por otros orfebres consagrados a la diosa Artemisa. La prédica de Pablo habría irritado a Demetrio, quien fabricaba pequeños santuarios
de plata copiando el de Artemis de Éfeso, con no pocas ganancias para él.

El tono del relato de los Hechos y el cuadro que describe es diferente del de las epístolas paulinas, por lo cual algunos estudiosos no están seguros de su historicidad. En cambio otros, aun señalando la ausencia de estas noticias en los escritos de Pablo, encuentran en sus cartas posibles alusiones a la tumultuosa estancia del Apóstol en Éfeso. Las dificultades que Pablo habría padecido en Éfeso sugieren que el Apóstol podría haber sufrido prisión allí. Esta posibilidad es importante no solo como hecho biográfico, sino además a la hora de datar el tiempo y lugar en que Pablo escribió su Epístola a los filipenses y la Epístola a Filemón, cuyas redacciones en el decir del propio Apóstol tuvieron lugar mientras estaba prisionero (Filipenses 1, 12-14; Filemón 1, 8-13).
No se puede aseverar si, luego de su estancia en Éfeso, Pablo marchó inmediatamente a Corinto o pasó de Macedonia al Ilírico, por vez primera, para girar una breve visita de evangelización. En cualquier caso, Pablo llegó a Corinto, en la que probablemente sería su tercera visita a aquella ciudad. Permaneció tres meses en Acaya.

En aquella época Pablo escribió la que, según la mayoría de los especialistas, fue la última carta de su autoría que se conserva: la Epístola a los romanos, datada de los años 55 a 58. Esta carta es el testimonio más antiguo de la existencia de la comunidad cristiana de Roma, y su nivel de importancia es tal que Bornkamm llega a referirse a ella como «el testamento de Pablo». Pablo señala entonces su proyecto de visitar Roma, y desde allí marchar a Hispania y el Occidente.

Entre tanto, Pablo venía pensando en regresar a Jerusalén. En ese tiempo procuró que sus iglesias gentiles realizaran una colecta para los pobres de Jerusalén. Cuando ya había decidido embarcarse en Corinto con rumbo a Siria, algunos judíos tramaron contra él una conjura y Pablo resolvió regresar por tierra, a través de Macedonia. Acompañado por algunos discípulos de Berea, Tesalónica, Derbe y Éfeso, Pablo se embarcó en Filipos hacia Tróade, pasando luego por Aso y Mitilene. Bordeando la costa de Asia Menor, navegó desde la isla de Quíos a la isla de Samos y luego a Mileto, donde pronunció un importante discurso a los ancianos de la Iglesia de Éfeso convocados allí. Luego navegó hasta la isla de Cos, Rodas, Patara de Licia y Tiro de Fenicia, Tolemaida y Cesarea Marítima. Por tierra llegó a Jerusalén, donde habría logrado entregar la colecta que tan arduamente había reunido.

Se sabe por la Epístola a los romanos 15 que Pablo veía con cierta preocupación su retorno a Jerusalén, tanto por la posibilidad de ser perseguido por los judíos como por la reacción que pudiera tener la comunidad de Jerusalén hacia su persona y hacia la colecta realizada por las comunidades que él había fundado.
Llamativamente, los Hechos de los Apóstoles no comentan la entrega de la colecta, lo que podría ser indicio de un final conflictivo en el cual Pablo no alcanzó a disolver los recelos que aún perduraban en la comunidad de Jerusalén respecto de su predicación.

La última etapa de la vida de Pablo, que abarca desde su apresamiento en Jerusalén hasta su presencia en Roma, tiene como fuente fundamental el relato de Hechos de los Apóstoles 21, 27 - 28, 31, aunque el autor de Hechos no trata el deceso del Apóstol. Si bien autores cualificados de diversas extracciones reconocen que el relato no responde a criterios estrictos de historicidad al detalle, sin embargo también se considera que el relato atesora varias noticias históricas sin duda fidedignas.

Santiago aconsejó a Pablo que su comportamiento durante su estadía en Jerusalén fuera el de un judío piadoso y practicante, y Pablo aceptó, todo lo cual se considera digno de crédito. Cuando el período ritual de setenta días estaba por cumplirse, algunos judíos procedentes de la provincia de Asia vieron a Pablo en los recintos del Templo y le acusaron de patrocinar una violación de la Ley y de haber profanado la santidad del Templo introduciendo en él a unos griegos. Intentaron matarlo en una revuelta, de la que fue sustraído mediante el arresto por parte del tribuno de la cohorte romana con asiento en la Fortaleza Antonia. Conducido ante el Sanedrín, Pablo se defendió y terminó por suscitar una disputa entre los fariseos y los saduceos, ya que éstos últimos no creían en la resurrección mientras que los fariseos sí. Seguidamente, los judíos se habrían confabulado para matar a Pablo pero el tribuno lo envió al procurador de la provincia de Judea, Marco Antonio Félix, que residía en Cesarea Marítima, ante quien volvió a defenderse. El procurador postergó el juicio y dejó a Pablo en prisión durante dos años, Bornkamm considera que tanto el traslado de Pablo a Cesarea Marítima como la postergación de su juicio son datos fiables desde la crítica histórica. El caso fue revisado solo después de la llegada del siguiente procurador, Porcio Festo. Por haber apelado al César, Pablo fue enviado a Roma. La cronología más tradicional de la vida de Pablo ubicaba la redacción de la Epístola a los filipenses y de la Epístola a Filemón en este período de cautividad de Pablo en Cesarea Marítima, o posteriormente en su prisión en Roma.

Del azaroso viaje de Pablo a Roma en calidad de prisionero, se pueden obtener algunos datos fidedignos que incluyen la prolongada duración de la travesía, el acompañamiento de que fue objeto, y una detención obligada en la isla de Malta, que pudo extenderse durante tres meses.

El libro de los Hechos de los Apóstoles otorgó a la llegada de Pablo a Roma una importancia adicional al mero carácter histórico: para él significaba el cumplimiento de lo que consideraba ya previsto por Jesús en el comienzo del mismo libro respecto de que el Evangelio sería llevado a todas las naciones. Algunos estudiosos señalan además cierta ironía apologética en la forma en que el libro de los Hechos de los Apóstoles describe la llegada de Pablo a Roma: no por libre voluntad, como se lo había propuesto una década antes sin lograrlo, sino como prisionero sujeto al César, con lo que los romanos se convirtieron en agentes indirectos del afianzamiento del evangelio en el centro mismo de su Imperio.

La cautividad de Pablo en Roma, considerada un hecho fidedigno, habría tenido una duración de dos años, tiempo en que el Apóstol no vivió encarcelado sino en custodia lo que, sin embargo, acotó sus libertades.

Una de las cuestiones sobre la que no existe una definición clara es si, luego de esa custodia domiciliaria de Pablo en Roma, se produjo su liberación seguida de algún otro viaje (por ejemplo, si llevó adelante su proyecto de viajar a Hispania), antes de morir en la misma Roma. Favorecen esta hipótesis la Primera epístola de Clemente y el Fragmento Muratoriano. En el presente se tiende a desconsiderar estas noticias como carentes de suficiente sustento. Resulta razonable pensar que el autor que finalizó la escritura de los Hechos de los Apóstoles hacia el año 80 conocía el final de Pablo. Si Pablo hubiese sido liberado anteriormente de su prisión, esto habría sido señalado en el libro, lo que no sucede. Un Congreso internacional sobre los últimos años del apóstol de los gentiles tuvo lugar en el Centro Tarraconense "El Seminario" entre los días 25 y 29 de junio de 2013, organizado por el Instituto Superior de Ciencias Religiosas San Fructuoso (INSAF), y fue ocasión para debatir nuevamente sobre los últimos años de vida del Apóstol, su eventual consideración como apóstata judío y su posible condena al exilio en Hispania. Tanto quienes piensan que Pablo llegó a Tarraco, como los que piensan que nunca llegó, admiten que por el momento no es posible llegar a una conclusión clara y definitiva sobre el tema, aunque —según el profesor de Nuevo Testamento y decano de la Facultad de Teología de Cataluña Armand Puig i Tàrrech— existen razones para afirmar como «plausible y altamente probable» que Pablo haya realizado una misión en Tarragona en condiciones penosas debidas a su condición de exiliado.

En cambio, tanto la tradición eclesiástica como los análisis historiográficos y exegéticos coinciden en señalar que la muerte de Pablo acaeció en Roma bajo el gobierno de Nerón, y que tuvo un carácter violento. 

Ya Ignacio de Antioquía señaló el martirio de Pablo en su "Carta a los efesios" XII, escrita probablemente en la primera década del siglo II. Respecto de la fecha, existe una tradición de su muerte en la misma época que Pedro (año 64) o un poco más tarde (67). Con todo, el mandato de Nerón se extendió entre los años 54 y 68, y la mayoría de los autores modernos tienden a señalar que la muerte del Apóstol se produjo antes de lo apuntado por Eusebio de Cesarea, más precisamente en el año 58, o a lo sumo a principios de la década de 60.

Eusebio de Cesarea describe que "está registrado que Pablo fue decapitado en la misma Roma, y que Pedro también fue crucificado bajo Nerón."

Tertuliano describe que Pablo sufrió una muerte similar a la de Juan el Bautista, quien fue decapitado:

Dionisio de Corinto, en una carta a la iglesia de Roma (166-174 d.C.), declaró que Pablo y Pedro fueron martirizados en Italia. Eusebio también cita el pasaje de Dionisio.

Lactancio nos relata en su obra "Sobre la muerte de los perseguidores (318 d.C.)" lo siguiente:
San Jerónimo, en su obra "De Viris Illustribus" (392 d.C.), menciona que "Pablo fue decapitado en el decimocuarto año de Nerón y que fue enterrado en la vía Ostia en Roma".

El texto apócrifo escrito en el año 160 conocido bajo el título de "Hechos de Pablo" señaló que el martirio de Pablo habría sido por decapitación.

Se encuentra documentada la forma en que se desarrolló prontamente el culto a Pablo en Roma y cómo se expandió posteriormente por distintas localidades europeas y norteafricanas.

Entre las fuentes más antiguas que vinculan la muerte de Pablo con Roma se encuentran el testimonio de su sepultura en la "vía Ostiensis" por parte del presbítero Caius a fines del siglo II o principios del siglo III, y un calendario litúrgico del siglo IV sobre el entierro de los mártires.

Asimismo, la "Pasión de Pablo" del Pseudo Abdías (siglo VI) señaló la sepultura del Apóstol «fuera de la ciudad […], en la segunda milla de la vía Ostiense», más precisamente «en la hacienda de Lucina», una matrona cristiana, donde más tarde se levantaría la basílica de San Pablo Extramuros.

Hacia el siglo V, el texto apócrifo del Pseudo Marcelo, conocido bajo el título de "Hechos de Pedro y Pablo" 80, señaló que el martirio de Pablo habría sido por decapitación en las "Acque Salvie", en la vía Laurentina, hoy abadía delle Tre Fontane, con un triple rebote de su cabeza que aseguraba haber causado la generación de tres vías de agua. Esta noticia es independiente de todas las anteriores y tardía, lo que sugiere su carácter legendario.

Tras una serie de excavaciones realizadas en la basílica romana de San Pablo Extramuros desde 2002, un grupo de arqueólogos del Vaticano descubrieron en 2006 restos humanos óseos en un sarcófago de mármol ubicado bajo el altar mayor del templo. La tumba data aproximadamente del año 390. Mediante la técnica de datación por medición del carbono-14, pudo determinarse que los restos óseos datan del siglo I o II. En junio de 2009, el papa Benedicto XVI anunció los resultados de las investigaciones realizadas hasta ese momento y expresó su convicción de que, por los antecedentes, ubicación y datación, podría tratarse de los restos del Apóstol.

Tanto durante su vida como en las siguientes generaciones, la figura y el mensaje de Pablo de Tarso fueron motivo de debate, generaron juicios de valor marcadamente contrastantes, y llegaron a suscitar reacciones extremas. De hecho, el propio Clemente de Roma sugirió que Pablo fue entregado a la muerte «por celos y envidias».
Por una parte, tres de los padres apostólicos de los siglos I y II, Clemente de Roma, Ignacio de Antioquía (particularmente en su "Carta a los romanos") y Policarpo de Esmirna (en su "Epístola a los filipenses"), se refirieron a Pablo y manifestaron su admiración por él. Policarpo llegó a expresar que no sería capaz de aproximarse a «la sabiduría del bienaventurado y glorioso Pablo»:

Por otra, la corriente judeocristiana de la Iglesia primitiva tendió a ser refractaria a Pablo, a quien pudo considerar rival de Santiago y Pedro, los líderes de la Iglesia de Jerusalén. De allí que especialistas como Bornkamm interpreten que la Segunda epístola de Pedro, un escrito canónico tardío datado de los años 100-150, expresa cierta «cautela» respecto de las epístolas paulinas. Si bien esta carta menciona a Pablo como «querido hermano», parece tratar sus escritos con alguna reserva por las dificultades que podrían suscitarse en su comprensión, con lo que «los débiles o no formados podrían torcer su doctrina, para su propia perdición» (2 Pedro 3, 15-16).

Los padres de la Iglesia subsiguientes avalaron y utilizaron las cartas de Pablo de forma sostenida. Ireneo de Lyon, a fines del siglo II y a propósito de la sucesión apostólica en las distintas iglesias, señaló a Pablo junto a Pedro como base de la Iglesia de Roma. Contra los extremismos, tanto de los judeocristianos antipaulinos como de Marción y de los gnósticos, el propio Ireneo expuso su postura según la cual existía consonancia entre los evangelios, los Hechos de los Apóstoles, las cartas paulinas y las Escrituras hebreas:

Quizá el culmen de la influencia de Pablo de Tarso entre los padres de la Iglesia haya tenido lugar en la teología de Agustín de Hipona, en particular contra el pelagianismo. La diversidad notable de valoraciones de la figura y obra de Pablo continuaron a través del tiempo, y se puede resumir en el decir de Romano Penna:

Las interpretaciones que de los escritos de Pablo de Tarso hicieron Martín Lutero, Juan Calvino tuvieron influencia importante en la Reforma Protestante del siglo XVI. En el siglo XVIII, el epistolario paulino fue fuente de inspiración para el movimiento de John Wesley en Inglaterra. En el siglo XIX, resurgió la hostilidad declarada contra Pablo. Quizá el detractor más extremo en su ferocidad haya sido Friedrich Nietzsche en su obra "El Anticristo", donde acusa a Pablo y a las primeras comunidades cristianas de desvirtuar totalmente el mensaje de Jesús:

Paul de Lagarde quien pregonaba una «religión alemana» y una «iglesia nacional», atribuyó lo que él consideró la «evolución nefasta del cristianismo» al hecho de que «una persona absolutamente incompetente (Pablo) logró influir en la iglesia». En las antípodas, la teología dialéctica de Karl Barth, un antecedente intelectual relevante en la lucha contra el nacionalsocialismo, nació con el comentario de 1919 de este teólogo suizo a la Carta a los romanos.

Con todo, Raymond E. Brown previno acerca de dos tendencias: (1) la que propende a maximizar ciertas perspectivas referidas a Pablo, y (2) la que extrema las diferentes posturas que pudieran haber existido en las primeras comunidades cristianas. Más allá de las diferencias entre el cristianismo paulino por un lado y el judeocristianismo de Santiago y Pedro por otro, ellos mantuvieron una fe en común. Y la fecha tardía de la redacción de la Segunda epístola de Pedro permite suponer que las diferencias de opinión existentes entre las distintas corrientes básicas del cristianismo primitivo no sofocaron su pluralidad interna, tal como cristalizó en el canon bíblico.

La teología de la redención fue uno de los principales asuntos abordados por Pablo. Pablo enseñó que los cristianos fueron redimidos de la Ley (ver supersesionismo) y del pecado por la muerte de Jesús y su resurrección. Su muerte fue una expiación y, por la sangre de Cristo, se estableció la paz entre Dios y el hombre. Por el bautismo, un cristiano toma su parte en la muerte de Jesús y en su victoria sobre la muerte, recibiendo gratuitamente una renovada condición de hijo de Dios.

Pablo era judío, de la escuela de Gamaliel, de denominación fariseo, mencionando esto último como algo de lo que se sentía orgulloso (Fil 3:5).
El punto principal de su mensaje era que los gentiles no tienen necesidad de circuncidarse al igual que los judíos (1Cor 3:2), de hecho una buena parte de sus enseñanzas es un énfasis a los gentiles para que comprendan que su salvación no depende de copiar los rituales judíos; sino que tanto judíos como gentiles, en última estancia, son salvos por gracia Divina [claro que la gracia Divina se aplica por medio de la Fe (fidelidad)]. Los estudiosos contemporáneos, sin embargo, debaten acerca de si cuando Pablo habla de "fe/fidelidad en/de Cristo" (el genitivo griego es susceptible de ambas interpretaciones, objetiva y subjetiva) se refiere en todos los casos a la fe en Cristo como algo necesario para alcanzar la salvación (no sólo por parte de los gentiles, sino también de los judíos) o si en ciertos casos se refiere más bien a la fidelidad del propio Cristo hacia los hombres (como instrumento de la salvación divina dirigida a los judíos y los gentiles por igual)

Fue el pionero en comprender que el mensaje de salvación de Jesús que comenzaba en Israel, se expandía a toda criatura independientemente de su origen. Para Saulo (en hebreo: Shaúl) los seguidores gentiles de Jesús no deben seguir los mandamientos de la Torá (ley) que son exclusivos al pueblo de Israel. Y así queda establecido en el Concilio de Jerusalén (Gal 2:7-9), que los gentiles sólo deben guardar los preceptos de los gentiles (comúnmente conocidos en el judaísmo como: preceptos noájidas; Hch 21:25; Talmud, Sanedrín 56a y b).

Muchas de sus enseñanzas, al ser dirigidas a un pueblo gentil eran mal entendidas y mal interpretadas (2P 3:15-16). Algunos judíos por un lado interpretaron que Pablo enseñaba a abandonar la Torá de Moisés (Hch 21:28; Hch 21:21), lo cual no era cierto, y él mismo lo tuvo que desmentir (Hch 25:8; Hch 21:24,26).
Por otro lado, había gentiles que interpretaban que la salvación por gracia les permitía pecar, y también lo tuvo que desmentir (Rom 6:15).

Recientemente, algunos investigadores como Krister Stendahl, Lloyd Gaston, John G. Gager, Neil Elliott, William S. Campbell, Stanley K. Stowers, Mark D. Nanos, Pamela Eisenbaum, Paula Fredriksen, Caroline Johnson Hodge, David J. Rudolph y, en España, Carlos A. Segovia, han defendido que Pablo no buscó superar ni reformar el judaísmo, sino incorporar a los gentiles a Israel por medio de Cristo sin obligarles a renunciar a su condición de gentiles. Esta interpretación recibe el nombre "nuevo enfoque radical sobre Pablo" y contrasta tanto con su interpretación cristiana tradicional como con la llamada "nueva perspectiva sobre Pablo" de James D. G. Dunn y Nicholas Thomas Wright, según la cual Pablo se propuso reformar el judaísmo.

Un versículo en la Primera Epístola a Timoteo, tradicionalmente atribuido a Pablo, muchas veces es utilizado como mayor fuente de autoridad en la Biblia para que las mujeres sean vedadas al sacramento del orden, además de otras posiciones de liderazgo y ministerio en el cristianismo. La Epístola a Timoteo es también muchas veces utilizada por muchas iglesias para negarles el voto en asuntos eclesiásticos y posiciones de enseñanza para público adulto y también el permiso para el trabajo misionero.

Este pasaje parece estar diciendo que las mujeres no deben tener en la iglesia ningún papel de liderazgo frente a los hombres. Si ella también prohíbe a las mujeres enseñar a otras mujeres o a niños es dudoso, pues incluso las iglesias católicas -que prohíben el sacerdocio femenino- permiten que abadesas enseñen y asuman posiciones de liderazgo sobre otras mujeres. Cualquier interpretación de esta parte de las Escrituras tiene que confrontarse con las dificultades teológicas, contextuales, sintácticas y léxicas de estas pocas palabras.

El teólogo JR Daniel Kirk encontró un importante papel para las mujeres en la iglesia antigua, como por ejemplo cuando Pablo elogió a Febe por su trabajo como diaconisa y también Junia,}} considerada por algunos como la única mujer en ser citada en el Nuevo Testamento entre los apóstoles. Kirk apunta a estudios recientes que llevaron a algunos a concluir que el paso que obliga a las mujeres a "quedarse calladas en las iglesias" en 1 Corintios 14, 34 fue una adición posterior, aparentemente por un autor diferente y no era parte de la carta original de Pablo a la iglesia de Corinto. Otros, como Giancarlo Biguzzi, alegan que la restricción de Pablo sobre las mujeres en Corintios es genuina, pero se aplica al caso particular de prohibirlas de hacer preguntas o de conversar, y no una prohibición generalizada contra las mujeres hablar, pues en 1 Corintios 11, 5 Pablo afirma el derecho de las mujeres de profetizar.
El tercer ejemplo de Kirk de una visión más inclusiva está en Gálatas 3, 28 Al anunciar un fin dentro de la iglesia de las divisiones que eran tan comunes en todo el mundo, concluye destacando que ""...había mujeres del Nuevo Testamento que enseñaron y tenían autoridad en la iglesia antigua y que estas enseñanzas y esta autoridad eran sancionadas por Pablo y que el apóstol mismo ofrece un paradigma teológico dentro del cual la superación de la subyugación de la mujer es un resultado esperado"".

El carácter y el legado de Pablo se verificaron: (1) en las comunidades por él fundadas y en sus colaboradores; (2) en sus cartas auténticas; y (3) en las llamadas cartas deuteropaulinas, surgidas quizá de una escuela que nació y creció en torno al Apóstol. Es a partir de ese legado inmediato que surgió todo su influjo posterior.

Pablo utilizó para con sus comunidades y colaboradores un lenguaje apasionado. A los tesalonicenses les escribió que eran su esperanza, su gozo, su corona, su gloria; a los filipenses les dijo que Dios era testigo de cuánto los amaba con el entrañable amor de Jesucristo, y que resplandecían como antorchas en el mundo. A los miembros de la comunidad de Corinto les advirtió que no sería indulgente con ellos, pero no sin antes comentarles que les había escrito con muchas lágrimas para que supieran cuán grande era el amor que les tenía.

Se especula que Pablo debió ser un hombre capaz de suscitar profundos sentimientos de amistad, ya que sus cartas dan muestras de lealtad por parte de un amplio abanico de personajes con nombre propio. Timoteo, Tito, Silas, todos formaron parte del equipo paulino, llevando sus cartas y sus mensajes, a veces en circunstancias difíciles. Los esposos cristianos Priscila –también llamada Prisca– y Aquila, cuya amistad hacia Pablo de Tarso resultó entrañable, fueron capaces de levantar su tienda y partir con él desde Corinto a Éfeso y luego ir a Roma, de donde habían sido exiliados previamente, para preparar la llegada del Apóstol. Vidal sugiere que en Éfeso fueron ellos quienes, en una intervención riesgosa, habrían logrado la liberación de Pablo, lo que justificó el encomio del Apóstol:

A ellos se suma Lucas, a quien por tradición se identifica con el autor del evangelio homónimo y de los Hechos de los Apóstoles. Se menciona su nombre entre los de los colaboradores de Pablo. Según la Segunda epístola a Timoteo, habría acompañado a Pablo hasta su final (2 Timoteo 4, 11).

Las cartas auténticas de Pablo son un conjunto de escritos neotestamentarios conformado por las siguientes obras:

Este "corpus" de epístolas auténticas es único en más de un sentido:

Aunque las cartas tuvieron por función inmediata abordar problemas resultantes de situaciones concretas, es muy verosímil que las comunidades a las cuales estas cartas estuvieron dirigidas las hayan atesorado, y que prontamente las compartieran con otras comunidades paulinas. Así, resulta altamente probable que hacia fines del siglo I estos escritos ya existieran como "corpus", resultante del trabajo de una escuela paulina que recopiló sus cartas para conformar el legado escrito del Apóstol.

Existe, además de las cartas de Pablo, un conjunto de escritos epistolares que se presentan como suyos pero que la crítica moderna, conocedora del fenómeno de la pseudoepigrafía típico de las obras antiguas orientales y griegas, atribuye a diferentes autores asociados con Pablo. Se trata de las siguientes obras:

El hecho de que se sugiera que estos escritos canónicos son pseudoepigráficos o deuteropaulinos, lejos de quitarle notoriedad al Apóstol la incrementaron, porque significa que una «escuela», quizá ya establecida en torno al mismo Pablo y depositaria de su legado, recurrió a la autoridad del Apóstol para validar sus escritos.

Se denomina teología paulina al estudio razonado, sistemático e integral del pensamiento de Pablo de Tarso, que experimentó desarrollos y retoques en las sucesivas interpretaciones que se hicieron de sus escritos. La presentación sumaria de la teología de san Pablo es muy ardua. La mayor dificultad de cualquier intento de sistematización del pensamiento del Apóstol radica en que Pablo no era un teólogo sistemático, por lo cual cualquier categorización y ordenamiento parece responder más a las preguntas del exégeta que a esquemas paulinos.

Por mucho tiempo el debate estuvo supeditado a una disyuntiva. Según la tesis luterana clásica, el tema fundamental de la teología paulina sería el de la justificación de la fe sin las obras de la Ley. A partir de esa tesis se llegó a considerar que en la doctrina paulina así entendida estaba el núcleo central del anuncio cristiano. En el siglo XX, la postura a favor del principio de la "sola fide" fue una constante en el trasfondo y en la orientación del pensamiento de Rudolf Karl Bultmann y también se presentó, con una variedad de matices, en seguidores suyos tales como Ernst Käsemann o G. Bornkamm.

Desde el punto de vista del catolicismo, si bien la justificación forma parte del mensaje paulino, no constituye su núcleo central único. El argumento tradicional católico sostenía que Dios, más que «declarar justo» al hombre, hace justo al hombre transformándolo.

En los últimos años, diferentes estudiosos protestantes, tales como Krister Stendahl, Ed Parish Sanders, y James D. G. Dunn, criticaron la postura luterana clásica que oponía una fe cristiana portadora de la gracia y de la libertad contra un presunto judaísmo tradicional afecto al legalismo y exaltación soberbia de la observancia de las prescripciones mosaicas. Después de presentar la dificultad de «escribir una teología de Pablo», James Dunn propuso en su libro a modo de esquema lo siguiente: Dios y la humanidad – la humanidad bajo interdicción – el Evangelio de Jesucristo – el comienzo de la salvación – el proceso de la salvación – la Iglesia – la ética.

Los autores católicos (Lucien Cerfaux, Rudolf Schnackenburg, y particularmente Joseph A. Fitzmyer) centraron la teología de Pablo en su pensamiento sobre Cristo, particularmente sobre su muerte y su resurrección. J. Fitzmyer señaló la cristología como centro de la teología paulina. Para él, la teología paulina sería una teología cristocéntrica, es decir, una teología cuyo eje principal es Cristo muerto y resucitado. Otros autores como Joachim Gnilka y Giuseppe Barbaglio hablan de un teocentrismo paulino, lo que quiere implicar que todo el pensamiento de Pablo arranca de Dios y vuelve a Él.

Por otra parte, una detallada observación de las epístolas paulinas auténticas permite advertir que en el pensamiento del Apóstol se produjo una evolución y que, en consecuencia, no se podría hablar de un único centro de interés en su predicación. G. Barbaglio propuso que el Apóstol escribe una «teología en epístola». De allí que el esquema de Barbaglio consistió en presentar la teología de cada carta siguiendo cronológicamente cada una de las siete epístolas auténticamente paulinas, para finalizar con un capítulo titulado: «Coherencia de la teología de Pablo: hermenéutica del Evangelio».

Según R. Penna, se tiende a aceptar que en el centro del pensamiento de Pablo se encuentra el «evento-Cristo», hecho concluyente en «su teología». La discusión discurre sobre las consecuencias (antropológicas, escatológicas, eclesiológicas) de ese dato. Brown sugirió que todas las propuestas encierran parte de verdad, pero derivan de «juicios analíticos» posteriores a Pablo.

Pablo, como otros apóstoles relevantes, tuvo un amplísimo tratamiento en el arte. En especial, su episodio de conversión fue tratado por maestros italianos como Parmigianino (Museo de Historia del Arte de Viena), Miguel Ángel (mural en la Capilla Paulina del Palacio Apostólico de la Ciudad del Vaticano) y Caravaggio (Basílica de Santa María del Popolo, Roma). Otros momentos frecuentemente escogidos fueron la predicación en el Areópago (Rafael, Capilla Sixtina -también pintó el rechazo del mago Elimas y el sacrificio de Listra-), el descenso en canasta de las murallas de Damasco, el naufragio, el episodio de las serpientes, el éxtasis, la estancia en prisión y el martirio.

No suele aparecer en las series referidas a los doce apóstoles que conocieron en vida a Cristo, pero muy a menudo se le representa en pareja con Simón Pedro. En este caso suelen distinguirse por sus atributos: en san Pedro, las llaves que simbolizan su elección como jefe de la Iglesia, y en san Pablo la espada que simboliza su martirio -además de referirse a un pasaje de su carta a los Efesios: "la espada del Espíritu, que es la palabra de Dios"-). También es frecuente la presencia de un libro que representa su condición de autor de textos neotestamentarios (aunque esto también identifica a Pedro y a otros apóstoles). A veces se representa a Pedro y Pablo como teólogos debatiendo.

El origen de su iconografía, que fija unos rasgos característicos y repetidos a lo largo de los siglos, se remonta al arte paleocristiano, que la entronca con la tradición greco-romana de representación de filósofos como Plotino.





</doc>
<doc id="8838" url="https://es.wikipedia.org/wiki?curid=8838" title="Taquión">
Taquión

Un taquión (del griego ταχυόνιον "takhyónion" de ταχύς "takhýs" 'rápido, veloz') es toda aquella partícula hipotética capaz de moverse a velocidades superlumínicas. A los taquiones se les atribuyen muchas propiedades extrañas, sobre todo por parte de los autores de ciencia ficción.

En términos de la teoría de la relatividad especial de Einstein, un taquión es una partícula hipotética con un cuadrimomento de tipo espacial. Esto implica que si su energía y momento son reales, su masa en reposo convencional aparente sería un número imaginario. Por lo que la (pseudo)norma de Minkowski de su cuadrimomento sería negativa, ya que:

El tiempo propio que experimenta un taquión es también imaginario. Un curioso efecto es que a diferencia de partículas reales, la velocidad de un taquión "crece" cuando su energía decrece. Esto es una consecuencia de la relatividad especial debido a que, hipotéticamente, un taquión tiene masa cuadrada negativa. De acuerdo con Einstein, la energía total de una partícula es la masa en reposo "m" por la velocidad de la luz al cuadrado y multiplicada a su vez por el factor de Lorentz, es decir, la energía total viene dada por la relación:
}</math>
Para materia ordinaria, esta ecuación demuestra que "E" aumenta con la velocidad, convirtiéndose infinita a medida que "v" (velocidad) se aproxima a "c," la velocidad de la luz. Si "m" es imaginaria, por otra parte, el denominador de la fracción necesita ser imaginario para mantener a la energía como un número real. El denominador sería imaginario si el número en la raíz cuadrada es negativo, lo cual solo pasa si "v" es mayor que "c." 

Un taquión está limitado a la porción de tipo espacial del gráfico de energía-momento. Por tanto, nunca puede ir a velocidades inferiores a la de la luz. Curiosamente, mientras su energía disminuye, su velocidad aumenta.

Si existieran los taquiones y pudieran interactuar con la materia ordinaria, podría violarse el principio de causalidad.

En la teoría de la relatividad general, es posible construir espacio-tiempos en los cuales las partículas se propaguen más rápidamente que la velocidad de la luz, relativo a un observador distante. Un ejemplo es la métrica de Alcubierre. Sin embargo, estos no serían taquiones en el sentido anterior, puesto que no superarían la velocidad de la luz localmente.

En la teoría cuántica de campos, un taquión es el cuanto de un campo, usualmente un campo escalar el cual tiene una masa al cuadrado negativa. La existencia de tal partícula es un significado de la inestabilidad del vacío espacio-temporal, porque la energía del vacío tiene un máximo en vez de un mínimo. Un pequeño impulso podría causar una decadencia de amplitudes exponenciales que al mismo tiempo podrían inducir a una condensación de taquiones. El mecanismo de Higgs es un ejemplo elemental, pero es bueno darse cuenta que una vez que el campo taquiónico alcanza el mínimo de su potencial, su cuanta dejan de ser taquiones para convertirse en bosones de Higgs con masa positiva.

Los taquiones se encuentran en muchas versiones de la teoría de cuerdas. En general, la teoría de cuerdas establece que lo que vemos como "partículas" —electrones, fotones, gravitones, etc...— son en realidad diferentes estados vibratorios de la misma cuerda. La masa de una partícula puede ser deducida como de la vibración ejercida por la cuerda; en otras palabras, la masa depende de la nota que la cuerda este tocando. Los taquiones frecuentemente aparecen en el espectro de estados de cuerdas permisibles, como queriendo decir que en algunos estados tienen masas al cuadrado negativas, y por lo tanto masas imaginarias.

Partiendo de la ecuación de la Teoría de la relatividad:
}</math>
Factorizando el -1 en el denominador y operando:
</math>
Cambio de variable :formula_1
luego se multiplica el numerador y denominador por "i":
</math>
Si se define formalmente una "masa del taquión" mediante formula_2 se tiene:
.</math>
Se analiza la raíz y se obtiene que para que sea real, formula_3, la velocidad de la partícula debe ser mayor que "c" (velocidad de la luz).

En mecánica cuántica y teoría cuántica de campos un valor imaginario de la masa o la energía puede ser interpretado como una partícula inestable que decae en otras partículas, o como un estado inestable del vacío que da lugar a otros estados. En concreto la parte imaginaria de la energía está directamente relacionada con el tiempo de desintegración de dicho estado. Así los estados con energía real al ser su parte imaginaria nula pueden existir por tiempo indefinido. Para los estados o partículas con masa o energía imaginaria el tiempo de desintegración es inversamente proporcional a la parte imaginaria:

Siendo "E" la energía total compleja y siendo formula_4 la constante de Planck (partida de 2 pi) según el cociente entre la parte real e imaginaria de la energía μ = (Re "E")/(Im "E") las partículas inestables pueden clasificarse en:

En la ciencia ficción ha existido de forma magistral el uso de los taquiones como un todo que responde a la gran interrogante sobre cómo viajar y ser el amo del tiempo. Algunos ejemplos:




</doc>
<doc id="8840" url="https://es.wikipedia.org/wiki?curid=8840" title="Hipótesis (método científico)">
Hipótesis (método científico)

El término hipótesis está formado por dos palabras de origen griego: "hipo", que significa subordinación o por debajo y "tesis", conclusión que se mantiene con razonamiento, con lo cual podemos decir que la hipótesis sería "lo que se pone en la base". La hipótesis es un enunciado no verificado, una vez refutado o confirmado dejara de ser hipótesis y seria un enunciado verificado. La hipótesis es una conjetura científica que requiere una contrastación con la experiencia. Para ella no son suficientes los argumentos persuasivos, por más elaborados que sean. Se puede decir entonces, que de ciertas hipótesis se deducen otras y así sucesivamente hasta llegar a ciertos enunciados básicos, de observación directa.

Una hipótesis científica es una proposición aceptable que ha sido formulada a través de la recolección de información y datos, aunque no esté confirmada, sirve para responder de forma alternativa a un problema con base científica.

Una hipótesis puede usarse como una propuesta provisional que no se pretende demostrar estrictamente, o puede ser una predicción que debe ser verificada por el método científico. En el primer caso, el nivel de veracidad que se otorga a una hipótesis dependerá de la medida en que los datos empíricos apoyan lo afirmado en la hipótesis. Esto es lo que se conoce como contrastación empírica de la hipótesis o bien proceso de validación de la hipótesis. Este proceso puede realizarse mediante confirmación (para las hipótesis universales) y o mediante verificación (para las hipótesis existenciales).

Como se ha dicho, una hipótesis es una conjetura posible que se establece en forma de proposición afirmativa, en futuro simple o en condicional. Una hipótesis no se establece en forma de pregunta, como por ejemplo: "¿pueden los gansos sobrepasar los 85 km/h volando?" sino que de una suposición, de la que se cree que es algo viable y veraz, se afirma por ejemplo que: "los gansos pueden sobrepasar volando los 85 km/h"; o bien se asegura que: "los gansos sobrepasarán volando los 85 km/h"; o bien: "si un grupo de gansos escogido puede superar los 85 km/h, entonces podremos concluir que los gansos pueden sobrepasar volando los 85 km/h".

Además, especialmente desde Karl Popper, se ha insistido en que las hipótesis formuladas deben ser falsables, es decir, deben estar formuladas de una forma clara que permita construir un experimento que potencialmente pueda corroborar o contradecir la hipótesis. Si bien, diversas críticas al falsacionismo más simplista, han señalado que la falsabilidad no es una condición suficiente, aunque generalmente necesaria.

Los pasos de la hipótesis son: reunir información, compararla, dar posibles explicaciones, escoger la explicación más probable y formular una o más hipótesis. Después de hacer todos estos pasos (en la ciencia) se realiza una experimentación, en la que se confirma la hipótesis o no. Si la hipótesis es confirmada, entonces lo planteado como hipótesis es verdadero. En caso de que no sea confirmada, la hipótesis es falsa. y en caso de ser falsa hay que formular otra hipótesis con los datos reales que se han conseguido, para tener los datos correctos y un resultado eficaz.

Una hipótesis de investigación representa un elemento fundamental en el proceso de investigación. Después de formular un problema, el investigador enuncia la hipótesis, que orientará el proceso y permitirá llegar a conclusiones concretas del proyecto que recién comienza.

Toda hipótesis constituye un juicio o proposición, una afirmación o una negación de algo. Sin embargo, es un juicio de carácter especial. Las hipótesis son proposiciones provisionales y exploratorias y, por tanto, su valor de veracidad o falsedad depende críticamente de las pruebas empíricas disponibles. En este sentido, la replicabilidad o repetibilidad de los resultados es fundamental para confirmar una hipótesis como solución de un problema.

La hipótesis de investigación es el elemento que condiciona el diseño de la investigación y responde provisionalmente al problema, verdadero motor de la investigación. Como se ha dicho, esta hipótesis es una aseveración que puede validarse estadísticamente. Una hipótesis explícita es la guía de la investigación, puesto que establece los límites, enfoca el problema y ayuda a organizar el pensamiento. 

Una hipótesis se considera explicación y por tanto toma cuerpo como elemento fundamental de una teoría científica, cuando el conocimiento existente en el área permite formular predicciones razonables acerca de la relación de dos o más elementos o variables. 

Dicha hipótesis indica el tipo de relación que se espera encontrar:


Para que sea admitida como cuerpo de conocimiento científico, la hipótesis tiene que poder establecer una cuantificación determinada o una proporción matemática que permita su verificación estadística, pues el argumento meramente inductivo no es científicamente concluyente."

Las hipótesis son el punto de enlace entre la teoría y la observación. Su importancia es que dan rumbo a la investigación al sugerir los pasos y procedimientos que deben darse en la búsqueda del conocimiento.

Cuando una hipótesis de investigación ha sido bien elaborada, y en ella se observa claramente la relación o vínculo entre dos o más variables, es factible que el investigador pueda:



Asimismo, cada tipo de hipótesis tiene sus características extra.

En un trabajo de investigación generalmente se plantean dos hipótesis mutuamente excluyentes: la hipótesis nula o hipótesis de nulidad (formula_1) y la hipótesis de investigación (formula_2). La hipótesis de investigación es una afirmación especial cuya validez se pretende demostrar, y si las pruebas empíricas no apoyan decididamente la hipótesis de investigación, entonces se aceptará la hipótesis nula, abandonándose la hipótesis de investigación.

En algunos casos es posible plantear hipótesis alternas o hipótesis alternativas. El análisis estadístico de los datos servirá para determinar si se puede o no aceptar formula_2. Cuando se rechaza formula_1, significa que el factor estudiado ha influido significativamente en los resultados y es información relevante para apoyar la hipótesis de investigación formula_2 planteada. Plantear hipótesis de investigación que no sea excluyente con formula_1 supondría una aplicación incorrecta del razonamiento estadístico.

Algunas hipótesis involucran variables cuantitativas que pueden poseer una relación causal establecida. En ocasiones el investigador tendrá control o capacidad de observación sobre unas variables y sobre otras no, en estas dimensiones las variables involucradas pueden clasificarse en:

En esta sección se proponen algunos ejemplos de las diferentes tipologías de hipótesis que se pueden hacer:




</doc>
<doc id="8843" url="https://es.wikipedia.org/wiki?curid=8843" title="Síndrome">
Síndrome

En medicina, un síndrome
(del griego συνδρομή "syndromé", 'concurso') es un cuadro clínico o un conjunto sintomático que presenta alguna enfermedad con cierto significado y que por sus propias características posee cierta identidad; es decir, un grupo significativo de síntomas y signos (datos semiológicos), que concurren en tiempo y forma, y con variadas causas o etiología.

Todo síndrome es una entidad clínica que asigna un significado particular o general a las manifestaciones semiológicas que la componen. El síndrome es plurietiológico porque tales manifestaciones semiológicas pueden ser producidas por diversas causas. De este modo, es común diferenciar entre formas primarias y secundarias de un mismo síndrome. Las formas primarias corresponden a aquellos síndromes no relacionables con una etiología o enfermedad conocida, mientras que las formas secundarias son aquellos síndromes con etiología conocida o al menos vinculados clínicamente con otra enfermedad. Por ejemplo, el síndrome de Raynaud describe un trastorno vasomotor que afecta fundamentalmente a las manos y que puede aparecer como forma primaria/idiopática o asociado a numerosas causas como enfermedades autoinmunes, hematológicas, fármacos, etc.

Si bien por definición, síndrome y enfermedad son entidades clínicas con un marco conceptual diferente, hay situaciones en la patología que dificultan una correcta identificación de ciertos procesos en una categoría o en otra.

Como término médico los primeros ejemplos seguros los encontramos en Galeno, s. II d. C., quien cita la palabra reiteradamente como un término propio de médicos empíricos. Galeno documenta la primera aparición de muchas palabras en el vocabulario de la medicina a pesar de que él no fue un creador de vocabulario.

Celio Aureliano, médico del Norte de África del s. V d. C. que escribía en latín, tradujo con el término "concursus", por lo que no aparece la palabra en su forma griega transliterada al latín —es decir, "syndrome"— hasta el Renacimiento.

El primer idioma moderno en que se documenta la palabra es en inglés en una fecha tan temprana como 1541 en una traducción de Galeno de Copland; algunos años más tarde aparece en francés.




</doc>
<doc id="8844" url="https://es.wikipedia.org/wiki?curid=8844" title="Consumo de oxígeno">
Consumo de oxígeno

El consumo de oxígeno (O), en fisiología, corresponde al volumen de dioxígeno que el cuerpo consume, que se relaciona con el metabolismo de la persona en determinadas condiciones fisiológicas (reposo o ejercicio). Es un valor complejo que varía con el sexo, la edad y la superficie corporal, además de variar en cuadros donde está afectada la actividad metabólica. (Por ejemplo hipertiroidismo, sepsis, etc). El valor normal en reposo es 3,5 mL/kg/min
El consumo de oxígeno es imprescindible para la vida humana.


</doc>
<doc id="8845" url="https://es.wikipedia.org/wiki?curid=8845" title="Volumen minuto respiratorio">
Volumen minuto respiratorio

En fisiología respiratoria, el volumen minuto respiratorio, volumen minuto, ventilación minuto o volumen expirado minuto es el volumen de gas inhalado (volumen minuto inhalado) o exhalado (volumen minuto exhalado) desde los pulmones de una persona por minuto.

Es un parámetro importante en medicina respiratoria debido a su relación con los niveles sanguíneos de dióxido de carbono. Puede ser medido con algunos aparatos tales como por ejemplo un respirómetro de Wright, o puede ser calculado de otros parámetros respiratorios conocidos. Nótese que aunque su nombre implica que se trata de un volumen; de hecho se trata de un caudal (representa un cambio de volumen a lo largo del tiempo).

Se utilizan varios símbolos para representar el volumen minuto. Entre estos se incluyen: formula_1, VM, y V.

El volumen minuto puede medirse directamente o calcularse a partir de otros parámetros conocidos.

El volumen minuto es la cantidad de gas inhalado o exhalado desde los pulmones de una persona por minuto. Puede ser medido por un espirómetro de Wright o por cualquier otro dispositivo capaz de realizar mediciones acumulativas de un flujo de gas, tales como los ventiladores mecánicos.

Si se conocen tanto el volumen tidal (V) y la frecuencia respiratoria (ƒ o F), el volumen minuto puede calcularse multiplicando estos dos valores. Se debe además tener en cuenta el efecto del espacio muerto en la ventilación alveolar, como se ve más abajo en relación con otros parámetros fisiológicos.

formula_2

Los niveles de dióxido de carbono sanguíneo (PaCO) por lo general varían inversamente con el volumen minuto. Por ejemplo, una persona con un volumen minuto aumentado (p. ej. debido a hiperventilación podría demostrar un nivel de dióxido de carbono en sangre más bajo. Un organismo normal puede alterar su volumen minuto respiratorio para mantener la homeostasis fisiológica.

Un volumen minuto en reposo para humanos es de aproximadamente 5–8 litros. El volumen minuto por lo general disminuye en situaciones de reposo, y aumenta con el ejercicio. Por ejemplo, durante las actividades diurnas el volumen minuto ronda aproximadamente los 12 litros. Conducir una bicicleta aaumenta la ventilación minuto por un factor de 2 o 4 dependiendo del nivel de ejercicio involucrado. La ventilación minuto durante el ejercicio moderado puede estar entre los 40 y 60 litros por minuto.

Hiperventilación es el término que define a una ventilación minuto mayor que la fisiológicamente apropiada. Hipoventilación por otra parte describe a un volumen minuto más bajo que lo fisiologicamente apropiado.

El volumen minuto comprende la suma de la ventilación alveolar y la ventilación de espacio muerto. Esto es:

formula_3

donde formula_4 es la ventilación alveolar, y formula_5 representa la ventilación de espacio muerto.




</doc>
<doc id="8848" url="https://es.wikipedia.org/wiki?curid=8848" title="Enfermedad autoinmune">
Enfermedad autoinmune

Una enfermedad autoinmune, también enfermedad autoinmunitaria, es una enfermedad causada por el sistema inmunitario, que ataca las células del propio organismo. En este caso, el sistema inmunitario se convierte en el agresor y ataca y destruye a los propios órganos y tejidos corporales sanos, en vez de protegerlos. Existe una respuesta inmunitaria exagerada contra sustancias y tejidos que normalmente están presentes en el cuerpo.

Se han identificado más de 80 enfermedades autoinmunes. Las más comunes son la enfermedad celíaca, la diabetes tipo 1, la artritis reumatoide, el lupus eritematoso sistémico y la esclerosis múltiple.


En general, se acepta que la interacción entre los factores ambientales y los genes de susceptibilidad específicos es la responsable de la aparición de las enfermedades autoinmunitarias. Menos del 10% de personas con una mayor susceptibilidad genética desarrollan la enfermedad, lo que sugiere un fuerte desencadenante ambiental, que afecta también al progreso y pronóstico de la enfermedad. La teoría actual es que los antígenos absorbidos por el intestino pueden estar involucrados. 

Poco tiempo después de que las enfermedades autoinmunitarias fueran identificadas por primera vez hace más de
un siglo, los investigadores empezaron a asociarlas con infecciones víricas y bacterianas. Esta asociación se explicaba mediante un mecanismo denominado "imitación molecular", basado en el estrecho parecido entre antígenos (o, más correctamente, epítopos) de los microorganismos y autoantígenos. Esta teoría postula que la inducción de una respuesta inmunitaria contra el antígeno microbiano provoca a continuación una reacción cruzada con autoantígenos y la aparición de procesos autoinmunes; una vez activados estos procesos, la respuesta autoinmune llega a ser independiente de la exposición continua al desencadenante ambiental y, en consecuencia, el proceso se autoperpetúa y se vuelve irreversible. 

Otra teoría deja entrever que los microorganismos exponen autoantígenos al sistema inmunitario por medio del daño directo a los tejidos durante la infección activa. Este mecanismo ha recibido el nombre de "efecto transeúnte". Sigue pendiente de aclaración el fenómeno por el cual los patógenos imitan a los autoantígenos, liberan autoantígenos secuestrados o ambos fenómenos.

Recientemente, se ha propuesto que el aumento de la higiene y una falta de exposición a diversos microorganismos son responsables de la epidemia de enfermedades autoinmunes que se está experimentando desde los años sesenta-setenta. La esencia de la "hipótesis de la higiene" sostiene que la incidencia creciente de enfermedades de origen inmunitario (incluyendo las autoinmunes) se debe, al menos en parte, al estilo de vida y a los cambios ambientales que nos han hecho "demasiado limpios". Independientemente de si las enfermedades autoinmunes se deben a una exposición demasiado intensa o demasiado escasa a los microorganismos, actualmente se considera en general que la inmunidad adaptativa y el desequilibrio entre las respuestas de Th1, Th2, Th17 y linfocitos T reguladores, son elementos clave en el desarrollo de enfermedades autoinmunes.

La alteración de la permeabilidad intestinal está implicada en el desarrollo de un creciente número de enfermedades, entre ellas las enfermedades autoinmunes, en las que el aumento de la permeabilidad intestinal permite el paso de antígenos desde el intestino a la sangre, produciendo una respuesta inmune que puede dirigirse contra cualquier órgano o tejido, en individuos predispuestos genéticamente.

En la mayoría de los casos, el aumento de la permeabilidad intestinal aparece antes que la enfermedad y provoca una anormalidad en la exposición al antígeno que desencadena el proceso multiorgánico causante del desarrollo de enfermedades autoinmunes.

Un denominador común de las enfermedades autoinmunes es la presencia de varios procesos preexistentes que provocan una respuesta autoinmune. El primero consiste en una susceptibilidad genética del sistema inmunitario a reconocer, e interpretar de un modo potencialmente erróneo, un antígeno ambiental presentado dentro del tubo digestivo. En segundo lugar, debe haber una exposición al antígeno. Finalmente, el antígeno debe ser presentado al sistema inmunitario, tras su paso a través de la barrera intestinal, que normalmente es bloqueado cuando ésta funciona correctamente. El epitelio intestinal es la superficie mucosa más grande del organismo e interactúa con el entorno. Cuando la mucosa intestinal está sana, con la permeabilidad intacta, constituye la principal barrera para evitar el paso de macromoléculas (nutrientes incompletamente digeridos y ciertas bacterias intestinales). Cuando la permeabilidad intestinal está dañada (aumentada), la barrera intestinal pierde su función protectora y pasan al torrente sanguíneo moléculas que no deberían pasar, provocando la aparición de reacciones inmunitarias.

Otro factor crítico para la capacidad de respuesta inmunológica intestinal es el complejo mayor de histocompatibilidad. Los genes HLA de clases I y II codifican para glicoproteínas que enlazan péptidos y este complejo HLA-péptido es reconocido por ciertos receptores de linfocitos T en la mucosa intestinal. La susceptibilidad a desarrollar al menos 50 enfermedades se ha asociado con alelos específicos HLA de clase I o II.

Los dos factores más potentes que provocan aumento de la permeabilidad intestinal son ciertas bacterias intestinales y la gliadina (fracción proteica del gluten), independientemente de la predisposición genética, es decir, tanto en celíacos como en no celíacos. Otras posibles causas son la prematuridad, la exposición a la radiación y la quimioterapia.

La siguiente hipótesis resume los tres puntos clave que explican la patogénesis de las enfermedades autoinmunes:


Las nuevas teorías sobre las causas que provocan el desarrollo de enfermedades autoinmunes implican que después de que ha sido activado el proceso autoinmune, éste no se perpetúa a sí mismo, sino que puede ser modulado, o incluso frenado, evitando la interacción continua entre los genes y el entorno a través de la eliminación del factor o los factores ambientales desencadenantes.



</doc>
<doc id="8850" url="https://es.wikipedia.org/wiki?curid=8850" title="Cáncer de pulmón de células pequeñas">
Cáncer de pulmón de células pequeñas

El cáncer del pulmón de células pequeñas (SCLC, por sus siglas en inglés "Small-cell lung cancer"), también llamado carcinoma microcítico de pulmón, es una de las formas de cáncer en el pulmón, normalmente clasificado como terminal. Los estudios han mostrado que habitualmente este tipo de cáncer de pulmón ya se ha propagado en el momento en que se detecta (aunque tal propagación no se pueda ver en los rayos X ni en otras pruebas de imagen), de manera que generalmente el SCLC no tiene cura.

El SCLC conforma cerca del 15% de todos los casos de cáncer de pulmón. El factor de riesgo más importante en la aparición del cáncer de pulmón de células pequeñas es el humo del cigarrillo, bien sea por el mismo fumador o por el humo de segunda mano. Es también importante la asociación con la exposición crónica a asbesto o a radón.

Los tumores de células pequeñas se comportan como masas hiliares de difícil delimitación radiológica, que en su crecimiento engloban las estructuras hiliares tanto bronquiales como vasculares, comprimiendo progresivamente su luz, incluso sin que endoscópicamente se vean lesiones infiltrativas claras. Más excepcionalmente se presentan como nódulos o masas parenquimatosas alejadas del hilio.

Los estudios con microscopía electrónica muestran gránulos densos de neurosecreción en algunas de las células tumorales. Los gránulos son similares a los que se encuentran en las células argentafines neuroendocrinas de Kulchistsky del epitelio bronquial, especialmente en el feto y en neonatos. La existencia de gránulos de neurosecreción, la capacidad de algunos de estos tumores de segregar hormonas polipeptídicas y la presencia de marcadores neuroendocrinos en el estudio inmunohistoquímico, como enolasa específica de las neuronas, sustancias tipo parathormona, y otros productos con actividad hormonal, sugieren que este tumor deriva de las células de la capa basal del revestimiento bronquial con programación neuroendocrina y posiblemente sea un "APUDOMA". El carcinoma anaplásico de "células en avena" es el tipo histológico que con mayor frecuencia se asocia a la producción ectópica de hormonas y a síndromes paraneoplásicos.

Como su nombre lo indica, el carcinoma microcítico está formado por células pequeñas, de forma redondeada u oval, de aspecto linfocitario, aunque con un tamaño dos veces al de un linfocito, con citoplasma escaso, núcleo redondo y cromatina granular densa (en sal y pimienta). Otros carcinomas de células pequeñas están formadas por células fusiformes o poligonales (carcinoma de células pequeñas, fusiformes o poligonales). Las células crecen en masas que no muestran diferenciación glandular ni escamosa.

El examen físico y los antecedentes clínicos y familiares del individuo pueden iniciar el curso a un diagnóstico acertado. Ciertas pruebas de radiología, como la radiografía del tórax, una TAC del cerebro, el pecho y el abdomen, así como otros exámenes incluyendo broncoscopia, biopsias y citologías corroboran el diagnóstico y el estadio del cáncer.

En la "etapa limitada", el tratamiento más comúnmente usado es una combinación de dos o más medicamentos de quimioterapia. Estos serían cisplatino o carboplatino combinado con etopósido, normalmente administrados durante aproximadamente seis meses. Actualmente se están realizando estudios para determinar si la adición de topotecán o paclitaxel mejorará la supervivencia.

La radioterapia torácica no se realiza en aquellos pacientes que padecen una enfermedad pulmonar grave (además del cáncer) o algunos otros tipos de problemas médicos graves. Algunas veces, si el SCLC está muy localizado, el cáncer se extirpa mediante cirugía y posteriormente se administra una quimioterapia adyuvante de combinación (poliquimioterpia).

En la "etapa avanzada o diseminada" la quimioterapia puede tratar la enfermedad y permitir vivir más tiempo y mejor al paciente. La probabilidad de que el cáncer se reduzca con quimioterapia es aproximadamente de un 70 a un 80%. Otra vez, carboplatino o cisplatino junto con etopósido son los medicamentos que se administran normalmente. Sin embargo, eventualmente el cáncer se vuelve resistente al tratamiento. Algunas veces se usa la radioterapia para controlar los síntomas del crecimiento dentro del pulmón o de la propagación a los huesos y al cerebro. Algunas veces se tratan con radioterapia preventiva en el cerebro.

La tasa de supervivencia de un año para las personas con SCLC en etapa limitada que reciben tratamiento con quimioterapia y radioterapia (éste es el grupo más favorable) es de un 60%. A los dos años la tasa disminuye a un 30%, y a los 5 años disminuye de un 15 a un 10%. Debido a la carencia de resultados satisfactorios, los médicos están estudiando otros métodos para tratar estos cánceres. Los estudios clínicos de nuevos medicamentos de quimioterapia o de otros tratamientos nuevos tales como inmunoterapia o terapia genética, son una opción que vale la pena y que puede beneficiar tanto al paciente individual como a futuros pacientes.

Se han realizado muchos estudios para determinar si un tratamiento de radioterapia en el tórax (normalmente en el centro, donde el cáncer se propaga a los ganglios linfáticos) mejorará las expectativas en comparación con la quimioterapia sola. Estos estudios han mostrado que la radioterapia proporciona un pequeño beneficio. Sin embargo, habrá más toxicidad con radioterapia junto con la quimioterapia. Es posible que se experimente más dificultad para respirar (disnea) debido a los daños causados al pulmón, y también dificultad para tragar (disfagia) debido a que el esófago se encuentra en el campo de radiación.

Aproximadamente un 20 a un 30% de las personas que padecen SCLC en etapa avanzada viven 1 año. A los dos años, sólo un 5% permanecen vivos. Y sólo de 1 a 2% de las personas con SCLC en etapa avanzada sobreviven cinco años después de la detección del cáncer. Si el paciente está demasiado enfermo para soportar la quimioterapia, el mejor plan es proporcionar cuidados paliativos. Esto incluiría el tratamiento de cualquier dolor, problemas respiratorios y otros síntomas que el paciente pueda experimentar. En los casos de cáncer del pulmón en etapa avanzada el principal problema puede ser el dolor. El crecimiento del cáncer alrededor de ciertos nervios puede causar un intenso dolor. Sin embargo, este dolor puede aliviarse con medicamentos. La radioterapia también puede ser de utilidad.

Es de crecimiento rápido y altamente invasivos. Característicamente invade mediastino y produce con mayor frecuencia el síndrome de la vena cava superior. En general la causa más frecuente de síndrome de vena cava superior es el oat cell y en segundo lugar serían los linfomas. El SCLC comúnmente se propaga al cerebro. Si no se toman medidas preventivas, aproximadamente un 50% de las personas con SCLC tendrán metástasis en el cerebro. Por esta razón, si hay una buena respuesta al tratamiento inicial, es posible que se administre radioterapia al cerebro para evitar una metástasis cerebral. Esto también puede aumentar ligeramente la probabilidad de una mayor supervivencia. Un problema que los médicos han informado es que los pacientes que reciben radioterapia en el cerebro pueden sufrir efectos secundarios tales como problemas de memoria y torpeza. No está totalmente aclarado si estos síntomas son el resultado directo de la radiación. La mayoría de los médicos recomendará la radioterapia en el cerebro si el paciente ha presentado una respuesta completa (desaparición de todo el cáncer aparentemente) después de la quimioterapia. Tal radiación profiláctica (preventiva) del cerebro ha dado como resultado ventajas en la supervivencia en general, de acuerdo con una reciente revisión de varios estudios recopilados.



</doc>
<doc id="8852" url="https://es.wikipedia.org/wiki?curid=8852" title="Linfocito">
Linfocito

El linfocito es un tipo de leucocito que proviene de la diferenciación linfoide de las células madre hematopoyéticas ubicadas en la médula ósea y que completa su desarrollo en los órganos linfoides primarios y secundarios (médula ósea, timo, bazo, ganglios linfáticos y tejidos linfoides asociados a las mucosas). Los linfocitos circulan por todo el organismo a través del aparato circulatorio y el sistema linfático.

Son los leucocitos de menor tamaño (entre 9 y 18 μm), y representan aproximadamente el 30 % (del 20 a 40 %) del total en la sangre periférica. Su morfología es variable, de acuerdo con la cual se clasifican en linfoblastos, prolinfocitos y linfocitos propiamente tal, ya sea inactivos o activados (como los plasmocitos). Presentan un gran núcleo esférico que se tiñe de violeta-azul y la cantidad de citoplasma varía entre escaso (situación más frecuente, en la cual el citoplasma se observa como un anillo periférico de color azul) a abundante. En el citoplasma se encuentran algunas mitocondrias, ribosomas libres y un pequeño aparato de Golgi.

La función principal de los linfocitos es la regulación de la respuesta inmunitaria adaptativa (o específica), reaccionando frente a materiales extraños (microorganismos, células tumorales o antígenos en general). Para ello se diferencian en tres líneas de células reactivas: los linfocitos T que se desarrollan en el timo y que participan en la respuesta inmunitaria celular; los linfocitos B, que se desarrollan en la médula ósea y luego migran a diferentes tejidos linfáticos, que son las encargadas de la respuesta inmunitaria humoral, transformándose en plasmocitos que producen anticuerpos; y las células NK (natural killer) que destruyen células infectadas. Los linfocitos T y B presentan receptores específicos, las asesinas naturales(NK) no.

Estas células se localizan fundamentalmente en la linfa y los órganos linfoides y en la sangre. Tienen receptores para antígenos específicos y, por tanto, pueden encargarse de la producción de anticuerpos y de la destrucción de células anormales. Estas respuestas ocurren en el interior de los órganos linfoides, los cuales, para tal propósito, deben suministrar un entorno que permita la interacción eficiente entre linfocitos, macrófagos y antígeno extraño.

La tipología de los linfocitos es difícil de catalogar según su morfología, por lo que se ocupan propiedades tales como la reactividad a anticuerpos monoclonales que identifican sus "CD" o cúmulos de diferenciación (del inglés "cluster of differentiation"), los cuales son un conjunto de marcadores biológicos para la identificación celular; el estado de reconfiguración de los genes de cadena pesada y cadena ligera de inmunoglobulina o receptores de linfocitos T y B; y la expresión de inmunoglobulinas en la superficie.

Los linfocitos B maduran en dos etapas: la primera, sin mediar antígenos, que ocurre en la médula ósea y otra dependiente de la exposición a antígenos que ocurre en los órganos linfáticos secundarios (ganglios linfáticos, bazo y tejidos linfoides asociados a las mucosas) donde se transforman en células productoras y secretoras de anticuerpos. Los distintos tipos de linfocitos B en estas etapas son:

Los linfocitos son los responsables de la respuesta humoral, transformándose en plasmocitos que producen glucoproteínas denominadas anticuerpos o inmunoglobulinas ("Ig"), las cuales se adhieren a un antígeno específico (al cual reconocen de manera unívoca). Todos los anticuerpos producidos por un linfocito B son específicos para un solo antígeno, por eso dichos anticuerpos se denominan monoclonales. También los linfocitos B interactúan con los linfocitos T con lo que proliferan y cambian el isotipo de inmunoglobulina (IgM, IgE, IgG, IgD, IgA ) que secretan, manteniendo la especificidad del antígeno. También los linfocitos B pueden actuar como células presentadoras de antígenos.

Linfocitos T (timodependientes, ya que se diferencian en el timo): Detectan antígenos proteicos asociados a moléculas del complejo mayor de histocompatibilidad (MHC o CMH)

Las células NK luchan contra el antígeno presente en la célula infectada o cancerígena de manera inmediata; mientras tanto la respuesta inmune adaptativa está generando células T citotóxicas específicas, para acabar con esas células en un segundo paso.

El tráfico de linfocitos entre los tejidos, el torrente sanguíneo y los ganglios linfáticos, permite que las células sensibles a los antígenos los busquen y sean reclutadas en sitios en los cuales se está desarrollando una respuesta. A su vez hay una diseminación de las células de memoria que permite la organización de una respuesta más amplia.

Las integrinas pueden unirse a la matriz extracelular, a las proteínas plasmáticas y a otras moléculas de la superficie celular, sus ligandos complementarios incluyen las adresinas vasculares de superficie, presentes en el endotelio de los vasos sanguíneos.

Estos receptores guía actúan como puertas selectivas que permiten que las poblaciones particulares de linfocitos tengan acceso al tejido apropiado.

Las quimiocinas como SLC (quimiocina del tejido linfoide secundario) presentadas por el endotelio vascular, tienen un papel importante para la detección de linfocitos; los receptores de la integrinas están involucrados en la regulación positiva funcional de las integrinas.




</doc>
<doc id="8854" url="https://es.wikipedia.org/wiki?curid=8854" title="Provincia de Zaragoza">
Provincia de Zaragoza

Zaragoza es una de las cincuenta provincias de España, con capital en la homónima Zaragoza. Ubicada en la comunidad autónoma de Aragón, limita al norte con Navarra y Huesca, al este con Lérida y Tarragona, al sur con Teruel, al suroeste con Guadalajara y al este con Soria y La Rioja, siendo junto a Burgos la provincia española que limita con más provincias (8).

La provincia de Zaragoza abarca una superficie de 17 274 km². En 2009 la población era de 970 313 habitantes y la densidad de población de 56,17 hab/km². La provincia de Zaragoza la conforman 293 municipios y tres entidades locales menores.

Las siguientes comarcas con capital en la provincia de Huesca comprenden algunos municipios de la provincia de Zaragoza:

Principales municipios por población
La provincia de Zaragoza es la segunda de España —de un total de cincuenta— en porcentaje de habitantes concentrados en su capital: 69,28 % frente al 31.96 % del conjunto de España.

Principales municipios por extensión



</doc>
<doc id="8855" url="https://es.wikipedia.org/wiki?curid=8855" title="Proyecto Genoma Humano">
Proyecto Genoma Humano

El Proyecto Genoma Humano (D.D.H.H) fue un proyecto de investigación científica con el objetivo fundamental de determinar la secuencia de pares de bases químicas que componen el ARN e identificar y cartografiar los aproximadamente 20.000-25.000 genes del genoma humano desde un punto de vista físico y funcional. 

En el año 2016, Julio, se completó la secuencia del genoma humano, incompleta antes, aunque no se conoce la función del todo.
El proyecto, dotado con 3000 millones de dólares, fue fundado en 1990 en el Departamento de Energía y Ciencias Trapianas y los Institutos Nacionales de la Salud de los Estados Unidos, bajo la dirección del doctor Francis Collins, quien lideraba el grupo de investigación público, conformado por múltiples científicos de diferentes países, con un plazo de realización de 15 años. Debido a la amplia colaboración internacional, a los avances en el campo de la genómica, así como los avances en la tecnología computacional, un borrador inicial del genoma fue terminado en el año 2000 (anunciado conjuntamente por el expresidente Bill Clinton y el ex primer ministro británico Tony Blair el 26 de junio de 2000), finalmente el genoma completo fue presentado en abril del 2003, dos años antes de lo esperado. Un proyecto paralelo se realizó fuera del gobierno por parte de la Corporación Celera. La mayoría de la secuenciación se realizó en las universidades y centros de investigación de los Estados Unidos, Canadá, Nueva Zelanda, Gran Bretaña y España.

El genoma humano es la secuencia de ADN de un ser humano. Está dividido en fragmentos que conforman los 23 pares de cromosomas distintos de la especie humana (22 pares de autosomas y 1 par de alosomas). El genoma humano está compuesto por aproximadamente entre 22500 y 25000 genes distintos. Cada uno de estos genes contiene codificada la información necesaria para la síntesis de una o varias proteínas (o ARN funcionales, en el caso de los genes ARN). El "genoma" de cualquier persona (a excepción de los gemelos idénticos y los organismos clonados) es único.

Conocer la secuencia completa del genoma humano puede tener mucha relevancia cuanto a los estudios de biomedicina y genética clínica, desarrollando el conocimiento de enfermedades poco estudiadas, nuevas medicinas y diagnósticos más fiables y rápidos. Sin embargo descubrir toda la secuencia génica de un organismo no nos permite conocer su fenotipo. Como consecuencia, la ciencia de la genómica no podría hacerse cargo en la actualidad de todos los problemas éticos y sociales que ya están empezando a ser debatidos. Por eso el PGH necesita una regulación legislativa basada en la ética.

Antes de los ochenta ya se conocía la secuencia de genes sueltos de algunos organismos, como también se conocían los genomas de entidades subcelulares, tales como virus y plásmidos. Así pues, no fue hasta 1986 cuando el Ministerio de Energía (DOE), concretó institucionalmente el Proyecto Genoma Humano (PGH) durante un congreso en Santa Fe. El PGH contaba con una buena suma económica y sería utilizado para estudiar los posibles efectos de las radiaciones sobre el ADN. Al siguiente año, en el congreso de biólogos en el Laboratorio de Cold Spring Harbor, el Instituto Nacional de la Salud (NIH) quiso participar del proyecto al ser otro organismo público con mucha más experiencia biológica, si bien no tanta en la organización de proyectos de esta magnitud.

El debate público que suscitó la idea captó la atención de los responsables políticos, no solo porque el Proyecto Genoma Humano era un gran reto tecnocientífico, sino por las tecnologías de vanguardia que surgirían, así como porque el conocimiento obtenido aseguraría la superioridad tecnológica y comercial del país. Antes de dar luz verde a la iniciativa del PGH se necesitó por un lado el informe de 1988 de la Oficina de Evaluación Tecnológica del Congreso (OTA) y el del Consejo Nacional de Investigación (NRC). Ese año se inauguró HUGO (Organización del Genoma Humano) y James D. Watson fue nombrado alto cargo del proyecto. Sería reemplazado por Francis Collins en abril de 1993, en gran parte por su enemistad con Bernadine Healy que era su jefe por aquel entonces. Tras esto el nombre del Centro cambió a Instituto Nacional de Investigaciones del Genoma Humano (NHGRI).

En 1990 se inauguró definitivamente el Proyecto Genoma Humano calculándose quince años de trabajo. Sus objetivos principales en una primera etapa eran la elaboración de mapas genéticos y físicos de gran resolución, mientras se ponían a punto nuevas técnicas de secuenciación, para poder abordar todo el genoma. Se calculó que el Proyecto Genoma Humano estadounidense necesitaría unos 3000 millones de dólares y terminaría en 2005. En 1993 los fondos públicos aportaron 170 millones de dólares, mientras que la industria gastó aproximadamente 80 millones. Con el paso de los años, la inversión privada cobró relevancia y amenazó con adelantar a las financiaciones públicas. 

El proyecto genoma humano tiene una extensión que es el proyecto microbioma humano . El mismo intenta caracterizar las comunidades microbianas encontradas en diversas localizaciones del cuerpo humano para determinar las posibles correlaciones entre los cambios del microbioma y el estado de salud.

Se consideraría al microbioma como la rama más alta de al último órgano humano por investigar.

En 1984 comenzaron las actividades propias del PGH, coincidiendo con la idea de fundar un instituto para la secuenciación del genoma humano por parte de Robert Sanshheimerm, en ese momento Rector de la Universidad de California. De forma independiente el Departamento de Energía de Estados Unidos (DOE) se interesó por el proyecto, al haber estudiado los efectos que las actividades de sus programas nucleares producían en la genética y en las mutaciones. Entonces se conocía como "Proyecto HUGO".
En su comienzo, el Proyecto Genoma Humano, enfrentó a dos clases de científicos: de un lado, los biólogos moleculares universitarios y del otro, biólogos de institutos de investigación del Instituto Nacional de Salud, organismo estatal que percibía grandes sumas económicas federales destinadas a la investigación. Si bien el enfrentamiento se basó en la preocupación de ambos científicos por la magnitud y los costes de la empresa a llevar a cabo, existían sobre todo discrepancias para definir las vías más adecuadas a la hora de lograr los objetivos fijados. Solo debemos observar los 28.2 millones de dólares destinados al periodo 88-89 para ubicarnos “materialmente”. Por su parte, los Estados Unidos se comprometieron a destinar parte de los fondos económicos del proyecto al estudio de los aspectos éticos y sociales del PGH.

James Watson asumió en 1988 la dirección ejecutiva de la Investigación del Genoma Humano en el NIH (Instituto Nacional de Salud). Al asumir el cargo, firmó un acuerdo de cooperación con el DOE mediante el cual ambas instituciones se ayudarían mutuamente. De esta forma el PGH comenzó con el liderazgo del NIH en lugar del DOE. El interés internacional por el proyecto creció de forma notable, motivado fundamentalmente por no quedar por detrás de Estados Unidos en un tema de tanta importancia. Para evitar repeticiones y solapamientos en los logros, se creó HUGO (Organización del Genoma Humano) para coordinar los trabajos de investigación. 

En 1994 Craig Venter funda, con un financiamiento mixto, el Instituto para la Investigación Genética (TIGR) que se dio a conocer públicamente en 1995 con el descubrimiento de la secuencia nucleotídica del primer organismo completo publicado, la bacteria "Haemophilus influenzae" con cerca de 1740 genes (1.8 Mb). En mayo de 1998 surgió la primera empresa relacionada con el PGH llamada Celera Genomics. La investigación del proyecto se convirtió en una carrera frenética en todos los laboratorios relacionados con el tema, ya que se intentaba secuenciar trozos de cromosomas para rápidamente incorporar sus secuencias a las bases de datos y atribuirse la prioridad de patentarlas. 
El 6 de abril de 2000 se anunció públicamente la terminación del primer borrador del genoma humano secuenciado que localizaba a los genes dentro de los cromosomas. Los días 15 y 16 de febrero de 2001, las dos prestigiosas publicaciones científicas estadounidenses, Nature y Science, publicaron la secuenciación definitiva del Genoma Humano, con un 99.9% de fiabilidad y con un año de antelación a la fecha presupuesta. Sucesivas secuenciaciones condujeron finalmente al anuncio del genoma esencialmente completo en abril de 2003, dos años antes de lo previsto. En mayo de 2006 se alcanzó otro hito en la culminación del proyecto al publicarse la secuencia del último cromosoma humano en la revista Nature.

Una extensión del proyecto genoma humano es el del microbioma humano, que intenta caracterizar las comunidades microbianas encontradas en diversas localizaciones del cuerpo humano para determinar las posibles correlaciones entre los cambios de dicho microbioma y el estado de salud. Algunos autores consideran al microbioma humano el último órgano por investigar. 

Desde el principio de la investigación, se propuso desarrollar el PGH a través de dos vías independientes, pero relacionadas y ambas esenciales: 

El genoma humano está compuesto por aproximadamente 30 000 genes, cifra bastante próxima a la mencionada en el borrador del proyecto, publicado en el año 2000, ocasión en la que los genes oscilaban entre 26 000 y 38 000. Otra peculiaridad del genoma humano es que la cifra de genes es solo dos o tres veces mayor que la encontrada en el genoma de "Drosophila", y cualitativamente hablando, existen genes comunes a los de bacterias y que no han sido hallados en nuestros ancestros.

Los humanos poseen poco más de 3 mil millones de bases nitrogenadas, similar al tamaño de genomas de otros vertebrados.

En estos momentos son una realidad las bases de datos donde se almacena toda la información surgida del Proyecto Genoma Humano. Si accedemos a Internet podremos conocer libremente aspectos de alto interés en la comparación entre genomas de distintas especies de animales y plantas. Gracias al uso libre de este conocimiento es posible determinar la función de los genes, así como averiguar cómo las mutaciones influyen en la síntesis de proteínas.

Se ha inducido un gran desarrollo tecnológico a partir de la creación de herramientas de análisis de datos generadas en el Proyecto Genoma Humano. Este desarrollo facilitará y hará posible definir los temas de estudio futuros con vistas a las tareas pendientes. Entre las tecnologías beneficiadas gracias al PGH figuran las de manejo computacional de datos, las que permiten la generación de las anteriores, técnicas de biología molecular relacionadas con la secuenciación de trozos de ADN automáticamente y aquellas que permiten ampliar la cantidad de material genético disponible como la RCP pero no es posible realizar esta acción porque está fuera de las leyes universales propuestas por la ONU en cualquier parte del globo terrestre
Se ha producido una importante corriente de liberación de derechos que anteriormente estaban en manos del Estado, en relación a la transferencia de tecnologías al sector privado. Esta medida ha suscitado aplausos y críticas. Por un lado se amplía el acceso libre a los datos del Proyecto con lo que muchas más personas pueden seguir estudiando este campo, pero por otro esto puede suponer el incremento de poder de ciertos sectores que a su vez, aumentaran su influencia en la sociedad.

Para terminar, se puede afirmar que el objetivo relacionado con el estudio de la ética del PGH es un tema de gran controversia actual, y ha necesitado de grandes sumas de dinero estatales así como de un importante trabajo de laboratorios e investigadores. Todo esto ha provocado un deterioro del apoyo a otros proyectos de investigación no menos importantes, que se han visto muy afectados o incluso cancelados.

Existen dos técnicas de cartografía genética principales: el ligamiento, que intenta averiguar el orden de los genes; y la cartografía física, que se encarga de estudiar la distancia de los genes en el interior del cromosoma. Las dos técnicas utilizan marcadores genéticos, que son características moleculares o físicas que se heredan, y son detectables y distintas para cada individuo. 

Thomas Hunt Morgan desarrolló en la década de 1900 la cartografía mediante ligamiento al estudiar la frecuencia con la que ciertas características se heredaban unidas en moscas de la fruta. Así llegó a la conclusión de que algunos genes debían estar ligados en los cromosomas. Los mapas de ligamiento humano se han creado estudiando pautas de herencia de familias muy extensas y con varias generaciones conocidas. Aunque al principio se limitaban a los rasgos físicos heredables, fácilmente reconocibles, actualmente hay técnicas más elaboradas que permiten crear mapas de ligamiento comparando la posición de genes diana en comparación con el orden de los marcadores genéticos o de partes conocidas del ADN.

La cartografía física es capaz de medir la distancia real entre puntos de los cromosomas. Las técnicas más avanzadas combinan robótica, informática y uso de láser para calcular la distancia entre marcadores genéticos conocidos. Para conseguirlo, se fragmenta el ADN de los cromosomas humanos aleatoriamente. A continuación se duplican muchas veces para estudiar en los clones, que son las secuencias duplicadas, la ausencia o presencia de marcas genéticas identificables. Los clones que comparten varias marcas provienen de segmentos solapados normalmente. Estas regiones pueden utilizarse después para determinar el orden de las marcas en los cromosomas y su secuencia. Para obtener la secuencia real de nucleótidos hacen falta mapas físicos altamente detallados que recogen el orden de las piezas clonadas con exactitud. 

En el Proyecto Genoma Humano se utilizó un método de secuenciación desarrollado por Frederick Sanger, bioquímico británico y dos veces premio Nobel. Este método replica piezas específicas de ADN y las modifica de modo que acaben en una forma fluorescente. 

Actualmente se detecta el nucleótido modificado del extremo de las cadenas con modernos secuenciadores de ADN automáticos. Estos determinan los nucleótidos que hay exactamente en la cadena. A continuación se combina esta información de manera informatizada, y así se reconstruye la secuencia de pares de bases del ADN original.

Un aspecto muy importante es duplicar rápidamente y con exactitud el ADN, tanto para después cartografiarlo como para secuenciarlo. Al comienzo de la investigación en este campo se clonaba el material genético introduciéndolo en organismos unicelulares de rápida división, pero en la década de los ochenta se generalizó el uso de la PCR (reacción en cadena de polimerasa). Esta técnica se puede automatizar fácilmente y es capaz de copiar una sola molécula de ADN muchos millones de veces en poco tiempo. Kary Mullis obtuvo el Premio Nobel de Química por idearla, en 1993.

El PGH e IHGSC internacional (sector público) recogieron el semen de hombres y la sangre de mujeres de muchos donantes diferentes, pero solo unas pocas de estas muestras fueron estudiadas después realmente. Así se garantizó que la identidad de los donantes estuviera salvaguardada de modo que nadie supiera qué ADN sería el secuenciado. También han sido utilizados clones de ADN de varias bibliotecas, la mayoría de las cuales fueron creadas por el Dr. J. Pieter de Jong. Se comunicó de manera informal, pero es bien conocido por la comunidad en general, que gran parte del ADN secuenciado provenía de un único donante anónimo de Buffalo, Nueva York, su nombre en clave era RP11. Los científicos encargados utilizaron principalmente los glóbulos blancos de dos hombres y dos mujeres elegidos al azar.

El trabajo sobre la interpretación de los datos del genoma se encuentra todavía en sus etapas iniciales. Se prevé que un conocimiento detallado del genoma humano ofrecerá nuevas vías para los avances de la medicina y la biotecnología. Por ejemplo, un número de empresas, como Myriad Genetics ha empezado a ofrecer formas sencillas de administrar las pruebas genéticas que pueden mostrar la predisposición a una variedad de enfermedades, incluyendo cáncer de mama, los trastornos de la hemostasia, la fibrosis quística, enfermedades hepáticas y muchas otras. Además, la etiología de los cánceres, la enfermedad de Alzheimer y otras áreas de interés clínico se consideran susceptibles de beneficiarse de la información sobre el genoma y, posiblemente, pueda a largo plazo conducir a avances significativos en su gestión. 

Hay también muchos beneficios tangibles para los biólogos. Por ejemplo, un investigador de la investigación de un determinado tipo de cáncer puede haber reducido su búsqueda a un determinado gen. Al visitar la base de datos del genoma humano en la World Wide Web, este investigador puede examinar lo que otros científicos han escrito sobre este gen, incluyendo (potencialmente) la estructura tridimensional de su producto; su/s función/es; sus relaciones evolutivas con otros genes humanos, o genes de ratones, levaduras, moscas de la fruta; las posibles mutaciones perjudiciales; las interacciones con otros genes; los tejidos del cuerpo en el que este gen es activado; las enfermedades asociadas con este gen u otro tipo de datos. Además, la comprensión más profunda de los procesos de la enfermedad en el ámbito de la biología molecular puede determinar nuevos procedimientos terapéuticos. Dada la importancia del ADN en biología molecular y su papel central en la determinación de la operación fundamental de los procesos celulares, es probable que la ampliación de los conocimientos en este ámbito facilite los avances médicos en numerosas áreas de interés clínico que puede no haber sido posible por otros métodos.

El análisis de las similitudes entre las secuencias de ADN de diferentes organismos es también la apertura de nuevas vías en el estudio de la evolución. En muchos casos, las cuestiones de evolución ahora se pueden enmarcar en términos de biología molecular y, de hecho, muchos de los grandes hitos evolutivos (la aparición de los ribosomas y orgánulos, el desarrollo de planes de embriones con el cuerpo, el sistema inmune de vertebrados) pueden estar relacionados a nivel molecular. Muchas de las preguntas acerca de las similitudes y diferencias entre los seres humanos y nuestros parientes más cercanos (los primates, y de hecho los otros mamíferos) se espera que sean iluminados por los datos de este proyecto. 

El Proyecto Diversidad del Genoma Humano (PDGH), derivado de investigaciones dirigidas a la asignación del ADN humano - que varía entre los grupos étnicos - que se rumorea que ha sido detenido, realmente continúa y hasta la fecha ha arrojado nuevas conclusiones. En el futuro, el PGH podría exponer nuevos datos en la vigilancia de las enfermedades, el desarrollo humano y la antropología. El PGH podría desbloquear secretos y crear nuevas estrategias para combatir la vulnerabilidad de los grupos étnicos a ciertas enfermedades. También podría mostrar cómo las poblaciones humanas se han adaptado a estas vulnerabilidades.

Además, el PGH tiene una consecuencia muy importante, y es que se pueden conocer la base molecular de ciertas enfermedades hereditarias y que se puede realizar un diagnóstico de las mismas:

Una de las aplicaciones más directas de conocer la secuencia de genes que componen el genoma humano es que se puede conocer la base molecular de muchas enfermedades genéticas y se puede realizar un diagnóstico adecuado. Algunas de estas enfermedades son las siguientes:

Estos son algunos ejemplos de enfermedades que se han podido diagnosticar gracias, de una u otra manera, al conocimiento de las secuencias genéticas tras la secuenciación del genoma por el Proyecto Genoma Humano. El diagnóstico de cierta enfermedad, gracias al PGH se puede realizar de manera presintomática y prenatal.

El conocimiento de la base molecular de las enfermedades permite realizar el diagnóstico presintomático y gracias a él tomar medidas preventivas, como alteraciones en el estilo de vida, evitar la exposición a factores de riesgo, realizar un seguimiento continuo del individuo o realizar intervenciones puntuales, para poder tratar la enfermedad aunque todavía no haya aparecido.

En cuanto al diagnóstico prenatal, éste consiste en un conjunto de técnicas que sirven para conocer la adecuada formación y el correcto desarrollo del feto antes de su nacimiento, para poder conocer posibles malformaciones desde los primeros estadios de desarrollo del embrión. La técnica más común de diagnóstico prenatal es la amniocentesis, que consiste en el análisis del líquido amniótico que rodea al feto durante el embarazo. Las células desprendidas del feto y que flotan en dicho líquido sirven para obtener un recuento exacto de cromosomas y para detectar cualquier estructura cromosómica anormal. El diagnóstico prenatal conlleva una importante polémica. Las mujeres cuyo hijo se observe que presentan características de padecer cierta enfermedad o que presentan malformaciones en sus cromosomas, decidirán abortar, lo que para los detractores del aborto es una aberración. La polémica está también alimentada por el hecho de que se pueden conocer tanto enfermedades que se desarrollen desde el primer día de vida del individuo como enfermedades que pueden aparecer a su edad avanzada, como el Alzheimer, por ejemplo. En ese caso, ¿abortaríamos a un feto que puede presentar la Enfermedad de Alzheimer casi al final de su vida, privándole de una vida previa normal? Esto conlleva también a realizar un baremo de qué enfermedades podrían considerarse suficientes para realizar el aborto, poniéndose por ejemplo, el daltonismo. 

Por otra parte, y como consecuencia del desarrollo de las técnicas de la fecundación in vitro, hoy en día se puede realizar el conocido como diagnóstico genético preimplantacional (DGPI). Éste permite testar los embriones desde un punto de vista genético y cromosómico para así elegir el que se encuentre sano e implantarlo en el útero de la madre. El DGPI evita la gestación de un niño afectado genética o cromosómicamente, y conlleva la decisión de los padres de realizar, en su caso, un aborto terapéutico.

Una vez que se conocen qué genes producen qué enfermedades, y las características para diagnosticar una enfermedad conociendo la secuencia de bases, es necesario realizar una terapia para acabar con esa enfermedad, ya que de ser de otra manera, el diagnóstico de una enfermedad no es más que una carga emocional que el paciente tiene que soportar de la mejor manera posible, conviviendo con la impotencia y la ansiedad que le puede suponer a un paciente el saber que en un determinado lapso de tiempo es posible que padezca una enfermedad. Una consecuencia, por tanto, del PGH es desarrollar terapias contra las enfermedades que ha diagnosticado. Se conocen la terapia génica, la terapia farmacológica y la medicina predictiva:

Aunque la medicina proporciona la base para la evolución de la bioética, actualmente somos testigos de su aplicación a la investigación científica relacionada. Así pues, el PGH ha dado lugar a una de las áreas de conocimiento biológico con mayor crecimiento. Los conocimientos genómicos derivados del Proyecto Genoma Humano, se utilizan para mejores y más rápidos diagnósticos basados en el análisis directo del ADN, e incluso para el diagnóstico prenatal en aquellos casos en los que se sospecha que el bebé tenga alteraciones morfológicas, funcionales o ponga en peligro la vida de su madre. También es posible aplicar este conocimiento a personas asintomáticas para averiguar si han heredado de algún progenitor una mutación causal de una enfermedad genética que pueda desarrollarse en el futuro. 

Así planteado el tema, se percibe entonces una importante brecha entre la capacidad diagnóstica y predictiva del conocimiento genómico por un lado, y la falta de intervenciones preventivas y terapéuticas por otro, lo que lleva a conflictos éticos surgidos del Proyecto Genoma Humano. Además hay determinadas áreas como el asesoramiento a parejas en riesgo de transmitir enfermedades genéticas a su descendencia, que han suscitado mucho interés y para las que se han dictado una serie de principios éticos:

Otro problema de gran importancia es la obtención de patentes de genes por parte de compañías biotecnológicas, gobiernos y centros de investigación universitarios, para una posterior venta o explotación comercial, sin tener en cuenta que parte de los fondos empleados en el PGH era de los contribuyentes. También debemos observar el PGH contextualizado social e históricamente, atendiendo a la desigualdad social y económica entre países, que va a producir una inequidad en el acceso a los beneficios que se extraigan de la investigación.

Una solución a todas estas tensiones podría ser la formación de profesores de ciencias o la enseñanza directa a estudiantes como una forma de abrir las mentes y aclarar definitivamente el alcance del Proyecto Genoma Humano en la sociedad. Pero es imprescindible incorporar temas de bioética a los programas de enseñanza.

Tanto en Estados Unidos como en la Unión Europea se han desarrollado programas para contemplar las consecuencias éticas y sociales de la investigación científica y que no se produzcan conflictos. En Estados Unidos se encuentra el ELSI y fuera de ellos se encuentra la Declaración Universal sobre el Genoma Humano y los Derechos Humanos, promovida por la UNESCO

El ELSI es el Programa Ético, Legal y Social (Ethical, Legal and Social Implications Research Program, en inglés) que desarrolló el NHGRI (National Humane Genome Research Institute, en inglés, o Instituto Nacional de Investigación del Genoma Humano, de Estados Unidos) en 1990. Este programa permite un acercamiento a la investigación científica teniendo en cuenta las implicaciones éticas, legales y sociales que ésta supone, al mismo tiempo que se está investigando para, de esta manera, poder identificar los posibles futuros problemas y solucionarlos antes de que la información científica se extienda.
El programa de investigación ELSI tiene un papel muy importante en todo lo relacionado con el PGH, y se encarga de analizar las implicaciones éticas y sociales de la investigación genética de la siguiente manera:

Para alcanzar estas metas, las actividades y la investigación del programa de ELSI se centran en cuatro áreas del programa:

El ELSI también ha iniciado una serie de emprendimientos educacionales que están dirigidos a entrenar a profesionales de la salud para que puedan interpretar los nuevos tests diagnósticos basados en el ADN que comenzarán a surgir más y más frecuentemente gracias a la información obtenida del PGH. Además de esta formación de profesionales de la salud también se necesita que los políticos y el público en general tengan un criterio suficiente sobre algunos asuntos críticos relacionados con las pruebas genéticas. Por ello, es necesario extender la información genética en las escuelas, los medios de comunicación, alentar la discusión pública sobre el tema y suministrar también información a los políticos. Una de las iniciativas es el establecimiento de la Coalición Nacional para la Educación de los Profesionales de la Salud en Genética (NCHPEG), también en EE. UU., pero rápidamente se queda insuficiente ya que sólo abarca a los profesionales de la Salud.

Así como Estados Unidos tiene un programa para regular las implicaciones sociales y éticas que tienen las investigaciones científicas para tratar de regularlas y que no haya conflictos, la UNESCO redactó en 1997 la “"Declaración Universal sobre el Genoma Humano y los Derechos Humanos"”, cuyo prefacio es el siguiente:
Federico Mayor, 3 de diciembre de 1997."

Está compuesta por 25 artículos que se dividen en las siguientes áreas, destacando en cada una de ellas un determinado artículo:

Entramos ahora en los que posiblemente sean los dos puntos más importantes de la controversia causada por el PGH, que se pasan a explicar a continuación:

El ELSI tiene un papel muy importante en el campo de la discriminación genética. Cuando se dieron los primeros pasos del PGH, los científicos tuvieron muy claro desde el principio que era necesario realizar un estudio ético y social, inicialmente a pequeña escala y si era necesario, a mayor; sobre alguna enfermedad que pudiera tener lugar en la sociedad, para evitar cualquier tipo de discriminación genética. Un ejemplo interesante de discriminación genética tuvo lugar en Estados Unidos durante los años setenta y relacionada con una campaña que realizó el gobierno para detectar portadores del gen de la anemia de células falciformes.

La anemia de células falciformes, además, tiene un componente relacionado con la raza muy importante, ya que es la enfermedad genética más frecuente entre la población negra. Se trata de una enfermedad recesiva bastante cruel ya que los que la sufren no pueden realizar esfuerzos, ya que corren un grave riesgo de sufrir una insuficiencia respiratoria aguda que les ocasione repentinamente la muerte. Pues bien, la discriminación genética aparece cuando el gobierno realizó un estudio poblacional para detectar individuos que portaran este gen. La anemia de células falciformes no tiene cura y por tanto, si alguien era diagnosticado de anemia de células falciformes no poseía la más mínima esperanza de curación. El problema se hizo patente cuando el gobierno declaró obligatorio en varios estados realizar la prueba de detección a los recién nacidos y a los escolares, sin seguir un programa paralelo de orientación genética que pudiera ofrecer consejo a las familias afectadas, y cuando el público comenzó a confundir a las personas portadoras (heterocigóticas) con las enfermas, debido a la completa falta de una campaña informativa. Por si esto fuera poco, Linus Pauling, que había descubierto el método de análisis de la hemoglobina, realizó unas desafortunadas declaraciones en las que sugería que se marcara de alguna manera a los portadores para que no se mezclaran y no tuvieran hijos entre sí. La información que se recogió en este estudio pasó a formar parte del historial médico de los niños que estaban afectados. Las compañías de seguros comenzaron entonces a negarse a formalizar el seguro si conocían que su posible cliente padecía anemia de células falciformes, e incluso si era simplemente portador del gen. También el mercado de trabajo comenzó a discriminar a los enfermos y portadores. A las personas de color que portaban el gen se les negaba por ejemplo el trabajo en compañías aéreas porque se pensaba que su sangre reaccionaría mal al encontrarse a bajas presiones causados por la altura del avión (algo que es erróneo).

Un gran problema que tuvo el caso de la anemia de células falciformes en los años setenta fue que no se conocían métodos de estudio del feto y que tampoco estaba permitido el aborto. Esto se ha podido superar actualmente y es un problema menor para el programa ELSI, ya que ahora sí existe la posibilidad de detectar la enfermedad en el feto y, además de que ya está permitido, el aborto terapéutico tiene una aceptación social casi mayoritaria.

En definitiva, es necesario realizar un estudio social y ético y dar la información necesaria a la opinión pública para que no se produzcan casos de discriminación genética, si ya no tan llamativos como el de la anemia de células falciformes en EE. UU., pero sí a menor escala como puede ser la predisposición hacia enfermedades cardíacas o a las discapacidades mentales, por ejemplo.

El concepto de «patente de genes» aparece también con la secuenciación del genoma producida por el PGH. Y es que resulta necesario compatibilizar las expectativas terapéuticas y de avance científico con las expectativas de aspecto económico, procurando encontrar un equilibrio razonable entre el altruismo que unos buscan en el conocimiento público de la información proporcionada por el PGH y otros que encuentran esta información suficiente para sacarle provecho económico. Es necesario combinar la moralidad con el interés económico. El elemento fundamental de todo esto se encuentra en las empresas privadas que realizan investigaciones en el genoma humano. Como tales empresas privadas, necesitan obtener un beneficio que supla las grandes inversiones que hacen en investigaciones para obtener posteriormente productos farmacéuticos, desarrollar terapias clínicas u otras aplicaciones. Para esto, necesitan proteger sus hallazgos para que nadie se aproveche de su esfuerzo. La cuestión reside en determinar cuál es el marco jurídico apropiado para garantizar debidamente esas expectativas de beneficio. Es, por tanto, lógico que se tratara de amparar bajo la protección de las patentes a los descubrimientos relacionados con la descodificación y aislamiento del ADN, considerándolo una sustancia o estructura que, como otras, se encuentra en la naturaleza y de cuyo conocimiento se puede derivar algún uso diagnóstico y con el fin de compensar las inversiones económicas realizadas. De este modo, los investigadores o instituciones que patentaran la secuencia parcial o total de cierto gen podrían ser acreedores de los derechos que se derivaran de ella para la obtención de fármacos. Por otro lado, hay gente que piensa que las patentes no hacen más que impedir el desarrollo biotecnológico y que la información que se encuentra en los genes debería ser de acceso público.

Las patentes sobre secuencias totales o parciales de genes continúan estando en una importante controversia y se pueden encontrar tres posiciones diferentes:

El Proyecto Genoma Humano permite obtener información de la estructura genética de un individuo, pero en principio sólo se queda ahí. Esa información estructural permite conocer la base molecular de muchas enfermedades y, sobre esa base, realizar el mejor diagnóstico posible. Pero, desde un punto de vista biológico, el PGH es la antesala de un proyecto mucho más interesante y dinámico, y es el proyecto proteoma humano. Gracias a la proteómica se puede conocer cómo la secuencia genética se transforma en una proteína que va a desarrollar cierta función.





</doc>
<doc id="8856" url="https://es.wikipedia.org/wiki?curid=8856" title="Bioinformática">
Bioinformática

La bioinformática, según una de sus definiciones más sencillas, es la aplicación de tecnologías computacionales y la estadística a la gestión y análisis de datos biológicos. Los términos bioinformática, biología computacional y, en ocasiones, biocomputación, son utilizados en muchas situaciones como sinónimos, y hacen referencia a campos de estudios interdisciplinares muy vinculados que requieren el uso o el desarrollo de diferentes técnicas estudiadas universitariamente en la Ingeniería Informática como ciencia aplicada de la disciplina informática. Entre estas pueden destacarse las siguientes: matemática aplicada, estadística, ciencias de la computación, inteligencia artificial, química y bioquímica con las que el Ingeniero Informático soluciona problemas al analizar datos, o simular sistemas o mecanismos, todos ellos de índole biológica, y usualmente (pero no de forma exclusiva) en el nivel molecular. El núcleo principal de estas técnicas se encuentra en la utilización de recursos computacionales para solucionar o investigar problemas sobre escalas de tal magnitud que sobrepasan el discernimiento humano. La investigación en biología computacional se solapa a menudo con la biología de sistemas.

Los principales esfuerzos de investigación en estos campos incluyen el alineamiento de secuencias, la predicción de genes, montaje del genoma, alineamiento estructural de proteínas, predicción de estructura de proteínas, predicción de la expresión génica, interacciones proteína-proteína, y modelado de la evolución.

Una constante en proyectos de bioinformática y biología computacional es el uso de herramientas matemáticas para extraer información útil de datos producidos por técnicas biológicas de alta productividad, como la secuenciación del genoma. En particular, el montaje o ensamblado de secuencias genómicas de alta calidad desde fragmentos obtenidos tras la secuenciación del ADN a gran escala es un área de alto interés. Otros objetivos incluyen el estudio de la regulación genética para interpretar perfiles de expresión génica utilizando datos de chips de ADN o espectrometría de masas.

Como se ha avanzado en la introducción, los términos bioinformática, biología computacional y biocomputación son utilizados a menudo como sinónimos, apareciendo con frecuencia en la literatura básica de forma indiferenciada en sus usos comunes. Sin embargo, hay conformadas áreas de aplicación propias de cada término. El NIH ("National Institutes of Health", Institutos Nacionales de la Salud de los Estados Unidos), por ejemplo, aún reconociendo previamente que ninguna definición podría eliminar completamente el solapamiento entre actividades de las diferentes técnicas, define explícitamente los términos bioinformática y biología computacional:


De esta forma, la bioinformática tendría más que ver con la información, mientras que la biología computacional lo haría con las hipótesis. Por otra parte, el término biocomputación suele enmarcarse en las actuales investigaciones con biocomputadores y, por ejemplo, T. Kaminuma lo define de la siguiente forma:


Aparte de las definiciones "formales" de organismos o instituciones de referencia, los manuales de esta materia aportan sus propias definiciones operativas, lógicamente vinculadas en mayor o menor medida con las ya vistas. Como ejemplo, David W. Mount, en su difundido texto sobre bioinformática, precisa que:
Por otra parte, y según el mismo autor:
Por último, se encuentra en ocasiones una categorización explícita de estos conceptos según la cual la bioinformática es una subcategoría de la biología computacional. Por ejemplo, la bióloga Cynthia Gibas anota que:
No obstante, y refiriéndose a su propio texto ("Developing Bioinformatics Computer Skills", desarrollo de habilidades computacionales para bioinformática), enseguida pasa a aclarar que:
En muchas ocasiones, por lo tanto, los términos serán intercambiables y, salvo en contextos de cierta especialización, el significado último se mantendrá claro utilizando cualquiera de ellos.

En lo que sigue, y además de los hechos relevantes directamente relacionados con el desarrollo de la bioinformática, se mencionarán algunos hitos científicos y tecnológicos que servirán para poner en un contexto adecuado tal desarrollo.

Arrancaremos esta breve historia en la década de los 50 del pasado siglo XX, años en los que Watson y Crick proponen la estructura de doble hélice del ADN (1953), se secuencia la primera proteína (insulina bovina) por F. Sanger (1955), o se construye el primer circuito integrado por Jack Kilby en los laboratorios de Texas Instruments (1958).

En los años 60, L. Pauling elabora su teoría sobre evolución molecular (1962), y Margaret Dayhoff, una de las pioneras de la bioinformática, publica el primero de los "Atlas of Protein Sequences" (1965), que tendrá continuidad en años posteriores, se convertirá en una obra básica en el desarrollo estadístico, algunos años más tarde, de las matrices de sustitución PAM, y será precursor de las actuales bases de datos de proteínas. En el área de la tecnología de computadores, se presentan en el ARPA ("Advanced Research Projects Agency", agencia de proyectos de investigación avanzados) los protocolos de conmutación de paquetes de datos sobre redes de ordenadores (1968), que permitirán enlazar poco después varios ordenadores de diferentes universidades en EE.UU.: había nacido ARPANET (1969), embrión de lo que posteriormente será Internet.

En 1970 se publica el algoritmo Needleman-Wunsch para alineamiento de secuencias; se establece el Brookhaven Protein Data Bank (1971), se crea la primera molécula de ADN recombinante (Paul Berg, 1972), E. M. Southern desarrolla la técnica Southern blot de localización de secuencias específicas de ADN (1976), comienza la secuenciación de ADN y el desarrollo de software para analizarlo (F. Sanger, software de R. Staden, 1977), y se publica en 1978 la primera secuencia de genes completa de un organismo, el fago Φ-X174 (5.386 pares de bases que codifican 9 proteínas). En ámbitos tecnológicos vinculados, en estos años se asiste al nacimiento del correo electrónico (Ray Tomlinson, BBN, 1971), al desarrollo de Ethernet (protocolo de comunicaciones que facilitará la interconexión de ordenadores, principalmente en redes de ámbito local) por Robert Metcalfe (1973), y al desarrollo del protocolo TCP ("Transmission Control Protocol", protocolo de control de transmisión) por Vinton Cerf y Robert Kahn (1974), uno de los protocolos básicos para Internet.

En la década de los 80 se asiste, en diversas áreas, a importantes avances:



En los años 90 asistimos a los siguientes eventos:



A destacar que en los años 2000 están culminando múltiples proyectos de secuenciación de genomas de diferentes organismos: en 2000 se publican, entre otros, el genoma de "Arabidopsis thaliana" (100 Mb) y el de "Drosophila melanogaster" (180 Mbp). Tras un borrador operativo de la secuencia de ADN del genoma humano del año 2000, en 2001 aparece publicado el genoma humano (3 Gbp). Poco después, en 2003, y con dos años de adelanto sobre lo previsto, se completa el "Human Genome Project". Por mencionar algunos de los genomas analizados en los años siguientes, anotaremos que en 2004 aparece el borrador del genoma de Rattus norvegicus (rata), en 2005 el del chimpancé, en 2006 el del macaco rhesus, en 2007 el del gato doméstico, y en 2008 se secuencia por primera vez el genoma de una mujer. Gracias al desarrollo de las técnicas adecuadas, asistimos actualmente a un aluvión de secuenciaciones de genomas de todo tipo de organismos.

En 2003 se funda en España el Instituto Nacional de Bioinformática, soportado por la Fundación Genoma España (fundada, a su vez, un año antes y que pretende constituirse en instrumento del estado para potenciar la investigación en este campo). En 2004, la estadounidense FDA ("Food and Drug Administration", agencia para la administración de alimentos y fármacos) autoriza el uso de un chip de ADN por primera vez. En 2005 se completa el proyecto HapMap (catalogación de variaciones genéticas en el ser humano). En 2008 UniProt presenta el primer borrador del proteoma completo del ser humano, con más de veinte mil entradas.

Poco a poco, los primeros programas bioinformáticos se van perfeccionando, y vemos versiones más completas como la 2.0 de ClustalW (reescrito en C++ en 2007).

Desde que el fago Φ-X174 fue secuenciado en 1977 (secuencia provisional: un año más tarde se publicaría la secuencia completa definitiva), las secuencias de ADN de cientos de organismos han sido decodificadas y guardadas en bases de datos. Esos datos son analizados para determinar los genes que codifican para ciertas proteínas, así como también secuencias reguladoras. Una comparación de genes en una especie o entre especies puede mostrar similitudes entre funciones de proteínas, o relaciones entre especies (uso de filogenética molecular para construir árboles filogenéticos).

Con la creciente cantidad de datos, desde hace mucho se ha vuelto poco práctico analizar secuencias de ADN manualmente. Hoy se usan programas de computadora para estudiar el genoma de miles de organismos, conteniendo miles de millones de nucleótidos. Estos programas pueden compensar mutaciones (con bases intercambiadas, borradas o insertadas) en la secuencia de ADN, para identificar secuencias que están relacionadas, pero que no son idénticas. Una variante de este alineamiento de secuencias se usa en el proceso de secuenciación.

La secuenciación conocida como ""shotgun"" (o "por perdigonada": fue usada, por ejemplo, por el Instituto de Investigación Genómica -"The Institute for Genomic Research", TIGR, hoy "J. Craig Venter Institute"- para secuenciar el primer genoma de bacteria, el Haemophilus influenzae) no da una lista secuencial de nucleótidos, pero en cambio nos ofrece las secuencias de miles de pequeños fragmentos de ADN (cada uno de aproximadamente 600 a 800 nucleótidos de largo). Las terminaciones de estos fragmentos se superponen y, cuando son alineados de la manera correcta, constituyen el genoma completo del organismo en cuestión.

El secuenciamiento "shotgun" proporciona datos de secuencia rápidamente, pero la tarea de ensamblar los fragmentos puede ser bastante complicada para genomas muy grandes. En el caso del Proyecto Genoma Humano, llevó varios meses de tiempo de procesador (en una estación DEC Alpha de alrededor del 2000) para ensamblar los fragmentos. El "shotgun sequencing" es el método de elección para todos los genomas secuenciados hoy en día y los algoritmos de ensamblado genómico son un área crítica de la investigación en bioinformática.

Otro aspecto de la bioinformática en análisis de secuencias es la búsqueda automática de genes y secuencias reguladoras dentro de un genoma. No todos los nucleótidos dentro de un genoma son genes. Dentro del genoma de organismos más avanzados, grandes partes del ADN no sirven a ningún propósito obvio. Este ADN, conocido como "ADN basura", puede, sin embargo, contener elementos funcionales todavía no reconocidos. La bioinformática sirve para estrechar la brecha entre los proyectos de genoma y proteoma (por ejemplo, en el uso de secuencias de ADN para identificación de proteínas).

En el contexto de la genómica, anotación es el proceso de marcado de los genes y otras características biológicas de la secuencia de ADN. El primer sistema software de anotación de genomas fue diseñado en 1995 por Owen White, quien fue miembro del equipo que secuenció y analizó el primer genoma en ser descodificado de un organismo independiente, la bacteria Haemophilus influenzae. White construyó un software para localizar los genes (lugares en la secuencia de DNA que codifican una proteína), el ARN de transferencia, y otras características, así como para realizar las primeras atribuciones de función a esos genes. La mayoría de los actuales sistemas de anotación genómica trabajan de forma similar, pero los programas disponibles para el análisis del genoma se encuentran en continuo cambio y mejora.

La Biología evolutiva es el estudio del origen ancestral de las especies, así como de su cambio a través del tiempo. La informática ha apoyado a los biólogos evolutivos en diferentes campos clave. Ha permitido a los investigadores:

Los esfuerzos futuros se centrarán en reconstruir el cada vez más complejo árbol filogenético de la vida.
El área de investigación de las ciencias de la computación denominada computación evolutiva se confunde ocasionalmente con la Biología evolutiva computacional, pero ambas áreas no guardan relación. Dicho campo se centra en el desarrollo de algoritmos genéticos y otras estrategias de resolución de problemas con una marcada inspiración evolutiva y genética.

La biodiversidad de un ecosistema puede definirse como el conjunto genómico completo de todas las especies presentes en un medio ambiente particular, sea este una biopelícula en una mina abandonada, una gota de agua de mar, un puñado de tierra, o la biosfera completa del planeta Tierra. Se utilizan bases de datos para recoger los nombres de las especies, así como de sus descripciones, distribuciones, información genética, estado y tamaños de las poblaciones, necesidades de su hábitat, y cómo cada organismo interactúa con otras especies. Se usa software especializado para encontrar, visualizar y analizar la información; y, lo que es más importante, para compartirla con otros interesados. La simulación computacional puede modelar cosas tales como dinámica poblacional, o calcular la mejora del acervo genético de una variedad (en agricultura), o la población amenazada (en biología de la conservación). Un potencial muy excitante en este campo es la posibilidad de preservar las secuencias completas del ADN, o genomas, de especies amenazadas de extinción, permitiendo registrar los resultados de la experimentación genética de la Naturaleza "in silico" para su posible reutilización futura, aún si tales especies fueran finalmente perdidas.

Pueden citarse, como ejemplos significativos, los proyectos Species 2000 o uBio.

La expresión génica de muchos genes puede determinarse por la medición de niveles de mRNA mediante múltiples técnicas, incluyendo microarrays de ADN, secuenciación de EST ( Expressed Sequence Tag), análisis en serie de la expresión génica ("Serial Analysis of Gene Expression" - SAGE), MPSS (Massively Parallel Signature Sequencing), o diversas aplicaciones de hibridación in situ. Todas estas técnicas son extremadamente propensas al ruido y/o sujetas a sesgos en la medición biológica, y una de las principales áreas de investigación en la biología computacional trata del desarrollo de herramientas estadísticas para separar la señal del ruido en los estudios de expresión génica con alto volumen de procesamiento. Estos estudios se usan a menudo para determinar los genes implicados en un desorden: podrían, por ejemplo, compararse datos de microarrays de células epiteliales cancerosas con datos de células no cancerosas para determinar las transcripciones que son activadas o reprimidas en una población particular de células cancerosas.

La regulación génica es la compleja orquestación de eventos que comienzan con una señal extracelular tal como una hormona, que conducen a un incremento o decremento en la actividad de una o más proteínas. Se han aplicado técnicas bioinformáticas para explorar varios pasos en este proceso. Por ejemplo, el análisis del promotor de un gen implica la identificación y estudio de las secuencias motivo en los alrededores del ADN de la región codificante de un gen. Estos motivos influyen en el alcance según el cual esa región se transcribe en ARNm. Los datos de expresión pueden usarse para inferir la regulación génica: podrían compararse datos de microarrays provenientes de una amplia variedad de estados de un organismo para formular hipótesis sobre los genes involucrados en cada estado. En un organismo unicelular, podrían compararse etapas del ciclo celular a lo largo de variadas condiciones de estrés (choque de calor, inanición, etc.). Podrían aplicarse, entonces, algoritmos de agrupamiento (algoritmos de "clustering", o análisis de "cluster") a esa información de expresión para determinar qué genes son expresados simultáneamente. Por ejemplo, los promotores de estos genes se pueden buscar según la abundancia de secuencias o elementos regulatorios.

Los microarrays de proteínas y la espectrometría de masas de alto rendimiento pueden proporcionar una instantánea de las proteínas presentes en una muestra biológica. La bioinformática está muy comprometida en dar soporte a ambos procedimientos. La aproximación a los microarrays de proteínas encara similares problemas a los existentes para microarrays destinados a ARNm, mientras que para la espectrometría de masas el problema es casar grandes cantidades de datos de masa contra masas predichas por bases de datos de secuencias de proteínas, además del complicado análisis estadístico de muestras donde se detectan múltiples, pero incompletos, péptidos de cada proteína.

En el cáncer, los genomas de las células afectadas son reordenados en complejas y/o aún impredecibles maneras. Se realizan esfuerzos masivos de secuenciación para identificar sustituciones individuales de bases (o puntos de mutación de nucleótidos) todavía desconocidos en una variedad de genes en el cáncer. Los bioinformáticos continúan produciendo sistemas automatizados para gestionar el importante volumen de datos de secuencias obtenido, y crean nuevos algoritmos y software para comparar los resultados de secuenciación con la creciente colección de secuencias del genoma humano y de los polimorfismos de la línea germinal. Se están utilizando nuevas tecnologías de detección física, como los microarrays de oligonucleótidos para identificar pérdidas y ganancias cromosómicas (técnica denominada hibridación genómica comparativa), y los arrays de polimorfismos de nucleótido simple para detectar "puntos de mutación" conocidos. Estos métodos de detección miden simultáneamente bastantes cientos de miles de posiciones a lo largo del genoma, y cuando se usan con una alta productividad para analizar miles de muestras, generan terabytes de datos por experimento. De esta forma las masivas cantidades y nuevos tipos de datos proporcionan nuevas oportunidades para los bioinformáticos. A menudo se encuentra en los datos una considerable variabilidad, o ruido, por lo que métodos como el de los modelos ocultos de Márkov y el análisis de puntos de cambio están siendo desarrollados para inferir cambios reales en el número de copias de los genes (número de copias de un gen particular en el genotipo de un individuo, cuya magnitud puede ser elevada en células cancerígenas).

Otro tipo de datos que requiere novedosos desarrollos informáticos es el análisis de las lesiones encontradas de forma recurrente en buen número de tumores, principalmente por análisis automatizado de imagen clínica.

La predicción de la estructura de las proteínas es otra importante aplicación de la bioinformática. La secuencia de aminoácidos de una proteína, también llamada estructura primaria, puede ser determinada fácilmente desde la secuencia de nucleótidos sobre el gen que la codifica. En la inmensa mayoría de los casos, esta estructura primaria determina únicamente una estructura de la proteína en su ambiente nativo. (Hay, por supuesto, excepciones, como la encefalopatía espongiforme bovina, o "mal de las vacas locas"; ver, también, prion.) El conocimiento de esta estructura es vital para entender la función de la proteína. En ausencia de mejores términos, la información estructural de las proteínas se clasifica usualmente como estructura secundaria, terciaria y cuaternaria. Una solución general viable para la predicción de tales estructuras permanece todavía como problema abierto. Por ahora, la mayoría de los esfuerzos han sido dirigidos hacia heurísticas que funcionan la mayoría de las veces.

Una de las ideas clave en bioinformática es la noción de homología. En la rama genómica de la bioinformática, se usa la homología para predecir la función de un gen: si la secuencia de gen "A", cuya función es conocida, es homóloga a la secuencia de gen "B", cuya función es desconocida, puede inferirse que B podría compartir la función de A. En la rama estructural de la bioinformática, la homología se usa para determinar qué partes de una proteína son importantes en la formación de la estructura y en la interacción con otras proteínas. En la técnica denominada modelado por homología, esta información se usa para predecir la estructura de una proteína una vez conocida la estructura de una proteína homóloga. Esta es, actualmente, la única vía para predecir estructuras de proteínas de una manera fiable.

Un ejemplo de lo anterior es la similar homología proteica entre la hemoglobina en humanos y la hemoglobina en las legumbres (leghemoglobina). Ambas sirven al mismo propósito de transportar oxígeno en el organismo. Aunque las dos tienen una secuencia de aminoácidos completamente diferente, sus estructuras son virtualmente idénticas, lo que refleja sus prácticamente idénticos propósitos.

Otras técnicas para predecir la estructura de las proteínas incluyen el enhebrado de proteínas ("protein threading") y el modelado "de novo" (desde cero), basado en las características físicas y químicas.

Al respecto, pueden verse también motivo estructural ("structural motif") y dominio estructural ("structural domain").

El núcleo del análisis comparativo del genoma es el establecimiento de la correspondencia entre genes (análisis ortólogo) o entre otras características genómicas de diferentes organismos. Estos mapas intergenómicos son los que hacen posible rastrear los procesos evolutivos responsables de la divergencia entre dos genomas. Una multitud de eventos evolutivos actuando a diferentes niveles organizativos conforman la evolución del genoma. Al nivel más bajo, las mutaciones puntuales afectan a nucleótidos individuales. Al mayor nivel, amplios segmentos cromosómicos experimentan duplicación, transferencia horizontal, inversión, transposición, borrado e inserción. Finalmente, los genomas enteros están involucrados en procesos de hibridación, poliploidía y endosimbiosis, conduciendo a menudo a una súbita especiación.

La complejidad de la evolución del genoma plantea muchos desafíos excitantes a desarrolladores de modelos matemáticos y algoritmos, quienes deben recurrir a un espectro de técnicas algorítmicas, estadísticas y matemáticas que se extienden desde exactas, heurísticas, con parámetros fijados, y mediante algoritmos de aproximación para problemas basados en modelos de parsimonia, hasta algoritmos "Márkov Chain Monte Carlo" para análisis Bayesiano de problemas basados en modelos probabilísticos.

Muchos de estos estudios están basados en la detección de homología y la computación de familias de proteínas.

La biología de sistemas implica el uso de simulaciones por ordenador de subsistemas celulares (tales como redes de metabolitos y enzimas que comprenden el metabolismo, caminos de transducción de señales, y redes de regulación genética), tanto para analizar como para visualizar las complejas conexiones de estos procesos celulares. La vida artificial o la evolución virtual tratan de entender los procesos evolutivos por medio de la simulación por ordenador de sencillas formas de vida (artificial).

Se están usando tecnologías de computación para acelerar o automatizar completamente el procesamiento, cuantificación y análisis de grandes cantidades de imágenes biomédicas con alto contenido en información. Los modernos sistemas de análisis de imagen incrementan la habilidad del observador para realizar análisis sobre un amplio o complejo conjunto de imágenes, mejorando la precisión, la objetividad (independencia de los resultados según el observador), o la rapidez. Un sistema de análisis totalmente desarrollado podría reemplazar completamente al observador. Aunque estos sistemas no son exclusivos del campo de las imágenes biomédicas, cada vez son más importantes tanto para el diagnóstico como para la investigación. Algunos ejemplos:

En las últimas dos décadas, decenas de miles de estructuras tridimensionales de proteínas han sido determinadas por cristalografía de rayos X y espectroscopia mediante resonancia magnética nuclear de proteínas (RMN de proteínas). Una cuestión central para los científicos es si resulta viable la predicción de posibles interacciones proteína-proteína solamente basados en esas formas 3D, sin realizar experimentos identificativos de estas interacciones. Se han desarrollado una variedad de métodos para enfrentarse al problema del acoplamiento proteína-proteína, aunque parece que queda todavía mucho trabajo en este campo.

Las herramientas de software para bioinformática van desde simples herramientas de línea de comandos hasta mucho más complejos programas gráficos y servicios web autónomos situados en compañías de bioinformática o instituciones públicas. La más conocida herramienta de biología computacional entre los los biólogos es, probablemente, BLAST, un algoritmo para determinar la similitud de secuencias arbitrarias con otras secuencias, probablemente residentes en bases de datos de proteínas o de secuencias de ADN. El NCBI ("National Center for Biotechnology Information", EE.UU.), por ejemplo, proporciona una implementación muy utilizada, basada en web, y que trabaja sobre sus bases de datos.

Para alineamientos múltiples de secuencias, el clásico ClustalW, actualmente en su versión 2, es el software de referencia. Puede trabajarse con una implementación del mismo en el EBI (Instituto Europeo de Bioinformática).

BLAST y ClustalW son sólo dos ejemplos de los muchos programas de alineamiento de secuencias disponibles. Existe, por otra parte, multitud de software bioinformático con otros objetivos: alineamiento estructural de proteínas, predicción de genes y otros motivos, predicción de estructura de proteínas, predicción de acoplamiento proteína-proteína, o modelado de sistemas biológicos, entre otros. En y pueden encontrarse sendas relaciones de programas o servicios web adecuados para cada uno de estos dos objetivos en particular.

Se han desarrollado interfaces basadas en SOAP y en REST ("Representational State Transfer", transferencia de estado representacional) para una amplia variedad de aplicaciones bioinformáticas, permitiendo que una aplicación, corriendo en un ordenador de cualquier parte del mundo, pueda usar algoritmos, datos y recursos de computación alojados en servidores en cualesquiera otras partes del planeta. Las principales ventajas radican en que el usuario final se despreocupa de actualizaciones y modificaciones en el software o en las bases de datos. Los servicios bioinformáticos básicos, de acuerdo a la clasificación implícita del EBI, pueden clasificarse en:

La disponibilidad de estos servicios web basados en SOAP a través de sistemas tales como los servicios de registro, (servicios de distribución y descubrimiento de datos a través de servicios web) demuestra la aplicabilidad de soluciones bioinformáticas basadas en web. Estas herramientas varían desde una colección de herramientas autónomas con un formato de datos común, y bajo una única interface autónoma o basada en web, hasta sistemas integradores y extensibles para la gestión del flujo de trabajo bioinformático.













</doc>
<doc id="8860" url="https://es.wikipedia.org/wiki?curid=8860" title="Uchepo">
Uchepo

El uchepo es un plato típico de la Tierra Caliente del Estado de Michoacán y Guerrero, México. Consiste en un tamal elaborado con maíz tierno (elote) molido, al cual en ocasiones se le agrega leche. Tiene un sabor dulce y su consistencia es suave. 

El uchepo se sirve solo, o bien con una salsa de tomatillo verde o salsa de jitomate cocido y acompañado de queso fresco o crema. Como postre suele servirse bañado en leche condensada. 

Aunque se le considera originario del estado de Michoacán, el uchepo puede encontrarse en otros estados de la República con el nombre de tamal de elote.

Para preparar uchepo para aproximadamente 4 personas es necesario que se cuente con 6 elotes tiernos, 10 gramos de mantequilla, 3 cucharadas de azúcar, 1 cucharadita de sal y hojas de elote frescas. Una vez que se cuenta con todos los ingredientes, se pasa a la preparación del mismo. Se toman los elotes y se les quitan todos los granos y se muelen; a la vez se hace una mezcla entre la mantequilla, la sal y el azúcar, que una vez teniendo dicha mezcla se le agrega a los granos de elote molidos. Una vez que se combinó, se comienza a elaborar masa en forma de tamalitos y se enrollan en las hojas de elote, poniéndolos a cocer durante 30 minutos en una vaporera, siendo el resultado esperado que la masa se pueda desprender con facilidad la hoja de elote del uchepo.



</doc>
<doc id="8861" url="https://es.wikipedia.org/wiki?curid=8861" title="Deidad">
Deidad

Una deidad, o un dios, es aquel al que normalmente se le atribuyen poderes importantes (aunque a algunas deidades no se les atribuye poder alguno). Puede ser adorado, concebido como santo, divino, sagrado o inmortal, tenido en alta estima, respetado o temido por sus adeptos y seguidores. Las deidades se representan con gran variedad de formas, pero con frecuencia con forma humana o animal; se les asignan personalidades y conciencias, intelecto, deseos y emociones como los humanos. Se le atribuyen fenómenos naturales tales como rayos, inundaciones y tormentas, así como milagros. Pueden ser concebidos como las autoridades o controladores de cada aspecto de la vida humana (tales como el nacimiento, la muerte o la otra vida). Algunas deidades son consideradas las directoras del tiempo y el propio destino, los dadores de la moralidad y las leyes humanas, los jueces definitivos del valor y el comportamiento humanos y los diseñadores y creadores de la Tierra o el Universo. Sin embargo, en las religiones monoteístas abrahámicas se considera blasfemo imaginar a la deidad con cualquier forma concreta.

La palabra española «deidad» procede del latín "deitas", ‘naturaleza divina’. Al igual que el sánscrito "deva", ‘ser celestial’ o ‘dios’, proviene de la raíz protoindoeuropea *"deiwos", ‘brillar’. De esta misma raíz derivan varias palabras relacionadas con el cielo: "dies", ‘día’ o "divum", ‘cielo abierto’.

De algunas deidades se piensa que son invisibles o inaccesibles para los humanos (morando principalmente en lugares sobrenaturales, remotos o apartados y sagrados, tales como el Cielo, el Infierno, el firmamento, el inframundo, bajo el mar, en la cima de montañas altas, en bosques profundos o en un plano sobrenatural o esfera celestial; o incluso en la mente y/o el subconsciente humano), revelándose o manifestándose en raras pero escogidas veces a los humanos y dándose a conocer principalmente por sus efectos.

En el monoteísta suele creerse que un único dios que mora en el Cielo también es omnipresente e invisible.

En el politeísmo los dioses se conciben como un contrapunto a los humanos. En el reconstruido e hipotético protoindoeuropeo los humanos eran descritos como "tkonion", ‘terrenales’, en oposición a los dioses, que eran "deivos", ‘celestiales’. Esta relación casi simbiótica está presente en muchas culturas posteriores: los humanos son definidos por su posición de súbditos a los dioses, a los que nutren con sacrificios, y los dioses son definidos por su soberanía sobre los humanos, castigándoles y recompensándoles, pero también dependientes de su adoración y en ocasiones la gente trata a su dios como alguien que les sirve a ellos.

El límite entre humano y divino no es en modo alguno absoluto en la mayoría de las culturas. Los semidioses son la descendencia de la unión entre un humano y una deidad, y las mayoría de las casas reales de la Antigüedad reclamaban ascendencias divinas. Comenzando con Neferirkara (siglo XXV a. C.), los faraones del Antiguo Egipto se hacían llamar «Hijos de Ra». Algunos gobernantes humanos, tales como los faraones del Imperio Medio, los emperadores japoneses y algunos emperadores romanos, han sido deidades adoradas por sus súbditos incluso en vida. El primer gobernante de quien se sabe que reclamó su divinidad es Naram-Sin (siglo XXII a. C.). En muchas culturas se cree que gobernantes y otras personas prominentes o santas se transforman en deidades tras su muerte (véase Osiris y canonización).

Se destaca también, que los panteones de diversas culturas cuentan tanto con deidades benefactoras como mundanas.


</doc>
<doc id="8865" url="https://es.wikipedia.org/wiki?curid=8865" title="Marcos el Evangelista">
Marcos el Evangelista

San Marcos Evangelista (en griego: Μάρκος; en latín: "Mārcus"; en copto: Μαρκοϲ; en hebreo: מרקוס) (siglo I), también conocido como Marcos el Evangelista, es considerado tradicionalmente el autor del Evangelio de Marcos y el fundador y primer obispo de la Iglesia de Alejandría.

Suele identificársele con Juan, llamado Marcos. Este personaje aparece varias veces en los Hechos de los Apóstoles. Se le cita por primera vez en Hechos 12:12, cuando Simón Pedro, milagrosamente liberado de la cárcel, se refugia en casa de María, madre de "Juan, por sobrenombre Marcos". Acompañó a Pablo de Tarso y a Bernabé en el primer viaje de Pablo (), pero se separó de ellos cuando llegaron a Panfilia, regresando a Jerusalén ( en estos versículos se hace referencia a él simplemente como "Juan"). Cuando Pablo iba a iniciar su segundo viaje, tuvo una grave disputa con Bernabé a propósito de "Juan, llamado Marcos": Bernabé quería que fuese con ellos, pero Pablo se negaba, ya que les había abandonado en el viaje anterior. Pablo y Bernabé terminaron por separarse, y Marcos acompañó al segundo en su viaje a Chipre ().

No está claro si este personaje, "Juan, llamado Marcos" es el mismo al que se hace referencia en algunas epístolas atribuidas a Pablo, concretamente en , , y en la Primera Epístola de Pedro (). En Colosenses se dice de él que es sobrino de Bernabé (), lo que podría explicar que este disputase con Pablo acerca de Marcos.

En el final de la Primera Epístola de Pedro, este se refiere a "mi hijo Marcos". Mientras que las iglesias copta, católica y ortodoxa aseguran que se trata de un hijo espiritual (es decir, que Marcos hubiese sido bautizado por Pedro) o que simplemente Pedro le tenía mucho cariño, varios teólogos protestantes no tienen inconveniente en interpretar que podría tratarse de un hijo biológico, fundamentado en la palabra griega "huios", que se aplica a descendientes.

Según el Evangelio que se le atribuye, cuando Jesús fue apresado en el Huerto de los Olivos, le seguía un joven envuelto en una sábana, habiendo especulado algunos con la posibilidad de que este joven fuera el mismo Juan Marcos.

La tradición dice que Marcos evangelizó como Obispo de Alejandría, en Egipto, donde realizó varios milagros y estableció una iglesia y su famosa escuela cristiana, nombrando un obispo, tres presbíteros y siete diáconos y murió allá como mártir el lunes de Pascua 25 de abril del año 68 (o en el 64 según algunas fuentes). Se narra que siete años antes de su martirio viajó a Marmarica y Libia. 

Según la tradición, la Iglesia Copta de Etiopía tiene su origen en las prédicas de San Marcos, autor del Segundo Evangelio en el siglo I, que llevó el cristianismo a Egipto en la época del emperador Nerón.

Los "Hechos de San Marcos", un escrito de mitad del siglo IV, refieren que San Marcos fue arrastrado por las calles de Alejandría, atado con cuerdas al cuello. Después lo llevaron a la cárcel y al día siguiente le volvieron a aplicar el mismo martirio hasta que falleció.

Marcos es considerado por la tradición cristiana el autor del evangelio que lleva su nombre. Puesto que él no fue discípulo directo de Jesús, por lo que basó su relato -siempre según la tradición- en las enseñanzas de Pedro. El autor más antiguo que asignó a Marcos la autoría de este evangelio fue Papías de Hierápolis, en la primera mitad del siglo II, en un testimonio citado por Eusebio de Cesarea.

"«y el anciano decía lo siguiente: Marcos, que fue intérprete de Pedro, escribió con exactitud todo lo que recordaba, pero no en orden de lo que el Señor dijo e hizo. Porque él no oyó ni siguió personalmente al Señor, sino, como dije, después a Pedro. Éste llevaba a cabo sus enseñanzas de acuerdo con las necesidades, pero no como quien va ordenando las palabras del Señor, más de modo que Marcos no se equivocó en absoluto cuando escribía ciertas cosas como las tenía en su memoria. Porque todo su empeño lo puso en no olvidar nada de lo que escuchó y en no escribir nada falso»". (Eusebio, Hist. Ecl. iii. 39).

Desde el siglo II se dio por sentado que Marcos era el autor de este evangelio. Aunque es imposible tener ningún tipo de certeza a este respecto, se ha aducido convincentemente que no hay ninguna razón por la cual los primeros cristianos tuvieran que adjudicar la autoría de este evangelio a un personaje desconocido que no fue discípulo directo de Jesús, en lugar de atribuírsela a uno de los apóstoles.

En el 828, las reliquias atribuidas a San Marcos fueron llevadas de Alejandría por navegantes italianos, que las trasladaron a Venecia, donde se conservan en la Basílica de San Marcos, construida expresamente para albergar sus restos. Los coptos creen que la cabeza del santo quedó en Alejandría. Cada año, en el día 30 del mes de Babah, la Iglesia Copta conmemora la consagración de la iglesia de San Marcos, y la aparición de la cabeza del santo en la iglesia copta de San Marcos, en Alejandría, donde se conservaría su cabeza.

Se asocia a San Marcos con el león porque su Evangelio empieza hablando del desierto, y el león era considerado el rey del desierto y porque su Evangelio empieza hablando del río Jordán y a sus alrededores había muchas fieras, entre ellas el león. También se dice que es el león porque en su Evangelio comienza hablando de Juan el Bautista como ""Voz que clama en el desierto"", voz que sería como la de un león.

La Iglesia católica celebra su fiesta el 25 de abril.

San Marcos es el patrono de Venecia desde el año 828, sustituyendo a San Teodoro. La historia cuenta que dos mercaderes, Buono Tribuno da Malomocco y Rustico da Torcello robaron sus reliquias de su tumba de Alejandría en Egipto y las escondieron en una carga de carne de cerdo para que los guardias musulmanes no lo descubrieran. Cuando llegaron a Venecia, lo donaron al Dux. Giustiniano Partecipazio que las colocaría en el castillo. Inmediatamente se comenzó a construir un santuario siguiendo el modelo de la basílica de los Doce Apóstoles de Constantinopla, la cual fue consagrada en el año 832. Un siglo más tarde fue destruida el año 976 por un incendio durante la insurrección del Dux Candiano IV. La antigua iglesia de san Teodoro presentaba una tipología de planta de cruz griega inscrita en el espacio central. Este tipo de planta estuvo difundida en Oriente durante el siglo VII como las iglesias de la Dormición de Nicea, San Nicola di Mira, la iglesia de la Virgen de Haks y diversos ejemplos en Armenia. Los restos de la antigua iglesia parece ser que se vieron englobados en la nueva San Marcos construida entre 1063 y 1094.

San Marcos Evangelista es el santo patrono de San Marcos, Nicaragua.

En la localidad madrileña de San Martín de la Vega, se celebra su patronazgo.

Por Acuerdo Gubernativo del 16 de mayo de 1934, la feria titular de la cabecera departamental del departamento de San Marcos, conocida como Feria Departamental de Primavera, se celebra del 22 al 28 de abril; siendo el día principal el 25, fecha en que la Iglesia Católica conmemora a San Marcos Evangelista patrono del municipio y departamento.



</doc>
<doc id="8866" url="https://es.wikipedia.org/wiki?curid=8866" title="Evangelio de Marcos">
Evangelio de Marcos

El Evangelio según Marcos o Evangelio de Marcos (en griego, κατὰ Μᾶρκον εὐαγγέλιον; abreviado, Mc) es el segundo libro del Nuevo Testamento de la Biblia cristiana. 

Es el más breve de los cuatro evangelios canónicos y también el más antiguo según la opinión mayoritaria de los expertos bíblicos. 

Entre los estudiosos existe un amplio consenso en datar el Evangelio de Marcos a finales de los años 60 del siglo I d.C., o poco después del año 70 d.C. Su autor es desconocido, aunque una tradición cristiana tardía lo atribuye a Marcos, personaje citado en otros pasajes del Nuevo Testamento. Narra la vida de Jesús de Nazaret desde su bautismo por Juan el Bautista hasta su resurrección.

Existe una estrecha relación entre los tres evangelios sinópticos (Marcos, Mateo y Lucas). De los 662 versículos que componen el Evangelio de Marcos, 406 son comunes tanto con Mateo como con Lucas, 145 sólo con Mateo y 60 sólo con Lucas. Únicamente 51 versículos de Marcos no tienen paralelo en ninguno de los otros dos sinópticos. 

La tradición cristiana había establecido que el evangelio más antiguo era el de Mateo. Se había llegado a afirmar que el de Marcos era un resumen de los Evangelios de Mateo y Lucas.

Weisse y Wilke, de modo independiente, en 1838 concluyeron que el Evangelio de Marcos no era un resumen de Mateo y Lucas, sino que era anterior a ellos y más bien les había servido de fuente. Además, Weisse estableció la teoría de que existía una fuente común a Mateo y Lucas. Johannes Weiss, en 1890, denominó con la letra Q a esta fuente (de "Quelle", que significa ‘fuente’ en alemán). La teoría de las dos fuentes fue analizada y sistematizada por Heinrich Julius Holtzmann.

La hipótesis más extendida para explicar la relación entre Marcos y los otros dos evangelios sinópticos, Mateo y Lucas, es hoy la teoría de las dos fuentes. Esto no quiere decir que todos los expertos la acepten, ni que no puedan oponérsele diversas objeciones. Hay bastante acuerdo, sin embargo, en que Marcos fue el primero de los cuatro evangelios en ser redactado. 

En el marco de la teoría de las dos fuentes, las posibles relaciones entre el Evangelio de Marcos y la fuente Q han sido estudiadas por autores como L. Burton Mack ("The Lost Gospel: The Book of Q and Christian origins", 1993) y Udo Schnelle ("The History and Theology of the New Testament Writings", 1998).

No existen pruebas definitivas acerca de quién fue el autor de este evangelio. El texto no incluye ninguna indicación sobre su autoría. 

La tradición cristiana, sin embargo, ha atribuido el evangelio a Marcos, discípulo de Pedro, personaje citado en las epístolas de Pablo de Tarso (concretamente en Col 4,10), en los Hechos de los apóstoles (Hch 12,12-25; Hch 13,15; Hch 15,37), donde es presentado como compañero de Pablo y en la primera epístola de Pedro, que lo llama "mi hijo" (1 Pedro 5:13).

La base de esta tradición se encuentra en algunas referencias de los primitivos autores cristianos a la idea de que Marcos puso por escrito los recuerdos del apóstol Pedro. Eusebio de Cesarea, que escribió a comienzos del siglo IV, cita en su "Historia eclesiástica" un fragmento de la obra hoy perdida de Papías de Hierápolis, de comienzos del siglo II. Papías, a su vez, remonta su testimonio a Juan el Presbítero.

Hacia el año 180, Ireneo de Lyon, escribió: 

El apologista Justino Mártir cita una información que se encuentra también en el Evangelio de Marcos diciendo que son las memorias de Pedro (Dial. 106.3). En , el discurso de Pedro resume las líneas generales del Evangelio de Marcos. Por otro lado, no parece haber ninguna razón por la cual los primitivos cristianos tuvieran que adjudicar la autoría de este evangelio a un personaje oscuro que no fue discípulo directo de Jesús, en lugar de atribuírsela a uno de los apóstoles.

Algunos autores actuales consideran sumamente dudosa la atribución a Marcos, dado que la teología de este evangelio parece más cercana a las ideas de Pablo de Tarso que a las de Pedro, que sale bastante malparado en el relato marcano. Tanto los errores del autor en cuestiones referentes a la geografía palestinense, como lo que se sabe del proceso de composición de la obra no parecen abonar la teoría de la escritura de este evangelio por un discípulo directo de Pedro. Parece demostrado que antes de la escritura de este evangelio circulaban ya oralmente breves relatos sobre Jesús y sus dichos ("perícopas"), y que el autor recopiló estos materiales heterogéneos.

El autor, se trate o no de Marcos, parece ser que se dirige predominantemente a pagano-cristianos, más que a judeocristianos. Cada vez que emplea un término en hebreo o en arameo, lo traduce al griego, lo que hace suponer que se dirige a una audiencia no familiarizada con estos idiomas. Utiliza la traducción al griego de la Biblia, la Biblia de los Setenta, y no su versión original hebrea, y no está familiarizado con la geografía de Palestina.

El evangelista utiliza en algunas de sus citas y expresiones la versión griega de la Biblia, en lugar de usar la versión hebrea o aramea, como sería de esperar en un judío originario de Judea.


El texto del Evangelio de Marcos tiene abundantes expresiones semíticas. Para algunos autores, esto sería indicio de que se basa en un texto arameo (o varios textos, según teorías modernas). Destacan los siguientes: 

También se destacan otras construcciones sintácticas de influencia aramea, hasta en 38 versículos.

Desde la época de Clemente de Alejandría a finales del siglo II se sostuvo que este evangelio fue escrito en Roma, basándose en los latinismos que aparecen en el texto, como "denarius" o "legión". Algunos de los latinismos empleados por Marcos que no aparecen en los otros evangelios son "σπεκουλατορα" ("speculatora", soldados de la guardia, ), "ξεστων" (corrupción de "sextarius", vaso, ) o "κεντυριων" ("centurión", , ). Se detectaron también paralelismos entre el Evangelio de Marcos y la Epístola a los romanos de san Pablo.

Sin embargo, la hipótesis del origen romano del Evangelio de Marcos fue cuestionada por autores como Reginald Fuller ("A Critical Introduction to the New Testament"), dado que los latinismos presentes en el evangelio marcano suelen ser términos relacionados con la vida militar, por lo que eran muy probablemente palabras conocidas en todas las regiones del Imperio Romano en las que existían guarniciones militares. Se ha propuesto como alternativa la posibilidad de que fuese redactado en Antioquía. Sin embargo, no existen indicios claros acerca del lugar donde fue compuesto el Evangelio de Marcos.

La idea más extendida es que el Evangelio de Marcos fue escrito para una comunidad cristiana helenística de lengua griega radicada en algún lugar del Imperio Romano. Parece que los destinatarios de este Evangelio desconocían las tradiciones judías, ya que en varios pasajes el autor las explica (, , ). También desconocían probablemente el arameo, ya que se traducen al griego las frases "ταλιθα κουμ" ("talitha kum", ) αββα ("abba", ), y el hebreo, que también se traduce κορβαν ("Corban", ). 

Las citas del Antiguo Testamento proceden en general de la "Biblia de los Setenta", traducción al griego (, , ). 

Además, en el evangelio es perceptible una cierta actitud antijudía en la caracterización de los fariseos, o en la atribución a los miembros del Sanedrín, más que a las autoridades romanas, de la responsabilidad de la muerte de Jesús.

Si se acepta la hipótesis de que el texto fue redactado en una fecha temprana y si se da por hecho que el autor es Marcos es posible que:



La mayoría de los estudiosos bíblicos data la redacción de este evangelio en su estado actual entre los años 65 y 75.

El año 65 como "terminus a quo" se debe a dos motivos, fundamentalmente: por un lado, según lo que mayoritariamente se acepta sobre el proceso de composición de este Evangelio, se requirió cierto tiempo para que se desarrollasen las diferentes tradiciones orales sobre Jesús (perícopas) que se cree el autor de Marcos utilizó para la confección de su obra. En segundo lugar, se cree que ciertos pasajes del texto reflejan los acontecimientos de la primera guerra judía, según se conocen por otras fuentes, especialmente las obras de Flavio Josefo, aunque se discute si la destrucción del Templo de Jerusalén (que tuvo lugar en el año 70) se había producido ya o se consideraba próxima. Los eruditos que consideran que ya se había producido basan su opinión sobre todo en el análisis del capítulo decimotercero de Marcos (Mc 13), conocido como "Apocalipsis Sinóptico" o "Pequeño Apocalipsis de Marcos", y en algunos otros fragmentos.

El año 80 es considerado por la mayoría el "terminus ad quem" para la adaptación de este Evangelio, ya que, en el marco de la teoría de las dos fuentes, se cree que el de Marcos es el Evangelio más antiguo, y que fue utilizado como fuente por Mateo y Lucas, que escribieron, según se cree, entre los años 80 y 100. Varios autores consideran que lo más probable es que fuese compuesto antes del año 68, año del martirio de Marcos en Alejandría.

Algunos eruditos, sin embargo, han propuesto una revisión radical de esta cronología: algunos de ellos proponen fechas muy tempranas, mientras que otros lo datan en épocas tan tardías como la Rebelión de Bar Kojba.
La hipótesis de la datación temprana recibió un impulso importante cuando el erudito español Josep O’Callaghan Martínez afirmó que el papiro 7Q5, un manuscrito descubierto en Qumrán, era un fragmento del Evangelio de Marcos. De ser cierta esta hipótesis, existirían secciones escritas del Evangelio de Marcos que podrían ser anteriores al año 50, es decir, apenas posteriores a la muerte de Jesús. Esto significaría un revés para las hipótesis de quienes sostienen que los Evangelios canónicos fueron redactados tardíamente, incluso después del siglo II, y para quienes consideran que los relatos evangélicos sobre Jesús son míticos. Muchos exégetas (entre ellos Kurt Aland, Bruce Metzger, Joseph Fitzmyer, o el qumranólogo Julio Trebolle) rechazaron la hipótesis de O'Callaghan de que el papiro se corresponda con el texto del Evangelio de Marcos, y con ellos otros autores que revisaron el tema.

El Evangelio de Marcos relata la historia de Jesús de Nazaret desde su bautismo hasta su resurrección. A diferencia de los otros dos sinópticos, no contiene material narrativo acerca de la vida de Jesús anterior al comienzo de su predicación. 

Marcos está de acuerdo en lo esencial con la teología paulina: lo único importante en Jesús es su muerte y su resurrección. No obstante, a diferencia de Pablo, se ocupa de consignar los hechos y dichos de Jesús. 

En Marcos se relatan cuatro exorcismos practicados por Jesús: 


Estos exorcismos son recogidos en los otros sinópticos: el segundo y el cuarto tanto por Lucas como por Mateo; el primero sólo por Lucas (Lc 4, 31-37) y el tercero sólo por Mateo (Mt 15, 21-28)

Existen otros ocho relatos detallados de curaciones de diversas dolencias realizadas por Jesús:


De estas ocho curaciones, seis son recogidas en los otros dos sinópticos, y sólo dos de ellas (la curación del sordomudo de la Decápolis y la del ciego de Betsaida) no aparecen en ninguno de los otros evangelios. Es destacable el hecho de que sólo en estas dos curaciones emplea Jesús medios mágicos (concretamente, utiliza la saliva para "abrir" la lengua y los oídos de uno y los ojos del otro). 

J. M. González Ruiz: “Paralelos en las teologías marquiana y paulina”, en "Revista Catalana de Teología" 14 (1989); pp. 323-332.

El final del Evangelio de Marcos, a partir de Mc 16, 9, en el que se narran las apariciones de Jesús resucitado a María Magdalena, a dos discípulos que iban de camino y a los once apóstoles, así como la ascensión de Jesús, es casi seguro que se trata de una adición posterior. 

De hecho, en la nota a pie de página de la Biblia de Jerusalén podemos leer lo siguiente: 

"El final de Marcos vv 9-20, forma parte de las Escrituras inspiradas; es considerado como canónico. Esto no significa necesariamente que haya sido redactado por Marcos. De hecho, se pone en duda su pertenencia a la redacción del segundo Evangelio".

De hecho, los versículos 9-20 no aparecen en ninguno de los manuscritos conservados más antiguos y se ha comprobado que el estilo es muy diferente al resto del Evangelio. Orígenes, en el siglo III, cuando cita los relatos de la Resurrección, se refiere a los otros tres canónicos, pero no a Marcos. Algunos manuscritos, además, añaden otros finales diferentes del actual.

La incógnita es si Marcos quiso que tuviese este final, si tuvo que finalizar bruscamente por alguna razón desconocida o si hubo un final que se perdió.

El texto arameo que probablemente sirvió de fuente a Marcos parece ser en realidad una recopilación de narraciones en fragmentos diversos, que pudieron llegar a los evangelistas como una colección de textos, o bien ya interconectados en una primera historia evangélica. Algunos autores de la tercera búsqueda del Jesús histórico consideran que puede clasificarse cada una de estas unidades literarias en función de sus coincidencias o divergencias entre los evangelios. De este modo, entre las más antiguas se destacarían las narraciones de la Pasión, y entre las más modernas, las de infancia y los materiales propios de cada evangelista. 




</doc>
<doc id="8870" url="https://es.wikipedia.org/wiki?curid=8870" title="Estequiometría">
Estequiometría

En química, la estequiometría (del griego "στοιχειον", "stoikheion", 'elemento' y "μετρον", "métrón", 'medida') es el cálculo de las relaciones cuantitativas entre los reactivos y productos en el transcurso de una reacción química. Estas relaciones se pueden deducir a partir de la teoría atómica, aunque históricamente se enunciaron sin hacer referencia a la composición de la materia, según distintas leyes y principios.

El primero que enunció los principios de la estequiometría fue Jeremias Benjamin Richter (1762-1807), en 1792, quien describió la estequiometría de la siguiente manera:

También estudia la proporción de los distintos elementos en un compuesto químico y la composición de mezclas químicas.

Una reacción química se produce cuando hay una modificación en la identidad química de las sustancias intervinientes; esto significa que no es posible identificar a las mismas sustancias antes y después de producirse la reacción química, los reactivos se consumen para dar lugar a los productos.

A escala microscópica una reacción química se produce por la colisión de las partículas que intervienen ya sean moléculas, átomos o iones, aunque puede producirse también por el choque de algunos átomos o moléculas con otros tipos de partículas, tales como electrones o fotones. Este choque provoca que las uniones que existían previamente entre los átomos se rompan y se facilite que se formen nuevas uniones. Es decir que, a escala atómica, es un reordenamiento de los enlaces entre los átomos que intervienen. Este reordenamiento se produce por desplazamientos de electrones: unos enlaces se rompen y otros se forman, sin embargo los átomos implicados no desaparecen, ni se crean nuevos átomos. Esto es lo que se conoce como "ley de conservación de la masa", e implica los dos principios siguientes:


En el transcurso de las reacciones químicas las partículas subatómicas tampoco desaparecen, el número total de protones, neutrones y electrones permanece constante. Y como los protones tienen carga positiva y los electrones tienen carga negativa, la suma total de cargas no se modifica. Esto es especialmente importante tenerlo en cuenta para el caso de los electrones, ya que es posible que durante el transcurso de una reacción química salten de un átomo a otro o de una molécula a otra, pero el número total de electrones permanece constante. Esto que es una consecuencia natural de la ley de conservación de la masa se denomina ley de conservación de la carga e implica que:


Las relaciones entre las cantidades de reactivos consumidos y productos formados dependen directamente de estas leyes de conservación, y por lo tanto pueden ser determinadas por una ecuación (igualdad matemática) que las describa. A esta igualdad se le llama ecuación estequiométrica.

Una ecuación química es una representación escrita de una reacción química. Se basa en el uso de símbolos químicos que identifican a los átomos que intervienen y como se encuentran agrupados antes y después de la reacción. Cada grupo de átomos se encuentra separado por símbolos (+) y representa a las moléculas que participan, cuenta además con una serie de números que indican la cantidad de átomos de cada tipo que las forman y la cantidad de moléculas que intervienen, y con una flecha que indica la situación inicial y la final de la reacción. Así por ejemplo en la reacción: 

Tenemos los grupos de átomos (moléculas) siguientes:


Los subíndices indican la atomicidad, es decir la cantidad de átomos de cada tipo que forman cada agrupación de átomos (molécula). Así el primer grupo arriba representado, indica a una molécula que está formada por 2 átomos de oxígeno, el segundo a dos moléculas formadas por 2 átomos de hidrógeno, y el tercero representa a un grupo de dos moléculas formadas cada una por 2 átomos de hidrógeno y uno de oxígeno, es decir dos moléculas de agua.

Es el número de moléculas de un determinado tipo que participa en una ecuación química dada en el orden en el que está escrita. En el siguiente ejemplo:

El coeficiente del metano es 1, el del oxígeno 2, el del dióxido de carbono 1 y el del agua 2. Los coeficientes estequiométricos son en principio números enteros, aunque para ajustar ciertas reacciones alguna vez se emplean números fraccionarios.

Cuando el coeficiente estequiométrico es igual a 1, no se escribe. Por eso, en el ejemplo CH y CO no llevan ningún coeficiente delante.

Así por ejemplo


Debe leerse como 1(O) es decir, un grupo de moléculas de oxígeno. Y la expresión:


Debe leerse como 2(HO), es decir dos grupos o moléculas, cada uno de los cuales se encuentra formado por dos átomos de hidrógeno y uno de oxígeno.

Dado que una ecuación química es una representación simplificada o mínima de una reacción química, es importante considerar todos los datos representados; ya que perder de vista a alguno significa no entender realmente la situación representada. Los símbolos y subíndices representan a las especies químicas que participan, y los coeficientes representan al número de moléculas de cada tipo que se encuentran participando de la reacción.

Finalmente la flecha indica cual es el sentido predominante en el cual la reacción química progresa. Así en el ejemplo anterior vemos que CH y O se encuentran en la situación "antes de", es decir del lado de los reactivos y HO y CO se encuentran en la situación de "después de", es decir del lado de los productos. La ecuación completa debería leerse así:

Se dice que una ecuación química se encuentra ajustada, equilibrada o balanceada cuando respeta la ley de conservación de la materia, según la cual la cantidad de átomos de cada elemento debe ser igual del lado de los reactivos (antes de la flecha) y en lado de los productos de la reacción (después de la flecha). 

Para balancear una ecuación, se deben ajustar los coeficientes, y no los subíndices. Esto es así porque cada tipo de molécula tiene siempre la misma composición, es decir se encuentra siempre formada por la misma cantidad de átomos, si modificamos los subíndices estamos nombrando a sustancias diferentes:

HO es agua común y corriente, pero HO es peróxido de hidrógeno una sustancia química totalmente diferente. Al modificar los coeficientes sólo estamos diciendo que ponemos más o menos de tal o cual sustancia.

Por ejemplo, en la reacción de combustión de metano (CH), éste se combina con oxígeno molecular (O) del aire para formar dióxido de carbono (CO) y agua. (HO). La reacción sin ajustar será:

En esta ecuación, las incógnitas son "a", "b", "c" y "d", que son los denominados coeficientes estequiométricos. Para calcularlos, debe tenerse en cuenta la ley de conservación de la materia, por lo que la suma de los átomos de cada elemento debe ser igual en los reactivos y en los productos de la reacción. Existen tres métodos principales para balancear una ecuación estequiométrica, que son, el método de tanteo, el método algebraico y el método de ion-electrón para ecuaciones de tipo redox.

El método de tanteo se basa simplemente en modificar los coeficientes de uno y otro lado de la ecuación hasta que se cumplan las condiciones de balance de masa. No es un método rígido, aunque tiene una serie de delineamientos principales que pueden facilitar el encontrar rápidamente la condición de igualdad.


En el ejemplo, se puede observar que el elemento que participa con un estado de oxidación de mayor valor absoluto es el carbono que actúa con estado de oxidación (+4), mientras el oxígeno lo hace con estado de oxidación (-2) y el hidrógeno con (+1).

Comenzando con el carbono, se iguala de la forma más sencilla posible, es decir con coeficiente 1 a cada lado de la ecuación, y de ser necesario luego se corrige.

Se continúa igualando el oxígeno, se puede observar que a la derecha de la ecuación, así como está planteada, hay 3 átomos de oxígeno, mientras que a la izquierda hay una molécula que contiene dos átomos de oxígeno. Como no se deben tocar los subíndices para ajustar una ecuación, simplemente añadimos media molécula más de oxígeno a la izquierda:

O lo que es lo mismo:

Luego se iguala el hidrógeno. A la izquierda de la ecuación hay cuatro átomos de hidrógeno, mientras que a la derecha hay dos. Se añade un coeficiente 2 frente a la molécula de agua para balancear el hidrógeno:

El hidrógeno queda balanceado, sin embargo ahora se puede observar que a la izquierda de la ecuación hay 3 átomos de oxígeno (3/2 de molécula) mientras que a la derecha hay 4 átomos de oxígeno (2 en el óxido de carbono (II) y 2 en las moléculas de agua). Se balancea nuevamente el oxígeno agregando un átomo más (1/2 molécula más) a la izquierda:

O lo que es lo mismo:

Ahora la ecuación queda perfectamente balanceada. El método de tanteo es útil para balancear rápidamente ecuaciones sencillas, sin embargo se torna súmamente engorroso para balancear ecuaciones en las cuales hay más de tres o cuatro elementos que cambian sus estados de oxidación. En esos casos resulta más sencillo aplicar otros métodos de balanceo.

El método algebraico se basa en el planteamiento de un sistema de ecuaciones en la cual los coeficientes estequiométricos participan como incógnitas, procediendo luego despejar estas incógnitas. Es posible sin embargo que muchas veces queden planteados sistemas de ecuaciones con más incógnitas que ecuaciones, en esos casos la solución se halla igualando cualquiera de los coeficientes a 1 y luego despejando el resto en relación a él. Finalmente se multiplican todos los coeficientes por un número de modo tal de encontrar la menor relación posible entre coeficientes enteros.

En el ejemplo:

para el elemento hidrógeno (H) hay 4·a átomos en los reactivos y 2·d átomos en los productos. De esta manera se puede plantear una condición de igualdad para el hidrógeno:

Y procediendo de la misma forma para el resto de los elementos participantes se obtiene un sistema de ecuaciones:

Con lo que tenemos un sistema lineal de tres ecuaciones con cuatro incógnitas homogéneo:

Al ser un sistema homogéneo tenemos la solución trivial:

Pero debemos buscar una solución que no sea trivial, ya que esta implicaría que no hay "ningún" átomo, y no describe el planteo químico, proseguimos a simplificar:

Si, la tercera ecuación, la cambiamos de signo, la multiplicamos por dos y le sumamos la primera tendremos:

Pasando d al segundo miembro, tenemos:

Con lo que tenemos el sistema resuelto en función de d:

Se trata en encontrar el menor valor de d que garantice que todos los coeficientes sean números enteros, en este caso haciendo d= 2, tendremos:

Sustituyendo los coeficientes estequimétricos en la ecuación de la reacción, se obtiene la ecuación ajustada de la reacción:

Ésta dice que 1 molécula de metano reacciona con 2 moléculas de oxígeno para dar 1 molécula de dióxido de carbono y 2 moléculas de agua.

Al fijar arbitrariamente un coeficiente e ir deduciendo los demás pueden obtenerse valores racionales no enteros. En este caso, se multiplican todos los coeficientes por el mínimo común múltiplo de los denominadores. En reacciones más complejas, como es el caso de las reacciones redox, se emplea el método del ion-electrón.

Las reacciones electroquímicas se pueden balancear por el método ion-electrón donde la reacción global se divide en dos "semirreacciones" (una de oxidación y otra de reducción), se efectúa el balance de carga y elemento, agregando H, OH, HO y/o electrones para compensar los cambios de oxidación.
Antes de empezar a balancear se tiene que determinar en que medio ocurre la reacción, debido a que se procede de una manera en particular para cada medio.

Se explicará por medio de un ejemplo, cuando manganésica reacciona con bismutato de sodio.

También se explicará por medio de un ejemplo, cuando el permanganato de potasio reacciona con el sulfito de sodio.

Ecuación balanceada:

Cuando los reactivos de una reacción están en cantidades proporcionales a sus coeficientes estequiométricos se dice:


Las tres expresiones tienen el mismo significado.

En estas condiciones, si la reacción es completa, todos los reactivos se consumirán dando las cantidades estequiométricas de productos correspondientes.

Si no en esta forma, existirá el reactivo limitante que es el que está en menor proporción y que con base en él se trabajan todos los cálculos.

Ejemplo

La ecuación química que representa la reacción química es:
Se tienen las siguientes equivalencias a partir de la reacción química y las masas atómicas citadas:

Esta última relación es consecuencia de la fórmula química del oxígeno molecular (formula_20)

Entonces para determinar la masa de oxígeno podemos realizar los siguientes "pasos": determinamos las moles de átomos de carbono (primer factor), con estas moles fácilmente determinamos las moles de moléculas de oxígeno (segundo factor a partir de coeficientes de la ecuación química), y finalmente obtenemos la masa de oxígeno (tercer factor)

realizadas las operaciones:

Los cálculos estequiométricos se basan en las relaciones fijas de combinación que hay entre las sustancias en las reacciones químicas balanceadas. Estas relaciones están indicadas por los subíndices numéricos que aparecen en las fórmulas y por los coeficientes. Este tipo de cálculos es muy importante y se utilizan de manera rutinaria en el análisis químico y durante la producción de las sustancias químicas en la industria. 
Los cálculos estequiométricos requieren una unidad química que relacione las masas de los reactantes con las masas de los productos. Esta unidad química es el mol.

Ejemplo de la vida diaria.
La estequiometria la podemos usar por ejemplo cuando vamos al médico porque tenemos un dolor ocasionado por una infección, el doctor debe de sacar la cuenta de nuestro peso con los gramos que contiene el medicamento y sobre la base de esto sacar la medida exacta para saber cuántas pastillas o cuantos mililitros nos tenemos que tomar de dichos medicamentos.




</doc>
<doc id="8873" url="https://es.wikipedia.org/wiki?curid=8873" title="Coregonus lavaretus">
Coregonus lavaretus

La farra o lavareto es un pez de agua dulce, parecido al salmón, que vive principalmente en los lagos alpinos. Tiene la cabeza pequeña y aguda, la boca pequeña, la lengua corta, el lomo verdoso y el vientre plateado. Su carne es muy apreciada.

Posiblemente es conespecífico del «corégono» ("Coregonus clupeaformis" Mitchill, 1818) y a menudo hay disputas sobre la correcta clasificación de ambos.

Los machos pueden alcanzar los 73 cm de longitud total y los 10 kg de peso.

Desovan durante la noche.

Comen crustáceos planctónicos y bentónicos.

Es depredado por el "Coregonus peled" –en Rusia–, el lucio europeo ("Esox lucius") -en Inglaterra i Gales-, 'el 'Sander lucioperca" -en Finlandia- y la trucha común ("Salmo trutta") -en Finlandia-. 

Vive en áreas de clima templado (entre 4-16 °C).

Se encuentra en Europa y se ha introducido en Irán (1965-1967).


</doc>
<doc id="8874" url="https://es.wikipedia.org/wiki?curid=8874" title="Sarah Bernhardt">
Sarah Bernhardt

Sarah Bernhardt (París, 23 de octubre de 1844-Ib., 26 de marzo de 1923) fue una actriz de teatro y cine francesa.

Sarah Bernhardt nació el 23 de octubre de 1844 en el número 5 de la calle de l'École-de-Médecine, en París. Su nombre real era Rosine Bernardt. Su madre era una mujer de religión judía de origen neerlandés llamada Judith-Julie Bernardt (1821-1876), alias Youle. Se ganaba la vida como cortesana junto con su hermana Rosine. Julie tuvo varias hijas más. En abril de 1843 tuvo dos niñas gemelas que fallecieron a las dos semanas. Tras Sarah, tuvo a Jeanne (fecha de nacimiento desconocida) y a Régine en 1855, que murió de tuberculosis en 1873. Todas fueron hijas de padres distintos y desconocidos. Sarah Bernhardt nunca supo quién era su padre biológico, aunque se cree que era el duque de Morny, medio hermano de Napoleón III.

Sarah pasó los primeros cuatro años de su vida en Bretaña al cuidado de un ama de cría. La primera lengua que Sarah aprendió fue el bretón y por esta razón, al iniciar su carrera teatral, adoptó la forma bretona de su apellido, «Bernhardt». En esta época sufrió un accidente que muchos años después le acarrearía graves problemas de salud. Cayó de una ventana, rompiéndose la rodilla derecha. Aunque sanó sin problemas, la rodilla le quedó delicada para siempre, y en 1914, a causa de una dolorosa inflamación de esa misma rodilla, tuvieron que amputarle la pierna derecha. Tras el accidente, su madre la llevó consigo a París, donde permaneció dos años. A punto de cumplir siete años ingresó en la Institución Fressard, un internado para señoritas próximo a Auteuil. Permaneció allí dos años. En 1853 entró en el colegio conventual Grandchamp, cercano a Versalles. En este colegio participó en su primera obra teatral, "Tobías recupera la vista", escrita por una de las monjas. También aquí fue bautizada e hizo la primera comunión. El ambiente místico del colegio le hizo plantearse el hacerse monja.

Tras abandonar Grandchamp a los 15 años, su madre trató de introducirla en el mundo galante para que se ganara la vida como cortesana. Pero Sarah, influenciada por su educación conventual, se negó repetidamente a ello. Julie Bernard tenía un salón en su piso parisiense donde se reunían sus clientes. Entre ellos estaba el medio hermano de Napoleón III, el duque de Morny. Morny aconsejó que Sarah se inscribiera en el Conservatorio de música y declamación. Gracias a los contactos del duque, Sarah entró sin dificultad en 1859. En 1861 ganó un segundo premio en tragedia y una mención honorífica en comedia. 

Finalizados sus estudios en el Conservatorio, entró, de nuevo gracias a los influyentes contactos de Morny, en la Comédie-Française. Debutó el 11 de agosto de 1862 con la obra "Iphigénie," de Jean Racine. Su fuerte carácter le atrajo problemas con sus compañeros, lo que provocó que abandonara la Comédie por primera vez en 1863. Tres semanas más tarde fue contratada por el Teatro Gymnase, donde hizo siete pequeños papeles en distintas obras. Actuó por última vez el 7 de abril de 1864 con la obra "Un mari qui lance sa femme". 

Ese mismo año conoció a uno de los grandes amores de su vida, Charles-Joseph Lamoral, príncipe de Ligne. Inició una apasionada relación con él, hasta que quedó embarazada y el príncipe la abandonó. El 22 de diciembre de 1864 dio a luz a su único hijo, Maurice Bernhardt. Sin oficio y habiendo fracasado momentáneamente en el mundo del teatro, siguió los pasos de su madre, convirtiéndose en cortesana de lujo. Sarah no abandonó su actividad como cortesana hasta que su carrera teatral se hubo afianzado con éxito y pudo mantenerse sólo con el trabajo que le reportaba el teatro.

Tres años más tarde, en 1867 debutó en el Teatro del Odéon con "Las mujeres sabias" ("Les femmes savantes") de Molière. Ahí empezó su verdadera carrera profesional. Participó en muchos montajes teatrales, alternando la vida teatral con la vida galante. La fama le llegó repentinamente en 1869 con "Le Passant," de François Coppée, una obra en verso de un solo acto. Sarah, además, hizo por primera vez en esta obra un papel masculino, el del trovador Zanetto. Repetiría más veces haciendo de hombre en varias obras más ("Lorenzaccio", "Hamlet" y "L'Aiglon").

En 1870, durante la guerra franco-prusiana, habilitó el Odeón como hospital para convalecientes, donde cuidó con dedicación a los heridos de guerra. En 1871 el improvisado hospital tuvo que ser cerrado por problemas de salubridad.
TTras la derrota francesa y la caída de Napoleón III, muchos intelectuales, exiliados por estar en contra del emperador, pudieron regresar a Francia, entre ellos Victor Hugo. El regreso de Hugo fue trascendental en la vida de Bernhardt, ya que el escritor la eligió para protagonizar el reestreno de su obra Ruy Blas. Bernhardt además protagonizó otra obra de Hugo, "Hernani". "Ruy Blas" la encumbró a cotas de éxito inimaginables. Regresó a la "Comédie-Française" como una gran estrella y allí afianzó su repertorio y sus múltiples registros como actriz.

El estilo de actuación de Bernhardt se basaba en la naturalidad. Detestaba profundamente las viejas normas del teatro francés, donde los actores declamaban histriónicamente y hacían gestos exagerados. Rompió con todo lo establecido, profundizando en la psicología de los personajes. Estudiaba cada gesto y cada entonación del texto que debía decir, buscando la perfección natural sin que se notara ningún tipo de artificio. Destaca en su arte que, representando siempre a grandes heroínas de tragedia o reinas, huyó de la sobreactuación y de la afectación. Son famosas sus escenas de muerte, en las que en vez de, según sus propias palabras, «ofrecer toda una retahíla de patologías"»" tales como estertores, toses, gemidos agónicos, profundizaba en el acto de morir desde el punto de vista psicológico y sentimental.

Aparte de su profesión de actriz, se interesó por la escultura y la pintura, llegando a exponer varias veces en el Salón de París entre los años 1874 y 1896. Recibió distintos premios y menciones honoríficas en ambas disciplinas. Escribió también tres libros: su autobiografía titulada "Ma double vie, Petite Idole" y "L´art du Théâtre: la voix, la geste, la pronontiation."

Bernhardt se especializó en representar las obras en verso de Jean Racine, tales como "Iphigénie", "Phédre" o "Andromaque". Destacó especialmente, entre muchas otras, en "La Dame aux camélias", de Dumas hijo, "Théodora", de Sardou, "L'Aiglon", de Edmond Rostand, "Izéïl", de Silvestre y Morand, "Macbeth", de Shakespeare y "Jeanne D'Arc," de Jules Barbier.

En 1879 realizó su primera salida de Francia, concretamente a Inglaterra, donde estuvo seis semanas haciendo dos representaciones diarias y obtuvo un éxito rotundo. Al llegar al país fue recibida espectacularmente, lo que indica que su fama había cruzado las fronteras de Francia. En esta primera visita conoció a un joven escritor llamado Oscar Wilde. Años más tarde, en 1893, Bernhardt aceptaría representar su obra "Salomé". Ese mismo año, Sarah fue ascendida a Socio Pleno de la Comédie-Française. Los Socios Plenos son la jerarquía más alta de esta institución.

Tras su espectacular éxito en Inglaterra decidió hacer su primera gira americana. Partió a los Estados Unidos el 15 de octubre de 1880. El éxito fue total. Bernhardt haría repetidas giras por los Estados Unidos (sus famosas «giras de despedida») y también recorrió toda América del Sur, llegando a actuar en Brasil, Perú, Argentina, Chile... Viajaba en tren y en barco y llegó a cruzar el cabo de Hornos. En Estados Unidos su éxito era tal que le habilitaron un tren con siete vagones de lujo llamado "Sarah Bernhardt Special", que era de uso exclusivo de la actriz. Sus giras le llevaron a Australia y visitó las islas Hawái y las islas Sandwich. Actuó en Egipto y en Turquía. Asimismo recorrió Europa, actuando en Moscú, Berlín, Bucarest, Roma, Atenas. En su periplo, actuó no solo en grandes teatros, sino también en teatros de ínfima categoría. 

Bernhardt tuvo una agitada vida sentimental, en la que destacan nombres como Gustave Doré, Victor Hugo, Jean Mounet-Sully, Jean Richepin, Philippe Garnier, Gabriele D'Annunzio, Eduardo, Príncipe de Gales, entre otros. Se casó una sola vez, con un oficial griego llamado Jacques Aristidis Damala. Damala era hijo de un rico armador y era adicto a la morfina. Nació en El Pireo en 1842. Bernhardt se casó con él el 4 de abril de 1882 y fue un matrimonio tempestuoso. Sarah intentó convertir en actor a Damala, pero fracasó. La actriz le impartió clases de actuación y le dio el papel de Armand Duval en "La Dame aux Camélias". Se eran infieles mutuamente, y un día Damala, abrumado por el éxito de su mujer, por las constantes burlas de los actores de la compañía de Bernhardt y la mala relación con Maurice Bernhardt, se alistó en la Legión, siendo destinado a Argelia. Meses más tarde regresó con Sarah. Las separaciones y reconciliaciones fueron continuas hasta que Sarah decidió irse de gira por todo el continente americano en 1887 y Damala ya no la acompañó. Era la separación definitiva. Permanecieron casados hasta la muerte de Damala por los efectos secundarios del abuso continuado de morfina, en 1889, a la edad de 42 años. Bernhardt lo enterró en Atenas y adornó la tumba con un busto tallado por ella misma.

Sarah Bernhardt fue también la primera actriz empresaria del mundo del espectáculo. A raíz de una relación muy tensa con el director de la Comédie-Française, Perrin, Bernhardt rompió su contrato y dimitió como Socio Pleno el 18 de marzo de 1880. La Comédie pleiteó contra ella, ganando el juicio. Sarah Bernhardt tuvo que renunciar a su pensión de 43 000 francos que habría tenido de pensión si hubiese permanecido un mínimo de veinte años en la Comédie, y además se la condenó a 100000 francos de multa, que nunca llegó a pagar. Tras su esplendorosa primera gira americana, que le había hecho ganar una gran fortuna, Bernhardt arrendó el teatro Porte-Saint-Martin en 1883. En este teatro produjo y actuó en obras como "Frou-Frou" y "La Dame aux camélias", entre otras. Durante sus giras, el teatro permanecía abierto y se estrenaban obras continuamente con distinto éxito comercial. Bernhardt no dudaba en apoyar el teatro de vanguardia, así que, además del repertorio clásico, en el Porte-Saint-Martin se estrenaban obras de nuevos autores que rompían con el teatro tradicional. Tras unos años, Bernhardt arrendó el Théatre de la Renaissance, donde representó muchas obras de éxito. En 1899 alquiló por veinticinco años el enorme Theâtre des Nations, único teatro donde actuaría en Francia durante los últimos veinticuatro años de su vida.

Su vida familiar no fue sencilla. Tuvo una relación tensa y distante con su madre, Julie. Su progenitora nunca fue una madre cariñosa e interesada, y esto hizo que Sarah siempre buscase su aprobación y su cariño. Julie Bernard sentía predilección tan solo por su hija Jeanne y descuidó totalmente la educación de su hija menor, Régine. Sarah Bernhardt sentía predilección por su hermana pequeña Régine, y cuando logró ser independiente, se la llevó a vivir consigo para alejarla de la madre y de las intenciones de esta de convertirla también en cortesana. Lamentablemente, a causa del abandono afectivo que sufrió y del ambiente del piso de su madre, Régine se prostituyó a los trece años. Falleció a los dieciocho, en 1873, de tuberculosis. Su otra hermana, Jeanne, también fue cortesana durante una época y siempre que tenía necesidad de dinero. Para apartarla de la mala vida, Bernhardt se la llevó consigo con su compañía y la acompañó en varias de sus giras por Estados Unidos y Europa. Era una actriz mediocre, pero hacía pequeños papeles y vivía una vida de lujo junto a su hermana. Se sabe que sufrió crisis de neurosis a causa de su adicción a la morfina y que estuvo ingresada en el hospital de La Pitié-Salpetrière en París, al cuidado del doctor Jean-Martin Charcot. En cambio, el hijo de Sarah, Maurice, siempre estuvo muy unido a su madre. Vivió siempre a su sombra, malgastando auténticas fortunas en el juego, en viajes y en una vida regalada.
El siglo XX empezó con un gran éxito, "L'Aiglon", de Edmond Rostand. La obra fue estrenada el 15 de marzo de 1900 y obtuvo un éxito sin precedentes. Sarah hizo 250 representaciones de "L'Aiglon" y, tras esto, hizo otra gira a Estados Unidos para representarla. En Nueva York representó la obra en el Metropolitan Opera House y cosechó un enorme éxito. Probó suerte también con el recién nacido cine. En 1900 filmó "Le Duel d'Hamlet", haciendo ella de Hamlet. En 1906 rodó "La Dame aux camélias", con Lou Tellegen, su amante de aquel momento, haciendo de Armand Duval. Bernhardt, cuando la vio, se horrorizó y mandó destruir el negativo, que afortunadamente todavía existe. Rodó también "Elisabeth, reine d'Anglaterre", dirigida por Louis Mercanton. En 1913 filmó "Jeanne Doré", dirigida por Tristan Bernard. Esta película se considera la mejor rodada por Bernhardt y donde se puede observar mejor su arte interpretativo. La película se conserva en la Cinématèque de Paris.

En 1914 le fue concedida la Legión de Honor. En 1915 la rodilla derecha, la misma que se había fracturado de niña, había llegado a provocarle dolores insoportables. Para colmo, durante una de sus interpretaciones de la obra dramática "Tosca" —la misma que Puccini hizo triunfar en el género operístico—, en la última escena, cuando la heroína se lanza desde un barranco, no se tomaron las medidas de seguridad pertinentes; Sarah se lanzó y se hirió la pierna. Aunque hacía ya varios años que padecía molestias constantes, durante el año 1914 fue empeorando, hasta que no hubo otro remedio que amputar en febrero de 1915. Una vez recuperada de la amputación, y ya empezada la Primera Guerra Mundial, la actriz decidió hacer una gira tras las trincheras francesas haciendo actuaciones para animar a las tropas. Organizó varias giras con su compañía y recorrió toda Francia. Aun con la pierna amputada, Sarah Bernhardt siguió actuando. Recitaba monólogos, poemas o representaba actos famosos de su repertorio de obras en las que no debía estar de pie. Siguió también participando en películas tras la guerra. Su salud fue empeorando hasta sufrir un grave ataque de uremia que estuvo a punto de matarla. En 1922 vendió su mansión en el campo de Belle-Île-en-Mer, donde había rodado años atrás una película documental sobre su vida. Cuando le llegó la muerte estaba rodando una película, "La Voyante". El rodaje se estaba realizando en su casa, en el Boulevard Péreire, puesto que la actriz estaba ya muy delicada de salud. El 15 de marzo de 1923, tras rodar una escena, quedó totalmente agotada, hasta que se desmayó. Nunca se recuperó. Once días más tarde, el 23 de marzo, fallecía en brazos de su hijo Maurice.

Su entierro fue multitudinario. Unos 150 000 franceses acudieron a despedirla. Fue inhumada en el cementerio parisino del Père-Lachaise.

A pesar de ser llamada «la divina Sarah» por su carácter excéntrico y caprichoso, Sarah Bernhardt trabajó en innumerables proyectos teatrales demostrando un carácter perseverante, una gran profesionalidad y dedicación a su arte.



Carmen Verlichak, "Las diosas de la Belle Époque y de los 'años locos"', Editorial Atlántida, Buenos Aires, 1996 (ISBN 950-08-1599-0)



</doc>
<doc id="8875" url="https://es.wikipedia.org/wiki?curid=8875" title="Alejandro Dolina">
Alejandro Dolina

Alejandro Ricardo Dolina (Morse, 20 de mayo de 1944) es un escritor, músico, conductor de radio y de televisión y actor argentino. Realizó estudios de Derecho, Música, Letras e Historia. Es conocido dentro y fuera de su país por sus obras literarias y su clásico programa radial "La venganza será terrible".

Dolina nació en Morse, cerca de Baigorrita, en la provincia de Buenos Aires, y pasó su primera infancia en la localidad bonaerense de Caseros. Su madre, Delfa Virginia Colombo (1922-1994), era maestra. Su padre era contador, ejecutivo de Plavinil Argentina.

Estudió música y literatura desde la juventud. Aunque siempre ha evitado comentarios sobre su vida privada, a menudo comparte anécdotas relativas a su juventud en compañía de músicos y juerguistas profesionales. Tuvo diversos empleos. Se sabe que fue operario de ENTEL y estudiante de Derecho.

A los 22 años abandonó la carrera de Derecho y estuvo desempleado. En una fiesta conoció a Manuel Evequoz quien, interesado por la fina inteligencia y el humor de Dolina, trabó amistad y le consiguió trabajo en una agencia publicitaria. Esto significó su introducción en los medios de comunicación y el descubrimiento de su vocación en el ambiente. Dolina fue un gran amigo de Evequoz y en él está inspirado su personaje Manuel Mandeb. Evequoz pertenecía a Montoneros y desapareció durante la dictadura de 1976. El personaje fue creado mientras Evequoz vivía. No obstante, sus textos serían publicados en la década siguiente.

Desde su juventud fue aficionado al tango, a la filosofía y la literatura. La mujer tiene un rol fundamental en su discurso y aun en sus motivaciones, cuando afirma que «todo lo que hago lo hago para levantar minas». Esa cita es erróneamente atribuida a Dolina, ya que en verdad pertenece al humorista Caloi, que lo puso en boca del personaje Alexis Dolinades, inspirado en él. Dolina retoma esta afirmación en su obra "Lo que me costó el amor de Laura" (1998): «Se ha dicho que el hombre hace todo lo que hace con el único fin de enamorar mujeres».

A principios de la década de 1970, Dolina inició su carrera en publicidad y escribió artículos para "Satiricón", una revista que, por medio del humor, comentaba temas de la política, sociedad y estilo de vida del momento. Durante este período trabajó con Carlos Trillo, quien también se dedicaba a la publicidad y se convertiría luego en un exitoso guionista de historietas.

En 1978, después de que la revista "Satiricón" fuese clausurada por la Junta Militar que gobernaba el país, Dolina comenzó a escribir para la revista "Humor". Durante esos años, Dolina se dedicó a escribir sobre el honor, el amor, la amistad y hasta creó cierta mitología centrada en personajes como el Ángel Gris de Flores, el escritor ficticio Manuel Mandeb y otros. Esas historias fueron publicadas en el libro "Crónicas del Ángel Gris" en 1987 y más tarde transformadas en un musical. Estos personajes aparecerían en todos sus libros posteriores.

En 2011 fue una de las voces de las piezas publicitarias del Banco Provincia.

En 1975 hizo sus primeras participaciones radiales en "Mañanitas nocturnas", programa de Carlos Ulanovsky y Mario Mactas que se transmitió por Radio Argentina. Interpretaba a un periodista llamado Gómez. Allí apareció por primera vez el personaje del Sordo Gancé, un músico improvisado, presente hasta hoy en las emisiones de "La venganza será terrible".

El 2 de abril de 1985 Dolina debutó en radio al conducir un programa que se emitía por Radio El Mundo, "Demasiado tarde para lágrimas", junto a Adolfo Castelo.
Bajo el mismo nombre, el programa se trasladó en 1989 a Radio Rivadavia y, brevemente, durante 1991 (apenas un mes) a LRA Radio Nacional. Luego pasó a la radio Viva FM, cambiando su nombre por "El ombligo del mundo". Durante 1993 continuó en FM Tango bautizado, por motivos contractuales, como "La venganza será terrible", llegando a Radio Continental (1994-2000 y 2002-2006), y Radio Del Plata, donde se transmitió solamente durante 2001 mientras, a la misma hora, Radio Continental emitía programas grabados de temporadas anteriores. A finales de 2006, el programa se trasladó a Radio 10, donde permanecería hasta fines de 2009. Desde febrero de 2010 a diciembre de 2011 se emitió por LRA Radio Nacional, con Patricio Barton todas las noches, y Gabriel Schultz y Jorge Dorio en forma alternada. A partir de enero de 2012 el programa se emite, ya sin Schultz, por Radio del Plata en dúplex con 360 TV.
Por su trabajo en este programa Dolina ganó, en 1991, el Premio Konex al mejor conductor.

Su programa de radio fue líder en su franja horaria desde el primer año de emisiones, con un encendido superior a la mitad de los radio escuchas de todo el país. Considerado ya un clásico de la radiofonía del Río de la Plata, suele hacer presentaciones en vivo del programa incluso fuera de la Ciudad de Buenos Aires. Una de las últimas funciones de 2012 se llevó a cabo en el ex cine de Burzaco, donde la gente del partido de Almirante Brown y de los partidos aledaños llenó el recinto y a la finalización del evento, lo ovacionó de pie. Según dice el mismo Dolina «es extraño cómo se sostiene una audiencia numéricamente tan grande en un país donde se supone que no se lee, cuando para entender mi programa al menos hay que haber hojeado dos libros». Su labor diaria es tanto una invitación a la historia y la literatura como al surrealismo. Logra hacer prosa tanto de un fragmento de la Odisea como de un decálogo de consejos para quitar mejor las manchas de la ropa. Su capacidad de improvisación como narrador, actor y músico asombra día a día.
En marzo de 2013, realizó un programa especial desde el Espacio Memoria y Derechos Humanos, donde funcionara la ex Escuela de Mecánica de la Armada.

En septiembre de 2016, renuncia a Radio Del Plata y Dolina aseguró que "La venganza será terrible" volverá en otra emisora. Desde el 16 de septiembre se emite el programa por AM 750 en su horario habitual de la medianoche.

Luego de "Crónicas del Ángel Gris" (1987), su libro más exitoso hasta el momento, publicó "El libro del Fantasma" (1999), "Bar del Infierno" (2005) (colecciones de cuentos) y su primera novela, "Cartas marcadas" (2012). Aborda temas históricos, filosóficos y costumbristas en torno a los Hombres Sensibles de Flores, sus personajes recurrentes. De clara influencia borgeana, alterna la literatura fantástica (historias de ángeles, demonios, metamorfosis y milagros), el ensayo («Bovarismo descendente» en "El libro..."; «El otro infierno» en "El bar...", entre otros) y el relato histórico («Elisa Brown», «Saint Germain», etc.).

Dolina es cantor y compositor. En sus programas de radio y televisión siempre incluyó segmentos musicales. En 1990 adaptó las "Crónicas" y presentó la comedia musical "El barrio del Ángel Gris". Recibió por ella el premio Argentores. En 1998, grabó su opereta "Lo que me costó el amor de Laura" junto a Mercedes Sosa, Sandro, Joan Manuel Serrat y Ernesto Sabato, entre otros. En 2002, adaptó algunos de sus viejos radioteatros y grabó "Radiocine". En 2004 editó el CD "Tangos del Bar del Infierno".

Protagonizó dos programas: "La barra de Dolina" (1989 por Canal 11, 1990 por ATC) y "Bar del Infierno" (2003, Canal 7). Además participó del programa "Fuga de cerebros" emitido hacia 1991 por el entonces canal ATC, junto a Lalo Mir, Elizabeth Vernaci y Manuel Wirzt.

Se emitió en 2011 por la señal del Canal Encuentro y luego por la Televisión Pública el documental ficticio "Recordando el show de Alejandro Molina", escrito y protagonizado por Dolina bajo de la dirección de Juan José Campanella. Esta serie de trece capítulos de media hora contó con la participación de Ale y Martín Dolina, Patricio Barton, Gillespi, Coco Silly, Gabriel Rolón, Manuel Moreira entre otros.









En 2001 obtuvo el nombramiento de Ciudadano Ilustre de la ciudad de Buenos Aires. Asimismo, en 2003 fue declarado Visitante Ilustre de la ciudad de Montevideo, Uruguay.

El 18 de octubre de 2014 la Universidad Nacional de San Juan, a través de su rector y presidente del Consejo Superior, Oscar Nasisi, aprobó la otorgación del Título Doctor Honoris Causa a Alejandro Dolina.



</doc>
<doc id="8878" url="https://es.wikipedia.org/wiki?curid=8878" title="Intrón">
Intrón

Un intrón es una región del ADN que forma parte de la transcripción primaria de ARN, pero a diferencia de los exones, son eliminados del transcrito maduro, previamente a su traducción.

El número y longitud de los intrones varía enormemente entre especies, así como entre los genes de una misma especie. Por ejemplo, el pez globo, "Takifugu rubripees", tiene pocos intrones en su genoma; mientras que los mamíferos y las angiospermas (plantas con flores) suelen presentar numerosos intrones.

La palabra "intrón" se deriva del término "región intragénica", es decir una región dentro de un gen. A pesar de ser a veces llamados "secuencias interventoras" el término puede referirse a cualquiera de las muchas familias de secuencias internas de ácidos nucléicos que no están presentes en el gen final, como lo son las inteínas, las secuencias no traducidas (UTR) y los nucleótidos eliminados en la edición del RNA.
Los intrones fueron descubiertos por Phillip Allen Sharp y Richard J. Roberts, lo que les supuso ganar el en 1993. El término intrón fue introducido por el bioquímico estadounidense Walter Gilbert en 1978.

Los intrones pueden representar un sitio alternativo de splicing, pudiendo dar diferentes tipos de proteínas. El control del splicing está regulado por una amplia variedad de señales moleculares. Los intrones también pueden contener “información antigua”, es decir, fragmentos de genes que probablemente se expresaban pero que actualmente no se expresan.

Tradicionalmente se ha afirmado que los intrones son fragmentos de ADN carentes de información. Sin embargo esta afirmación es cuestionada y actualmente goza de pocos adeptos. Se sabe que los intrones contienen varias secuencias pequeñas que son importantes para un ajuste eficiente.

Algunos intrones del grupo I y II son ribozimas capaces de catalizar su propio splicing fuera del ARN. El descubrimiento de estas propiedades auto-catalíticas supuso el a Thomas R. Cech y Sidney Altman en 1989.

Actualmente se reconocen cuatro clases de intrones:

Los intrones del grupo I, II y III son intrones que sufren de autosplicing mediante reacciones de transesterificación. La frecuencia con la que encontramos estos intrones en el genoma es relativamente rara si la comparamos con la frecuencia de los intrones spliceosomales.

Los intrones del grupo II y III son muy similares y presentan una estructura secundaria altamente conservada. De hecho a veces los intrones del grupo III son identificados como intrones del grupo II debido a su similitud funcional y estructural. 

Los intrones del grupo I están presentes en los genes de ARNr de algunos eucariotas inferiores y en los genes mitocondriales de hongos. Se caracterizan por eliminarse mediante un proceso autocatalítico que requiere de una guanosina o un nucleótido de guanosina libre; así como por carecer de secuencias consenso en los puntos de empalme, aunque pueden tenerlas en su interior.

Los del grupo II y III se eliminan mediante un proceso autocatalítico que requiere de una adenina o de un spliceosoma, respectivamente. En ambos grupos, durante el proceso de empalme de los exones, se forma una estructura en lazo característica denominada lariat.

Los del grupo IV están presentes en los ARNt de los eucariotas y se caracterizan por ser los únicos que se eliminan mediante un corte endonucleótido seguido de un ligamiento en lugar de la reacción de transesterificación

Existen dos modelos, contrapuestos, que explican el origen y la evolución de los intrones nucleares o ayustosomales. Estos modelos se conocen como "intrones tempranos (IE)" o "intrones tardíos (IL)". 

El modelo IE propone que los intrones eran extremadamente numerosos en los ancestros de procariotas y eucariotas; y se fueron perdiendo a lo largo de la evolución. Este modelo se basa en la hipótesis de que los intrones fueron mediadores que facilitaron la combinación de exones, facilitando por tanto la evolución de nuevos genes.

El modelo IL propone que los intrones aparecieron tras la divergencia de procariotas y eucariotas. Este modelo se basa en la observación de que los intrones ayustosomales únicamente se han encontrado en eucariotas








</doc>
<doc id="8880" url="https://es.wikipedia.org/wiki?curid=8880" title="Los Pitufos">
Los Pitufos

Los Pitufos (en el original francés, "Les Schtroumpfs") son unos personajes creados por el dibujante belga Peyo en la historieta "La flauta de los seis pitufos" ("La Flûte à Six Schtroumpfs"), de su serie "Johan y Pirluit", para el semanario "Le Journal de Spirou" el 23 de octubre de 1958. Tal fue el éxito de estas criaturas azules de pequeño tamaño, equivalentes a gnomos o duendes benignos, que al año siguiente empezaron a protagonizar su propia serie de historietas, así como películas, series de dibujos animados y videojuegos.
El nombre original de los pitufos en francés es "Schtroumpfs". Su nombre en español se le ocurrió a Miguel Agustí, redactor jefe de la revista "Strong", donde fueron publicados por vez primera en castellano en 1969. Durante más de un mes, estuvo buscando un nombre que pudiera conjugarse hasta que recordó el personaje de Patufet, figura emblemática del folclore catalán (y el nombre de una célebre revista infantil de preguerra en catalán). De ahí derivó el nombre de "Los Pitufos", que se mantendría en las siguientes versiones españolas (menos en TBO, donde aparecieron brevemente a mediados de los 70 rebautizados como "Los Tebeítos").

Los pitufos hicieron su aparición, como estrictos secundarios, en el episodio "La Flûte à Six Trous", publicado en los números 1047 a 1086 del semanario "Le Journal de Spirou", de la serie "Johan y Pirluit". En este episodio, Pirluit encuentra una flauta mágica que le roban más tarde y, ante la necesidad de hacerse con otra el mago Homnibus, envía a los dos amigos a una tierra desconocida, el País Maldito, en donde viven los Pitufos.

En "La Guerre des Sept Fontaines" (1959), Peyo volvió a introducir a los pitufos de forma prudencial, aún anecdótica. Los pitufos tenían un gran éxito y con "Los pitufos negros" inauguraron en julio de ese año una colección de mini-relatos incluidos con "Le Journal de Spirou":

En enero de 1960, La Flûte à Six Trous se editó en álbum con el título de "La Flûte à Six Schtroumpfs" ("La Flauta de los Seis Pitufos"), prueba de las tendencias del mercado. Peyo creyó poder prescindir de ellos en el siguiente episodio de Johan y Pirluit, "L'Anneau des Castellac", iniciado en agosto de ese año, pero el experimento no fue satisfactorio, ya que las ventas de los episodios "con pitufos" superaban a las de "sin pitufos", como le hizo notar el editor.
En el duodécimo episodio de Johan y Pirluit, "Le Pays Maudit" (1964), los Pitufos están omnipresentes en la historia, de principio a fin. Se habían hecho tan populares que provocarían la desaparición casi total de "Johan et Pirlouit", ante la falta de tiempo de su creador para dedicarse a ella por entero.

Sin embargo, no fue hasta 1963, con la historieta "Pitufofonía en do", que los pitufos empezaron a aparecer de forma serializada en "Spirou", además en álbumes publicitarios.

Las nuevas entregas de la serie dejaron entonces de aparecer en la revista "Spirou", pero en noviembre de 1989 se lanzó una nueva revista, "Schtroumpf!", dirigida a los más pequeños.

Tras la muerte de Peyo en 1992, la serie continuó con su hijo Thierry Culliford a cargo de los guiones:

En España, sus historietas aparecieron traducidas al castellano en las revistas "Strong" (1969-1971), "TBO" (1974-1975), "Zipi y Zape", "Zipi y Zape Especial" y "Super Zipi y Zape" (1979-1980), "Pulgarcito" (1981-1982) o "Fuera Borda" (1984-1985). En formato álbum han sido editados sucesivamente por Argos (1969-1971), Bruguera (1979-1983), Grijalbo (1983-1985), Ediciones B (1991-1992), Planeta-DeAgostini (2006-2007) y actualmente Norma.

En el año 2008, se organizaron diversos actos con motivo de su 50 aniversario, incluyendo una exposición retrospectiva en el Centro Belga de la BD con el título de "L'Union fait la Schtroumpf".

Los pitufos constituyen una comunidad secreta de pequeños seres azules que viven en setas u hongos en lo profundo del bosque, durante el Medioevo. No usan nombres propios para tratarse entre sí, y tienen todos el mismo tamaño, apariencia y vestimenta (unos pantalones y un gorro frigio blanco, que son rojos sólo para el Gran Pitufo o Papá Pitufo que los lidera), pero entre ellos sí que parecen distinguirse; a pesar de ello existe una serie de personajes recurrentes, que se distinguen por sus virtudes o defectos, por sus aficiones o por alguna otra peculiaridad, y son llamados por ella: tenemos así al Pitufo Gafotas (Pitufo Filósofo en Hispanoamérica), el Pitufo Bromista, el Pitufo Valiente, la Pitufina, el Pitufo Goloso, el Pitufo Gruñón, el Pitufo Manitas, el Pitufo Vanidoso, el Pitufo Poeta, el Pitufo Simple, el Pitufo Perezoso, el Pitufo Labrador, el Pitufo Deportista...

Su lengua entremezcla palabras humanas normales con la palabra "pitufo", usada tanto como nombre, como adjetivo ("pitufado/a") o como verbo ("pitufar"), y que a los humanos suena siempre igual, pero que ellos parecen distinguir sin problemas; por ejemplo, en la historieta "El País maldito" (1964) un pitufo huye de su hogar para pedir ayuda a Johan y Pirluit contra "el pitufo que pitufa pitufo", y se levanta una fuerte polémica tratando de averiguar qué significa la expresión, que de hecho acabó por referirse a "un dragón que echa fuego".

Sin embargo, en la mayoría de las ocasiones sus enemigos son el brujo Gargamel y su gato Azrael.

La banda sonora de la serie de televisión ha recurrido a menudo a notables piezas de música clásica como "Scheherezade" de Nicolái Rimski-Kórsakov o la "Sinfonía n.º 8 (Inacabada)" de Franz Schubert (ambientación de suspense, generalmente en presencia de Gargamel).

La obra de Peyo ha suscitado interpretaciones ideológicas dispares, más o menos fundamentadas.

"Les Schtroumpfs noirs" (1959), por ejemplo, ha sido tachada de racista.

Se ha llegado a afirmar incluso que la misma sociedad pitufa, con su elogio del comunitarismo y el líder supremo, constituye una apología del totalitarismo, tanto comunista como nazi. Según esta interpretación, Gargamel y Azrael serían una clara caricatura del judío y del capitalismo.

En 2011 el francés Antoine Buéno publicó "El pequeño libro azul: análisis crítico y político de la sociedad de los pitufos", donde recoge estas ideas.

Lo que sí es indudablemente cierto es que Peyo reflejó su visión de lo que le rodeaba en "Los pitufos", atreviéndose a retratar la división lingüística de su país en "Schtroumpf vert et Vert Schtroumpf", ya en 1972.










</doc>
<doc id="8883" url="https://es.wikipedia.org/wiki?curid=8883" title="Tejuino">
Tejuino

El tejuino o tesgüino del náhuatl "tecuin", latir el corazón es una cerveza de maíz, que consumen diversos grupos étnicos de México, principalmente en el norte del país (yaquis y pimas de Sonora, tarahumaras y tubares de Chihuahua y Durango), en el noroeste y oeste (huicholes de Jalisco y Nayarit) y en menor proporción, en el sur (zapotecas de Oaxaca).

Para los tarahumaras, al igual que para otros grupos étnicos como los huicholes, el tesgüino constituye la bebida preferida en sus eventos sociales, festividades religiosas y deportivas, y en las llamadas tesgüinada, que son reuniones en las que se toman decisiones políticas y económicas importantes para la comunidad, o en las que se realizan trabajos o labores difíciles que requieren de la participación comunitaria de los hombres. Así, el tesgüino es empleado como una forma de pago; bebida embriagante y vehículo para la administración de diversas plantas medicinales; además, mezclado con leche materna o diluido en agua, es consumido por lactantes y niños por lo que puede considerársele como un complemento importante en su dieta. 

El proceso de elaboración de la bebida varía de un grupo étnico a otro, aunque generalmente se hace con granos de maíz germinados en la oscuridad, que son molidos en metate y cocidos en suficiente agua durante varias horas hasta obtener un atole amarillento que, una vez frío, se cuela. El líquido recuperado se vacía en olla tesgüineras, se le adiciona el catalizador o fortificador y se deja fermentar de 1 a 10 días o más. Es importante señalar que las ollas tesgüineras nunca se lavan por lo que presentan, adheridos a sus paredes, residuos de fermentaciones previas. El tesgüino no se filtra ni pasteuriza por los que contiene los microorganismos vivos que producen la fermentación, las sustancias metabolizadas por ellos y los residuos de los vegetales utilizados.

Por otro lado, el tejuino es mestizo y es una bebida refrescante que también se encuentra en Guadalajara y en Mazatlán, entre otras ciudades. Aunque también se hace con maíz germinado, a este se le agrega piloncillo o azúcar.

Se bebe con limoncillo han, sal y chile piquín al gusto o sin agregarle nada, es de sabor agridulce y con un bajo grado de alcohol. Los vendedores ambulantes ofrecen tejuino en los pueblos y ciudades de la región, y es muy poco común encontrarlo en heladerías o neverias.

Hay dos tipos reconocidos: tejuino y tesgüino; el tejuino puede o no tener algo de fermentación no más fuerte que el tepache, el tesgüino es fermentado al máximo para que produzca licor. 

También existen dos tipos de tejuino: el tejuino blanco y el tejuino oscuro, elaborado con piloncillo).

En Nochistlan se llevan a cabo las festividades de San Sebastián. En esta fiesta es tradicional el tejuino, hecho a base de maíz, con la receta de los antiguos caxcanes que poblaban esta región. Cada noche del 17 al 20 de enero se reparte tejuino en cántaros a los asistentes a la fiesta, la cual se realiza en casa de los festejantes. Se hace con un tipo especial de maíz. Tiene un sabor amargo fuerte, es espeso y color café. No se modifica la receta original, se toma natural, es decir, no se le añaden hielos, sal o limón.

Hay quienes elaboran el tejuino fermentando masa (maíz en nixtamal, molido para formar una pasta con la que se hacen las tortillas).

Hay quienes agregan nieve de limón en sustitución del hielo y el limón. También hay quienes agregan hielo raspado (utilizado para los "raspados" o "nieve raspada") en vez de hielo en trozos.

Los Huicholes lo utilizan principalmente en sus festividades y actividades, llevando a cabo un proceso de fermentación (bebida alcohólica).



</doc>
<doc id="8884" url="https://es.wikipedia.org/wiki?curid=8884" title="Chocolate">
Chocolate

El chocolate (en náhuatl: "xocolātl") es el alimento que se obtiene mezclando azúcar con dos productos derivados de la manipulación de las semillas del cacao: la masa del cacao y la manteca de cacao. A partir de esta combinación básica, se elaboran los distintos tipos de chocolate, que dependen de la proporción entre estos elementos y de su mezcla, o no, con otros productos tales como leche y frutos secos.

El cacao ha sido cultivado por muchas culturas durante al menos tres milenios en Mesoamérica. La evidencia más temprana del uso del cacao pertenece a la cultura Mokaya de México, con vestigios de bebidas de chocolate que datan de 1900 a. C. Sin embargo, los olmecas de La Venta
en Tabasco fueron los primeros humanos en saborear, en forma de bebida, las habas de cacao molidas, las cuales mezclaban con agua y le añadían diversas especias, hierbas y guindillas, y también fueron quienes comenzaron a cultivar el cacao en México. De hecho, la mayoría de la gente mesoamericana hizo bebidas de chocolate, incluidos los mayas y aztecas.

El botánico Carlos Linneo lo llamó Theobroma, que significa alimento de los dioses, llegando hasta el punto de ser objeto de culto para mayas y aztecas.

El cultivo, el uso y la elaboración cultural del cacao eran tempranos y extensos en Mesoamérica, a los cuales el árbol del cacao es nativo. Cuando se poliniza, la semilla del árbol de cacao forma finalmente una especie de vaina u oreja de 10 a 35 cm de largo colgada de las ramas, dentro de la vaina hay de 30 a 40 granos almendrados de color pardo-rojo incrustados en una pulpa dulce y viscosa. Los frijoles son amargos debido a los alcaloides dentro de ellos, la pulpa dulce puede haber sido el primer elemento consumido por los seres humanos. La evidencia sugiere que puede haber sido fermentado y servido como una bebida alcohólica ya en 1400 aC.

Mientras que los investigadores no están de acuerdo en que la cultura mesoamericana domesticó por primera vez el árbol del cacao, el uso del frijol fermentado en una bebida parece haber surgido en México. Los científicos han podido confirmar su presencia en vasos de todo el mundo mediante la evaluación de la "huella química" detectable en las muestras de contenidos que quedan. Se ha encontrado un recipiente de cerámica con residuos de la preparación de bebidas de chocolate en sitios arqueológicos que datan del período Formativo Temprano (1900-900 aC). Por ejemplo, una embarcación de este tipo encontrada en un yacimiento arqueológico olmeca en la costa del Golfo de Veracruz (México) data la preparación del chocolate por parte de los pueblos pre-olmecas desde 1750 aC. En la costa del Pacífico de Chiapas, México, un sitio arqueológico de Mokayanan proporciona pruebas de las bebidas de cacao que datan incluso antes, a 1900 aC. 

Hacia el año 1500 a.C., los olmecas de La Venta en Tabasco, México, fueron los primeros humanos en saborear, en forma de bebida, las habas de cacao molidas, las cuales mezclaban con agua y le añadían diversas especias, hierbas y guindillas, y también fueron quienes comenzaron a cultivar el cacao en México. La evidencia más temprana de la domesticación de la planta de cacao data de la cultura Olmeca desde el período Preclásico. En Los olmecas lo usaban para rituales religiosos o como bebida medicinal, sin recetas para uso personal. Todavía queda poca evidencia de cómo se procesó la bebida.

En 2008 el Instituto Nacional de Antropología e Historia de México publicó estudios de las Universidades de Columbia, Arizona, Yale, Wisconsin y Kennesaw, en los que los análisis aplicados a una vasija encontrada en las excavaciones de Cerro Manatí, ubicado dentro del ejido del Macayal, en el municipio de Hidalgotitlán, Veracruz, concluyen que el consumo de cacao puede haberse dado 800 años antes de lo que se creía, en el período formativo (1900-900 a. C.). La vasija está datada mediante carbono 14 en 1750 a. C. y contiene restos de teobromina, componente marcador de la presencia de cacao en las vasijas es de alrededor del 1100 a. C. en el sitio arqueológico de Puerto Escondido (noreste de la actual Honduras), más recientes estudios (octubre de 2007) emprendidos por el equipo de arqueólogos dirigidos por John Henderson (Universidad Cornell) y Rosemary Joyce (Universidad de California en Berkeley) no solo ratifican que ya en el 1000 a. C. se consumía el chocolate en la región sino que muy probablemente en ésta el consumo se inició hacia "ca." el 1500 a. C. Se encontró en muestras de cerámica de Belice de entre el 600 al 400  a. C. Según Michael Coe, la bebida fue popularizada en Mesoamérica por los olmecas, pero la evidencia indica una popularidad más temprana.

En los primeros tiempos el consumo parece haber sido en forma de una especie de «cerveza»; es decir, una bebida basada en la fermentación más que de los granos del cacao de la pulpa del mismo. Tal «cerveza de chocolate», cuyos restos se hallan en las vasijas cerámicas de Puerto Escondido, tendría una importante función ritual y muy probablemente se utilizaba en las celebraciones de matrimonios. Bastante posteriormente, los olmecas, mayas y mexicas (entre otras civilizaciones mesoamericanas) comenzaron a consumir el chocolate derivado de la pasta de los granos aliñada o aderezada con chile. En forma semi líquida y líquida, el chocolate solía ser bebida preferida de las realezas, que lo consumían en vasos especiales (jícaras). Igualmente era considerado (con razón) un alimento tonificante o energizante, que se podía consumir mezclado en una masa de harina de maíz mezclada con chiles y miel.

Los mexicas premiaban a los mejores guerreros de la época otorgándoles el derecho de consumir libremente chocolate. También a los soldados se les otorgaban especies de bolitas hechas con polvo de cacao para que pudieran preparar su chocolate a lo largo de la guerra.
De acuerdo a la mitología maya, Kukulkán le dio el cacao a los mayas después de la creación de la humanidad, hecha de maíz (Ixim) por la diosa Xmucané (Bogin 1997, Coe 1996, Montejo 1999, Tedlock 1985). Los mayas celebraban un festival anual en abril, para honrar al dios del cacao, "Ek Chuah", un evento que incluía sacrificios de perros y otros animales con marcas pintadas de chocolate, ofrendas de cacao, plumas, incienso e intercambio de regalos.

Sin embargo, nuevos estudios arqueológicos revelan que el cacao es, en efecto, de origen sudamericano, y que se consumía en Ecuador desde hace unos 5500 años.

El cacao también era utilizado como moneda en las culturas prehispánicas ya que era uno de los productos que se utilizaban para pagar el tributo al "tlatohani".

Durante el siglo XVII, el chocolate era considerado tanto un medicamento como un alimento, y no una bebida de compañía y de placer.



Según la Revista Internacional de Acupuntura, el tipo de cacao más preciado es el criollo, oriundo de Venezuela. Sus granos, muy aromáticos, son menos ácidos y muy poco amargos. Los árboles de la variedad forastero son robustos, resistentes y, por su alto rendimiento representan la parte principal de la producción mundial. Sus granos poseen un sabor más intenso, muy pocos aromas secundarios y son amargos o ácidos.

Para muchos, la palabra "chocolate" es una adaptación de la palabra náhuatl "xocolātl", que hacía referencia a una «bebida espumosa hecha de cacao» y cuyo significado literal es "agua agria".
Se postulan por tanto dos etimologías para "xocolātl":


Tras el tratamiento al que se somete a las habas de cacao en las zonas de recolección, estas se envían a las distintas fábricas chocolateras. Al llegar, los granos se examinan y se clasifican.

Lo primero que se realiza es el lavado y tostado de las habas del cacao; el objetivo es aumentar el aroma y favorecer el desprendimiento de la piel de las semillas. Un sistema de cepillado posterior permite eliminar esas pieles y cualquier otra impureza o cuerpo extraño.

A continuación, se realiza la torrefacción de las habas del cacao ya tostadas, un proceso importantísimo para la calidad final del producto. En unas grandes "esfera"s giratorias, las habas se tuestan durante unos pocos minutos a entre 110 y 120 °C., eliminándose la humedad y la acidez, al tiempo que se favorece el desarrollo de los aromas. Cada tipo de grano que formará parte de una determinada mezcla de chocolate se tuesta por separado.

Después de su enfriamiento, las habas, cuyas cáscaras han comenzado a explotar por el efecto de la torrefacción, se llevan a una máquina de descascarillar y cribar, que abre los granos tostados y separa los pellejos, ligeros, de la parte comestible, más pesada.
Las cáscaras y hollejos se reciclan como compost para jardines, o para elaborar mantecas de baja calidad llamadas comercialmente Cocoa.

La cocoa tiene un perfume y un sabor relativamente similar al del chocolate en polvo, pero que carece de las características originales del chocolate hecho a base de cacao. Se consigue mayormente de manera industrializada y es de color marrón oscuro. La cascarilla sirve para hacer bebidas todavía típicas de algunos lugares, solo se pone a macerar un puñado de cascarilla unos minutos, luego esta se hierve con leche y se bebe caliente. Pero esta bebida resultante carece de nutrientes y en algunas ocasiones la cascarilla suele contener cobre en cantidades altas. Por ser muy amarga, la Cocoa es un recurso con gran rendimiento económico sobre todo para las industrias que modifican el sabor de la Cocoa con grandes márgenes de azúcar, que por ser tan amarga necesita.

El siguiente paso es la mezcla. Determinadas cantidades de diferentes variedades de granos son pesadas e introducidas en un depósito cilíndrico, previamente a su paso a las máquinas de molienda. La mezcla de diferentes granos para hacer cacao en polvo es menos exigente que la del chocolate.

A continuación, se muelen las habas del cacao. Las habas trituradas pasan a través de una batería de molinos y se someten a un batido a una temperatura constante de 60-80º; la duración de este tratamiento puede ir de las 18 a las 72 horas. La duración influye en la textura del chocolate resultante: a menos batido, mayor aspereza. Por efecto de la trituración, el tejido celular de las habas, que contiene de un 50 a un 60 % de manteca de cacao, permite la liberación en parte de esta grasa, que luego se licúa por efecto del calor generado por el frotamiento. El resultado es una pasta fluida pero densa, la "pasta de cacao": una suspensión de sustancias con cacao en manteca de cacao.

Para su utilización en los diferentes productos, esta pasta se homogeneiza y se calienta a 100º, para ser luego propulsada en prensas hidráulicas. Se extrae así la mayor cantidad posible de "manteca de cacao", que se filtra y se compacta en grandes bloques. La pasta de cacao, con un porcentaje de grasa reducido entre el 8 y el 22 %, se presenta en forma de pan u hogaza. Esta parte sólida es durísima, pues se solidifica a 600 atmósferas.

El característico crujido y el delicado brillo del buen chocolate es debido a la estructura cristalina de la "manteca de cacao".

La "manteca de cacao", aparte de su utilización en la elaboración de chocolates, se usa en jabones y cosmética, por tener un punto de fusión ligeramente inferior a la temperatura corporal, lo que la convierte en una base perfecta para lápices de labios y otras cremas.

El sabor final del chocolate depende de la selección y mezcla de diversos tipos de granos de cacao. A estos tipos de granos de cacao pueden subdividirse entre las "variedades fuertes" y las "suaves", que se suelen mezclar proporcionalmente:



La elaboración del chocolate pasa por su última fase con la cuidadosa mezcla de la pasta y la manteca de cacao con azúcar, refinando la composición resultante por medio de trituradoras-refinadoras que producen una pasta muy delgada. A continuación, se efectúa la operación más importante, el "conchado" (o "concheado"), que le dará al chocolate toda su finura y su untuosidad.

El "conchado" es un amasado suplementario en artesas que, originalmente, tenían forma de concha. La pasta es batida y estirada en la artesa por unos rodillos, con un lento movimiento de vaivén, durante un periodo de tiempo y a una temperaturas que varían según el producto que se quiera obtener (en todo caso, unas horas y, a menudo, varios días). Todas estas operaciones se realizan a una temperatura superior al punto de fusión de la manteca de cacao que, por lo tanto, se mantiene líquida.

El último paso es el "templado", que consiste en fundir completamente el chocolate a 50 °C para que se rompan las estructuras cristalinas de la manteca de cacao, enfriarlo a 30º para devolverle la estructura, y, finalmente, aumentar ligeramente la temperatura para que los cristales se agrupen de nuevo en pequeñas cadenas.

Normalmente, el chocolate lleva añadida vainilla (o algún derivado como la vainillina) como aromatizante, y lecitina de soja como emulsionante y estabilizante para mejorar la textura y mantener las cualidades del chocolate; en total, ambos productos no superan el 1 % del chocolate.

Los distintos tipos de chocolate se elaboran modificando las proporciones entre sus componentes y añadiendo otros productos a la composición básica de pasta, manteca y azúcar. Su presentación puede ser en forma de "tableta" o en "polvo":

El chocolate negro (llamado también "chocolate fondant"; "chocolate amargo"; "chocolate bitter"; "chocolate amer"; "chocolate duro") es el chocolate propiamente dicho, pues es el resultado de la mezcla de la pasta y manteca del cacao con azúcar, sin el añadido de ningún otro producto (exceptuando el aromatizante y el emulsionante más arriba citados). Las proporciones con que se elabora dependen del fabricante. No obstante, se entiende que un chocolate negro "debe" presentar una proporción de pasta de cacao superior, aproximadamente, al 50 % del producto, pues es a partir de esa cantidad cuando el amargor del cacao empieza a ser perceptible. En cualquier caso, existen en el mercado tabletas de chocolate negro con distintas proporciones de cacao, llegando incluso hasta el 99 %.

Se considera un alimento afrodisiaco porque gracias a su contenido de magnesio ayuda a combatir las contracciones musculares y dolores premenstruales. En lugares como Oaxaca, México se añade almendra a su preparación y es elaborado en piedra, en la actualidad existen grupos de artesanos como los "chocolateconalas" que aún mantienen esta tradición.

En algunos casos, se suele sustituir el azúcar por algún edulcorante (principalmente sucralosa). En este caso se utiliza para regímenes dietéticos.

El chocolate de cobertura es el chocolate que utilizan los chocolateros y los pasteleros como materia prima. Puede ser "negro" o "con leche", pero en todo caso se trata de un chocolate con una proporción de manteca de cacao de alrededor del 30 %, lo que supone el doble que en los otros tipos de chocolate. La cobertura se usa para conseguir un alto brillo al templar el chocolate y porque se funde fácilmente y es muy moldeable.

El chocolate a la taza es el chocolate negro (normalmente, con una proporción de cacao inferior al 50 %), al que se le ha añadido una pequeña cantidad de fécula (normalmente, harina de maíz) para que a la hora de cocerlo aumente su espesor. Suele disolverse en leche. Hoy en día, es posible encontrar también este chocolate en los comercios en forma ya líquida.

El chocolate con leche es el derivado del cacao más popular. Se trata, básicamente, de un dulce, por lo que la proporción de pasta de cacao suele estar por debajo del 40 %. No obstante, buena parte de las más importantes marcas de chocolate producen tabletas de chocolate con leche con proporciones de cacao inusuales, por encima incluso del 50 %, dirigidas tanto al mercado de los "gourmets" como al negocio de la pastelería. El chocolate con leche, como su nombre indica, lleva leche añadida, en polvo o condensada.

En el caso del chocolate blanco, estrictamente, no se trata de chocolate como tal, pues carece en su composición de la pasta de cacao, que es la materia que aporta las propiedades del cacao. Se elabora con manteca de cacao (por lo menos, el 20 %), leche (en polvo o condensada) y azúcar. Es un producto extremadamente energético y dulce (no posee regusto amargo). Visualmente muy atractivo, es un elemento decorativo muy usado en la repostería.

El chocolate relleno, como indica su nombre, es una cubierta de chocolate (en cualquiera de sus variantes y con un peso superior al 25 % del total) que recubre frutos secos (avellanas, almendras...), licores, frutas, etc., así como galletas tipo "wafer".

Degustar el chocolate consiste en experimentar, analizar y apreciar sus características organolépticas con los cinco sentidos. Es importante recordar que la temperatura y humedad del ambiente pueden repercutir en la degustación.

Un buen chocolate tendrá un color marrón muy oscuro y brillante, uniforme, sin ningún tipo de mácula, burbujas o hendiduras.
El tacto debe ser firme, nunca pegajoso, y, al partirlo, debe ofrecer una resistencia mínima; si al partirlo forma astillas, está demasiado seco; y si es difícil de partir está muy ceroso. En la boca, la disolución será fácil, continuada y completa, esto es, sin rastro alguno de granulosidades.
Al partirlo, el sonido debe ser seco pero quebradizo.
Se tendrán en cuenta la olfacción directa y la indirecta (por vía retronasal).
El sabor debe ser básicamente amargo con un punto de acidez y de dulzor, y después puede haber toques de piña, plátano, vainilla, canela, azafrán, etc.
Aunque para disfrutar de un verdadero chocolate es necesario manipular en esencia semilla de cacao con márgenes equilibrados de azúcar, esto no sucede con el chocolate que se conoce comúnmente, ya que todo el chocolate se industrializa a modo de separar manteca y pasta de cacao. Con eso se modifica el sabor y también la calidad. Un buen chocolate es aquel que en esencia de ingredientes está hecho a base de cacao sin modificar sus sustancias naturales. El chocolate artesanal es un buen ejemplo.

El chocolate en polvo tiene por objeto su disolución en leche. Se elabora con una proporción de cacao que oscila entre un 25 y un 32 %, y se presenta más o menos desgrasado.

Los bombones son porciones pequeñas (apropiadas para ser ingeridas en un solo bocado) de una mezcla sólida de chocolate (negro, blanco o con leche) o de una cubierta de chocolate (negro, blanco o con leche) rellena de distintos elementos.

Constituyen, al lado del chocolate en tableta y en polvo, la forma más importante y extendida de presentar comercialmente el chocolate. A diferencia de las otras presentaciones, los bombones están asociados a comportamientos de gratitud, regalo o reconocimiento en las relaciones sociales. Su producción está muy cuidada por prácticamente la totalidad de las industrias chocolateras.


"Para la composición nutricional, véanse al final los enlaces externos sugeridos"

Los dos principales ingredientes del chocolate son calóricos: "la grasa" y "el azúcar".









Es rico en polifenoles-flavonoides, como la epicatequina, potentes antioxidantes que protegen al sistema circulatorio, en especial al corazón, el "chocolate negro" es particularmente rico en polifenoles que entre otros efectos benéficos previene o reduce los efectos del SFC y encefalomielitis miálgica.

Posee un elevado dosaje de promotor de serotonina gracias al triptofano, un aminoácido muy importante en nuestro organismo regulador de neurotransmisores y un buen dosaje de anandamida, ambos psicotrópicos naturalmente existentes en el ser humano y obtenidos en dosis suficientes (mínimas) al consumir chocolate, facilitan una sensación de placer ("sin" caer en la irrealidad o la estupefacción), por su parte, tal sensación de placer refuerza al sistema inmune, también se ha observado que la ingesta de chocolate compensa las inversiones de péptidos que suelen ocurrir en el sistema nervioso central de los seres humanos durante su adolescencia cuando se enamoran.

Las principales contraindicaciones conocidas (marzo de 2007) al consumo de chocolate son las siguientes: exceso de calorías (esto si se realiza una dieta excesiva de chocolate, más aún si éste va mezclado con grasas hidrogenadas, azúcares o glúcidos añadidos), cuando el consumo de productos industrializados en base al chocolate es frecuente también es común que se substituya los glúcidos por sacarinas o por ciclamatos los cuales pueden asimismo conllevar riesgos para la salud.

El chocolate "negro" es el que se considera actualmente más benéfico ya que el chocolate blanco es pobre en cacao pero con muchas grasas y glúcidos. Se aconsejan hasta 100 g de chocolate negro por día, esto disminuye el riesgo de accidentes vasculares y de hipertensión, aunque estudios (Druk taubel, Diane Becker, Norma Hollemberg) publicados a inicios de julio de 2007 señalan que el solo consumo de una pequeña barra de chocolate negro por día ya reduce la presión sistólica en un 8 % o 9 %. No obstante, un artículo publicado en diciembre de 2007 en "The Lancet" señala que muchos fabricantes de chocolate quitan los flavonoides por su gusto amargo, añadiendo por contrapartida endulzantes y grasas.

Aunque el chocolate es comúnmente ingerido por placer, existen efectos beneficiosos sobre la salud asociados a su consumo. El cacao o el chocolate negro benefician al sistema circulatorio. Otros efectos beneficiosos sugeridos incluyen efecto anticanceroso, estimulador cerebral, antitusígeno y antidiarreico. Un efecto afrodisíaco aún debe probarse.

Por otro lado el consumo incontrolado de una gran cantidad de cualquier alimento rico en calorías, como el chocolate, incrementa el riesgo de obesidad de no haber un correspondiente aumento en la actividad física. El chocolate crudo es rico en manteca de cacao, un componente graso que es removido durante el refinamiento del chocolate para luego ser añadido nuevamente en proporciones variables durante el proceso de fabricación. Los fabricantes pueden añadir otras grasas, azúcares y leche, todo lo cual incrementa el contenido calórico del chocolate.

Hay riesgo de leve intoxicación por plomo por algunos tipos de chocolate.
El chocolate es tóxico para muchos animales por su insuficiente capacidad para metabolizar la teobromina.

Un estudio difundido por la BBC indicó que el derretir chocolate en la boca produjo un aumento en actividad cerebral y ritmo cardíaco que fue más intenso que el asociado con el beso apasionado y además duró cuatro veces más.

Es importante señalar que el chocolate puede producir reacciones alérgicas en algunas personas, las cuales deben evitar su consumo.

Estudios recientes sugirieron que el cacao o el chocolate negro puede poseer ciertos efectos beneficiosos sobre la salud humana. Esto es principalmente causado por una particular sustancia presente en el cacao llamada epicatechin. El cacao posee una acción significativa como antioxidante, protegiendo contra la oxidación LDL, quizás más que otros alimentos y bebidas ricos en antioxidantes polifenoles. Algunos estudios también observaron una moderada reducción en la presión sanguínea después de ingerir chocolate negro diariamente. Ha habido una dieta llamada "Dieta del chocolate" que enfatiza el comer chocolate y polvo de cacao en cápsulas. Sin embargo, el consumo de chocolate de leche o chocolate blanco, o leche entera con chocolate negro parece negar ampliamente el beneficio en la salud. El polvo de cacao procesado (también llamado chocolate holandés), procesado con álcali, reduce en gran medida la capacidad antioxidante comparado con el polvo de chocolate en crudo. El proceso del cacao con álcali destruye la mayoría de los flavonoides.

Un tercio de la grasa en el chocolate viene en forma de una grasa saturada llamada ácido esteárico y una grasa monoinsaturada llamada ácido oleico. De todas formas, a diferencia de otras grasas saturadas, el ácido esteárico no eleva los niveles de colesterol LDL en el torrente sanguíneo. Consumir niveles relativamente altos de chocolate negro y cacao no parece elevar los niveles séricos de colesterol LDL y algunos estudios indican que podría reducirlos. De hecho, cantidades pequeñas pero regulares de chocolate negro bajan la posibilidad de un ataque cardíaco, un resultado del desequilibrio de colesterol según la hipótesis lipídica.

La creencia popular romántica comúnmente identifica el chocolate como un afrodisíaco. Las propiedades afrodisíacas del chocolate están más frecuentemente asociadas con el simple y sensual placer de su consumo. Aunque no hay prueba de que el chocolate es un afrodisíaco, un regalo de chocolates es un ritual de cortesía familiar.

Estudios sugieren que un tipo especial de cacao podría ser nootrópico y retrasar el declinamiento de la función cerebral que ocurre durante el envejecimiento.

Otra investigación indica que el chocolate puede ser efectivo para prevenir la tos persistente. Se halló que la teobromina fue casi un tercio más efectiva que la codeína, la medicación líder para la tos.

Los flavonoides pueden inhibir el desarrollo de diarrea sugiriendo efectos antidiarréicos del cacao.

La mayor preocupación que tienen los nutricionistas es que aunque el comer chocolate negro puede no afectar el colesterol sérico, la presión arterial o la oxidación LDL, no se sabe aún si afecta favorablemente ciertos marcadores biológicos de la patología cardiovascular. Además, la cantidad necesaria para tener este efecto proveería relativamente grandes cantidades de calorías que de no ser usadas propiciarían un aumento de peso. La obesidad es un factor de riesgo significativo para muchas enfermedades incluyendo las cardiovasculares.

Hay una creencia popular que el chocolate puede causar acné. Esta creencia no está basada en estudios científicos. Varios estudios apuntan no al chocolate sino a la elevada naturaleza glicémica de ciertos alimentos como azúcar, jarabe de maíz y otros carbohidratos simples como causa de acné. El chocolate en sí tiene un bajo índice glucémico. Otras causas dietarias de acné no pueden ser aún excluidas pero se requiere de una investigación más rigurosa.

El chocolate tiene una de las concentraciones más altas de plomo entre los productos que componen la típica dieta occidental, con el potencial de causar intoxicación leve. Estudios recientes han mostrado que si bien los granos absorben poco el plomo, este tiende a unirse a la corteza del cacao y la contaminación puede ocurrir durante el proceso de fabricación. Una publicación reciente encontró cantidades significantes de plomo en el chocolate. En un estudio del USDA de 2004, los niveles medios de plomo en muestras testeadas variaron desde 0.0010 a 0.0965 µg de plomo por gramo de chocolate, pero otro estudio de un equipo de investigación suizo de 2002 encontró que algunos chocolates contenían hasta 0.769 µg por gramo, cercano a los límites internacionales (voluntarios) estándares para plomo en polvo de cacao o granos, que es 1 µg de plomo por gramo. En el 2006, la FDA (administración de drogas y alimentos, la autoridad estadounidense que regula estos productos) bajó en un quinto la cantidad de plomo permitido en caramelos, pero el cumplimiento es solo voluntario. Mientras que estudios muestran que el plomo consumido con el chocolate puede no ser absorbido por el cuerpo humano, no hay ningún umbral para el efecto del plomo en el funcionamiento cerebral de los niños e incluso pequeñas cantidades de plomo pueden causar déficits en el desarrollo neurológico, incluyendo un IQ desigual.

En cantidades suficientes, la teobromina encontrada en el chocolate es tóxica para animales como caballos, perros, loros, roedores pequeños y gatos debido a que estos son incapaces de metabolizar esta sustancia química con efectividad. Si son alimentados con chocolate, la teobromina permanecerá en su torrente sanguíneo hasta 20 horas y estos animales pueden experimentar episodios epilépticos, ataques cardíacos, hemorragias internas y eventualmente la muerte. El tratamiento médico efectuado por un veterinario incluye la inducción del vómito dentro de las dos horas posteriores a la ingestión y la administración de benzodiacepinas o barbitúricos para las convulsiones, antiarrítmicos para las arritmias cardíacas y la diuresis de fluidos.

Un perro típico de 20 kg (40 lb) normalmente experimentará un gran dolor intestinal después de comer menos que 240 gramos de chocolate negro pero no necesariamente presentará bradicardia o taquicardia a menos que coma por lo menos medio kilo (1,1 lb) de chocolate de leche. El chocolate negro tiene 2 o 5 veces más teobromina y por lo tanto es más peligroso para los perros. De acuerdo al Manual Veterinario Merck, aproximadamente 1.3 gramos de chocolate de panadero por kilogramo de peso corporal canino es suficiente para causar síntomas de toxicidad. Por ejemplo, una barra de chocolate de panadero típico de 25 gramos serían suficientes para causar síntomas en un perro de aproximadamente 20 kg.
Por supuesto que el chocolate de panadero raramente se consume directamente debido a su gusto desagradable pero otros chocolates negros pueden tener una toxicidad extrapolada basada en este caso. Como los perros gustan del sabor de productos del chocolate tanto como los humanos y son capaces de encontrar y comer cantidades más grandes que las típicas que se sirven a humanos, deberían ser alejados de su alcance. Existen informes sobre que la moltura hecha de la corteza del grano de cacao es peligrosa para perros y el ganado.

El chocolate contiene una variedad de sustancias, algunas de las cuales tienen un efecto en la química orgánica. Estas incluyen:


El chocolate es un leve estimulante para humanos principalmente debido a la presencia de teobromina.







</doc>
<doc id="8886" url="https://es.wikipedia.org/wiki?curid=8886" title="Exón">
Exón

El exón es la región de un gen que no es separada durante el proceso de corte y empalme y, por tanto, se mantienen en el ARN mensajero maduro. En los genes que codifican una proteína, son los exones los que contienen la información para producir la proteína codificada en el gen. En estos casos, cada exón codifica una porción específica de la proteína completa, de manera que el conjunto de exones forma la región codificante del gen. En eucariotas los exones de un gen están separados por regiones largas de ADN (llamadas intrones) que no codifican. 
El término "exón" fue acotado por el bioquímico norteamericano Walter Gilbert en 1978:

Esta definición fue realizada originalmente para la transcripción del código proteínico, pero luego fue trasladada. El término más tarde llegó a incluir secuencias eliminadas de rRNA y tRNA, y también fue utilizado más tarde para las moléculas de ARN procedentes de diferentes partes del genoma que luego son ligadas por trans-splicing.

Si bien se consideró en un primer momento que los exones (en comparación con los intrones) son los que llevan la "información" dentro de un gen, se ha demostrado que no siempre es así. Así por ejemplo existen pseudogenes que poseen la estructura de un gen activo (incluido sus exones) y sin embargo no se transcriben.

A la derecha se puede observar un diagrama mostrando como los exones y los intrones se localizan de manera intercalada en un gen. A cada extremo de un gen existe una región no traducida del mismo (UTR: UnTraslated Region). La transcripción de un gen a ADN, genera un RNA mensajero inmaduro. Este ARN mensajero lleva a cabo el proceso de splicing, en el que se escinden los intrones y las regiones no traducidas. Una vez que el ARN mensajero ha madurado, puede ser traducido a una proteína.

Es importante mencionar que un mismo gen puede producir diferentes proteínas gracias a un splicing alternativo. Mediante este proceso, algunos exones pueden ser eliminados junto con los intrones que los flanquean. De esa manera se crean diferentes versiones de ARN mensajeros que son traducidas a su vez en diferentes proteínas. Cabe notar que este splicing alternativo, no es de ninguna manera un proceso aleatorio sino que ha evolucionado de manera que las diferentes proteínas así creadas sean todas funcionales. 



</doc>
<doc id="8894" url="https://es.wikipedia.org/wiki?curid=8894" title="Ciclopentanoperhidrofenantreno">
Ciclopentanoperhidrofenantreno

El ciclopentanoperhidrofenantreno (también llamado ciclopentanoperhidrofenantreno, esterano o gonano) es un hidrocarburo policíclico que puede considerarse un producto de la saturación del fenantreno asociado a un anillo de ciclopentano. Posee 17 átomos de carbono. De esta base estructural derivan los esteroides (el colesterol y sus derivados, como la progesterona, la aldosterona, el cortisol y la testosterona son ejemplos de compuestos que contienen un núcleo de ciclopentanoperhidrofenantreno) derivados del fenantreno.

Las sustancias derivadas de este núcleo muestran grupos metilo -CH, en las posiciones 10 y 13 para integrar los carbonos 18 y 19; generalmente existe una cadena alifática en el carbono 17, la longitud de dicha cadena y la presencia de metilos en el carbono 10 y 13 determina las diferentes estructuras de estas sustancias.

Es el grupo más nuevo y en éste hay cuatro progestágenos importantes: levonorgestrel, desogestrel, gestodeno y norgestimato.

Es un progestágeno usado en las formulaciones anticonceptivas que tiene una actividad biológica aproximadamente 80 veces más potente que la progesterona endógena. Tiene un efecto androgénico porque compite con la testosterona para fijarse a la proteína transportadora, y además tiene actividad antiestrogénica. Sin embargo, estos preparados tienen una influencia desfavorable sobre la relación entre el LDL-colesterol y el HDL-colesterol por su efecto androgénico y cuando se administran en combinación con 30 µg de etinilestradiol reducen la fracción LDL y elevan la HDL. El avance más importante que se ha producido en los últimos años ha sido la obtención de tres gestágenos derivados del levonorgestrel y que son los llamados “progestágenos de tercera generación”.

Disponible desde 1982, fue el primero de los progestágenos más selectivos que llegó a estar disponible para uso en los AO. Es 2,8 veces más potente que el levonorgestrel, tiene pocos efectos androgénicos (prácticamente no compite con la testosterona para ligarse a la proteína transportadora) y no tiene influencia sobre la actividad estrogénica, lo que permite emplear bajas dosis de estrógenos en los preparados anticonceptivos y reducir la aparición de efectos secundarios (Dibbelt et al, 1991). Además, limita la penetración de espermatozoides a través del moco cervical y es un poderoso inhibidor de la ovulación.

Fue el segundo progestágeno de tercera generación disponible comercialmente. No necesita del metabolismo hepático para ser biológicamente activo. Es 1,5 veces más potente que el levonorgestrel. No interfiere con el metabolismo de la testosterona y evita, en gran medida, los efectos androgénicos secundarios de los AO que contienen norgestrel y levonorgestrel. A diferencia del desogestrel tiene un importante efecto antiestrogénico lo cual limita la dosis utilizada.

Es el más nuevo de todos. Los ensayos biológicos han demostrado que tiene una alta selectividad progestacional y su afinidad para unirse con los receptores androgénicos es menor que la del gestodeno y el levonorgestrel, lo que le confiere una elevada actividad progestacional con muy baja actividad androgénica (Huber, 1991).

Con los conocimientos actuales, el perfil de un AO no se puede representar simplemente por la suma de ambos componentes; sino por una compleja interacción entre los dos. Esta interacción se puede manifestar con un potente sinergismo entre el estrógeno y el progestágeno o, por el contrario, con un efecto antagónico entre ambos.


</doc>
<doc id="8898" url="https://es.wikipedia.org/wiki?curid=8898" title="Ley del péndulo">
Ley del péndulo

Consideremos un péndulo cuyo brazo mide l, en el campo gravitacional de intensidad g (usualmente: 9,81 m.s), y sujeto a pequeñas oscilaciones.
El período T de oscilación del péndulo es dado por la fórmula:

formula_1

Sea θ el ángulo en radianes que hace el brazo con la vertical y m la masa del péndulo, al extremo de su brazo, que se mueve con la velocidad : v = l·θ'.

La energía cinética del péndulo es: 
Se puede tomar su energía potencial igual a: 
Este sistema no pierde energía, por la suma de energía cinética y potencia es constante 

Al derivar se obtiene:

Se puede simplificar por m·l (no nulos) y por θ' (no idénticamente nulo), lo que da:
Como se supone que θ es siempre pequeño, se puede remplazar sen θ por θ cometiendo un error del orden de θ (porque sin θ = θ + O(θ)).

Entonces equivale a:
Un movimiento oscilatorio sigue la ley 

lo que implica que 

donde formula_2es la velocidad angular de la ley y formula_3 el ángulo máximo.

Identificando y se obtiene 
formula_4, es decir formula_5.

Concluimos recordando que formula_6.


</doc>
<doc id="8899" url="https://es.wikipedia.org/wiki?curid=8899" title="Período">
Período

Período o periodo (del latín "periŏdus") se utiliza regularmente para designar el intervalo de tiempo necesario para completar un ciclo repetitivo, o simplemente el espacio tiempo que dura algo.<br>
Puede referirse a:






</doc>
<doc id="8900" url="https://es.wikipedia.org/wiki?curid=8900" title="Hipótesis de la señal">
Hipótesis de la señal

La hipótesis de la señal relata los mecanismos como el transporte biológico de las proteínas en las células a los orgánulos apropiados por inserción en las membranas o segregadas fuera de la célula. Las proteínas recién sintetizadas están dotadas de una señal intrínseca que debe ser descifrada en los lugares de destino. Günter Blobel sugirió, en 1975, que esa señal determina su capacidad para dirigirse hacia la membrana del retículo endoplásmico y atravesarla, y a esta teoría la llamó "hipótesis de la señal".

La señal consiste en un péptido hidrofóbico, el péptido señal, constituido por unos 20 aminoácidos en orden particular, que son los primeros que aparecen cuando se está sintetizando la cadena polipeptídica. Este péptido parece que señala a la célula que la proteína a la cual está unido debe ser transportada, pues cuando el péptido se une a una proteína citoplasmática esto provoca que la proteína sea secretada. Parece ser que la hidrofobicidad del péptido señal es uno de los factores más importantes para que se produzca el transporte de la proteína. Los principios moleculares descritos por Blobel que constituyen la base de este proceso son universales. los peptidos señal tienen una determinada carga, ya que poseen lys o arg en sus secuencias codificantes, el lado positivo, marcado por estos aminoácidos queda siempre mirando el citosol. Además la translocación estará determinada por esta carga y de ella dependerá también el coste para la célula llevar a cabo la translocación, debido a que la carga debe estar siempre mirando el lado citosólico muchas veces la cadena proteica es más larga por el lado en que esta la carga negativa del péptido señal lo que implicaría un mayor trabajo desplazar esta larga cadena por el translocón (proteína de membrana que cumple la función de translocar la proteína.


</doc>
<doc id="8901" url="https://es.wikipedia.org/wiki?curid=8901" title="Günter Blobel">
Günter Blobel

Günter Blobel (Waltersdorf, Silesia, Alemania; 21 de mayo de 1936-Nueva York, 18 de febrero de 2018) fue un biólogo germano-estadounidense.

En 1987 se nacionalizó estadounidense.

En 1967 se graduó en Oncología, realizó toda su carrera en la Universidad Rockefeller de Nueva York, de la que fue profesor y en cuyo laboratorio de biología celular (en el Instituto Médico Howard Hughes) trabajó.

Ganó el Premio Nobel de Fisiología o Medicina en 1999 por sus trabajos realizados en la década de 1970, al descubrir que las proteínas tienen señales intrínsecas que gobiernan su transporte y situación en la célula. Estas investigaciones abrieron la vía para crear fármacos que se dirigen al lugar del organismo donde deben actuar. Donó la totalidad del premio a la ciudad de Dresde. "Asistí muy de cerca a la destrucción de Desde, y ninguna otra cosa me ha impresionad más", comentó en cuando realizó la donación.






</doc>
<doc id="8902" url="https://es.wikipedia.org/wiki?curid=8902" title="Numeral (lingüística)">
Numeral (lingüística)

Un numeral es un nombre propio para un número, las lenguas naturales en tanto que lenguajes que permiten hacer afirmaciones sobre realidades físicas, disponen de subsistemas lingüístico-cognitivos capaces de nombrar números y contar.

Los sistemas de numeración de las lenguas naturales se basan en la cuenta de dedos. Por eso la práctica totalidad de los sistemas de numeración de las lenguas de la tierra usa sistemas de numeración basados en la base 10 o la base 20.

Las mayoría de las lenguas indoeuropeas utilizan un sistema de numeración decimal, lo que significa que los nombres de los números se agrupan en series de diez, y que existen raíces para los números del uno al nueve, el diez, el cien, el mil y los demás nombres son derivados de las raíces para los numerales citados. Otras familias de lenguas emplean también el sistema decimal y existen familias de lenguas donde se usan sistemas vigesimales (vasco, lenguas mayas, lenguas utoaztecas, etc.). Algunas lenguas tienen subsistemas de base cinco, dentro de un sistema decimal o vigesimal. 

Sin embargo, no se conocen sistemas de cuenta amplios no basados en la base 10 o la base 20. Esta restricción parece relacionada con que la idea orignal de contar estaba asociada a los dedos de las manos o el conjunto de dedos de manos y pies. Los sistemas no basados en base 10 o 20 son escasos y aparecen en lenguas australianas de forma generalizada y de forma marginal en algunas lenguas americanas, como el Waimirí y el Arará (dos lenguas caribe), el Resiguaro (una lengua arahwak), el Harákmbet y el Andoque (lenguas aisladas), que suelen usar sistemas cuasi-binarios.
Son palabras que expresan orden o cantidad de forma precisa.

Algunas austronesias y melanesias, entre ellas el māorí, algunas lenguas de Sulawesi y algunas de Papúa Nueva Guinea, usan para contar sistemas basados en el 'cuatro'. En estas lenguas el término "asu" o "aso" (derivado del javanés "asu", 'perro'), se usa para 'cuatro' dado que los perros en estas culturas son el cuadrúpedo más abundante. Se ha propuesto que este sistema podría haber surgido en el contexto de granjeros que intercambiaban animales en un mercado, así cincuenta "asu" representaría un conjunto de 200 cosas, si a esa cantidad se le substraen 30 "asu" (120) se llegaría rápidamente a inferir que sólo quedan 20 "asu" (80), y la generalización de la idea de contar las cosas como formando "asu" completos habría dado origen a este curioso sistema de contaje. Nótese que este sistema podría estar cognitivamente relacionado con el uso de docenas como sistema de contaje extendido debido a que la aritmética de la división es simple en docenas.

Los sistemas quinarios que usan la base 5 también están testimoniados. Obviamente dichos sistemas al igual que los sistemas decimales y vigesimales derivan del contaje de dedos (una mano tiene cinco dedos). Un ejemplo de este tipo de sistema se encuentra en la lengua api, una lengua de Vanuatu, donde 'cinco' y 'mano' se dicen simplemente "luna" 'cinco, mano', 'diez' se dice "lua-luna" 'dos-cinco', 'quince' se deice "tolu-luna" 'tres-cinco', etc. El número 'once' se llama "lua-luna tai" 'dos-cinco uno' y 'diecisiete' "tolu-luna lua" 'tres-cinco dos'. Un sistema básicamente idéntico se encuentra en emberá-catio (familia chocó).

Aunque los sistemas puramente quinarios no son frecuentes, si es muy frecuente el uso de la base cinco como "base auxiliar" o "sub-base" empelada para algunos números, así en muchas lenguas del mundo es frecuente que el 6 se llame 'cinco/mano y uno', 7 'cinco/mano y dos ', etc. Por ejemplo el náhuatl tiene una base vigesimal (base, 20) pero emplea 5 como sub-base: 6 "chikasē", 7 "chikōme", 8 "chikēi" y 9 "chiknāwi".

El kanum es un ejemplo exótico de lengua con un sistema de contaje en base 6. Las lenguas sko, sin embargo, usan una base 24, en donde se usa sub-base 6.

El sistema octal usa el número 8 como base. Dicho sistema está testimoniado en el Idioma yuki de California y en las lenguas pame de México central. Esto se debe a que ambos grupos humanos usan los huecos entre los dedos de las manos para contar más que los dedos mismos.

Las lenguas sko usan un sistema de cuenta en base 24, donde el 6 es empleado como sub-base.

El ngiti usa base 32.

El ekari usa una base 60 para contar. En Sumeria parece haberse empleado la base 60 para ciertos usos, aunque siempre con 10 como sub-base, quizá un sistema mixto entre sistemas decimales y duodecimal. El empleo de este curioso sistema en Sumeria está detrás del uso moderno de los grados sexagesimales de la medida de los ángulos en grados, minutos y segundos. Uso del que deriva también la medida del tiempo en minutos y segundos, así como la división de la circunferencia en 360º.

Se ha publicado que el supyire tiene un sistema de cuenta en base 80, en esta lengua se cuenta en veintenas (usando 5 y 10 como sub-bases) hasta 80 y entonces en ochentenas hasta 400, y a partir de ahí de 400 en 400.
799 [i.e. 400 + (4 x 80) + (3 x 20) + {10 + (5 + 4)}]’

Un número importante de lenguas de Melanesa usan (o usaban) sistemas de cuenta basados en las partes del cuerpo en lugar de sobre una base numérica. Este sistema no tiene palabras exclusivas para números, sino que más bien los nombres de ciertas partes del cuerpo se emplean para contar. Por ejemplo entre 1 y 4 se emplean los dedos, 5 se llama 'pulgar', 6 'muñeca [de la mano]', 7 'codo', 8 'hombro', etc. Así a través del cuerpo y luego bajando por el otro brazo, así el meñique opuesto representa el 17 (Islas Torres) o 23 (lenguas eleman). Para números más allá de estos valores, se usa el torso, las piernas y los dedos de los pies, o uno puede contar hacia atrás hasta el otro brazo y de nuevo al primero.

También es un tema bien estudiado, qué principios aritméticos usan los sistemas de numeración de las lenguas del mundo. Por ejemplo, la suma es universal en todas las lenguas ("dieciocho" = "diez" y "ocho" = 10 + 8), para expresar un número que puede expresarse mediante un cierto número de veces la base más un número inferior a la base. La multiplicación es también muy frecuente para expresar ciertos números superiores a la base ("doscientos" = 2 x 100). En algunas lenguas aparece implícitamente la substracción para formar números (latín "duōdēvigintī" '18' = 2 antes de 20 = 20 - 2). En cambio, la división es totalmente marginal en la formación de numerales y generalmente se restringe a fracciones como 1/2.

En muchas lenguas además de los numerales cardinales o numerales comunes usados para contar existen otros tipos adicionales de numerales: ordinales, partitivos, distributivos, multiplicativo, etc. El latín es un ejemplo de lenguas con todos estos tipos de numerales:

En lenguas flexivas estos numerales ordinales son formas derivadas de los lexemas que se usan para formar los numerales comunes.

Los numerales reconstruidos para la protolengua madre de diversas familias lingüísticas. Entre las lenguas de Eurasia se tienen:
Para las lenguas de África se tiene:
Para las lenguas de América se tiene:
Y para las lenguas de Oceanía:



</doc>
<doc id="8903" url="https://es.wikipedia.org/wiki?curid=8903" title="Febe (mitología)">
Febe (mitología)

En la mitología griega, Febe (en griego antiguo Φοίβη - "Phœbē": ‘brillo’ del intelecto), la de la corona de oro, era una de las Titánides originales, los hijos gigantes de Urano y Gea. Febe acudió al lecho de Ceo y de él concibió a Leto y a Asteria. Recibió el control del oráculo de Delfos de Temis, de acuerdo con algunas pocas fuentes, y posteriormente se lo daría a Apolo.

También se aplicaba su nombre como epíteto a Artemisa en su papel de diosa de la luna, que se consideraba femenina.



</doc>
<doc id="8906" url="https://es.wikipedia.org/wiki?curid=8906" title="Charles Babbage">
Charles Babbage

Charles Babbage FRS (Teignmouth, Devonshire, Gran Bretaña, 26 de diciembre de 1791-Londres, 18 de octubre de 1871) fue un matemático y científico de la computación británico. Diseñó y desarrolló parcialmente una calculadora mecánica capaz de calcular tablas de funciones numéricas por el método de diferencias. También diseñó, pero nunca construyó, la máquina analítica para ejecutar programas de tabulación o computación; por estos inventos se le considera como una de las primeras personas en concebir la idea de lo que hoy llamaríamos una computadora, por lo que se le considera como «El Padre de la Computación». En el Museo de Ciencias de Londres se exhiben partes de sus mecanismos inconclusos. Parte de su cerebro conservado en formol se exhibe en el Royal College of Surgeons of England, sito en Londres.

Babbage intentó encontrar un método por el cual se pudieran hacer cálculos automáticamente por una máquina, eliminando errores debidos a la fatiga o aburrimiento que sufrían las personas encargadas de compilar las tablas matemáticas de la época. Esta idea la tuvo en 1812. Tres diversos factores parecían haberlo motivado: una aversión al desorden, su conocimiento de tablas logarítmicas, y los trabajos de máquinas calculadoras realizadas por Blaise Pascal y Gottfried Leibniz. En 1822, en una carta dirigida a "sir" Humphry Davy en la aplicación de maquinaria al cálculo e impresión de tablas matemáticas, discutió los principios de una máquina calculadora. Además diseñó un plano de computadoras.

Entre 1833 y 1842, Babbage lo intentó de nuevo; esta vez, intentó construir una máquina que fuese programable para hacer cualquier tipo de cálculo, no solo los referentes al cálculo de tablas logarítmicas o funciones polinómicas. Esta fue la máquina analítica. El diseño se basaba en el telar de Joseph Marie Jacquard, el cual usaba tarjetas perforadas para realizar diseños en el tejido.
Babbage adaptó su diseño para conseguir calcular funciones analíticas. La máquina analítica tenía dispositivos de entrada basados en las tarjetas perforadas de Jacquard, un procesador aritmético, que calculaba números, una unidad de control que determinaba qué tarea debía ser realizada, un mecanismo de salida y una memoria donde los números podían ser almacenados hasta ser procesados. Se considera que la máquina analítica de Babbage fue la primera computadora de la historia. Un diseño inicial plenamente funcional de ella fue terminado en 1835. Sin embargo, debido a problemas similares a los de la máquina diferencial, la máquina analítica nunca fue terminada por Charles. En 1842, para obtener la financiación necesaria para realizar su proyecto, Babbage contactó con "sir" Robert Peel. Peel lo rechazó, y ofreció a Babbage un título de caballero que fue rechazado por Babbage. Lady Ada Lovelace, matemática e hija de Lord Byron, se enteró de los esfuerzos de Babbage y se interesó en su máquina. Promovió activamente la máquina analítica, y escribió varios programas para la máquina analítica. Los diferentes historiadores concuerdan que esas instrucciones hacen de Ada Lovelace la primera programadora de computadoras de la historia.

Charles Babbage ha sido considerado por algunos como el padre de las computadoras modernas, pero sin duda también puede ser considerado el padre de las impresoras modernas. Más de 150 años después de sus planos y un trabajo minucioso del Museo de Ciencias de Londres, dieron como resultado la construcción de la "Máquina Analítica". Los planos del matemático y científico incluían un componente de impresión, el cual ha sido reconstruido por el Museo y es funcional. Esta impresora consta de 8000 piezas mecánicas y pesa aproximadamente 2,5 toneladas.

Fue tan innovadora para su época y podemos apreciarlo hoy, que es capaz de imprimir automáticamente los resultados de un cálculo y un usuario puede cambiar parámetros como espacio entre líneas, elegir entre dos tipografías, número de columnas y otros. Su sofisticación llega a tal punto que puede generar (fabricar) los moldes de las impresiones que podrían ser usados por las imprentas aún hoy en día. Esta impresora lamentablemente no lleva un nombre ya que Babbage la incluyó en sus planos de la "Máquina Analítica", pero basta con aludir a ella como la "impresora de Babbage" para reconocer en este hombre un visionario.

Babbage es recordado también por otras realizaciones. La promoción del cálculo infinitesimal es quizás la primera entre ellas. En 1812, Babbage funda la Sociedad Analítica. La tarea primordial de esta sociedad, conducida por el estudiante Robert Woodhouse, era promover el cálculo leibniziano, o cálculo analítico, sobre el estilo de cálculo newtoniano. El cálculo de Newton era torpe y aproximado, y era usado más por razones políticas que prácticas. La Sociedad Analítica incluía a "sir" John Herschel y George Peacock entre sus miembros. En los años 1815-1817 contribuyó en el «cálculo de funciones» de las "Philosophical Transactions" -transacciones filosóficas-, y en 1816 fue hecho miembro de la Royal Society.

Charles Babbage también logró resultados notables en criptografía. Rompió la cifra auto llave de Vigenère, así como la cifra mucho más débil que se llama cifrado de Vigenère hoy en día. La cifra del auto llave fue llamada «la cifra indescifrable», aunque debido a la confusión popular muchos pensaron que la cifra apolialfabética más débil era indescifrable. El descubrimiento de Babbage fue usado en campañas militares inglesas, y era considerado un secreto militar. Como resultado, el mérito por haber descifrado esta clave le fue otorgado a Friedrich Kasiski, quien descifró también este sistema criptográfico algunos años después.

De 1828 a 1839 Babbage fue profesor de matemáticas en Cambridge. Escribió artículos en distintas revistas científicas, y era miembro activo de la Astronomical Society —sociedad astronómica— en 1820 y de la Statistical Society —sociedad estadística— en 1834. Durante los últimos años de su vida residió en Londres, dedicándose a la construcción de máquinas capaces de la ejecución de operaciones aritméticas y cálculos algebraicos.

Propuso el sistema de franqueo postal que utilizamos hoy en día. Hasta entonces el coste de enviar una carta dependía de la distancia que tenía que viajar; Babbage advirtió que el coste del trabajo requerido para calcular el precio de cada carta superaba el coste del franqueo de esta y propuso un único coste para cada carta con independencia del sitio del país al que era enviada.

Fue el primero en señalar que la anchura del anillo de un árbol dependía de la meteorología que había hecho ese año, por lo que sería posible deducir climas pasados estudiando árboles antiguos.

Inventó el "apartavacas", un aparato que se sujetaba a la parte delantera de las locomotoras de vapor para que las vacas se apartasen de las vías del ferrocarril.

Se interesó también por temas políticos y sociales e inició una campaña para deshacerse de los organilleros y músicos callejeros de Londres, aunque estos pasaron al contraataque y se organizaban en torno a su casa tocando lo más alto que podían.





</doc>
<doc id="8909" url="https://es.wikipedia.org/wiki?curid=8909" title="Eduardo Duhalde">
Eduardo Duhalde

Eduardo Alberto Duhalde (Lomas de Zamora, 5 de octubre de 1941) es un político, abogado y notario (escribano) argentino. Ocupó la vicepresidencia de la Nación durante el primer mandato de Carlos Saúl Menem, aunque renunció a este cargo para asumir como gobernador de la provincia de Buenos Aires; y entre 2002 y 2003 fue presidente de Argentina por aplicación de la Ley de Acefalía. Candidato en las Elecciones presidenciales de Argentina de 2011, obtuvo el 5,86 % de los votos emitidos.
Está casado con "Chiche" Duhalde (Hilda Beatriz González de Duhalde, 1946-), quien también participó activamente en política y fue electa diputada y senadora por la provincia de Buenos Aires.

Hijo de don Tomás Duhalde Gorostegui (1907-1977) y de doña María Esther Maldonado Aguirre (1913-2004), Duhalde militó desde temprana edad en el Partido Justicialista. Se recibió de abogado en la Universidad de Buenos Aires en 1970; cuatro años más tarde, fue electo Concejal de Lomas de Zamora y por acefalía terminaría siendo Intendente de su ciudad natal, luego de que las presiones de la Unión Obrera Metalúrgica (Sindicato argentino) terminaran con la baja de la intendencia del por entonces Intendente de la ciudad, Pedro Pablo Turner, apoyado por la Juventud Peronista. Bandas armadas presionaron a otros dos Concejales para que renunciaran al puesto, mientras la Triple A (Alianza Anticomunista Argentina) llegaba a Lomas de Zamora para asesinar a Turner y a sus partidarios de la Juventud Peronista, fuertes en la universidad local y en algunos barrios. Durante la intendencia de Duhalde, la Triple A y comandos de la policía provincial llevaron a cabo varios asesinatos y secuestros, entre ellos la Masacre de Pasco.

Duhalde fue depuesto por el golpe de Estado de 1976. En el retorno democrático 1983 fue electo intendente de Lomas de Zamora para el período 1983-1987. Según declaró a la revista "Noticias", Duhalde fue visitado en ese entonces por un coronel que le solicitó su apoyo, así como a otros funcionarios, para un posible golpe de estado contra Raúl Alfonsín (1927-2009). Tal supuesto hecho nunca fue hecho público. Al ocurrir eso, Duhalde dijo que había acudido a denunciar la situación ante el propio presidente.

Al terminar su mandato como intendente fue electo diputado nacional (1987-1989).

En el año 1989 participó de las elecciones presidenciales, acompañando como candidato a vicepresidente a Carlos Menem, imponiéndose a la postre sobre la fórmula de la Unión Cívica Radical, Eduardo Angeloz-Juan Manuel Casella. Dos años después renunció a la vicepresidencia y se postuló como candidato a gobernador de la provincia de Buenos Aires.

Su gobierno en la provincia tendría un gran apoyo popular, reflejado en victorias holgadas en varias de las elecciones provinciales posteriores en las cuales la provincia se convertiría en un importante sustento electoral de las victorias nacionales del peronismo.

La gestión de Duhalde en la provincia vio un gran aumento en la construcción e inauguración de edificios, establecimientos y obras públicas consecuencia de una distribución de los fondos de la Coparticipación Federal bastante generosa para con la provincia de Buenos Aires.

Dado que la constitución provincial no contemplaba la posibilidad de la reelección, realizó una reforma constitucional en 1994, al tiempo que se reformaba también la nacional. Los principales partidos opositores de entonces, la Unión Cívica Radical, el Frente Grande y el MODIN, unieron a sus constituyentes en un bloque conjunto para impedir que se oficializara la cláusula reeleccionaria. Pero inesperadamente, el Modín desertó a sus aliados y le dio a Duhalde el apoyo que necesitaba para aprobar el cambio, el cual se sometió sin embargo a un plebiscito que decidiría si se permitiría o no la reelección. El duhaldismo se impuso tanto en la aprobación del "Sí" de dicho plebiscito como en las elecciones de gobernador que tuvieron lugar en 1995 por amplia mayoría.

Fue derrotado por Fernando de la Rúa en las elecciones presidenciales de 1999 tras obtener el 38,28 % de los sufragios, frente al 48,37 % de su opositor.

En octubre de 2001 había sido elegido senador nacional por amplio margen. Luego accedió a la presidencia en el caos subsiguiente a la renuncia de De la Rúa, provocada por la crisis económica, social y política que tuvo su clímax cuando el ministro de Economía Domingo Cavallo instauró el corralito. Duhalde es acusado de ser promotor y fogonero de los saqueos a comercios y supermercados en la crisis que llevaría a la renuncia del presidente radical Fernando De la Rúa; el expresidente interino hace responsable de los mismos a piqueteros y partidos de izquierda.

El 2 de enero de 2002 Duhalde fue elegido Presidente de la Nación argentina por la Asamblea Legislativa en medio del caos en las calles de Buenos Aires. Se llega a esa decisión a través de un amplio consenso en el peronismo y la oposición para que Duhalde piloteara el país, sumido en la confusión de una crisis terminal, en el ínterin preelectoral. Duhalde fue investido por los diputados y senadores con 262 votos a favor, 21 en contra y 18 abstenciones, y con mandato hasta el 10 de diciembre de 2003, esto es, hasta agotar el ejercicio cuatrienal para el que había sido elegido De la Rúa. No habría, por tanto, comicios anticipados, siendo la opinión mayoritaria de los legisladores que lo que urgía era obtener un Ejecutivo estable con el máximo apoyo partidista

Duhalde ―que en vísperas de la asunción presidencial había expresado su temor a que se produjera una "guerra civil" en Argentina― empezó por reconocer que el país estaba "quebrado" y "fundido", y anunció un Gobierno de unidad nacional con la triple misión de "reconstruir la autoridad política e institucional, garantizar la paz social y sentar las bases para el cambio del modelo económico y social".

Entre las medidas de su gobierno interino se destacan la búsqueda de la pacificación del país a través de instrumentos como el Diálogo Argentino, de distintas medidas económicas tendientes a la reactivación de una economía argentina que venía de sufrir varios años de recesión: devaluación de la moneda, que dio fin a la Ley de Convertibilidad, la pesificación forzada de los depósitos bancarios en moneda extranjera, y una serie de medidas sociales tendientes a atenuar los efectos de una economía recesiva que había incrementado la pobreza e indigencia hasta índices nunca vistos antes en la Argentina.

En materia de política exterior, se recuerda el tajante rechazo de su gobierno al golpe de estado contra Hugo Chávez en 2002, las posturas diplomáticas argentina y cubana fueron de importancia para aislar internacionalmente al gobierno de facto.

Su plan económico productivista permitió que la economía argentina cambiara radicalmente, sobre todo a partir del segundo semestre del 2002. Ya en el arranque de 2003 los efectos positivos del cambio de rumbo económico impulsado por Duhalde y gestionado por Roberto Lavagna ya estaban haciéndose notar. La actividad económica resurgía gracias a que el peso devaluado estaba espoleando el comercio exportador y la producción industrial local en detrimento de las importaciones de bienes, de manera que la caída registrada en 2002 del 10,9 % del PIB, dio paso a un crecimiento del 5 % en el primer trimestre de 2003.

El 26 de junio de 2002 en las inmediaciones de la estación ferroviaria de la ciudad de Avellaneda, en el conurbano de la provincia de Buenos Aires, Argentina se produjo la represión de una manifestación de grupos piqueteros. En la persecución fueron asesinados por efectivos de la Policía Bonaerense los jóvenes activistas Maximiliano Kosteki y Darío Santillán pertenecientes al "Movimiento de Trabajadores Desocupados (MTD) Guernica" y el "MTD Lanús", respectivamente, nucleados en la "Coordinadora de Trabajadores Desocupados Aníbal Verón". Además se registraron 33 heridos por balas de plomo entre los manifestantes. Ante el impacto generado, Duhalde anticipó seis meses el llamado a elecciones presidenciales.

Para las siguientes elecciones presidenciales del 27 de abril de 2003, Duhalde decidió dar su apoyo ―junto con su bastión electoral, el gran Buenos Aires (que concentra al 38% de electores de todo el país)― a Néstor Kirchner, quien resultó electo presidente.

Una vez en la presidencia (desde el 25 de mayo de 2003) y tras un período inicial de cordialidad, Kirchner se enfrentó políticamente a Duhalde, derrotándolo en su bastión de la provincia de Buenos Aires en las elecciones legislativas de octubre de 2005. No obstante, "Chiche" Duhalde (la esposa del expresidente) alcanzó a obtener una banca en el Senado y también logró que varios candidatos afines ingresaran a la Cámara de Diputados.

En diciembre de 2003, Eduardo Duhalde asumió como titular de la Comisión de Representantes Permanente del Mercosur, tarea que desempeñó hasta el año 2005. Durante su gestión se destaca la creación de la Comunidad Sudamericana de Naciones que luego devengaría en la actual UNaSur (Unión de Naciones Suramericanas).

En las elecciones presidenciales de 2011, Duhalde alcanzó un magro resultado con su partido Unión Popular (enfrentado al kirchnerismo). Si bien en las primarias de agosto de 2011 finalizó en tercer lugar con el 12,12% de los votos -apenas unas milésimas por debajo de Ricardo Alfonsín-, luego en las elecciones generales de octubre del mismo año alcanzó apenas un 5,86 %.

Actualmente, Eduardo Duhalde encabeza el Movimiento Productivo Argentino, entidad dedicada al mundo económico con un perfil productivista de la cual es miembro fundador junto a otros políticos como Raúl Alfonsín (1927-2009) y una gran cantidad de empresarios.

En agosto de 2008, Eduardo Duhalde afirmó que el titular del Partido Justicialista, Néstor Kirchner, le hacía acordar «al Führer [por Adolfo Hitler<nowiki>]</nowiki> y a Mussolini».
Recordó lo que llamó su «primer enfrentamiento público» con Kirchner, cuando «él criticaba a Macri y yo le dije que no había que criticarlo a Macri: había que criticar la falta que le hacen al país 400 empresarios como Macri».

Al día siguiente ―tras una retahíla de reacciones contrarias desde todo el espectro político― tuvo que retractarse de sus dichos y «se mostró arrepentido».

Durante el Gobierno de Mauricio Macri, quien asumió el 10 de diciembre de 2015, el expresidente Eduardo Duhalde comenzó a postular la idea de un "cogobierno" que contemple a la aposición. "Parto de la idea de que el esquema gobierno-oposición está caduco y que las convicciones que resume la frase 'El que gana gobierna y el que pierde acompaña' ya no son útiles en nuestros tiempos", postuló en distintas notas de opinión. Su propuesta fue: "Para superar los incontables inconvenientes que encuentran los gobiernos de todo signo, debemos avanzar hacia un sistema donde el ganador de una elección conduce y los otros partidos con representación parlamentaria integran un cogobierno". 

A pesar de que Duhalde escribió dos libros sobre el tema de la lucha contra las drogas ―"Hacia un mundo sin drogas" (1994) y "Política, familia, sociedad y drogas" (1997)―, varias personalidades de la política vincularon a Eduardo Duhalde con el narcotráfico.

En 2005, la diputada Elisa Carrió acusó a Eduardo Duhalde de ser «el mayor responsable político de la droga en el país». Afirmó públicamente que «Duhalde controlaba la droga en la provincia de Buenos Aires». Duhalde demandó a Carrió por calumnias e injurias. En 2009, la jueza federal María Servini de Cubría convocó a ambos contendientes. En su descargo, Carrió afirmó que no había culpado a Duhalde de ser narcotraficante, ni le había imputado un delito, sino que le adjudicó responsabilidad política en el asunto. Durante el encuentro, la diputada ratificó que Duhalde controlaba la droga en la provincia de Buenos Aires, aunque aclaró que si sus dichos afectaron al expresidente, ella le pedía perdón si se había sentido ofendido. Carrió aseguró que no se retractó de sus dichos. La jueza decidió realizar un sobreseimiento en el juicio.

Sin embargo jamás se ha podido demostrar nada al respecto. Los juicios iniciados por el expresidente a cada una de las personas que lo han tratado de vincular con las drogas finalizaron demostrando que no solo Duhalde era inocente, sino que además sus detractores habían mentido de manera orgánica y armónica.

Entre las personas a las cuales les ha iniciado juicio y luego tuvieron que retractarse por decisión judicial figuran Elisa Carrió y Luis D’Elía quien fue condenado a pagarle 150 000 pesos argentinos por sus dichos.

En 2010, Duhalde escribió "Es hora de que me escuchen. El peligro de los narcoestados" (refiriéndose al problema de las drogas en la Argentina).




</doc>
<doc id="8911" url="https://es.wikipedia.org/wiki?curid=8911" title="Presidente de la Nación Argentina">
Presidente de la Nación Argentina

El presidente de la Nación Argentina es el jefe de Estado y jefe de Gobierno, responsable político de la administración general de la República Argentina y comandante en jefe de las Fuerzas Armadas.

Entre otros poderes y responsabilidades, el Artículo 99 de la Constitución de la Nación Argentina encarga al presidente «expedir las instrucciones y reglamentos que sean necesarios para la ejecución de las leyes», hace del presidente el comandante en jefe de las Fuerzas Armadas, lo autoriza a nombrar oficiales ejecutivos y judiciales, lo sitúa al frente de la política exterior de Argentina, y permite al presidente conceder indultos o moratorias, aprobar o vetar leyes, introducir legislación mediante decretos de necesidad y urgencia, y declarar el estado de sitio y la intervención federal.

El presidente es elegido mediante el sufragio directo con posibilidad de una segunda vuelta electoral para un mandato de cuatro años. Desde la reforma de la Constitución Argentina de 1994, el mandatario tiene la posibilidad de una reelección inmediata, pudiendo repetir nuevamente el mandato después de transcurrido un período. En caso de muerte, destitución, dimisión o renuncia de un presidente, el vicepresidente asume la presidencia.

Hasta la fecha, ha habido un total de cuarenta y nueve personas que asumieron el cargo y cincuenta y dos presidencias. De las personas elegidas para el cargo, tres murieron en el cargo por causas naturales, doce son dictadores que se arrogaron el cargo de presidente "de facto" y ocho renunciaron. El primer presidente fue Bernardino Rivadavia con el cargo de «presidente de las Provincias Unidas del Río de la Plata», creado por ley del Congreso del 6 de febrero de 1826. Después de su renuncia desapareció el cargo en la legislación argentina hasta que en 1853 fue restablecido por una nueva constitución, disponiendo que la elección se realizaba por seis años, sin posibilidad de reelección inmediata; la designación se hacía por votación indirecta en un Colegio electoral y si ningún candidato lograba más de la mitad de los votos, decidía el Congreso entre los dos más votados. Justo José de Urquiza fue el primer elegido de acuerdo con el nuevo régimen y desempeñó el cargo como «presidente de la Confederación Argentina», al igual que su sucesor, Santiago Derqui, quien luego de las reformas constitucionales de 1860, asumió como «presidente de la Nación Argentina», título vigente hasta la fecha. Adolfo Rodríguez Saá fue el que menos tiempo permaneció en el cargo, con tan solo 7 días, y Julio Argentino Roca, con sus 12 años en el puesto, fue el que permaneció por más tiempo.

El actual presidente es Mauricio Macri, de la alianza Cambiemos, que tomó posesión el 10 de diciembre de 2015.

Los orígenes de la Argentina como nación se remontan a 1776 cuando, en el marco de las llamadas reformas borbónicas, el rey de España creó el Virreinato del Río de la Plata –que abarcaba aproximadamente los territorios de las actuales Argentina, Bolivia, Paraguay, Uruguay y sur de Brasil– separándolo del Virreinato del Perú. El jefe de Estado seguía siendo el rey, representado localmente por el virrey que, en general, eran nacidos en España.

En la Revolución de Mayo de 1810, se formó en Buenos Aires el primer gobierno autónomo en el territorio del virreinato, conocido como la Primera Junta, que sustituyó al virrey pero continuó gobernando en nombre del rey de España. Más tarde se transformó en la Junta Grande, cuando se unieron representantes de otras ciudades del interior y luego el gobierno se delegó primero en un triunvirato y luego en un poder ejecutivo unipersonal con el nombre de Director Supremo creado por la Asamblea Nacional de 1813.

El cargo de Director Supremo se mantuvo cuando, luego de declarada la independencia el 9 de julio de 1816 por un Congreso reunido en la ciudad de San Miguel de Tucumán, se aprobó una Constitución en 1819, pero debido a circunstancias políticas, la misma nunca entró en vigor, el poder central se disolvió y el país quedó como una confederación de provincias.

Una nueva constitución aprobada en 1826 creó por primera vez el cargo de presidente, para el cual fue electo Bernardino Rivadavia, el primer presidente argentino. Debido a la guerra entre Argentina y Brasil, Rivadavia renunció después de un breve período de tiempo, y la oficina se disolvió poco después.

Una guerra civil entre “unitarios“ (unitaristas, es decir gobierno central con sede en Buenos Aires) y “federales” (federalistas con plena autonomía de las provincias) se produjo en las décadas siguientes. En este momento, no había una autoridad central y lo más cercano a ello era el representante de relaciones exteriores, por lo general el gobernador de la Provincia de Buenos Aires. El último en llevar este título fue Juan Manuel de Rosas que, en los últimos años de su gobierno, fue elegido jefe supremo de la Confederación Argentina, adquiriendo poder efectivo en el resto del país.

En 1852, Rosas fue derrocado y se convocó a una asamblea constituyente. Esta constitución, aún en vigor, estableció un gobierno federal nacional, con la oficina del presidente, electo mediante el Colegio electoral. El período de mandato se fijó en seis años, sin posibilidad de reelección. El primer presidente elegido en virtud de la Constitución fue Justo José de Urquiza. Después de una breve interrupción en 1860, la sucesión de presidentes se realizó respetando las normas legales hasta que a partir de 1930 fue interrumpida por varios golpes de Estado, con lo cual se intercalaron presidentes "de facto" y otros legalmente elegidos.

En 1930, 1943, 1955, 1962, 1966 y 1976, golpes militares depusieron los presidentes electos. En 1930, 1943 y 1955 las Fuerzas Armadas designaron militares con el título de presidente. Como resultado de los golpes de estado de 1966 y de 1976, el gobierno federal fue ejercido por una junta militar integrada por los jefes de cada una de las tres ramas de las Fuerzas Armadas (Ejército, Fuerza Aérea y Armada) la que, a su vez, designó a un militar como presidente. En 1962, antes que los militares alcanzaran a nombrar un nuevo presidente, asumió ese cargo el presidente provisional del Senado que era el reemplazante legal del presidente depuesto.

Es discutible si estos jefes de estado militares adecuadamente pueden ser llamados presidentes, dada la ilegitimidad de sus gobiernos. La posición del gobierno argentino actual es que los militares que desempeñaron el Poder Ejecutivo entre 1976 y 1983 no fueron explícitamente presidentes legítimos, por lo cual no se le ha reconocido derecho a una pensión presidencial. La situación de los anteriores presidentes militares no está definida, si bien todos ellos en la actualidad ya han fallecido.

La Constitución de 1826 establecía que el poder ejecutivo es ejercido por una persona bajo el título de "presidente de la República Argentina" (artículo 68). Los requisitos eran ser ciudadano argentino, tener treinta y seis años, nueve como ciudadano y un capital de diez mil pesos (artículos 24 y 69). Tenía un mandato de cinco años y no podía ser reelecto (artículo 71). En caso de enfermedad, muerte, renuncia o destitución el cargo era ejercido por el presidente del senado (artículo 72). Era elegido de la siguiente forma (artículos 73 al 80): En la capital y en cada provincia se formaba una junta de 15 electores quienes votaban cuatro meses antes de que finalice el mandato con "balotas firmadas". Una vez terminada la votación y el escrutinio, el acta iba dirigida al presidente del Senado quien junto a cuatro miembros del congreso hacían el conteo final. El que reunía las dos terceras partes de los votos era proclamado presidente. En caso de que ninguno lo reuniere, era elegido por los 2/3 del congreso. Podía ser destituido por acusación de la Cámara de Representantes por "delitos de traición, concusión, malversación de los fondos públicos, violación de la Constitución, particularmente con respecto a los derechos primarios de los ciudadanos, u otros crímenes que merezcan pena infamante o de muerte". y ser destituido por el senado

Sus atribuciones eran (artículos 81 al 101) publicar y hacer ejecutar las leyes, convocar al congreso, hacer anualmente la apertura de sesiones, ordena las elecciones legislativas, ser el comandante de las fuerzas de mar y tierra necesitando el permiso del congreso para mandar al ejército en persona, proveer la seguridad interior y exterior, tomar medidas para garantizar la paz, hacer tratados con aprobación del senado, nombrar y destituir a sus cinco ministros, nombrar embajadores y demás agentes con aprobación del senado, recibir delegaciones extranjeras, expedir las cartas de ciudadanía, ejercer el patronato general de las iglesias, "Todos los objetos y ramos de Hacienda y Policía, los establecimientos públicos, y nacionales, científicos y de todo género, formados y sostenidos con fondos del Estado las casas de moneda, Bancos nacionales, correos, postas y caminos son de la suprema inspección y resorte del Presidente de la República", aplicar indultos y nombrar jueces de la corte suprema. Además, nombraba a los gobernadores de las provincias a propuesta de un Consejo de Administración y aprobaba sus presupuestos. Todo proyecto de ley debía pasar por el poder ejecutivo quien las aprobaba u objetaba.

Los artículos 71.º a 90.º contenían las estipulaciones relativas al poder ejecutivo. El titular de este era unipersonal, y llevaba el título de "Presidente de la Confederación Argentina". Un vicepresidente, electo juntamente con él, lo supliría en caso de ausencia, inhabilidad o renuncia.

Los requisitos para la elección como presidente eran similares a los exigidos para los senadores; se les añadía la condición de nativo, o de ser hijo de uno en caso de haber nacido fuera del territorio nacional, y la práctica de la religión católica, única concesión a los "montoneros". Su mandato se extendería por un período de seis años, sin posibilidad de reelección hasta que un período completo hubiese pasado; ninguna causa permitía la extensión de este más allá de los seis años cumplidos desde la fecha original de asunción.

El procedimiento para la elección presidencial era indirecto; el electorado de cada provincia escogería un número de delegados, igual al doble de la cantidad total de diputados y senadores que se eligiesen por la misma. Los electores de cada provincia votarían discrecionalmente a los candidatos que juzgasen más convenientes, y remitirían copia sellada de su resolución al Senado de la Nación; una vez recibidas todas las listas, la Asamblea Legislativa realizaría el escrutinio de estas. De haber como resultado mayoría absoluta de un candidato, la proclamación sería automática. En caso de no contar ninguno con la misma, la Asamblea Legislativa elegiría inmediatamente y a simple pluralidad de sufragios entre los dos candidatos más votados, o más en caso de haber empate en el primer o segundo puesto. En este último caso, de no haber candidato con mayoría absoluta en primera instancia, se realizaría balotaje entre los dos candidatos más votados en la primera vuelta. El "quorum" para esta elección era de tres cuartas partes de los congresistas.

De acuerdo con el primer inciso del artículo 90.º, el presidente era la autoridad suprema de la Confederación, en lo que se denomina un régimen "presidencialista": no respondía de sus acciones, dentro del marco impuesto por la Constitución, a ninguna autoridad superior, y no requería de la aprobación del Congreso para el ejercicio de las atribuciones que le competen. Era además el titular del poder ejecutivo de la ciudad designada capital federal, y el jefe de las fuerzas armadas.

El presidente gozaba de facultades legislativas: además de la sanción y promulgación de las leyes dictadas por el Congreso, incluyendo la facultad de veto, estaba a su cargo la expedición de los reglamentos necesarios para la aplicación de la ley, llamados "decretos", aunque respetando el espíritu original de la misma. La firma de tratados con otros estados estaba a su exclusivo cargo, así como la decisión de dar o no trámite a los documentos emitidos por el pontífice católico.

Como autoridad en materia de política exterior, es el encargado del nombramiento de embajadores y otros ministros destinados a la negociación con las potencias extranjeras; la elección y remoción de los titulares de embajada requería acuerdo senatorial —un vestigio de la influencia de la constitución norteamericana, en la que el Senado comparte con el presidente la potestad sobre las relaciones exteriores, sobre los convencionales—, pero la de los funcionarios de rango inferior estaba enteramente a su cargo. Por lo mismo, era la autoridad a cargo de la gestión de los asuntos militares, disponiendo del ejército, designando a los oficiales de este —con acuerdo del Senado, en caso de los puestos superiores del escalafón—, emitiendo patentes de corso, declarando la guerra o decretando el estado de sitio cuando su causa es el ataque de una potencia extranjera.

Su implicación con las tareas del Congreso no se limitaba a la promulgación de las leyes: estaba a cargo del presidente la apertura de las sesiones en Asamblea Legislativa, en la que comunicaba al mismo sus consideraciones acerca de su tarea, y la prórroga o convocatoria a sesiones fuera del período ordinario.

Con respecto al poder judicial, estaba a su cargo la designación de los jueces de los tribunales federales, para lo que requería el acuerdo senatorial; además, contaba con la facultad de indultar a los condenados por delitos de jurisdicción federal, salvo en casos de juicio político. No tenía la facultad de imponer condenas, pero sí de —en estado de sitio— decretar el arresto temporal o el traslado de personas, salvo que estas prefiriesen abandonar el territorio nacional. Si no contaba con el acuerdo del Congreso al dictarlas, estas medidas caducaban automáticamente a los 10 días.

Como encargado de la administración nacional, le estaba encomendada la recaudación de la renta nacional y su aplicación, dentro del marco de la ley de presupuesto; tenía facultad para otorgar el goce de licencias o montepíos, y para recabar cualquier clase de información por parte de la administración nacional.

La Constitución fijaba como ayudantes del presidente a cinco ministros, elegidos por este, en carteras de Interior, de Relaciones Exteriores, de Hacienda, de Justicia, Culto e Instrucción Pública, y de Guerra y Marina. El refrendo ministerial era necesario para los decretos de gobierno. Los ministros estaban además obligados a dar informes al Congreso en la apertura de sesiones, y facultados a tomar parte en los debates de este, aunque sin voto. La tarea era incompatible con el ejercicio del poder legislativo nacional.


El presidente de la Nación tiene las siguientes atribuciones:

El presidente es el jefe supremo de la Nación, jefe de gobierno y responsable político de la administración general del país (inciso 1), y está a la cabeza del poder ejecutivo del gobierno, cuya responsabilidad es «Expedir las instrucciones y reglamentos que sean necesarios para la ejecución de las leyes» (inciso 2). Para llevar a cabo este deber, se le otorga el control de los cuatro millones de empleados del poder ejecutivo federal.

Al presidente le corresponde el nombramiento y remoción de varios miembros del poder ejecutivo. Embajadores (Según la ley 20.957, el Gobierno puede designar hasta 25 embajadores de su confianza sin aprobación del Senado, que deben dejar sus funciones cuando el presidente termina su mandato), ministros plenipotenciarios y encargados de negocios, son todos designados por el presidente con el «consejo y consentimiento» de una mayoría del Senado; y por sí solo nombra y remueve a miembros del y otros oficiales federales (inciso 7). Los nombramientos realizados mientras el Senado no está en periodo de sesiones son temporales y expiran al final de la siguiente sesión del Senado (inciso 19).

Supervisa el ejercicio de sus ministros (inciso 10) y puede pedir los informes que crea convenientes (inciso 17). Generalmente, el presidente puede cesar y llenar vacantes a los funcionarios ejecutivos a su discreción (inciso 19).

Quizás el más importante de todos los poderes presidenciales es su posición al frente de las Fuerzas Armadas de Argentina como su Comandante en Jefe (inciso 12). Mientras que el poder de declarar la guerra corresponde constitucionalmente al Congreso (inciso 15), el presidente comanda y dirige a sus ejércitos y es responsable de planear la estrategia militar y concesionar empleos y grados militares (incisos 13 y 14).

Junto con las fuerzas armadas, el presidente también está al frente de la política exterior. A través del Ministerio de Relaciones Exteriores y el Ministerio de Defensa, el presidente es responsable de la protección de los argentinos en el extranjero y de los ciudadanos argentinos en territorio nacional. El presidente decide si hay que reconocer nuevas naciones y nuevos gobiernos, recibe sus ministros y admite cónsules, y negocia tratados con otras naciones, que se hacen vigentes en Argentina cuando son aprobados por las dos terceras partes del Senado (inciso 11). El presidente también puede negociar «acuerdos ejecutivos» con poderes extranjeros que no están sujetos a la confirmación de Senado.

En materia de seguridad interior, el presidente está facultado para declarar el estado de sitio (inciso 16) o la intervención federal (inciso 20) de una o varias provincias en caso de ataque exterior con acuerdo del senado y en caso de conmoción interior debe convocar al congreso para su tratamiento, pero no podrá condenar por sí o aplicar penas (artículo 23).

Concede jubilaciones, retiros, licencias y pensiones (inciso 6); y puede ausentarse del territorio nacional, con permiso del Congreso (inciso 18).

El primer poder conferido al presidente por la Constitución es el poder legislativo del veto presidencial. Cualquier proyecto de ley aprobado por el Congreso deberá ser presentado al presidente antes de que pueda convertirse en ley. Una vez que la norma legal ha sido presentada, el presidente tiene tres opciones:
Luego las promulga y hace publicarlas en el Boletín Oficial. Además, el presidente tiene capacidad para introducir legislación directamente, a través de decretos de necesidad y urgencia, siempre que no regulen en materia penal, tributaria, electoral o el régimen de los partidos políticos, en acuerdo general con los ministros y siendo analizados y aprobados por el Congreso (inciso 3).

El presidente puede desempeñar un papel importante en su conformación, sobre todo si el partido político del presidente tiene mayoría en una o ambas Cámaras del Congreso. Los miembros del poder ejecutivo no pueden ocupar simultáneamente su puesto y un escaño en el Congreso, pero es habitual que redacten la legislación y que un Senador o Diputado la presente por ellos. El presidente puede influir de una forma importante en el poder legislativo a través del informe anual, escrito u oral al hacer la apertura de las sesiones ordinarias, que constitucionalmente debe presentar al Congreso. Este discurso a menudo perfila la oferta legislativa para el año próximo (inciso 8). De acuerdo con los artículos 63 y 99 inciso 9, el presidente puede convocar a una o a ambas Cámaras del Congreso para una sesión extraordinaria mediante un decreto, indicando los proyectos de ley a tratar.

El presidente también tiene la facultad de proponer jueces federales, incluidos miembros de la Corte Suprema de Argentina, sobre la base de una propuesta vinculante del Consejo de la Magistratura (inciso 4). Sin embargo, estos nombramientos requieren la confirmación del Senado por dos tercios de los miembros presentes y esto puede suponer un escollo importante ante la posibilidad de que un presidente quisiera formar una judicatura federal con una postura ideológica particular. También puede conceder perdones e indultos, pero no intervenir en acusaciones de la Cámara de Diputados (inciso 5).

En la Constitución de 1994, en su artículo 89 de la sección segunda, sobre el poder ejecutivo, marca los requisitos para ser presidente:

La campaña presidencial contemporánea comienza antes de las elecciones primarias, cuando los partidos políticos hacen una selección de candidatos. En las elecciones primarias, quedan habilitados a las elecciones generales quienes pasan el piso del 1,5% del padrón electoral.

Desde 2015, los candidatos participan en debates televisados a escala nacional. Los nominados de cada partido hacen campaña a lo largo de todo el país para explicar sus programas electorales, convencer a los votantes y solicitar contribuciones a la campaña.

Entre 1853 y 1994, el presidente fue elegido mediante el voto indirecto, es decir, se elegían electores para el Colegio Electoral que permitía ganar por amplias mayorías, ya que, con excepción de la elección de Domingo Faustino Sarmiento en 1868, en todas las demás el candidato ganador había logrado la mayoría en el Colegio en la elección. La elección a presidente era a voto cantado y con fraude, siendo necesario la mayoría absoluta (la mitad más uno de los electores) para ser presidente. La sanción de la Ley Sáenz Peña en 1912 instauró el voto secreto y obligatorio entre hombres mayores de 18 años usando el padrón militar siendo aplicado en las elecciones de 1916, 1922 y 1928 ganadas por la Unión Cívica Radical.

Tras el golpe de estado de 1930, las elecciones de 1931 y 1937 se realizaron con fraude electoral y la Unión Cívica Radical proscrita hasta 1935. Con la reforma de 1949, se eliminó el colegio electoral y se instauró el voto directo con la inclusión de las mujeres mediante una ley de 1947, siendo necesario el 50% de los votos afirmativos para ser electo con posibilidad de segunda vuelta electoral entre las dos primeras fuerzas. Así fue reelecto Juan Domingo Perón en 1951. El golpe militar de 1955 reinstauró el sistema electoral anterior con la proscripción de los partidos Peronista y Comunista, aplicándolo a las elecciones de 1958 y 1963. En 1972, la dictadura militar gobernante instauró el voto directo universal, necesitando el 50% de los votos afirmativos con posibilidad de balotaje entre las fórmulas que sacaron más del 15% de los votos y el fin de las proscripciones. Esto se aplicó a las elecciones de marzo y septiembre de 1973. Tras el fin del Proceso de Reorganización Nacional, dictadura que gobernaba desde 1976, las elecciones de 1983 y 1989 se llevaron a cabo mediante el voto indirecto universal con colegio electoral necesitando la mayoría absoluta de los electores para ser presidente.

En 1994 se reformó la Constitución y dispuso, que el candidato es elegido directamente por el pueblo en doble vuelta. La elección es efectuada dentro de los dos meses anteriores a la finalización del mandato del presidente saliente. En la primera vuelta, si la fórmula obtiene el 45 % más uno de los votos, u obteniendo 40 % supera por 10 % al segundo, computando únicamente los votos afirmativos, esto es excluyendo los votos en blanco o nulos, sus integrantes serán proclamados como presidente y vicepresidente. En la segunda vuelta, a los 30 días posteriores a la elección, la fórmula será proclamada por mayoría simple de los votos afirmativos.

De acuerdo con el artículo 91, el mandato presidencial comienza el mismo día en que cesa el mandato anterior. Esta fecha marca el principio del mandato de cuatro años tanto del presidente como del vicepresidente. Antes de poder ejercer, debe realizar un acto de toma de posesión del cargo y, de acuerdo con la Constitución, se requiere que preste el juramento presidencial en manos del presidente del Senado y ante ambas cámaras del congreso:

Aunque no es una exigencia, los presidentes han utilizado tradicionalmente una Biblia para prestar el juramento, y añadiendo al principio de este «"Juro por dios, nuestro señor, y sobre estos santos evangelios."» y hacia el final «"Si así no lo hiciere, que Dios y la Patria me lo demanden"». Aunque esta fórmula, como la anterior, suele ser modificada levemente por quien preste juramento.
El acto comienza con el recorrido desde la Casa Rosada hacia el Congreso por la Avenida de Mayo junto a la escolta del Regimiento de Granaderos a Caballo, quienes entonan la Marcha de San Lorenzo. El presidente electo es recibido por el , el presidente de la Cámara de Diputados y dos comisiones compuestas por cuatro diputados y cuatro senadores cada una llamada "exterior" que lo recibe al ingresar y la "interior" que lo recibe en el Salón Azul. Mientras se dirige al recinto de la Cámara de Diputados donde está reunida la Asamblea Legislativa, pasa por el cordón de honor de cadetes de la Policía Federal, hace una reverencia a la copia de la Constitución de 1853 en el Salón Azul y firma los libros de honor de ambas cámaras. Tras pasar por el Salón de los Pasos Perdidos y una vez llegado al recinto, el presidente provisional del Senado invita primero al vicepresidente y luego al presidente a prestar juramento. Tras ello, el vicepresidente invita al presidente a dar un discurso. Luego se dirige a la Casa Rosada acompañado por la escolta presidencial, ya entonando la Marcha de Ituzaingó, para recibir los atributos presidenciales (la banda y el bastón de mando) de manos del presidente saliente en el Salón Blanco, tomar juramento a su equipo de gobierno y recibir a delegaciones extranjeras en el Palacio San Martín.

El "Reglamento de Protocolo y Ceremonial de la Presidencia", que data de la década de 1960, rige lo referido a la ceremonia formal del traspaso de mando: la entrega de la banda y del bastón. Estipula que la misma debe desarrollarse en el Salón Blanco de la Casa Rosada:

Concluida la ceremonia prescripta, el reglamento indica:

El primer mandatario que recibió los atributos presidenciales fue Domingo Faustino Sarmiento, de manos de Bartolomé Mitre el 12 de octubre de 1868, en un recordado y tumultuoso acto, en el que cientos de jóvenes simpatizantes del presidente electo entraron por la fuerza al recinto de la Casa de Gobierno donde se llevaba adelante la ceremonia; y apretujándose con los invitados especiales, rompieron vidrios y ventanas; llegando a subirse a los muebles de la sala, para poder apreciar mejor el momento. Históricamente, el traspaso desde Domingo Faustino Sarmiento se hizo en la Casa Rosada y solo se recuerdan cuatro excepciones porque la ley no lo prohíbe: José María Guido en 1962 realizada en el Palacio de Justicia, y Néstor Kirchner (2003) y Cristina Fernández de Kirchner (2007 y 2011) realizadas en el recinto de la Cámara de Diputados de la Nación.

De acuerdo con la reforma aprobada en 1994, la duración del mandato del presidente es de "cuatro años con posibilidad de reelección inmediata por otros cuatro años". Una persona que cumplió dos mandatos consecutivos queda habilitada para otra reelección una vez transcurrido al menos un período presidencial desde que dejó el cargo.

Estas restricciones se aplican en la misma forma para quienes hayan desempeñado como vicepresidentes en uno o en los dos períodos.

Según la Constitución de 1853 y hasta 1994, el presidente tenía mandato por "seis años", sin posibilidad de reelección consecutiva. La reforma de 1949 permitía la reelección sin limitación alguna, pero fue dejada sin efecto por resolución del gobierno militar surgido en 1955, que ratificó la convención constituyente de 1957, con lo cual se retornó al régimen de 1853. El gobierno surgido del golpe militar de 1966 limitó la duración del mandato a "cuatro años" con una reelección consecutiva mediante un estatuto transitorio en 1972 que solo se aplicó para una elección y luego no fue ratificado.

La oficina presidencial puede quedar vacante por varias circunstancias: muerte, dimisión y destitución.

En el caso de la renuncia al cargo, el presidente redacta una carta de renuncia dirigida al , la cual debe ser aceptada por el congreso reunido en Asamblea Legislativa. Este hecho ocurrió ocho veces en la historia: Bernardino Rivadavia en 1827, Santiago Derqui en 1861, Miguel Juárez Celman en 1890, Luis Sáenz Peña en 1895, Roberto Marcelino Ortiz en 1942, Héctor Cámpora en 1973, y Fernando de la Rúa y Adolfo Rodríguez Saá en 2001. 

En cuanto a la destitución, en el artículo 53 de la constitución faculta a la Cámara de Diputados acusar ante el Senado al presidente, vicepresidente, Jefe de Gabinete, ministros y jueces de la Corte Suprema por «mal desempeño o por delito en sus funciones; o por crímenes comunes» por la mayoría de dos terceras partes. Así se inicia el proceso de juicio político (artículos 59 y 60) por parte del Senado presido por el presidente de la Corte Suprema (en el caso de que el presidente sea el acusado), siendo declarado culpable por las dos terceras partes con destitución del cargo e inhabilitación para ejercer cargos públicos y la condena según el Código Procesal Penal.

En caso de que el presidente no pudiese continuar ejerciendo el cargo, por motivos tales como enfermedad, ausencia, muerte, renuncia o destitución, el cargo es ejercido por el "vicepresidente" (artículo 88 de la "Constitución nacional").

En caso de requerirse un reemplazo para el presidente en una circunstancia en la que no se disponga de un vicepresidente, la Constitución establece en su art. 88 que corresponde al Congreso establecer quien asumirá el cargo. A tal fin se sancionó la Ley 20.972, de Acefalía, estableciendo la línea sucesoria para ese caso: transitoriamente el Poder Ejecutivo debe ser desempeñado por el , a falta de este el presidente de la Cámara de Diputados, y a falta de ambos por el presidente de la Corte Suprema de Justicia. Ese funcionario estará a cargo del Poder Ejecutivo sin asumir el título de «presidente».

Si la vacancia es transitoria estos funcionarios deben ejercer el Poder Ejecutivo hasta el retorno del presidente. Si la vacancia no es transitoria, el Congreso en asamblea, dentro del plazo de dos días debe elegir un presidente para gobernar hasta que se realicen nuevas elecciones (artículo 88 de la "Constitución nacional"). Ese funcionario debe ser elegido entre los senadores, diputados o gobernadores. Tales fueron los casos de Vicente López y Planes en 1827, José María Guido en 1962, Raúl Lastiri en 1973; Ramón Puerta, Adolfo Rodríguez Saá, Eduardo Camaño y Eduardo Duhalde en 2001 y 2002; y Federico Pinedo en 2015.

El vicepresidente es el compañero de fórmula del presidente, siendo ambos los dos únicos miembros electos del poder ejecutivo argentino. El vicepresidente es el reemplazante del presidente en caso de viajes o licencias. Un caso notable fue el del vicepresidente Marcos Paz, quien reemplazó de manera interina al presidente Bartolomé Mitre, durante cinco años, mientras este último dirigía en el frente las tropas argentinas en la Guerra del Paraguay. Paz murió mientras ejercía la presidencia, lo que obligó a Mitre a retornar a Buenos Aires para reasumir el mando. Es también el reemplazante del presidente en caso de muerte o renuncia, de forma definitiva. Tales fueron los casos de los vicepresidentes Juan Esteban Pedernera en 1861, Carlos Pellegrini en 1890, José Evaristo Uriburu en 1895, José Figueroa Alcorta en 1906, Victorino de la Plaza en 1914, Ramón Castillo en 1942 y María Estela Martínez de Perón en 1974.

Asimismo, también es presidente del Senado de la Nación Argentina, aunque sin derecho a voto salvo caso de empate.

El 29 de marzo de 1962 se produjo un levantamiento militar con el objetivo de derrocar al presidente Arturo Frondizi, del partido Unión Cívica Radical Intransigente, quien se negó a renunciar. Frondizi fue detenido por los militares y llevado a la Isla Martín García, previendo los rebeldes que, al día siguiente, el teniente general Raúl Poggi, líder de la insurrección victoriosa, asumiría la presidencia.

La noche del 29 de marzo de 1962, algunas personalidades civiles encabezadas por un miembro de la Corte Suprema de Justicia de la Nación, el doctor Julio Oyhanarte, elaboraron una maniobra para evitar que el quiebre institucional fuera total. Fue así como tomaron la detención de Frondizi como un caso de acefalía que permitía asumir la presidencia a quien estuviera en el primer lugar de la línea sucesoria según la Ley 252, que en el caso era el doctor José María Guido, un senador del mismo partido que Frondizi que presidía provisionalmente la Cámara de Senadores, debido a la renuncia anterior del vicepresidente Alejandro Gómez. Basados en esa interpretación hicieron que esa misma noche Guido jurara ante la Corte Suprema de Justicia como nuevo presidente.

Los militares golpistas terminaron aceptando la situación y convocaron a Guido en la Casa Rosada para comunicarle que sería reconocido como presidente, en tanto y en cuanto se comprometiera por escrito a ejecutar las medidas políticas indicadas por las Fuerzas Armadas, siendo la primera de ellas anular las elecciones en las que había ganado el peronismo. Guido aceptó las imposiciones militares, firmó un acta dejando constancia de ello y fue entonces habilitado por estos para instalarse con el título de presidente, pero clausurando el Congreso Nacional e interviniendo todas las provincias.

De este modo Guido asumió los poderes ejecutivo y legislativo del país, bajo control y supervisión de las Fuerzas Armadas, que se reservaron el derecho de removerlo, pero manteniendo intacto el Poder Judicial.

A raíz de golpes militares de Estado que derrocaron a los gobiernos constitucionales hubo presidentes militares y civil de facto en 1930-1932, 1943-1944, 1955-1958, 1962-1963, 1966-1973 y 1976-1983 que ejercieron además de las facultades propias del presidente también las que correspondían al Congreso. El análisis sobre la validez posterior de sus actos llevó a la formulación posterior de la doctrina de los gobiernos de facto.

Esa doctrina fue dejada sin efecto por la reforma constitucional de 1994 (artículo 36), la que declaró «usurpadores» a quienes hayan interrumpido la observancia de la Constitución por actos de fuerza.

El artículo 29 de la Constitución de 1853 tenía un artículo que consideraba la suma del poder público como «traición a la Patria», pero estaba referida a los gobernantes de jure. Por ese motivo en la reforma constitucional de 1994 se incluyó el artículo 36 que dice:
En síntesis, este artículo establece:

Los atributos presidenciales demuestran la dignidad de la Primera Magistratura de la Nación, siendo símbolos regidos por costumbre que representan al presidente de la República ante los ciudadanos. Tradicionalmente son cinco: la Banda Presidencial, el Bastón de Mando, la Marcha militar Ituzaingó, el Estandarte o Bandera de presencia presidencial y el "sillón de Rivadavia". La entrega de los atributos representan el momento en el cual el mandatario saliente transmite la autoridad presidencial al nuevo presidente. Cada presidente recibe una banda y un bastón nuevos, que suelen conservar como recuerdo de su paso por el cargo una vez que cesan en sus mandatos. 

El sillón presidencial de Casa Rosada, o mal llamado "sillón de Rivadavia" porque se cree que lo utilizó Bernardino Rivadavia, corresponde a la primera presidencia de Julio Argentino Roca. Fue comprado a la Casa Forest de París en 1885 y está conformado de madera de nogal italiana, siendo decorado con la técnica dorado a la hoja, con lámina de oro. Es utilizado desde entonces por todos los presidentes del país.
El estandarte presidencial es una bandera heráldica de un paño color celeste adornado con el escudo de la Nación Argentina situado en su parte central y acompañados de cuatro estrellas de cinco puntas colocadas en cada uno de sus vértices, utilizada como insignia del presidente de la Nación Argentina y se enarbola en el lugar en que se encuentra el presidente de la Nación. 

La marcha es una pieza musical solo de melodía utilizada en los actos oficiales donde se presenta el presidente para indicar su llegada. La partitura fue encontrada en un cofre, entre los trofeos de batalla tras la victoria de Carlos María de Alvear en la Batalla de Ituzaingó durante la Guerra del Brasil. Se cree que fue compuesta por el emperador brasileño Pedro I para el marqués de Barbacena, comandante de sus tropas, en caso de un posible triunfo en Ituzaingó. Se utilizó por primera vez a ese efecto el 25 de mayo de 1827, y —con la excepción de un interludio entre el 26 de enero de 1946 y el 28 de agosto de 1959, en que la reemplazó a ese efecto la marcha San Lorenzo— se ha utilizado desde entonces.

El bastón de mando (o vara de mando, también denominado manípulo) es un complemento protocolario que denota en la persona que lo porta, autoridad o mando sobre un grupo o colectivo identitario. Desde 1983 el orfebre Juan Carlos Pallarols, que proviene de una familia de artesanos cuyo taller data de 1750, confecciona el bastón que consiste en una vara de madera urunday (proveniente de Chaco y Misiones) de noventa centímetros (de acuerdo con la estatura del presidente electo) con una empuñadura de plata adornada por el Escudo Nacional y flores de veinticuatro cardos, una por cada provincia; y tres pimpollos, que representan las Islas del Atlántico Sur. Desde 1932 bajo el gobierno de José Félix Uriburu hasta entonces los fabricaba el artesano Luis Ricciardi como se estilaba: con caña de Malaca, puño de oro 18 quilates adornado con el escudo nacional, regatón de oro y dos borlas. 

La banda presidencial es una cinta delgada de tela con los colores de la Bandera bordada con un sol y terminada en una borla de hilos de oro, que se coloca en forma cruzada atravesando el hombro derecho y cayendo hacia el costado izquierdo, sobre la indumentaria para significar que su portador es titular de una dignidad u honor y, en tal carácter, debe ser reconocido por todos.

La banda y el bastón presidencial fueron establecidos por la Asamblea del Año XIII el 26 de enero de 1814 al instaurar el cargo de Director Supremo de las Provincias Unidas del Río de la Plata, siendo Gervasio Antonio de Posadas la primera persona en utilizarlos. La banda era bicolor, blanca en el centro y azul en los costados, terminada en una borla de oro, hasta la gobernación de Juan Manuel de Rosas en que se cambió al rojo punzó siendo restaurado el color original durante la presidencia de Justo José de Urquiza e incluyendo el bordado con un sol. Durante las presidencias conservadoras fue bordado con el escudo nacional en dorado. El 24 de abril de 1944 el presidente Edelmiro Julián Farrell emitió el Decreto-Ley 10.302/1944; donde instituyó los símbolos patrios. En su artículo 4° legisló sobre la Banda Presidencial, pasando de ser un atributo tradicional a una distinción jurídica:

El presidente tiene su oficina en la sede del Gobierno, la Casa Rosada. Desde 1862, el entonces presidente Bartolomé Mitre se instaló en el antiguo Fuerte de Buenos Aires, que había sido residencia de gobernadores y virreyes españoles, y demás autoridades de los sucesivos gobiernos patrios a partir de 1810. Su sucesor, Domingo Faustino Sarmiento, decidió embellecer la morada del Poder Ejecutivo Nacional, dotándola de jardines y pintando las fachadas de color rosado, con el que, posteriormente, se continuó caracterizando. La construcción de la actual Casa de Gobierno comenzó en 1873, cuando por decreto se ordenó construir el edificio de Correos y Telégrafos en la esquina de Balcarce e Hipólito Yrigoyen. Pocos años después, el presidente Julio Argentino Roca decidió la construcción del definitivo Palacio de Gobierno en la esquina de Balcarce y Rivadavia, edificación similar al vecino Palacio de Correos. Ambos edificios se unieron en 1886 mediante el pórtico que hoy constituye la entrada de la Casa Rosada que da hacia Plaza de Mayo.

La residencia oficial ha ido cambiando a lo largo de la historia. Rivadavia (1826-27) residió en la Casa de los Virreyes, en el antiguo Fuerte de Buenos Aires, mientras que sus sucesores residieron en sus casas particulares. La excepción fue Roque Sáenz Peña quien acondicionó un cuarto en el primer piso en la Casa Rosada debido a que su enfermedad le impedía movilizarse con facilidad. A mediados de la década de 1930 el estado adquiere la propiedad de Carlos Madariaga y su esposa Josefina Anchorena, ubicada en la calle Suipacha 1034 de la Ciudad de Buenos Aires, para convertirla en Residencia Presidencial. El primer presidente que la utiliza es Roberto Ortiz junto a su señora María Luisa Iribarne. En 1937 (durante la presidencia de Agustín Pedro Justo) el estado adquiere la residencia construida por Mariano Unzué y Mercedes Baudrix en 1887, ubicada en un espléndido parque diseñado por Thays, rodeado por las calles Agüero, Alvear (Libertador) y Austria, en el barrio de Recoleta, Buenos Aires. El primer y único presidente que la utilizó de forma permanentemente fue Juan Domingo Perón y su señora Eva Duarte.

La Quinta presidencial de Olivos fue donada por la familia Anchorena Olaguier en 1918 bajo la presidencia de Yrigoyen. Desde 1918 los presidentes la fueron utilizando cómo residencia ocasional y de verano, siendo el primero que la habita de forma permanente es Pedro Eugenio Aramburu. Desde entonces se utiliza como residencia oficial y permanente. En la Quinta de Olivos falleció el presidente Juan Domingo Perón el 1 de julio de 1974 mientras ejercía su tercer mandato, siendo hasta ahora el único presidente que murió allí. La Quinta es un gran complejo residencial compuesto de un vasto parque situado en la localidad de Olivos del partido de Vicente López. La residencia presidencial ocupa el edificio principal, de líneas neoclásicas, construido por Prilidiano Pueyrredón en 1854. Las fachadas se conservan como en el siglo XIX, sin embargo, los interiores y el parque sufrieron reformas con el paso de los diferentes presidentes. 

Dispone de una residencia de verano en la localidad de Chapadmalal (provincia de Buenos Aires), la que se denomina Unidad Presidencial de Chapadmalal. La casa de Chapadmalal, construida durante el primer gobierno de Perón, cuenta con una playa privada, un mirador con vistas a la costa y varios jardines. Fue remodelada por última vez en los 90, durante la presidencia de Carlos Menem.

El presidente y vicepresidente disfrutan de un sueldo pagado por el Tesoro de la Nación, que no podrá ser alterado en el período de sus nombramientos. Durante el mismo período no podrán ejercer otro empleo, ni recibir ningún otro emolumento de la Nación, ni de provincia alguna. El sueldo del presidente en bruto es de 173.000 pesos argentinos.

Para desplazarse el mandatario utiliza aviones que forman parte de la Agrupación Aérea Presidencial:


El automóvil presidencial que actualmente se utiliza es una Mercedes Benz Vito, aunque tiene a disposición una Volkswagen Touareg y una Chrysler Town & Country

La Casa Militar es la encargada de la protección del presidente y su familia. Con base en la Casa Rosada, la Casa Militar es conducida por un oficial superior de las Fuerzas Armadas, cuyo cargo es rotativo cada dos años y "debe proveer la seguridad del presidente, de sus familiares directos, como también de la Casa de Gobierno, la residencia presidencial de Olivos y otros lugares de residencia transitoria que disponga el jefe del Estado". Tiene el "control operacional" de tres agrupaciones principales: Coordinación, Logística y Comunicaciones; Aérea, y Seguridad e Inteligencia. Esta última integrada por el histórico Regimiento de Granaderos a Caballo (como escolta presidencial) y la Policía Federal en su división Custodia Presidencial (como custodia personal del presidente y su familia en los desplazamientos terrestres). La Custodia Presidencial tiene su base en la Casa de Gobierno y en la residencia de Olivos, cuyo perímetro y sector externo están a cargo de la policía bonaerense.

El Escuadrón Ayacucho del Regimiento de Granaderos a Caballo, creado en 1812 por el General Don José de San Martín e instaurado el 15 de julio de 1907 por el presidente José Figueroa Alcorta como escolta del Presidente de la Nación -hasta ese momento lo era el Regimiento 8 de Caballería, que llevaba el nombre del General Mariano Necochea y el uniforme histórico de sus Cazadores que hicieron la campaña de los Andes-, cumple las funciones de escolta y seguridad del Presidente de la Nación en la Casa de gobierno, custodia de los restos del General Don José de San Martín en el mausoleo situado en la Catedral Metropolitana, el Izamiento y arrío de la Bandera Oficial de la Nación en la Plaza de Mayo y participan en todos los actos de ceremonial que se realizan en la Casa de Gobierno y en la Catedral Metropolitana. Llevan el uniforme tradicional: botas negras hasta la rodilla, espuelas, una chaquetilla con pechera adornada con botones, sable enfundado en un costado, cuello rígido y el morrión en la cabeza para la famosa Escolta Presidencial. Los granaderos que realizan la custodia del ingreso presidencial y los que custodian los restos de San Martín deben pasar las dos horas que dure su guardia quietos y firmes, en posición de “estatua”. 

Los "edecanes presidenciales" son tres ayudantes militares, uno por cada rama de las Fuerzas Armadas, un teniente coronel, un capitán de fragata y un vice comodoro, que se distinguen por el cordón dorado que usan encima del uniforme, cuya principal misión es acompañar, proteger y asistir al presidente en todas sus actividades oficiales y representarlo en los eventos protocolares que específicamente les encomiende. El primer edecán que registra la historia argentina fue el capitán Juan María Escobar, quien acompañó al presidente de la Primera Junta, Cornelio Saavedra.

Son designados por el propio presidente, de una lista de candidatos presentada por el Ministerio de Defensa y elaborada por las propias Fuerzas Armadas. En el cumplimiento de sus labores, que prestan las 24 horas del día, coordinan la agenda protocolar del presidente –deben indicar discretamente el fin de una actividad o una audiencia para dar paso a la siguiente–, y reciben y tramitan las instrucciones que les entrega el mandatario. Son también los únicos -además del secretario privado- que tienen acceso directo al despacho presidencial, participan de las reuniones más reservadas y conocen con anticipación los nombramientos que hará el Presidente, porque ellos tendrán que ubicar después al funcionario elegido. En los actos oficiales llevan el discurso que pronunciará el presidente en una carpeta, pero también una copia en el bolsillo. Además, deben ser discretos y estar en condiciones de responder cualquier pregunta del presidente. Los tres edecanes se dividen el trabajo por semanas, quedando uno de turno cada semana, y deben cumplir sus funciones por dos años. En caso de que el presidente asista a una actividad en un recinto militar lo acompaña el edecán respectivo, y para las ceremonias de Estado concurren los tres.

Cada presidente una vez terminado su mandato puede ejercer otros cargos políticos. Algunos presidentes han tenido carreras significativas después de dejar el cargo. Tal es el caso de José Figueroa Alcorta que fue presidente de la corte suprema, siendo el único argentino en presidir los tres poderes, o el de Néstor Kirchner que fue secretario general de la UNASUR. Vicente López y Planes (Buenos Aires en 1852) y Justo José de Urquiza (Entre Ríos entre 1868 y 1870) ejercieron la gobernación de sus provincias natales. Bartolomé Mitre, Julio Argentino Roca, Hipólito Yrigoyen, Marcelo Torcuato de Alvear, Arturo Frondizi, Juan Domingo Perón, Raúl Alfonsín y Néstor Kirchner ejercieron el liderazgo de sus respectivos partidos políticos e incluso se presentaron a elecciones nuevamente, algunos reelegidos con éxito como Roca en 1898 y Perón en 1973, y otros presidentes sirvieron en el Congreso después de abandonar la Casa Rosada como Carlos Pellegrini y Carlos Menem.

Los exmandatarios y exmandatarias poseen tras terminar su mandato la protección vitalicia de la Policía Federal Argentina y perciben una asignación mensual vitalicia equivalente al sueldo de un juez de la Corte Suprema, según la Ley 24.018. Al fallecer, la pensión pasa a la viuda o viudo que cobrará el 75% pero tendrá que renunciar a toda pensión estatal. Para el goce de estos beneficios, deben residir dentro del territorio argentino. Arturo Illia y Raúl Alfonsín donaron todos sus años de jubilación a la caridad.

Tras sus fallecimientos, los presidentes reciben homenajes como decretar tres días de duelo nacional, y el funeral de estado, y sus familias donan sus pertenencias del mandato a museos como el Museo Casa Rosada (creado del antiguo Museo Presidencial Casa Rosada) cuya colección está conformada por objetos personales, retratos, esculturas y documentos de quienes han ocupado el cargo de «presidente» y objetos referentes al contexto social, económico y político de cada etapa presidencial, incluyéndose presidencias recientes, y el Museo Histórico Sarmiento.

Como modo de homenaje, toda persona que ejerció la primera magistratura es retratada en mármol de Carrara y depositada en el Hall de Honor de la Casa Rosada junto a todos los presidentes argentinos excepto los de facto, desde Cornelio Saavedra y Bernardino Rivadavia hasta Néstor Kirchner (los bustos que faltan son María Estela Martínez de Perón, Carlos Menem, Fernando de la Rúa, Adolfo Rodríguez Saá y Eduardo Duhalde. Cristina Fernández de Kirchner debería ser agregada a partir del año 2023). Los primeros bustos expuestos en ese salón fueron realizados entre 1883 y 1884, encargados por el presidente Julio Argentino Roca. Data de aquella época la tradición de agregar el busto de los primeros mandatarios luego de que finaliza su período de mandato. Estos bustos estuvieron ubicados inicialmente en los Recintos Presidenciales del primer piso, pero en 1973, durante la presidencia de Alejandro Lanusse, se decidió su traslado al Hall de Honor y se dictó el Decreto 4022, que rige la colocación de los Bustos Presidenciales, indicando que esto se hará una vez transcurrido un lapso no menor a dos períodos presidenciales, tras la finalización del mandato correspondiente. En 2016 los 28 bustos fueron reubicados en forma cronológica, además las esculturas de los presidentes "de facto" José F. Uriburu, Pedro Ramírez y Edelmiro Farrell fueron retiradas, y se agregó el de Miguel Juárez Celman, que estaba abandonado en un depósito.




</doc>
<doc id="8912" url="https://es.wikipedia.org/wiki?curid=8912" title="Carlos Menem">
Carlos Menem

Carlos Saúl Menem (Anillaco, La Rioja, 2 de julio de 1930) es un político y abogado argentino que fue presidente de la Nación Argentina por el Partido Justicialista desde 1989 hasta 1999. Desde 2005 es senador nacional por la provincia de La Rioja.

Cumplió su primer período presidencial de 1989 a 1995, luego de Raúl Alfonsín, e impulsó la reforma constitucional argentina de 1994, que disminuyó la extensión del mandato a cuatro años y habilitó una reelección presidencial inmediata. Esto le permitió presentarse como candidato y ser reelecto en 1995 para un segundo mandato, que se extendió hasta 1999, año en que fue sucedido por Fernando de la Rúa. Fue el presidente argentino con el mayor periodo continuado de gobierno, seguido por Juan Domingo Perón y Cristina Kirchner.

En las tres elecciones presidenciales para las cuales se presentó fue el candidato con mayor número de votos, pero en la tercera vez, en 2003, en que no alcanzó el mínimo necesario para ser elegido en la primera vuelta, desistió su candidatura al decidir no presentarse al balotaje.

Está condenado a 10 años de prisión de cumplimiento efectivo por el contrabando agravado de armas a Croacia y Ecuador cuando Argentina era garante del Tratado de Paz, y condenado a 4 años y 6 meses de prisión efectiva e inhabilitación perpetua para ejercer cargos públicos por el delito de peculado y el pago de sobresueldos. Menem no cumple la condena en prisión debido a que la misma fue apelada y aún resta que sea confirmada o dejada sin efecto por los tribunales superiores.

Nació en Anillaco, provincia de La Rioja. Sus padres, Saúl Menem (1898-1975) y Mohibe Akil (1907-1977), eran de origen sirio. Estudió abogacía en la Universidad Nacional de Córdoba. Salió al conocimiento público cuando defendió profesionalmente a presos políticos durante el gobierno militar de la Revolución Libertadora.

Fue detenido por primera vez en 1956 durante el gobierno del general Pedro Eugenio Aramburu. Al año siguiente fundó la Juventud Peronista de su provincia, mientras se desempeñaba como asesor legal de la Confederación General del Trabajo y de otros sindicatos de La Rioja. 

En 1973, con la vuelta de la democracia, fue electo gobernador de su provincia. Inicialmente mostró una adhesión a las posturas del peronismo revolucionario, que abandonaría a fines de 1973:

En marzo de 1976, tras el derrocamiento de la presidenta María Estela Martínez de Perón, fue depuesto y detenido por la Junta Militar que gobernó el país durante el "Proceso de Reorganización Nacional" y permaneció en esa situación durante cinco años, primero en la cárcel y luego confinado en una pequeña localidad de la provincia de Formosa, llamada Las Lomitas, hasta quedar en libertad en febrero de 1981.

En octubre de 1983, con el fin del régimen militar y el regreso de la democracia al país, Menem fue electo nuevamente como gobernador de La Rioja. 

En 1988 derrotó a Antonio Cafiero en las elecciones internas del Partido Justicialista y fue proclamado candidato a la Presidencia de la Nación por el Frente Justicialista Popular (FREJUPO), una coalición del justicialismo con otros partidos menores. En las elecciones del 14 de mayo de 1989 fue electo Presidente de la Nación con el 47 % de los votos, superando al candidato de la Unión Cívica Radical, Eduardo Angeloz (que obtuvo el 32,45 % en la lista de la Unión Cívica Radical más un 4,59 % en la lista de la Confederación Federalista Independiente, que lo llevaba también como candidato, o sea, 37,04 % en total), y sucedió al Presidente Raúl Alfonsín, al que debió reemplazar cinco meses antes de la finalización de su mandato debido a la crisis provocada por la hiperinflación que afectó al país.

Carlos Menem asumió la presidencia el 8 de julio de 1989, tras el retiró anticipado de Raúl Alfonsín. Fue por entonces la primera sucesión presidencial entre dos presidentes constitucionales desde 1928, y la primera desde 1916 entre presidentes de diferentes partidos políticos.

El principal problema que debió enfrentar al asumir la presidencia fue el de una economía en crisis con hiperinflación. El gobierno de Menem en un principio parecía plegarse a los principios del Consenso de Washington, aunque solo fueron aplicados finalmente los puntos 6 y 9 y parcialmente al 8 () al no cumplir con varios de los puntos propuestos tuvo un gran déficit fiscal, atraso cambiario, aumento del gasto público y una gran presión impositiva. Finalmente las reformas realizadas tuvieron un carácter mucho más liberal que lo que ya había, pero igualmente fueron insuficientes en la estabilidad a largo plazo. Con la aprobación de la Ley de Reforma del Estado fue autorizado a privatizar varias empresas estatales, en la forma que el presidente estimara conveniente. Las primeras privatizaciones efectuadas fueron las de la empresa telefónica Entel y la de Aerolíneas Argentinas. Las mismas, y otras posteriores, se privatizaron rápidamente buscando conseguir con ello réditos mediáticos que instalaran la idea de la voluntad reformista del gobierno, pero dicha rapidez condujo luego a numerosas críticas y denuncias de irregularidades, omisiones y casos de corrupción. Pronto se privatizaron también la red vial, los canales televisivos (con la excepción de ATC, hoy Canal 7), gran parte de las redes ferroviarias, Yacimientos Petrolíferos Fiscales y Gas del Estado.
Se desreguló la economía, reduciendo cupos, aranceles y prohibiciones de importaciones, y se estableció la libertad de precios. Con el aumento de impuestos como los del Valor Agregado y Ganancias se aumentó la recaudación fiscal. Aun así, a pesar de dicho aumento y de los ingresos generados por las privatizaciones, la situación económica se mantenía convulsionada y a fines de 1989 se produjo una segunda hiperinflación. El ministro de economía de entonces, Erman González, apeló al Plan Bonex: confiscó los depósitos a plazo fijo y los cambió por bonos de largo plazo en dólares. Asimismo, restringió fuertemente la emisión monetaria.

Durante la gestión de Domingo Cavallo, ministro de Economía de su gobierno, se estableció la Ley de Convertibilidad, cuya aplicación se prolongaría hasta la crisis argentina de fines de 2001 y comienzos de 2002. El Banco Central de la República Argentina estaba obligado a respaldar la moneda argentina con sus reservas en una relación de cambio en la que un dólar equivalía a un peso. De esta forma se restringía la emisión de billetes como medio de financiamiento del Estado. Esta ley de carácter intervencionista contradice en gran parte las políticas aceptadas como neoliberales, al fijar un precio libre de la economía con una ley.

Estas medidas lograron una estabilidad económica sin inflación significativa que ofreció un clima favorable para el surgimiento de inversiones y el ingreso de capitales desde otros países, produciéndose un marcado crecimiento del PBI. Ese aumento estuvo caracterizado por el incremento del sector servicios, mientras el PBI industrial se contraía y se privatizaba la economía. La estabilidad económica fue, entonces, sólo aparente, ya que la capacidad de emplear mano de obra disminuía y se cerraban incontables establecimientos industriales. Durante su gobierno la deuda externa pública se multiplicó desde los 45.000 millones que había dejado el gobierno de Alfonsín, hasta llegar finalmente en el 2000 a 145.000 millones. En los servicios públicos las privatizaciones produjeron mejoras de calidad en algunos rubros (electricidad, telefonía), mientras que en otros el impacto fue negativo, como en los transportes ferroviarios, este último en particular por el cierre masivo de los servicios de pasajeros de larga distancia, ocurrido el 10 de marzo de 1993. Si bien los servicios de trenes privatizados urbanos del área metropolitana y cargas en general registraron leves mejorías, finalmente con la crisis de 2001 y la devaluación, desnudaron las frágiles condiciones contractuales que llevaron a las empresas a la quiebra, el posterior vaciamiento de su infraestructura y finalmente a un deterioro del servicio en parte sostenido por subsidios. Al mismo tiempo, los principales inconvenientes económicos generados por esta política fueron una disminución de la competitividad basada en el tipo de cambio y un crecimiento del desempleo.

Al asumir Menem el gobierno, los valores de desocupación y subocupación habían alcanzado picos históricos (8,1% y 8,6% de la población económicamente activa, respectivamente). Luego de un período de lenta disminución (6,9% y 8,3% en mayo de 1992), el desempleo y el subempleo volvieron a crecer durante la "crisis del Tequila", hasta alcanzar un pico de 18,4% y 11,3% en mayo de 1995, tras lo cual bajaron levemente hasta 12,4% y 13,6% en octubre de 1998. Para el final de su gobierno, estas cifras eran de 13,8% y 14,3%. Contribuyeron al aumento del desempleo y el subempleo los despidos masivos en las empresas públicas privatizadas, la terciarización de actividades y las sucesivas medidas de flexibilización laboral.

En política exterior, desde el inicio mismo de su mandato se promovió un alineamiento automático con los Estados Unidos, de modo tal que la Argentina abandonó el Movimiento de Países No Alineados. El ministro Guido Di Tella se refirió a dicho alineamiento en forma humorística como las "relaciones carnales", pero más adelante el término sería tomado por los críticos de dicha política internacional para referirse a ésta en forma denigratoria.

Poco después de la implementación del Plan Bonex tuvo lugar el Swiftgate, en el cual la empresa estadounidense Swift denunció verse perjudicada en una operación comercial al no aceptar otorgar un soborno. Swift recurrió al embajador de los Estados Unidos, Terence Todman, y el propio gobierno estadounidense tomó cartas en el asunto. Finalmente, a principios de 1991 renunció todo el gabinete menemista. Fue durante dicho escándalo que José Luis Manzano pronunció su difundida frase "Yo robo para la corona".

En otros planos, en 1991 Menem promovió la formación del Mercosur y restableció relaciones diplomáticas con el Reino Unido, interrumpidas desde la Guerra de Malvinas. Suspendió el servicio militar obligatorio tras el escándalo a raíz del Caso Carrasco. Indultó a militares de la anterior dictadura (1976-1983) y a militantes de organizaciones guerrilleras que habían actuado principalmente durante la década del setenta, lo que sin embargo no calmó el descontento de los militares que amenazaron después con otro intento de golpe. 

Durante su gobierno, se modificó por ley del Congreso el número de integrantes de la Corte Suprema de Justicia, elevándolo a nueve miembros. Parte de la prensa denominó a esta corte ampliada la "mayoría automática", aduciendo que en la mayor parte de los casos polémicos los votos de estos cinco jueces coincidían con la posición del gobierno.

Durante su gestión la Argentina fue blanco de dos ataques terroristas: el 17 de marzo de 1992 se produjo el primer atentado contra la embajada de Israel, donde murieron 29 personas y el 18 de julio de 1994 otra explosión de una bomba en el edificio de la AMIA (Asociación Mutual Israelita Argentina) provocó 85 muertos. Distintas fuentes, entre ellas dirigentes de la comunidad judía, afirmaron que el atentado fue perpetrado por organizaciones fundamentalistas islámicas con sede en el Líbano, bajo la organización de Irán, y acusaron al presidente de desviar la investigación que conduciría a la responsabilidad de ese país. Un ex miembro de la inteligencia iraní aseguró que Menem recibió dinero para desvincular a ese país del ataque. En el 2004 un tribunal federal comprobó que el juez que hacía 10 años investigaba la causa, Juan José Galeano, habría sobornado, siguiendo instrucciones del gobierno de Menem, a uno de los inculpados para que incriminara a oficiales de la policía bonaerense. Más aún, en junio de 2006, Hugo Anzorreguy, jefe de la Secretaría de Inteligencia del Estado (SIDE) durante el gobierno menemista, manifestó ante el juez federal que Menem había ordenado dicho soborno utilizando dinero de los fondos públicos. Hasta la fecha la investigación no se ha completado.

Las denuncias de corrupción sobre su gobierno no impidieron que su gestión mantuviera una imagen favorable debido al éxito en la faz económica. En 1993, su Ministro del Interior, Gustavo Béliz, renunció a su cargo y declaró públicamente que el presidente "estaba rodeado de corruptos". Cuando Menem anunció su propósito de hacer aprobar una ley que declarara la necesidad de convocar a una convención constituyente que modificara la Constitución Nacional y permitiera su reelección, se planteó un conflicto interpretativo respecto del artículo 30 de dicha Constitución. Menem sostenía -al igual que lo había hecho Perón en 1949- que la mayoría de dos tercios de los miembros del Congreso requerida para ello debía calcularse sobre los que estuvieran presentes en la sesión, en tanto la oposición sostenía -tal como lo había hecho la Unión Cívica Radical en 1949- que se necesitaban dos tercios del número de representantes que integraba cada Cámara. El punto era crucial porque los partidarios de la reforma no alcanzaban la mayoría necesaria si se aplicaba esta última interpretación.

Menem presionó con utilizar la interpretación que lo favorecía e incluso convocó a un plebiscito no vinculante para que la ciudadanía opinara sobre la reforma, pero finalmente entró en negociaciones con el líder de la oposición radical, Raúl Alfonsín, quien aceptó que se convocara a una Convención para la reforma de la Constitución Nacional que incluyera, además del tema de la reelección, otras cláusulas que le interesaban, tales como la elección de un tercer senador por la minoría, el establecimiento de la figura del Jefe de Gabinete y la incorporación a la Constitución de diversos convenios internacionales, entre otros puntos, todo lo cual configuró el llamado Pacto de Olivos. La reforma fue aprobada por la convención en 1994 y permitió la reelección de Menem al año siguiente.


El 14 de mayo de 1995 se realizaron las elecciones presidenciales que arrojaron el siguiente resultado:

En su segundo mandato, Menem mantuvo las políticas económicas de su primera etapa de gobierno. Esta vez, sin embargo, el comienzo de una recesión en el tercer trimestre de 1998 y nuevas acusaciones de corrupción tuvieron como consecuencia un descenso en su popularidad: luego de un nuevo intento de reforma constitucional —esta vez fallido—, Menem terminó su gobierno el 10 de diciembre de 1999 traspasándole el mando al presidente electo Fernando de la Rúa.El 3 de noviembre de 1995 explotaron los depósitos de la Fábrica Militar de Río Tercero. Murieron siete personas y se produjeron daños materiales en la ciudad. Se sospechó que en realidad no fue un accidente, y que lo que se buscaba era ocultar un faltante de armas (ver sección Vida pública después de sus presidencias).

El hijo del presidente, Carlos Menem Jr., «"Carlitos"», murió el 15 de marzo de 1995 junto al corredor de autos Silvio Oltra durante un viaje en helicóptero, a los 26 años de edad. Los peritos determinaron que el aparato cayó al embestir cables de alta tensión pero su madre, Zulema Yoma, insistió siempre en que su hijo había sido atacado por proyectiles y que el propio gobierno estaba ocultando las pruebas del hecho pues, según su versión, la muerte de su hijo fue planeada por el entorno del presidente.

Al principio el presidente no apoyó la teoría de su esposa y, poco después de la muerte de su hijo, Zulema se divorció de él, de quien ya estaba separada de hecho. Luego se presentó como querellante en la causa, abandonando la teoría del accidente.

La causa fue archivada el 16 de octubre de 1998 por el Juez Villafuerte Ruzo, al considerar que se trató de un accidente, pues la nave se estrelló luego de golpear con cables de alta tensión; pero ante el recurso de Zulema fue reconsiderada por la Corte Suprema, que en abril de 2001 decidió rechazar el recurso para reabrirla.

En 2010 la causa fue reabierta. El 8 de julio de 2014 el expresidente Carlos Menem realizó una presentación por escrito en la que manifestó: "Luego de indagar y estudiar los hechos y circunstancias que rodean la causa –aunque inicialmente no fue así-, llegué a la conclusión de que la caída del helicóptero, y la consecuente muerte de mi hijo, fue el resultado de un atentado".

Además de Zulema Yoma. varios sectores de la opinión pública también sospechan que no se trató de un accidente, como se insistió en un principio, sino de un posible ajuste de cuentas o venganza por acuerdos político-mafiosos no cumplidos. Se basan en que el desguace del helicóptero se hizo inmediatamente, sin posibilidad de un nuevo peritaje; en que se produjeron varias muertes por asesinato o causas poco claras de 14 personas relacionadas con la investigación (entre testigos, investigadores y peritos); y en la falta de medidas concretas por parte del gobierno para esclarecer el caso.

En mayo de 2001 contrajo matrimonio con la ex modelo (Miss Universo 1987) y conductora televisiva chilena Cecilia Bolocco, con la cual inició los trámites de divorcio en febrero de 2007. Con ella tiene un hijo: Máximo Menem Bolocco, quien nació en 2003. 
El 7 de junio de 2001 fue detenido por una acusación de tráfico de armas a Ecuador y Croacia durante su gobierno (en 1991 y 1996) y quedó bajo arresto domiciliario hasta el 21 de noviembre del mismo año, cuando la Corte Suprema emitió un fallo absolutorio en su favor. Se le acusaba de haber desviado el destino de armas que, según los decretos por él firmados, iban a ir a Panamá, Venezuela y Bolivia, pero terminaron en Ecuador y Croacia. El desvío de armas a Ecuador iba en contra del impedimento que tenía la Argentina de vendérselas, por ser garante de la paz entre ese país y el Perú, que se encontraban en medio de un conflicto bélico, en el Tratado de Río de Janeiro, y a pesar del apoyo incondicional, político y logístico, que el Perú le brindó a la Argentina en la Guerra de las Malvinas. En el caso de Croacia, el envío de armas estaba vedado por el embargo de la ONU en el conflicto que dividió a Yugoslavia. Durante el año 2007 volvió a ser procesado por el juez Rafael Caputo, aunque disponía de la protección de los fueros parlamentarios por ser Senador de La Rioja.
En octubre de 2008 se le inició un juicio oral al respecto. Sus abogados solicitaron el 3 de febrero de 2008 la anulación del juicio oral. El 13 de septiembre de 2011 fue absuelto por el Tribunal Oral en lo Penal Económico 3. La decisión se tomó por la mayoría, con la sola disidencia del magistrado Gustavo Losada. El fiscal anunció que apelaría la sentencia. El 8 de marzo de 2013, la Cámara Federal de Casación Penal revocó la absolución del expresidente y actual senador y lo condenó como coautor de contrabando a Croacia y Ecuador. También fue sentenciado Oscar Camilión. Posteriormente, el Tribunal de Primera Instancia resolverá las penas.
También se le imputó la posesión de cuentas bancarias no declaradas en bancos suizos. En esta causa fue absuelto tras un dictamen de la jueza suiza Christine Junod en el que se declaraba que no poseía ninguna cuenta bancaria en ese país.

En cuanto a la causa de la voladura de la fábrica de Río Tercero, ocurrida a poco de comenzar su segunda presidencia, la cámara federal de Córdoba anuló su indagatoria en la investigación sobre las causas de ese suceso en noviembre de 2008, a 13 años de ocurrido. La razón por la cual se lo desvinculó de la causa es que el entonces fiscal Carlos Stornelli nunca solicitó la indagatoria del ex mandatario.

La noche del 21 de noviembre de 2001, el mismo día en que fue liberado del arresto domiciliario al que estuvo sometido por la causa de la venta ilegal de armas a Ecuador, Menem lanzó su candidatura presidencial en La Rioja, junto a su entonces esposa, Cecilia Bolocco.

En enero de 2003, los justicialistas esperaban dirimir en su interna qué candidato los representaría en las próximas elecciones generales. No obstante, el Congreso Nacional del Partido Justicialista, reunido el 24 de enero en el miniestadio de Lanús, anuló las internas partidarias y aprobó el sistema de "neolemas" mediante el cual autorizó a Carlos Menem, Adolfo Rodríguez Saá y Néstor Kirchner a participar directamente en la elección general convocada para el 27 de abril. Estas nuevas reglas de participación electoral fueron impulsadas por el presidente Eduardo Duhalde, quien impulsaba la candidatura de Kirchner, entonces gobernador de Santa Cruz. Menem, en cambio, quería que hubiera internas. En tanto, la jueza electoral María Servini de Cubría aceptó la apelación presentada por el duhaldismo a la resolución que prohíbe el uso de neolemas para dirimir la fórmula presidencial del Partido Justicialista. Menem no pudo revertir la decisión partidaria mediante la vía judicial.

La fragmentación electoral del peronismo dispersaba el voto dando más posibilidades a un balotaje. En las elecciones del 27 de abril iban a presentarse 19 candidatos, provenientes de un peronismo dividido en tres fracciones.

En las mismas, Menem, candidato de la alianza "Frente por la Lealtad - Ucede", se ubicó en el primer lugar, con 4.740.907 de votos (24,45%), y Kirchner, del "Frente para la Victoria", lo secundó, con 4.312.517 de votos (22,24%). Tercero se ubicó el ex ministro de Economía radical Ricardo López Murphy, encabezando un frente disidente, con 3.173.475 votos (16,37%); cuarto Adolfo Rodríguez Saa, con 2.735.829 votos (14,11%), y quinta Elisa Carrió. El candidato de la U.C.R., Leopoldo Moreau, quedó relegado a uno de los últimos lugares, con el 2,34% de los votos.

Como ninguno de los candidatos obtuvo la mayoría necesaria para imponerse en la primera ronda, se llegó a segunda vuelta. La reforma de 1994 de la Constitución argentina prescribe, en los artículos 94 y 96, que se debe realizar el balotaje si el triunfador obtiene menos del 45% de los votos y hay una diferencia menor de diez puntos porcentuales con el segundo candidato.

Se preparó entonces una nueva elección en segunda vuelta entre los dos candidatos más votados (el propio Menem y el gobernador Néstor Kirchner), para el 18 de mayo de 2003. Menem decidió renunciar a su candidatura al considerar que la ventaja en votos de su rival era irreversible; de este modo, terminó siendo víctima del mecanismo ideado por su propio equipo. Por aplicación del artículo 155 del Código Electoral Nacional argentino, Ley n° 19.945, que estipula que ""en caso de renuncia de los dos candidatos de cualquiera de las dos fórmulas más votadas en la primera vuelta, se proclamará electa a la otra"", al retirarse del balotaje la fórmula Menem-Romero, quedó proclamada la fórmula Kirchner-Scioli.

El 23 de octubre de 2005 se presentó a elecciones para Senador Nacional por su provincia y obtuvo la banca correspondiente a la minoría; los dos escaños por la mayoría fueron ganados por la fracción del presidente Néstor Kirchner, liderada a nivel local por el ex gobernador Ángel Maza. De esta forma, Menem volvió a ocupar un cargo público exactamente seis años después de dejar la Presidencia.

En mayo de 2007, Menem se presentó como opositor al presidente Néstor Kirchner, al que ha denostado públicamente en reiteradas ocasiones. Según su opinión, Kirchner no aplica una política imparcial con respecto al juzgamiento de militares y civiles acusados de haber cometido actos de violación de los derechos humanos en las décadas del 70 y 80, ya que sólo pretende juzgar a los cabecillas de la última dictadura militar, pero no a los militantes de organizaciones guerrilleras.

Se presentó como candidato a gobernador de La Rioja el 19 de agosto de 2007, pero fue derrotado: obtuvo cerca del 22% de los votos, y quedó tercero detrás de Luis Beder Herrera y de Ricardo Quintela. Antes de esta última derrota, Menem se presentaba públicamente como candidato a la presidencia para las elecciones de octubre de 2007. Finalmente, desistió de postularse.
En el año 2008 la empresa Siemens AG manifestó que entre 1998 y 2004 había pagado sobornos a distintos funcionarios, entre los cuales estaba Menem, a fin de obtener un contrato multimillonario con el Estado para fabricar los documentos nacionales de identidad, lo cual fue negado por el imputado.

El 17 de julio de 2008, la figura del Senador Menem fue de gran importancia en la sesión sobre la Resolución 125, relativa a la imposición de retenciones móviles a la exportación de productos agropecuarios, en la cual se dirimió el conflicto que enfrentaba al Gobierno Nacional con el sector agrícola-ganadero. Durante el debate, que se extendió por dieciocho horas, el expresidente estuvo en gran parte ausente a causa de una internación de urgencia por una fuerte neumonía. Esto dio lugar a numerosas especulaciones, ya que con su ausencia votarían 71 de los 72 senadores, permitiéndole al oficialismo vencer por 36 votos contra 35 de la oposición. Finalmente, pasada la medianoche, el Senador se hizo presente en el recinto, pronunció un fuerte discurso en contra de las retenciones móviles, anunció públicamente su proyecto ingresado por mesa de entradas y anticipó que votaría en contra del proyecto oficialista. Esto fue calificado como un acto de coraje cívico por el ex Secretario General de la Presidencia, Alberto Kohan. Finalmente, la sesión terminó empatada con 36 votos a favor e idéntico número en contra, y debió desempatar el entonces Vicepresidente, Julio César Cleto Cobos.

Menem incursionó en el automovilismo deportivo, compitiendo asiduamente en Rally en los años 1980 con un Peugeot 504 primero y luego con un Renault 18, el que luego le cedió a su amigo Juan María Traverso.
Después de asumir el cargo como presidente y durante el ejercicio de su mandato, Menem dedicó especial atención a los medios de difusión, con lo cual aumentó su protagonismo público y su perfil mediático.
Según algunos medios periodísticos, en oportunidad de brindar una conferencia de prensa, Menem habría afirmado que entre sus lecturas se hallaba una obra escrita por Sócrates. El error de Menem habría consistido en que se desconoce que exista algo escrito por dicho filósofo.

En otra ocasión, al dar comienzo al ciclo lectivo de 1996 en Tartagal (Salta), informó a los niños de una humilde escuelita del empobrecido Norte argentino que se licitaría un sistema de vuelos estratosféricos ""desde una plataforma que quizás se instale en la provincia de Córdoba. Esas naves espaciales van a salir de la atmósfera, van a remontar a la estratósfera y desde ahí elegir el lugar donde quieran ir, de tal forma que en una hora y media podamos, desde Argentina, estar en Japón, en Corea o en cualquier parte"". ""Parece un sueño, pero está en marcha. No me lo contaron: yo recibí a los empresarios de la compañía de aviación estadounidense Lockheed»"". Como la compañía aeronáutica Lockheed había sido directamente involucrada, su representante, Harry Radclifee, explicó el proyecto en los siguientes términos:

""El presidente se refería a un programa actualmente en desarrollo en EEUU. Es un programa de la NASA que ahora está en la fase uno. Es la fase preliminar, la de diseño y desarrollo"".

Radcliffe indicó que los primeros vuelos espaciales servirían para colocar satélites en el espacio con fines científicos y que a partir del año 2005 «podría haber vuelos con pasajeros».

Otro proyecto impulsado por Menem y duramente criticado por la prensa fue el de construir una "Aeroísla", una isla artificial en el Río de la Plata a la cual sería trasladado el Aeroparque Jorge Newbery. El proyecto, apoyado por Álvaro Alsogaray, nunca prosperó.

Indultó a responsables del Terrorismo de Estado en Argentina en las décadas de 1970 y 1980, y a integrantes de grupos guerrilleros como el ERP y Montoneros; condecoró a Augusto Pinochet e intentó implantar la pena de muerte. Cuando en 1993 renunció su ministro del Interior, Gustavo Béliz, este lo acusó de elegir en lugar de "funcionarios honestos" a "alcahuetes y mediocres".

En la actualidad encabeza el partido "Lealtad y Dignidad", junto a Mercedes Landa.

Está condenado a 7 años de prisión de cumplimiento efectivo por el contrabando agravado de armas a Croacia y Ecuador cuando Argentina era garante del Tratado de Paz, El 13 de junio de 2013 el Tribunal Oral en lo Penal Económico Nº 3 condenó al expresidente Carlos Menem a 7 años de prisión de cumplimiento efectivo por la causa de tráfico de armas a Croacia y Ecuador.
Por su parte, el ex ministro de Defensa Oscar Camillión fue condenado a 5 años de prisión.
En 2011, los 12 condenados habían sido absueltos en un juicio oral. Más adelante, en marzo, la Cámara Federal de Casación Penal revocó ese fallo y condenó a 12 acusados. Así se convirtió en el primer expresidente de la Argentina en ser condenado a prisión.

El 1 de diciembre de 2015, el Tribunal Oral Federal 4 condenó a Menem a 4 años y seis meses de prisión por el delito de peculado e inhabilitación perpetua para ejercer cargos públicos.

Su ministro de economía Domingo Cavallo también fue hallado culpable, condenado a 3 años y 6 meses de prisión, inhabilitación perpetua para ejercer cargos públicos, y a un decomiso de 220.868 pesos.
El tribunal también encontró culpable al ex ministro de Justicia Raúl Granillo Ocampo, por el cobro de sobresueldos, le impuso una pena de tres años y tres meses de cárcel y el decomiso de 1.350.000 pesos.

Menem no cumple la condena en prisión debido a sus fueros como senador.




</doc>
<doc id="8915" url="https://es.wikipedia.org/wiki?curid=8915" title="Copa Mundial de Fútbol">
Copa Mundial de Fútbol

La Copa Mundial de la FIFA, también conocida como Copa Mundial de Fútbol, Copa del Mundo o simplemente Mundial, cuyo nombre original fue el Campeonato Mundial de Fútbol, es el principal torneo internacional oficial de fútbol masculino a nivel de selecciones nacionales en el mundo. Además existen otras competiciones que también son copas mundiales de fútbol, entre las que destacan la Copa Mundial Femenina de Fútbol, con sus respectivas categorías con límite de edad, la Copa Mundial de Fútbol Sub-20 y la Copa Mundial de Fútbol Sub-17, todas organizadas por la FIFA.

Este evento deportivo se realiza cada cuatro años desde 1930, con la excepción de los años de 1942 y 1946, en los que se suspendió debido a la Segunda Guerra Mundial. Cuenta con dos etapas principales: un proceso clasificatorio en el que participan en la actualidad cerca de 200 selecciones nacionales y una fase final realizada cada cuatro años en una sede definida con anticipación en la que participan 32 equipos durante un periodo cercano a un mes; Para la elección del Mundial del 2026, el torneo será el primero en incorporar 48 equipos, luego de que la FIFA aprobase la expansión de los habituales 32 equipos en el mes de enero de 2017. La fase final del torneo es el evento deportivo de una sola disciplina más importante del mundo (la final de la Copa Mundial de Fútbol de 2002 fue vista por más de 1100 millones de personas), y el segundo más importante a nivel general después de los Juegos Olímpicos.

La Copa Mundial de la FIFA ha sido realizada en 20 ocasiones, en las que ocho países han alzado la copa: es el equipo más exitoso, con cinco victorias; e le siguen con cuatro trofeos; y la han ganado dos veces, en tanto que , y se han titulado campeones en una sola ocasión. El torneo presenta un fuerte dominio de los equipos europeos y sudamericanos: los primeros ganaron el título en 11 ocasiones, mientras que los sudamericanos lo ganaron 9 ocasiones y solo dos equipos de otras confederaciones geográficas han llegado a semifinales: en 1930 y en 2002. Además, Brasil (en 1958 y 2002), España (en 2010) y Alemania (en 2014) han sido los únicos equipos que han ganado fuera de su continente.

El primer encuentro internacional de este deporte se remonta al partido disputado entre y el 30 de noviembre de 1872. El fútbol en ese tiempo era prácticamente desconocido fuera de las islas Británicas, pero lentamente comenzó a desarrollarse en otras partes del mundo. El fútbol debutó como un deporte de demostración en los Juegos Olímpicos de París 1900, experiencia repetida en Saint Louis 1904 y los Juegos Intercalados de 1906 en Atenas. 

El 21 de mayo de 1904 se fundó la Federación Internacional de Asociaciones de Fútbol (FIFA por sus siglas en francés) con el fin de organizar el desarrollo del deporte. Dentro de sus ideas originales surgió la posibilidad de realizar en 1906 un torneo internacional en Suiza, pero finalmente la propuesta fracasó. Sin embargo, la idea se mantuvo y se concretó cuando en Londres se organizaron los IV Juegos Olímpicos en 1908 y se declaró al fútbol como deporte olímpico oficial. A cargo de la Football Association (no afiliada a la FIFA aún, pero con quien mantenía una estrecha relación), el primer lo ganó el , seguido por y los . 

Con el paso de los años el torneo olímpico de fútbol se mantuvo, pero como un evento amateur. En 1909 Sir Thomas Lipton organizó un torneo profesional entre clubes que representaba a cada país en la ciudad de Turín. Este torneo se denomina a veces como la «primera Copa Mundial». En 1914 la FIFA reconoció el torneo olímpico como un «campeonato mundial de fútbol para amateurs» y decidió hacerse responsable del desarrollo de dicho evento. Tras la Primera Guerra Mundial se realizó el primer torneo intercontinental en los 1920, donde participaron 13 equipos europeos junto al seleccionado de .

En los , el primero organizado por la FIFA, se integraron los equipos sudamericanos. En dicho evento, se coronó campeón, revalidando su título cuatro años más tarde, en Ámsterdam 1928.

Durante los Juegos Olímpicos de 1928, la FIFA organizó un congreso donde se decidió finalmente la realización de un torneo de fútbol profesional de nivel internacional en 1930. Inmediatamente varios países europeos presentaron su candidatura (Italia, Hungría, los Países Bajos, España y Suecia) junto a la de Uruguay. Jules Rimet, presidente de la FIFA en esos años, estaba a favor de la realización en el país sudamericano, tanto por sus éxitos deportivos como porque el país celebraría el centenario de la Jura de la Constitución.

Finalmente, Uruguay salió electo por unanimidad, pero eso no implicó el apoyo europeo a la realización del torneo fuera de su continente. Los países europeos invitados al torneo rechazaron su participación argumentando que no podían costear el largo viaje transatlántico en medio de la crisis económica que había azotado al mundo en esos años. A pesar de que Uruguay se ofreció a solventar los costos, solo , , y acudieron a la cita. Tras el boicot, los organizadores debieron disminuir el número de participantes en el torneo, de 16 a 13.

A pesar de las complicaciones iniciales, el torneo fue un éxito. Para el torneo, la intención de los organizadores era que todos los partidos se disputaran en un solo estadio, el Estadio Centenario, construido especialmente para la celebración de la Copa Mundial y como celebración del centenario de la independencia uruguaya. Fue diseñado por Juan Scasso y Rimet lo llamó el "templo del fútbol". Con una capacidad para 90.000 espectadores, era el mayor estadio del mundo fuera de las Islas Británicas. Sin embargo, las fuertes lluvias acaecidas en Montevideo antes de la inauguración del campeonato impidieron que su construcción fuera finalizada a tiempo. Dada esta situación los organizadores se vieron obligados a buscar otros estadios para celebrar en ellos los primeros partidos, el Gran Parque Central y el Estadio Pocitos, escenarios donde se jugaron de manera simultánea los dos primeros partidos en la historia de la Copa Mundial. El Estadio Centenario fue oficialmente inaugurado el sexto día de competición y a partir de ese momento todos los partidos se jugaron ahí. Finalmente, los equipos del Río de la Plata avanzaron a la final, y se enfrentaron el 30 de julio de 1930 en el recién inaugurado Estadio Centenario. Tras ganar el encuentro por 4:2, los locales se coronaron como los primeros campeones mundiales de fútbol ante 93.000 personas.

Italia organizó la segunda Copa Mundial en 1934. Como respuesta al boicot realizado en 1930 por los países europeos, Uruguay y otros países americanos se retiraron del torneo. La Copa Mundial se había convertido en muy poco tiempo en un gran acontecimiento que recibía las miradas de todo el mundo, por lo que el caudillo fascista Benito Mussolini usó el torneo para la exaltación del nacionalismo, buscando publicitar el poder italiano con una victoria en la competición. Para ello no dudó en asegurar la naturalización de varios jugadores argentinos, como Luis Monti, Raimundo Orsi, Enrique Guaita y Attilio Demaría, y también del brasileño Anfhiloquio Marqués Filo, italianizado como Anfilogino Guarisi. Italia llegó a la final del torneo donde se enfrentó a . Tras una serie de errores arbitrales, Angelo Schiavio anotó el gol del triunfo italiano durante la prórroga, que coronó a Italia como campeona del mundo. Varios jugadores de aquel equipo reconocieron haber jugado la final bajo amenazas del "Duce". El naturalizado Monti declaró:

En los años posteriores el advenimiento de la Segunda Guerra Mundial se hacía cada vez más presente. La Copa Mundial de Fútbol de 1938 realizada en Francia contó con las deserciones de , debido a la Guerra Civil, y , debido al estallido de la Segunda Guerra Sino-japonesa, mientras la clasificada no participó en el torneo al ser incorporada a Alemania tras el "Anschluss". Ya en el torneo propiamente tal, fue repudiada por el público mientras los jugadores realizaban el saludo nazi. Además, los equipos americanos (a excepción de y ) nuevamente boicotearon el torneo, luego de que fuera otorgada la sede a un país europeo a pesar del compromiso inicial de alternar la sede entre ambos continentes.

En el ámbito deportivo, Italia mostró su capacidad ofensiva llegando a la final del torneo tras derrotar a Brasil de Leônidas, una de las figuras del torneo. Los italianos se enfrentaron a la potente y la derrotaron con 4:2, convirtiéndose en el primer equipo en alcanzar el bicampeonato. Mussolini, al igual que en 1934, no estaba dispuesto a ver perder a su equipo. El seleccionador italiano, Vittorio Pozzo, recibió un telegrama antes del partido final en el que solo podía leerse «Vencer o morir». Además, obligó a sus jugadores a vestir para la final camisetas negras, símbolo del fascismo italiano.

Para el torneo de 1942, Argentina, Brasil y la Alemania nazi presentaron sus candidaturas, pero tras el inicio de la Segunda Guerra Mundial la FIFA decidió la suspensión de todos los eventos mientras el conflicto perdurase, provocando la cancelación de los torneos de y . En ese último año, la FIFA decidió que la Copa Mundial fuera reanudada tan pronto como fuera posible. Como la mayoría de los países europeos estaban devastados por la guerra, ninguno tenía la capacidad para organizar el torneo, por lo que Brasil presentó su candidatura y salió electo por la FIFA para realizar la Copa Mundial de Fútbol de 1950. 
Diversos países se retiraron del torneo, incluida la (por pretender jugar con futbolistas descalzos) y , el múltiple campeón de Sudamérica durante la década de 1940 por decisión interna, reduciendo el número de participantes de 16 a 13. Sin embargo, el evento marcó el ingreso por primera vez de los diversos equipos del Reino Unido a los procesos clasificatorios. Así, participó por primera vez en la Copa Mundial, mas quedó eliminada rápidamente a pesar de su favoritismo. Tras la primera ronda, , , y se clasificaron a un grupo final de donde saldría el campeón del torneo. El seleccionado brasileño derrotó por sendas goleadas a los equipos europeos, por lo que su victoria parecía asegurada. En el último partido, Brasil se enfrentó a Uruguay, que había tenido una irregular actuación, con una victoria sobre Suecia y un empate ante España. Por lo tanto, aunque no se tratase de una final, el campeón saldría de ese último partido, en el que a Brasil le bastaba un empate. Todo estaba listo en el Estadio Maracaná para las celebraciones del triunfo brasileño ante cerca de 175.000 espectadores, los diarios locales ya habían anunciado el partido como el de la primera victoria mundial de Brasil. Empero, los uruguayos lograron derrotar a los brasileños y coronarse campeones, después de remontar un 1:0 inicial, para acabar con un 1:2. El llamado "Maracanazo" es considerado como una de las más grandes sorpresas en la historia del deporte. En el otro partido, Suecia venció a España con 3:1 obteniendo el tercer puesto y dejando a los ibéricos en cuarta posición.
En 1954, la Copa Mundial regresó a Europa cuando Suiza, país neutral durante la guerra, fue la sede de la V Copa Mundial. Durante el desarrollo del torneo se produjeron tres de los partidos más recordados en la historia de la competición. En los cuartos de final, el "Equipo de oro", nombre con el que se conocía al equipo de , se enfrentó a la selección brasileña, que después del "Maracanazo" decidió cambiar el color blanco de su camiseta por el actual amarillo con ribetes verdes. El partido, que enfrentó a dos de las mejores escuadras del torneo, se convirtió en uno de los encuentros más infames de la historia: la excesiva violencia hizo que fuera conocido tradicionalmente como la "Batalla de Berna", en la que participaron tanto jugadores como entrenadores. En la misma ronda, derrotó a por 7:5, en el encuentro con mayor número de goles anotados en la historia. La final se disputó el 4 de julio de 1954 en el Wankdorfstadion, entre los húngaros, que vencieron en el alargue a en lo que fue la primera derrota uruguaya en los mundiales, y la , equipo que regresaba al torneo después de la prohibición establecida tras la derrota germana en la Segunda Guerra Mundial. Ambos equipos se habían enfrentado en la primera ronda y los magiares habían goleado 8:3 a sus rivales, por lo que una victoria de la Alemania Occidental parecía imposible. Sin embargo, pese a que a los 8 minutos de haber comenzado el encuentro los húngaros empezarían ganando por 0:2, los alemanes alcanzaron la victoria remontándolo con un 3:2 derrotando al combinado liderado por Ferenc Puskás, y alzaron por primera vez el trofeo Jules Rimet. El encuentro conocido como el "Milagro de Berna" se considera como uno de los hechos que marcaron el fin del período de posguerra de Alemania y su renacer, como también uno de las mayores sorpresas en la historia de la competición. Prueba de ello es la película sobre el partido, titulada "El milagro alemán".

Suecia fue el país destinado a realizar la Copa Mundial de Fútbol de 1958. El torneo fue el primero en ser transmitido a través de la televisión, dando así inicio a la expansión del torneo hacia otros continentes. En el ámbito deportivo, la de Just Fontaine alcanzaría el tercer lugar del torneo tras ser derrotados en semifinales por . Los sudamericanos se enfrentarían en la final al , en el Estadio Råsunda de Estocolmo. Pelé era la gran promesa brasileña, pero eran pocos los que lo conocían. Durante una serie de partidos de preparación frente a clubes italianos previos al inicio del mundial, Pelé sufrió una lesión de rodilla. Estuvo cerca de abandonar la delegación brasileña, pero finalmente acudió a Suecia, donde no pudo debutar hasta el partido de cuartos de final frente a . Suyo fue el único gol del partido, y en semifinales frente a Francia anotó un total de tres. En la final, con un marcador de 5:2, Brasil se coronó campeón del mundo por primera vez en la historia. Aunque los suecos se pondrían en ventaja temprana, la aparición de Vavá y Pelé, con dos goles cada uno, revertiría la situación.

Brasil nuevamente brillaría en el torneo siguiente, realizado en Chile a mediados de 1962. Pelé, ya convertido en uno de los mejores jugadores del momento, no pudo participar debido a una lesión a comienzos del evento, pero la magia de Garrincha llevaría al equipo brasileño a levantar por segunda vez la Copa al derrotar en la final a , frente a más de 60.000 personas instaladas en el Estadio Nacional de Santiago de Chile. Cabe resaltar que en ese torneo, el colombiano Marcos Coll marcó en la portería del legendario arquero Lev Yashin el empate 4:4 de su selección contra la con un gol olímpico, el único marcado en la historia de los mundiales. Mientras que los , después de derrotar a los en el infame partido de fase de grupos conocido como la "Batalla de Santiago", lograron llegar hasta el tercer puesto al derrotar por un gol a la .
En 1966 la Copa sería realizada en Inglaterra, cuna del fútbol. La selección de Brasil quedaría eliminada en la primera ronda después de ser derrotada en violentos partidos por y , este último llegó a semifinales liderado por Eusébio. Uruguay y Argentina tampoco llegaron lejos, luego de quedar eliminados en cuartos de final tras arbitrajes polémicos. , campeona en esos momentos de la Copa de Europa, se vio apeada en la primera ronda de clasificación tras perder contra Alemania y Argentina.

Desde su debut en 1950, no había podido tener una buena actuación, por lo que esta era su oportunidad de demostrar su paternidad. Los locales se enfrentaron a Alemania Federal ante un Estadio de Wembley repleto apoyando a su selección. Tras empatar en el tiempo regular se realizó una prórroga. En el minuto 101, Geoff Hurst disparó contra la portería germana y el balón sería despejado por el guardameta. El tiro sería considerado gol por el árbitro, desatando una polémica que persiste hasta el día de hoy sobre si el balón cruzó completamente la línea de gol. Cuando quedaban segundos para que el partido finalizara y todo el equipo germano intentaba descontar, Bobby Moore atrapó un balón que conectó con Hurst, quien realizó un disparo lejano, anotando el 4:2 final, desatando la alegría en las graderías. Minutos después, Moore recibiría la Copa Jules Rimet de las manos de la reina Isabel II.

A pesar del fracaso de 1966, la escuadra brasileña llegó a México dispuesta a ganar el Mundial de 1970. La "verdeamarela" se enfrentó en primera ronda a los campeones defensores, Inglaterra. Brasil, que incluía en sus filas no solo a Pelé, sino a otros grandes jugadores como Jairzinho, Tostão, Rivelino y Carlos Alberto, derrotó por la cuenta mínima a los ingleses en uno de los encuentros más memorables del torneo.

Brasil avanzó invicto hasta las finales, donde se enfrentaría al ganador del partido entre y . Los italianos habían goleado a la , mientras los germanos se clasificaron tras derrotar a los ingleses en tiempo extra, reeditando la final del torneo previo. El encuentro de semifinal partió con un temprano gol de Roberto Boninsegna. Cuando se jugaban los descuentos, Karl-Heinz Schnellinger anotó y forzó la prórroga, en que cada equipo anotó dos goles más. Alemania, exhausta tras el partido ante Inglaterra y con Franz Beckenbauer lesionado, no pudo aguantar la presión y fue derrotada por 4:3. Un monumento levantado posteriormente en el Estadio Azteca conmemora hasta el día de hoy el llamado "Partido del Siglo", considerado por muchos como el mejor de la historia.

Brasil e Italia se enfrentaron el 21 de junio de 1970 en Ciudad de México para definir cuál de los dos equipos se adjudicaría para siempre el Trofeo Jules Rimet, premio que sería entregado al primer equipo en ganar tres veces el torneo. Durante el primer tiempo ambos equipos estuvieron igualados a un gol, pero la artillería brasileña estallaría en el segundo tiempo, en el que los italianos pagaron el esfuerzo realizado frente a Alemania, anotando tres goles más. Brasil derrotó por 4:1 a Italia, coronándose como tricampeón con una de las escuadras más valoradas en la historia del fútbol. En el partido por el tercer lugar, Alemania Federal derrota a Uruguay 1:0.

Durante la década de los años 60 comenzaron a ser lanzados los primeros sistemas de satélites. En México 1970, y gracias al sistema de Telstar, se transmitieron por primera vez imágenes en color del evento para el resto del planeta. Debido a esto, el evento comenzó a popularizarse con rapidez en el resto del mundo. Prueba de ello es la cantidad de países inscritos para el proceso clasificatorio: en 1962 se inscribieron 56 países y en 1970 fueron 75. Cuando el Mundial regresó a Europa para la Copa Mundial de 1974 organizada por Alemania Occidental había 99 participantes, principalmente de las recién independizadas naciones africanas.

Rápidamente el evento comenzó a convertirse en uno de los principales eventos deportivos, alcanzando la popularidad de los mismísimos Juegos Olímpicos. La Copa Mundial comenzó a volverse en un rentable negocio, que se iniciaría con la primera mascota del torneo: el león "Willie", que representó al mundial realizado por Inglaterra. La empresa deportiva Adidas se convertiría en auspiciante oficial del evento desde 1970 y sería el proveedor oficial de los balones, modernizando notablemente el tradicional deporte.

Después de haber sido derrotado en la final de 1966 y en semifinales de 1970, el comandado por Franz Beckenbauer confiaba en que finalmente lograrían levantar la Copa en su propio país. A pesar de iniciar el torneo de 1974 con una derrota frente a sus rivales de la , los germanos llegaron hasta la final del torneo, realizada en el Estadio Olímpico de Múnich. Su rival en la final fue la selección de los , llamada la "Naranja Mecánica" por el color naranja de la casaca y su facilidad por crear fútbol técnico muy ofensivo y vencer a sus rivales. En la segunda fase se disputaron dos liguillas de cuatro equipos. Alemania se impuso en su grupo, venciendo a la sorprendente , y los Países Bajos quedaron primeros, por delante de Brasil y Argentina. Polonia venció a Brasil en la lucha por el tercer puesto, logrando así su mejor resultado hasta la fecha.

En la final, el Fútbol Total de Johan Cruyff parecía superar a la disciplina de los locales cuando se pusieron en ventaja con el partido recién comenzado. Cruyff forzó un penalti y Johan Neeskens lo convirtió, cuando Alemania todavía no había podido ni siquiera tocar el balón. Pero la marca de estos últimos a la estrella neerlandesa y los goles de Paul Breitner y Gerd Müller finalmente le darían la victoria por 2:1 a Alemania, que sería el primer equipo en levantar el nuevo trofeo del torneo. La hegemonía del fútbol europeo estaba discutida entre Cruyff y el líder alemán, Franz Beckenbauer, ganadores de los últimos Balones de Oro. En referencia a esto, el germano declaró: «Cruyff era mejor jugador que yo, pero yo gané el Mundial».

Tras más de 48 años de espera, finalmente Argentina fue seleccionada para ser sede de la Copa Mundial de 1978. Sin embargo, la organización del torneo se vería afectada por el rechazo internacional a la dictadura militar que se había instalado en el país en 1976 y a las violaciones a los derechos humanos cometidas durante ese período. A pesar de las protestas iniciales ningún país se retiró de la competición, pero los neerlandeses sufrieron la deserción de Cruyff por dichos motivos. Esto no pesaría en el rendimiento de la "Naranja Mecánica", que nuevamente sería finalista después de sobrepasar a Italia y Alemania en la fase grupal de la segunda ronda. Su rival sería la , que clasificaría tras derrotar por 6:0 en un polémico partido a . En la final, disputada en el Estadio Monumental de Buenos Aires, Mario Kempes sería la figura de la victoria sudamericana por 3:1. 
Debido al éxito del torneo, el número de equipos participantes aumentó de 16 a 24 desde la Copa Mundial de 1982 disputada en España, para así darle más oportunidades de participación a equipos de Norteamérica, África, Asia y Oceanía. A diferencia del mundial anterior en que solo participaron en total 3 países de estos continentes, en España participó el doble. A pesar de ello, los nuevos participantes no lograron éxito pues ninguno de ellos clasificó a la segunda ronda, aunque se deben destacar las participaciones de , que quedó eliminado por diferencia de goles al igualar en puntos con Italia, y . La eliminación de este último país generó una fuerte controversia luego de que Alemania derrotara por 1:0 a Austria, cifra necesaria para que ambos países germanohablantes clasificaran en desmedro de los norteafricanos. 

, que contaba con jugadores como Zico, Falcão y Sócrates, fue la sensación de la primera ronda al ganar con facilidad sus tres partidos, mientras se perfilaba como uno de los favoritos junto a su estrella, Michel Platini. Sin embargo, estos dos equipos serían eliminados respectivamente por los eventuales finalistas del torneo: y . Italia clasificaría a semifinales luego de que los tres tantos de Paolo Rossi les dieran la victoria sobre los sudamericanos durante la segunda ronda. En tanto, la dramática semifinal entre franceses y alemanes se definiría tras la primera tanda de penaltis realizada en un Mundial. Luego de que los alemanes remontaran un 3:1 en la prórroga, alcanzarían el pase a la final al ganar por 5:4 desde los once pasos. En la final, los itálicos se impusieron fácilmente alcanzando el tricampeonato; Rossi, la figura del equipo campeón, se quedaría con los dos premios creados ese año: el botín de oro al goleador del torneo y el balón de oro, entregado al mejor jugador.

España, como anfitriona, tuvo una participación modesta: en la primera ronda tras vencer a Yugoslavia y empatar con Honduras. Sin embargo, la floja primera fase le costó su encuadramiento en el grupo de Alemania e Inglaterra en la segunda fase. La eliminación de España supuso la destitución del seleccionador José Santamaría.

Colombia había sido elegida para ser la sede de la XIII Copa Mundial a realizarse en 1986, sin embargo, el país organizador desistió luego de verse imposibilitado de cumplir las fuertes exigencias impuestas por Hermann Neuberger, vicepresidente de la FIFA. Ante la renuncia colombiana, el organismo internacional decidió que México acogiera nuevamente el torneo, debido a que mantenía en gran parte la infraestructura dejada por el torneo de 1970.

La primera ronda del torneo se realizó con normalidad, destacando a como el primer equipo africano que pasó a la segunda ronda. En la segunda ronda, sin embargo, comenzaron a destacarse los equipos favoritos: , que había derrotado a los campeones defensores en octavos de final, enfrentó en un dramático partido a , el cual finalizó con la victoria gala en la ronda de penaltis. Sin embargo, los sueños de Platini se verían nuevamente truncados en semifinales por .

En la otra llave del torneo, avanzaba imparable, en gran parte debido al talento de Diego Armando Maradona. En cuartos de final el equipo albiceleste debía confrontar a , uno de sus más tradicionales rivales, especialmente tras el estallido de la Guerra de las Malvinas cuatro años antes. El enfrentamiento destacó por dos de los goles más recordados en la historia de este deporte: en el minuto 51 Maradona anotó un gol con su mano (conocido como "la mano de Dios") y en el 54 el mismo Maradona recorrió 62 metros en 10 segundos, sobrepasando a 6 ingleses, antes de anotar el denominado "Gol del Siglo", considerado el mejor gol en la historia del fútbol.

La final sería disputada entre alemanes y argentinos en el Estadio Azteca ante más de 110.000 espectadores. Cuando faltaban menos de quince minutos para el final del partido los sudamericanos lideraban por 2:0, pero los dirigidos de Franz Beckenbauer lograron igualar el marcador agregando dramatismo. Sin embargo un gol de Jorge Burruchaga en el minuto 84 definiría la victoria argentina. Maradona, elegido el mejor jugador del torneo, sería el encargado de levantar el segundo título mundial de su país.

La revancha de Alemania se concretaría cuatro años después, cuando fuera Italia la sede de la Copa Mundial de 1990. En este torneo, se convirtió en una de las sorpresas al derrotar en el partido inaugural a la escuadra argentina y avanzar finalmente hasta los cuartos de final, siendo eliminados por Inglaterra en la prórroga. A pesar de ello, este mundial ha sido considerado como uno de los de más baja calidad, debido a un fútbol extremadamente defensivo, lo que se vio reflejado en la baja cifra de goles (la más baja de la historia) y el gran número de partidos definidos en penaltis, entre los que se encontraron las dos semifinales. El torneo finalizaría con una mediocre final entre alemanes y argentinos, caracterizada por los errores arbitrales y la expulsión de dos jugadores de la "Albiceleste". Un solitario gol de penal de Andreas Brehme cinco minutos antes del pitazo final le daría la Copa por tercera vez a la escuadra de Alemania Occidental, algunos meses antes de que se concretara el proceso de reunificación de dicho país.

Con el fin de promover el fútbol en Estados Unidos, la principal potencia mundial tras el fin de la Guerra Fría, la FIFA decidió que la Copa Mundial de 1994 fuera disputada en dicho país, generando amplias críticas debido a la realización del torneo en un lugar donde el fútbol era prácticamente desconocido y donde ni siquiera existía una liga profesional. Esto no impidió que el Mundial fuera un éxito, alcanzando cerca de 3,6 millones de espectadores, una marca imbatida hasta el día de hoy.

El torneo se vio manchado con el asesinato, una vez finalizada la participación del equipo de Colombia, de su defensa Andrés Escobar luego que este accidentalmente cometiera un autogol. También significó el fin de la brillante carrera internacional de Maradona, después que diera positivo su test de dopaje. En el ámbito deportivo, Romário fue el artífice de la impecable campaña de Brasil hasta la final del torneo, en la que se enfrentó a que había llegado a dichas instancias a pesar de haber disputado sufridos encuentros. Los dos tricampeones se enfrentaron en el Rose Bowl, pero ninguno fue capaz de convertir durante el tiempo reglamentario. El campeonato se definiría por primera vez en una tanda de penales. Después que Roberto Baggio fallara en su disparo, Brasil conquistó su tetracampeonato cuando había estado sin levantar la copa durante 24 años.

La nueva generación brasileña comenzó nuevamente a reinar y era la gran favorita para alcanzar el pentacampeonato en Francia 1998, el primer torneo que contó con 32 equipos participantes. A pesar de la ausencia de Romário, Brasil contó con jugadores como Ronaldo y Rivaldo que llevaron a los brasileños a su segunda final consecutiva. En dicho encuentro se enfrentó a la , que había llegado a dicha instancia justo después de derrotar en semifinales a la sorprendente selección de , que en su primera participación en un Mundial había alcanzado el tercer lugar. Aunque los galos habían tenido una irregular campaña durante la segunda ronda, en el encuentro decisivo fueron superiores y el buen juego de Brasil prácticamente se desvaneció. Zinedine Zidane se convirtió en la estrella del partido al anotar dos de los tres goles de "Les Bleus", los cuales les darían el primer título a su país.

Cuatro años más tarde el torneo se disputaría por primera vez en tierras asiáticas, cuando Corea del Sur y Japón realizaran conjuntamente el Mundial de 2002. El evento generó una enorme inversión en ambos países, especialmente en cuanto a infraestructura: 18 nuevos estadios fueron construidos en total, con un costo que superó los 4.500 millones de dólares y se instaló tecnología de última generación para acoger a las 32 selecciones clasificadas de un total de 199 equipos inscritos, marcando un nuevo hito.

A pesar de haber sufrido en el proceso clasificatorio, nuevamente demostró su poderío, al ganar todos sus partidos durante el torneo. Ronaldo, que había sido opacado en la final de 1998 por Zidane, anotó ocho goles y se convirtió en el jugador con más tantos anotados desde 1970. En la final disputada en Yokohama, los brasileños no tuvieron problemas en superar a . El guardameta alemán Oliver Kahn, que había sido uno de los principales artífices de la campaña de su combinado permitiendo solo un gol en todo el torneo, no pudo detener dos disparos de Ronaldo que permitieron a Brasil coronarse pentacampeón.

El torneo de 2002 mostró una serie de resultados sorpresivos, entre los que destacaron las eliminaciones en primera ronda de algunos de los equipos favoritos para ganar el torneo, como , y , que se convirtió en el peor campeón defensor de la historia del evento. Otros equipos alcanzaron resultados destacables: se convirtió en el primer equipo asiático en llegar a semifinales junto a la sorprendente , mientras y la debutante accedieron a la ronda de los ocho mejores. Sin embargo, los errores arbitrales marcaron un punto negro en el desarrollo del torneo, hecho que fue reconocido incluso por el propio presidente de la FIFA, Joseph Blatter.
Blatter, que había ascendido a la presidencia de la FIFA con la promesa de llevar el Mundial por primera vez a África, sufrió un fuerte revés cuando por un voto de diferencia Alemania derrotó a Sudáfrica en la elección de la sede de la Copa Mundial de 2006.

Brasil, que contaba en sus filas con Ronaldinho, era considerado el máximo favorito para levantar el trofeo, pero su desempeño fue ampliamente criticado aún cuando clasificaron invictos a la segunda ronda y Ronaldo alcanzó el récord de goles anotados en la historia de la competición. Alemania y Francia, que por otro lado casi no albergaban esperanzas de lograr un buen resultado, comenzaron a progresar a medida que avanzaba en el torneo. La primera ronda no presentó grandes sorpresas en general y la mayoría de los favoritos pasaron a la siguiente fase, a excepción de la que fue sobrepasada por y en el denominado "grupo de la muerte".

La supremacía europea se comenzó a manifestar durante la segunda fase. En cuartos de final, los penaltis marcaron el fin de la competición para e , que fueron derrotados respectivamente por Alemania y . Brasil acabaría con su condición de invicto desde la derrota en la final de 1998 al perder nuevamente frente a la escuadra francesa. Reeditando la recordada semifinal de 1970, Italia y Alemania se enfrentaron nuevamente en dicha instancia; luego de mantenerse durante gran parte del partido sin anotar, los italianos accederían a la final al marcar dos goles minutos antes de acabar la prórroga. Italia se enfrentaría a Francia, que clasificó tras derrotar a los lusitanos con un penalti anotado por Zidane. El partido disputado en el Estadio Olímpico de Berlín se desarrolló extremadamente parejo para ambos equipos, que durante los primeros 45 minutos habían anotado un gol cada uno. En la prórroga un polémico incidente provocaría la expulsión de Zinedine Zidane al golpear al italiano Marco Materazzi. Sin su capitán, Francia se enfrentó a la definición desde los once pasos. David Trezeguet erró un tiro, lo que permitiría a Italia coronarse como campeona del Mundial por cuarta vez.
Tras el fracaso de la elección de un país africano para la Copa de 2006, la FIFA decidió establecer un sistema de "rotación continental" que permitiera que cada evento fuera organizado al menos una vez por cada confederación en un cierto período. África sería el primer continente elegido y Sudáfrica fue ampliamente apoyada como la sede de la Copa Mundial de Fútbol de 2010. De igual forma, el Mundial de 2014 fue asignada a Sudamérica, siendo Brasil el único postulante. Sin embargo, el sistema de rotación fue puesto en jaque ante los diversos problemas de organización que enfrentaron ambos países y a la presión ejercida por otros países con aspiraciones. Ante esto, la FIFA decidió imponer un nuevo sistema de rotación que impide la postulación de un país del continente anfitrión por dos ediciones tras albergar el evento; así, ningún país africano pudo postularse para la Copa Mundial de 2018 y ninguno sudamericano para 2018 y 2022.

El evento de 2010, organizado por Sudáfrica, mostró una serie de resultados sorpresivos, donde favoritos para ganar el torneo fueron eliminados en primera fase, como y , o tuvieron que sufrir hasta el último partido para pasar a la segunda fase, como , y . La mayoría de los equipos africanos tuvieron un mediocre desempeño pese a su localía, siendo el primer anfitrión de una Copa del Mundo en no pasar la primera fase. Por otro lado, los cinco miembros de la Conmebol destacaron tanto en primera ronda como en octavos de final, clasificando cuatro a la ronda de los ocho mejores (luego de que fuera eliminado por ). 

Esta dominación sudamericana se derrumbaría en los cuartos de final: y , dos de los equipos con mejor desempeño hasta el momento, fueron eliminados por Alemania y los , respectivamente. España clasificó a su primera semifinal derrotando con solamente un gol a , mientras sería la única selección de Sudamérica que llegó a semifinales, al eliminar a en la definición a penales. En dicho polémico partido, el delantero Luis Suárez detuvo un gol con la mano en el último minuto de la prórroga y que daba la victoria al único seleccionado africano en dichas instancias, ante lo cual se pitó un penalti que fue desperdiciado por Asamoah Gyan, lo cual finalmente implicó la derrota ghanesa. En la final, se enfrentaron los Países Bajos y España, quienes derrotaron respectivamente a Uruguay y Alemania. España llegó por primera vez a una final en su historia al reeditar en semifinales su victoria de la final de la Eurocopa 2008 ante los alemanes, rompiendo finalmente su racha de malos resultados mundialistas. En el estadio Soccer City de Johannesburgo, Andrés Iniesta anotó el único gol del partido a pocos minutos de que terminara la prórroga, que le dio su primer título mundial a España.

En 2014, después de 36 años, la organización de la Copa Mundial de la FIFA regresó a tierras sudamericanas para disputar su vigésima edición, en Brasil. En la primera ronda, se dieron algunos resultados sorprendentes, como las eliminaciones de Portugal y el campeón defensor España (marcando el curioso hecho de que en los últimos cuatro mundiales, los tres campeones defensores europeos fueron eliminados en fase de grupos: Francia en 2002, Italia en 2010 y España en 2014). Además, en el llamado "grupo de la muerte" que tenía a Inglaterra e Italia como favoritos para pasarlo, la selección de Costa Rica sorprendió a todos quedando primera, mientras que Uruguay clasificó como segundo, dejando a ambos favoritos del grupo eliminados en primera ronda. En este grupo, en el partido entre Italia y Uruguay se produjo la recordada mordida del delantero uruguayo Luis Suárez al zaguero italiano Giorgio Chiellini, que sería severamente sancionada por FIFA. 

Por su parte, tanto la selección local como Argentina, dos de los máximos favoritos a obtener el título, avanzaron sin problemas a la segunda ronda de la mano de sus figuras Neymar y Lionel Messi, aunque no siempre mostrando un juego muy vistoso. En un principio, se hizo clara la supremacía de los equipos sudamericanos jugando de local: cinco de seis avanzaron a los octavos de final. Además, por primera vez en la historia de la Copa Mundial, dos selecciones africanas llegaron a pasar a segunda ronda: Nigeria y Argelia. 

En la ronda de los 16 mejores no se dieron grandes sorpresas, salvo el hecho de que Costa Rica llegara a los cuartos de final, por primera vez en su historia. También Colombia y Bélgica llegaron a dicha instancia, que si bien no son potencias del fútbol, no pueden ser consideradas como sorpresas debido a la buena calidad de sus planteles. De hecho, el goleador del torneo fue el colombiano James Rodríguez. 

A la ronda de semifinales llegaron sin embargo selecciones de alta tradición. En el primer partido, la selección Verde-Amarela, sin su figura Neymar (se había lesionado en cuartos), sufrió una vergonzosa goleada por 1:7 ante la selección teutona, en el partido que la prensa bautizó como "el Mineirazo". En este partido, el segundo gol de Alemania fue convertido por Miroslav Klose quien se convirtió en el máximo goleador de la historia de la Copa Mundial de la FIFA con 16 tantos, superando al brasileño Ronaldo. La segunda semifinal enfrentó a neerlandeses y argentinos. Tras empatar sin goles en los 120 minutos, los sudamericanos triunfaron en los tiros desde el punto penal. En la definición por el tercer puesto, Brasil fue nuevamente humillado, perdiendo por 0:3 ante los Países Bajos. 

La final entre alemanes y argentinos, disputada el domingo 13 de julio en el Estadio Maracaná de Río de Janeiro, se convirtió en la más repetida de la historia (tercera final entre Argentina y Alemania, en 1986 ganaron los primeros y en 1990, los segundos). Los teutones llegaban como favoritos debido a la abultada goleada contra los locales. Sin embargo, el encuentro resultó ser muy parejo, tanto así que tras igualar 0:0 en el tiempo reglamentario, se debió acudir a la prórroga. Ya en la segunda mitad de la prórroga, cuando todo indicaba que el nuevo campeón se definiría en la tanda de penaltis, un solitario gol de Mario Götze le permitió a Alemania coronarse por cuarta vez campeón del mundo, siendo así la primera vez que un combinado no sudamericano se consagra en América del Sur.

En la actualidad la Copa Mundial de Fútbol es uno de los eventos con mayor influencia a nivel mundial. En su edición de 2006, el torneo fue seguido por una audiencia acumulada a lo largo de todo su desarrollo superior a los 32 mil millones de espectadores en 207 países y se inscribieron 197 selecciones nacionales, lo que equivale a casi la totalidad de estos equipos.

La Copa Mundial de Fútbol consta de dos etapas: una fase clasificatoria y una ronda final, considerada esta última usualmente como el evento en sí mismo. El número de participantes en esta ronda final ha variado con el paso de los años: 16 participantes hasta 1978 (a excepción de los mundiales de 1930 y 1950 con 13 participantes cada uno), 24 entre 1982 y 1994, 32 desde 1998 y próximamente 48 a partir de 2026.

La fase clasificatoria se ha disputado desde 1934. En ella, las selecciones nacionales que desean participar en el torneo se enfrentan en una serie de encuentros. Para ello, las asociaciones de fútbol que dirigen estas selecciones deben ser miembros plenos tanto de la FIFA como de alguna de las seis confederaciones continentales existentes en la actualidad:

Cada una de estas confederaciones organizan un sistema de elección de sus representantes a través de encuentros deportivos. El número de representantes de cada confederación es definido previamente por la FIFA a través de la entrega de cupos, algunos de los cuales son completos equivalentes a un equipo en la fase final y otros son compartidos, en los que un equipo debe definir su clasificación a la ronda final ante un representante de otra confederación en un proceso denominado generalmente repechaje, repesca o "play-offs".

Por ejemplo, para el Mundial de Brasil 2014, la FIFA estableció la siguiente distribución de los cupos clasificatorios:

A estos cupos se suma el equipo del país organizador del torneo, que desde los orígenes del torneo (a excepción de 1934) ha tenido ese derecho. Los equipos campeones del torneo previo deben en la actualidad participar del proceso clasificatorio, aunque tuvieron el derecho de clasificación automática entre 1938 y 2002.

La fase final del torneo es realizada cada cuatro años y en ella participan los equipos que sortearon exitosamente el proceso clasificatorio y aquellos clasificados por derecho propio. Esta etapa del torneo se realiza a lo largo de un mes exclusivamente en el país organizador designado con anterioridad. Sin embargo, en la Copa Mundial de Fútbol de 2002 el evento fue realizado por Corea del Sur y Japón conjuntamente; aunque la experiencia fue un éxito, el complejo proceso logístico necesario ha hecho que la FIFA considere evitar este tipo de torneos en el futuro.

El país organizador es electo por el Comité Ejecutivo de la FIFA, el cual se reúne seis años antes en Zúrich para poder tomar la decisión. El Comité Ejecutivo está compuesto por diversos representantes de las diferentes confederaciones y es presidido por el . El Comité Ejecutivo realiza una votación simple hasta lograr una mayoría absoluta de votos para determinar el país anfitrión de la Copa. En caso de que haya empate, es el presidente del organismo el encargado de dirimir la situación. En ocasiones anteriores se han logrado acuerdos previos entre los representantes de las candidaturas que han evitado la realización de votaciones o han generado votaciones unánimes. Así, por ejemplo en el 35º Congreso de la FIFA realizado en Londres durante 1966 los representantes de Alemania Occidental, Argentina y España retiraron sus candidaturas al aceptar la propuesta de organizar los torneos de 1974, 1978 y 1982 respectivamente, mientras en 1996 Corea del Sur y Japón aceptaron fusionar sus candidaturas en una sola y así evitar la votación.

La FIFA establece una serie de requisitos para poder organizar el torneo, especialmente en cuanto a infraestructura. En los últimos años las exigencias establecen al menos la existencia en el país de entre 8 a 10 estadios que superen los 40.000 espectadores. En caso de que estas exigencias no sean cumplidas, la FIFA tiene la posibilidad de asignar la sede a otro país.

La elección de la sede ha sido históricamente influida por el poder de las confederaciones continentales. En sus comienzos el torneo fue boicoteado tanto por países europeos como sudamericanos cuando la sede no era elegida en su continente. Para evitar esto, tras el receso producido por la Segunda Guerra Mundial se estableció un sistema de rotación "de facto" entre Europa y Sudamérica, los continentes con mayor tradición en la realización del torneo. Posteriormente, el cupo sudamericano se vería ampliado hacia todo el continente americano, permitiendo la inclusión de México y los Estados Unidos. En 1996 la FIFA insistió en la elección de una sede en Asia y posteriormente lo haría para África. En esta última elección, sin embargo, Alemania se impuso en la elección de la sede de la Copa Mundial de Fútbol de 2006, por lo que la FIFA instituyó una política de rotación continental. Bajo esta premisa, se estipuló la obligatoriedad de candidaturas africanas para 2010 y sudamericanas para 2014. En el caso de la elección de este último torneo, Brasil fue el único candidato por lo que se decidió revisar esta política para evitar este suceso. Así, la FIFA estableció en 2007 modificar este criterio permitiendo la postulación de cualquier país para las copas mundiales de 2018 en adelante, a excepción de aquellos provenientes de confederaciones que han albergado alguno de los dos torneos previos. Así pues, hasta la edición del año 2022, los países europeos habrán sido sede en once ocasiones, los sudamericanos en cinco ocasiones, los norteamericanos en tres ocasiones, los asiátiacos en dos y los africanos en una sola edición.

A lo largo de la historia de la Copa Mundial se han utilizado diversos sistemas de competición para poder determinar al equipo que se coronará como el mejor del mundo. Sin embargo, existe en general el patrón de establecer dos rondas en la competición, a excepción de 1934 y 1938, ediciones en las que se utilizó un formato único de eliminación directa.

Para la primera ronda del torneo los equipos son distribuidos en grupos de cuatro integrantes, aunque previamente, y debido a la retirada de algunos competidores, han existido grupos de hasta dos combinados. Para ello el comité organizador realiza un sorteo previo en el que se establecen a los mejores equipos como cabezas de series y se procura evitar que equipos de la misma confederación se enfrenten durante la primera ronda, a excepción de los equipos de la UEFA que por su mayor número es imposible que queden todos separados.

En cada uno de los grupos todos los equipos se enfrentan en un cuadrangular simple. Cada equipo acumula a lo largo de estos partidos una puntuación en función de sus resultados: 3 puntos por victoria (hasta 1994 eran solo 2), 1 por empate y 0 por derrota. Los cuatro equipos son ordenados de acuerdo a su puntuación en forma descendiente. En caso de que haya dos o más equipos con igual puntuación, existen otros criterios de desempate, que en el presente son:
Los criterios nombrados anteriormente permiten determinar qué equipos se clasifican a la segunda ronda. Cuando el número de participantes es una potencia de dos (8, 16 o 32) clasifican los dos mejores equipos del grupo, pero cuando no es así (por ejemplo, 24 participantes) pueden clasificar algunos de los mejores terceros.

En segunda ronda se han utilizado diversos mecanismos de clasificación para la ronda final. La mayoría de estos corresponden a un sistema de eliminación directa con octavos de final, cuartos de final, semifinales, un partido definitorio del tercer y cuarto lugar y la final. Algunos torneos sin embargo realizaron un nuevo cuadro de grupos para determinar a los finalistas; hasta el torneo de 1950, la final se determinaba a través de un sistema grupal entre los cuatro semifinalistas, sin embargo, el resultado de los diversos encuentros hizo que los últimos dos partidos fueran los que definían al campeón, por lo que son considerados comúnmente como la única final.

Los partidos de eliminación directa, a diferencia de los de primera vuelta, no pueden finalizar con un empate. En caso de que los equipos que se enfrentan finalicen el tiempo reglamentario igualados en número de goles, se realiza una prórroga de dos tiempos de quince minutos cada uno (en 1998 y 2002 se utilizó el sistema de "gol de oro"). En caso de que finalizada esta prórroga la igualdad se mantenga, se realiza una tanda de penaltis de cinco tiros, extensible hasta que haya un equipo que logre la victoria.

Esta tabla muestra los principales resultados de la fase final de cada Copa Mundial de Fútbol.

Más de 200 equipos diferentes han sido parte de los procesos clasificatorios y 77 han participado a lo largo de la fase final de la Copa Mundial. De ellos, 12 han llegado a la final del torneo y 8 han alcanzado la victoria. Cada uno de estos equipos tiene derecho a colocar una estrella sobre la insignia oficial de su federación por cada uno de los campeonatos ganados.

Brasil es el equipo más exitoso, al alcanzar cinco campeonatos, seguido por Italia y Alemania con cuatro. En términos estadísticos, Brasil es el equipo con más victorias, seguido por Alemania e Italia. De los 4 títulos ganados por Alemania, 3 pertenecen a Alemania Federal y el ganado recientemente pertenece a la unificada; también, ha sido la selección con más participaciones en finales, en un total de 8.

Brasil e Italia son, además, los únicos equipos que han ganado dos torneos consecutivamente: Italia lo logró en 1934 y 1938, mientras que los sudamericanos lo lograron en 1958 y 1962. Ambos equipos se han enfrentado en dos finales (1970 y 1994), en ambas ha salido victorioso Brasil. La final de 1970, además, fue la primera en que se coronó a un tricampeón, al cual se le otorgó definitivamente el trofeo Jules Rimet.

De los ocho equipos campeones, todos, a excepción de Brasil y España, han sido campeones al menos una vez cuando el torneo fue organizado en su casa. Por otro lado, Brasil, España y Alemania son los equipos que han ganado un torneo fuera de su continente: en Suecia 1958 y Corea del Sur-Japón 2002 para el primero, en Sudáfrica 2010 para el segundo y en Brasil 2014 para el tercero. En cambio, México fue la única selección que ha sido sede dos veces sin haber obtenido el título.

Alemania vs. Argentina es el partido más repetido en finales de la Copa del Mundo; en México 86; Italia 90 y Brasil 2014.

En cuanto a las participaciones, Brasil es el único equipo presente en todos los eventos (20 en total), le siguen Alemania e Italia con 18 participaciones, Argentina con 16, México con 15 y España, Inglaterra y Francia con 14.

La lista a continuación muestra a los 24 equipos que han estado entre los cuatro mejores de alguna edición del torneo.

En "cursiva", se indica el torneo en que el equipo fue local.

Cerca de 6000 jugadores han participado en la Copa Mundial y muchos de ellos han pasado a la historia. De ellos, un grupo selecto ha participado en múltiples oportunidades del evento: 33 jugadores han estado presentes en cuatro eventos, aunque solo 16 han jugado efectivamente en cuatro fases finales mundiales, mientras que solo dos jugadores han disputado encuentros en cinco torneos: el guardameta mexicano Antonio Carbajal entre 1950 y 1966 y el alemán Lothar Matthäus entre 1982 y 1998 (período en el cual incluso ganó el campeonato). En cuanto a partidos disputados, Matthäus jugó 25 partidos, récord que se mantiene hasta la actualidad. El italiano Paolo Maldini por otro lado, es el jugador que ha jugado mayor cantidad de minutos, con 2217 minutos en sus cuatro participaciones entre 1990 y 2002.

En cuanto a goles, los dieciséis del alemán Miroslav Klose lo convierten en el jugador que más goles ha marcado en todos los eventos de la Copa, superando los quince del brasileño Ronaldo y los catorce del alemán Gerd Müller. En la Copa Mundial de Fútbol de 1958, el francés Just Fontaine marcó 13 anotaciones, cifra que se ha mantenido como la mayor cantidad de goles alcanzada en un solo evento, siendo sus únicas anotaciones en mundiales.

Roger Milla, futbolista de Camerún se convirtió en el jugador más veterano (42 años) en marcar un gol en un mundial ante Rusia en el Mundial de 1994 disputado en Estados Unidos.

En el Mundial de Brasil 2014, el portero colombiano Faryd Mondragón, se adjudicó el récord de ser el futbolista más longevo en disputar un partido en la historia de las Copas del Mundo de Fútbol, ingresando en el minuto 85´ en lugar de David Ospina. A la fecha, Faryd tenía 43 años y 3 días. 

A continuación se listan los jugadores con más anotaciones en la Copa Mundial. En "cursiva" se indican los jugadores que se mantienen activos a la fecha.

El entrenador italiano :Vittorio Pozzo es el único que ha obtenido en dos ocasiones el campeonato de la Copa Mundial de Fútbol, en las ediciones de Italia 1934 y Francia 1938. Cabe señalar que todos los entrenadores que han ganado algún campeonato han sido de la misma nacionalidad de las selecciones que dirigieron.

Durante las 20 ediciones de la Copa Mundial disputadas hasta 2014, se han marcado 2379 goles. De estos, 41 han sido autogoles. La Copa Mundial ha sido escenario de algunos de los goles más famosos de la historia del fútbol. Dentro de ellos destacan la llamada "Mano de Dios" y el "Gol del Siglo", ambos marcados por Diego Maradona durante el mismo partido de la Copa Mundial de Fútbol de 1986.

A nivel de torneos, la Copa Mundial de Fútbol de 1998 y la Copa Mundial de Fútbol de 2014 son las que han tenido mayor número de goles, con 171 anotaciones en sus 64 partidos disputados cada una, mientras el menor número fue en 1930 y 1934 con 70 goles (aunque la primera edición contó con 18 partidos, uno más que la edición de 1934). Considerando el número de partidos, el mayor número de goles por partido fue en el Mundial de 1954, con 5,38 tantos por encuentro; la cifra menor, en tanto, fue de 2,21 goles por partido en el Mundial de 1990. Esto refleja en general la evolución del fútbol: durante los primeros años del torneo, el fútbol se caracterizaba por su aspecto ofensivo y los partidos tenían un promedio cercano a los 4 goles por partido. Posterior a la Copa Mundial de 1954, sin embargo, el deporte comenzó a desarrollar más los aspectos defensivos, lo que influyó en la disminución de los goles anotados: desde 1958 en adelante, el promedio de goles no superó los 3 por partido.

Los sorteos de los equipos participantes han producido, en ocasiones, encuentros entre selecciones de nivel muy diferente, lo que se ha reflejado en goleadas. Sin embargo, no todos los encuentros con un alto número de goles anotados se deben únicamente a goleadas: el partido con más goles anotados fue el disputado entre y la local en la Copa Mundial de 1954, el cual finalizó con una victoria austríaca por 7:5. La final con más anotaciones, en tanto, fue la disputada en 1958 por y , que terminó con la victoria de los primeros por 5:2. Por otro lado, la final entre e en 1994 finalizó sin goles, por lo que se recurrió a una serie de penaltis, donde los sudamericanos pudieron levantar su cuarta copa mundial. También se destaca el reciente 7:1 de Alemania a Brasil, siendo la mayor goleada recibida por un país anfitrión.

A continuación se listan las mayores goleadas en la Copa Mundial de Fútbol: 

Durante la realización de la Copa Mundial la organización dispone la entrega de diversos premios de acuerdo a la participación de los equipos y jugadores a lo largo del torneo. Sin lugar a dudas el principal premio es el título de campeón del evento; El equipo que logra coronarse como campeón recibe el Trofeo de la Copa Mundial de la FIFA por cuatro años. El equipo recibe además una réplica del trofeo y su nombre es grabado en la base de la original. Esta copa es entregada luego de que la Copa Jules Rimet fuera adjudicada de manera definitiva (tal y como lo establecía el reglamento) a Brasil cuando se coronó campeón por tercera vez en 1970. El equipo ganador además recibe un premio monetario, que en la última edición alcanzó los 16 millones de euros (equivalentes a más de 19 millones de dólares). La copa original fue diseñada por Abel Lafleur. El diseño actual es de Silvio Gazzaniga.

Desde el inicio del torneo, uno de los premios más importantes es al goleador del evento, es decir, el jugador que anota más goles durante la realización de la fase final de cada Copa Mundial. Desde la Copa Mundial de 1982 el premio fue instituido oficialmente como el «Botín de Oro». Desde el Mundial de 2006 fueron además entregados el «botín de plata» y el «botín de bronce», para los jugadores en el segundo y tercer lugar de la estadística de goleadores. Si hay dos o más jugadores con la misma cantidad de goles, cada uno recibe el premio correspondiente, sin tomar en cuenta la cantidad de minutos jugados por cada uno o si los goles fueron anotados en penaltis. Estadísticamente, se destaca el Campeonato de 1962, en el que hubo 6 goleadores, siendo la menor cantidad de goles marcados por un goleador.

El premio «Balón de Oro» es entregado al mejor jugador de cada edición de la Copa Mundial de la FIFA. Este reconocimiento se entrega desde la Copa Mundial de 1982.

Durante la realización del campeonato, la FIFA crea una lista con los 10 mejores jugadores del evento a su juicio. Los jugadores de esta lista son posteriormente votados por los representantes de la prensa especializada. El balón de oro es entregado al que haya obtenido más votos, mientras el balón de plata y de bronce se entrega al segundo y tercer más votados, respectivamente.

El proceso de elección ha sido criticado en las últimas ediciones, pues es realizado previo a la final del campeonato. Esto ha provocado que algunos jugadores hayan sido electos, pero en la final del torneo hay otro que destaca o simplemente el elegido no cumple con las expectativas.

Algunos de los otros premios entregados en la Copa Mundial en la actualidad se incluyen:

Además, en cada torneo se elige un "equipo estelar" en que se listan los mejores jugadores de cada evento en cada una de las demarcaciones.

Desde que fue por primera vez televisada en 1954, la Copa Mundial ha sido uno de los eventos deportivos más vistos a lo largo del mundo e incluso ha superado a los Juegos Olímpicos. La Copa Mundial de Fútbol de 2002, por ejemplo, tuvo una audiencia acumulada superior a los 28,8 mil millones de espectadores y solamente la final tuvo 1.100 millones en todo el mundo.

Además, es uno de los sucesos más influyentes que existen en la actualidad. Para muchos países la realización del torneo en su patria o incluso la participación del equipo nacional es un hecho histórico de gran relevancia. Por ejemplo, la victoria alemana en la Copa Mundial de Fútbol de 1954 es considerado como uno de los momentos claves para la recuperación de dicho país tras la derrota en la Segunda Guerra Mundial. El torneo también ha sido utilizado con motivos propagandísticos, tanto por el fascismo en Italia 1934 como por la dictadura militar argentina en 1978. Una parte de su impacto cultural, lo da también las tertulias existentes entre los hinchas del fútbol, antes, durante y después de cada partido, que son parte en oficinas, restaurantes y hasta en el transporte público, organizando asados, almuerzos y hasta reuniones familiares o de amigos en las casas, para esperar los partidos, de acuerdo al huso horario del país en que se efectúa la transmisión televisiva, como así también del país que la organiza.

La gran repercusión del torneo a lo largo del mundo ha servido también como plataforma para la difusión de la cultura y representaciones artísticas de los países anfitriones. Una muestra de ello fue el "Walk of Ideas", una serie de estatuas monumentales representando los principales inventos generados en Alemania y que fue construida durante la realización de la Copa Mundial de Fútbol de 2006. La música también ha tenido un lugar de importancia: la mayoría de los torneos han contado con temas oficiales, los que han alcanzado gran popularidad a lo largo del mundo. Ricky Martin, tras el lanzamiento del tema oficial de Francia 1998, "La copa de la vida", pudo dar inicio a su exitosa carrera fuera del mundo hispanohablante. 

El desarrollo tecnológico ha sido sumamente importante para que la Copa Mundial pudiera ser el evento que es. Sin lugar a dudas la televisión jugó un rol vital en la difusión del torneo a los diferentes continentes y así convertirlo en un torneo realmente mundial. Los primeros partidos fueron transmitidos durante la Copa Mundial de Fútbol de 1954 debido a la formación algunos años antes de la Unión Europea de Radiodifusión (Eurovisión). Siete partidos fueron transmitidos en vivo a Francia, Italia, Bélgica, los Países Bajos, Dinamarca, el Reino Unido, Alemania y Suiza, anfitriona del torneo. Cuatro años más tarde la cifra de países que recibieron la imagen en blanco y negro aumentó a 63, mientras que la final de Inglaterra 1966 sería el primer encuentro transmitido en color, pero esta tecnología se popularizaría en 1978. Con el lanzamiento de los sistemas de tecnología satelital el evento pudo ser transmitido en directo más fácilmente y en más países, reemplazando los resúmenes compactos que se daban en algunos países. Con el paso de los años, la tecnología permitió una mejor definición de las imágenes y ya desde la Copa Mundial de 2002, Internet se convirtió en una de las principales herramientas de comunicación, permitiendo no solo la utilización del Marcador Virtual que ponen las Páginas Web de los medios de comunicación, sino también permite la transmisión de los partidos completos y si la gente se lo perdió, permite verlo, como si fuera tiempo real o vía on demand. La televisión de alta definición debutaría durante la final de ese mismo torneo y se extendería al evento completo, cuatro años después. Asimismo, el canal que realiza la transmisión de los partidos, suele vender los derechos de los mismos a varias radioemisoras, para efectuar la transmisión parcial o total del Campeonato Mundial de Fútbol. En el caso de la radio, el público al que va dirigida la transmisión de los partidos, es principalmente gente que camina en las calles con reproductores de MP3 o MP4, pueblos donde no alcanza la televisión como cobertura y automovilistas. Con la utilización de la Norma Japonesa de TV Digital, se permite la utilización de TV en HD con dicha norma en vehículos particulares y autobuses.

Otro elemento que experimentaría un gran avance tecnológico de la mano del Mundial es el balón de fútbol. En los primeros eventos se utilizaron balones de cuero rellenados con una vejiga para darle consistencia, pero con el paso de los años fueron evolucionando y mejorando sus características. En México 1970 los balones naranjas de cuero fueron finalmente desechados, dando paso a las tradicionales pelotas de color blanco con cascos negros poligonales. Este nuevo balón fue denominado "Telstar", en honor al satélite que hacía posible la transmisión del evento a diversos rincones del orbe. En Argentina 78 y España 82 fue famoso por su difusión el balón "Tango". Cuatro años más tarde se utilizarían por primera vez materiales sintéticos para aumentar la impermeabilidad del balón y en 1986 sería el material principal del balón "Azteca". Con el paso de los años el balón ha ido mejorando progresivamente, haciéndose cada vez más liviano y veloz y perfeccionando su curvatura, hasta llegar en 2006 al "Teamgeist", que con catorce cascos (dieciocho menos que los de su antecesor, "Fevernova") unidos por termosoldadura lo hacen casi esférico en su totalidad. Para el 2010 se utilizó el balón "Jabulani". En el mundial de Brasil, 2014, se utilizó el balón "Brazuca".




</doc>
<doc id="8916" url="https://es.wikipedia.org/wiki?curid=8916" title="Péptido">
Péptido

Los péptidos (del griego πεπτός, "peptós", digerido) son un tipo de moléculas formadas por la unión de varios aminoácidos mediante enlaces peptídicos.

Los péptidos, al igual que las proteínas, están presentes en la naturaleza y son responsables de un gran número de funciones, muchas de las cuales todavía no se conocen.

La unión de un bajo número de aminoácidos da lugar a un péptido, y si el número es alto, a una proteína, aunque los límites entre ambos no están definidos. Orientativamente:

Los péptidos se diferencian de las proteínas en que son más pequeños (tienen menos de 10.000 o 12.000 Daltons de masa) y que las proteínas pueden estar formadas por la unión de varios polipéptidos y a veces grupos prostéticos. Un ejemplo de polipéptido es la insulina, compuesta por 51 aminoácidos y conocida como una hormona de acuerdo a la función que tiene en el organismo de los seres humanos.

El enlace peptídico es un enlace, covalente entre el grupo amino (–NH) de un aminoácido y el grupo carboxilo (–COOH) de otro aminoácido. Los péptidos y las proteínas están formados por la unión de aminoácidos mediante enlaces peptídicos. El enlace peptídico implica la pérdida de una molécula de agua y la formación de un enlace covalente CO-NH. Es, en realidad, un enlace amida sustituido. Podemos seguir añadiendo aminoácidos al péptido, pero siempre en el extremo COOH terminal.

Para nombrar un péptido se empieza por el aminoácido que porta el grupo –NH terminal, y se termina por el aminoácido que porta el grupo -COOH. En el sistema clásico cada aminoácido se representa por tres letras, y en el moderno, impuesto por la genética molecular, por una letra. Si el primer aminoácido de nuestro péptido fuera alanina y el segundo serina tendríamos el péptido alanil-serina, Ala-Ser, o AS.

Puesto que tienen un grupo amino terminal y un carboxilo terminal, y pueden tener grupos R ionizables, los péptidos tienen un comportamiento ácido-base similar al de los aminoácidos.

Los péptidos, al igual que aminoácidos y proteínas son biomoléculas con un carácter anfótero que permiten la regulación homeostática de los organismos.

Es de destacar este comportamiento en las enzimas, péptidos que funcionan como catalizadores biológicos de las reacciones metabólicas, ya que tienen una capacidad de funcionamiento dentro de ciertos niveles de pH. En caso de superarse se produce una descompensación de cargas en la superficie de la enzima, que pierde su estructura y su función (se desnaturaliza).

Son las mismas que las de los aminoácidos, es decir, las que den respectivamente sus grupos amino, carboxilo y R.
Estas reacciones (sobre todo las del los grupos amino y carboxilo) se han empleado para secuenciar péptidos.

En cuanto a las reacciones del grupo amino, es muy interesante la reacción con el reactivo de Sanger para secuenciar, ya que si tenemos el 2,4-dinitrofenil-péptido y lo hidrolizamos por hidrólisis ácida, se hidrolizarán todos los enlaces peptídicos y obtendremos el dinitrofenil del primer aminoácido de la secuencia, el –NH terminal, más el resto de los aminoácidos disgregados en el medio. 

Con esta reacción Sanger consiguió secuenciar la insulina.

En esta reacción, el núcleo coloreado de dinitrobenceno se une al átomo de nitrógeno del aminoácido para producir un derivado amarillo, el derivado 2,4-dinitrofenil o DNP-aminoácido. El compuesto DNFB reaccionara con el grupo amino libre del extremo amino de un polipéptido, así como también con los grupos amino de los aminoácidos libres. El enlace C–N que se forma es por lo general mucho más estable que un enlace peptídico. De esta forma, haciendo reaccionar una proteína nativa o un polipéptido intacto con el DNFB, hidrolizando la proteína en ácido y aislando los DNP-aminoácidos coloreados, puede identificarse el grupo amino terminal del aminoácido en una cadena polipeptídica. El grupo amino terminal de la lisina y algunos otros grupos funcionales de las cadenas laterales también reaccionarán con el DNFB.

Sin embargo, después de la hidrólisis, solo el derivado del grupo amino terminal del aminoácido original tendrá su grupo α-amino bloqueado; asimismo, tales DNP-α-aminoácidos pueden separarse de otros derivados DNP mediante procedimientos de extracción simples. Con cualquiera de los variados métodos cromatográficos se podrá identificar a los DNP-α-aminoácidos 

Pero este proceso consume mucha energía, ya que, teniendo el primer aminoácido hay que obtener los demás rompiendo por otras zonas.
Esto se evita con la degradación de Edman (también es una reacción de aminoácidos):
Como la ciclación se da en condiciones ácidas suaves, no se rompen los enlaces, y se da la feniltiohidantoína del aminoácido –NH terminal y queda el resto del péptido intacto.

Se separan ambos compuestos y por cromatografía se detecta. Con el resto del péptido se sigue con el mismo procedimiento hasta tener la secuencia completa.

Este método se conoce como degradación de Edman, y es la reacción que usan los secuenciadores automáticos de proteínas. Pero estos secuenciadores sólo pueden secuenciar los 20 o 30 primeros aminoácidos, por lo que tendremos que hidrolizar y seguir después. Esto es porque el rendimiento no es del 100% y perdemos péptido poco a poco, y al final no nos queda. Sólo las enzimas consiguen un rendimiento al 100%.

También podemos secuenciar empezando por el extremo carboxilo-terminal, para lo que se usan enzimas como la carboxipeptidasa. Es una proteasa que hidroliza los enlaces peptídicos. Ésta en concreto es una exoproteasa (ataca a la proteína por un extremo) que ataca al extremo carboxilo terminal.

Se emplean 2 tipos, la carboxipeptidasa A y B. Catalizan la misma reacción, pero tienen especificidad distinta. La A sólo rompe el enlace peptídico si el aminoácido carboxilo-terminal es hidrofóbico. La B lo rompe si es básico.

Hay que controlar muy bien el tiempo de reacción, ya que cuando se libera un carboxilo terminal el siguiente aminoácido se convierte en el carboxilo terminal.

Respecto a las reacciones de los grupos R, existen muchos reactivos que reaccionan de forma específica con determinados grupos R (OH de la serina, tiol de la cisteína...). Esto se usa para ver qué aminoácido es esencial para el funcionamiento de la proteína.

Dentro de las reacciones de los grupos R, una interesante desde el punto de vista de aislamiento y purificación de proteínas es la del grupo tiólico (-SH) de la cisteína, que es fuertemente reductor. En presencia de O tiene mucha tendencia a oxidarse. Si hay dos moléculas de cisteína, en presencia de oxígeno, se oxidan para originar una molécula de cistina:

Esto ocurre frecuentemente en una proteína, cuando se pliega y dos moléculas de cisteína quedan próximas en el espacio, generando un puente disulfuro. El puente disulfuro ocurre de forma natural, y debe formarse para estabilizar la estructura tridimensional de la proteína.
Sin embargo, puede que no deba ocurrir de forma natural, por ejemplo, si hay cisteínas esenciales expuestas (necesarias para la funcionalidad). 

Cuando aislamos una proteína de su entorno natural, ponemos a la proteína en presencia de oxígeno, con lo que esos grupos tiólicos se pueden oxidar, y la proteína perder su funcionalidad.

Para evitar esto, en los medios de aislamiento y purificación de proteínas añadimos β-mercapto-etanol, cuyo grupo tiólico es más reductor que el de la propia cisteína; tiene más tendencia a oxidarse.

De modo que al añadir β-mercapto-etanol, éste se oxida y protege así los grupos tiólicos de la cisteína.

Cuando queremos estudiar la composición de aminoácidos de una proteína tenemos que hidrolizarla completamente, con lo que tenemos una mezcla de todo el conjunto de aminoácidos libres que constituyen dicha proteína.

Para evitar, en toda esta manipulación, que las Cys que tengamos en el medio se oxiden, tenemos que proteger su grupo tiólico añadiendo como reactivo iodoacetato:

Así transformamos la cisteína en carboximetilcisteína.


</doc>
<doc id="8928" url="https://es.wikipedia.org/wiki?curid=8928" title="Alelo">
Alelo

("de uno para con el otro") o aleloide, es cada una de las formas alternativas que puede tener un mismo gen que se diferencian en su secuencia y que se puede manifestar en modificaciones concretas de la función de ese gen (producen variaciones en características heredadas como, por ejemplo, el color de ojos o el grupo sanguíneo). Dado que la mayoría de los mamíferos son diploides, poseen dos juegos de cromosomas, uno de ellos procedente del padre y el otro de la madre. Cada par de alelos se ubica en igual locus o lugar del cromosoma.
Por alelo debe entenderse el valor de dominio que se otorga a un gen cuando rivaliza contra otro gen por la ocupación de posición final en los cromosomas durante la separación que se produce durante la meiosis celular. De ese valor de dominación del alelo procreador resultará la trasmisión, idéntica o distinta, de la copia o serie de copias del gen procreado. De acuerdo con esa potencia, un alelo puede ser dominante y expresarse en consecuencia en el hijo solamente con una de las copias procreadoras, por lo tanto si el padre o la madre lo poseen el cromosoma del hijo lo expresará siempre; o bien puede ser un alelo recesivo, por lo tanto se necesitarán dos copias del mismo gen, dos alelos, para que se exprese en el cromosoma procreado, esto es, deberá ser provisto al momento de la procreación por ambos progenitores.

El concepto de "alelo" se entiende a partir de la palabra "alelomorfo" (en formas alelas) es decir, algo que se presenta de "diversas" formas dentro de una población de individuos.

 
Por 


Los alelos son formas alternas de un gen, que difieren en secuencia o función. 

Toda característica genéticamente determinada depende de la acción de cuando menos un par de genes homólogos, que se denominan alelos.


En función de su expresión en el fenotipo, se pueden dividir en:


La frecuencia de alelos en una población diploide se puede utilizar para predecir las frecuencias de los correspondientes genotipos (véase ley de Hardy-Weinberg). Para un modelo simple, con dos alelos:

donde "p" es la frecuencia de un alelo y "q" es la frecuencia del alelo alternativo, cuya suma da uno. Luego, "p" es la fracción de la población homocigota para el primer alelo, 2"pq" es la fracción de heterocigotas, y "q" es la fracción homocigota para al alelo alternativo. Si el primer alelo es dominante sobre el segundo, entonces la fracción de la población que mostrará un fenotipo dominante es "p" + 2"pq", y la porción con el fenotipo recesivo es "q".

Para tres alelos:

En el caso de alelos múltiples en locus diploide, el número de genotipos posibles (G) según un número de alelos (a) se da por la expresión:


http://es.wikipedia.org/wiki/Alelo#Bibliograf.C3.ADa


</doc>
<doc id="8929" url="https://es.wikipedia.org/wiki?curid=8929" title="Centrómero">
Centrómero

En Genética, el centrómero es la constricción primaria que, utilizando tinciones tradicionales, aparece menos teñida que el resto del cromosoma. Es la zona por la que el cromosoma interacciona con las fibras del huso acromático desde profase hasta anafase, tanto en mitosis como en meiosis, y es responsable de realizar y regular los movimientos cromosómicos que tienen lugar durante estas fases. Además, el centrómero contribuye a la nucleación de la cohesión de las cromátidas hermanas. En la estructura del centrómero intervienen tanto el ADN centromérico como proteínas centroméricas.

En la levadura de gemación ("Saccharomyces cerevisiae") el ADN centromérico consta únicamente de 125 pb y está conservado entre los diferentes cromosomas. Sin embargo, el ADN centromérico en metazoos puede constar de megabases, y no contiene secuencias consenso fácilmente identificables (ver la revisión de Choo en 1997). A pesar de las diferencias entre el ADN centromérico de levaduras y metazoos, el cinetocoro se ensambla en ambos casos sobre nucleosomas centroméricos que contienen una forma especializada de histona H3 (Cse4p en levaduras o su homólogo CENP-A en metazoos).

El ADN centromérico se organiza en forma de heterocromatina constitutiva, que permanece condensada en casi todas las células somáticas de un organismo. Estas regiones son pobres en genes y pueden inducir la represión de la expresión génica de las regiones adyacentes de manera epigenética. Este fenómeno se denomina "variegación por efecto de posición" (PEV, por "Position Effect Variegation"). La aparición ocasional de centrómeros "de novo" (neocentrómeros) sugiere que más que la secuencia del ADN "per se", la característica primaria de los centrómeros es la organización estructural de los dominios centroméricos. La selección del centrómero puede ser también el resultado de un complejo número de parámetros, como el momento de su replicación, la posición dentro del núcleo celular, así como otras características heredables de la estructura de la cromatina.

El centrómero tiene un comportamiento diferente durante la anafase mitótica y la anafase-I de la meiosis, de manera que durante la anafase mitótica las cromátidas hermanas se separan a polos opuestos ("segregación anfitélica") mientras que en la anafase-I de la meiosis lo que se separa a polos opuestos son los cromosomas homólogos completos, cada uno constituido por dos cromátidas ("segregación sintélica").

Cada cromosoma posee dos brazos, uno largo (llamado "q") y otro corto (llamado "p") separados por el centrómero, los cuales se conectan de forma metacéntrica, submetacéntrica, acrocéntrica, holocéntrica o telocéntrica. 

Un cromosoma metacéntrico es un cromosoma cuyo centrómero se encuentra en la mitad del cromosoma, dando lugar a brazos de igual longitud.

Cuatro pares de los cromosomas humanos poseen una estructura metacéntrica, el 1, el 3, el 19 y el 20.

Un cromosoma submetacéntrico es un cromosoma en el cual el centrómero se ubica de tal manera que un brazo es ligeramente más corto que el otro.

La mayor parte de los cromosomas humanos son submetacéntricos excepto los cromosomas 1, 3, 19, 20 y el X que son metacéntricos y 13, 14, 15, 21 y 22 que son acrocéntricos. Además, el cromosoma Y a veces es considerado submetacéntrico aunque otros lo describen como acrocéntrico sin satélite.

Un cromosoma acrocéntrico es un cromosoma en el que el centrómero se encuentra más cercano a uno de los telómeros, dando como resultado un brazo muy corto (p) y el otro largo (q).

De los 23 pares de cromosomas humanos el cromosoma 13, el 14, el 15, el 21 y el 22 son acrocéntricos y actúan como organizadores nucleolares.

Aun cuando el concepto es ampliamente aceptado y distribuido entre la comunidad científica, realmente, un cromosoma telocéntrico como tal no existe. Supuestamente en este tipo de cromosomas el centrómero está localizado en un extremo del mismo, pero la región telocéntrica no permite que molecularmente haya otra estructura finalizando al cromosoma. De hecho, el acortamiento del telomero o su ausencia total causa inestabilidad en los cromosomas y la consecuente Translocación robertsoniana. Por tanto, el término telocéntrico es incorrecto y debe considerarse el término subtelocéntrico, el cual implica que el telómero se ubica al final así no sea visible y que el centrómero esta después invariablemente.

Ninguno de los cromosomas humanos presenta esta característica; pero, por ejemplo, los 40 cromosomas del ratón común son subtelocéntricos.

Los análisis llevados a cabo en "S. cerevisiae" para aislar el ADN centromérico (ADN CEN) de todos sus cromosomas, han establecido que en todos los centrómeros de levaduras estudiados existen tres regiones muy conservadas:


En la cromatina en su forma nativa, el ADN CEN es un segmento de 220-250 pb protegido de la acción de las nucleasas y flanqueado en ambos extremos por sitios hipersensibles al corte y un conjunto de nucleosomas altamente organizados, que contienen la histona especializada Cse4p (el homólogo en levaduras de CENP-A) en lugar de H3. 

Las mutaciones en las regiones I y II reducen pero no inactivan la función del centrómero, mientras que las que ocurren en la región III lo inactivan completamente. En los mutantes por deleción de la región CDEII, la función centromérica puede restablecerse casi completamente insertando una secuencia de ADN compuesta sólo de A-T al azar y de tamaño equivalente. Por tanto, en esta región los factores críticos para una segregación cromosómica correcta son el contenido en A-T y la longitud del ADN, más que la secuencia de nucleótidos, quizás porque influyen en la conformación del ADN centromérico.

CDEIII es la zona de unión del complejo proteico CBF3 (por "Centromere Binding Factor 3"), compuesto por las proteínas Ndc10p, Cep3p, Ctf13p, y Skp1p (esta proteína - también denominada p23- es además parte del complejo SCUL implicado en los procesos de degradación mediados por ubiquitina necesarios para la progresión a través del ciclo celular). En ausencia de CBF3, el cinetocoro no es funcional, tanto "in vivo" como "in vitro", y todas las proteínas del cinetocoro conocidas, incluida Cse4p, presentan alteración de la asociación con el centrómero. Sin embargo, la unión de CBF3 al ADN centromérico "in vivo" no requiere Cse4p. Por tanto, la unión específica de CBF3 a la región CDEIII participa en la definición de la localización del cinetocoro en levaduras. Otra proteína esencial para la viabilidad de las células de levaduras es CBF5p, que co-purifica con el complejo CBF3 en condiciones de baja astringencia. Esta proteína también se une a microtúbulos "in vitro", y parece ser importante en la transición G/S del ciclo celular, ya que la eliminación de CBF5p bloquea la división celular antes de que ocurra la replicación del ADN.

Por otro lado, CDEI es el sitio de unión de un homodímero de Cbf1p. Cbf1p no es esencial para la función del cinetocoro, pero induce el plegado del ADN y por tanto puede contribuir a la estructura de nivel superior del centrómero. Cbf1p tiene similitud estructural e identidad de secuencia limitada a CENP-B, que se une al ADN centromérico de metazoos y también induce el plegado del ADN. 

Mif2p es una proteína esencial en levaduras, similar a CENP-C de metazoos, cuya eliminación produce fallos en la segregación cromosómica, retraso en mitosis y microtúbulos con morfología aberrante. MIF2 interacciona genéticamente con tres de los genes que codifican proteínas centroméricas: CBF1, CBF3a y CBF3b. Aunque la unión de Mif2 al centrómero depende de Ndc10, su localización en el ADN CEN está muy disminuida en mutantes para Cse4. Mif2 presenta un dominio acídico y otro dominio rico en prolina, lo que se denomina un "gancho AT" ("AT hook"), un motivo que es común a las proteínas que se unen a secuencias de ADN ricas en AT (como las proteínas HMGI(Y) de mamíferos), lo que sugiere que Mif2 se une a la región CDEII.

En "S. cerevisiae", el ADN centromérico se replica en una etapa temprana de la fase S, tal vez porque es necesario replicar el ADN centromérico para iniciar el ensamblaje de los cinetocoros hermanos.

El ADN centromérico de "Schizosaccharomyces pombe" es considerablemente más complejo que el de "S. cerevisiae" y comparte algunas propiedades con los centrómeros regionales de los eucariotas superiores. El centrómero de "S. pombe" está constituido por 40-100 kb de ADN organizado en distintos tipos de repeticiones específicas de los centrómeros (las repeticiones tipo K), que están a su vez organizadas en una gran repetición invertida. Como ocurre en eucariotas superiores, la organización de los centrómeros en la levadura de fisión varía considerablemente entre diferentes cromosomas y entre cepas muy próximas. El centro de la repetición invertida, el núcleo central, contiene una secuencia de 4-7 kb que es fundamental para la función centromérica. Las regiones centrales del centrómero de "S. pombe" se organizan en una estructura inusual, que depende de la existencia de un elemento dentro de las repeticiones tipo K (denominado "enhancer" centromérico) y que es fundamental para la función del centrómero. Las regiones K y el núcleo central son las dianas de los mecanismos epigenéticos que afectan a la función del centrómero "in vivo". Por otro lado, existe redundancia funcional tanto entre las repeticiones tipo K como en el núcleo central.

La función precisa de las secuencias repetidas de ADN en el centrómero no está muy clara, pero probablemente tienen una función estructural en el apareamiento y la segregación cromosómica (revisado por Karpen y Allshire en 1997), y sólo indirectamente producen silenciamiento transcripcional. Las secuencias repetidas más externas de los centrómeros de "Schizosaccharomyces pombe" son heterocromáticas y son necesarias para el ensamblaje de un centrómero activo. A partir de estas secuencias repetidas se generan tránscritos que son procesados por los componentes de la maquinaria de RNAi y medían el silenciamiento de la cromatina. (Véase también Ensamblaje de heterocromatina mediante RNAi en "S. pombe").

La determinación de los centrómeros de metazoos constituye una tarea difícil y esquiva. En animales y plantas, los centrómeros están incluidos en regiones de ADN satélite altamente repetido, que resulta difícil de analizar incluso con los métodos de mapeo más potentes. Estas regiones de ADN satélite están embebidas en regiones de heterocromatina constitutiva, que se mantiene silenciada en la mayor parte de las células somáticas de un organismo. La ausencia mayoritaria de genes activos en las regiones centroméricas es una característica que parece que se ha adquirido progresivamente a través de la evolución.

En "Drosophila melanogaster" la secuencia AATAACATAG está repetida en tándem en regiones próximas al centrómero. Dado que estas secuencias cortas de 10 pb no son representativas del genoma de una especie, suele suceder que su contenido en G+C es diferente al contenido en G+C del resto del genoma. Esto hace que cuando el ADN de una especie eucarionte se centrifuga en gradiente de densidad de cloruro de cesio, aparezca una banda principal que contiene la mayor parte del ADN de la especie y una banda satélite (minoritaria) que está formada por una secuencia corta de ADN repetida en tándem. El ADN satélite en algunas especies tiene mayor densidad que el ADN principal (mayor contenido en G+C) y en otras especies tiene menor densidad y, por tanto, menor contenido en G+C que el ADN principal. Cuando el ADN satélite de ratón se marca radiactivamente y se realiza una hibridación "in situ" con el ADN de cromosomas metafásicos mitóticos, se observa que el marcaje radiactivo (hibridación) se produce en regiones próximas al centrómero. El ADN satélite también se denomina α-satélite y es uno de los componentes del genoma eucariótico que evoluciona más rápidamente. 
A pesar de que en la mayor parte de los casos no se han detectado motivos especialmente definidos, una secuencia candidata en el ADN satélite tendría que estar repetida en cientos de kilobases, ya que éste es el tamaño mínimo de un centrómero funcional que se ha identificado en diferentes organismos. Por ejemplo, en Drosophila la unidad mínima necesaria de repeticiones en tándem es de 420 kb, en maíz se necesitan 500 kb y en humanos la unidad mínima consiste de 100 kb. Una característica interesante de la mayor parte del ADN satélite es su unidad de longitud, ya que aunque no se han detectado secuencias con motivos conservados, la longitud de la unidad que se repite es muy parecida entre organismos. En primates, por ejemplo, la unidad básica que se repite tiene 171 pb, en el pez "Sparus aurata" la repetición centromérica tiene 186 pb, en el insecto "Chironomus pallidivittatus" tiene 155 pb, en "Arabidopsis thaliana" y en el maíz tiene 180 pb, y en el arroz 168 pb. La estrecha variación en longitud de la unidad que se repite en el ADN satélite corresponde aproximadamente al rango de longitud del ADN que rodea a un nucleosoma, y repeticiones más largas, como las que se encuentran en los centrómeros del cerdo (340 pb) corresponden aproximadamente a la longitud de dos nucleosomas. Hay excepciones notorias, como las repeticiones pentaméricas que se encuentran en "Drosophila melanogaster". En general, la selección de la longitud de un nucleosoma podría limitar la evolución del ADN centromérico, de acuerdo con su función estructural en el genoma. 

Estas unidades mínimas (denominadas monómeros) se encuentran normalmente asociadas de forma cabeza-cola. En las regiones centroméricas del núcleo funcional, el ADN satélite se organiza en una unidad repetida que consta de múltiples monómeros. La unidad multimonomérica se repite a su vez muchas veces, generando un vector (array) de nivel superior. Los vectores de nivel superior de ADN satélite son la organización típica de las regiones centroméricas humanas y se extienden a través de megabases de ADN que se encuentran mayoritariamente ininterrumpidos por ningún tipo de inserción o mutación. Por tanto en animales y plantas se presentan centrómeros "regionales", frente a los centrómeros "puntuales" que se encuentran en levaduras.

Sin embargo, a pesar de que el ADN satélite se encuentra en los centrómeros humanos nativos, se han detectado centrómeros humanos generados "de novo" (neocentrómeros) que carecen de α-satélite u otras repeticiones en tándem, lo que indica que el ADN satélite no es fundamental para definir un centrómero funcional.

Una característica conservada y heredable de los centrómeros es la presencia en sus nucleosomas de una variante especial de la histona H3, que se encuentra únicamente en el núcleo de la región centromérica. Esta histona específica de los centrómeros se denomina CENP-A en mamíferos ("centromeric protein A"), Cid ("centromere identifier") en "Drosophila", Cse4 en "S. cerevisiae" y Cnp1 en "S. pombe" (revisado por Choo en 2001). La presencia de esta variante de la histona H3 parece ser fundamental para el ensamblaje del cinetocoro y distingue la placa interna del cinetocoro de la heterocromatina pericéntrica, que contiene la histona H3 normal. CENP-A presenta algunas características que la diferencian de la histona H3 normal, como una cola NH-terminal no canónica, un plegamiento divergente y una región lazo 1 más largo. Aunque la histona H3 está sometida a una fuerte selección evolutiva, las histonas centroméricas son sorprendentemente divergentes. Esta diferencia podría deberse a la necesidad de H3 de interaccionar con todo el genoma, mientras que la variante centromérica sólo necesita interaccionar con el ADN centromérico correspondiente. Este ADN está formado por ADN satélite, que es uno de los componentes del genoma eucariótico que evoluciona más rápidamente. Se ha propuesto que la interacción entre la histona centromérica y el ADN centromérico es responsable de la longitud similar a la que rodea un nucleosoma de las repeticiones del ADN satélite.

Un estudio realizado en "Drosophila" identificó que las regiones centrales de los centrómeros se replican como dominios aislados en estadios tempranos de la fase S, antes de la replicación de la heterocromatina pericéntrica, que se replica de forma tardía. Si en el momento que se replican los centrómeros, la región del núcleo en la que se localizan los centrómeros excluye la histona H3 pero secuestra la histona centromérica, la compartimentalización aseguraría que sólo CENP-A esté disponible para el ensamblaje de la cromatina centromérica. Este modelo se apoya en varias líneas de evidencias en diferentes organismos.

Además de CENP-A, se han identificado otros componentes constitutivos en el centrómero. Uno de ellos es CENP-C, que está conservado evolutivamente, aunque sólo comparte un motivo de 20 aminoácidos con su homólogo en "S. cerevisiae", Mif2. También se han encontrado homólogos en otras especies, como HCP-4 en "C. elegans". La localización centromérica de CENP-C depende de CENP-A y se ha sugerido que CENP-C podría interaccionar con la estructura de la cromatina alterada por CENP-A. Sin embargo, aunque la zona de unión de CENP-C al ADN se ha mapeado en la zona central de la proteína, no se ha identificado una secuencia específica de unión. Parece además que CENP-C presenta la capacidad de unirse a ARN de forma específica, aunque la contribución de estas capacidades a la localización de CENP-C no está clara.

CENP-B es la única proteína centromérica que se une a una secuencia de ADN específica de 17 pb (la "caja CENP-B"), que se encuentra en un subconjunto de monómeros de α-satélite. La función de CENP-B en los centrómeros no está clara, ya que a diferencia con CENP-A o CENP-C, CENP-B no es esencial para la función mitótica (de hecho, el ratón knockout para CENP-B es viable). CENP-B puede estar presente tanto en centrómeros activos como inactivos, lo que sugiere que no está asociada simplemente a la función centromérica. Además, algunos centrómeros funcionales carecen de cajas CENP-B (como el centrómero del cromosoma Y, por ejemplo). Sin embargo, se ha demostrado que la unión de CENP-B aumenta la eficiencia de la unión de CENP-A en cromosomas humanos artificiales. A pesar de no ser esencial en humanos, CENP-B está conservada a través de varios phyla y en "S. pombe" se encuentran tres homólogos que sí son esenciales para la viabilidad celular.

Estas tres proteínas centroméricas están organizadas de forma diferente en el centrómero humano. CENP-B está presente en todo el vector de nivel superior, mientras que CENP-A y CENP-C se encuentran sólo en algunos bloques de unidades repetidas, intercalados con bloques que contienen nucleosomas canónicos (que incluyen histona H3) y con modificaciones de histonas más características de eucromatina que de heterocromatina. Se cree que esos bloques de CENP-A se auto-organizan para presentar una "superficie" combinada que organiza el resto de las proteínas del cinetocoro, que servirá como sitio de anclaje de los microtúbulos. Se considera además que los monómeros pericéntricos que flanquean las repeticiones centroméricas están generalmente desprovistos de proteínas centroméricas, y están empaquetados en nucleosomas canónicos que poseen modificaciones de histonas características de heterocromatina, y unidos a proteínas específicas de heterocromatina como HP1. Esta heterocromatina pericéntrica es importante tanto para definir los límites de los dominios centroméricos como para reclutar las cohesinas que mantendrán unidas las cromátidas hermanas hasta la anafase durante el ciclo celular.

El proceso de segregación cromosómica está sometida a una fuerte presión evolutiva, dado que la pérdida o ganancia de cromosomas (una situación denominada aneuploidía) puede producir importantes alteraciones fenotípicas, como el síndrome de Down en humanos, por ejemplo. Por ello, la maquinaria encargada de distribuir los cromosomas entre las células hijas durante la división celular presenta una gran sofisticación y está sometida a un estricto control (véase checkpoint de mitosis). Los centrómeros son las regiones cromosómicas sobre las que se ensamblan los cinetocoros, que son las estructuras proteicas responsables del anclaje de los cromosomas al huso mitótico, y por ello la zona responsable del movimiento cromosómico y su regulación. Sin embargo, a pesar de ello las secuencias de ADN que definen las secuencias centroméricas están muy poco conservadas y evolucionan rápidamente incluso entre especies muy relacionadas. Esto no quiere decir que las secuencias de ADN del centrómero son hipermutables, sino que las variantes de las secuencias se fijan por expansión y contracción, y pueden aparecer "de novo" en sitios nuevos (neocentrómeros). Estos cambios en el ADN centromérico tienen lugar debido a la existencia de diferentes procesos mutacionales, como errores en la replicación del ADN, intercambio desequilibrado, transposición y excisión. Las proteínas centroméricas también presentan signos inesperados de una rápida evolución. Por todo ello, se ha sugerido que en el núcleo de esta rápida evolución existe un conflicto genético en funcionamiento.

Parece ser que la arquitectura en un vector ("array") de nivel superior que se observa en los centrómeros de humanos podría haber aparecido recientemente en un centrómero en la evolución de los primates (alrededor de la separación gorila-orangután) y se extendió a los otros cromosomas vía transposición. Posteriormente, los intercambios desiguales o conversiones génicas amplificaron los vectores de nivel superior, dando lugar a la arquitectura en vectores centroméricos de nivel superior que es específica de la especie humana y que se observa en diferentes cromosomas humanos. Además, se han generado algunas variantes por mutación que se han fijado en algunos centrómeros. La comparación de unidades monoméricas y unidades vectoriales de nivel superior que se encuentran en los centrómeros de cromosomas ortólogos (por ejemplo, entre chimpancés y humanos) ha llevado al descubrimiento sorprendente de que los vectores centroméricos de diferentes especies son más divergentes entre sí que las unidades pericéntricas. Esta observación es anti-intuitiva, porque el vector de ADN satélite centromérico es el centrómero funcional y está sometido a una fuerte presión selectiva, mientras que las regiones de heterocromatina pericéntrica no lo están. Por tanto, la observación es paradójica: las unidades de ADN satélite que están fuertemente limitadas dentro de una especie han evolucionado rápidamente entre especies. 

Esta paradoja ha llevado a pensar que alguna fuerza selectiva debe dirigir la rápida fijación de las mutaciones en los vectores centroméricos, imponiendo un sesgo para manterner las mutaciones, incrementando de esta forma las tasas de mutación del vector completo. Se ha sugerido que esta fuerza selectiva puede ser la ventaja conferida a los centrómeros durante la meiosis femenina, o "deriva-centromérica": nuevas variaciones en la secuencia de α-satélite, una nueva organización o simplemente un incremento en la cantidad de α-satélite proporciona una mayor oportunidad de incorporación de CENP-A y por tanto una mayor capacidad para la unión de microtúbulos. La asimetría de la tétrada meiótica femenina proporciona una oportunidad para los cromosomas de competir por ser incluido en el núcleo del ovocito mediante una orientación favorable durante la meiosis. Los centrómeros que aprovechan esta oportunidad en meiosis I "ganan", y una ligera ventaja en cada meiosis femenina es suficiente para fijar una variación centromérica favorable. 

Como contrapartida, mientras que la deriva centromérica puede generar una ventaja selectiva en la meiosis femenina, puede producir defectos en la meiosis masculina, pues en este caso un centrómero mutado se apareará con otro normal, generándose una diferencia de tensión que puede activar el checkpoint de mitosis, provocando la muerte celular y con ello una disminución de la fertilidad masculina. Una forma de contrarrestar este efecto en la meiosis masculina sería la aparición de mutaciones en las proteínas centroméricas con alteración en su capacidad de unión al ADN y que equilibraran la tensión centromérica. La proteína candidata más probable es CENP-A.

Si este proceso tiene lugar en dos poblaciones aisladas de la misma especie, las configuraciones del ADN satélite y CENP-A divergirán rápidamente. En cada población, CENP-A evolucionará para suprimir los efectos deletéreos de la evolución del ADN satélite. De esta forma, las nuevas variantes de CENP-A resultarán incompatibles con el ADN satélite de la otra población. Cruces entre ambas poblaciones resultarán en defectos en los híbridos. Por tanto, el proceso evolutivo entre CENP-A y el ADN satélite da lugar al inicio del aislamiento reproductivo entre las dos poblaciones (véase también Mecanismos de aislamiento reproductivo). Esto quiere decir que la evolución centromérica tiene como consecuencia inevitable la especiación.






</doc>
<doc id="8930" url="https://es.wikipedia.org/wiki?curid=8930" title="Terapia génica">
Terapia génica

La terapia génica consiste en la inserción de elementos funcionales ausentes en el genoma de un individuo. Se realiza en las células y tejidos con el objetivo de tratar una enfermedad o realizar un marcaje.

La técnica todavía está en desarrollo, motivo por el cual su aplicación se lleva a cabo principalmente dentro de ensayos clínicos controlados, y para el tratamiento de enfermedades severas o bien de tipo hereditario o adquirido. Al principio se planteó sólo para el tratamiento de enfermedades genéticas, pero hoy en día se plantea ya para casi cualquier enfermedad.

Entre los criterios para elegir este tipo de terapia se encuentran:





Aunque se han utilizado enfoques muy distintos, en la mayoría de los estudios de terapia génica, una copia del gen funcional se inserta en el genoma para compensar el defectivo. Si ésta copia simplemente se introduce en el huésped, se trata de terapia génica de adición. Si tratamos, por medio de la recombinación homóloga, de eliminar la copia defectiva y cambiarla por la funcional, se trata de terapia de sustitución.

Actualmente, el tipo más común de vectores utilizados son los virus, que pueden ser genéticamente alterados para dejar de ser patógenos y portar genes de otros organismos. No obstante, existen otros tipos de vectores de origen no vírico que también han sido utilizados para ello. Así mismo, el ADN puede ser introducido en el paciente mediante métodos físicos (no biológicos) como electroporación, biobalística...

Las células diana del paciente se infectan con el vector (en el caso de que se trate de un virus) o se transforman con el ADN a introducir. Este ADN, una vez dentro de la célula huésped, se transcribe y traduce a una proteína funcional, que va a realizar su función, y, en teoría, a corregir el defecto que causaba la enfermedad.

La gran diversidad de situaciones en las que podría aplicarse la terapia génica hace imposible la existencia de un solo tipo de vector adecuado. Sin embargo, pueden definirse las siguientes características para un "vector ideal" y adaptarlas luego a situaciones concretas:


Los vectores van a contener los elementos que queramos introducir al paciente, que no van a ser sólo los genes funcionales, sino también elementos necesarios para su expresión y regulación, como pueden ser promotores, potenciadores o secuencias específicas que permitan su control bajo ciertas condiciones.

Podemos distinguir dos categorías principales en vectores usados en terapia génica: virales y no virales.

Todos los virus son capaces de introducir su material genético en la célula huésped como parte de su ciclo de replicación. Gracias a ello, pueden producir más copias de sí mismos, e infectar a otras células.

Algunos tipos de virus insertan sus genes físicamente en el genoma del huésped, otros pasan por varios orgánulos celulares en su ciclo de infección y otros se replican directamente en el citoplasma, por lo que en función de la terapia a realizar nos puede interesar uno u otro.

Algo común a la mayoría de estrategias con virus es la necesidad de usar líneas celulares "empaquetadoras" o virus helpers, que porten los genes que les eliminamos a nuestros vectores y que permiten la infección.

El genoma de los retrovirus está constituido por ARN de cadena sencilla, en el cual se distinguen tres zonas claramente definidas: una intermedia con genes estructurales, y dos flanqueantes con genes y estructuras reguladoras. Cuando un retrovirus infecta a una célula huésped, introduce su ARN junto con algunas enzimas que se encuentran en la matriz, concretamente una proteasa, una transcriptasa inversa y una integrasa.

La acción de la retrotranscriptasa permite la síntesis del ADN genómico del virus a partir del ARN. A continuación, la integrasa introduce este ADN en el genoma del huésped. A partir de este punto, el virus puede permanecer latente o puede activar la replicación masivamente.

Para usar los retrovirus como vectores víricos para terapia génica inicialmente se eliminaron los genes responsables de su replicación y se reemplazaron estas regiones por el gen a introducir seguido de un gen marcador.

Del genoma vírico quedaban las secuencias LTR; y los elementos necesarios para producir los vectores a gran escala y para transformar las células son aportados desde otros vectores, bien plasmídicos o bien en líneas celulares específicas. En el caso de usar vectores plasmídicos, estrategias como cotransformar con varios plásmidos distintos que codifiquen para las proteínas del retrovirus, y que la transcripción de sus secuencias esté sometida a promotores eucariotas puede contribuir a minimizar el riesgo de que por recombinación se generen virus recombinantes.

Actualmente se buscan estrategias como la anterior para conseguir una mayor seguridad en el proceso. La adición de colas de poliadenina al transgén para evitar la transcripción de la segunda secuencia LTR es un ejemplo de esto.

Los retrovirus como vector en terapia génica presentan un inconveniente considerable, y es que la enzima integrasa puede insertar el material genético en cualquier zona del genoma del huésped, pudiendo causar efectos deletéreos como la modificación en el patrón de la expresión (efecto posicional) o la mutagénesis de un gen silvestre por inserción.

Ensayos de terapia génica utilizando vectores retrovirales para tratar la inmunodeficiencia combinada grave ligada al cromosoma X (X-SCID) representan la aplicación más exitosa de la terapia hasta la fecha. Así, más de veinte pacientes han sido tratado en Francia y Gran Bretaña, con una alta tasa de reconstitución del sistema inmunitario. Sin embargo, ensayos similares fueron restringidos en los Estados Unidos cuando se informó de la aparición de leucemia en pacientes.

 Los adenovirus presentan un genoma de ADN bicatenario, y no integran su genoma cuando infectan a la célula huésped, sino que la molécula de ADN permanece libre en el núcleo celular y se transcribe de forma independiente. Esto supone que el efecto posicional o la mutagénesis por inserción no se dan en estos vectores, lo cual no quiere decir que no tengan otros inconvenientes. Además, debido al hecho de que en su ciclo natural se introducen en el núcleo de la célula, pueden infectar tanto células en división como células quiescentes.

A los vectores de primera generación se les eliminó parte del gen E1, básica para la replicación, y a los de 2.ª, se les eliminaron otros genes tempranos en el ciclo del virus. En ambos casos, cuando se realiza una infección con una concentración elevada de virus, se produce la expresión de otros genes que provocan una respuesta inmune considerable.

Por ello, los últimos vectores basados en adenovirus prácticamente han sido desprovistos de la mayor parte de sus genes, con la excepción de las regiones ITR (regiones repetidas de forma invertida), y la zona necesaria para la encapsidación.

 Los AAV son virus pequeños con un genoma de ADN monocatenario. Pueden integrarse específicamente en el cromosoma 19 con una alta probabilidad. Sin embargo, el VAA recombinante que se usa como vector y que no contiene ningún gen viral, solo el gen terapéutico, no se integra en el genoma. En su lugar, el genoma vírico recombinante fusiona sus extremos a través del ITR (repeticiones terminales invertidas), apareciendo recombinación de la forma circular y episomal que se predice que pueden ser la causa de la expresión génica a largo plazo.

Las desventajas de los sistemas basados en AAV radican principalmente en la limitación del tamaño de DNA recombinante que podemos usar, que es muy poco, dado el tamaño del virus. También el proceso de producción e infección resultan bastante complejos. No obstante, como se trata de un virus no patógeno en la mayoría de los pacientes tratados no aparecen respuestas inmunes para eliminar el virus ni las células con las que han sido tratados.

 Los herpesvirus son virus de ADN capaces de establecer latencia en sus células huésped. Son complejos genéticamente hablando, pero para su uso como vectores tienen la ventaja de poder incorporar fragmentos de DNA exógeno de gran tamaño (hasta unas 30 kb). Además, aunque su ciclo lítico lo realizan en el lugar de infección, establecen la latencia en neuronas, las cuales están implicadas en numerosas enfermedades del sistema nervioso, y son por ello dianas de gran interés.

Los vectores herpesvíricos puestos en marcha han usado dos estrategias principales:

- La recombinación homóloga entre el genoma del virus completo y el contenido en un plásmido que llevaba el transgén en la zona que codifica para genes no esenciales en lo que se refiere a replicación e infección.

- El uso de vectores con orígenes de replicación del virus así como las correspondientes secuencias de empaquetamiento, y su introducción en estirpes celulares bien coinfectadas con virus silvestres o bien portadoras del resto de genes del mismo implicados en la encapsidación y replicación, para permitir la formación de partículas virales recombinantes con las que realizar el tratamiento.

No obstante, el uso de vectores basados en el HSV (virus del herpes simple), sólo puede llevarse a cabo en pacientes que no hayan sido infectados previamente por él, pues pueden presentar inmunidad.

 Los vectores virales descritos anteriormente tienen poblaciones naturales de células huésped que ellos infectan de manera eficiente. Sin embargo, algunos tipos celulares no son sensibles a la infección por estos virus.

Por ejemplo, el vector retrovírico más popular para el uso en pruebas de terapia génica ha sido el virus de la inmunodeficiencia en simios revestido con la cubierta de proteínas G del virus de la estomatitis vesicular. Este vector se conoce como VSV y puede infectar a casi todas las células, gracias a la proteína G con la cual este vector es revestido.

 Estos métodos presentan ciertas ventajas sobre los métodos virales, tales como facilidades de producción a gran escala y baja inmunogenicidad. Anteriormente, los bajos niveles de transfección y expresión del gen mantenían a los métodos no virales en una situación menos ventajosa; sin embargo, los recientes avances en la tecnología de vectores han producido moléculas y técnicas de transfección con eficiencias similares a las de los virus.

Éste es el método más simple de la transfección no viral. Consiste en la aplicación localizada de, por ejemplo, un plásmido con ADN desnudo. Varios de estos ensayos dieron resultados exitosos. Sin embargo, la expresión ha sido muy baja en comparación con otros métodos de transformación. Además de los ensayos con plásmidos, se han realizado ensayos con productos de PCR, y se ha obtenido un éxito similar o superior. Este logro, sin embargo, no supera a otros métodos, lo que ha llevado a una investigación con métodos más eficientes de transformación, tales como la electroporación, la sonicación, o el uso de la biobalística, que consiste en disparar partículas de oro recubiertas de ADN hacia las células utilizando altas presiones de gas.

El uso de oligonucleótidos sintéticos en la terapia génica tiene como objetivo la inactivación de los genes implicados en el proceso de la enfermedad.

Existen varias estrategias para el tratamiento con oligonucleótidos

Una estrategia, la terapia "antisentido" utiliza oligonucleótidos con la secuencia complementaria al RNAm del gen diana, lo que activa un mecanismo de silenciamiento génico. También se puede usar para alterar la transcripción del gen defectuoso, modificando por ejemplo su patrón de edición de intrones y exones.

También se hace uso de moléculas pequeñas de RNAi para activar un mecanismo de silenciamiento génico similar al de la terapia antisentido

Otra posibilidad es utilizar oligodesoxinucleótidos como un señuelo para los factores que se requieren en la activación de la transcripción de los genes diana. Los factores de transcripción se unen a los señuelos en lugar de al promotor del gen defectuoso, lo que reduce expresión de los genes diana.
Además, oligonucleótidos de ADN monocatenario han sido utilizados para dirigir el cambio de una única base dentro de la secuencia de un gen mutante.

Al igual que los métodos de ADN desnudo, requieren de técnicas de transformación para introducirse en la célula.

La creación de cromosomas humanos artificiales (HACs) estables es una de las posibilidades que se baraja en la actualidad como una de las formas de introducir ADN permanentemente en células somáticas para el tratamiento de enfermedades mediante el uso de la terapia génica. Presentan una elevada estabilidad, además de permitir introducir grandes cantidades de información genética.

El vector de ADN puede ser cubierto por lípidos formando una estructura organizada, como una micela o un liposoma. Cuando la estructura organizada forma un complejo con el ADN entonces se denomina lipoplexe.

Hay tres tipos de lípidos: aniónicos, neutros, o catiónicos. Inicialmente, lípidos aniónicos y neutros eran utilizados en la construcción de lipoplexes para vectores sintéticos. Sin embargo, estos son relativamente tóxicos, incompatibles con fluidos corporales y presentan la posibilidad de adaptarse a permanecer en un tejido específico. Además, son complejos y requieren tiempo para producirlos, por lo que la atención se dirigió a las versiones catiónicas. Éstos, debido a su carga positiva, interaccionan con el ADN, que presenta carga negativa, de tal forma que facilita la encapsulación del ADN en liposomas. Más tarde, se constató que el uso de lípidos catiónicos mejoraba la estabilidad de los lipoplexes. Además, como resultado de su carga, los liposomas catiónicos interactúan también con la membrana celular, y se cree que la endocitosis es la principal vía por la que las células absorben los lipoplexes.

No obstante, los lípidos catiónicos presentan efectos tóxicos dependientes de dosis, lo que limita la cantidad que de ellos se puede usar y por tanto la terapia en sí.

Debido a las deficiencias de muchos de los sistemas de transferencia génica se han desarrollado algunos métodos híbridos que combinan dos o más técnicas. Los virosomas son un ejemplo, y combinan liposomas con el virus inactivado VIH o el virus de la gripe.

 Un dendrímero es una macromolécula muy ramificada con forma esférica o variable. Su superficie puede ser funcional de muchas formas y de ésta derivan muchas de sus propiedades. Además, su tamaño, en la escala nano-, permite su uso en biomedicina.

Las células diana se seleccionan en función del tipo de tejido en el que deba expresarse el gen introducido, y deben ser además células con una vida media larga, puesto que no tiene sentido transformar células que vayan a morir a los pocos días. Igualmente, se debe tener en cuenta si la diana celular es una célula en división o quiescente, porque determinados vectores virales, como los retrovirus, sólo infectan a células en división.

En función de estas consideraciones, las células diana ideales serían las células madre, puesto que la inserción de un gen en ellas produciría un efecto a largo plazo. Debido a la experiencia en trasplante de médula ósea, una de las dianas celulares más trabajadas son las células madre hematopoyéticas. La terapia génica en estas células es técnicamente posible y es un tejido muy adecuado para la transferencia ex vivo. Otras dianas celulares con las que se ha trabajado son:


La terapia génica apareció a partir de la década de 1970 para intentar tratar y paliar enfermedades de carácter genético y se dieron las primeras pruebas con virus, las cuales fracasaron. Años más tarde, en la década de 1980, se intentó tratar la talasemia usando betaglobina. En este caso fue un éxito en modelos animales aunque no se pudo usar en humanos.

En 1990, W. French Anderson propone el uso de células de médula ósea tratadas con un vector retroviral que porta una copia correcta del gen que codifica para la enzima adenosina desaminasa, la cual se encuentra mutada. Es una enfermedad que forma parte del grupo de las inmunodeficiencias severas combinadas (SCID). Realizó la transformación ex-vivo con los linfocitos T del paciente, que luego se volvieron a introducir en su cuerpo. Cinco años más tarde, publicaron los resultados de la terapia, que contribuyó a que la comunidad científica y la sociedad consideraran las posibilidades de esta técnica.

No obstante, el apoyo a la terapia fue cuestionado cuando algunos niños tratados para SCID desarrollaron leucemia. Las pruebas clínicas se interrumpieron temporalmente en el 2002, a causa del impacto que suposo el caso de Jesse Gelsinger, la primera persona reconocida públicamente como fallecida a causa de la terapia génica. Su muerte se debió al uso del vector adenoviral para la transducción del gen necesario para tratar su enfermedad, lo cual causó una excesiva respuesta inmune, con un fallo multiorgánico y muerte cerebral. Existe una bibliografía numerosa sobre el tema, y es destacable el informe que la FDA emitió señalando el conflicto de intereses de algunos de los médicos implicados en el caso así como los fallos en el procedimiento. En el año 2002, cuatro ensayos en marcha de terapia génica se paralizaron al desarrollarse en un niño tratado una enfermedad similar a la leucemia. Posteriormente, tras una revisión de los procedimientos, se reanudaron los proyectos en marcha.

Un equipo de investigadores de la Universidad de California, en Los Ángeles, insertó genes en un cerebro utilizando liposomas recubiertos de un polímero llamado polietilenglicol (PEG). La transferencia de genes en este órgano es un logro significativo porque los vectores virales son demasiado grandes para cruzar la barrera hematoencefálica. Este método tiene el potencial para el tratamiento de la enfermedad del Parkinson.

También en ese año se planteó la interferencia por ARN para tratar la enfermedad de Huntington.

 Científicos del NIH tratan exitosamente un melanoma metastásico en dos pacientes, utilizando células T para atacar a las células cancerosas. Este estudio constituye la primera demostración de que la terapia génica puede ser efectivamente un tratamiento contra el cáncer.

En noviembre del mismo año, Preston Nix de la Universidad de Pensilvania informó sobre VRX496, una inmunoterapia para el tratamiento del HIV que utiliza un vector lentiviral para transportar un DNA antisentido contra la envuelta del HIV. Fue la primera terapia con un vector lentiviral aprobada por la FDA para ensayos clínicos. Los datos de la fase I/II ya están disponibles.

 El 1 de mayo de 2007, el hospital Moorfields Eye y la universidad College London´s Institute of Ophthalmology, un año después el Hospital de Niños de Filadelfia anunciaron el primer ensayo de terapia génica para la enfermedad hereditaria de retina. La primera operación (en Inglaterra) se llevó a cabo en un varón británico de 23 años de edad, Robert Johnson, a principios de este año. Mientras que en Filadelfia Corey Haas fue el primer niño en obtener este tipo de terapeutica. La Amaurosis congénita de Leber es una enfermedad hereditaria que causa la ceguera por mutaciones en el gen RPE65. Los resultados de la Moorfields/UCL se publicaron en New England Journal of Medicine. Se investigó la transfección subretiniana por el virus recombinante adeno-asociado llevando el gen RPE65, y se encontraron resultados positivos. Los pacientes mostraron incremento de la visión, y no se presentaron efectos secundarios aparentes. Los ensayos clínicos de esta terapia se encuentran en fase II.
Una de las etapas a realizar es la determinación del tipo molecular que atiñe cada enfermedad (o http://es.wikipedia.org/wiki/Distrofias_de_la_retina). A nivel retiniano se realiza en América Latina por la corporación Virtual Eye Care MD, conocida y de renombre por su calidad a realizar este trabajo de relación entre fenotipo y genotipo. www.virtualeyecaremd.com

Investigadores de la Universidad de Míchigan en Ann Arbor (Estados Unidos) desarrollaron una terapia genética que ralentiza y recupera las encías ante el avance de la enfermedad periodontal, la principal causa de pérdida de dientes en adultos. Los investigadores descubrieron una forma de ayudar a ciertas células utilizando un virus inactivado para producir más cantidad de una proteína denominada receptor TNF. Este factor se encuentra en bajas cantidades en los pacientes con periodontitis. La proteína administrada permite disminuir los niveles excesivos de TNF, un compuesto que empeora la destrucción ósea inflamatoria en pacientes que sufren de artritis, deterioro articular y periodontitis. Los resultados del trabajo mostraron que entre el 60 y el 80 por ciento de los tejidos periodontales se libraban de la destrucción al utilizar la terapia génica.

En septiembre de 2009, se publicó en Nature que unos investigadores de la Universidad de Washington y la Universidad de Florida fueron capaces de proporcionar visión tricromática a monos ardilla usando terapia génica.

En noviembre de ese mismo año, la revista Science publicó resultados alentadores sobre el uso de terapia génica en una enfermedad muy grave del cerebro, la adrenoleucodistrofia, usando un vector retroviral para el tratamiento.

El 2 de noviembre la Comisión Europea autorizo a Glybera, una empresa alemana(Amsterdam), a lanzar un tratamiento para un extraño desorden genético - la deficiencia de lipoproteinlipasa(LPL).

Son numerosas las enfermedades objeto de la terapia génica, siendo las más características las tratadas a continuación:

El primer protocolo clínico aprobado por la FDA para el uso de la terapia génica fue el utilizado en el tratamiento de la deficiencia en adenosín deaminasa (ADA) que provoca un trastorno de la inmunidad, en 1990. En estos pacientes no se ha podido retirar el tratamiento enzimático exógeno necesario para su supervivencia, sino sólo disminuirlo a la mitad y se ha detectado la persistencia en la expresión del gen aún después de cuatro años de iniciado el protocolo. Aunque no se haya logrado la completa curación de los pacientes (que consistiría en retirar todo el aporte enzimático exógeno) este constituye un hecho inédito en la historia terapéutica.
En 2009 se hace un nuevo experimento en el que extraen células hematopoyéticas de la médula ósea para la introducción del gen ADA ex vivo mediante un retrovirus modificado (GIADA). Las células modificadas se vuelven a introducir en el paciente. Los resultados de este experimento fueron exitosos porque ninguno de los pacientes desarrollo leucemia (como si había ocurrido con el empleo de retrovirus). Además, todos los pacientes desarrollaron una expresión correcta del gen ADA durante los años de seguimiento que se les hizo y consiguieron un aumento de células sanguíneas. De esta manera 8 de los nueve pacientes no necesita tratamiento enzimático exógeno para complementar la terapia génica.

El tratamiento del cáncer hasta el momento ha implicado la destrucción de las células cancerosas con agentes quimioterapéuticos, radiación o cirugía. Sin embargo, la terapia génica es otra estrategia que en algunos casos ha logrado que el tamaño de tumores sólidos disminuya en un porcentaje significativo. Los principales métodos que utiliza la terapia génica en el cáncer son:

El síndrome de Wiskott-Aldrich (WAS) es una enfermedad recesiva ligada al cromosoma X caracterizada por eczema, trombocitopenia, infecciones recurrentes, inmunodeficiencia así como una gran tendencia a los linfomas y a las enfermedades autoinmunes. También hay una versión más suave de esta enfermedad conocida como trombocitopenia ligada al cromosoma X o XLT caracterizada por microtrombocitopenia congénita con plaquetas de pequeño tamaño. Ambas enfermedades están producidas por mutaciones en el gen WAS que codifica para una proteína multidominio que sólo se expresa en células hematopoyéticas, WASP. Por lo tanto, la mayoría de los que padecen este síndrome sufren una muerte prematura debido a una infección, hemorragia, cáncer o anemia grave autoinmune. Actualmente, se han realizado tratamientos eficaces en pacientes con el síndrome de Wiskott-Aldrich por medio de trasplantes de médula ósea o sangre del cordón umbilical de un donante HLA idéntico o compatible.
En 2010 se publica un estudio que muestra importante mejoras en dos niños diagnosticados con la enfermedad. La terapia consistió en extraer las células madre hematopoyéticas y volvérselas a trasferir tras integrarles el gen WAS en el genoma. Tras la terapia génica, detectaron niveles significativos de la proteína WASP en las diferentes células del sistema inmune de los pacientes. El resultado fue que los pacientes tuvieron varias mejoras significativas: uno de ellos se recuperó por completo de la anemia autoinmune y el otro paciente redujo el eczema que sufría.

La β-talasemia constituye un desorden genético con mutaciones en el gen de la β-globulina que reduce o bloquea la producción de esta proteína. Los pacientes con esta enfermedad padecen anemia severa y requieren trasfusiones de sangre a lo largo de toda su vida. La terapia génica tiene como objetivo sanar las células madre de la médula ósea mediante la transferencia de la β-globina normal o gen de β-globina en células madre hematopoyéticas (CMH) para producir de forma permanente los glóbulos rojos normales. Para llevarlo a cabo se pretende emplear lentivirus porque varios estudios muestran la corrección de la β-talasemia en modelos animales. Los objetivos de la terapia génica con esta enfermedad son: optimizar la transferencia de genes, la introducción de una gran cantidad de CMH modificadas genéticamente y reducir al mínimo las consecuencias negativas que pueden derivarse de la integración al azar de los vectores en el genoma.

Un concepto muy importante del que radican algunos aspectos de la seguridad de la terapia génica es el de la barrera Weismann. Se refiere al hecho de que la información hereditaria sólo va de células germinales a células somáticas, y no al revés.

La naturaleza de la propia terapia génica y sus vectores, implica que en muchas ocasiones los pacientes deben repetir la terapia cada cierto tiempo porque ésta no es estable y su expresión es temporal.

La respuesta inmune del organismo ante un agente extraño como un virus o una secuencia de ADN exógena. Además, esta respuesta se refuerza en las sucesivas aplicaciones de un mismo agente.

Problemas relacionados los vectores virales. Podrían contaminarse tanto por sustancias químicas como por virus con capacidad de generar la enfermedad. Implican también riesgos de respuesta inmune

Trastornos multigénicos: representan un reto muy grande para este tipo de terapia, ya que se trata de enfermedades cuyo origen reside en mutaciones en varios genes, y aplicar el tratamiento se encontraría con las dificultades clásicas de la terapia multiplicadas por el número de genes a tratar.

Posibilidad de inducir un tumor por mutagénesis. Esto puede ocurrir si el ADN se integra por ejemplo en un gen supresor tumoral. Se ha dado este caso en los ensayos clínicos para SCID ligada al cromosoma X, en los cuales 3 de 20 pacientes desarrollaron leucemia.

 El primer ejemplo de terapia génica en mamíferos fue la corrección de la deficiencia en la producción de la hormona del crecimiento en ratones. La mutación recesiva little (lit) produce ratones enanos. A pesar de que estos presentan un gen de la hormona del crecimiento aparentemente normal, no producen mARN a partir de este gen.

En series de televisión como Dark Angel, el tema de la terapia génica se menciona como una de las prácticas realizadas en niños transgénicos y sus madres. También en la serie Alias, aparece la terapia génica molecular como explicación a dos individuos idénticos.

Es un elemento fundamental en la trama de videojuegos como Bioshock o Metal Gear Solid, y desempeña un papel importante en la trama de películas como Muere otro día, de James Bond o Soy leyenda, de Will Smith, entre otras muchas.



- Conceptos de Genética, William S. Klug, Michael R.Cummings y Charlotte A. Spencer. Pearson. 8.ª edición.

- Kimmelman J (2005). "Recent developments in gene transfer: risk and ethics." BMJ. 2005 Jan 8;330(7482):79-82. "Review".

- Virus patógenos. Luis Carrasco y José María Almendral del Río. Editorial Hélice. ISBN 84-934106-0-8



</doc>
<doc id="8932" url="https://es.wikipedia.org/wiki?curid=8932" title="Enlace peptídico">
Enlace peptídico

El enlace peptídico es un enlace entre el grupo amino (–NH) de un aminoácido y el grupo carboxilo (–COOH) de otro aminoácido. Los péptidos y las proteínas están formados por la unión de aminoácidos mediante enlaces peptídicos. El enlace peptídico implica la pérdida de una molécula de agua y la formación de un enlace covalente CO-NH. Es, en realidad, un enlace amida sustituido. La formación de este enlace requiere aportar energía, mientras que su rotura (hidrólisis) la libera.

Podemos seguir añadiendo aminoácidos al péptido, pero siempre en el extremo COOH terminal.

Para nombrar el péptido se empieza por el NH terminal por acuerdo. Si el primer aminoácido de nuestro péptido fuera alanina y el segundo serina tendríamos el péptido alanil-serina.

En las décadas de 1940 y 1950, estudios basados en la difracción de rayos X sobre muestras de cristales de aminoácidos, dipéptidos y tripéptidos realizados por Linus Pauling y Robert Corey ayudaron a comprender la estructura del enlace peptídico, observándose que:

Esta ordenación planar rígida es el resultado de la estabilización por resonancia del enlace peptídico. Por ello, el armazón de un péptido está constituido por la serie de planos sucesivos separados por grupos metileno sustituidos en los que sí puede haber giro. Pero no todos los giros son posibles, lo que impone restricciones importantes al número posible de conformaciones que puede adoptar una proteína.

Si denominamos "Φ" al valor del ángulo que puede adoptar el enlace N-C, y "Ψ" al del enlace C-C, solo existirán unos valores permitidos para Φ y Ψ (ver gráfico de Ramachandran); y dependerá en gran medida del tamaño y características de los grupos R sucesivos.

El enlace peptídico puede romperse por hidrólisis (añadiendo agua). En su presencia se romperá liberando 8–16 kilojulios/mol (2–4 kcal/mol) de energía libre. En la naturaleza este proceso es extremadamente lento (más de 1000 años), pero hay formas de acelerarlo:





</doc>
<doc id="8934" url="https://es.wikipedia.org/wiki?curid=8934" title="Cariotipo">
Cariotipo

El cariotipo, específicamente idiograma, es el patrón cromosómico de una especie expresado a través de un código, establecido por convenio, que describe las características de sus cromosomas. 
Debido a que en el ámbito de la clínica suelen ir ligados, el concepto de cariotipo se usa con frecuencia para referirse a un cariograma, el cual es un esquema, foto o dibujo de los cromosomas de una célula metafásica ordenados de acuerdo a su morfología (metacéntricos, submetacéntricos, telocéntricos, subtelocéntricos y acrocéntricos) y tamaño, que están caracterizados y representan a todos los individuos de una especie.
El cariotipo es característico de cada especie, al igual que el número de cromosomas; el ser humano tiene 46 cromosomas (23 pares porque somos diploides o 2n) en el núcleo de cada célula, organizados en 22 pares autosómicos y 1 par sexual (hombre XY y mujer XX). Cada brazo ha sido dividido en zonas y cada zona, a su vez, en bandas e incluso las bandas en sub-bandas, gracias a las técnicas de marcado. No obstante puede darse el caso, en humanos, de que existan otros patrones en los cariotipos, a lo cual se le conoce como aberración cromosómica.

Los cromosomas se clasifican en 7 grupos, de la A a la G, atendiendo a su longitud relativa y a la posición del centrómero, que define su morfología. De esta manera, el cariotipo humano queda formado así: 


Mediante el cariotipado se pueden analizar anomalías numéricas y estructurales, cosa que sería muy difícil de observar mediante genética mendeliana.

Ante todo, deben guardarse las máximas condiciones de esterilidad. Además, debe cumplirse lo siguiente:


El estudio de los cariotipos es posible debido a la tinción. Usualmente un colorante adecuado es aplicado después de que las [células] hayan sido detenidas durante la división celular mediante una solución de colchicina.
Para humanos los glóbulos blancos son los usados más frecuentemente porque son fácilmente inducidos a crecer y dividirse en cultivo de tejidos. 

Algunas veces las observaciones pueden ser realizadas cuando las células no se están dividiendo 
(interfase). El sexo de un neonato feto puede ser determinado por observación de células en la interfase (ver punción amniótica y corpúsculo de Barr).

La mayoría (pero no todas) las especies tienen un cariotipo estándar. El ser humano normalmente tiene 22 pares de cromosomas autosómicos y un par de cromosomas sexuales. El cariotipo normal para la mujer contiene dos cromosoma X denominado 46,XX, y el varón un cromosoma X y uno Y, denominado 46 XY. Cualquier variación de este cariotipo estándar puede llevar a anormalidades en el desarrollo.

En los laboratorios de Citogenética se utilizan varias técnicas de bandeo cromosómico. En este sentido, destaca el método de tinción de las bandas de quinacrina (bandas Q). Fue el primero en emplearse, requiere un microscopio de fluorescencia, aunque su uso ya no está tan extendido como el de las bandas de giemsa (bandas G). Para producir estas bandas G se aplica una tinción de Giemsa tras digerir parcialmente las proteínas cromosómicas con tripsina. Las bandas reversas (bandas R) requieren tratamiento por calor y en ellas se invierte el patrón normal blanco y negro que se observa en las bandas Q y G. Este método destaca por su gran utilidad en la tinción de los extremos distales de los cromosomas. Existen otras técnicas de tinción como las bandas C y las NOR (región de organizadores nucleolares), tiñendo estos últimos específicamente ciertas regiones del cromosoma. Así, las bandas C tiñen la heterocromatina constitutiva, que se localiza normalmente cerca de los centrómeros, y la tinción NOR marca los satélites y tallos de los cromosomas acrocéntricos.

Las bandas de alta resolución suponen la tinción de los cromosomas en profase o metafase precoz (prometafase) antes de alcanzar la condensación máxima. Los cromosomas en profase y prometafase están más elongados que los cromosomas en metafase; por este motivo, el número de bandas observadas, para el conjunto de cromosomas, aumenta desde 300-450 hasta casi 800. Ello permite detectar anomalías menos claras, que con las bandas convencionales no suelen apreciarse.
Para obtener este tipo de bandas se necesita añadir otro requisito para la realización del cariotipo. Se trata de un componente utilizado en quimioterapia, el metotrexato que junto con la colchicina se añade antes de realizar la tinción.

Tanto el paso de adición de mitógenos como la adición de colchicina son los pasos críticos para el estudio del cariotipo.
Es necesario realizar un recuento de, al menos, 12-25 células en metafase. Esto es debido a que si por ejemplo, al contar un núcleo le falta el cromosoma 21, puede ser que sea mosaico y que el resto de células sí presenten ese cromosoma. Otra opción, y más probable, es que sea un efecto de la adición del mitógeno, ya que este compuesto altera el proceso normal de división celular, favoreciendo las aneuploidías. Una última opción podría ser que los cromosomas se solapen y al proceder al analizar el cariotipo sólo se cuente un cromosoma cuando realmente hay dos. Por todas estas razones se deben contar un mínimo de 12-15 células en metafase que se encuentren bastante separados en el porta.

En el cariotipo clásico se suele utilizar una solución de Giemsa como tinción (específica para los grupos fosfato del ADN) para colorear las bandas de los cromosomas (Bandas-G), menos frecuente es el uso del colorante Quinacridina (se une a las regiones ricas en Adenosina-Timina). Cada cromosoma tiene un patrón característico de banda que ayuda a identificarla.

Los cromosomas se organizan de forma que el brazo corto de este quede orientado hacia la parte superior y el brazo largo hacia la parte inferior.

Algunos cariotipos nombran a los brazos cortos p y a los largos q. Además, las diferentes regiones y subregiones teñidas reciben designaciones numéricas según la posición a la que se encuentren respecto a estos brazos cromosómicos.

Por ejemplo, el síndrome de Cri du Chat implica una deleción en el brazo corto del cromosoma 5. Está escrito como 46, XX, 5p-. La región crítica para este síndrome es la deleción de 15.2, la cual es escrita como 46,XX, del(5)(p15.2)

El análisis espectral de los cariotipos (o SKY) se trata de una tecnología de citogenética molecular que permite el estudio y visualización de los 23 pares de cromosomas en forma simultánea.

Sondas marcadas fluorescentemente son hechas para cada cromosoma al marcar DNA específico de cada cromosoma
con diferentes fluoróforos. Debido a que hay un limitado número de fluoróforos espectralmente distintos, un
método de etiquetado combinatorio es usado para generar muchos colores diferentes.

La diferencias espectrales generadas por el etiquetado combinatorio son capturadas y analizadas usando un interferómetro agregado a un microscopio de fluorescencia.

El programa de procesamiento de imágenes entonces asigna un pseudocolor a cada combinación espectralmente diferente, permitiendo la visualización de cromosomas coloreados.

Esta técnica es usada para identificar aberraciones estructurales cromosómicas en células cancerígenas y otras patologías cuando el bandeo con Giemsa u otras técnicas no son lo suficientemente precisas.

Este tipo de técnicas mejorará la identificación y diagnóstico de las aberraciones cromosómicas en citogenética prenatal así como en células cancerosas.

El cariotipo digital es una técnica utilizada para cuantificar el número de copias de ADN en una escala genómica. Se trata de secuencias de locus de ADN específicos de todo el genoma que son aisladas y enumeradas.

Este método es también conocido como cariotipado virtual.


La variación de estos cromosomas es encontrada frecuentemente:


Levitsky fue el primero en dar una definición a cariotipo como el aspecto fenotípico de los cromosomas somáticos, en contraste con su contenido de genes. Este concepto siguió siendo estudiado con los trabajos de Darlington y White. 
La investigación y el interés por el estudio del cariotipo hizo que se planteara una pregunta : ¿cuántos son los cromosomas que contiene una célula diploide humana?

En 1912, Hans von Winiwarter demostró que el hombre tenía 47 cromosomas en espermatogonia y 48 en oogonia, concluyendo un mecanismo de determinación sexual XX/XO . Años después, en 1922 von Winiwarter no estaba seguro si el número cromosómico del hombre era 46 o 48. Para ello se necesitó un estudio más profundo para poder responder a esta pregunta.

Esto tomó hasta mediados de los años 1950 que fue cuando se dio como generalmente aceptado que el cariotipo de hombre incluye sólo 46 cromosomas. En los grandes monos el cariotipo es de 48 cromosomas por lo que se explicó que el cromosoma 2 de los humanos fue formado por una fusión de cromosomas hereditarios, reduciendo así el número de estos.

Aunque la replicación del ADN y la transcripción del ADN están altamente estandarizadas en eucariotas, no puede decirse lo mismo de sus cariotipos, ya que son sumamente variables entre especies en el número de cromosomas y en la organización detallada a pesar de haber sido construidos con las mismas macromoléculas.

Esta variación proporciona la base para una gama de estudios que podría llamarse citología evolutiva.

En algunos casos incluso hay significantes variaciones dentro de las especies. En una revisión del 2000 
Godfrey y Masters concluyen: "En nuestra visión, es poco probable que un proceso o el otro, puedan independientemente contar para el amplio rango de estructuras de cariotipo que son observadas.. Pero usadas en conjunto con otros datos filogenéticos, el fisionamiento cariotípico puede ayudar a explicar dramáticas diferencias en los números diploides entre especies estrechamente relacionadas, que antes fueron inexplicables.

A lo largo del tiempo, algunos de los organismos fueron eliminando la presencia de algunos componentes de su núcleo, así como la heterocromatina.


En "Ascaris suum", todos los precursores de células somáticas experimentan disminución de la cromatina.
Hay veces que se dan casos donde algunos cromosomas son anormales por lo que resulta un trastorno para el nuevo descendiente.

Un ejemplo de la variabilidad entre especies estrechamente relacionadas es el del muntjac (un mamífero de la familia de los cérvidos que vive en la India y el sudeste asiático), que fue investigado por Kurt Benirschke y su compañero Doris Wurster donde demostraron que el número diploide del muntjac Chino (Muntiacus reevesi) resultó ser de 46 y todos telocéntricos.
Cuando se estudió el cariotipo del muntjac Indio (Muntiacus muntjak) vieron que la hembra tenía 6 y el macho 7 cromosomas. 

El número de cromosomas en el cariotipo entre especies no relacionadas es enormemente variable. 

El récord más bajo le pertenece al nematodo "Parascaris univalens", donde el número haploide es n = 1; el récord más alto podría estar en algún lugar entre los helechos, con el helecho Lengua de Adder
"Ophioglossum" adelante con un promedio de 1262 cromosomas.

El récord más alto para animales podría estar entre el esturión de nariz corta "Acipenser brevirostrum" con solamente 372 cromosomas.

La existencia de cromosomas supernumerarios o B significa que el número de cromosomas puede variar incluso dentro de una misma población. (El cromosoma supernumerario se sitúa en el lugar del cromosoma normal 21. La fórmula de este triple cromosoma puede ser XXY o XYY).

La poliploidía (más de dos conjuntos de cromosomas homólogos en las células) se produce principalmente en las plantas. Ha sido de gran importancia en la evolución de estas según Stebbins. 
La proporción de las plantas con flores poliploides es de 30-35% y en el caso de las gramíneas un valor mucho más elevado, alrededor del 70%. 

La poliploidía en plantas inferiores (helechos y psilotales) también es común. Algunas especies de helechos han alcanzado niveles de poliploidía muy por encima de los niveles más altos conocidos en plantas con flores.
La poliploidía en animales es mucho menos común, alcanzando importancia en algunos grupos. En humanos se han registrado casos de embriones y fetos triploides (69, XXX) e incluso tetraploides (92, XXXX)que con un gran porcentaje acababan en aborto natural; en el caso poco frecuente de neonatos con dicha carga cromosómica, sus esperanzas de vida no superaban los pocos días postparto debido a diversas alteraciones en todos sus órganos.

La endopoliploidía se produce cuando los tejidos adultos de las células han dejado de dividirse por mitosis, pero los núcleos contienen más cantidad de cromosomas somáticos originales.

En muchos casos, los núcleos endodiploides contienen decenas de miles de cromosomas (no pueden contarse con exactitud). Las células no siempre contienen exactamente múltiplos (potencias de dos), razón por la cual el aumento en el número de conjuntos de cromosomas causados por la reproducción no es del todo exacto.

Este proceso (sobre todo estudiado en insectos y algunas plantas superiores) puede ser una estrategia de desarrollo para aumentar la productividad de los tejidos que son muy activos en la biosíntesis.

Este fenómeno ocurre esporádicamente a través del reino eucariota desde protozoo hasta el hombre;
Este es diverso y complejo, y sirve a la diferenciación y morfogénesis de muchas formas.
Vea paleopoliploidía para la investigación de duplicación de antiguos cariotipos.

El término es principalmente usado cuando el número de cromosomas varía dentro del cruce poblacional de especies. Esto puede también ser usado dentro de un grupo de especies estrechamente relacionado.

Clásicos ejemplos en plantas son el género "Crepis", donde el número gamético (= haploide) forma las series x = 3, 4, 5, 6, y 7; y "Crocus", donde cada número desde x = 3 hasta x = 15 es representado por al menos una especie. Evidencia de varios tipos muestran que las tendencias de evolución han ido en direcciones diferentes, en diferentes grupos.

Más cerca de casa, los grandes monos tienen 24x2 cromosomas, allí donde los humanos tienen 23x2.
El Cromosoma 2 humano fue formado por la mezcla de cromosomas ancestrales, reduciendo el número. La aneuploidía no es considerada normalmente -ploidía sino -somía, tal como la trisomía ó monosomía.

Estas anomalías pueden ser numéricas (presencia de cromosomas adicionales) o estructurales (translocaciones, inversiones a gran escala, supresiones o duplicaciones).

Las anomalías numéricas, también conocidas como aneuploidía, hacen referencia a cambios en el número de cromosomas, que pueden dar lugar a enfermedades genéticas. La aneuploidía se puede observar frecuentemente en células cancerosas. En los animales sólo son viables las monosomías y las trisomías, ya que las nulisomías son letales en individuos diploides.

Las anormalidades estructurales a menudo se derivan de errores en la recombinación homóloga. Ambos tipos de anomalías pueden ocurrir en los gametos y, por tanto, estarán presentes en todas las células del cuerpo de una persona afectada, o puede ocurrir durante la mitosis y dar lugar a mosaicos genéticos individuales que tiene normal y anormal algunas células.

Anomalías cromosómicas en humanos:

También se detectó la existencia de la trisomía 8, 9 y 16, aunque por lo general no sobreviven después de nacer. No se han registrado casos en humanos de trisomías en el cromosoma 1, ya que todas acaban en aborto natural y no llegan a nacer.

Hay algunos trastornos que se derivan de la pérdida de un solo trozo de cromosoma, entre ellas:

Estas anomalías cromosómicas también pueden ocurrir en células cancerosas de un individuo genéticamente normales.
Un ejemplo bien documentado es el de Cromosoma Filadelfia o la llamada translocación Filadelfia que es una anormalidad genética asociada a la leucemia mieloide crónica (LMC).

Esta anormalidad afecta a los cromosomas 9 y 22. El 95 por ciento de los enfermos de leucemia mieloide crónica presenta esta anormalidad, mientras el resto de los enfermos padecen translocaciones crípticas invisibles a las preparaciones mediante el método de banda G u otras translocaciones que afectan a otro u otros cromosomas de la misma forma que sucede con los cromosomas 9 y 22.

Partes de dos cromosomas, el 9 y el 22, intercambian sus posiciones. El resultado es que parte del gen de región de fractura (BCR, Breakpoint Cluster Region, en inglés) del cromosoma 22 (región q11) se fusiona con parte del gen ABL del cromosoma 9 (región q34). El gen ABL toma su nombre de «Abelson», el nombre de un virus causante de leucemias precursor de una proteína similar a la que produce este gen.

Desde de 1995 se emplean distintos símbolos para describir la anomalía que sufre un cromosoma en concreto o un cariotipo, siguiendo las reglas que impone el ISCN (siglas inglesas procedentes de Sistema Internacional de Nomenclatura para Citogenética Humana). Es decir, la fórmula cromosómica refleja la descripción simplificada de un cariotipo. En la fórmula cromosómica se registra el número total de cromosomas (incluidos los sexuales) seguido de una coma, tras la cual se escriben los cromosomas sexuales. Si existen aberraciones numéricas o estructurales de los autosomas, éstas se escriben a continuación, tras otra coma. Cuando hay un mosaico, es decir, coexisten dos o más poblaciones celulares diferentes, los cariotipos correspondientes a cada una se escriben separados por una barra; primero se escribe el que tiene menor número de cromosomas y luego sucesivamente los de mayor número. Algunos de los símbolos y abreviaturas usados para describir los cariotipos son:





</doc>
<doc id="8935" url="https://es.wikipedia.org/wiki?curid=8935" title="Ortocentro">
Ortocentro

Se denomina ortocentro (símbolo "H") al punto donde se cortan las tres alturas de un triángulo.

El nombre deriva del término griego "orto", que quiere decir recto, en referencia al ángulo formado entre las bases y las alturas.

El ortocentro se encuentra en el interior del triángulo si este es acutángulo; coincide con el vértice del ángulo recto si es rectángulo, y se halla en el exterior del triángulo si es obtusángulo.

Dado un triángulo cualquiera (excluyendo un triángulo rectángulo), el 'triángulo órtico o triángulo pedal respecto del dado, es el que tiene por vértices los pies de las tres alturas de este, es decir, las proyecciones de los vértices sobre los lados.







</doc>
<doc id="8936" url="https://es.wikipedia.org/wiki?curid=8936" title="Circunferencia circunscrita">
Circunferencia circunscrita

En geometría, la circunferencia circunscrita es la circunferencia que pasa por todos los vértices de un polígono y contiene completamente a dicha figura en su interior. El centro de la circunferencia circunscrita se llama circuncentro 
y su radio circunradio. 

Un polígono que tiene una circunferencia circunscrita se llama polígono cíclico. Todos los polígonos simples regulares, todos los triángulos y todos los rectángulos son cíclicos. En todo polígono cíclico, el circuncentro se halla en el punto de intersección de las mediatrices de los lados del polígono.

El circuncentro es el punto en el que se intersecan las tres mediatrices de un triángulo y es el centro de la circunferencia circunscrita.

Los vértices de un triángulo, como extremos de cada lado, se encuentran a la misma distancia de los puntos de sus mediatrices, luego el punto donde estas se cortan, será equidistante de los tres vértices: el circuncentro. Dicho punto se suele expresar con la letra O.

Sirve para trazar el círculo que pasa por los tres vértices del triángulo.

Tres casos de triángulos:

Los cuadriláteros inscritos poseen propiedades particulares, incluyendo que los ángulos opuestos son suplementarios que se deduce a partir de la generalización del arco capaz. 




</doc>
<doc id="8937" url="https://es.wikipedia.org/wiki?curid=8937" title="Albert Szent-Györgyi">
Albert Szent-Györgyi

Albert Szent-Györgyi de Nagyrápolt (Budapest, 16 de septiembre de 1893 – Woods Hole, Massachusetts, 22 de octubre de 1986) fue un fisiólogo húngaro, galardonado con el en 1937.

Su padre, Miklós Szent-Györgyi era terrateniente. Su madre, Jozefin, era hija de József Lenhossék y hermana de Mihály Lenhossék, ambos profesores de anatomía en la Universidad de Budapest.

Los trabajos de Szent-Györgyi estuvieron relacionados con la química de la respiración. En la Universidad de Szeged, empleó pimentón como fuente de vitamina C (el L-enantiómero del ácido ascórbico) y se dio cuenta de su actividad contra el escorbuto.
Estudió la oxidación celular y descubrió la vitamina C en 1927.

En 1937 recibió el por sus descubrimientos en relación con los procesos de combustión biológica, en especial los referidos a la vitamina C y la catálisis del ácido fumárico.

Nació el 16 de septiembre de 1893 en la ciudad de Budapest, capital de Hungría, que en aquellos momentos formaba parte del Imperio Austrohúngaro. Inició sus estudios de medicina en la Universidad de Budapest, que combinó con sus propias investigaciones en el laboratorio químico de su tío. Debido a la Primera Guerra Mundial, donde sirvió como médico, tuvo que interrumpir sus estudios. Durante la Gran Guerra se disparó en un pie para abandonar el frente, hecho que le permitió finalizar sus estudios el año 1917. 

Inició su investigación científica en la ciudad de Pozsony, hoy en día Bratislava. Cuando la ciudad se convirtió en parted de
Checoslovaquia en enero de 1919, abandonó la villa junto a la mayor parte de población de origen húngaro. Posteriormente desarrolló su investigación en la Universidad de Groningen, centrándose en la química de la respiración celular. Gracias a estos trabajos y a una beca de la Fundación Rockefeller viajó a la Universidad de Cambridge, donde se doctoró en 1927 gracias a su trabajo en el aislamiento del "ácido hexurónico", hoy en día denominado Vitamina C.

Posteriormente, en la Universidad de Szeged utilizó "Capsicum annuum" como fuente de vitamina C (el L-enantiómero del ácido ascórbico) y se dio cuenta de su actividad anti-escorbútica. El año 1937 fue galardonado con el premio Nobel de Medicina y Fisiología «por su descubrimiento relacionado con los procesos de combustión biológica, con especial referencia a la vitamina C y a la catálisis de los ácidos fumáricos».

Durante la Segunda Guerra Mundial participó activamente en la Resistencia húngara. Aunque Hungría se alió con las potencias del Eje, el primer ministro húngaro Miklós Kállay envió a Szent-Györgyi a la ciudad de Estambul el año 1944 para realizar negociaciones secretas con los Aliados.

Adolf Hitler dictó una orden de detención de Szent-Györgyi, pero este se escapó de la Gestapo y permaneció escondido dos años. Después de la guerra recuperó su puesto e incluso se especuló con la posibilidad que los soviéticos lo nombraran primer ministro, por lo que se afilió al Partido Comunista de Hungría y fue elegido miembro del Parlamento a la vez que creó el laboratorio de Bioquímica de la Universidad de Budapest y restableció la Academia de Ciencias de Hungría. Descontento con el gobierno, emigró a los Estados Unidos el año 1947, siendo nombrado ciudadano el año 1955.

Durante su estancia en Norteamérica, creó su propio laboratorio en la ciudad de Woods Hole, situada en el estado de Massachusetts e inició sus investigaciones sobre el cáncer, desarrollando teorías sobre física cuántica aplicadas a la bioquímica de esta enfermedad. Durante la década de 1970 su búsqueda lo condujo a deducir que los radicales libres eran una causa potencial del cáncer. Murió el 22 de octubre de 1986.

Su cita más célebre: "Descubrir algo significa mirar lo mismo que está viendo todo el mundo y percibirlo de manera diferente".




</doc>
<doc id="8938" url="https://es.wikipedia.org/wiki?curid=8938" title="Oráculo de Delfos">
Oráculo de Delfos

El oráculo de Delfos, en el Santuario de Delfos, fue un lugar de consulta a los dioses, en el templo sagrado dedicado principalmente al dios Apolo. Situado en Grecia, en la actual villa de Delfos, al pie del monte Parnaso, consagrado al propio dios y a las musas, en medio de las montañas de la Fócida, a 700 m sobre el nivel del mar y a 9,5 km de distancia del golfo de Corinto.

De las rocas de la montaña brotaban varios manantiales que formaban distintas fuentes. Una de las fuentes más conocidas desde muy antiguo era la fuente Castalia, rodeada de un bosquecillo de laureles consagrados a Apolo. La leyenda y la mitología cuentan que en el monte Parnaso y cerca de esta fuente se reunían algunas divinidades, diosas menores del canto, la poesía, llamadas musas junto con las ninfas de las fuentes, llamadas náyades. En estas reuniones Apolo tocaba la lira y las divinidades cantaban.

El oráculo de Delfos alcanzó gran notoriedad en toda Grecia desde mediados del s. VIII a.C., cuando Apolo pítico se convirtió en el patrón de las empresas coloniales. Más adelante llegó a ser el centro religioso del mundo helénico.

Hay diversas propuestas acerca del origen del topónimo de Delfos. Una de ellas propone que viene de Delfine (Δελφινης), que era el nombre del dragón mitológico que custodiaba el oráculo antes de la llegada de Apolo. También se ha escrito que su origen parte de un mito según el cual Apolo se convirtió en delfín para atraer a un barco cretense, del que quería utilizar a la gente como sacerdotes; los cretenses desembarcaron y fundaron Crisa y se les encargó ser sacerdotes del templo y que adorasen al dios bajo el nombre de "Apolo Delfinio" para rememorar su conversión en delfín. Al templo de Apolo se le llamó igualmente "Delfinion" (Δελφίνιoν).

El santuario se construyó en el lugar conocido en la antigüedad como Pito,nombre que en griego presenta dos formas (ambas femeninas): Πυθώ, -οῦς y Πυθών, -ῶνος (Homero. Il. 2.519 y 9.405; Od.8.80). Este nombre (que carece de etimología aceptada) se relaciona con el de la gran serpiente o dragón que, según la mitología, vigilaba el oráculo primitivo (véase el siguiente apartado). En la antigüedad se intentó dar una etimología al nombre de Pito que lo relacionara con las funciones del santuario. A estos intentos de etimología popular se refieren su relación con el verbo "pythomai" (πύτωμαι) = "pudrir", que se relacionaría con el hecho de que Apolo habría dejado pudrirse a la serpiente tras haberla matado; o con el verbo pynthanomai (πυνθάνομαι) = "informarse, aprender" que se referiría a las funciones del propio oráculo.

Del término "Pitón" provienen los de "pitia" (Πυθία) o "pitonisa",nombre de las sacerdotisas del templo, que interpretaban las respuestas.

Hay testimonios de ocupación humana cercana al emplazamiento del santuario de Delfos de época arcaica desde el Neolítico, concretamente en una gruta del macizo del Parnaso. Ya en época micénica y en el mismo emplazamiento del santuario hubo primero (c. 1400 a.C.) una pequeña aldea que fue abandonada en algún momento entre 1100 y 800 a. C.

El santuario propiamente dicho apareció después de esta fecha con un altar, al que siguió un primer templo. El nombre de Pito se relaciona en la mitología con el de una gran serpiente o dragón Pitón hijo de la diosa Gea (la Tierra) que vigilaba un oráculo consagrado a su madre, o bien era compartido por Poseidón y Gea. Una tradición indica que Gea cedió a Temis su parte y esta lo regaló a Apolo. Por otra parte, Poseidón intercambió la suya con Apolo por Calauria. Sin embargo, la versión más difundida dice que, con el fin de establecer su propio oráculo con el que guiar a los hombres, Apolo mató a Pitón con su arco y tomó posesión del oráculo. Para establecer el culto del nuevo santuario desvió un barco de sacerdotes cretenses. (cf. Himno Homérico a Apolo). En el lugar original de este templo había exhalaciones de vapores subterráneos, según una tradición antigua no verificada por la moderna arqueología. (cf. Estrabón, 9.3.5).

No se sabe que la elección de este personaje se hacía sin ninguna distinción de clases. A la candidata sólo se le pedía que su vida y sus costumbres fueran irreprochables. El nombramiento era vitalicio y se comprometía a vivir para siempre en el santuario. Durante los siglos de apogeo del oráculo fue necesario nombrar hasta tres pitonisas para poder atender con holgura las innumerables consultas que se hacían por entonces. Sin embargo, en los tiempos de decadencia sólo hubo una, suficiente para los pocos y espaciados oráculos que se requerían.

Los consultantes tenían una entrevista con ella unos días antes del oráculo. Este hecho está perfectamente documentado en las noticias que dan los autores de la Antigüedad. El oráculo se celebraba un día al mes, el día 7 que se consideraba como la fecha del nacimiento de Apolo. Los consultantes eran de todo tipo, desde grandes reyes hasta gente pobre. En primer lugar se ofrecía un sacrificio en el altar que había delante del templo. A continuación se pagaban las tasas correspondientes y por último el consultante se presentaba ante la Pitia y hacía sus consultas oralmente, según se cree.

Se conoce muy poco sobre el rito que se seguía en el oráculo. Se sabe que la Pitia se sentaba en un trípode que estaba en un espacio llamado "aditon", al fondo del templo de Apolo Pitio. Αδυτων significa "fondo del santuario" y τo αδυτoν significa "lugar sagrado de acceso prohibido".

En el oráculo de Dódona se hacían las consultas grabadas en laminillas de plomo de las que se han encontrado bastantes ejemplares en las excavaciones. La Pitia daba respuestas (el verdadero oráculo) que un sacerdote recogía y escribía en forma de verso. Después se le entregaba al consultante. En un primer momento, las sentencias de la pitonisa se hacían en verso, pero a mucha gente le parecía extraño que, siendo Apolo el dios de la música, tuvieran las predicciones tan mala calidad rítmica y melódica. Así que pronto la pitonisa comenzó a predecir en prosa.

Uno de los enigmas con el que se enfrentan los estudiosos del tema es el gran número de aciertos que tuvo el oráculo de Delfos. La fe en él era total, incluso si se equivocaba, porque en ese caso se decía que el fallo era la interpretación de lo dicho y no el oráculo en sí.

Durante siglos ha corrido una leyenda en forma de verdad histórica acerca del oráculo y el estado de la Pitonisa. Dicha leyenda se difundió a partir de los autores cristianos de los siglos III y IV, como Orígenes y San Juan Crisóstomo. Eran tiempos en que la época de la Grecia clásica se veía como un acérrimo paganismo al que había que ridiculizar. De esta manera los escritores inventaron algo que a través de los siglos tuvo siempre mucho éxito. Lo describían así:

Lo cierto es que no se ha encontrado hasta el momento ninguna descripción sobre el momento del oráculo en los escritores griegos o latinos. Ningún autor pagano ha descrito nunca una escena de consulta, ni siquiera Plutarco en su obra "Diálogos píticos". Por otra parte, los estudios arqueológicos y geológicos recientes hechos en la zona del templo de Apolo aseguran que en la roca no existe la fisura profunda de que se habla en la leyenda. 

Tradicionalmente se conocen dos oráculos dados al rey Creso:

Creso (560-546 a. C.) fue el último rey de Lidia. Se cuenta (en Heródoto: "Historia" I, 53 y en Cicerón: "Sobre la adivinación" II, 115, 11) de él que en una ocasión envió una consulta al oráculo, pues se estaba preparando para invadir el territorio persa y quería saber si el momento era propicio. El oráculo fue así: ἤν στρατεύηται ἐπὶ Πέρσας, μεγάλην ἀρχήν μιν καταλύσειν / Croesus Halyn penetrans magnam pervertet opum vim / "Creso, si cruzas el río Halys (que hace frontera entre Lidia y Persia), destruirás un gran imperio". La respuesta se interpretó como favorable y dando por hecho que el gran imperio era el de los persas. Pero el “gran imperio” que se destruyó en aquel encuentro fue el suyo, y Lidia pasó a poder de los persas. Esto es un ejemplo de la ambigüedad en las respuestas. Muchas de ellas fueron recogidas por autores clásicos. En realidad el oráculo no trataba de adivinar los hechos, sino de dar buenos consejos, cosa que no era demasiado difícil, ya que en el santuario se disponía de la última noticia y de los últimos acontecimientos del mundo conocido.Según Jenofonte y algunas colecciones griegas de versos gnómicos, ante una consulta del mismo rey se le respondió la famosa frase: εἰ θνητός εἶ, βέλτιστε, θνητὰ καὶ φρόνει "Si eres humano, procura pensar en cosas humanas". Esta máxima se basa en la idea que para conseguir la felicidad y la autoestima hay que conocer los propios límites y aceptarlos.

Según algunas tradiciones, la primera pitia o pitonisa que actuó en el oráculo de Delfos se llamaba Sibila, y su nombre se generalizó y se siguió utilizando como nominativo de esta profesión. Ni Homero (siglo IX al VIII a. C.) ni Hesíodo (siglo VIII a. C.) hablan de las sibilas; su nombre aparece por primera vez en el siglo VI a. C. y es el filósofo Heráclito de Éfeso (544 – 484 a. C.) el primer informador de estos personajes. Se pensaba que las sibilas eran oriundas de Asia y que en cierto modo sustituyeron a las antiguas pitias.

La descripción bastante exacta de cómo fue el recinto sagrado se conoce gracias a las informaciones de Pausanias en el siglo II d. C. y a la confirmación de esos escritos hecha por las excavaciones arqueológicas.

Una cerca sagrada llamada períbola rodeaba todo el enclave del santuario. En la esquina sureste del recinto comenzaba la vía sacra que iba subiendo montaña arriba, serpenteando y pasando por delante de pequeñas edificaciones llamadas "tesoros" y de diversos monumentos, hasta llegar al templo del oráculo, templo de Apolo y continuando hasta el estadio en lo más alto. El peregrino accedía por la puerta principal de esta vía sagrada.

En el valle pueden verse cientos de olivos plantados, cuya extensión llega hasta el golfo de Corinto. Se dice que es el mayor olivar del mundo.

Los llamados "tesoros" (gr. θεσαυρυς, pronúnciase ""tesaurus"") eran pequeñas capillas donde se guardaban los exvotos y las donaciones que frecuentemente eran muy ricas y valiosas, verdaderas joyas. Se sabe que existían todas estas capillas:


En la terraza que se extendía delante del templo de Apolo estaba situado el altar de los sacrificios. Se construyó además un teatro (en el siglo IV a.c.) y un estadio, con 7.000 plazas para espectadores, para los "juegos píticos" (este último en el 582 a. C.). También había un hipódromo, que aún está sin localizar.

Al aire libre y salpicadas por todo el recinto se hallaban las estatuas de mármol o de bronce, regalos de reyes o de ciudades, en agradecimiento a los servicios prestados por el oráculo.




El "ónfalos" es el "ombligo del mundo". La leyenda cuenta que el dios Zeus mandó volar a dos águilas desde dos puntos opuestos del Universo. Las águilas llegaron a encontrarse aquí, en Delfos, donde una piedra cónica llamada ónfalos señala el lugar. La piedra, en forma de medio huevo, fue descubierta durante las excavaciones cerca del templo de Apolo. 

Estas piedras que representan el ombligo del mundo eran un símbolo del centro, del lugar donde empezaría la creación del mundo. Al colocarlas en un determinado espacio, lo sacralizaba y lo convertía en el centro religioso. En el caso del ónfalos de Delfos, así fue y este santuario se convirtió en el ombligo o centro religioso de toda Grecia.

En algunas monedas encontradas en el recinto se puede ver la imagen del ónfalos, esquematizada y representada por un punto en el centro de un círculo. La piedra mencionada se halla expuesta en el museo de Delfos.

Por la arqueología y los escritos antiguos se sabe que en el siglo VIII a. C. hubo en este lugar de Delfos edificios sagrados. Pausanias, el historiador griego del siglo II d. C., recoge la tradición y entre otras cosas cuenta que los tres primeros templos fueron construidos, uno con laurel, otro con cera de abeja mezclada con plumas y el tercero con bronce.

La arqueología demuestra que en esta época ya era famoso el nombre de Apolo no sólo en el lugar, sino en tierras lejanas. Los exvotos sacados a la luz en las excavaciones son muy significativos: Renombre de Apolo Pitio que era famoso en lugares remotos, caballos de Tesalia, trípodes del Peloponeso, soportes de recipientes de Creta, etc.

Pasado el tiempo fueron aumentando las ofrendas, sobre todo los exvotos de bronce. Se han encontrado escudos cretenses, cascos corintios, calderos con cabezas de grifos llegados desde Samos y el Peloponeso y estatuillas diversas.

A finales del siglo VII a. C. ya se construyen templos especiales para Apolo y Atenea; son de piedra, con columnas dóricas. Sus restos, pasado el tiempo, sirvieron para construir nuevos templos.

A comienzos del siglo VI a. C. tuvieron lugar dos acontecimientos que influyeron bastante en la evolución del santuario de Delfos. Uno fue la instalación en Delfos de la "anfictionía" y el otro, la reorganización de los "Juegos Píticos".

La anfictionía era una liga religiosa que agrupaba 12 pueblos (no ciudades), casi todos de la Grecia central. Tenía sus reuniones en el santuario de Deméter en Antela, cerca de las Termópilas. Como el oráculo de Delfos tenía ya un renombre mayor que el de Deméter, trasladaron allí la sede de esta confederación, sin por ello abandonar el otro santuario. Esta decisión dio lugar a las llamadas guerras sagradas que fueron tres.

Los Juegos Píticos tenían lugar al principio cada 8 años. Después lo acortaron a 4 y se alternaban con los Juegos Olímpicos. Consistían en pruebas atléticas, hípicas y concursos líricos. En Delfos se construyó en esta época un teatro y un hipódromo para la celebración de estos juegos, que se consideraban muy importantes.

Hubo un gran enriquecimiento tras la primera "guerra sagrada", en la que algunas ciudades griegas compitieron por obtener el control y la autoridad del santuario, con lo cual conseguían un reconocimiento de supremacía y prestigio sobre las otras ciudades y sobre algunos reinos extranjeros. Las aportaciones fueron tanto por parte de los griegos como de los pueblos bárbaros. Hay que destacar el regalo que hizo Creso (560-546 a. C.), último rey de Lidia, en esta ocasión: un león de oro sobre una base de lingotes de oro más un cuenco de oro que pesaba un cuarto de tonelada.

En la primera mitad del siglo VI a. C. se hicieron unas 12 fundaciones de tesoros en torno al templo de Apolo. Este viejo templo ardió en el año 548 a. C. y tras el incendio su reconstrucción fue lenta. Hasta el año 505 a. C. no se terminó el nuevo templo, más grande que el anterior y cuya construcción se llevó a cabo gracias a una familia llamada Alcmeónidas, de Atenas. Según cuenta Heródoto, esta familia gestionó la aportación de dinero en todo el mundo griego.

Las aportaciones de exvotos y ofrendas, más las construcciones de tesoros durante esta época, fueron cuantiosas:

Durante este siglo ocurrieron una serie de catástrofes que en nada beneficiaron al santuario de Delfos:

Durante la época del helenismo, difundida por los sucesores de Alejandro Magno se construyó un teatro nuevo y un estadio nuevo.

Los etolios (señores de Delfos) regalaron numerosas ofrendas en forma de columnas y estatuas. Pero los donantes más generosos de esta época fueron los reyes de Pérgamo que en varias ocasiones ofrecieron dinero y mano de obra para el mantenimiento del santuario. El rey de Pérgamo Átalo I regaló un conjunto monumental para celebrar su victoria sobre los gálatas. La donación fue de tal calidad que los etolios de Delfos junto con los componentes de la anfictionía mandaron erigir unas estatuas de Átalo I y de Eumenes II sobre unos pilares y las colocaron junto a la fachada del templo. También Perseo de Macedonia regaló una estatua con su efigie, pero más tarde su vencedor el general romano Lucio Emilio Paulo la mandó quitar para sustituirla por una que le representaba a él.

Son de esta época la epigrafía que cubría los muros de los edificios y del muro poligonal. En ella puede leerse los textos sobre los "derechos honoríficos" y sobre "la liberación de esclavos". Apolo era quien garantizaba dicha liberación, después de habérsele pagado la suma correspondiente. También es de esta época la epigrafía del tesoro de los atenienses.

Comenzó el declive en con la ocupación romana, durante el siglo I a. C. y continuó hasta el siglo III d.C., durante este período el oráculo, respetado aún, fue sin embargo perdiendo prestigio y visitantes. En el siglo I a. C. fue cuando se hizo la talla de una fuente rupestre en la pared de la garganta Castalia, allá donde desde antiguo se encontraba el manantial sagrado.

Los fondos para el mantenimiento del santuario, de sus monumentos y de sus tesoros van menguando a grandes pasos; la hierba crece entre los edificios, de manera salvaje, la madera se pudre y la suciedad empieza a notarse. Hubo además un incendio en el templo de Apolo que el emperador Domiciano (81-96) hizo reparar. El escritor griego Plutarco (c. 46-125), que además fue administrador de la anfictionía en los últimos años de su vida, escribió por entonces sus "Diálogos píticos" y en este libro comenta la "impresión de abandono" que le daba el santuario de Delfos.

A pesar de todo, la anfictionía continuaba reuniéndose, organizaba los "Juegos Píticos", levantaba algunas estatuas a los cónsules y emperadores romanos y el oráculo seguía siendo consultado. Pero las peticiones son ya de otro estilo: ya no se le pide consejo sobre posibles enfrentamientos, reinados, gobernantes, etc., las consultas del momento son consejos sobre viajes, matrimonios y otros asuntos domésticos. El oráculo ha dejado de influir en la política y el devenir de los pueblos. Su último momento de algo de esplendor se da bajo el gobierno de los Antoninos, en el siglo II de nuestra era. Los emperadores siguieron manteniendo una regular correspondencia con el oráculo. Esta correspondencia ha llegado hasta nuestros días grabada sobre los contrafuertes del templo de Apolo.

El emperador romano Adriano (c. 76-138) también visitó Delfos. Allí hizo levantar una estatua (que ha sido hallada en las excavaciones) en homenaje a su favorito Antínoo, que había muerto ahogado misteriosamente en el río Nilo.Herodes Ático (101-177), político y orador griego, sofista y protector de las letras, además de poseer una gran riqueza, donó parte de ésta a Delfos para reconstruir las gradas del estadio. También mandó erigir estatuas de su familia.

Pero ya por el siglo II d.c. el santuario recibía visitantes que eran más curiosos que fieles. Los viajeros llegaban allí para curiosear y no para utilizar el recinto como lugar sagrado. Pausanias fue uno de estos visitantes que llegó en calidad de hombre culto y amante de las antigüedades y luego contó sus impresiones como historiador. Para las gentes del siglo II el apogeo y utilización del santuario como lugar sagrado estaba tan lejano como pueden estar para los habitantes del siglo XXI los acontecimientos del Renacimiento. Ya en el año 87 a. C., Sila se había apropiado de muchas riquezas sagradas y de las ofrendas hechas en metales preciosos, lo mismo que el emperador Nerón en el siglo I. En el siglo IV el emperador romano Constantino I el Grande se llevó a Constantinopla (actual Estambul) una de las pocas piezas grandes que aún quedaban: la columna serpentina que se levantaba exenta y que nadie consideraba de valor después de que los focenses se llevaron 700 años antes su trípode de oro. Todavía se conserva.

En el siglo III los hérulos, godos y bastarnos recorrieron en intensas campañas toda la Grecia Central, Ática y el Peloponeso, arrasando y saqueando. En Delfos destruyeron algunas de las estatuas que quedaban en pie y el resto se vino abajo después del edicto de Teodosio el Grande, emperador romano (c. 346-395), con el que se pretendía acabar oficialmente con todos los "ídolos del paganismo", clausurando así definitivamente el oráculo de Delfos, que cesó su actividad en el año 390. La desolación fue total al cabo de los años y de los centenares de estatuas que antaño poblaron el recinto, no quedó ni una en pie.

El recinto de Delfos nunca llegó a estar deshabitado. Después de que se hubo olvidado por completo la razón de su existencia, sus ruinas se fueron recubriendo y se fue edificando toda una pequeña ciudad.

Tras la ocupación romana y la imposición del monoteísmo cristiano (y prohibición del politeísmo), durante el siglo V de nuestra era, el área de Delfos fue sede de un arzobispado, y para ello se desmanteló el oráculo, construyeron iglesias utilizando como material el mármol de los monumentos; se construyó una basílica, y grandes edificaciones religiosas, borrando así prácticamente toda evidencia del gran oráculo de Delfos. En el siglo XVIII los eruditos se plantearon la duda del lugar exacto en que habría estado el célebre santuario de Apolo. Por los textos antiguos se tenía una idea, pero era casi imposible dar con ningún vestigio. Hasta que gracias a un hallazgo fortuito empezaron los estudios sistemáticos y las excavaciones.

En 1676 Jacques Spon (francés) y George Wheler (inglés) llegaron al emplazamiento del santuario, convertido en un poblado llamado en ese momento "Castri". En su visita por el lugar se fijaron en unas inscripciones en la iglesia de un monasterio que había sido construido justamente sobre los muros del antiguo gimnasio. En estas inscripciones leyeron la palabra Delphi. Lo mismo les ocurrió en algunas casas del poblado. En estos años no pasó de ser una noticia para los historiadores; no hubo excavaciones.

Pasados dos siglos, en 1840, un arqueólogo alemán llamado Karl Otfried Müller trabajó en esta zona y descubrió entre las casas del poblado una parte del gran muro poligonal del recinto del santuario. El descubrimiento fue una llamada a seguir trabajando. Llegaron más arqueólogos franceses y alemanes, que fueron poco a poco descubriendo indicios y vestigios de la joya arqueológica que se escondía en aquel lugar. Pero la tarea era muy difícil pues la presencia del poblado impedía hacer excavaciones en serio. Empezaron entonces los tratos y los proyectos para trasladar a otro sitio todo el poblamiento de Castri, hasta que en 1881 hubo una convención entre el gobierno griego y el gobierno francés (muy interesado en las excavaciones) para expropiar, trasladar y reconstruir el nuevo emplazamiento, que es la ciudad actual llamada Delfí (Delfos).Comenzó una gran actividad arqueológica dirigida por el jefe de la Escuela Francesa de Atenas, Théophile Homolle. Fueron apareciendo piezas, restos de estatuas criselefantinas (es decir, estatuas que tenían la cara, las manos y los pies de marfil y el cabello de oro), piedras de edificios, columnas rotas, etc. Después vinieron las restauraciones llevadas a cabo por la Escuela francesa de Arqueología más una subvención del Ayuntamiento de Atenas y aportaciones particulares de ciudadanos griegos. De esta forma vieron la restauración:
Muchas de las piezas fueron llevadas al museo de Delfos que en la actualidad es uno de los más ricos de Grecia en el tema de la Antigüedad, entre otras el famoso auriga de bronce de tamaño natural ofrendado por Polyzelos, la Esfinge de Naxos, los mellizos de Argos y una copia romana del "ónfalos" que era la piedra en forma de huevo que señalaba el centro u "ombligo de mundo" en Delfos y que fue encontrado durante las excavaciones hechas al templo de Apolo.




</doc>

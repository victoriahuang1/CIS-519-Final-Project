<doc id="9665" url="https://es.wikipedia.org/wiki?curid=9665" title="Normas ISO 9000">
Normas ISO 9000

ISO 9000 es un conjunto de normas sobre calidad y gestión de calidad, establecidas por la Organización Internacional de Normalización (ISO). Se pueden aplicar en cualquier tipo de organización o actividad orientada a la producción de bienes o servicios. Las normas recogen tanto el contenido mínimo como las guías y herramientas específicas de implantación como los métodos de auditoría.

ISO 9000 especifica la manera en que una organización opera sus estándares de calidad, tiempos de entrega y niveles de servicio. Existen más de 20 elementos en los estándares de esta ISO que se relacionan con la manera en que los sistemas operan.


Su implementación aunque supone un duro trabajo, ofrece numerosas ventajas para las empresas, como pueden ser:


La normalización con base sistemática de la operación y científica nace a finales del siglo XIX, con la Revolución Industrial, ante la necesidad de producir más y mejor. Pero el impulso definitivo llegó con la primera Guerra Mundial (1914-1918). Ante la necesidad de abastecer a los ejércitos y reparar los armamentos, fue necesario utilizar la industria privada, a la que se le exigía unas especificaciones de intercambiabilidad y ajustes precisos. Nació para limitar la diversidad antieconómica de componentes, piezas y suministros, y favorecer su intercambiabilidad, facilitando la producción en serie, la reparación y mantenimiento de los productos y servicios, así como facilitar las relaciones externas entre países que necesitan piezas estándares, y ofreciendo garantías de cumplimiento de requisitos del cliente.
El 22 de diciembre de 1917, los ingenieros alemanes Naubaus y Hellmich, constituyen el primer organismo dedicado a la normalización: NADI - Normenausschuß der Deutschen Industrie - Comité de Normalización de la Industria Alemana. Este organismo comenzó a emitir normas bajo las siglas: DIN que significaban Deutsche Industrie Norm
(Norma de la Industria Alemana). En 1926 el NADI cambio su denominación por: DNA - Deutscher Normenausschuß - Comité de Normas Alemanas, que si bien siguió emitiendo normas bajos las siglas DIN, estas pasaron a significar "Das Ist Norm" - Esto es norma Y más recientemente, en 1975, cambio su denominación por: DIN - Deutsches Institut für Normung - Instituto Alemán de Normalización.

Rápidamente comenzaron a surgir otros comités nacionales en los países industrializados, así en Francia, en 1918 se constituyó la Asociación Francesa de Normalización (AFNOR). En 1919 en Inglaterra se constituyó la organización privada "British Standards Institution" (BSI).
Ante la aparición de todos estos organismos nacionales de normalización, surgió la necesidad de coordinar los trabajos y experiencias de todos ellos, con este objetivo se fundó en Londres en 1926 la: International Federation of the National Standardizing Associations – ISA. Tras la Segunda Guerra Mundial, este organismo fue sustituido en 1947, por la International Organization for Standardization - ISO - Organización Internacional para la Normalización. Con sede en Ginebra, y dependiente de la ONU.
Esta familia de normas apareció en 1987, tomando como base la norma británica BS 5750 de 1987, experimentando su mayor crecimiento a partir de la versión de 1994. 

La principal norma de la familia es la ISO 9001:2015: Sistemas de Gestión de la Calidad - Requisitos.

Otra norma vinculante a la anterior es la - Sistemas de Gestión de la Calidad - Directrices para la mejora del desempeño.

Las normas ISO 9000 de 1994 estaban principalmente dirigidas a organizaciones que realizaban procesos productivos y, por tanto, su implantación en las empresas de servicios planteaba muchos problemas. Esto fomentó la idea de que son normas excesivamente burocráticas.

Con la revisión de 2000 se consiguió una norma menos complicada, adecuada para organizaciones de todo tipo, aplicable sin problemas en empresas de servicios e incluso en la Administración Pública, con el fin de implantarla y posteriormente, si lo deciden, ser certificadas conforme a la norma ISO 9001.

La versión anterior, publicada el 13 de noviembre de 2008 fue sustituida por la versión vigente, que fue publicada el 23 de septiembre de 2015.

La única norma de la familia ISO 9000 que se puede certificar es la ISO 9001:2015.

Para verificar que se cumplen los requisitos de la norma, existen unas entidades de certificación que auditan la implantación y aplicación, emitiendo un certificado de conformidad. Estas entidades están vigiladas por organismos nacionales que regulan su actividad.

Para la implantación o preparación previa, es muy conveniente que apoye a la organización una empresa de consultoría, que tenga buenas referencias, y el firme compromiso de la Dirección de que quiere implantar el Sistema, ya que es necesario dedicar tiempo del personal de la empresa para implantar el Sistema de gestión de la calidad.
Con el fin de ser certificado conforme a la norma ISO 9001 (única norma certificable de la serie), las organizaciones deben elegir el alcance que vaya a certificarse, los procesos o áreas que desea involucrar en el proyecto, seleccionar un registro, someterse a la auditoría y, después de terminar con éxito, someterse a una inspección anual para mantener la certificación.

Los requerimientos de la norma son genéricos, a raíz de que los mismos deben ser aplicables a cualquier empresa, independientemente de factores tales como: tamaño, actividad, clientes, planificación, tipo y estilo de liderazgo, etc. Por tanto, en los requerimientos se establece el "qué", pero no el "cómo". Un proyecto de implementación involucra que la empresa desarrolle criterios específicos y que los aplique, a través del SGC, a las actividades propias de la empresa. Al desarrollar estos criterios coherentes con su actividad, la empresa construye su Sistema de Gestión de la Calidad.

En el caso de que el auditor encuentre áreas de incumplimiento, la organización tiene un plazo para adoptar medidas correctivas, sin perder la vigencia de la certificación o la continuidad en el proceso de certificación (dependiendo de que ya hubiera o no obtenido la certificación).

Un proyecto de implementación, involucrará, como mínimo: 


Las normas ISO se clasifican en:




</doc>
<doc id="9668" url="https://es.wikipedia.org/wiki?curid=9668" title="ARN no codificante">
ARN no codificante

Un ARN no codificante (ncRNA) es una molécula de ARN funcional, que no se traduce en una proteína. Sinónimos menos frecuentes son, no codifica proteínas ARN (npcRNA), ARN no mensajero (nmRNA) y ARN funcionales (fRNA).
El término small ARN ( sRNA) se utiliza a menudo por sus siglas en las bacterias ncRNAs. La secuencia de ADN de la que un ARN no codificante se transcribe, a menudo se llama un gen de ARN.

Los genes de ARN no codificante incluyen funcionalidades abundantes y muy importantes como ARN de transferencia (tRNA) y ARN ribosomal (rRNA), así como también en ARN, tales como snoRNAs, microARNs, siRNAs y piRNAs y el ncRNA largo, que incluyen ejemplos tales como Xist y HOTAIR(HOX antisense intergenic RNA).
El número de ncARNs codificantes en el genoma humano es desconocido; sin embargo, recientes estudios transcriptómicos y bioinformáticos sugieren la existencia de miles de ncARNs. Como la función de muchos de los ncARNs identificados recientemente no está confirmada, es posible que muchos sean no funcionales.

Los ácidos nucleicos fueron descubiertos por primera vez en 1868 por Friedrich Miescher, para 1939 el ARN había sido implicado en la síntesis de proteínas. Dos décadas más tarde, Francis Crick predijo un componente funcional de ARN que mediaba la traducción, sugirió que el ARN es más adecuado para pares de bases con la transcripción de ARNm de un puro polipéptido.

El primer ARN no codificante en ser caracterizado fue un tARN de alanina el cual se hallaba en la levadura utilizada para pastelería, su estructura se publicó en 1965. Para producir una muestra (purificada) de tARN de alanina, Robert W. Holley "et al". utilizó 140 Kg. de levadura de pastelería comercial para obtener no más de 1g de tARNA purificada para su análisis. El tARN de 80 nucleótidos fue secuenciado primero y digerido en ribonucleasa pancreática (producto de fragmentos que producían Citosina o Uridina) y después en ribonucleasa takadiastasa Tl (producto de fragmentos que producía Guanosina). La cromatografía y la identificación de los extremos 5' y 3' va ayudar después a ordenar los fragmentos para estabilizar la secuencia de ARN. De las tres estructuras propuestas originalmente para este tARN, la estructura en forma de trébol va a ser propuesta de forma independiente en siguientes publicaciones. La estructura secundaria en forma de trébol va ser concretada después de un análisis con cristalografía de rayos X realizado por dos grupos de investigación independientes en 1974.

El ARN ribosomal fue el siguiente en ser descubierto, seguido por el UARN a principios de los años 80. Desde entonces el descubrimiento de nuevos ARNs no codificantes ha continuado con los snoRNAs, Xist, CRISPR y muchos más. Adiciones notables recientes incluyen Riboswitches y el miARN, el descubrimiento del mecanismo del ARNi fue descubierto por Craig C. Mello y Andrew Fire (ganadores en el 2006 del Premio Nobel de Medicina y Fisiología).

Los ARNs no codificantes pertenecen a diversos grupos y participan en muchos procesos celulares. Estos van desde los ncARNs de vital importancia que se conservan en la mayoría de la vida celular a través de ncARNs más transitorios, que son específicos para una o unas pocas especies relacionadas. Se cree que los ncARNs más conservados son llamados fósiles o reliquias moleculares de LUCA y el mundo de ARN.

Muchos de los ncARNs esenciales y abundantes conservados están implicados en la traducción. Las partículas de ribonucleoproteina (RNP) o ribosomas son llamadas "fabricas" donde la traducción tiene lugar en la célula. El ribosoma cuenta con más del 60% de ARN ribosomal, formado por tres ncARNs en procariotas y cuatro ncARNs en eucariotas. Los ARNs ribosomales catalizan la traducción de secuencias de nucleótidos a proteína. Otro tipo de ncARNs, los ARN de transferencia, forman una molécula adaptadora” entre el mARN y las proteínas. Los snoARNs H/ACA box y C/D box son ncARNs que se encuentran en arqueas y eucariotas y la ARNasa MRP está restringida solo para las eucariotas, ambos grupos de ncARN están implicados en la maduración del rARN. Los snoARNs guían modificaciones covalentes de rARN, tARN y nARNs, la ARNasa MRP rompe el primer espaciador interno trascrito entre 18S u 5,8S rARNs. El ncARN omnipresente, RNasa P, es un pariente evolutivo de la RNasa MRP. La RNasa P madura secuencias de tARN generando 5' extremos maduros de tARN y cortanto el 5' elemento líder de los tARNs precursores. Otro omnipresente RNP llamado SRP reconoce y transporta nuevas proteínas al retículo endoplasmatico en eucariotas y a la membrana plasmática en procariotas. En bacterias el ARN mensajero de trasferencia (tmRNA)N) es un RNP implicado en rescatar ribosomas estancados, etiquetando polipéptidos incompletos y promoviendo la degradación de mARN en mal estado.

En las eucariotas la spliceosome realiza las reacciones de empalme, esenciales para retirar las secuencias de los Intrones. Este proceso es necesario para la formación de mARN maduro. El spliceosome es un empalmador RNP a menudo también conocido como snRNP o tri-snRNP. Hay dos formas diferentes de spliceosoma, la mayor y la menor. Los componentes de la ncRNA spliceosome principales U1, U2, U4, U5, U6. Los componentes de la ncRNA spliceosome menores son U11, U12, U5, U4atac y U6atac.
Otro grupo de intrones pueden catalizar su propia eliminación de las transcripciones acogidas, a esto se le conoce como auto-empalme de ARNs y se dividen en dos grupos principales, el grupo I de intrones catalíticos y el grupo II de intrones catalíticos. Estos ncARNs catalizan su propia escisión a partir de los precursores de mARN, tARN y rARN en una gran diversidad de organismos.

En mamíferos, se ha descubierto que los snoARNs pueden regular el empalme alternativo del ARNm. Por ejemplo, el snoARN HBII-52 regula el empalme del receptor de serotonina 2C.

En los nematodos, el SmY ncARN parece estar involucrada en el trans-splicing del mARN.

La expresión de muchos miles de genes son regulados por ncRNAs. Esta regulación se puede producir en el trans o en cis.

En las eucariotas superiores el microARN regula la expresión génica. Un solo microARN puede reducir los niveles de expresión de centenares de genes. Es el mecanismo por el cual las moléculas de microARN maduro actúan a través de moléculas parcialmente complementarias a uno o más mensajeros mARNs, generalmente en 3'UTRs. La función principal de los microARNs es inhibir la expresión genética.

Se ha demostrado que la ncARN RNasa P también influye sobre la expresión génica. En el núcleo humano, la RNasa P es necesaria para la trascripción normal y eficiente de diversos ncARNs transcritos por el ARN polimerasa III. Estos incluyen genes de tARN, 5S rARN, SRP ARN y U6 snARN. La ARNasa P ejerce su función en la trascripción asociándose con la Pol III y la cromatina de los genes activos de tARN y 5S rARN.

Se ha demostrado que el 7SK ARN, un ncARN de metazoos, actúa como un regulador negativo del factor de elongación P-TEFb de la ARN polimerasa II, y que esta actividad está influenciada por las vías de respuesta al estrés.

El ncARN bacterial, 6S ARN, se asocia específicamente en la holoenzima ARN polimerasa, que cuenta con el factor específico sigma 70. Esta interacción reprime la expresión de un promotor dependiente de sigma 70 durante la fase estacionaria.

Otro ncARN bacterial, el OxyS ARN, reprime la traducción asociándose a secuencias de Shine-Dalgarno obstaculizando la unión de los ribosomas. El OxyS ARN es producido como respuesta al estrés oxidativo en Escherichia coli.

El B2 ARN es un pequeño trascrito de ncARN polimerasa III que reprime la trascripción del mARN en respuesta al choque térmico producido en las células del ratón. El B2 ARN inhibe la trascripción asociándose al núcleo de la Pol II. A través de esta interacción, el B2 ARN reúne complejos de preiniciación en el promotor y bloquea la síntesis de ARN.

Un estudio reciente ha demostrado que solo el acto de trascripción de la secuencia de ncARN puede tener influencia sobre la expresión genética. La trascripción a ncARN de la ARN polimerasa II se necesita para la remodelación de la cromatina en los Shizosaccharomyces pombe. La cromatina adopta progresivamente una configuración abierta, así es como diversas especies de ncARN son transcritos.

Un número de ncARNs están unidos a los 5 'UTRs de los genes codificantes de proteínas e influencian su expresión de diversas maneras. Por ejemplo, un riboswitch puede unir directamente una molécula objetivo pequeña y esta unión afecta la actividad genética. 

Se encuentran secuencias de ARN líder en la zona superior del primer gen de los peronés biosintetizadores de aminoácidos. Estos elementos de ARN forman una de las dos posibles estructuras en regiones codificadas con secuencias péptidas muy cortas que son ricos en aminoácidos producto final del operon. Una estructura degradadota se forma cuando existe un exceso de los aminoácidos reguladores y el movimiento ribosómico sobre la trascripción del líder no se simplifica. Cuando hay una deficiencia de tARN cargado del aminoácido regulador, el ribosoma actúa formando la estructura no-degradativa. Así permite al ARN polimerasa transcribir el operón. Los ARN líderes conocidos son el operó líder de histidina, el operón líder de leucina, el operón líder de treonina y el operón líder de triptòfan.

El IRE (“Iron reponse elements”, Elementos de respuesta al hierro) están unidos por IRP (Iron response proteins”, proteínas de respuesta al hierro). El IRE se encuentra en UTRs de diferentes mARNs producidos que participan en el metabolismo del hierro. Cuando la concentración del hierro es baja, los IRPs conducen la IRE mARN de ferritina para la represión de su traducción.

IRES (“Internal ribosome entry sites”, sitios de entrada internos de los ribosomas) són una estructura de ARN que permiten la iniciación de la traducción en la mitad de la secuencia de mARN como parte del proceso de síntesis proteica.

Los ARNs Piwi-interactivos (piARNs) se expresan en los testículos y las células somáticas de los mamíferos, forman proteínas de ARN con las proteínas Piwi. Estas complejas piARNs están relacionadas a la inhibición de la trascripción genética de los retrotransposones y otros elementos genéticos de las células de la línea germinal, especialmente aquellas implicadas en la espermatogénesis.

El CRISPR (“Clustered Regularly Interspaced Short Palindromic Repeats”) son repeticiones encontradas en el DNA de muchas bacterias y de las arqueas. Las repeticiones están separadas por espacios de longitud similar. Se ha demostrando que estos espacios pueden derivar de infecciones víricas y, consecuentemente, pueden ayudar a las células a protegerse de las infecciones.

La telomerasa es una enzima RNP que añade repeticiones de secuencias específicas de DNA ("TTAGGG" en vertebrados) a regiones teloméricas, las cuales se encuentran al final de los cromosomas de los eucariotas. Los telomeros contienen DNA condensado, que dan estabilidad a los cromosomas. La enzima es una transcriptasa inversa que lleva ARN telomerasa, que es utilizada para alargar los telòmeros, que se acortan después de cada ciclo de replicación.

Xist (“X-inactive-specific transcript”, trascriptor especifico de X inactivo) es un gen largo ncARN que se encuentra en los cromosomas X de los mamíferos placentarios y actúa como un efector del proceso de inactivación del cromosoma X y la formación de cuerpos de Barr. Un RNA antisentido, Tsix, es un regulador negativo de Xist. Los cromosomas X con falta de expresión de Tsix (por lo tanto tiene un alto nivel de trascripción de Xist) son inactivados más frecuentemente que en los cromosomas normales. Existen también los drosofilidos, que utilizan de igual forma un sistema de determinación del sexo XY, los ARNs roX (ARN del cromosoma X) están involucrados en la compensación de la dosis. Tanto Xist como roX presentan una regulación epigenètica de la trascripción a través de la captación de enzimas modificadoras de las histonas.

Los ARNs bifuncionales son ARNs que tienen dos funciones diferentes. También son conocidos como ARNs de doble función. La mayoría de los ARNs bifuncionales conocidos son mARNs que codifican una proteína y ncARNs. Sin embargo, también hay un creciente número de ncARNs que se dividen en dos categorías diferentes de ncARN. Por ejemplo, H/ACA caja snoARN y miARN.

Dos ejemplos bien conocidos de ARNs bifuncionales son el SgrS ARN y el ARNIII. Sin embargo, se conocen también otros ARNs bifuncionales existentes: SRA, VegT ARN, Oskar ARN y ENOD40.

Al igual que con las proteínas, las mutaciones o los desequilibrios en el repertorio ncRNA en el cuerpo pueden causar una variedad de enfermedades.

Muchos ncARNs muestran patrones anormales de expresión en tejidos cancerosos. Estos incluyen miARNs, mARN-like ncARNs largos, GAS5 SNORD 50, ARN telomerasa y YARNs. Los miARNs están involucrados en una gran escala regulatoria de muchos genes codificadores de proteínas; los YARN son importantes para la iniciación de la replicación de DNA, El ARN telomerasa funciona como inicio para la telomerasa, una RNP encargada de alargar o mantener las regiones teloméricas en los extremos de los cromosomas. La función directa del mARN-like ncARNs lago es muy clara.

Se ha demostrado que las mutaciones de la línea germinal en los precursores primarios del miR-16-1 y del miR-15 son más frecuentes en los pacientes con leucemia linfoide crónica que en la población controlada.

Se ha sugerido que un SNP extraño (rs11614913) que está superpuesto a la hsa-mir-196a2 está asociado al carcinoma pulmonar de las células pequeñas. Del mismo modo, una pantalla de 17 miARNs que se ha previsto que regula un número de variaciones de genes asociados al cáncer de mama presentara variaciones al miR17 y miR-30c-1. Estos pacientes no eran portadores de mutaciones BRCA1 ni BRCA2, arrojando la posibilidad de que el cáncer de mama hereditario pudiera ser causado por variaciones de estos miARNs.

La supresión de los 48 ejemplares de la C / D caja snoRNA SNORD116 ha demostrado ser la causa principal del síndrome de Prader-Willi. Síndrome de Prader-Willi es un trastorno del desarrollo asociado con comer en exceso y dificultades de aprendizaje. SNORD116 tiene sitios de potencial objetivo en un número de genes codificadores de proteínas, y podría tener un papel en la regulación del empalme alternativo.
llll

El lugar cromosómico que contiene el ARN nucleolar pequeño SNORD115 de grupo de genes se ha duplicado en aproximadamente el 5% de individuos con rasgos autistas. Un modelo de ratón diseñado para tener una duplicación de la agrupación SNORD115 muestra el comportamiento de tipo autista.

Se ha demostrado que las mutaciones en ARNses MRP causan hipoplasia cartílago cabello, una enfermedad asociada a un grupo de síntomas tales como baja estatura, escasez de cabello, anormalidades esqueléticas y una supresión del sistema inmune que se produce frecuentemente entre los Amish y los Filandeses. La variante mejor caracterizada es una transición A-G del nucleótido 70 que esta en una región bucle de dos bases 5' de un pseudoknot (estructura secundària del DNA) conservada. De todas maneras, muchas otras mutaciones entre ARNasa MRP también causan hipoplasia cartílago cabello.

El antisense RNA BACE1-AS está trascrito desde la banda opuesta del BACE1 y una regulación progresiva en los pacientes con Alzheimer. BACE1-AS regula la expresión de BACE1 Mitjançant lo cual incrementa la estabilización del BACE1 mARN y genera BACE1 adicional a través de un mecanismo post-traduccional de prealimentación. Por el mismo mecanismo también se aumentan las concentraciones de beta-amieloide, el principal constituyente de las placas seniles. Las concentraciones BACE1-AS son elevadas en individuos con la enfermedad de Alzheimer y en ratones transgènicos con la proteína precursora del amiloide.

Variaciones en la región de inicio del miR-96 maduro se ha asociado a una enfermedad autosòmica dominante de progresiva perdida de audición en humanos y ratones. En los ratones homocigotos mutantes se descubrió que eran profundamente sordos, no mostrando respuestas cocleares. En los humanos y en los ratones heterocigotos progresivamente pierden su habilidad de escuchar.

Muchas publicaciones han comenzado a utilizar el término fARN, en comparación al ncARN, para describir regiones funcionales a nivel del ARN que podrían o no mantener separados los ARN transcriptos. Por tanto, cada ncARN es un fARN, pero existen fARN (tales como riboswitches, elementos SEICS y otras regiones cis-reguladoras) que no son ncARN. Sin embargo fARN también podría incluir mARN ya que este ARN codifica para proteínas y, por tanto, es funcional. Además, los ARNs evolucionados artificialmente también están englobados por el término fARN. Algunas publicaciones afirman que los términos ncARN y fARN son casi sinónimos.




</doc>
<doc id="9670" url="https://es.wikipedia.org/wiki?curid=9670" title="Anticuerpo">
Anticuerpo

Los anticuerpos (también conocidos como inmunoglobulinas, abreviado Ig) son glucoproteínas del tipo gamma globulina. Pueden encontrarse de forma soluble en la sangre u otros fluidos corporales de los vertebrados, disponiendo de una forma idéntica que actúa como receptor de los linfocitos B y son empleados por el sistema inmunitario para identificar y neutralizar elementos extraños tales como bacterias, virus. 

El anticuerpo típico está constituido por 2 unidades estructurales básicas, cada una de ellas con dos grandes cadenas pesadas y dos cadenas ligeras de menor tamaño, que forman, por ejemplo, monómeros con una unidad, dímeros con dos unidades o pentámeros con cinco unidades. Los anticuerpos son sintetizados por un tipo de leucocito denominado linfocito B. Existen distintas modalidades de anticuerpo, isotipos, basadas en la forma de cadena pesada que posean. Se conocen cinco clases diferentes de isotipos en mamíferos que desempeñan funciones diferentes, contribuyendo a dirigir la respuesta inmune adecuada para cada distinto tipo de cuerpo extraño que encuentran.

Aunque la estructura general de todos los anticuerpos es muy semejante, una pequeña región del ápice de la proteína es extremadamente variable, lo cual permite la existencia de millones de anticuerpos, cada uno con un extremo ligeramente distinto. A esta parte de la proteína se la conoce como región hipervariable. Cada una de estas variantes se puede unir a una "diana" distinta, que es lo que se conoce como antígeno. Esta enorme diversidad de anticuerpos permite al sistema inmune reconocer una diversidad igualmente elevada de antígenos. La única parte del antígeno reconocida por el anticuerpo se denomina epítopo. Estos epítopos se unen con su anticuerpo en una interacción altamente específica que se denomina adaptación inducida, que permite a los anticuerpos identificar y unirse solamente a su antígeno único en medio de los millones de moléculas diferentes que componen un organismo.

El reconocimiento de un antígeno por un anticuerpo lo "marca" para ser atacado por otras partes del sistema inmunitario. Los anticuerpos también pueden neutralizar sus objetivos directamente, mediante, por ejemplo, la unión a una porción de un patógeno necesaria para que éste provoque una infección.

La extensa población de anticuerpos y su diversidad se genera por combinaciones al azar de un juego de segmentos genéticos que codifican diferentes lugares de unión al antígeno (o "paratopos"), que posteriormente sufren mutaciones aleatorias en esta zona del gen del anticuerpo, lo cual origina una diversidad aún mayor. Los genes de los anticuerpos también se reorganizan en un proceso conocido como conmutación de clase de inmunoglobulina que cambia la base de la cadena pesada por otra, creando un isotipo de anticuerpo diferente que mantiene la región variable específica para el antígeno diana. Esto posibilita que un solo anticuerpo pueda ser usado por las diferentes partes del sistema inmune. La producción de anticuerpos es la función principal del sistema inmunitario humoral.

En general, como ya se dijo en la introducción, se considera que anticuerpo e inmunoglobulina son equivalentes, haciendo referencia el primer término a la función, mientras que el segundo alude a la estructura. El término gammaglobulina se debe a las propiedades electroforéticas de las inmunoglobulinas solubles en suero, si bien algunas inmunoglobulinas migran con las fracciones alfa, beta e incluso con la albúmina.

En 1890 comenzó el estudio de los anticuerpos cuando Emil Adolf von Behring y Shibasaburo Kitasato describieron la actividad de los anticuerpos contra la difteria y la toxina tetánica. Behring y Kitasato propusieron la teoría de la inmunidad humoral, que establecía la existencia de un mediador en el suero sanguíneo que podría reaccionar con un antígeno extraño, dándole el nombre de anticuerpo. Su idea llevó en 1897 a Paul Ehrlich a proponer la teoría de la cadena lateral de la interacción entre antígeno y anticuerpo y a lanzar la hipótesis de que existían receptores (descritos como "cadenas laterales") en la superficie de las células que se podrían unir específicamente a toxinas —en una interacción de tipo "llave-cerradura"— y que esta reacción de acoplamiento era el desencadenante de la producción de anticuerpos.

En 1904, siguiendo la idea de otros investigadores de que los anticuerpos se daban libres en la sangre, Almroth Wright sugirió que los anticuerpos solubles revestían las bacterias para señalarlas para su fagocitosis y destrucción en un proceso denominado opsonización.

En los años 1920, Michael Heidelberger y Oswald Avery descubrieron la naturaleza de los postulados anticuerpos al observar que los antígenos podían ser precipitados por ellos y demostrando que éstos eran un tipo de proteínas. 
A finales de los años 1930 John Marrack examinó las propiedades bioquímicas de las uniones antígeno-anticuerpo. Luego, en los años 1940 tiene lugar el siguiente avance de importancia, cuando Linus Pauling confirmó la teoría de la llave y la cerradura propuesta por Ehrlich mostrando que las interacciones entre anticuerpos y antígenos dependían más de su forma que de su composición química. En 1948, Astrid Fagreaus descubrió que los linfocitos B en su forma de célula plasmática eran responsables de la producción de anticuerpos.

Los siguientes trabajos de investigación se concentraron en la caracterización de la estructura molecular de los anticuerpos:

Los linfocitos B activados se diferencian en células plasmáticas, cuyo papel es la producción de anticuerpos solubles o bien en linfocitos B de memoria, que sobreviven en el organismo durante los años siguientes para posibilitar que el sistema inmune recuerde el antígeno y responda más rápido a futuras exposiciones al agente inmunógeno. Los anticuerpos son, por tanto, un producto esencial del sistema inmunitario adaptativo que aprenden y recuerdan las respuestas a patógenos invasores. Los anticuerpos se encuentran en dos formas: en forma soluble secretada en la sangre y otros fluidos del cuerpo y en forma unida a la membrana celular que está anclada a la superficie de un linfocito B.

Los anticuerpos solubles son secretados por un linfocito B activado (en su forma de célula plasmática) para unirse a sustancias extrañas y señalizarlas para su destrucción por el resto del sistema inmune. También se les podría llamar "anticuerpos libres" hasta que se unen a un antígeno y acaban como parte de un complejo antígeno-anticuerpo o como "anticuerpos secretados".
En estas formas solubles se unen a las inmunoglobulinas moléculas adicionales. En la IgM, por ejemplo, encontramos una glucoproteína unida a la Fracción constante mediante puentes disulfuro de unos 15 KD llamada cadena J. Al isotipo IgA, además, se le une la llamada "pieza de secreción". Se trata de una glucoproteína que se forma en las células epiteliales y glándulas exocrinas, y que posteriormente se une a la inmunoglobulina para facilitar su secreción. (Peña, 1998)

La forma anclada a membrana de un anticuerpo se podría llamar "inmunoglobulina de superficie" (sIg) o "inmunoglobulina de membrana" (mIg), que no es secretado: siempre está asociado a la membrana celular. Forma parte del "receptor del linfocito B" (BCR), que permite a éste detectar cuando un antígeno específico está presente en el organismo, desencadenando la activación del linfocito B. El BCR se compone de anticuerpos IgD o IgM unidos a la superficie de membrana y sus heterodímeros asociados Ig-α e Ig-β que tienen capaz de producir la transducción de señal del reconocimiento del anticuerpo a la célula. Un linfocito B humano típico tiene entre 50.000 y 100.000 anticuerpos unidos a su superficie. Tras el acoplamiento del antígeno, éstos se agrupan en grandes parches cuyo diámetro puede exceder de 1μm en balsas lipídicas que aislan los BCRs (receptores de la célula B) de la mayor parte de los restantes receptores de señalización celular.
Estos parches podrían mejorar la eficiencia de la respuesta inmune celular. En los seres humanos, la superficie celular está libre de otras proteínas alrededor de los receptores de los linfocitos B en distancias de algunos miles de angstroms, lo cual reduce de tal manera las influencias que compiten con su función, que incluso aísla a los BCRs.

Los anticuerpos pueden presentarse en distintas variedades conocidas como isotipos o clases. En mamíferos placentados existen cinco isotipos de anticuerpos conocidos como IgA, IgD, IgE, IgG e IgM. Se nombran mediante el prefijo "Ig" que significa inmunoglobulina y difieren en sus propiedades biológicas, localizaciones funcionales y capacidad para reconocer diferentes tipos de antígenos como se muestra en la tabla.

El isotipo cambia durante el desarrollo y la activación de los linfocitos B. Antes de la maduración de estos últimos, cuando aún no se han expuesto a su antígeno, se conocen como linfocitos B vírgenes y solo expresan el isotipo IgM en su forma anclada a la superficie celular. Los linfocitos comienzan a expresar tanto IgM como IgD cuando alcanzan la madurez y en ese momento están listos para responder a su antígeno. La activación de los linfocitos B sigue al encuentro y unión de éste con su antígeno, lo que estimula a la célula para que se divide y se diferencie en una célula productora de anticuerpos denominada plasmática. En esta forma activada, los linfocitos B comienzan a secretar anticuerpos en lugar de anclarlos a la membrana. Algunas células hijas de los linfocitos B activados sufren un cambio isotípico, un mecanismo que provoca que la producción de anticuerpos en las formas IgM o IgD se trasmute a los otros tipos, IgE, IgA o IgG, que desempeñan distintos papeles en el sistema inmunitario.

Se entiende por alotipo las pequeñas diferencias en la secuencia de aminoácidos en la región constante de las cadenas ligeras y pesadas de los anticuerpos producidos por los distintos individuos de una especie, que se heredan de forma mendeliana (Peña, 1998).
En seres humanos se han descrito 3 tipos de determinantes alotípicos:

El idiotipo es el epítopo propio de una molécula perteneciente a un clon en particular. Este elemento forma parte o está muy próximo al lugar de reconocimiento del antígeno, y está situado en la porción variable Fab. En otras palabras, es el paratopo, o la región cercana de una inmunoglobulina puede ser reconocido como un epitopo por ciertos linfocitos (Staff, 2003). Según la Teoría de Jerne, La formación de anticuerpos antiidiotipo formaría una red (red de Jerne) cuya función sería la regulación de la síntesis de nuevas inmunoglobulinas. (Peña, 1998).

Los anticuerpos son proteínas plasmáticas globulares pesadas (~150 kDa), también conocidas como inmunoglobulinas. Tienen cadenas de azúcares unidas a alguno de sus residuos aminoácido. En otras palabras, los anticuerpos son glucoproteínas. La unidad básica funcional de cada anticuerpo es el monómero de inmunoglobulina, que contiene una sola unidad de Ig. Los anticuerpos secretados también pueden ser diméricos con dos unidades Ig, como en el caso de las IgA, tetraméricos con cuatro unidades Ig como en el caso de las IgM de teleósteo, o pentaméricos con cinco unidades de IgM, como en el caso de las IgM de mamíferos. 
Las primeras investigaciones sobre la estructura de los anticuerpos fueron realizados mediante sencillas digestiones con pepsina y papaína por Rodney Robert Porter y Gerald M. Edelman, seguidas de electroforesis. Ambos recibieron por ello el Premio Nobel de medicina en 1972. También fue importante la figura de Alfred Nisonoff:

El monómero de Ig es una molécula en forma de "Y" que consta de dos cadenas de polipéptido; dos "cadenas pesadas" idénticas y dos "cadenas ligeras" idénticas conectadas por enlaces disulfuro. Cada cadena se compone de dominios estructurales llamados dominios Ig. Estos dominios contienen entre 70 y 110 aminoácidos y se clasifican en diferentes categorías, por ejemplo en variables (IgV) y constantes (IgC) de acuerdo con su tamaño y función. Tienen un "pliegue inmunoglobulina" característico en el cual dos láminas beta generan una forma de "sándwich", permaneciendo juntas por interacciones entre cisteínas bien conservadas a lo largo de la evolución, así como otros aminoácidos cargados.

Hay cinco tipos de Ig en mamíferos que se nombran por letras griegas: α, δ, ε, γ y μ. El tipo de cadena pesada presente define la "clase" (isotipo) del anticuerpo. Estas cadenas se encuentran en los anticuerpos IgA, IgD, IgE, IgG, e IgM respectivamente. Las distintas cadenas pesadas difieren en tamaño y composición: α y γ contienen aproximadamente 450 aminoácidos, mientras que μ y ε poseen aproximadamente 550 aminoácidos. 
Las cadenas pesadas γ, α y δ tienen una región constante compuesta de "tres" dominios estructurales Ig en tándem y una región bisagra para proporcionarle flexibilidad. Las cadenas pesadas μ y ε tienen una región constante compuesta por "cuatro" dominios inmunoglobulina. La región variable de la cadena pesada difiere en los anticuerpos producidos en los diferentes linfocitos B, pero es idéntica para todos los anticuerpos producidos por el mismo linfocito B o por su línea clonal. La región variable de cada cadena pesada es de aproximadamente 110 aminoácidos y está compuesto por un único dominio Ig.

Recientemente se ha podido determinar la topología "in vivo" del gen de la cadena pesada, "Igh", siendo este uno de los primeros estudios en este campo. El resultado es que la cromatina se dispone formando giros sucesivos unidos por "linkers", dando lugar a formas similares a una flor. La posición relativa de los distintos segmentos varía drásticamente a lo largo del desarrollo del linfocito B, permitiendo así un mayor rango de interacciones genómicas.

En los mamíferos hay dos tipos de cadena ligera, llamados lambda (λ) y kappa (κ). Una cadena ligera contiene dos dominios sucesivos: un dominio constante y un dominio variable. La longitud aproximada de la cadena ligera es de 211 a 217 aminoácidos. Cada anticuerpo contiene dos cadenas ligeras que son siempre idénticas. Solo un tipo de cadena ligera, κ o λ, está presente dentro del mismo anticuerpo en mamíferos. Otros tipos de cadenas ligeras como la cadena iota (ι), se encuentran en los vertebrados inferiores como los condrictios y teleósteos.

Algunas partes del anticuerpo tienen funciones únicas. Los extremos de la "Y", por ejemplo, contienen el lugar que se une al antígeno y por tanto, reconoce elementos extraños específicos. Esta región del anticuerpo se llama "fragmento de unión al antígeno" o región Fab. Está compuesta de un dominio constante y otro variable de cada una de las cadenas ligera y pesada del anticuerpo. El paratopo está conformado por los dominios variables de la cadena pesada y ligera en el extremo amino terminal del monómero de anticuerpo. 
El papel que desempeña la base de la "Y" consiste en modular la actividad de la célula inmunitaria. Esta región se llama "fragmento cristalizable" o Fc y está compuesta por dos o tres dominios constantes de ambas cadenas pesadas, dependiendo de la clase del anticuerpo. Mediante la unión a proteínas específicas la región Fc se asegura que cada anticuerpo genera una respuesta inmune apropiada para un antígeno dado. La región Fc también se une a varios receptores celulares como el receptor del Fc y otras moléculas del sistema inmunitario como las proteínas del complemento. Al efectuar esto, media en diferentes efectos fisiológicos incluyendo la opsonización, lisis celular y desgranulación de las células cebadas, basófilos y eosinófilos.

Puesto que los anticuerpos se dan de forma libre en el torrente sanguíneo, se dice que son parte del sistema inmunitario humoral. Los anticuerpos circulantes son producidos por líneas clonales de linfocitos B que responden específicamente a un antígeno que puede ser un fragmento de proteína de la cápside viral, por ejemplo. Los anticuerpos contribuyen a la inmunidad de tres formas distintas: pueden impedir que los patógenos entren en las células o las dañen al unirse a ellas (neutralización). Pueden estimular la eliminación de un patógeno por los macrófagos y otras células revistiendo al patógeno (opsonización) y pueden desencadenar la destrucción directa del patógeno estimulando otras respuestas inmunes como la vía del complemento (lisis).

Los anticuerpos que se unen a la superficie de los antígenos, por ejemplo, en una bacteria, atraen los primeros componentes de la cascada del complemento mediante su región Fc e inician la activación del sistema "clásico" del complemento. Esto acaba con la muerte de la bacteria de dos formas: Primero, la unión de las moléculas del complemento con el anticuerpo marca al microbio para la ingestión por los fagocitos en un proceso llamado opsonización. Estos fagocitos son atraídos por ciertas moléculas del complemento. En segundo lugar, algunos componentes del sistema del complemento forman un complejo de ataque a membrana para ayudar a los anticuerpos a matar a la bacteria por medio de lisis. Los anticuerpos más efectivos en la activación del Sistema del Complemento son los de tipo IgM y los de tipo IgG subclase 1 y 3 (IgG1 e IgG3).

Para combatir a los patógenos que se replican en el exterior de las células, los anticuerpos se unen a los patógenos para ensamblarlos juntos provocando su aglutinación. Puesto que un anticuerpo tiene al menos dos paratopos se puede unir a más de un antígeno acoplándose a epítopos idénticos portados en las superficies de esos antígenos. Revistiendo al patógeno, los anticuerpos estimulan las funciones efectoras contra éste en las células que reconocen la región Fc.

Aquellas células que reconocen los patógenos revestidos tienen receptores del Fc que, como su nombre indica, interactúan con la región Fc de los anticuerpos IgA, IgG, e IgE. El acoplamiento de un anticuerpo particular con el receptor Fc de una determinada célula desencadena en ella una función efectora: los fagocitos realizarán la fagocitosis, las células cebadas y los neutrófilos producirán la degranulación, las células asesinas naturales liberarán citoquinas y moléculas citotóxicas que finalmente acabarán con la destrucción del microbio invasor. Los receptores Fc son específicos del isotipo, lo que da una mayor flexibilidad al sistema inmune, afectando solo al mecanismo inmune adecuado para los distintos patógenos.

Prácticamente todos los microorganismos pueden desencadenar la respuesta de los anticuerpos. El reconocimiento y la erradicación con éxito de tipos muy distintos de estos últimos requiere que los anticuerpos posean una enorme diversidad. Su composición de aminoácidos varía para permitirles interactuar con antígenos muy diferentes. Se ha estimado que los seres humanos generan unos 10 mil millones de anticuerpos diferentes, cada uno de ellos capaz de unirse a un epítopo distinto. Aunque se genera un enorme repertorio de diferentes anticuerpos en un mismo individo, el número de genes disponible para fabricar estas proteínas es limitado. En los vertebrados han evolucionado diferentes mecanismos genéticos complejos para permitir que los linfocitos B generen esta diversidad a partir de un número relativamente pequeño de genes de anticuerpos.

La región (locus) del cromosoma que codifica un anticuerpo es grande y contiene varios genes diferentes para cada dominio del anticuerpo - el locus que contiene los genes para las cadenas pesadas(IGH@) se encuentra en humanos en el cromosoma 14 y los loci que contienen los genes lambda y kappa de la cadena ligera (IGL@ e IGK@) se encuentran en los cromosomas 22 y 2. Uno de estos dominios es conocido como "dominio variable", que está presente en todas las cadenas ligeras y pesadas de los anticuerpos, pero pueden ser diferentes entre los distintos anticuerpos generados por las variadas líneas de linfocitos B. Las diferencias entre los dominios variables se localizan en tres bucles conocidos como regiones hipervariables (HV-1, HV-2 y HV-3) o regiones determinantes de la complementariedad (CDR1, CDR2 y CDR3). Las CDRs se mantienen entre los dominios variables por regiones de marco conservado. El locus de la cadena pesada contiene unos 65 genes de dominio variable distintos, que difieren en sus CDRs. Combinando estos genes con varios genes de otros dominios se genera un gran contingente de anticuerpos con un alto grado de variabilidad. A esta combinación se la denomina "recombinación V (D) J, que explicamos a continuación.

La recombinación somática de las inmunoglobulinas, conocida también como "Recombinación V (D) J", consiste en la generación de una región variable de inmunoglobulina exclusiva. La región variable de cada inmunoglobulina pesada está codificada por varias partes, que se conocen como segmentos. Estos son conocidos como segmento variable (V), diversidad (D) y de acoplamiento —joining, en inglés— (J). Los segmentos V, D y J se encuentran en las cadenas pesadas. En las ligeras solo encontramos los segmentos V y J. Hay múltiples copias de todos estos segmentos organizadas en tándem en el genoma de los mamíferos. En la médula ósea cada linfocito B en desarrollo ensambla la región variable de su inmunoglobulina seleccionando y combinando al azar un segmento V con uno D y otro J (o bien uno V y otro J en la cadena ligera). Puesto que existen múltiples copias ligeramente distintas para cada secuencia genética de los segmentos, se darían diferentes combinaciones que mediante este proceso generan un elevado número de paratopos y también diferentes especificidades de antígeno.

Tras la producción de una inmunoglobulina funcional por un linfocito B durante la recombinación V(D)J no podrá expresar ninguna región variable diferente (a este proceso se le conoce como exclusión alélica). Así pues, cada linfocito B solo puede producir anticuerpos que contienen un solo tipo de cadena variable.

Otro mecanismo que genera diversidad en los anticuerpos tiene lugar en los linfocitos B maduros. Tras la activación por antígeno, los linfocitos B comienzan a proliferar rápidamente. En estas células en rápida división, los genes que codifican los dominios variables de las cadenas pesadas y ligeras sufren una gran tasa de mutación puntual mediante un proceso llamado "hipermutación somática" (SHM). Ésta produce aproximadamente el cambio de un nucleótido por gen variable y célula en cada división celular. Como consecuencia, cualquier célula hija de una línea de linfocitos B adquiere una ligera diferencia en la secuencia de aminoácidos de los dominios variables de sus cadenas de anticuerpos.

La hipermutación somática sirve para incrementar la diversidad del reservorio de anticuerpos e influye en la afinidad de la unión entre el antígeno y el anticuerpo. Algunas mutaciones puntuales terminarán por producir anticuerpos que tienen interacciones más débiles (baja afinidad) con su antígeno que el anticuerpo original, mientras que otras generarán anticuerpos con una interacción más fuerte (alta afinidad). Los linfocitos B que expresan anticuerpos de elevada afinidad en su superficie recibirán una fuerte señal para que sobrevivan durante las interacciones con otras células, mientras que las que expresan anticuerpos de baja afinidad morirán por apoptosis. Así pues, los linfocitos B que expresan anticuerpos con una afinidad más elevada por su antígeno competirán con ventaja contra aquellos de menor afinidad en su función y supervivencia. El proceso de generación de anticuerpos con afinidad aumentada progresivamente se llama "maduración de la afinidad". La maduración de la afinidad tiene lugar en los linfocitos B maduros tras la recombinación V(D)J y es dependiente del soporte que reciban de los linfocitos T colaboradores.
La Conmutación de la clase de la inmunoglobulina es un proceso biológico que tiene lugar tras la activación de los linfocitos B, lo cual le permite la producción de diferentes clases de anticuerpos (IgA, IgE, o IgG). Estas clases están definidas por las regiones constantes (C) de la cadena pesada de la inmunoglobulina. Inicialmente los linfocitos B vírgenes expresan solo IgM e IgD de superficie con regiones de unión al anticuerpo idénticas. Cada isotipo está adaptado para una función distinta y por tanto, tras la activación, se necesita un anticuerpo con un efector IgG, IgA o IgE para la eliminación eficaz del antígeno. La conmutación de clase permite a la progenie de un solo linfocito B producir anticuerpos de diferentes isotipos. Solo la región constante de la cadena pesada del anticuerpo cambia durante la conmutación de clase. Las regiones variables, y por tanto la especificidad de antígeno, permanece invariable. De ese modo se producen efectores con la función adecuada para cada amenaza del antígeno. La conmutación de clase se inicia por citoquinas. El isotipo generado depende de que citoquinas estén presentes en el entorno del linfocito B.

El proceso tiene lugar en el gen de la cadena pesada por un mecanismo conocido como recombinación de conmutación de clase ("class switch recombination" o CSR). Este mecanismo se basa en secuencias de nucleótidos conservadas, llamadas regiones de conmutación "(Regiones switch o S)", que se encuentran en un punto de la secuencia de ADN anterior a los genes de la región constante (excepto en la cadena δ). La hebra de ADN se escinde por la actividad de ciertas enzimas en dos regiones S concretas. El exón del dominio variable se vuelve a empalmar mediante un proceso llamado unión de extremos no homóloga ("non-homologous end joining" o NHEJ) a la región constante elegida (γ, α o ε). Este proceso concluye formando un gen de inmunoglobulina que codifica un anticuerpo de un isotipo diferente.

La conversión génica es un intercambio no recíproco, en el que la secuencia donante no se modifica, mientras que el gen aceptor adquiere un segmento del donante por recombinación homóloga. Aunque este mecanismo para generar diversidad en los anticuerpos se conocía, no se le había dado la suficiente relevancia hasta ahora. Se sabe que es muy importante en aves, las cuales usan en sus cadenas ligeras y pesadas un gran número de pseudogenes semejantes a las secuencias D, situadas al principio de la secuencia del gen de las cadenas de inmunoglobulina. Posteriormente, estos segmentos cambian somáticamente la única región V, pudiendo también estar sometidas a hipermutación. Este mecanismo, curiosamente, también está presente en algunos mamíferos, como los conejos.

Una vez reagrupados todos los segmentos, se produce un solo mARN, que se poliadenila. Este ARN abandona el núcleo, dirigiéndose a los ribosomas del retículo endoplásmico rugoso, donde comienza su traducción. Posteriormente se produce la glicosilación de los mismos en la parte luminal del RER y el ensamblaje, cuyo proceso es el siguiente H+H → H2+L → H2L2. Constituye una excepción la IgM, uniéndose primero una cadena pesada con una ligera. Su destino final, como receptor o bien ser secretada, depende de si posee o no un fragmento añadido de 19 aminoácidos en la zona C-terminal. Este péptido se incorpora a la síntesis mediante un proceso de splicing. Su presencia determina una región hidrofóbica capaz de anclarse a la membrana celular (Peña, 1998).

El desarrollo de organismos complejos, con tejidos y varias líneas celulares necesitó del desarrollo de nuevas moléculas para asegurar, por un lado, que las células se adherían a otras de la misma colonia y por otro, la defensa ante posibles intrusos parásitos o patógenos. Tres tipos de moléculas, las lectinas, las LLR's y las inmunoglobulinas, han sido utilizadas a lo largo de la evolución en el desarrollo de sistemas inmunitarios. Sus patrones operativos se mezclan en ocasiones para combinar sus propiedades, aunque existen pocas moléculas que contengan los tres, como es el caso del gen de la enfermedad poliquística renal (PKD1).

Muchos estudios aportan pruebas importantes de que la superfamilia de las inmunoglobulinas tienen representantes entre las bacterias y arqueas o que al menos las presentes en este grupo y las de eucariotas podrían tener un antepasado común, desde el cual evolucionaron de forma divergente. Así, se han atribuido a este grupo de proteínas "semejantes a inmunoglobulina" bacterianas (BIg's) al receptor de la Fc de Ig en "Streptococcus agalactiae", y la endoglucanasa C de "Cellumonas fimi". También existen otros ejemplos como la invasina de "Yersinia pseudotuberculosis" o las Lig ("Leptospiral Ig-like") de diversas especies de "Leptospira". Tras el hallazgo en "Streptococcus" se descubrió una proteína de este tipo en el fago T4. En esta ocasión se destacó que su papel estaba relacionado con la adhesividad celular.

Las proteínas con dominios Ig son comunes en eucariotas unicelulares, y hasta cierto punto su estructura es un rasgo conservado. Un ejemplo de ello sería las alfa aglutininas en "Saccharomyces cerevisiae". Se trata de moléculas que medían la adhesión celular y que guardan grandes homologías con el grupo CD2 - CD4 en humanos, cuyo papel es en parte similar, interviniendo en este último caso la adhesión de los linfocitos T con las células presentadoras de antígenos y las células diana.
Sin embargo, es en los grupos de animales pluricelulares más primitivos, los parazoa, donde los científicos intentan hallar respuestas al origen del sistema inmunitario adaptativo. En este sentido, se han dirigido varios trabajos de investigación hacia este grupo, y en especial hacia una esponja considerada como fósil viviente, "Geodia cydonium" y también "Suberites domuncula". En esta primera se encuentran muchos de los tipos de proteínas que también están implicadas en la inmunidad de mamíferos. En especial, hay dos tipos de la superfamilia de las inmunoglobulinas distintas, las unidas a receptor tirosín kinasa, y las moléculas no enzimáticas de adhesión de las esponjas. Curiosamente, los dominios correspondientes ya demuestran polimorfismo, y aún más, aunque cumplen papeles que son simultáneamente de receptores, y de moléculas de adherencia celular, se sobre regulan en experimentos de injerto.

En definitiva, la superfamilia de las inmunoglobulinas intervino en el surgimiento de la multicelularidad al mantener la integridad estructural de los organismos distinguiendo de lo propio de lo ajeno. Esto es debido a que gracias a sus capacidades de generar módulos, de unirse específicamente a otras proteínas y de formar bastones, así como de oligomerizarse y generar diversidad por splicing alternativo a partir de material genético limitado, se convierten en ideales para mediar la adherencia celular y como receptores de superficie de membrana.

En la búsqueda de precedentes del sistema inmunitario adaptativo, encontramos varios ejemplos de proteínas de la superfamilia de las Ig en protóstomos que cumplen un papel en la defensa inmunitaria, como la hemolina en gusanos de seda, o la proteína Dscam en "Drosophila melanogaster", así como proteínas relacionadas con el fibrinógeno con dominios Ig (FREPs) en gasterópodos. Algunas de estas proteínas, que representan una barrera de tipo innato, pueden tener isoformas solubles y ancladas a membrana, y generar diversidad por splicing alternativo, y en zonas de la molécula diferentes a las cadenas variables de vertebrados.

Muchos de los elementos del sistema inmune adaptativo, incluidas las células especializadas, están ya preconfigurados en los organismos más basales de los deuteróstomos. Se han realizado trabajos en el erizo de mar "Strongylocentrotus purpuratus", encontrándose un rico sistema inmunitario con homólogos de importantes reguladores inmunitarios y hematopoyéticos de vertebrados, algunos de ellos críticos. Se especula por ello que la presión evolutiva clave para el desarrollo del complejo sistema inmunitario en deuteróstomos no fue tanto la amenaza de patógenos como la existencia de una rica variedad de organismos simbiontes, circunstancia que los propios seres humanos ponemos en evidencia en nuestra flora intestinal.
Como ilustración de este punto, se ha visto que el 60 % de las especies de equinodermos se asocian con simbiontes bacterianos.
En tunicados continúa el aumento de la complejidad del sistema inmune. En la ascidia "Botryllus schlosseri", durante experimentos de injertos no compatibles, se detectaron muchas proteínas que revelan un complejo sistema inmune innato y algunas proteínas con dominio inmunoglobulina. Y lo que resulta más sorprendente, también se puede encontrar un homólogo convincente de RAG1, contiguo a una estructura similar a RAG2. Posteriormente expondremos la importancia de esto último.
Sin embargo, es en cefalocordados donde encontramos las primeras huellas de nuestras actuales inmunoglobulinas. Se han realizado múltiples estudios en el anfioxo "Branchiostoma floridae", encontrando unas curiosas proteínas, llamadas VCBP (por Proteínas tipo V que contienen dominios que se unen a quitina) con grandes homologías con las regiones V (variables) de las inmunoglobulinas, ciertamente implicadas en la respuesta inmunitaria, pero carentes de su variabilidad. Estudios cristalográficos han demostrado que probablemente se trata de una molécula semejante al ancestro de las actuales regiones variables de vertebrados.

En los actuales agnatos se dan alguno de los rasgos que identifican un moderno sistema inmunitario adaptativo, mientras que otros están ausentes. Por una parte, existen células que ya contienen gran parte de la maquinaria molecular de los linfocitos. Esto sugiere una evolución de este tipo celular en los vertebrados más basales, y posiblemente en un protocordado. Existen varias proteínas Ig con dominios semejantes a V, que incluso contienen regiones V y J, aunque están codificados en un único exón y no es reorganizable. Sin embargo, no poseen un sistema inmunitario como el de los vertebrados, basado en los clásicos anticuerpos solubles, receptores de membrana, reorganización y empalme por RAG. En lugar de ello, esta función es asumida por una serie de proteínas ricas en repeticiones de leucina, que incluso pueden sufrir una compleja recombinación, a resultas de la cual se obtiene una variabilidad equiparable a la de los anticuerpos (10). Esto constituye un extraordinario ejemplo de evolución paralela.

Todos los autores revisados en este artículo coinciden en que la emergencia del moderno sistema inmunitario tuvo que suceder hace 500 millones de años, durante la explosión cámbrica. Probablemente lo harían dentro de un contexto en el que existirían muchas formas y combinaciones de módulos de proteínas de las que muchas desaparecerían por las presiones selectivas. En este sentido, una de las cuestiones que suscita el apartado anterior es que si la evidencia paleontológica indica que los peces mandibulados actuales proceden de los agnatos, y estos carecen del mismo sistema recombinación de los modernos sistemas inmunitarios, Seguramente debió existir un antepasado común, un ostracodermo ancestral que poseyera ambos sistemas. De acuerdo con este punto de vista, el sistema de recombinación V (D) J probablemente representa un desarrollo evolutivo convergente en una rama de los ostracodermos que precedió a la línea de los gnatóstomos.

En cuanto a las clases de las inmunoglobulinas, en peces encontramos análogos a la clase IgM, así como la IgD, identificada en muchas especies de teleósteos; También existen muchas exclusivas, como las que contienen las cadenas pesadas ζ y τ. Posiblemente son isotipos anteriores a la IgM en la evolución. En el caso de los condrictios también encontramos isotipos exclusivos, además de IgM. Se trata de las IgW (IgX o IgNARC) y las IgNAR.

El tipo IgG surge en anfibios y continúa en reptiles, mientras que el tipo IgA aparentemente surge en un antepasado común entre aves y mamíferos. El tipo IgE parece ser exclusivo de mamíferos (Peña, 1998).

En muchos diagnósticos es común la detección de anticuerpos como prueba de confirmación de la patología. Para ello se realiza una prueba serológica. Como ejemplos, en ensayos bioquímicos para el diagnóstico de enfermedades, se estima el título de anticuerpos contra el virus de Epstein-Barr o contra la enfermedad de Lyme. Si no se encuentran esos anticuerpos significa que la persona no está infectada o que lo estuvo hace "mucho" tiempo y los linfocitos B que generaban estos anticuerpos se han reducido de forma natural.

En la inmunología clínica se valora por nefelometría (o turbidimetría) los niveles de las distintas clases de inmunoglobulinas para caracterizar el perfil de anticuerpos del paciente. Por ejemplo, una observación en elevación del título de las distintas clases de inmunoglobulina puede ser útil en ocasiones para determinar la causa del daño hepático mediante diagnóstico diferencial. En este sentido, un título elevado de IgA indicaría cirrosis alcohólica; si lo que está elevado son las IgM se sospecha de hepatitis viral y cirrosis biliar primaria, mientras que la IgG está elevada en hepatitis vírica, autoinmune y cirrosis.

Las enfermedades autoinmunes se puede diagnosticar por anticuerpos que se unen a epítopos del propio organismo; muchos de ellos se pueden detectar mediante análisis de sangre. Un ejemplo sería el caso de los anticuerpos dirigidos contra los antígenos de superficie de eritrocitos en la anemia hemolítica mediada por el sistema inmunitario, que se detectan mediante la prueba de Coombs. Esta prueba también se usa para rastrear anticuerpos en la preparación de transfusiones de sangre y también en las mujeres en el periodo prenatal.

En la práctica existen muchos métodos inmunodiagnósticos basados en la detección de complejos antígeno-anticuerpo que se utilizan en el diagnóstico de enfermedades infecciosas, por ejemplo ELISA, inmunofluorescencia, Western blot, inmunodifusión e inmunoelectroforesis.

La terapia de anticuerpos monoclonales se emplea en el tratamiento de enfermedades como la artritis reumatoide, esclerosis múltiple, psoriasis, y muchas formas de cáncer, incluyendo el linfoma no Hodgkin, cáncer colorrectal, cáncer de cabeza y cuello y cáncer de mama.
Algunas inmunodeficiencias, como la agammaglobulinemia ligada al cromosoma X y la hipogammaglobulinemia consisten en una carencia parcial o completa de anticuerpos. Estas enfermedades se tratan a veces induciendo una inmunidad a corto plazo llamada inmunidad pasiva. Ésta se adquiere a través de la infusión de anticuerpos "prefabricados" en forma de suero humano o animal, inmunoglobulina intravenosa o anticuerpos monoclonales en el individuo afectado.

Las llamadas "Rho (D) Inmunoglobulinas" o inmunoglobulilas anti-RhD son específicos del antígeno humano Rhesus D también conocido como factor Rhesus. De estos anticuerpos anti-RhD se conocen varias marcas comerciales, como RhoGAM, BayRHo-D, Gamulin Rh, HypRho-D, y WinRho SDF. El factor Rhesus es un antígeno que se encuentra en los eritrocitos. Los individuos Rhesus-positivo (Rh+) exhiben este anticuerpo en el glucocálix de sus eritrocitos, mientras que los individuos (Rh–) carecen de él.
Durante nacimiento normal, la sangre fetal puede pasar a la madre por traumas en el parto o complicaciones del embarazo. En el caso de incompatibilidad Rh entre la madre y el hijo, la consiguiente mezcla de sangre puede sensibilizar a una madre Rh- contra el antígeno Rh del hijo, haciendo que en los siguientes embarazos corran riesgo de eritroblastosis fetal. Los Anti-RhD se administran como parte del tratamiento prenatal para prevenir la sensibilización que pudiera tener lugar para evitarlo. Al tratar a la madre con anticuerpos anti-RhD antes e inmediatamente después del trauma y el parto destruye el antígeno Rh del feto en el cuerpo de la madre. Un tema importante es que esto sucede antes de que el antígeno pueda estimular los linfocitos B maternos que más tarde podrían "recordar" el antígeno Rh generando linfocitos B con memoria. Por tanto, su sistema humoral inmune no fabricará anticuerpos anti-Rh y no atacará los antígenos Rhesus de su bebé actual o futuro.

En investigación, los anticuerpos purificados se usan en muchas aplicaciones. Son muy habituales para identificar y localizar proteínas intra y extracelulares. Los anticuerpos se usan en la citometría de flujo para diferenciar los tipos celulares por las proteínas que expresan; los diferentes tipos celulares expresan también diferentes combinaciones de moléculas del cúmulo de diferenciación (CD) en su superficie y producen diferentes proteínas intracelulares, extracelulares y excretables. También se usan en inmunoprecipitación para separar las proteínas y cualquier cosa que esté unida a ellas (co-inmunoprecipitación) de otras moléculas en un lisado de células, en análisis Western blot para identificar proteínas separadas por electroforesis, y en inmunohistoquímica o inmunofluorescencia para examinar la expresión de proteínas en secciones de tejidos o localizar proteínas en el interior de las células con el auxilio de un microscopio. Las proteínas también se pueden detectar y cuantificar con anticuerpos, utilizando técnicas ELISA y ELISPOT.

En ocasiones se necesita producir anticuerpos específicos. Inyectando un antígeno en un mamífero, como ratón, rata o conejo si se requiere poca cantidad; Cabra, oveja o caballo si se requiere grandes cantidades. La sangre aislada de estos animales contiene "anticuerpos policlonales" —múltiples anticuerpos que se unen al mismo antígeno— en el suero sanguíneo, al cual se denomina antisuero. También se pueden inyectar antígenos en la yema de huevo de gallina para producirlos. Sin embargo, para aplicaciones analíticas es necesaria una mayor especificidad, sobre todo si se trata de detectar moléculas muy pequeñas, así como cuando se usan en aplicaciones terapéuticas en las que se desea bloquear o detectar marcadores muy específicos. Por ello la tecnología de los anticuerpos ha generado algunas variantes, entre las que se destacan:

Los anticuerpos mono y policlonales generados se pueden purificar utilizando proteína A/G o cromatografía de afinidad al antígeno.

Existen propuestas para la utilización terapéutica de anticuerpos monoclonales de camélido, también llamados nanoanticuerpos. Estos son excepcionales en el reino animal, dado su reducido tamaño, debido a que están compuestos únicamente por dos cadenas pesadas. Tales peculiaridades les permitirían acceder a localizaciones celulares y antígenos inaccesibles para los anticuerpos normales, además de ser posible su administración oral.
Para obtener antídotos contra venenos de picaduras por animales como serpientes o artrópodos, se fabrican antisueros mediante suero crudo o bien altamente enriquecido en inmunoglobulinas. Estos procedimientos producían un gran número de reacciones alérgicas, como anafilaxias o la enfermedad del suero. Para evitarlo, en los años 40 y 50 se realizaron estudios de proteólisis para reducir al mínimo la parte de la molécula implicada en la neutralización del veneno. Finalmente se encontró que el fragmento F (ab’)2, resultante de la digestión con pepsina de los anticuerpos, que carece de las zonas efectoras de la molécula, puede neutralizar igualmente venenos. El profesor Alejandro Alagón Cano propuso para este enfoque terapéutico el nombre de faboterapia, observándose una incidencia mucho menor de reacciones adversas al suero, así como un mejor alcance del compartimento extravascular.




Nota sobre licencia: Algunos de los contenidos del presente artículo han sido traducidos o modificados de , y de las wikipedias , y respectivamente, todas ellas bajo licencia .


</doc>
<doc id="9680" url="https://es.wikipedia.org/wiki?curid=9680" title="Música carnática">
Música carnática

La música carnática es la música clásica del sur de la India, en contraposición a la música clásica indostaní, que es la música clásica del norte de la India.
No está relacionada directamente con el estado indio de Karnataka.

Es diferente de la música indostaní en que enfatiza la estructura de la canción, es mucho más teorética y tiene más reglas estrictas. Igual que toda música india, los conceptos claves son el "raga" (escala musical), y el tāḷa (ritmo).

Las bases de la música carnática se escribieron posiblemente en la época del "Sama-vedá" (400 a. C.).
Purandara Dāsa (Kannada: ಪುರಂದರ ದಾಸ) (1484-1564) es un compositor destacado de la música carnática. Es ampliamente conocido como el Pitamaha (el padre o el abuelo) de Carnatic Música en honor a su significativa contribución a la música carnática.
Inicialmente, los instrumentos incluían un tipo de trompeta llamado un "náda-suaram" y una batería en forma de barril llamado un "tavil", que todavía se usan con frecuencia en las bodas y ceremonias hindúes.
En el pasado sobresalió el compositor Sri Naraiana Tirtha (1650-1745).
Los compositores más respetados de la música carnática fueron Tiagarala, Muttusuami Díxitar, y Shiama Shastri.

En contraste de la India del Norte, el Sur nunca fue conquistado por los mogules y, por lo tanto, su música representa formas más puras e indígenas. La música carnática quedó popular con la gente, y fue ejecutada como un ritual espiritual.

El contenido de las canciones de la música carnática es necesariamente religioso, específicamene hinduista, aunque no siempre alaba, ya que el compositor también puede quejarse o regañar a la deidad.

El tema de muchas composiciones «ligeras» hoy día es principalmente laico: la patria, la naturaleza, la comida, etcétera.

Música carnática permaneció relativamente poco afectada por influencias persas y árabes. Fue en este momento que la música carnática floreció en Vijayanagara, mientras el Imperio Vijayanagar alcanzó su mayor extensión. Purandara Dasa, quien es conocido como el padre (Pitamaha) de la música carnática, formuló el sistema que se utiliza comúnmente para la enseñanza de la música carnática . Venkatamakhin inventó y fue autor de la fórmula para el sistema de clasificación melakarta raga en su obra el sánscrito, el Chaturdandi Prakasika (1660 dC). Govindacharya es conocido por la expansión del sistema melakarta en el esquema raga Sampoorna - el sistema que es de uso común en la actualidad.
Música carnática fue patrocinada principalmente por los reyes locales del Reino de Mysore y Travancore en el 18 a través de 20 siglos. Algunas de la realeza de los reinos de Mysore y Travancore eran ellos mismos señalaron compositores y competentes en tocar instrumentos musicales, como la vina, vina rudra, violín, ghatam, flauta, mridangam, Nagaswara y swarabhat. Algunos famosos judiciales-músicos que dominan en la música fueron Veene Sheshanna (1852-1926) y Veene Subbanna (1861-1939), entre otros.
Con la disolución de los antiguos estados principescos y el movimiento independentista indio llegar a su conclusión en 1947, la música carnática pasó por un cambio radical en el clientelismo en un arte de las masas con actuaciones con boleto organizados por instituciones privadas llamadas Sabhas.

El "sárgam", es decir, el solfeo, de la música carnática es «sa-ri-ga-ma-pa-da-ni» (en contraste con el indostaní «sa-"re"-ga-ma-pa-"dja"-ni».) Las sílabas son abreviaciones de los nombres "sádllamam", "ríchabjam", "gándjáram", "mádjyamam", "pánchamam", "djáivatam" y "nichádam". Desigual a otros sistemas de música, cada elemento del solfeo (llamado una suara) puede tener hasta tres variaciones. Las excepciones son sádllama y pánchama, que tienen sólo una forma, y mádjyamam, que tiene dos formas. En una sola escala, llamado una raga, normalmente hay sólo una forma de cada suara, aunque en algunas ragas «ligeras», habrá dos, una para cuando se va hacia arriba (el arójanam), y una para cuando se va hacia abajo (el avarójanam), para el efecto artístico. A veces cuando uno canta en una raga, se cantará una nota que no esté ni en el arójanam ni en el avarójanam, pero que suena como si debiera incluirse en la raga por la emoción que la raga evoca. Ese tipo de suara se llama una aña suara. Una raga puede tener cinco, seis ó siete notas yendo hacia arriba, y cinco, seis ó siete notas yendo hacia abajo.

Hay bastante variación en el estilo y la estructura de las composiciones carnáticas, pero la mayoría siguen el formato siguiente de tres estrofas:

Este tipo de canción se llama un "kírtana" (कीर्तनं). Pero lo pasado describe sólo una forma para los "kírtanas". Es posible que entre la "anupálavi" y la "cháranam" haya otra estrofa, que no tiene palabras sino solo solfeo, llamada la "chítasuara" (चिट्टस्वरं).

Otra clase de composición carnática es el "varna", que contiene, en efecto, toda la información necesitada para cantar en un "raga": no sólo la escala, sino también cuáles "suaras" hay que enfatizar, cómo acercarse a una nota, frases clásicas y características, etcétera. Es como leer la "Biblia" en una lengua extranjera.

El cantante contemporáneo Dr. K. J. Yesudas es un embajador cultural de la música carnática.




</doc>
<doc id="9682" url="https://es.wikipedia.org/wiki?curid=9682" title="Himno Nacional de Guatemala">
Himno Nacional de Guatemala

La letra del Himno Nacional de Guatemala es original del poeta cubano José Joaquín Palma, y la música fue compuesta por el maestro compositor Rafael Álvarez Ovalle en 1897, con motivo de la realización de la Exposición Centroamericana del gobierno del general José María Reina Barrios. 

La letra y la partitura fueron impresas por primera vez en la revista cultura "La Ilustración Guatemalteca", en donde apareció el autor de la letra como Anónimo. No fue sino hasta en 1910, cuando ya estaba en las postrimerías de su vida, que el poeta y diplomático cubano José Joaquín Palma confesó que era él el autor; el gobierno del licenciado Manuel Estrada Cabrera lo premió tanto a él como a Rafael Álvarez Ovalle con coronas de laurel de oro en las Fiestas Minervalias de ese año. 

Por orden del presidente general Jorge Ubico, en 1934, algunos cambios fueron hechos por el pedagogo José María Bonilla Ruano a la letra del himno ya que la misma era muy guerrerista y reflejaba más a la guerra de independencia de Cuba -en la que Palma había participado activamente- que a la Independencia de Centroamérica.

En 1879, la Sociedad Literaria «El Porvenir», intentó infructuosamente crear un Himno Nacional para Guatemala. Hasta en 1889, el presidente de Guatemala, general Manuel Lisandro Barillas Bercián, convocó a un concurso para elegir la música que complementaría la letra del «Himno Nacional» escrita por el poeta Ramón P. Molina. En este certamen tomaron parte distinguidos compositores, y el triunfo fue adjudicado a la música presentada por Rafael Álvarez Ovalle.

En 1896, el gobierno del general José María Reina Barrios convocó a un nuevo concurso, «Considerando que se carece en Guatemala de un Himno Nacional, pues el que hasta hoy se conoce con ese nombre, no sólo adolece de notables defectos, sino que no ha sido declarado oficialmente como tal; y que es conveniente dotar al país de un himno que por su letra y música responda a los elevados fines en que todo pueblo culto presta esta clase de composiciones». De este nuevo concurso salió nuevamente premiada la obra de Rafael Álvarez Ovalle, esta vez musicalizando un poema amparado con el seudónimo de «Anónimo».

El triunfo concedido nuevamente al maestro Álvarez Ovalle le costó los momentos más amargos de su existencia, pues hubo descontento entre los que no ganaron, quienes incluso hicieron llegar su queja hasta el primer mandatario. Reina Barrios, en presencia de los miembros de su gabinete, otras personalidades y maestros de arte musical, volvieron a escuchar todas las composiciones que compitieron en el concurso, habiendo salido electa nuevamente por unanimidad, la del maestro Rafael Álvarez Ovalle.

Con respecto a la letra, el jurado calificador determinó lo siguiente:

Como puede verse, el poeta cubano José Joaquín Palma era miembro del jurado calificador.

El estreno del Himno Nacional tuvo lugar en el acto lírico literario celebrado en el Teatro Colón (Guatemala) la noche del domingo 14 de marzo de 1897, como uno de los principales puntos del programa de festejos de la Exposición Centroamericana, habiendo sido condecorado con medalla de oro y diploma de honor el maestro Rafael Álvarez Ovalle.

El autor de la letra del Himno Nacional de Guatemala permaneció en el más profundo misterio hasta 1911, en que se descubrió que su autor era el poeta cubano José Joaquín Palma, pues éste reveló en su lecho de muerte que él era el autor de la letra del himno.

El audio se genera automáticamente de la partitura, y sirve como pista de acompañamiento de piano para el canto.

Partitura sonora del HIMNO NACIONAL DE GUATEMALA

 
La letra original del Himno de Guatemala que escribió el poeta cubano José Joaquín Palma era guerrerista ya que Palma se inspiró más en la situación política que atravesaba su Cuba natal, que en la que vivió Guatemala durante su independencia: mientras que Centro América de separó del Imperio Español pacíficamente, Cuba libraba en la época en que Palma escribió el Himno una feroz guerra contra España. Por no reflejar la realidad guatemalteca, la letra fue modificada en 1934, según Decreto Gubernativo del gobierno del general Jorge Ubico Castañeda del 26 de julio de 1934.

He aquí la letra, tal y como la escribió José Joaquín Palma:

La letra del Himno Nacional fue modificada por el poeta y pedagogo guatemalteco José María Bonilla Ruano, por instrucciones del gobierno del general Jorge Ubico.

Tras las modificaciones realizadas por Bonilla Ruano en 1934, el himno nacional se canta con esta letra:

El Himno Nacional de Guatemala ha sido considerado por Carlos Labin, miembro de la Sociedad Americanista de París y de la Sociedad de Musicología de Francia como el «más original» de todos los himnos del continente americano.





</doc>
<doc id="9687" url="https://es.wikipedia.org/wiki?curid=9687" title="Política de Venezuela">
Política de Venezuela

La actual Constitución de Venezuela, aprobada en referéndum constitucional el 15 de diciembre de 1999 y promulgada cinco días después, establece que la República Bolivariana de Venezuela se constituye en un Estado Social y Democrático de Derecho y de Justicia que "propugna como valores superiores de su ordenamiento jurídico y de su actuación, la vida, la libertad, la justicia, la igualdad, la solidaridad, la democracia, la responsabilidad social y, en general, la preeminencia de los derechos humanos, la ética y el pluralismo político"..

En los términos establecidos en la Constitución de la República, Venezuela asume la forma de un Estado federal descentralizado, y se rige por los principios de integridad territorial, cooperación, solidaridad, concurrencia y corresponsabilidad. El mismo tiene como fines la protección y fomento de la persona y su humanidad, garantizar el ejercicio democrático de la voluntad popular, y la búsqueda de un estado de bienestar general. Para la consecución de tales metas, se señalan como vías el desarrollo de la educación y el trabajo.

Se establece además que la forma de Gobierno es la de una República presidencialista, encabezada por el Presidente de la República, con funciones de Jefe del Estado y Jefe del Poder Ejecutivo Nacional a la vez. La soberanía, la cual reside en el pueblo, se ejerce de dos maneras: directamente a través de la Constitución misma y de la ley, e indirectamente, mediante el sufragio, por el Poder Público, cuyos componentes están sometidos a dicha soberanía popular y se deben a ella. Todos los entes públicos están sujetos a lo previsto en esta Constitución. El Presidente tiene la facultad de dirigir las acciones del Gobierno.

El territorio nacional se divide en Estados, un Distrito Capital, dependencias federales y territorios federales. Los estados se organizan en Municipios. De la misma forma, el Poder Público se distribuye entre el Poder Nacional, el Poder Estatal y el Poder Municipal. Tanto el Gobierno nacional como el de las subdivisiones territoriales deben ser de naturaleza democrática, participativa, electiva, descentralizada, alternativa, responsable, pluralista y de mandatos revocables.

El Estado Venezolano se divide en 5 poderes, el poder Legislativo, Ejecutivo, Judicial, Ciudadano y Electoral; cada una de las ramas del Poder Público tiene sus funciones propias, pero los órganos a los que incumbe su ejercicio colaborarán entre sí en la realización de los fines del Estado. El Poder Público Nacional está conformado por los órganos y entes del Estado con competencia nacional que se enmarcan dentro de la Constitución de la República.

Las autoridades nacionales del Estado residen en Caracas, Distrito Capital, ya que según la Constitución Nacional, es el asiento de los órganos del Poder Público Nacional. La Administración Pública está al servicio de los ciudadanos y se fundamenta en los principios de honestidad, participación, celeridad, eficacia, eficiencia, transparencia, rendición de cuentas y responsabilidad, según lo exige el Artículo 141 de la Constitución de la República.

El gobierno nacional se ejerce por órgano del Presidente de la República, del Vicepresidente Ejecutivo, de los Ministros y de los demás funcionarios que establece la Constitución de la República y la ley. El Presidente de la República es elegido por sufragio directo, secreto y universal para un mandato de 6 años, teniendo la posibilidad de ser reelegido para nuevos períodos, de acuerdo a la Enmienda N.º1 de la Constitución Nacional. 

El Presidente de la República es el Jefe del Estado, Jefe del Poder Ejecutivo Nacional, Comandante en Jefe de la Fuerza Armada Nacional Bolivariana, y dirige las relaciones exteriores de la República. El Vicepresidente Ejecutivo es colaborador directo e inmediato del Presidente. Coordina las relaciones del Ejecutivo Nacional con la Asamblea Nacional, preside el Consejo Federal de Gobierno y suple las faltas temporales del Presidente de la República. Los Ministros son órganos directos del Presidente, y reunidos con éste y con el Vicepresidente, integran el Consejo de Ministros. El Procurador General de la República asiste, con derecho a voz, a las reuniones del Consejo de Ministros. Adicionalmente, el Presidente puede convocar al Consejo de Estado, siendo un órgano superior de consulta del Gobierno y de la Administración Pública Nacional para recomendar políticas de interés nacional en asuntos de especial trascendencia.

Es ejercido por la Asamblea Nacional de Venezuela a nivel federal, por los Consejos Legislativos a nivel estadal y por los Consejos Municipales y Cabildos Distritales en las entidades locales.

El órgano jurisdiccional superior en todos los órdenes es el Tribunal Supremo de Justicia y los demás tribunales que determine la ley. Éstos, conjuntamente con el Ministerio Público, la Defensa Pública, los órganos de investigación penal, los auxiliares y funcionarios de justicia, el sistema penitenciario, los medios alternativos de justicia, los ciudadanos que participan en la administración de justicia conforme a la ley y los abogados autorizados para el ejercicio, integran el Sistema de Justicia.

La máxima instancia del poder ciudadano se ejerce por órgano del Consejo Moral Republicano, conformado por el Ministerio Público, la Contraloría General de la República, y la Defensoría del Pueblo. Cualquiera de las máximas autoridades de los órganos que integran este Poder puede ser elegido como Presidente del Consejo Moral Republicano por períodos de un año, reelegibles. Entre sus funciones están prevenir, investigar y sancionar hechos que atenten contra la ética pública y la moral administrativa, velar por el buen uso del patrimonio público y preseleccionar a los candidatos a . El Ministerio Público es el encargado de garantizar en los procesos judiciales el respeto a los derechos y garantías constitucionales. Está bajo la dirección y responsabilidad del Fiscal General de la República, designado por la Asamblea Nacional para un período de siete años. La Contraloría General de la República es el órgano de control, vigilancia y fiscalización de los ingresos, gastos, bienes públicos y bienes nacionales, así como de las operaciones relativas a los mismos. Está bajo la dirección y responsabilidad del Contralor General de la República, designado por la Asamblea Nacional para un período de siete años. La Defensoría del Pueblo tiene a su cargo la promoción, defensa y vigilancia de los derechos y garantías establecidos en la Constitución de la República y en los tratados internacionales sobre derechos humanos, además de los intereses legítimos, colectivos o difusos de los ciudadanos. Está bajo la dirección y responsabilidad del Defensor del Pueblo, quien es designado por un único período de siete años.

El Poder Electoral se ejerce por órgano del Consejo Nacional Electoral, el cual tiene como órganos subordinados: la Junta Electoral Nacional, la Comisión de Registro Civil y Electoral y la Comisión de Participación Política y Financiamiento. Su objetivo es reglamentar y gestionar los procesos electorales así como la aplicación de la personalización del sufragio y la representación proporcional. El CNE mantiene, organiza, dirige y supervisa el Registro Civil y Electoral. La Constitución de 1999 incorpora la figura del referéndum revocatorio para todos los cargos de elección popular, los cuales pueden ser sometidos a nueva elección a la mitad del mandato, como una forma novedosa de permitir una decisión política de la ciudadanía sobre los funcionarios electos.

A nivel regional el Poder Electoral es ejercido por las juntas regionales y locales.

El sufragio en Venezuela es Universal, y se puede ejercer a partir de los 18 años.

La organización que se puede llamar propiamente el primer partido político venezolano es el Partido Liberal, creado por Tomás Lander y Antonio Leocadio Guzmán en 1840. Surgido como una respuesta al gobernante Partido Conservador, ambos se disputarían el poder a lo largo del siglo XIX. A partir de éstos surgirían partidos derivados, disueltos luego por la dictadura gomecista. En la posterior etapa democrática han surgido algunos de los partidos más importantes en la escena nacional, como Unión Republicana Democrática (URD, f. 1945), el Movimiento Electoral del Pueblo (MEP, f. 1967), La Causa R (f. 1971), ente otros.

En la actual vida política del país, los que cobran especial relevancia son el Partido Socialista Unido de Venezuela (PSUV), que es el partido de gobierno; Un Nuevo Tiempo (UNT) conformado como partido nacional en 2007 y en la actualidad es el primer partido de la coalición opositora; Primero Justicia (PJ), nacional desde 2003, Voluntad Popular (VP) por iniciativa de Leopoldo López en 2009; Acción Democrática (AD), de tendencia socialdemócrata, fundado en 1941 por Rómulo Gallegos y Rómulo Betancourt; el Comité de Organización Política Electoral Independiente (COPEI), de corte socialcristiano, fundado en 1946 por Rafael Caldera; el Movimiento al Socialismo (MAS), también socialdemócrata, creado en 1971; Patria Para Todos (PPT), en 1997; Por la Democracia Social (Podemos), en 2003 por Ismael García, y el Partido Comunista de Venezuela (PCV), fundado en 1931 y legalizado en 1945.

Algunos partidos de importancia a nivel regional son Proyecto Carabobo (fundado en 1997 con Proyecto Venezuela), el neoespartano Movimiento Regional de Avanzada (MRA, f. 2002), y el Movimiento Unido de Afrodescendientes Indígenas (f. 1997), entre otros.

Una particularidad del sistema partidista venezolano es la dispersión de las agrupaciones políticas que pasan desde la derecha hasta la izquierda, las escisiones son el modelo predominante, los casos más significativos han sido los que se presentan en el siguiente cuadro.

La política exterior venezolana ha variado de acuerdo a la naturaleza de su gobierno. Dado que en sus primeros años como nación independiente el país padeció un largo período de turbulencia interna, no pudo delinear una política internacional concreta, pero se enfocaba en la demarcación de límites. A principios del siglo XX, se tuvieron relaciones difíciles con las potencias europeas y con los Estados Unidos por la deuda extranjera, y se mantuvo neutral durante la Segunda Guerra Mundial hasta que tomó partido por los Aliados. En los años 1950, Venezuela mantenía lazos estrechos con dictaduras existentes para la época en Iberoamérica, y con Estados Unidos. El restablecimiento del sistema democrático de gobierno en 1958 genera cambios significativos en la política exterior de Venezuela, estando enmarcada dentro de la Constitución de 1961 y concretándose en tres lineamientos básicos: democracia, petróleo y presencia internacional activa. Bajo la Doctrina Betancourt, solo reconocía a los gobiernos democráticos. En los años 1980 integró junto con otros países el Grupo Contadora, para buscar la paz en los conflictos armados de América Central.

Según el Artículo 153 de la Constitución, Venezuela se propone a favorecer la integración latinoamericana y caribeña, privilegiando relaciones con Iberoamérica. En los últimos tiempos, el Gobierno venezolano se ha acercado a gobiernos de clara línea izquierdista y antiimperialista, a la vez que se han suscitado percances y distanciamientos en las relaciones diplomáticas con Colombia, México y Estados Unidos, aunque sin afectar significativamente las relaciones comerciales que prevalecen. Venezuela ha ocupado un sitial en el Consejo de Seguridad de la ONU en cuatro ocasiones, en los períodos de 1962 a 1963, de 1977 a 1978, de 1986 a 1987, y de 1992 a 1993. En 2006 se postuló una vez más sin resultar electa.

Venezuela posee un largo historial de reivindicaciones territoriales con Guyana y con Colombia. Los límites orientales del país con Guyana, trazados por el Laudo de París de 1899 (declarado nulo e írrito por Venezuela), van desde el monte Roraima hasta Punta Playa en el océano Atlántico. Sin embargo, Venezuela reclama el territorio denominado como Guayana Esequiba, que abarcaría desde la frontera entre ambos países hasta el río Esequibo, lo que hoy son las regiones 1 (Barima-Waini), 2 (Cuyuni-Mazaruni), 7 (Pomeroon-Supenaam), 8 (Potaro-Siparuni), 10 (Alto Takutu-Alto Essequibo) y la zona occidental de la 5 (Islas Essequibo-Demerara Occidental), apoyándose en el Acuerdo de Ginebra de 1966, suscrito con el Reino Unido.

Asimismo, mantiene un diferendo con Colombia sobre la soberanía del golfo de Venezuela. La disputa, que se remonta al momento de disolución de la Gran Colombia, se cree que tiene como móvil la presencia de hidrocarburos en el golfo, y que a su vez motivó el estallido de la Crisis de la Corbeta Caldas en 1987. La problemática fue abordada nuevamente en 2007, cuando se acordó continuar las negociaciones entre ambas partes.

Venezuela ha hecho aportes con personal militar, policial u observador para las misiones de paz de las Naciones Unidas operativas en América Central (ONUCA), Iraq y Kuwait (UNIKOM), El Salvador (ONUSAL), el Sahara Occidental (MINURSO), India y Pakistán (UNIPOM), Croacia-Bosnia y Herzegovina (UNPROFOR), y Guatemala (MINUGUA).

Venezuela pertenece a ABINIA, ACNUR, AEC, ALADI, ALBA, ASALE, BDC, BID, BIRD, CAF, CARICOM, CCI, CD, CELAC, CEPAL, CIN, CFI, CLAD, CNUCYD, COI, CPA, CPI, CPLP, Cruz Roja, CSI, FAO, FIDA, FIFA, FLAR, FMI, FSM, G-15, G-24, G-77, Grupo de Río, IFRC, Intelsat, Interpol, ISO, , Liga Árabe, Mercosur, MPNA, OACI, OEA, OEI, OHI, OIEA, OIM, OISS, OIT, , OLADE, OMA, OMC, OMGI, OMI, OMM, OMT, OMS, OMPI, ONU, ONUDI, , OPAQ, OPEP, OTCA, OTI, Petrocaribe, SELA, Unasur, Unesco, UIP, UIT, Unión Latina y UPU.




</doc>
<doc id="9688" url="https://es.wikipedia.org/wiki?curid=9688" title="Sabadell">
Sabadell

Sabadell (en español pronunciado como [saβaˈðel], en catalán [səβəˈðeʎ]) es una ciudad y municipio español de la provincia de Barcelona. Actualmente es cocapital de la comarca del Vallés Occidental, junto con Tarrasa. El municipio, quinto de Cataluña, después de Barcelona, Hospitalet de Llobregat, Tarrasa y Badalona, cuenta con 209 931 habitantes (INE 2017), denominados "sabadellenses (en catalán: sabadellencs/ sabadellenques)."

La ciudad fue pionera en la Revolución Industrial en Cataluña dentro del sector textil y a mitad del siglo XIX se convirtió en la ciudad lanera más importante de España, siendo conocida por el nombre de «la Manchester catalana» en la segunda mitad del siglo XIX. Aún hoy podemos observar numerosas chimeneas y vapores, muchos de ellos reconvertidos en lugares de servicios sociales tales como bibliotecas o Áreas de Juventud. Esta herencia textil ha dejado en la ciudad un marcado carácter industrial. A lo largo de las últimas décadas, Sabadell se ha ido diversificando alrededor del sector de los servicios.

Son diversas las teorías sobre el origen del topónimo de Sabadell siendo más populares las dos primeras:


El escudo heráldico que representa a la ciudad fue aprobado oficialmente en 1993 con el siguiente blasón:
La cebolla es la señal parlante tradicional, desde 1560, referente al nombre de la ciudad, y es también una alusión al origen humilde de la población, con su mercado agrícola (aunque hoy en día Sabadell sea un importante centro industrial y de servicios). Los cuatro palos de Aragón recuerdan que la ciudad perteneció a la Corona de Aragón; de hecho, fue vendida por Roger Bernardo III de Foix en 1366 a Elionor de Sicília, esposa de Pedro IV de Aragón. El rey la volvió a vender, esta vez a la ciudad de Barcelona, y volvió a la Corona definitivamente en 1473.

La bandera de Sabadell está formada por dos franjas, una blanca sobre una verde (travesada por otra franja amarilla). La bandera se diseñó por encargo en 1928.

Sabadell está situada en el centro de la comarca del Vallés Occidental, junto al río Ripoll. Limita al norte con el municipio de Castellar del Vallés y Senmanat, al oeste con Tarrasa y San Quirico de Tarrasa, al este con Polinyá y Santa Perpetua de Moguda, y al sur con Barberá del Vallés, Badia del Vallès y Sardañola del Vallés.

Los primeros habitantes de la comarca del Vallès eran agricultores y ganaderos que se establecieron en la zona hace 7000 años. De hecho en la zona de Can Roqueta se han descubierto restos que hacen de Sabadell el asentamiento más importante de campesinos y pastores de hace entre 3800 y 2700 años.

Con la romanización desaparece de la zona la cultura ibérica, que es sustituida por la romana en sólo 150 años. En el Vallès, a partir del siglo I aC, empiezan a aparecer los primeros asentamientos romanos, algunos sobre antiguos establecimientos ibéricos ya existentes (como el de La Salut).

En el siglo I, con Augusto como emperador, fue un momento de prosperidad económica y momento también en el que se fundaron unos conjuntos industriales, residenciales y de cultivo, la mayoría dedicados a la producción de vino. De entre las villas del Vallès destaca la de La Salut, aunque cerca también se han localizado las de Castellarnau y Can Feu. En los famosos exvotos de Vicarello hay la referencia a una villa o población con el nombre Arragonem. Tradicionalmente, se ha asociado el nacimiento de la ciudad de Sabadell con la mansión Arragonem (Arraona), a pesar de que ninguna evidencia arqueológica lo demuestre, la toponímia del sitio y la tradición historiográfica sitúan este fundus romano en el actual Parque de La Salut. Arragonem era un lugar de parada para el avituallamiento de las caballerías y de los viajeros que transitaban por la Vía Augusta. El mausoleo de Augusto data del siglo I y hoy apenas quedan restos. Estaba formado por un anillo circular coronado por un túmulo de tierra con cipreses y coronado por una escultura del emperador. En su interior había tres cámaras: una para el cuerpo de Augusto, otra para el de su esposa, Livia, y otro para su familia. Ya en la edad media fue reutilizado como fortaleza y en el siglo XIX como circo y teatro. Es posible que sobre el túmulo hubiese un templete circular coronado por una estatua.

Durante la Edad Media, el núcleo de población se produjo en el margen derecho del río Ripoll, con unas primeras edificaciones construidas al lado de la capilla de San Salvador (actual iglesia de San Félix), documentada desde 1076. Al lado del río también se construyeron los primero molinos de harina.

Las primeras referencias de un mercado cercano a Sant Salvador datan de 1069. La primera ocasión en que aparece el nombre de Sabadell es en 1050, cuando se comenta una vía entre Sabadell y San Cugat del Vallés.

Sabadell pasa a ser "villa real", consiguiendo privilegios que revitalizan la vida social y económica. En 1369, en Sabadell había 162 fuegos. Durante esta época se contaban las casas por fuegos. La ciudad estaba rodeada de murallas, fosas y portales.

Sabadell padece un descenso demográfico, pasando de 800 habitantes a unos 500, que se concentraban en el actual centro histórico, un importante cruce de caminos. En este momento la frontera con el término de Tarrasa llegaba hasta la calle de Les Valls. Además de las actividades agrícolas y comerciales, en Sabadell se desarrollaba también la industria. Al lado del río Ripoll se construyen los primeros molinos de traperos.

Entre los siglos XVI y XVIII, Sabadell comienza a crecer más allá de las murallas. De una superficie de 37 900 m² en el siglo XVI, llega a los 78 272 m² en el siglo XVIII.

La industria textil más importante durante los siglos XVI, XVII y XVIII fue la lanera, seguida (a mucha distancia) por la del tisaje de lino. En 1559 se creó el Gremi de Paraires (más tarde llamado Gremi de Fabricants) para establecer las reglas del oficio y favorecer el crecimiento de la actividad textil. A lo largo del siglo XVIII Sabadell tenía otras actividades industriales importantes, como la alfarería y la papelería.

Durante el siglo XIX, pasa a convertirse en ciudad industrial, destacando la instalación de la primera máquina de vapor en una fábrica textil (1838) y la fundación de la "Sociedad de Amantes de la Agricultura y la Industria de la Ciudad de Sabadell", que se preocupaba del abastecimiento de agua para la industria y la población, tema siempre grave para Sabadell.

De los 2000 habitantes que había a principios de siglo se pasa a los 23 294 (censo de 1900). El término municipal crece y se amplía desde la calle de Les Valls hasta la actual ronda Zamenhof y la calle Vilarrubias.

En cuanto a las infraestructuras, durante la segunda mitad del siglo llegó a la ciudad la línea de tren que la conectaba con Barcelona y Tarrasa, se instaló el alumbrado público en las calles del centro (primero de gas y más tarde eléctrico), y se hicieron las primeras cloacas.

Dos instituciones financieras nacen durante la segunda mitad del siglo: la Caixa d'Estalvis de Sabadell (1859) y el Banco Sabadell (1881), así como la Cámara de Comercio e Industria (1886), actualmente la entidad económico-empresarial más representativa de la ciudad y de su área de influencia.

En el ámbito cultural, es de destacar la renovación del Teatro Principal (1866) y la fundación de la Academia de Bellas Artes (1880).

En 1877, mediante un real decreto del rey Alfonso XII, la villa de Sabadell obtiene el título de ciudad. En aquel momento contaba con unos 18.000 habitantes, un número que se irá incrementando con la inmigración procedente del resto de Cataluña, de Alicante, Murcia y Valencia. La actividad económica se desarrolla básicamente entorno de la industria textil. La especialización en la producción de tejidos de lana transforman Sabadell en el primer centro textil lanero del Estado.

A principios del siglo XX fue la ciudad de la industria textil por excelencia, constituyendo el motor industrial en un territorio pobre por naturaleza. La población se multiplicó por ocho y la ciudad experimento un gran impulso industrial, sobre todo en el textil y la metalurgia también se modernizo su economía con los servicios. La gran actividad industrial provoca una avalancha migratoria durante las décadas de los 50, 60 y comienzo de los 70, provocando una expansión urbana sin orden ni concierto de la que nacen nuevos barrios como el de Ca n'Oriac y el de Torre-Romeu.

Edificios emblemáticos como los modernistas del Hotel Suís (1902), del Despatx Lluch (1908), de la Caixa d'Estalvis de Sabadell (1915), de la Torre de l'Aigua (1918) y el Mercat Central (1930), se construyeron durante la primera mitad del siglo.

En 1943 la Diputación provincial elige por mayoría a su alcalde José María Marcet Coll para el cargo de procurador en Cortes en la I Legislatura de las Cortes Españolas (1943-1946), representando a los Municipios de esta provincia.

El final de la dictadura franquista es un período de gran turbulencia en Sabadell. Las entidades, los movimientos vecinales y los grupos políticos y sindicales se implican a favor de la democracia (PSUC, CCOO, UGT, USO, etc.). La crisis económica de los 70 afecta a numerosas empresas de la ciudad, que terminan cerrando. Tras las elecciones municipales de 1979 se plantea un nuevo modelo de ciudad.

Entre 1980 y 1999 el Ayuntamiento, con su carismático alcalde Antoni Farrés al frente, tuvo como gran reto urbanizar la ciudad y crear equipamientos públicos. El crecimiento descontrolador de los últimos años había generado barrios que no contaban con las infraestructuras urbanas necesarias (alumbrado, cloacas, pavimentación de las calles, etc.).

La apariencia del municipio ha evolucionado desde las prestigiosas chimeneas de fábricas como Vapor Llong, hasta edificios ultramodernos, como la Torre Millenium.

En 1992 se inauguran dos nuevos parques públicos: el Parc Catalunya, de un kilómetro de longitud por 500 m de ancho y el Parque de Taulí.

La terciarización de la economía hace que las principales actividades del sector servicios aumenten en número de trabajadores. Las tres principales actividades son el comercio al detalle, instituciones financieras y comercio al por mayor. El Eix Macià (vía urbana que une la Plaza de Catalunya con la Plaza de la Concòrdia) se ha convertido en un centro de servicios que agrupa la oferta comercial y de negocios. El cambio urbanístico del centro histórico de la ciudad ha propiciado su reactivación económica en cierto sentido, permitiendo el paso del antiguo centro industrial y obrero a esta nueva economía del sector terciario.

La ciudad incorpora al barrio de nueva creación: Can Llong, e inicia la recuperación del entorno del río Ripoll con la creación de un parque fluvial.

Sabadell fue en los siglos XIX y XX una ciudad dedicada a la producción textil. Una de las familias más importantes en la historia de Sabadell ha sido la familia Turull y muchas más familias que se han dedicado al textil.

Sabadell se propone asumir los retos del futuro con una serie de proyectos que hacen referencia a las nuevas tecnologías de la información y la comunicación. Gran Vía Digital, pretende favorecer la instalación de empresas vinculadas a las nuevas tecnologías; el Parque de Salut será un macrocentro de formación e investigación en temas de medicina y farmacia; la "Ciudad de La Música" ofrecerá a la ciudad nuevos equipamientos, como un espacio para acoger congresos, un nuevo teatro, una nueva escuela de música y un nuevo hotel.

Sabadell es una ciudad sin rondas de circunvalación, afectando ello a la movilidad, ya que gran parte del tráfico ha de pasar por vías internas. Es posible que la construcción de las proyectadas Ronda Oest y Ronda del Ripoll puedan paliar esta situación, así como la mejora de la deficitaria red de ferrocarriles que podría ser utilizada para equilibrar el problema del tráfico.

La evolución de la ciudad mejora por momentos desde el siglo XX. Se ha conocido a personajes ilustres y construido edificios con sus nombres, como el Doctor Crusafont, nombre que da a un museo y laboratorio paleontológico. Es en este museo donde se analizó los restos de la nueva especie de primate conocida con el nombre de "Pau".

En el municipio —que tiene una superficie de 37,53 km²— cuenta según el padrón municipal para del INE con habitantes y una densidad de  hab./km².


Fuente: Idescat 2017

La ciudad está dividida en siete distritos. Cada distrito cuenta con uno o más sectores, que a su vez están formados por uno o más barrios.


El actual alcalde, Maties Serracant Camps, pertenece a la CUP.
Los resultados en las elecciones municipales de 2015 fueron los siguientes:


</doc>
<doc id="9690" url="https://es.wikipedia.org/wiki?curid=9690" title="King Kong (película de 1933)">
King Kong (película de 1933)

King Kong es una película estadounidense de aventuras de 1933 dirigida por Merian C. Cooper y Ernest B. Schoedsack y con Fay Wray, Robert Armstrong y Bruce Cabot como actores principales. La película fue producida por la compañía cinematográfica RKO Pictures y escrita por Ruth Rose y James Ashmore Creelman, basándose en una idea de Merian C. Cooper y Edgar Wallace. 

Trata sobre el hallazgo de Kong, un gorila gigante, en una isla prehistórica perdida y sobre cómo fue capturado y llevado a la civilización contra su voluntad. 

En 1932, un año antes del estreno de la película, Delos W. Lovelace publicó una novelización del guion de "King Kong", con algunas escenas que no están presentes en la película. "King Kong" fue estrenada por primera vez en Nueva York el 7 de marzo de 1933, en el teatro Radio City Music Hall.

El director de cine Carl Denham busca una chica para su nueva película, pero no aparece ninguna de su gusto. Cuando decide buscarla personalmente encuentra a Ann Darrow, una actriz de teatro desempleada por culpa de la crisis de 1929, y la convence para que vaya con él en un barco. Poco después, zarpan a bordo del "Venture", en el que navegan durante varias semanas en dirección a Indonesia, donde Denham quería realizar la película.

Finalmente, cuando llegan al lugar elegido para el rodaje, Denham revela su propósito a Englehorn, capitán del "Venture": el destino del viaje es una isla que no aparece en los mapas, la isla Calavera, donde se encuentra un misterioso ser llamado Kong, al cual quiere filmar.

Al llegar a la isla, descubren una aldea nativa y se dan cuenta de que hay una muralla que separa el poblado de la mayor parte de la isla. Aunque Denham, Englehorn y Ann se esconden entre el follaje, el jefe de la tribu los descubre. El capitán, que entiende la lengua de los aborígenes, trata que Denham establezca amistad con el jefe y, cuando este ve a Ann, propone cambiarla por seis mujeres de la tribu. El director rechaza la propuesta y los aborígenes deciden ir al barco por la noche para secuestrar a la chica; cuando los miembros de la tripulación se dan cuenta de ello, van en su búsqueda.

En la aldea, los nativos hacen un ritual para llamar a su ídolo Kong y entregarle a Ann como sacrificio; la tripulación llega a tiempo para impedirlo y hace huir a los aborígenes disparándoles, pero Kong consigue llevarse a la chica y tienen que perseguirles por la selva prehistórica que se encuentra tras la muralla. La tripulación, al seguir el rastro de Kong, es atacada y perseguida por un dinosaurio saurópodo y tienen que cruzar un acantilado a través de un puente. Kong se da cuenta de la persecución y derriba el puente, lo que hace que solo sobrevivan dos miembros de la tripulación: John Driscoll, que se había ocultado en una cueva entre las paredes del acantilado, y Carl Denham, que permanecía en la aldea.

Kong comienza a desarrollar una extraña atracción por Ann y tiene que luchar contra un tiranosaurio que la ataca. Entonces, el gorila y la chica se dirigen a una cueva a descansar, y Driscoll los sigue hasta allí. Accidentalmente, Driscoll derriba una roca y llama la atención de Kong, pero en ese momento, Ann es atacada por un pterosaurio y el gorila se enfrenta con él. Mientras está distraído, John aprovecha para bajar junto con la chica por una liana, pero al finalizar la pelea con el pterosaurio, Kong empieza a tirar de ella sin dejarles más opción que lanzarse a un río. Furioso, el gorila los persigue hasta la aldea y, al llegar a la costa, queda inconsciente por una granada que es lanzada por Denham.

Inmediatamente, deciden transportar a Kong a Nueva York, para ser exhibido públicamente en la carpa de un teatro. El contacto de Kong con un mundo que no conoce y el amor que siente por Ann lo hacen enfurecer hasta que se libera y queda suelto por la ciudad. Kong busca a la chica y al encontrarla, la sube al Empire State Building, donde es atacado por aviones, que logran hacerle caer del edificio y muere.


La película "King Kong" bebe de las historias literarias de aventuras sobre mundos perdidos, y en especial de la novela de Sir Arthur Conan Doyle "El mundo perdido" (1912) y de la de Edgar Rice Burroughs "La tierra olvidada por el tiempo" ("The Land That Time Forgot", 1918); en ambas, buena parte de la acción transcurre en selvas plagadas de animales prehistóricos. En 1925 hubo una adaptación al cine de la novela "El mundo perdido", con efectos especiales hechos por Willis O'Brien y su equipo, quienes después trabajarían en "King Kong".
El productor de "King Kong", Ernest E. Schoedsack tenía experiencia trabajando con monos, ya que antes había dirigido los documentales "Chang" en 1927 (con Cooper) y "Rango" en 1931. Aprovechándose de esa tendencia, la compañía Congo Pictures realizó la película "Ingagi" en 1930, que anunció como «un documental que mostraba el sacrificio de una mujer a un gorila gigante».

Willis O'Brien había hecho los efectos especiales para el largometraje inacabado de 1931 "Creation", dirigido por él mismo. Los modelos de los dinosaurios de esa película estaban inspirados en las ilustraciones realizadas por el artista estadounidense Charles R. Knight. Cooper aprobó las secuencias realizadas por O'Brien con la técnica de animación de paso de manivela en "Creation", y por eso decidió contratarlo para que se encargara de los efectos especiales en "King Kong". Los modelos de dinosaurios usados en "King Kong" fueron construidos en un principio para la filmación de "Creation" y algunas de las escenas filmadas de esta película se modificaron para incluirse en el guion de Kong.

En un principio, la película fue titulada "La octava maravilla" ("The Eighth Wonder" en el original inglés). Los folletos de prensa fueron enviados en 1932 para entusiasmar a los propietarios de los cines y que presentaran "La octava maravilla" en sus anuncios. En el guion original, el gorila es llamado tan solo Kong y fueron los publicistas quienes añadieron "King", de forma que en la película el nombre completo aparece solamente en los títulos de apertura y al finalizar los créditos.

La puerta gigante usada en "King Kong" fue quemada junto con un plató cinematográfico para una escena que recreaba el incendio de Atlanta en la película "Lo que el viento se llevó". Dicha puerta había sido construida originalmente para el largometraje de 1927, "Rey de reyes". 

Algunas escenas en la selva se filmaron en el mismo escenario de la película "El malvado Zaroff". Las ubicaciones del escenario de esa película tuvieron lugar en la península de Palos Verdes, pero las escenas de los acantilados se rodaron en San Pedro, Long Beach y Redondo Beach. La Isla Santa Catalina fue otra localización que se usó para filmar escenas ambientadas en la selva.

El modelo de Kong fue hecho con una especie de esqueleto de acero, rellenado con algodón y cubierto con látex para que pudiera moverse de manera natural. Este modelo fue recubierto después con pieles de oso. La retroproyección fue una técnica de composición fotográfica usada para que Kong pudiera aparecer en escena junto con Fay Wray. Para este mismo fin, se recrearon modelos en miniatura de los personajes.

Una primera versión de "King Kong" fue preestrenada en San Bernardino, California, en enero de 1933. En esa versión había una escena que mostraba como la tripulación es devorada por una araña, un cangrejo, un lagarto y un pulpo (todos gigantes) tras caer del puente que derribó Kong. Esta escena provocó que algunos de los espectadores gritaran, abandonaran la sala de cine o sufrieran desmayos, por lo que Cooper decidió eliminarla. En un memorándum del estudio, Merian C. Cooper dijo que él mismo descarto la escena porque "paraba la historia".

Las siguientes escenas fueron filmadas, pero nunca formaron parte del desarrollo de la película: 


Dicha escena si se añadió a la versión de 2005, y casualmente, el director del remake, Peter Jackson, realizó su propia recreación de la escena, basándose en el "storyboard" de 1933.
El estreno de 1933 de "King Kong" fue un éxito de taquilla inmediato y tuvo un gran impacto en la cultura popular de la década de 1930. También fue la primera película en estrenarse en dos de las salas de cine más grandes de Nueva York, al igual que fue la primera de los años 30 en la tendencia de las películas de terror. 

En 1938, "King Kong" fue reestrenada por primera vez, aunque algunas escenas como la de Kong arrancando el vestido de Ann, se consideraron inaceptables según el Código Hays. La película se reestrenó dos veces durante la década de 1940, una vez en 1942 y otra en 1946. Se reestrenó de nuevo en 1952, convirtiéndose en uno de los acontecimientos de ese año y generó más ganancias en taquilla que cuando se estrenó por primera vez en 1933. 

Las secuelas más significativas derivadas de King Kong hasta la década de 1950 fueron "Mighty Joe Young" (1949) y "Godzilla" (1954). En 1956, "King Kong" fue vendida para su transmisión en televisión después de su quinto reestreno. Desde ese momento, ha atraído la atención de televidentes que se convirtieron en seguidores de la película.

La película recibió buenas críticas desde su primer estreno, aunque Joe Bigelow de la revista "Variety" afirmó que "King Kong" sería una buena película de aventuras si «la audiencia se acostumbraba a los movimientos mecánicos, a los desperfectos de los animales expuestos y a la falsa atmósfera de la película».

El periódico "The New York Times" encontró en "King Kong" una película de aventuras fascinante: 

Susan Sontag, en 1964 hizo un ensayo llamado "Notes on Camp", que incluye a "King Kong" como parte del canon Camp. En 2002, Roger Ebert escribió en sus críticas sobre "Great Films" que los efectos no están a la altura de los estándares modernos, «pero que algo eterno y primitivo en "King Kong" de alguna manera todavía funciona».

"King Kong", en 2007 tenía una puntuación promedio de 100 % basada en cuarenta y seis críticas en la página web Rotten Tomatoes.

En 1991, la película fue considerada «cultural, histórica y estéticamente significativa» por la Biblioteca del Congreso de Estados Unidos y seleccionada para su preservación en el National Film Registry. En 1998, se clasificó en el puesto nº43 en la lista , del American Film Institute, y en 2007 llegó al puesto nº 41 en el .

También llegó al puesto n.º 12 en el listado , al 24.º en y, en junio de 2008, al puesto n.º 4 en el listado de las diez mejores películas de fantasía del AFI's 10 Top 10.

En abril de 2004, la revista "Empire" clasificó a "King Kong" como la mejor película de monstruos de todos los tiempos. En mayo de 2004, la revista "Total Film" clasificó la escena final de "King Kong" en el Empire State Building en el tercer puesto del listado de «Mejores escenas de muerte». "King Kong" fue añadida por la revista "Time" en un listado de las «100 mejores películas de todos los tiempos».

"King Kong" es uno de los personajes más conocidos de la historia del cine. Kong y las películas que ha protagonizado han sido referenciadas en la cultura popular en todo el mundo. King Kong consiguió el prestigio de ser un icono de la cultura popular y de ser una leyenda urbana. King Kong ha sido la inspiración de anuncios publicitarios, dibujos animados, cómics, películas, portadas de revista, obras de teatro, poesía, cortometrajes y programas de televisión. Otras referencias a King Kong se encuentran en forma de parodias.

King Kong logró alcanzar la cima de su popularidad en los años sesenta y setenta, como parte de una tendencia nostálgica por el cine de Hollywood de los años treinta. Durante esta época el personaje de Kong y la película eran comunes en la cultura popular. 

A mediados de los años sesenta, RKO Pictures comenzó a autorizar productos relacionados con King Kong debido a la demanda del público; como cómics, juegos, miniaturas y pósters.

El Festival de Cine Fantástico de Sitges lo incorporó a su logotipo y como imagen de marca.

"King Kong" fue el primer largometraje en mostrar un monstruo gigante en la civilización después de la película de cine mudo "El mundo perdido" (1925). En las películas de monstruos filmadas después de 1933 se hizo palpable la influencia de "King Kong". Estas incluyen: "The Beast from 20,000 Fathoms" (1953), "Godzilla" (1954) y "Parque Jurásico" (1993).
Las escenas de "King Kong" donde aparecen los dinosaurios fueron referenciadas o imitadas en las tres películas de "Parque Jurásico", especialmente en la segunda, "" (1997). En esta película, un tiranosaurio es trasladado desde una isla remota a la civilización, logra escaparse de sus captores y comienza a correr dentro de la ciudad. El barco que transportaba a esa bestia se llama The Venture, el mismo nombre del barco que se presenta en "King Kong". Kong es incluso mencionado en la primera película de "Parque Jurásico", cuando Jeff Goldblum en su papel como Ian Malcolm pregunta, "What have they got in there, King Kong?" («¿Qué tienen ahí, a King Kong?»). La pelea que libró Kong contra un tiranosaurio tanto en la versión de 1933 como en el remake de 1976 es muy similar a la pelea que se presenta en "Parque Jurásico III" entre un Spinosaurus y un tiranosaurio.

En "King Kong", una de las escenas más conocidas es la del gorila en la cima del Empire State Building con Ann Darrow. Esta escena ha sido copiada o parodiada en dibujos animados, cómics, películas de terror y comerciales de televisión. Los episodios "Monty Can't Buy Me Love", "Bart Has Two Mommies" y el segmento "King Homer" de "Treehouse of Horror III", ambos de la serie animada "Los Simpson", son parodias televisivas referentes a "King Kong".

La imagen de Kong ha sido usada para anuncios publicitarios, por ejemplo para publicidad de Coca Cola o Energizer. En 1990, Kongfrontation, una de las atracciones del parque temático Universal Studios Florida se convirtió una de las más populares hasta que fue cerrada en 2002.

Kong fue la inspiración para el videojuego de Nintendo "Donkey Kong" de 1981 y sus secuelas. En el juego, el jugador toma el rol de Jumpman y debe rescatar a su chica del gorila del mismo nombre.




</doc>
<doc id="9691" url="https://es.wikipedia.org/wiki?curid=9691" title="Artritis reactiva">
Artritis reactiva

La artritis reactiva es considerada una enfermedad reumática, del grupo de las espondiloartropatías seronegativas.

Se cree que es el resultado de la interacción de diversos factores genéticos (como el antígeno HLA-B27) y ambientales, que causan una reactividad inmunitaria anormal ante ciertos patógenos bacterianos, por lo que también es llamada artritis reactiva o postinfecciosa.

Aun cuando los términos "síndrome de Reiter" y "artritis reactiva" suelen usarse indistintamente, originalmente la descripción de Hans Reiter fue la de un cuadro secundario a una infección venérea (dolor en hipocondrio por perihepatitis, también llamado síndrome de Fitz-Hugh-Curtis, típico de la infección por gonococos o chlamydia trachomatis). Al existir artritis reactivas a otros patógenos (como es el caso de la fiebre reumática y la enfermedad de Lyme), la mayoría de los expertos concuerdan en describir como artritis reactivas sólo aquellas artropatías inflamatorias secundarias a procesos infecciosos cuyas características clínicas se asemejan al grupo de las espondiloartropatias.

Estas son un grupo de enfermedades que comparten características clínicas, patogenéticas y epidemiológicas entre sí. Se caracterizan principalmente por la afección axial (columna vertebral y articulación sacroiliaca), entesopatía (inflamación en las inserciones fibrocartilaginosas de ligamentos al hueso) y el hecho de ser, a diferencia de la artritis reumatoide, seronegativas al factor reumatoide. La enfermedad prototipo de este grupo es la espondilitis anquilosante.

Frecuentemente comienza con una infección genitourinaria o gastrointestinal.

Algúnos de los patógenos reconocidos son: "Chlamydiae, Salmonella, Shigella, Yersinia, Campylobacter" y otros.

Su manifestación inicial es la uretritis no gonocócica, que prepara el escenario para el síndrome de Reiter; el resto de las características se desarrollan de 1 a 5 semanas después del comienzo de la uretritis. Del mismo modo, puede ocurrir luego de un cuadro de gastroenteritis infecciosa (forma postdisentérica).

La tríada clásica de sus manifestaciones clínicas son inflamación de la uretra, conjuntivitis y artritis, pero también puede presentar entesitis, sacroileitis, uveítis, y diversas lesiones mucocutáneas (como la queratodermia blenorrágica y la balanitis circinada, entre otras).

Asimismo, existen afectaciones dermatológicas en forma de pápulas serosas con centro amarillo en plantas, palmas y en menor frecuencia en uñas, escroto, cuero cabelludo y tronco.


Ya en el siglo XVI se describía la asociación de uretritis con artritis.

En 1776, Stoll describió por primera vez esta asociación.

En 1818, el primero en describir la tríada completa (uretritis, artritis y conjuntivitis) fue el cirujano inglés sir Benjamin Collins Brodie (1783-1862). Incluyó 5 casos en su libro de texto "Pathological and surgical observations on diseases of the joints (Londres, 1818).

En 1916 ―durante la Primera Guerra Mundial― se publicaron dos informes por separado:
Informó que un joven teniente había sido admitido en el ejército el 14 de octubre de 1916. El paciente estaba gravemente enfermo con fiebre alta y diarrea sanguinolenta, y desarrolló uretritis, artritis y conjuntivitis. Después de que se demostró la presencia de espiroquetas, Reiter describió el caso como «Una espiroqueta previamente desconocida (Spirochaetosis artrítica)».
Reiter mismo no consideró que se tratara de un caso raro: «No hay razón para creer que no han ocurrido muchas enfermedades similares y que se han difundido por todo el país».

En la literatura científica francesa se utilizó siempre el término «síndrome de Fiessinger y Leroy» (o incluso «síndrome de Fiessinger, Leroy y Reiter»).

En 1942, un artículo de los médicos estadounidenses Bauer y Engelman describió el primer paciente conocido de Estados Unidos con artritis reactiva, a la que denominaron «síndrome de etiología desconocida, caracterizado por uretritis, conjuntivitis y artritis (mal llamado “enfermedad de Reiter”)».
El trabajo de estos dos médicos solo contenía una referencia (el artículo de Hans Reiter), y declaraba erróneamente: «Descrito por primera vez por Reiter, ha sido comúnmente conocido como “enfermedad de Reiter”».

Este epónimo se mantuvo en uso a pesar de no ser apropiado desde el punto de vista histórico y a pesar de los experimentos que Hans Reiter realizó en campos de concentración, que lo convirtieron en criminal de guerra.


</doc>
<doc id="9694" url="https://es.wikipedia.org/wiki?curid=9694" title="Cantar de gesta">
Cantar de gesta

Cantar de gesta es el nombre dado a la epopeya escrita en la Edad Media o a una manifestación literaria extensa perteneciente a la épica que narra las hazañas de un héroe cuyas virtudes representan modelos para un pueblo o colectividad durante el Medievo. 

En los siglos XI y XII, los juglares divulgaban oralmente los cantares de gesta, debido al analfabetismo de la sociedad de la época (véase Mester de juglaría). Aunque su longitud varía entre los 2.000 y los 20.000 versos, como media no solían exceder los 4.000. Esta cantidad de versos ya suponía que el juglar que lo recitaba en público tuviera que fragmentar su relato en más de una jornada. Esto parece demostrarse por la existencia de determinados pasajes (de entre 20 y 90 versos) en los que se hace un resumen de lo anteriormente acontecido, probablemente para refrescar la memoria del auditorio o introducir en el relato a los nuevos espectadores. Los cantares se agrupaban en tiradas variables de versos, que se relacionaban por tener la misma asonancia al final de cada verso y por constituir una unidad de significado, a menudo anunciado en la tirada anterior.

Sin embargo, si estos cantares de gesta han llegado hasta nuestros días, se debe a que se realizaron copias manuscritas de ellos. En general estas copias son bastante más tardías que las propias canciones. Estos cantares eran recitados por los juglares.

Los cantares de gesta fueron especialmente numerosos en Francia, donde probablemente eran compuestos en su mayoría por clérigos instruidos. Se conservan muchos manuscritos de cantares de gesta franceses. De ellos, la obra maestra es la "Chanson de Roland", en castellano "Cantar de Roldán" por su héroe central, llamado también Orlando en italiano. Narra, en términos apocalípticos, la derrota de la retaguardia del ejército de Carlomagno hostigada en el valle de Roncesvalles por el rey moro de Zaragoza Marsilio, aliado con el traidor a Carlomagno Ganelón. En esta batalla perece el héroe del cantar, Roldán, y su deuteragonista Oliveros, por confiar demasiado en sus propias fuerzas para repeler la agresión. Cuando Roldán toca el olifante para pedir ayuda ya es demasiado tarde. La venganza del emperador Carlomagno, del obispo Turpin y de los Doce Pares de Francia ocupa el apocalíptico final de la historia.
Algo posteriores al "Cantar de Roldán" son los cantares como el Cantar de los Aliscanos, el Charroi de Nimes o La coronación de Luis. Ello sitúa el siglo XII como el momento álgido de este tipo de literatura.

Las "chansons de geste" francesas estaban escritas en decasílabos o, más tardíamente, en alejandrinos asonantados (en los primeros tiempos) y luego consonantados (en sus últimas manifestaciones) agrupados en largas tiradas de extensión variable. Aparecieron hacia el fin del Siglo XI y fueron cantadas entre 1050 y 1150. Las últimas fueron producidas en el curso del siglo XV. A menudo anónimo, su autor la destinaba a ser cantada y acompañada musicalmente ante un público extenso y variado, popular o noble.

Un cantar de gesta francés cuenta, en los manuscritos conservados, unos noventa, con entre mil y veinte mil versos. No son rimados en consonante, sino simplemente asonantados. Como se trata de una literatura de carácter fundamentalmente oral, la repetición de la última vocal acentuada bastaba para quedarse en la memoria auditiva y como se ha dicho anteriormente, utilizaban algunos pasajes (resúmenes de lo ya contado) y fórmulas repetidas (epítetos añadidos a cada nombre propio que caracterizaban a tal personaje: Carlomagno "el de la barba florida") que permitían a los recitadores tiempo para hacer memoria de lo que debían contar. La recitación de estos largos poemas podía durar varios días seguidos.

Las copias conservadas, que como se indicó anteriormente se deben a escribas probablemente bastante posteriores, en muchos casos están redactadas en anglo-normando, aunque ello no prejuzgue que los cantares estuvieran escritos originalmente en dicha lengua. De hecho, se trata de copias no muy cuidadas (a diferencia de las realizadas por los copistas en los conventos), y únicamente las copias más tardías (también probablemente elaboradas ya por clérigos) a partir de los siglos XIII, XIV y XV mantienen coherencia ortográfica y un cierto cuidado. Este hecho hace pensar que las primeras copias eran instrumento de trabajo de los propios juglares. Además, diferentes versiones de los mismos cantares muestran que determinados aspectos o partes de las historias se desarrollaban más en unas versiones que en otras, e incluso ciertos acontecimientos se variaban o cambiaban de escenario.

El acto épico por excelencia es el acto guerrero; la narración del combate en sus diferentes etapas es casi constante: el encuentro de débiles fuerzas cristianas contra multitudes paganas, asaltos a lanza o espada, hazañas de fuerza y heridas, etcétera. Para ello existen descripciones y el uso de determinados recursos estilísticos: constante uso del tiempo presente, alternancia de diálogo y narración, énfasis y otras.

Los cantares de gesta franceses están escritos en lengua de oil y cantan el valor marcial de los caballeros, héroes de la época de Carlos Martel y Carlomagno, y sus batallas contra los moros. A estas leyendas históricas se añadieron después una gran cantidad de elementos maravillosos: gigantes, magia y monstruos aparecen entre los enemigos al lado de los sarracenos. Y los aspectos militares fueron decayendo en favor de los elementos maravillosos.

Los temas de los cantares de gesta franceses se convirtieron en la llamada "materia de Francia", que se opuso a la llamada "materia de Bretaña", constituida por las historias del rey Arturo y de los caballeros de la mesa redonda, y la "materia de Roma", que mezcla la mitología griega con narraciones de Alejandro Magno, Julio César y otras figuras de la antigüedad grecolatina presentadas como ejemplos caballerescos.

Cuando las costumbres medievales se refinaron y se volvieron más sutiles, se prefirieron a los cantares de gesta las narraciones cortesanas que estaban inspiradas más bien en las relaciones entre el caballero y su dama.

En los cantares de gesta solo el sistema de vida feudal interviene en escena. El héroe épico es un caballero dotado de una fuerza sobrehumana y capaz de sobrellevar todos los sufrimientos físicos, morales y psíquicos posibles. Es ejemplar por su fidelidad a su señor y es elegido por su perfección para representar siempre una colectividad cuya existencia está en juego. Con Carlomagno, por ejemplo, es "la dulce Francia", como para Ruy Díaz de Vivar es "Castilla la gentil", y luchan y sufren para defenderlo y vencer al fin. Las fuerzas divinas se añaden casi siempre para socorrerlos; la muerte es el momento más emocionante de la narración y muestra una lección dictada por la visión religiosa y feudal de la sociedad: el sufrimiento y la muerte son nobles cuando ellas llegan a causa de Dios y el soberano. El público es llamado a grandes emociones colectivas y religiosas. Los otros personajes poseen papeles definidos: son confidentes, traidores, enemigos, ayudantes, etcétera. Están en la narración para subrayar el heroísmo y las virtudes del héroe principal.

Quedan menos de cien cantares de gesta franceses; los trovadores de los siglos XIII y XIV los agruparon en tres grandes series llamadas ciclos. Cada ciclo comprende unos poemas épicos que se desarrollan en torno al mismo héroe o miembros de su familia. Se distinguen:




A estos se les suele agregar el Ciclo de las Cruzadas, integrado por "La Chanson d'Antioche", "La Chanson de Jérusalem" y "Les chetifs".

En Alemania, por otra parte, fue célebre también el Cantar de los Nibelungos.

En Rusia es particularmente reconocida la obra anónima el Cantar de las huestes de Ígor, escrita en eslavo antiguo y que data de finales del siglo XII.

Ni la épica medieval francesa ni la alemana perduran de forma oral ni poseen la vitalidad de la épica medieval española; fragmentos de los cantares de gesta españoles se recitan todavía en pueblos de España y América Latina, transmitidos de padres a hijos de forma oral: es el llamado Romancero viejo, y la temática medieval de los cantares de gesta continuó siendo motivo de inspiración para el teatro clásico en el Siglo de Oro.

Solo se ha conservado de forma escrita el "Cantar de mio Cid", el "Cantar de las Mocedades de Rodrigo" y unos cuantos versos del "Cantar de Roncesvalles". Los filólogos han reconstruido otros pasajes de la perdida épica castellana a partir de fragmentos mal prosificados en las crónicas, donde sirvieron como fuente de información.



Menor importancia tuvieron el "Mainete" (del que hay un testimonio inserto en la "Gran conquista de Ultramar") y otros.


Las últimas formas del cantar de gesta español se dan en el siglo XIV, perdiendo el realismo y la contención de los cantares antiguos, inclinándose por una mayor fabulación. De esta época es las "Mocedades de Rodrigo".

En Inglaterra, destaca la épica anglosajona, cuya obra más destacada es el "Beowulf", que ha obtenido el estatus de epopeya nacional, al mismo nivel que la "Ilíada", el "Cantar de los Nibelungos" en alemán, la "Canción de Roldán" en francés o el "Cantar de mio Cid" español. 

Existen otros poemas épicos, entre ellos "La Batalla de Finnsburh (The Fight at Finnsburh)" y "Wálder (Waldere)". La poesía heroica anglosajona fue transmitida oralmente de generación en generación. Conforme iba implantándose el cristianismo, los narradores iban introduciendo cuentos o figuras cristianas en las antiguas historias heroicas.

No hubo una épica posterior a la invasión normanda. Faltó una personalidad que aglutinara las muchas leyendas y tradiciones históricas disímiles o divergentes en poemas cohesionados y extensos, o porque fueron leyendas que surgieron de forma demasiado tardía, ya en época renacentista. Así ocurrió, por ejemplo, con las leyendas en torno a Robin Hood, que no se unificaron en un largo poema extenso, o con las leyendas suizas sobre Guillermo Tell.




</doc>
<doc id="9695" url="https://es.wikipedia.org/wiki?curid=9695" title="Romance">
Romance

El término romance puede hacer referencia a: 



</doc>
<doc id="9696" url="https://es.wikipedia.org/wiki?curid=9696" title="Crónica">
Crónica

Crónica es la denominación de un género literario incluido en la historiografía, que consiste en la recopilación de hechos históricos narrados en orden cronológico. La palabra viene del latín "chronica", que a su vez se deriva del griego "kronika biblios", es decir, libros que siguen el orden del tiempo.

==Características=
-Los hechos se narran según el orden temporal en que ocurrieron, a menudo por testigos presenciales o contemporáneos, ya sea en primera o en tercera persona. En la crónica se utiliza un lenguaje sencillo, directo, muy personal y admite un lenguaje literario con uso reiterativo de adjetivos para hacer énfasis en las descripciones. Emplea verbos de acción y presenta referencias de espacio y tiempo. La crónica lleva cierto distanciamiento temporal a lo que se le llama escritos históricos. Por medio de las crónicas se pueden redactar escritos, tomando las opiniones de varias personas para saber si esto es cierto o no, como en el libro "Crónica de una muerte anunciada" escrito por Gabriel García Márquez.

La literatura cronística no tiene el rigor metodológico de la historiografía científica, sus pretensiones son otras muy distintas, por lo que su utilización como fuente historiográfica se hace con la prevención necesaria por los historiadores; como hacen aquí al calificar la "Crónica de Alfonso III":

Los propios cronistas dejaron constancia de tales limitaciones:
En la Edad Media y el Renacimiento,la utilización de los términos "anales", "crónicas" e "historias" es ambigua, equívoca y, en la práctica, intercambiable.

Propiamente, los anales únicamente distinguen los hechos año por año, mientras que las crónicas son registros históricos en los que los hechos son siplemente registrados en el orden de su sucesión por un autor que es al menos en parte contemporáneo de los hechos que registra. El género de la crónica universal surge de la necesidad de introducir en el relato cronístico los orígenes del mundo y el hombre según la Biblia, continuando con la historia del pueblo elegido hasta el nacimiento, vida, muerte y resurrección de Cristo y el surgimiento y expansión del cristianismo; además procura establecer sincronismos entre la cronología bíblica y las de los imperios antiguos, la de Grecia (arcontes de Atenas) y la de Roma ("fasti" consulares).

Las crónicas que no tenían un propósito general, sino que se limitaban a reseñar cronológicamente los acontecimientos destacables de un personaje concreto, reciben a veces el nombre de crónicas particulares, con lo que se identifican con el género biográfico. Crónica particular es, por ejemplo, la de Pero Niño, llamada "El Victorial" (ca. 1463). También una de las múltiples obras sobre el Cid se denomina "Crónica particular de el Cid" (1512 y ediciones posteriores).

Las crónicas son también un género periodístico. Se las clasifica como "amarillas" o "blancas" según su contenido. Las "amarillas" tienen material más subjetivo y generalmente la voz autorizada es una persona o ciudadano común; las "blancas" usan material más objetivo y la voz autorizada es, generalmente, la autoridad, un profesional, etcétera.




</doc>
<doc id="9697" url="https://es.wikipedia.org/wiki?curid=9697" title="Ensayo">
Ensayo

El ensayo es un tipo de texto en prosa que analiza, interpreta o evalúa un tema. Se considera un género literario, al igual que la poesía, la narrativa y el drama.

Las características clásicas más representativas de un ensayo son:


Es un género literario dentro del más general de la didáctica.

Casi todos los ensayos modernos están escritos en prosa. Si bien los ensayos suelen ser breves, también hay obras muy voluminosas como la de John Locke "Ensayo sobre el entendimiento humano".

En países como Estados Unidos o Canadá, los ensayos se han convertido en una parte importante de la educación. Así, a los estudiantes de secundaria se les enseña formatos estructurados de ensayo para mejorar sus habilidades de escritura, o en humanidades y ciencias sociales se utilizan a menudo los ensayos como una forma de evaluar el conocimiento de los estudiantes en los exámenes finales, o ensayos de admisión son utilizados por universidades en la selección de sus alumnos.

Por otra parte, el concepto de "ensayo" se ha extendido a otros ámbitos de expresión fuera de la literatura, por ejemplo: un "ensayo fílmico" es una película centrada en la evolución de un tema o idea; o un "ensayo fotográfico" es la forma de cubrir un tema por medio de una serie enlazada de fotografías.

Un ensayo es una obra literaria relativamente breve, de reflexión subjetiva pero bien informada, en la que el autor trata un tema por lo general humanístico de una manera personal y sin agotarlo, y donde muestra cierta voluntad de estilo, de forma más o menos explícita, encaminada a persuadir al lector de su punto de vista sobre el asunto tratado. El autor se propone crear una obra literaria y no simplemente informativa, y versa sobre todo de temas humanísticos (literatura, filosofía, arte, ciencias sociales y políticas...), aunque también, más raramente, de asuntos científicos.

El ensayo, a diferencia del texto informativo, no posee una estructura definida ni sistematizada o compartimentada en apartados o lecciones, por lo que suele carecer de aparato crítico, bibliografía o notas, o estas son someras o sumarias (en el caso del ensayo escolar, es preciso aportar todas las fuentes); ya desde el Renacimiento se consideró un género más abierto que el medieval "tractatus" o tratado o que la suma, y se considera distinto a ellos no solo en su estructura libérrima y nada compartimentada en secciones, sino también por su voluntad artística de estilo y su subjetividad, ya que no pretende informar, sino persuadir o convencer del punto de vista del autor en el tratamiento de un tema que, como ya se ha dicho, no pretende agotar ni abordar sistemáticamente, como el tratado: de ahí su subjetividad, su carácter proteico y asistemático, su sentido artístico y su estructura flexible, que personaliza la materia.

El ensayo es una interpretación o explicación de un determinado tema —humanístico, filosófico, político, social, cultural, deportivo, por mencionar algunos ejemplos—, desarrollado de manera libre, asistemática, y con voluntad de estilo sin que sea necesario usar un aparataje documental.
En la Edad Contemporánea este tipo de obras ha llegado a alcanzar una posición central.

En la actualidad está definido como género literario, debido al lenguaje, muchas veces poético y cuidado que usan los autores, pero en realidad, el ensayo no siempre podrá clasificarse como tal. En ocasiones se reduce a una serie de divagaciones y elucubraciones, la mayoría de las veces de aspecto crítico, en las cuales el autor explora un tema concreto o expresa sus reflexiones sobre él, o incluso discurre y diserta sin tema específico.

Ortega y Gasset lo definió como «la ciencia sin la prueba explícita». Alfonso Reyes afirmó que «el ensayo es la literatura en su función ancilar» —es decir, como esclava o subalterna de algo superior—, y también lo definió como «el Centauro de los géneros». El crítico Eduardo Gómez de Baquero —más conocido como Andrenio— afirmó en 1917 que «el ensayo está en la frontera de dos reinos: el de la didáctica y el de la poesía, y hace excursiones del uno al otro». Y por su parte Eugenio d'Ors lo definió como la «poetización del saber».

Utiliza la modalidad discursiva expositivo-argumentativa y un tipo de «razonamientos blandos» que han sido estudiados por Chaïm Perelman y Lucie Ollbrechts-Tyteca en su "Tratado de la argumentación".

A esto convendría añadir además que en el ensayo existe, como ha apreciado el crítico Juan Marichal, una «voluntad de estilo», una impresión subjetiva que es también de orden formal.

Otros géneros didácticos emparentados con el ensayo son:


El desarrollo moderno y más importante del género ensayístico vino sobre todo a partir de los "Essais" (1580) del escritor renacentista francés Michel de Montaigne. Unos años después, Francis Bacon siguió su ejemplo y publicó sus "Essays" que en su primera edición de 1597 contenía 10 ensayos y en su tercera edición, la más amplia e impresa en 1625, contenía 59 ensayos.

Los precedentes más antiguos del ensayo hay que buscarlos en el género epidíctico de la oratoria grecorromana clásica; las "Cartas a Lucilio" (de Séneca) y los "Moralia" (de Plutarco) vienen a ser ya prácticamente colecciones de ensayos. En el siglo III d. C. Menandro el Rétor, aludiendo a ello bajo el nombre de «charla», expuso algunas de sus características en sus "Discursos sobre el género epidíctico":


En Grecia donde el ensayo tiene su origen, se consideraba como una proposición original que dispone elementos de creación, generación e innovación. Se partía del conocimiento normal (establecido) para romperlo. A partir de elementos que lo hacen, al conocimiento, diferente en: perspectiva, conjunción, relación, conformación, etc.

Los ensayos existían en Japón varios siglos antes de que se desarrollaran en Europa en un género denominado Zuihitsu que se remonta a casi los inicios de la literatura japonesa. Muchas de las primeras obras más notables de la literatura japonesa están en este género. Un ejemplo notable es Makura no Sōshi (El libro de la almohada) del siglo XI escrito por Sei Shonagon, dama de compañía de la emperatriz, en la que recogió sus experiencias diarias en la corte Heian. Un segundo ejemplo es Tsurezuregusa (Ensayos en ociosidad) escrito por el monje budista Yoshida Kenkō. Kenkō describió sus breves escritos de manera similar a Montaigne, refiriéndose a ellos como "pensamientos sin sentido", escritos en "horas muertas". Se trata de su trabajo más famoso y una de las obras más estudiadas de la literatura japonesa medieval.

En España el género surge con el Renacimiento en forma de epístolas, discursos, diálogos y misceláneas en el siglo XVI. La primera muestra del género son las "Epístolas familiares" (1539) de Fray Antonio de Guevara, todavía con forma de carta, quien se inspira además en las "Letras" (1485) de Fernando del Pulgar; también hay numerosos diálogos (casi siempre erasmistas, como los de los hermanos Alfonso y Juan de Valdés; el "Diálogo de la dignidad del hombre" de Fernán Pérez de Oliva...) escritos no solo en castellano, sino también en latín, o misceláneas como la de Luis Zapata (1592) o el "Jardín de flores curiosas" (1573) de Antonio de Torquemada. En el XVII se continúa con el "Pusilipo" (1629) de Cristóbal Suárez de Figueroa, las "Cartas filológicas" (1634) de Francisco Cascales y los "Errores celebrados de la antigüedad" (1653) de Juan de zabaleta.

Luego aparece sólidamente constituido a principios del siglo XVIII con el muy reimpreso "Teatro crítico universal" (1726-1740) y las "Cartas eruditas y curiosas" (1742-1760) del padre Benito Jerónimo Feijoo, quien los denomina "discursos" (de "discurrir") o "cartas"; a finales del mismo, bajo la vaga y falsa apariencia de novela epistolar, aparecen las "Cartas marruecas" (1789) de José Cadalso y las "Cartas económico-políticas" (1785-1795) de León de Arroyal.

Solamente en el siglo XIX tomará la denominación propia como género autónomo de "ensayo" cuando empiecen a escribirlos algunos autores de la Generación de 1868: Emilia Pardo Bazán ("La cuestión palpitante", 1883 y 1884), Juan Valera ("Disertaciones y juicios literarios", "La libertad en el arte"...), Marcelino Menéndez Pelayo, quien emplea ya el término ("Ensayos de crítica filosófica"), Leopoldo Alas ("Solos", 1881, y "Palique", 1894)... La prensa empieza a acogerlos en algunas revistas de fin de siglo y ya se encontrará completamente asentado propiamente con los escritos en el siglo XX por la Generación del 98: Miguel de Unamuno ("En torno al casticismo", 1895, y otros), José Martínez Ruiz ("Al margen de los clásicos", 1915), Pío Baroja ("La caverna del humorismo", 1919; "El tablado de Arlequín" y "Nuevo tablado de Arlequín", 1903 y 1917; "Vitrina pintoresca", 1935; "Momentum catastroficum", 1918), Ramiro de Maeztu ("Hacia otra España", 1899; "La crisis del humanismo", 1919) y Antonio Machado ("Juan de Mairena", 1936).

Destaca especialmente el Novecentismo, que contó con ensayistas tan dotados como José Ortega y Gasset ("Meditaciones del Quijote", 1914; "El Espectador" 1916-1934, 8 vols.; "España invertebrada", 1921; "La deshumanización del arte", 1925 etc.), Ramón Pérez de Ayala ("Las máscaras", 1917-1919; "Política y toros", 1918, etc.), Gregorio Marañón ("Ensayo biológico sobre Enrique IV de Castilla y su tiempo", 1930; "Tiempo nuevo y tiempo viejo", 1940; "Don Juan. Ensayo sobre el origen de su leyenda", 1940; "Ensayos liberales", 1946), Eugenio d'Ors ("Glosari", 1915-1917; "Oceanografía del Tedi", 1918; "Tres horas en el Museo del Prado. Itinerario estético", 1922), Rafael Cansinos Assens ("El divino fracaso", 1918; "Ética y estética de los sexos", 1921; "La nueva literatura" 1917-1927, 4 vols.; "Los temas literarios y su interpretación", 1924 etc.), Ramón Gómez de la Serna ("La utopía", 1909; "El concepto de nueva literatura", 1909; "El rastro", 1915; "Ismos", 1931), José Bergamín ("La cabeza a pájaros", 1934; "El arte de birlibirloque" - "La estatua de Don Tancredo" - "El mundo por montera" 1961; "Ilustración y defensa del toreo", 1974; "Beltenebros y otros ensayos sobre literatura española Barcelona", 1973; "El clavo ardiendo", 1974; "La importancia del demonio y otras cosas sin importancia", 1974; "Al fin y al cabo: (prosas)" 1981 etc.) o Manuel Azaña ("Ensayos sobre Valera"), entre otros.

El ensayo en Hispanoamérica cuenta con grandes figuras. Entre los precursores más influyentes cabe destacar al escritor argentino Domingo Faustino Sarmiento (1811-1888) con su "Facundo o Civilización y barbarie" (1845) y al uruguayo José Enrique Rodó (1871-1917) por su "Ariel" (1900). El mexicano José Vasconcelos (1881-1959) escribe sobre filosofía, estética e historia, pero es especialmente renombrado por sus ensayos de tema americano, por ejemplo su "La raza cósmica", donde postula que una raza mestiza americana es la que en el futuro dirigirá el mundo. El dominicano Pedro Henríquez Ureña (1884-1946) y el argentino Ricardo Rojas (1882-1957) exploran la identidad de sus respectivos países y los que escribe el peruano José Carlos Mariategui (1895-1930) están enfocados desde el punto de vista de las ciencias sociales. También son importantes el argentino Eduardo Mallea, el mexicano Leopoldo Zea y el cubano José Antonio Portuondo, entre muchos otros.

Ya en pleno siglo XX destacan poderosamente cuatro figuras por su amplitud de conocimientos y ancho de banda: el mexicano Alfonso Reyes (1889-1959) con "Cuestiones estéticas", "Visión del Anáhuac", "La experiencia literaria" o "El deslinde", entre otras obras; el ya citado Pedro Henríquez Ureña ("Ensayos críticos", "Historia de la cultura en América Latina", "Plenitud de América"); el muy original e influyente argentino Jorge Luis Borges ("Inquisiciones", "Otras inquisiciones", "Historia de la eternidad"...) y el mexicano Octavio Paz, bien con sus ensayos sobre la idiosincrasia mexicana ("El laberinto de la soledad"), bien con otros de tema más variado ("Las peras del olmo", "Cuadrivio").

La lógica es crucial en un ensayo y lograrla es algo más sencillo de lo que parece: depende principalmente de la organización de las ideas y de la presentación. Para lograr convencer al lector hay que proceder de modo organizado desde las explicaciones formales hasta la evidencia concreta, es decir, de los hechos a las conclusiones. Para lograr esto el escritor puede utilizar dos tipos de razonamiento: la lógica inductiva o la lógica deductiva.

De acuerdo con la lógica inductiva el escritor comienza el ensayo mostrando ejemplos concretos para luego deducir de ellos las afirmaciones generales. Para tener éxito, no sólo debe elegir bien sus ejemplos sino que también debe presentar una explicación clara al final del ensayo. La ventaja de este método es que el lector participa activamente en el proceso de razonamiento y por ello es más fácil convencerle.
De acuerdo con la lógica deductiva el escritor comienza el ensayo mostrando afirmaciones generales, las cuales documenta progresivamente por medio de ejemplos bien concretos. Para tener éxito, el escritor debe explicar la tesis con gran claridad y, a continuación, debe utilizar transiciones para que los lectores sigan la lógica/argumentación desarrollada en la tesis. La ventaja de este método es que si el lector admite la afirmación general y los argumentos están bien construidos generalmente aceptará las conclusiones.

Algunos de los ensayos más reconocidos, tanto en otros idiomas como en español, son los siguientes:


Los siguientes son fragmentos de ensayos.

La estructura del ensayo es sumamente flexible, ya que toda sistematización es ajena a su propósito esencial, que es deleitar mediante la exposición de un punto de vista persuasivo que no pretende agotar, sino explorar un tema, como sí haría (y sistemáticamente) el género literario meramente expositivo del tratado; por eso estas indicaciones son meramente orientativas.

Por eso su estructura, a nivel macro estilístico o micro estilístico, puede ser:


Esta flexibilidad, que permite a una persona escribir un texto expresando lo que sabe, siente y opina sobre cualquier tema, es muy empleada en la educación. En la escuela es una práctica habitual que los alumnos redacten ensayos. De hecho, el ensayo es el género que se emplea con más frecuencia, dadas las facilidades que permite. Cada vez que un profesor pide a los alumnos desarrollar un tema, o que se realicen una investigación y se ponga por escrito, es probable que se escriba en forma de ensayo.

Un ejemplo de los pasos a seguir por un estudiante que pretende escribir un ensayo escolar podrían ser los siguientes. Lo primero y antes de redactarlo hay que documentarse sobre el tema elegido hasta alcanzar un conocimiento suficiente lo cual supone buscar la información necesaria consultando fuentes bibliográficas o de cualquier otro tipo. El segundo paso sería organizar las ideas teniendo presente para quién se escribe, qué interesa exponer y cómo hacerlo mejor. Y finalmente redactarlo siguiendo un orden, escribiendo las ideas lo mejor expresadas que se pueda y comprobando que la información, el estilo, el punto de vista y el formato son coherentes y se ajustan a lo exigido.

Un ensayo escolar convencional se suele estructurar de forma encuadrada en 3 partes: introducción, desarrollo y conclusión:
Es la que expresa el tema y el objetivo del ensayo; explica el contenido y los subtemas o capítulos que abarca, así como los criterios que se aplican en el texto, es el 10% del ensayo y abarca más o menos 6 renglones.

Además, esta parte puede presentar el problema que plantea al tema al cual vamos a abocar nuestros conocimientos, reflexiones, lecturas y experiencias. Si este se plantea, entonces el objetivo del ensayo será presentar nuestro punto de vista sobre dicho problema (su posible explicación y sus posibles soluciones).

Una introducción en un ensayo científico suele ser la exposición de una hipótesis y de los motivos que nos han llevado a la misma. Una hipótesis es una teoría que se presenta para la solución de un problema y que a lo largo del desarrollo del ensayo se defenderá con todas las pruebas científicas que podamos aportar.

Cuando hablamos de un ensayo argumentativo en la introducción se presenta el trabajo y se expone la tesis. Una tesis en un ensayo argumentativo es similar a la hipótesis del científico. Se trata de una idea, una afirmación, que vamos a defender a lo largo de cuerpo o desarrollo del ensayo. Esta tesis se defiende con argumentos que no tienen por qué ser científicos, pueden ser opiniones subjetivas (En un ensayo científico las opiniones subjetivas deben estar validadas científicamente)

En un ensayo expositivo la introducción tiene la finalidad básica de captar el interés del lector en el argumento del ensayo. Aunque evidentemente esto se busca en todos los ensayos que se realizan en este caso es la base de esta parte de presentación.

Cuándo realizamos un ensayo de análisis literario en la introducción ponemos al lector en antecedentes sobre la obra que vamos a tratar y lo situamos el aspecto concreto de ésta que queremos analizar en nuestro ensayo.

Contiene la exposición y análisis del mismo tema, se plantean las ideas propias y se sustentan con información de las fuentes necesarias: libros, revistas, Internet, entrevistas entre otras. Constituye el 75 % del ensayo. En él va todo el tema desarrollado, utilizando la estructura interna: 50 % de síntesis, 15 % de resumen y 10 % de comentario.

Se sostiene la tesis, ya probada en el contenido, y se profundiza más sobre la misma, ya sea ofreciendo contestaciones sobre algo o dejando preguntas finales que motiven al lector a reflexionar.

En este apartado el autor expresa sus propias ideas sobre el tema, se permite dar algunas sugerencias de solución, cerrar las ideas que se trabajaron en el desarrollo del tema y proponer líneas de análisis para posteriores escritos.

Esta última parte mantiene cierto paralelismo con la introducción por la referencia directa a la tesis del ensayista, con la diferencia de que en la conclusión la tesis debe ser profundizada, a la luz de los planteamientos expuestos en el desarrollo.

Teoría del Ensayo




</doc>
<doc id="9699" url="https://es.wikipedia.org/wiki?curid=9699" title="Ingeniería de software">
Ingeniería de software

La ingeniería de software es la aplicación de un enfoque sistemático, disciplinado y cuantificable al desarrollo, operación y mantenimiento de software, y el estudio de estos enfoques, es decir, el estudio de las aplicaciones de la ingeniería al software.
Integra matemáticas, ciencias de la computación y prácticas cuyos orígenes se encuentran en la ingeniería.

Se citan las definiciones más reconocidas, formuladas por prestigiosos autores:

En 2004, la U. S. Bureau of Labor Statistics (Oficina de Estadísticas del Trabajo de Estados Unidos) contó 760 840 ingenieros de software de computadora.

El término "ingeniero de software", sin embargo, se utiliza de manera genérica en el ambiente empresarial, y no todos los que se desempeñan en el puesto de ingeniero de software poseen realmente títulos de ingeniería de universidades reconocidas.

Algunos autores consideran que "desarrollo de software" es un término más apropiado que "ingeniería de software" para el proceso de crear software. Personas como Pete McBreen (autor de "Software Craftmanship") cree que el término IS implica niveles de rigor y prueba de procesos que no son apropiados para todo tipo de desarrollo de software.

Indistintamente se utilizan los términos "ingeniería "de" software" o "ingeniería "del" software"; aunque menos común también se suele referenciar como "ingeniería "en" software".
En Hispanoamérica los términos más comúnmente usados son los dos primeros.

La creación del software es un proceso intrínsecamente creativo y la ingeniería del software trata de sistematizar este proceso con el fin de acotar el riesgo de fracaso en la consecución del objetivo, por medio de diversas técnicas que se han demostrado adecuadas sobre la base de la experiencia previa.

La ingeniería de software se puede considerar como la ingeniería aplicada al software, esto es, por medios sistematizados y con herramientas preestablecidas, la aplicación de ellos de la manera más eficiente para la obtención de resultados óptimos; objetivos que siempre busca la ingeniería. No es solo de la resolución de problemas, sino más bien teniendo en cuenta las diferentes soluciones, elegir la más apropiada.

La producción de software utiliza criterios y normas de la ingeniería de software, lo que permite transformarlo en un producto industrial usando bases de la ingeniería como métodos, técnicas y herramientas para desarrollar un producto innovador regido por metodologías y las buenas prácticas. Dicho producto es un medio que interviene en las funciones de sus usuarios para obtener un proceso productivo más eficaz y eficiente; hoy en día las empresas no podrían funcionar sin software por que este es un producto de uso masivo; por lo cual, el nivel de una empresa está determinado por la calidad de su infraestructura tecnológica y los productos desarrollados o adquiridos de acuerdo a sus necesidades

 Cuando aparecieron las primeras computadoras digitales en la década de 1940, el desarrollo de software era algo tan nuevo que era casi imposible hacer predicciones de las fechas estimadas de finalización del proyecto y muchos de ellos sobrepasaban los presupuestos y tiempo estimados.. Los desarrolladores tenían que volver a escribir todos sus programas para correr en máquinas nuevas que salían cada uno o dos años, haciendo obsoletas las ya existentes.

El término Ingeniería del software apareció por primera vez a finales de la década de 1950. La Ingeniería de software fue estimulada por la crisis del software de las décadas de entre 1960 y 1980. La Ingeniería del software viene a ayudar a identificar y corregir mediante principios y metodologías los procesos de desarrollo y mantenimiento de sistemas de software.

Aparte de la crisis del software de las décadas de entre 1960 y 1980, la ingeniería de software se ve afectada por accidentes que conllevaron a la muerte de tres personas; esto sucedió cuando la máquina de radioterapia Therac-25 emite una sobredosis masiva de radiación y afecto contra la vida de estas personas.
Esto remarca los riesgos de control por software, afectando directamente al nombre de la ingeniería de software.

A principios de los 1980, la ingeniería del software ya había surgido como una genuina profesión, para estar al lado de las ciencias de la computación y la ingeniería tradicional. Antes de esto, las tareas eran corridas poniendo tarjetas perforadas como entrada en el lector de tarjetas de la máquina y se esperaban los resultados devueltos por la impresora.

Debido a la necesidad de traducir frecuentemente el software viejo para atender las necesidades de las nuevas máquinas, se desarrollaron lenguajes de orden superior. A medida que apareció el software libre, las organizaciones de usuarios comúnmente lo liberaban.

Durante mucho tiempo, solucionar la crisis del software fue de suma importancia para investigadores y empresas que se dedicaban a producir herramientas de software.

Para la década de 1980, el costo de propiedad y mantenimiento del software fue dos veces más caro que el propio desarrollo del software, y durante la década de 1990, el costo de propiedad y mantenimiento aumentó 30 % con respecto a la década anterior. En 1995, muchos de los proyectos de desarrollo estaban operacionales, pero no eran considerados exitosos. El proyecto de software medio sobrepasaba en un 50 % la estimación de tiempo previamente realizada, además, el 75 % de todos los grandes productos de software que eran entregados al cliente tenían fallas tan graves, que no eran usados en lo absoluto o simplemente no cumplían con los requerimientos del cliente.

Algunos expertos argumentaron que la crisis del software era debido a la falta de disciplina de los programadores.

Cada nueva tecnología y práctica de la década de 1970 a la de 1990 fue pregonada como la única solución a todos los problemas y el caos que llevó a la crisis del software. Lo cierto es que la búsqueda de una única clave para el éxito nunca funcionó. El campo de la ingeniería de software parece un campo demasiado complejo y amplio para una única solución que sirva para mejorar la mayoría de los problemas, y cada problema representa solo una pequeña porción de todos los problemas de software.

El auge del uso del Internet llevó a un vertiginoso crecimiento en la demanda de sistemas internacionales de despliegue de información en la World Wide Web. Los desarrolladores se vieron en la tarea de manejar ilustraciones, mapas, fotografías y animaciones, a un ritmo nunca antes visto, con casi ningún método para optimizar la visualización y almacenamiento de imágenes. También fueron necesarios sistemas para traducir el flujo de información en múltiples idiomas extranjeros a lenguaje natural humano, con muchos sistemas de software diseñados para uso multilenguaje, basado en traductores humanos.

La ingeniería de software contribuyo alrededor de 90,000 millones de dólares por año ya que entra en juego el Internet; esto hace que los desarrolladores tuviesen que manejar imágenes mapas y animaciones para optimizar la visualización/almacenamiento de imágenes (como el uso de imágenes en miniatura). El uso de los navegadores y utilización de lenguaje HTML cambia drásticamente la visión y recepción de la información.

Las amplias conexiones de red crea la proliferación de virus informáticos y la basura en los correos electrónicos (E-mail) esto pone en una carrera contra el tiempo los desarrolladores para crear nuevos sistemas de bloqueo o seguridad de estas anomalías en la informática ya que se volvían sumamente tediosas y difíciles de arreglar

Después de una fuerte y creciente demanda surge la necesidad de crear soluciones de software a bajo costo, esto conlleva al uso de metodologías más simples y rápidas que desarrollan software funcional. Cabe señalar que los sistemas más pequeños tenían un enfoque más simple y rápido para poder administrar el desarrollo de cálculos y algoritmos de software.

La ingeniería de software aplica diferentes normas y métodos que permiten obtener mejores resultados, en cuanto al desarrollo y uso del software, mediante la aplicación correcta de estos procedimientos se puede llegar a cumplir de manera satisfactoria con los objetivos fundamentales de la ingeniería de software.

Entre los objetivos de la ingeniería de software están:

Son todas aquellas personas que intervienen en la planificación de cualquier instancias de software (por ejemplo: gestor, ingeniero de software experimentado, etc.), El número de personas requerido para un proyecto de software solo puede ser determinado después de hacer una estimación del esfuerzo de desarrollo...

Es el entorno de las aplicaciones (software y hardware) el hardware proporciona el medio físico para desarrollar las aplicaciones (software), este recurso es indispensable.

En los Estados Unidos, el software contribuyó a una octava parte de todo el incremento del PIB durante la década de 1990 (alrededor de 90,000 millones de dólares por año), y un noveno de todo el crecimiento de productividad durante los últimos años de la década (alrededor de 33.000 millones de dólares estadounidenses por año). La ingeniería de software contribuyó a US$ 1 billón de crecimiento económico y productividad en esa década. Alrededor del globo, el software contribuye al crecimiento económico de maneras similares, aunque es difícil de encontrar estadísticas fiables. 

Además, con la industria del lenguaje está hallando cada vez más campos de aplicación a escala global.

La ingeniería de software cambia la cultura del mundo debido al extendido uso de la computadora. El correo electrónico (e-mail), la WWW y la mensajería instantánea permiten a la gente interactuar de nuevas maneras. El software baja el costo y mejora la calidad de los servicios de salud, los departamentos de bomberos, las dependencias gubernamentales y otros servicios sociales. Los proyectos exitosos donde se han usado métodos de ingeniería de software incluyen a GNU/Linux, el software del transbordador espacial, los cajeros automáticos y muchos otros.

Es un lenguaje de modelado muy reconocido y utilizado actualmente que se utiliza para describir o especificar métodos. También es aplicable en el desarrollo de software.

Las siglas UML significan lenguaje unificado de modelado esto quiere decir que no pretende definir un modelo estándar de desarrollo, sino únicamente un lenguaje de modelado.

Un lenguaje de modelado consta de vistas, elementos de modelo y un conjunto de reglas sintácticas, semánticas y pragmáticas que indican cómo utilizar los elementos.

En esta materia nos encontramos con varios diagramas que se pueden usar tales como: casos de uso, de clases, componentes, despliegue, etc.

El objetivo de la notación para el modelado de procesos de negocios es proporcionar de una manera fácil de definir y analizar los procesos de negocios públicos y privados simulando un diagrama de flujo.
La notación ha sido diseñada específicamente para coordinar la secuencia de los procesos y los mensajes que fluyen entre los participantes del mismo, con un conjunto de actividades relacionadas.
Características básicas de los elementos de BPMN


Un diagrama de flujo de datos permite representar el movimiento de datos a través de un sistema por medio de modelos que describen los flujos de datos, los procesos que tranforman o cambian los datos, los destinos de datos y los almacenamientos de datos a la cual tiene acceso el sistema.

Su inventor fue Larry Constantine, basado en el modelo de computación de Martin y Estrin: flujo gráfico de datos. Con los diagramas de flujo de datos determina la manera en que cualquier sistema puede desarrollarse, ayuda en la identificación de los datos de la transacción en el modelo de datos y proporciona al usuario una idea física de cómo resultarán los datos a última instancia.

Las Herramienta CASE son herramientas computacionales (software) que están destinadas a asistir en los procesos de ciclo de vida de un software, facilitan la producción del software, varias se basan principalmente en la idea de un modelo gráfico.

Un objetivo de décadas ha sido el encontrar procesos y metodologías, que sean sistemáticas, predecibles y repetibles, a fin de mejorar la productividad en el desarrollo y la calidad del producto software, en pocas palabras, determina los pasos a seguir y como realizarlos para finalizar una tarea.

La ingeniería de software requiere llevar a cabo numerosas tareas agrupadas en etapas, al conjunto de estas etapas se le denomina ciclo de vida. Las etapas comunes a casi todos los modelos de ciclo de vida son las siguientes:

Se debe identificar sobre qué se está trabajando, es decir, el tema principal que motiva el inicio del estudio y creación del nuevo software o modificación de uno ya existente. A su vez identificar los recursos que se tienen, en esto entra el conocer los recursos humanos y materiales que participan en el desarrollo de las actividades. Es importante entender el contexto del negocio para identificar adecuadamente los requisitos.

Se tiene que tener dominio de la información de un problema, lo cual incluye los datos fuera del software (usuarios finales, otros sistemas o dispositivos externos), los datos que salen del sistema (por la interfaz de usuario, interfaces de red, reportes, gráficas y otros medios) y los almacenamientos de datos que recaban y organizan objetos persistentes de datos (por ejemplo, aquellos que se conservan de manera permanente).

También hay que ver los puntos críticos, lo que significa tener de una manera clara los aspectos que entorpecen y limitan el buen funcionamiento de los procedimientos actuales, los problemas más comunes y relevantes que se presentan, los motivos que crean insatisfacción y aquellos que deben ser cubiertos a plenitud. Por ejemplo: ¿El contenido de los reportes generados, satisface realmente las necesidades del usuario? ¿Los tiempos de respuesta ofrecidos, son oportunos?, etc.

Hay que definir las funciones que realizará el software ya que estas ayudan al usuario final y al funcionamiento del mismo programa.

Se tiene que tener en cuenta cómo será el comportamiento del software ante situaciones inesperadas como lo son por ejemplo una gran cantidad de usuarios usando el software o una gran cantidad de datos entre otros.

 Extraer los requisitos de un producto software es la primera etapa para crearlo. Durante la fase de análisis, el cliente plantea las necesidades que se presenta e intenta explicar lo que debería hacer el software o producto final para satisfacer dicha necesidad mientras que el desarrollador actúa como interrogador, como la persona que resuelve problemas. Con este análisis, el ingeniero de sistemas puede elegir la función que debe realizar el software y establecer o indicar cuál es la interfaz más adecuada para el mismo.

El análisis de requisitos puede parecer una tarea sencilla, pero no lo es debido a que muchas veces los clientes piensan que saben todo lo que el software necesita para su buen funcionamiento, sin embargo se requiere la habilidad y experiencia de algún especialista para reconocer requisitos incompletos, ambiguos o contradictorios. Estos requisitos se determinan tomando en cuenta las necesidades del usuario final, introduciendo técnicas que nos permitan mejorar la calidad de los sistemas sobre los que se trabaja.

El resultado del análisis de requisitos con el cliente se plasma en el documento ERS (especificación de requisitos del sistema), cuya estructura puede venir definida por varios estándares, tales como CMMI. Asimismo, se define un diagrama de entidad/relación, en el que se plasman las principales entidades que participarán en el desarrollo del software.

La captura, análisis y especificación de requisitos (incluso pruebas de ellos), es una parte crucial; de esta etapa depende en gran medida el logro de los objetivos finales. Se han ideado modelos y diversos procesos metódicos de trabajo para estos fines. Aunque aún no está formalizada, ya se habla de la ingeniería de requisitos.

La IEEE Std. 830-1998 normaliza la creación de las especificaciones de requisitos de software (Software Requirements Specification).

Finalidades del análisis de requisitos:

No siempre en la etapa de "análisis de requisitos" las distintas metodologías de desarrollo llevan asociado un estudio de viabilidad y/o estimación de costes. El más conocido de los modelos de estimación de coste del software es el modelo COCOMO

Los software tienen la capacidad de emular inteligencia creando un modelo de ciertas características de la inteligencia humana pero solo posee funciones predefinidas que abarcan un conjunto de soluciones que en algunos campos llega a ser limitado. Aun cuando tiene la capacidad de imitar ciertos comportamientos humanos no es capaz de emular el pensamiento humano porque actúa bajo condiciones.

Otro aspecto limitante de los software proviene del proceso totalmente mecánico que requiere de un mayor esfuerzo y tiempos elevados de ejecución lo que lleva a tener que implementar el software en una máquina de mayor capacidad.

La especificación de requisitos describe el comportamiento esperado en el software una vez desarrollado.
Gran parte del éxito de un proyecto de software radicará en la identificación de las necesidades del negocio (definidas por la alta dirección), así como la interacción con los usuarios funcionales para la recolección, clasificación, identificación, priorización y especificación de los requisitos del software.

Entre las técnicas utilizadas para la especificación de requisitos se encuentran:


Siendo los primeros más rigurosas y formales, los segundas más ágiles e informales.

La integración de infraestructura, desarrollo de aplicaciones, bases de datos y herramientas gerenciales, requieren de capacidad y liderazgo para poder ser conceptualizados y proyectados a futuro, solucionando los problemas de hoy. El rol en el cual se delegan todas estas actividades es el del Arquitecto.

El arquitecto de software es la persona que añade valor a los procesos de negocios gracias a su valioso aporte de soluciones tecnológicas.

La arquitectura de sistemas en general, es una actividad de planeación, ya sea a nivel de infraestructura de red y hardware, o de software.

Lo principal en este punto es poner en claro los aspectos lógicos y físicos de las salidas, modelos de organización y representación de datos, entradas y procesos que componen el sistema, considerando las bondades y limitaciones de los recursos disponibles en la satisfacción de las pacificaciones brindadas para el análisis.

Hay que tener en consideración la arquitectura del sistema en la cual se va a trabajar, elaborar un plan de trabajo viendo la prioridad de tiempo y recursos disponibles.
En los diseños de salidas entra los que es la interpretación de requerimientos lo cual es el dominio de información del problema, las funciones visibles para el usuario, el comportamiento del sistema y un conjunto de clases de requerimientos que agrupa los objetos del negocio con los métodos que les dan servicio.

La arquitectura de software consiste en el diseño de componentes de una aplicación (entidades del negocio), generalmente utilizando patrones de arquitectura. El diseño arquitectónico debe permitir visualizar la interacción entre las entidades del negocio y además poder ser validado, por ejemplo por medio de diagramas de secuencia.
Un diseño arquitectónico describe en general el "cómo" se construirá una aplicación de software. Para ello se documenta utilizando diagramas, por ejemplo:

Los diagramas de clases y de base de datos son los mínimos necesarios para describir la arquitectura de un proyecto que iniciará a ser codificado.
Dependiendo del alcance del proyecto, complejidad y necesidades, el arquitecto elegirá cuales de los diagramas se requiere elaborar.

Las herramientas para el diseño y modelado de software se denominan CASE ("Computer Aided Software Engineering") entre las cuales se encuentran:

Implementar un diseño en código puede ser la parte más obvia del trabajo de ingeniería de software, pero no necesariamente es la que demanda mayor trabajo y ni la más complicada.
La complejidad y la duración de esta etapa está íntimamente relacionada al o a los lenguajes de programación utilizados, así como al diseño previamente realizado.

Para el desarrollo de la aplicación es necesario considerar cinco fases para tener una aplicación o programa eficiente, estas son:


Consiste en comprobar que el software realice correctamente las tareas indicadas en la especificación del problema. Una técnica es probar por separado cada módulo del software (prueba unitaria), y luego probarlo de manera integral (pruebas de integración), para así llegar al objetivo.
Se considera una buena práctica el que las pruebas sean efectuadas por alguien distinto al desarrollador que la programó, idealmente un área de pruebas; sin perjuicio de lo anterior el programador debe hacer sus propias pruebas.
En general hay dos grandes maneras de organizar un área de pruebas, la primera es que esté compuesta por personal inexperto y que desconozca el tema de pruebas, de esta manera se evalúa que la documentación entregada sea de calidad, que los procesos descritos son tan claros que cualquiera puede entenderlos y el software hace las cosas tal y como están descritas. El segundo enfoque es tener un área de pruebas conformada por programadores con experiencia, personas que saben sin mayores indicaciones en qué condiciones puede fallar una aplicación y que pueden poner atención en detalles que personal inexperto no consideraría.

De acuerdo con Roger S. Pressman, el proceso de pruebas se centra en los procesos lógicos internos del software, asegurando que todas las sentencias se han comprobado, y en los procesos externos funcionales, es decir, la realización de pruebas para la detección de errores. Se requiere poder probar el software con sujetos reales que puedan evaluar el comportamiento del software con el fin de proporcionar realimentación a los desarrolladores. Es importante que durante el proceso de desarrollo del software no se pierda contacto con los interesados o solicitantes del desarrollo de Software, de esta manera los objetivos del proyecto se mantendrán vigentes y se tendrá una idea clara de los aspectos que tienen que probarse durante el período de pruebas.

Una implementación es la realización de una especificación técnica o algoritmos con un programa, componente software, u otro sistema de cómputo. Muchas especificaciones son dadas según a su especificación o un estándar. Las especificaciones recomendadas según el World Wide Web Consortium, y las herramientas de desarrollo del software contienen implementaciones de lenguajes de programación.
El modelo de implementación es una colección de componentes y los subsitemas que contienen. Componentes tales como: ficheros ejecutables, ficheros de código fuente y todo otro tipo de ficheros que sean necesarios para la implementación y despliegue del sistema.

La etapa de implementación del diseño de software es el proceso de convertir una especificación del sistema en un sistema ejecutable. Siempre implica los procesos de diseño y programación de software, pero, si se utiliza un enfoque evolutivo de desarrollo, también puede implicar un refinamiento de la especificación del software. Esta etapa es una descripción de la estructura del software que se va a implementar, los datos que son parte del sistema, las interfaces entre los componentes del sistema, y algunas veces los algoritmos utilizados.

Es todo lo concerniente a la documentación del propio desarrollo del software y de la gestión del proyecto, pasando por modelaciones (UML), diagramas de casos de uso, pruebas, manuales de usuario, manuales técnicos, etc; todo con el propósito de eventuales correcciones, usabilidad, mantenimiento futuro y ampliaciones al sistema.

Fase dedicada a mantener y mejorar el software para corregir errores descubiertos e incorporar nuevos requisitos. Esto puede llevar más tiempo incluso que el desarrollo del software inicial. Alrededor de 2/3 del tiempo de ciclo de vida de un proyecto está dedicado a su mantenimiento. Una pequeña parte de este trabajo consiste eliminar errores "(bugs)"; siendo que la mayor parte reside en extender el sistema para incorporarle nuevas funcionalidades y hacer frente a su .




La ingeniería de software, con el fin de ordenar el caos que era anteriormente el desarrollo de software, dispone de varios modelos, paradigmas y , estos los conocemos principalmente como modelos o ciclos de vida del desarrollo de software, esto incluye el proceso que se sigue para construir, entregar y hacer evolucionar el software, desde la concepción de una idea hasta la entrega y el retiro del sistema y representa todas las actividades y artefactos (productos intermedios) necesarios para desarrollar una aplicación. 

El ciclo de vida de un software contiene los siguientes procedimientos:

En ingeniería de software el modelo en cascada ―también llamado desarrollo en cascada o ciclo de vida clásico― se basa en un enfoque metodológico que ordena rigurosamente las etapas del ciclo de vida del software, esto sugiere una aproximación sistemática secuencial hacia el proceso de desarrollo del software, que se inicia con la especificación de requisitos del cliente y continúa con la planificación, el modelado, la construcción y el despliegue para culminar en el soporte del software terminado.

En ingeniería de software, el modelo de prototipos pertenece a los modelos de desarrollo evolutivo. Este permite que todo el sistema, o algunos de sus partes, se construyan rápidamente para comprender con facilidad y aclarar ciertos aspectos en los que se aseguren que el desarrollador, el usuario, el cliente estén de acuerdo en lo que se necesita así como también la solución que se propone para dicha necesidad y de esta manera minimizar el riesgo y la incertidumbre en el desarrollo, este modelo se encarga del desarrollo de diseños para que estos sean analizados y prescindir de ellos a medida que se adhieran nuevas especificaciones, es ideal para medir el alcance del producto, pero no se asegura su uso real.

Este modelo principalmente se aplica cuando un cliente define un conjunto de objetivos generales para el software a desarrollarse sin delimitar detalladamente los requisitos de entrada procesamiento y salida, es decir cuando el responsable no está seguro de la eficacia de un algoritmo, de la adaptabilidad del sistema o de la manera en que interactúa el hombre y la máquina.

Este modelo se encarga principalmente de ayudar al ingeniero de sistemas y al cliente a entender de mejor manera cuál será el resultado de la construcción cuando los requisitos estén satisfechos.

El modelo en espiral, que Barry Boehm propuso originalmente en 1986, es un modelo de proceso de software evolutivo que conjuga la naturaleza iterativa de la construcción de prototipos con los aspectos controlados y sistemáticos del modelo en cascada, es decir, cuando se aplica este modelo, el software se desarrolla en una serie de entregas evolutivas (ciclos o iteraciones), cada una de estas entregando prototipos más completas que el anterior, todo esto en función del análisis de riesgo y las necesidades del cliente.
Aunque el modelo espiral representa ventajas por sobre el desarrollo lineal, el cálculo de los riesgos puede ser muy complicado y por lo cual su uso en el ámbito real es muy escaso.

Es un modelo en el que el software se muestra al cliente en etapas refinadas sucesivamente. Con esta metodología se desarrollan las capacidades más importantes reduciendo el tiempo necesario para la construcción de un producto; el modelo de entrega por etapas es útil para el desarrollo de la herramienta debido a que su uso se recomienda para problemas que pueden ser tratados descomponiéndolos en problemas más pequeños y se caracteriza principalmente en que las especificaciones no son conocidas en detalle al inicio del proyecto y por tanto se van desarrollando simultáneamente con las diferentes versiones del código.

En este modelo pueden distinguirse las siguientes fases:

Cuando es por etapas, en el diseño global estas fases pueden repetirse según la cantidad de etapas que sean requeridas.

Entre sus ventajas tenemos:

Desarrollo iterativo y creciente (o incremental) es un proceso de desarrollo de software, creado en respuesta a las debilidades del modelo tradicional de cascada, es decir, este modelo aplica secuencias lineales como el modelo en cascada, pero de una manera iterativa o escalada según como avance el proceso de desarrollo y con cada una de estas secuencias lineales se producen incrementos (mejoras) del software.

Se debe tener en cuenta que el flujo del proceso de cualquier incremento puede incorporar el paradigma de construcción de prototipos, ya que como se mencionó anteriormente, este tipo de modelo es iterativo por naturaleza, sin embargo se diferencia en que este busca la entrega de un producto operacional con cada incremento que se le realice al software.

Este desarrollo incremental es útil principalmente cuando el personal necesario para una implementación completa no está disponible.

Este modelo ―como su nombre lo indica― utiliza las técnicas del diseño estructurado o de la programación estructurada para su desarrollo, también se utiliza en la creación de los algoritmos del programa. Este formato facilita la comprensión de la estructura de datos y su control.
Entre las principales características de este modelo se encuentran las siguientes:
Este modelo también presenta sus desventajas entre las cuales podemos mencionar algunas:
En el modelo estructurado las técnicas que comúnmente se utilizan son:

Estos modelos tienen sus raíces en la programación orientada a objetos y como consecuencia de ella gira entorno al concepto de clase, también lo hacen el análisis de requisitos y el diseño. Esto además de introducir nuevas técnicas, también aprovecha las técnicas y conceptos del desarrollo estructurado, como diagramas de estado y transiciones.
El modelo orientado a objetos tiene dos características principales, las cuales ha favorecido su expansión:

El RAD ("rapid application development:" ‘desarrollo rápido de aplicaciones’), es un modelo de proceso de software incremental, desarrollado inicialmente por James Maslow en 1980, que resalta principalmente un ciclo corto de desarrollo.

Esta es una metodología que posibilita la construcción de sistemas computacionales que combinen técnicas y utilidades CASE (Computer Aided Software Engineering), la construcción de prototipos centrados en el usuario y el seguimiento lineal y sistemático de objetivos, incrementando la rapidez con la que se producen los sistemas mediante la utilización de un enfoque de desarrollo basado en componentes.

Si se entienden bien los requisitos y se limita el ámbito del proyecto, el proceso RAD permite que un equipo de desarrollo cree un producto completamente funcional dentro de un periodo muy limitado de tiempo sin reducir en lo más mínimo la calidad del mismo.

El modelo de desarrollo concurrente es un modelo de tipo de red donde todas las personas actúan simultáneamente o al mismo tiempo. Este tipo de modelo se puede representar a manera de esquema como una serie de actividades técnicas importantes, tareas y estados asociados a ellas.

El modelo de proceso concurrente define una serie de acontecimientos que dispararan transiciones de estado a estado para cada una de las actividades de la ingeniería del software. Por ejemplo, durante las primeras etapas del diseño, no se contempla una inconsistencia del modelo de análisis. Esto genera la corrección del modelo de análisis de sucesos, que disparara la actividad de análisis del estado hecho al estado cambios en espera.
Este modelo de desarrollo se utiliza a menudo como el paradigma de desarrollo de aplicaciones cliente/servidor. Un sistema cliente/servidor se compone de un conjunto de componentes funcionales. Cuando se aplica a cliente/servidor, el modelo de proceso concurrente define actividades en dos dimensiones: una división de sistemas y una división de componentes. Los aspectos del nivel de sistemas se afrontan mediante dos actividades: diseño y realización.

La concurrencia se logra de dos maneras:

En realidad, el modelo de desarrollo concurrente es aplicable a todo tipo de desarrollo de software y proporciona una imagen exacta del estado actual de un proyecto. En vez de confinar actividades de ingeniería de software a una secuencia de sucesos, define una red de actividades, todas las actividades de la red existen simultáneamente con otras. Los sucesos generados dentro de una actividad dada o algún otro lado de la red de actividad inicia las transiciones entre los estados de una actividad.

El proceso unificado es un proceso de software genérico que puede ser utilizado para una gran cantidad de tipos de sistemas de software, para diferentes áreas de aplicación, diferentes tipos de organizaciones, diferentes niveles de competencia y diferentes tamaños de proyectos.

Provee un enfoque disciplinado en la asignación de tareas y responsabilidades dentro de una organización de desarrollo. Su meta es asegurar la producción de software de muy alta calidad que satisfaga las necesidades de los usuarios finales, dentro de un calendario y presupuesto predecible.

El proceso unificado tiene dos dimensiones:
La primera dimensión representa el aspecto dinámico del proceso conforme se va desarrollando, se expresa en términos de fases, iteraciones e hitos (milestones).

La segunda dimensión representa el aspecto estático del proceso: cómo es descrito en términos de componentes del proceso, disciplinas, actividades, flujos de trabajo, artefactos y roles.

El refinamiento más conocido y documentado del proceso unificado es el RUP (proceso unificado racional).

El proceso unificado no es simplemente un proceso, sino un marco de trabajo extensible que puede ser adaptado a organizaciones o proyectos específicos. De la misma manera, el proceso unificado de rational, también es un marco de trabajo extensible, por lo que muchas veces resulta imposible decir si un refinamiento particular del proceso ha sido derivado del proceso unificado o del RUP. Por dicho motivo, los dos nombres suelen utilizarse para referirse a un mismo concepto.

El software se ha convertido en algo muy necesario en nuestra sociedad actual, es la máquina que conduce a la toma de decisiones comerciales, sirve para la investigación científica moderna, es un factor clave que diferencia productos y servicios modernos, etc. Esto se da porque el software está inmerso en sistemas de todo tipo alrededor de nosotros.

El software de computadora es el producto que diseñan y construyen los ingenieros de software. Esto abarca programas que se ejecutan dentro de una computadora de cualquier tamaño y arquitectura, después de estar construido casi cualquier persona en el mundo industrializado, ya sea directa o indirectamente.

Los productos se pueden clasificar en:

Estos productos deben cumplir varias características al ser entregados, estas son:

Lo que constituye el producto final es diferente para el ingeniero y los usuarios, para el ingeniero son los programas, datos y documentos que configuran el software pero para el usuario el producto final es la información que de cierto modo soluciona el problema planteado por el usuario.

La ingeniería de software es una disciplina que está orientada a aplicar conceptos y métodos de ingeniería al desarrollo de software de calidad.

Los programas tienen muchas propiedades matemáticas. Por ejemplo la corrección y la complejidad de muchos algoritmos son conceptos matemáticos que pueden ser rigurosamente probados. El uso de matemáticas en la IS es llamado "métodos formales".

Los programas son construidos en una secuencia de pasos. El hecho de definir propiamente y llevar a cabo estos pasos, como en una línea de ensamblaje, es necesario para mejorar la productividad de los desarrolladores y la calidad final de los programas. Este punto de vista inspira los diferentes procesos y metodologías que se encuentran en la IS.

El desarrollo de software de gran porte requiere una adecuada gestión del proyecto. Hay presupuestos, establecimiento de tiempos de entrega, un equipo de profesionales que liderar. Recursos (espacio de oficina, insumos, equipamiento) por adquirir. Para su administración se debe tener una clara visión y capacitación en gestión de proyectos.

Para el desarrollo de un sistema de software es necesaria la colaboración de muchas personas con diversas competencias, capacidades e intereses. Al conjunto de personas involucradas en el proyecto se les conoce como participantes.

Al conjunto de funciones y responsabilidades que hay dentro del proyecto o sistema se le conoce como roles o papeles. Los roles están asociados a las tareas que son asignadas a los participantes, en consecuencia, una persona puede desempeñar uno o múltiples roles, así también un mismo rol puede ser representado por un equipo.

Es frecuente el uso de los términos "usuarios", "usuarios finales" y "clientes" como sinónimos, lo cual puede provocar confusión; estrictamente, el cliente (persona, empresa u organización) es quién especifica los requisitos del sistema, en tanto que el usuario es quien utiliza u opera finalmente el producto software, pudiendo ser o no el cliente.

Esta clase de participantes están relacionados con todas las facetas del proceso de desarrollo del software. Su trabajo incluye la investigación, diseño, implementación, pruebas y depuración del software.

En el contexto de ingeniería de software, el gestor de desarrollo de software es un participante, que reporta al director ejecutivo de la empresa que presta el servicio de desarrollo. Es responsable del manejo y coordinación de los recursos y procesos para la correcta entrega de productos de software, mientras participa en la definición de la estrategia para el equipo de desarrolladores, dando iniciativas que promuevan la visión de la empresa.

El usuario final es quien interactúa con el producto de software una vez es entregado. Generalmente son los usuarios los que conocen el problema, ya que día a día operan los sistemas.

Un ingeniero de software debe tener un código donde asegura, en la medida posible, que los esfuerzos realizados se utilizarán para realizar el bien y deben comprometerse para que la ingeniería de software sea una profesión benéfica y respetada. Para el cumplimiento de esta norma, se toman en cuenta ocho principios relacionados con la conducta y las decisiones tomadas por el ingeniero; donde estos principios identifican las relaciones éticamente responsables de los individuos, grupos y organizaciones donde participen. Los principios a los que deben sujetarse son sobre la sociedad, cliente y empresario, producto, juicio, administración, profesión, colegas y por último el personal.







</doc>
<doc id="9700" url="https://es.wikipedia.org/wiki?curid=9700" title="Paradigma de programación">
Paradigma de programación

Un paradigma de programación es una propuesta tecnológica adoptada por una comunidad de programadores y desarrolladores cuyo núcleo central es incuestionable en cuanto que únicamente trata de resolver uno o varios problemas claramente delimitados; la resolución de estos problemas debe suponer consecuentemente un avance significativo en al menos un parámetro que afecte a la ingeniería de software.

Un paradigma de programación representa un enfoque particular o filosofía para diseñar soluciones. Los paradigmas difieren unos de otros, en los conceptos y la forma de abstraer los elementos involucrados en un problema, así como en los pasos que integran su solución del problema, en otras palabras, el cómputo.

Tiene una estrecha relación con la formalización de determinados lenguajes en su momento de definición. Es un estilo de programación empleado.

Un paradigma de programación está delimitado en el tiempo en cuanto a aceptación y uso, porque nuevos paradigmas aportan nuevas o mejores soluciones que la sustituyen parcial o totalmente.

El paradigma de programación que actualmente es el más utilizado es la "orientación a objetos" (OO). El núcleo central de este paradigma es la unión de datos y procesamiento en una entidad llamada "objeto", relacionable a su vez con otras entidades "objeto".

Tradicionalmente, datos y procesamiento se han separado en áreas diferente del diseño y la implementación de software. Esto provocó que grandes desarrollos tuvieran problemas de fiabilidad, mantenimiento, adaptación a los cambios y escalabilidad. Con la OO y características como el encapsulado, polimorfismo o la herencia, se permitió un avance significativo en el desarrollo de software a cualquier escala de producción. La OO parece estar ligada en sus orígenes con lenguajes como Lisp y Simula, aunque el primero que acuñó el título de "programación orientada a objetos" fue Smalltalk.

En general la mayoría son variantes de los dos tipos principales, imperativa y declarativa:


Si bien puede seleccionarse la forma pura de estos paradigmas al momento de programar, en la práctica es habitual que se mezclen, dando lugar a la programación multiparadigma o lenguajes de programación multiparadigma.



</doc>
<doc id="9714" url="https://es.wikipedia.org/wiki?curid=9714" title="Herramienta CASE">
Herramienta CASE

Las herramientas CASE ("Computer Aided Software Engineering", Ingeniería de Software Asistida por Computadora) son diversas aplicaciones informáticas o programas informáticos destinadas a aumentar la productividad en el desarrollo de software reduciendo el costo de las mismas en términos de tiempo y de dinero.

Estas herramientas pueden ayudar en todos los aspectos del ciclo de vida de desarrollo del software en tareas como el proceso de realizar un diseño del proyecto, cálculo de costos, implementación de parte del código automáticamente con el diseño dado, compilación automática, documentación o detección de errores entre otras. Ya en los años 70 un proyecto llamado ISDOS diseñó un lenguaje y por lo tanto un producto que analizaba la relación existente entre los requisitos de un problema y las necesidades que estos generaban, el lenguaje en cuestión se denominaba PSL ("Problem Statement Language") y la aplicación que ayudaba a buscar las necesidades de los diseñadores PSA ("Problem Statement Analyzer").

Aunque ésos son los inicios de las herramientas informáticas que ayudan a crear nuevos proyectos informáticos, la primera herramienta CASE fue Excelerator que salió a la luz en el año 1984 y trabajaba bajo una plataforma PC.

Las herramientas CASE alcanzaron su techo a principios de los años 90. En la época en la que IBM había conseguido una alianza con la empresa de software AD/Cycle para trabajar con sus "mainframes" o computadoras centrales, estos dos gigantes trabajaban con herramientas CASE que abarcaban todo el ciclo de vida del software. Pero poco a poco los "mainframes" han ido siendo menos utilizados y actualmente el mercado de las Big CASE ha muerto completamente abriendo el mercado de diversas herramientas más específicas para cada fase del ciclo de vida del software.


Aunque es difícil y existen muchas formas de clasificarlas, las herramientas CASE se pueden clasificar teniendo en cuenta los siguientes parámetros:

La siguiente clasificación es la más habitual basada en las fases del ciclo de desarrollo que cubren:

Existen otros nombres que se le dan a este tipo de herramientas, y que no es una clasificación excluyente entre sí, ni con las fases del ciclo de vida del desarrollo:

Por funcionalidad se pueden diferenciar algunas como:




</doc>
<doc id="9716" url="https://es.wikipedia.org/wiki?curid=9716" title="Estructura de datos">
Estructura de datos

En ciencias de la computación, una estructura de datos es una forma particular de organizar datos en una computadora para que puedan ser utilizados de manera eficiente.

Diferentes tipos de estructuras de datos son adecuados para diferentes tipos de aplicaciones, y algunos son altamente especializados para tareas específicas.

Las estructuras de datos son un medio para manejar grandes cantidades de datos de manera eficiente para usos tales como grandes bases de datos y servicios de indización de Internet. Por lo general, las estructuras de datos eficientes son clave para diseñar algoritmos eficientes. Algunos métodos formales de diseño y lenguajes de programación destacan las estructuras de datos, en lugar de los algoritmos, como el factor clave de organización en el diseño de software.

Las estructuras de datos se basan generalmente en la capacidad de un ordenador para recuperar y almacenar datos en cualquier lugar de su memoria.

Existen numerosos tipos de estructuras de datos, generalmente construidas sobre otras más simples:

La mayoría de los lenguajes ensambladores y algunos lenguajes de bajo nivel, tales como BCPL, carecen de soporte de estructuras de datos. En cambio, muchos lenguajes de alto nivel y algunos lenguajes ensambladores de alto nivel, tales como MASM, tienen algún tipo de soporte incorporado para ciertas estructuras de datos, tales como los registros y arreglos. Por ejemplo, los lenguajes C y Pascal soportan estructuras y registros, respectivamente, además de arreglos y matrices multidimensionales.
La mayoría de los lenguajes de programación disponen de algún tipo de biblioteca o mecanismo que el uso de estructuras de datos en los programas. Los lenguajes modernos por lo general vienen con bibliotecas estándar que implementan las estructuras de datos más comunes. Ejemplos de ello son la biblioteca Standard Template Library de C++, las colecciones de Java y las librerías .NET de Microsoft.

En programación, una estructura de datos puede ser declarada inicialmente escribiendo una palabra reservada, luego un identificador para la estructura y un nombre para cada uno de sus miembros, sin olvidar los tipos de datos que estos representan. Generalmente, cada miembro se separa con algún tipo de operador, carácter o palabra reservada.

En el lenguaje de programación Pauscal, es posible crear una estructura de datos de la forma mencionada. La sintaxis básica es:

Para acceder a los miembros de una estructura, primero se debe crear una referencia a esta, generalmente con una variable de tipo; luego se pueden editar y obtener los datos de los miembros libremente.



</doc>
<doc id="9718" url="https://es.wikipedia.org/wiki?curid=9718" title="Información">
Información

La información es un conjunto organizado de datos procesados, que constituyen un mensaje que cambia el estado de conocimiento del sujeto o sistema que recibe dicho mensaje. Existen diversos enfoques para el estudio de la información:

Los datos sensoriales una vez percibidos y procesados constituyen una información que cambia el estado de conocimiento, eso permite a los individuos o sistemas que poseen dicho estado nuevo de conocimiento tomar decisiones pertinentes acordes a dicho conocimiento.

Desde el punto de vista de la ciencia de la computación, la información es un conocimiento explícito extraído por seres vivos o sistemas expertos como resultado de interacción con el entorno o percepciones sensibles del mismo entorno. En principio la información, a diferencia de los datos o las percepciones sensibles, tienen estructura útil que modificará las sucesivas interacciones del que posee dicha información con su entorno.

La palabra "información" deriva del sustantivo latino "informatio(-nis)" (del verbo "informare", con el significado de "dar forma a la mente", "disciplinar", "istruir", "enseñar"). Ya en latín la palabra "informationis" era usada para indicar un "concepto" o una "idea", pero no está claro si tal palabra pudiera haber influido en el desarrollo moderno de la palabra "información".

Por otra parte la palabra griega correspondiente era "μορφή" ("morfè", de la que por metatesis surgió la palabra latina "forma"), o si no "εἶδος" ("éidos", de la cual deriva la latina "idea"), esto es: "idea", "concepto" o "forma", "imagen"; la segunda palabra fue notoriamente usada técnicamente en el ámbito filosófico por Platón y Aristóteles para indicar la identidad ideal o esencia de algo (véase Teoría de las ideas). "Eidos" se puede también asociar a "pensamiento", "aserción" o "concepto".

En las sociedades humanas y en parte en algunas sociedades animales, la información tiene un impacto en las relaciones entre diferentes individuos. En una sociedad la conducta de cada individuo frente a algunos otros individuos se puede ver alterada en función de qué información disponible posee el primer individuo. Por esa razón, el estudio social de la información se refiere a los aspectos relacionados con la variación de la conducta en posesión de diferentes informaciones.

Para Gilles Deleuze, la información social es un sistema de control, en tanto que es la propagación de consignas que deberíamos de creer o hacer que creemos. En tal sentido la información es un conjunto organizado de datos capaz de cambiar el estado de conocimiento en el sentido de las consignas transmitidas.

En general la información tiene una estructura interna y puede ser calificada según varias características:

La historia de la información está asociada a su producción, tratamiento y transmisión. Una cronología de esa historia detallada puede ser:

Se considera que la generación y/o obtención de información persigue estos objetivos:


En relación con el tercer punto, la información como vía para llegar al conocimiento, debe ser elaborada para hacerla utilizable o disponible (este proceso empírico se llama Documentación y tiene métodos y herramientas propios), pero también es imposible que la información por sí sola dote al individuo de más conocimiento, es él quien valora lo significativo de la información, la organiza y la convierte en conocimiento. El dato, por así llamarlo, es en sí un "prefijo" de la información, es decir, es un elemento previo necesario para poder obtener la información.

Una noticia es el relato o redacción de un texto informativo que se quiere dar a conocer con sus propias reglas de construcción (enunciación) que se refiere a un hecho novedoso o atípico -o la relación entre hechos novedosos y/o atípicos-, ocurrido dentro de una comunidad o determinado ámbito específico, que hace que merezca su divulgación.

La noticia es un hecho periodístico, equivalente a lo que implica para la historia un acontecimiento. Dentro del ámbito de algunos medios de comunicación, es un género periodístico en el que la noticia es un "recorte de la realidad" sobre un hecho de actualidad que merece ser informado por algún tipo de criterio de relevancia social. El contenido de una noticia debe responder a la mayoría de las preguntas que se conocen como las "6W-H", de la escuela de periodismo norteamericana:


La información es un fenómeno que proporciona significado o sentido a las cosas. En sentido general, la información es un conjunto organizado de datos procesados, que constituyen un mensaje sobre un determinado ente o fenómeno. Los datos se perciben, se integran y generan la información necesaria para producir el conocimiento que es el que finalmente permite tomar decisiones para realizar las acciones cotidianas que aseguran la existencia. La sabiduría consiste en determinar correctamente cuándo, cómo, dónde y con qué objetivo emplear el conocimiento adquirido.

La información también indica mediante códigos y conjuntos de datos, los modelos del pensamiento humano. La información por lo tanto, procesa y genera el conocimiento humano. De esta manera, si por ejemplo organizamos datos sobre un país, tales como: número de habitantes, densidad de población, nombre del presidente, etc. y escribimos por ejemplo, el capítulo de un libro, podemos decir que ese capítulo constituye información sobre ese país.

El control y la manipulación es uno de los medios más poderosos de los gobiernos para promover el acatamiento de sus políticas. Así, los estados totalitarios y autoritarios buscan el monopolio de la información para promover el acatamiento de las políticas.
La información tiene por objetivo dar a conocer los hechos de manera efectiva e imparcial, mientras que la propaganda busca ganar adeptos para lograr un objetivo, sin importarle la veracidad de los hechos. Así la propaganda compite con el derecho como instrumento de poder.

El enfoque de la teoría de la información analiza la estructura matemática y estadística de los mensajes, con independencia de su significado u otros aspectos semánticos. Los aspectos en los que se interesa la teoría de la información son la capacidad de transmisión de los canales, la compresión de datos o la detección y corrección de errores.

Una forma de caracterizar nuestro estado de conocimiento del mundo, es a través de las probabilidades. Si sabemos que en el futuro pueden suceder "n" cosas diferentes formula_1, cada una con probabilidad formula_2 ese conjunto de probabilidades constituyen nuestro conocimiento del mundo, una información debería reducir nuestra incertidumbre, variando las probabilidades a formula_3. Si el segundo estado tiene menos incertidumbre es porque algunas cosas se han hecho más probables frente a otras alternativas que se han hecho menos probables.

Una forma de "medir la información" asociada a un mensaje o hecho observado es calcular como algunas probabilidades han crecido y otras han decrecido. Una medida conveniente de calcular la "concentración" de la certeza en algunas alternativas es la entropía estadística:

Un ejemplo lingüístico ilustra bien este caso. Supongamos que nos proponen adivinar la segunda letra de una palabra del español. y nos dicen que en la segunda posición podría aparecer cualquier letra del alfabeto. Por tanto la incertidumbre inicial se obtiene calculando la y calculando:

Sin embargo, si nos dan como pista que "la primera letra es una Z", entonces en segunda posición sólo puede aparecer A, O, U (aunque existen un puñado de casos excepcionales de "E" e "I") y por tanto con esa información se reduce mucha la incertidumbre:

La información cuantificada de la pista "la primera letra es una Z" resulta ser:

Las unidades de información resultan ser bits puesto que se han empleado logaritmos de base 2. Si la pista hubiera sido "la primera letra es una M", la segunda letra sólo podría ser A, E, I, O, U que es un conjunto más amplio y en este caso formula_4 y en este caso, la pista lleva menos información porque reduce menos la incertidumbre, el resultado en este caso es repitiendo los pasos anteriores de unos 2,50 bits.

La cantidad de información y el conocimiento desarrollado, aparentemente es enorme y tiene una metodología de recuperación, que eventualmente es infinita o total en un número muy amplio de soportes y sitios y el modelo sistémico de recuperación debe maximizar la búsqueda para asegurar su captura lo más completa posible dentro del entorno de este sistema complejo. En el caso de búsquedas en Internet y usando dos o más descriptores, los resultados numéricos que dan los motores de búsqueda, que contengan los dos o más términos juntos o muy próximos, ya es una medida de la cantidad de información conseguida y que es en expresión matemática el "ln" o logaritmo natural de la suma de las interacciones validadas. Valores de 2 o 3 serán óptimos.

En física existe una íntima conexión entre entropía e información:

Léon Brillouin publicó en 1959 "Science et théorie de l'information" (versión en inglés editada por vez primera en 1962) donde son examinadas las relaciones entre estas dos disciplinas. Adopta particularmente un punto de vista de físico y hace el lazo entra la entropía informacional de Shannon y la entropía estadística de Boltzmann en donde se arriesga que la información (y con la misma el lenguaje) es un factor neguentrópico es decir por el cual se puede anular la entropía.





</doc>
<doc id="9719" url="https://es.wikipedia.org/wiki?curid=9719" title="Ingeniería de sistemas">
Ingeniería de sistemas

La ingeniería de sistemas es un modo de enfoque interdisciplinario que permite estudiar y comprender la realidad, con el propósito de implementar u optimizar sistemas complejos. Puede también verse como la aplicación tecnológica de la teoría de sistemas a los esfuerzos de la ingeniería, adoptando en todo este trabajo el paradigma sistémico. La ingeniería de sistemas integra otras disciplinas y grupos de especialidad en un esfuerzo de equipo, formando un proceso de desarrollo centrado.

La Ingeniería de Sistemas tiene, como campo de estudio, cualquier sistema existente. Por ejemplo, la ingeniería de sistemas, puede estudiar el sistema digestivo o el sistema inmunológico humano, o quizá, el sistema tributario de un país específico. En este sentido si bien en algunos países se asocia ingeniería de sistemas como únicamente asociada a los sistemas informáticos, ello es incorrecto, ya que los sistemas informático son una pequeña parte de un enorme abanico de tipos y clases de sistemas.

La ingeniería de sistemas es la aplicación de las ciencias matemáticas y físicas para desarrollar sistemas que utilicen económicamente los materiales y fuerzas de la naturaleza para el beneficio de la humanidad.

Una de las principales diferencias de la ingeniería de sistemas respecto a otras disciplinas de ingeniería tradicionales, consiste en que la ingeniería de sistemas no construye productos tangibles. Mientras que los ingenieros civiles podrían diseñar edificios o puentes, los ingenieros electrónicos podrían diseñar circuitos, los ingenieros de sistemas tratan con sistemas abstractos con ayuda de las metodologías de la ciencia de sistemas, y confían además en otras disciplinas para diseñar y entregar los productos tangibles que son la realización de esos sistemas.

El origen del término ingeniería de sistemas se remonta a los Bell Telephone Laboratories en la década de 1940. La necesidad de identificar y manipular las propiedades de un sistema como un todo, que en proyectos de ingeniería complejos puede diferir enormemente de la suma de las propiedades de las partes, motivó a varias industrias, especialmente aquellas que desarrollaban sistemas para el Ejército de los Estados Unidos, a aplicar la disciplina.

Cuando ya no era posible confiar en la evolución del diseño para mejorar un sistema y las herramientas existentes no eran suficientes para satisfacer las crecientes demandas, se empezaron a desarrollar nuevos métodos que abordaban la complejidad directamente. La evolución continua de la ingeniería de sistemas comprende el desarrollo y la identificación de nuevos métodos y técnicas de modelado. Estos métodos ayudan a una mejor comprensión y al control del diseño y desarrollo de los sistemas de ingeniería a medida que se vuelven más complejos. En estos tiempos se desarrollaron herramientas populares que a menudo se usan en el contexto de la ingeniería de sistemas, incluidas USL, UML, QFD e IDEF0.

La ingeniería de Sistemas comenzó a desarrollarse en la segunda parte del siglo XX con el veloz avance de la ciencia de sistemas. Las empresas comenzaron a tener una creciente aceptación de que dicha ingeniería, podía gestionar el comportamiento impredecible y la aparición de características imprevistas de los equipos y proyectos con niveles de complejidad cada vez mayores (propiedades emergentes). Las decisiones tomadas al comienzo de un proyecto, cuyas consecuencias pueden no haber sido entendidas claramente, tienen una enorme implicación más adelante en la vida de un sistema. Un ingeniero de sistemas debe explorar estas cuestiones y tomar decisiones críticas.

Si bien inicialmente la ingeniería de sistemas solo era considerada un método, recientemente se le ha comenzado a considerar una disciplina dentro de la ingeniería. El objetivo de la enseñanza de la ingeniería de sistemas es formalizar diversas metodologías y de esta forma identificar métodos novedosos y oportunidades de investigación de forma similar a lo que se hace en otras ramas de la ingeniería. Como metodología, la ingeniería de sistemas posee una fuerte impronta holística e interdisciplinaria.

El alcance tradicional de la ingeniería comprende la concepción, diseño, desarrollo, producción y operación de los sistemas físicos. La ingeniería de sistemas, tal como se la concibió inicialmente, se encuentra dentro de dicho alcance. La "ingeniería de sistemas", en este sentido, se refiere al conjunto de conceptos distintivos, metodologías, estructuras organizacionales que han sido desarrolladas para enfrentar los desafíos de desarrollar la ingeniería de sistemas funcionales efectivos de dimensiones y complejidad sin precedentes dentro del tiempo, presupuesto, y otras limitaciones. El programa Apolo es un ejemplo importante de un proyecto de grandes dimensiones y complejidad organizado entorno a un enfoque de ingeniería de sistemas.

El uso del término "ingeniero de sistemas" ha evolucionado con el tiempo para abarcar un concepto más amplio y holístico de "sistemas" y de procesos de ingeniería. Esta evolución de la definición ha sido un tema de constante controversia, y el término continúa aplicándose tanto al alcance más restringido como al más amplio.

La ingeniería de sistemas tradicional se veía como una rama de la ingeniería en el sentido clásico, es decir, se aplicaba únicamente a sistemas físicos, como las naves espaciales y los aviones. Más recientemente, la ingeniería de sistemas ha evolucionado para adquirir un significado más amplio, especialmente cuando los seres humanos son vistos como un componente esencial de un sistema. Checkland, por ejemplo, capta el significado más amplio de la ingeniería de sistemas al afirmar que la "ingeniería" puede leerse en su sentido general: puede diseñar una reunión o un acuerdo político ".

De acuerdo con el alcance más amplio de la ingeniería de sistemas, el "Cuerpo de Conocimiento de Ingeniería de Sistemas" (SEBoK-"Systems Engineering Body of Knowledge") ha definido tres tipos de ingeniería de sistemas: (1) Ingeniería de Sistemas de Producto (PSE) es la ingeniería de sistemas tradicional centrada en el diseño de sistemas físicos que consiste en hardware y software. (2) Enterprise Systems Engineering (ESE) se refiere a la visión de las empresas, es decir, organizaciones o combinaciones de organizaciones, como sistemas. (3) La Ingeniería de Sistemas de Servicio (SSE) tiene que ver con la ingeniería de los sistemas de servicio. Checkland define un sistema de servicio como un sistema que se concibe para proveer servicio a otro sistema. La mayoría de los sistemas de infraestructura civil son sistemas de servicio.

La ingeniería de sistemas se enfoca en analizar y precisar las necesidades del cliente y la funcionalidad requerida al principio del ciclo de desarrollo, documentar los requerimientos y luego continuar con la síntesis del diseño y la validación del sistema al considerar el problema en su completitud, el ciclo de vida del sistema. Esto comprender por completo a todas las partes interesadas involucradas en el proyecto. Oliver, afirma que el proceso de ingeniería de sistemas se puede descomponer en


En el modelo de Oliver, el objetivo del Proceso de Gestión es organizar el esfuerzo técnico en el ciclo de vida, mientras que el Proceso Técnico incluye "evaluar la información disponible", "definir medidas de efectividad", "crear un modelo de comportamiento", "crear un modelo de estructura", "realizar un análisis de compromiso", y "crear un plan secuencial de construcción y ensayo".

Dependiendo de su aplicación, aunque hay varios modelos que se utilizan en la industria, todos ellos tienen como objetivo identificar la relación entre las diversas etapas mencionadas anteriormente e incorporar retroalimentación. Ejemplos de tales modelos incluyen el modelo de desarrollo en cascada y el modelo VEE.

El desarrollo del sistema a menudo requiere la contribución de diversas disciplinas técnicas. Al proporcionar una visión de sistemas (holística) del desarrollo, la ingeniería de sistemas ayuda a moldear a todos los contribuyentes técnicos en un esfuerzo unificado de equipo, formando un proceso de desarrollo estructurado que comprende desde el concepto hasta la producción y operación y, en algunos casos, hasta la terminación y eliminación . En una adquisición, la disciplina integradora integral combina contribuciones y equilibra las decisiones que compiten afectando el costo, cronograma y eficiencia, al tiempo que mantiene un nivel aceptable de riesgo que abarca todo el ciclo de vida del artículo.

Esta perspectiva a menudo se replica en los programas educativos, ya que los cursos de ingeniería de sistemas son impartidos por profesores de otros departamentos de ingeniería, lo que ayuda a crear un entorno interdisciplinario.

La necesidad de la ingeniería de sistemas surgió con el aumento de la complejidad de los sistemas y proyectos, a su vez aumentando exponencialmente la posibilidad de problemas entre diversos componentes y, por lo tanto, la falta de fiabilidad del diseño. Al hablar en este contexto, la complejidad incorpora no solo los sistemas de ingeniería, sino también la organización lógica humana de los datos. Al mismo tiempo, un sistema puede volverse más complejo debido a un aumento en el tamaño así como a un aumento en la cantidad de datos, variables o la cantidad de campos que están involucrados en el diseño. La Estación Espacial Internacional es un ejemplo de un sistema con tales características.

El desarrollo de algoritmos de control más inteligentes, el diseño de microprocesadores, y el análisis de sistemas del medio ambiente también caen dentro del ámbito de la ingeniería de sistemas. La ingeniería de sistemas promueve el uso de herramientas y métodos para comprender y gestionar mejor la complejidad de los sistemas. Algunos ejemplos de estas herramientas son:

Las herramientas de las que se sirve la ingeniería de sistemas son estrategias, procedimientos, y técnicas que ayudan a llevar a cabo la ingeniería de sistemas que requiere un proyecto o producto. El objetivo de estas herramientas abarca un amplio espectro, que comprende gestión de bases de datos, navegación de sistemas de información en forma gráfica, simulación, y razonamiento, para documentar producción, procesos neutrales de exportación /importación entre otros.

Existen numerosas definiciones de que constituye un sistema en el ámbito de la ingeniería de sistemas. Alguna definiciones enunciadas por organismos relevantes son:


Muchos de los campos relacionados podrían ser considerados con estrechas vinculaciones a la ingeniería de sistemas. Muchas de estas áreas han contribuido al desarrollo de la ingeniería de sistemas como área independiente.

Un sistema de información o (SI) es un conjunto de elementos que interactúan entre sí con el fin de apoyar las actividades de una empresa o negocio. No siempre un Sistema de Información debe estar automatizado (en cuyo caso se trataría de un sistema informático), y es válido hablar de Sistemas de Información Manuales. Normalmente se desarrollan siguiendo Metodologías de Desarrollo de Sistemas de Información.

El equipo computacional: el hardware necesario para que el sistema de información pueda operar.
El recurso humano que interactúa con el Sistema de Información, el cual está formado por las personas que utilizan el sistema.

Un sistema de información realiza cuatro actividades básicas: entrada, almacenamiento, procesamiento y salida de información.

Es la actualización de datos reales y específicos para la agilización de operaciones en una empresa.

La investigación de operaciones o (IO) se enseña a veces en los departamentos de ingeniería industrial o de matemática aplicada, pero las herramientas de la IO son enseñadas en un curso de estudio en Ingeniería de Sistemas. La IO trata de la optimización de un proceso arbitrario bajo múltiples restricciones.Se presentan las ideas fundamentales en las que se basa el enfoque de sistemas, los tipos de problemas de sistemas y las metodologías más adecuadas para abordarlos.

La ingeniería de sistemas cognitivos es una rama de la ingeniería de sistemas que trata los entes cognitivos, sean humanos o no, como un tipo de sistemas capaces de tratar información y de utilizar recursos cognitivos como la percepción, la memoria o el procesamiento de información. Depende de la aplicación directa de la experiencia y la investigación tanto en psicología cognitiva como en ingeniería de sistemas. La ingeniería de sistemas cognitivos se enfoca en cómo los entes cognitivos interactúan con el entorno. La ingeniería de sistemas trabaja en la intersección de:


Algunas veces designados como ingeniería humana o ingeniería de factores humanos, esta rama además estudia la ergonomía en diseño de sistemas. Sin embargo, la ingeniería humana suele tratarse como otra especialidad de la ingeniería que el ingeniero de sistemas debe integrar.

Habitualmente, los avances en ingeniería de sistemas cognitivos se desarrollan en los departamentos y áreas de informática, donde se estudian profundamente e integran la inteligencia artificial, la ingeniería del conocimiento y el desarrollo de interfaces hombre-máquina (diseños de usabilidad) de la ciencia

El Ingeniero de sistemas habitualmente aprende a programar, para dirigir a programadores y al momento de la creación de un programa debe saber y tener en cuenta los métodos básicos como tal, por eso es importante que aprenda a programar pero su función realmente es el diseño y planeación, y todo lo referente al sistema o redes, su mantenimiento y efectividad, respuesta y tecnología.




</doc>
<doc id="9720" url="https://es.wikipedia.org/wiki?curid=9720" title="Organización">
Organización

Las organizaciones son estructuras administrativas y sistemas administrativos creadas para lograr metas u objetivos por medio de los organismos humanos o de la gestión del talento humano y de otro tipo. Están compuestas por sistemas de interrelaciones que cumplen funciones especializadas. También es un convenio sistemático entre personas para lograr algún propósito específico.

Las organizaciones son el objeto de estudio de la Ciencia de la Administración, a su vez de otras disciplinas tales como la Comunicación, la Sociología, la Economía y la Psicología.

Una organización, es un grupo social compuesto por personas naturales, tareas y administraciones que forman una estructura sistemática de relaciones de interacción, tendientes a producir bienes, servicios o normativas para satisfacer las necesidades de una comunidad dentro de un entorno, y así poder lograr el propósito distintivo que es su misión. Es un sistema de actividades conscientemente coordinadas formado por dos o más personas; la cooperación entre ellas es esencial para la existencia de la organización. Una organización sólo existe cuando hay personas capaces de comunicarse y que están dispuestas a actuar conjuntamente para lograr un objetivo común. Es un conjunto de cargos con reglas y normas de comportamiento que han de respetar todos sus miembros, y así generar el medio que permite la acción de una empresa. La organización es el acto de disponer y coordinar los recursos disponibles (materiales, humanos y financieros). Funciona mediante normas y bases de datos que han sido dispuestas para estos propósitos.

Existen varias escuelas filosóficas que han estudiado la organización como sistema social y como estructura de acción, tales como el estructuralismo y el empirismo. Para desarrollar una teoría de la organización es preciso primero establecer sus leyes o al menos principios teóricos para así continuar elaborando una teoría sobre ellos. Un camino sería clasificar y mostrar las diferentes formas de organizaciones que han sido más , tales como la burocracia como administración o elementos que componen la organización y que igualmente han sido ya muy tratados, tales como el liderazgo formal e informal. Como metodología, esto se llama investigación operativa y en el ámbito de las ciencias sociales es el campo de estudio de la "sociología de la organización". Un nuevo uso está emergiendo en las organizaciones: la gestión del conocimiento. Típicamente, la organización está en todas partes, lo que dificulta su definición independiente o sin involucrarse en una aplicación particular.

Los fundamentos básicos que demuestran la importancia de la organización son:

Los elementos principales de toda organización son los siguientes:

Todas y cada una de las actividades establecidas en la organización deben relacionarse con los objetivos y propósitos de la empresa.

Fue establecido por Adam Smith hace 200 años. El trabajo se realizará más fácilmente si se subdivide en claramente relacionadas y delimitadas.

Es necesario establecer centros de autoridad de los que emane la comunicación necesaria para lograr los planes.

A cada grado de responsabilidad conferido, debe corresponder el grado de autoridad necesarios para cumplir dicha responsabilidad

Establece determinar un centro de autoridad y decisión para cada función



Dentro de las diferentes maneras de clasificación se encuentran las de tamaño, ya sea por la producción, capital, volumen de ventas y principalmente personal ocupado. La más común es la que se basa en el número de empleados:
Éstas pueden ser: nacionales, extranjeras, multinacionales o globalizadas, así como controladoras, franquiciadas o familiares.

Esta clasificación depende del propósito por el que fueron creadas además del origen de las aportaciones a su capital:

Dependiendo de su influencia económica:
Produce bienes mediante la extracción o la transformación de materias primas. De éstas se puede hacer otra clasificación en:
Son las empresas que actúan como intermediarias entre el productor y el consumidor. Su objetivo es la compra-venta de productos ya fabricados y su distribución. Estas se clasifican en:
La finalidad de las empresas de servicios es brindar un servicio con o sin fines de lucro. Por ejemplo: salud, transporte, educación, etc.

Tanto los valores como la filosofía están relacionados con la cultura de la organización, ya que dependiendo de esto pueden ser lucrativas o no lucrativas, por lo tanto se clasifican en:

Esta clasificación depende del nivel de tecnificación con el que cuente cada empresa.





Son los necesarios para desarrollar sus actividades al llevar a cabo su fin, difieren según sus actividades.

Recursos:




</doc>
<doc id="9721" url="https://es.wikipedia.org/wiki?curid=9721" title="Teseo">
Teseo

Teseo (en griego antiguo Θησεύς, "el que funda") fue un mítico , hijo de Etra y Egeo, aunque según otra tradición su padre fue Poseidón, el dios del mar.

El rey Egeo, que no había tenido descendencia con su esposa, consultó al oráculo de Delfos, que le respondió: «No abras tu odre hasta que regreses a Atenas.» Él no comprendió el oráculo pero Piteo, rey de Trecén y padre de Etra, sí lo entendió. Lo que el oráculo había querido decir era que si llegaba a Atenas sin haber hecho el amor , la primera mujer con la que yaciera tendría un heredero suyo. Como Piteo deseaba que su hija diera a luz al heredero del trono ateniense emborrachó a Egeo, y así consiguió que fecundara a Etra. 

En la noche en la que quedó embarazada, se creía que también Poseidón la había poseído. El dios la sorprendió en la isla de Esferia, a donde había ido, debido a un sueño, con el propósito de ofrecer libaciones sobre la tumba de Esfero. Etra dedicó por ello en la isla un templo a Atenea Apaturia y llamó a la isla Hiera en lugar de Esferia, introduciendo también entre las doncellas de Trecén la costumbre de dedicar sus zónulas a Atenea Apaturia en el día de su matrimonio. Según Plutarco, Piteo difundió esta versión sólo para que Teseo fuese considerado hijo de Poseidón, quien era muy reverenciado en su tierra. Egeo regresó a Atenas y Etra crio a su hijo en Trecén. 

Tras la concepción de Teseo, Egeo, por temor a los Palántidas, sus sobrinos, que querían el trono, decidió que su hijo no pasara la niñez con él y escondió su espada y sus sandalias bajo una roca que el niño no debía mover hasta que fuera lo suficientemente fuerte. Así que la infancia de Teseo transcurrió en compañía de su madre y su abuelo en la ciudad de Trecén. Cuando cumplió los dieciséis años su madre le reveló el secreto de su paternidad y llegado a esta edad, Teseo pudo levantar la piedra, calzarse las sandalias y envainar la espada de su padre e iniciar su viaje a Atenas para ser reconocido como hijo del rey.

Teseo fue un héroe fundador, como Perseo o Cadmo. Es uno de los personajes principales de "El sueño de una noche de verano" y "Los dos nobles caballeros, de William Shakespeare.

Teseo, que desde muy joven había destacado por su fuerza y su valentía, decidió dirigirse a Atenas en solitario para conocer a su progenitor sin temer los peligros que podía entrañar el viaje. Al contrario, deseaba emular las hazañas de su admirado Heracles, a quien le unió una buena amistad.
El primero en experimentar su valor fue Perifetes hijo de Hefesto, el salteador de caminos, que, a pesar de que era cojo, dominaba a la perfección una enorme maza de bronce con la que mataba a los viajeros: la misma maza que tan útil le sería a Teseo en el futuro, pues se quedó con ella tras darle muerte.

Otro de los bandidos a los que debió enfrentarse fue Sinis, el doblador de pinos, que tenía una manera peculiar de deshacerse de sus enemigos: doblaba dos pinos próximos, ataba las copas entre sí y un brazo de su víctima a cada una de ellas. Luego soltaba los árboles que, al enderezarse violentamente, desgarraban el cuerpo del desgraciado. Teseo, después de acabar con Sinis de la misma manera en que él acababa a sus víctimas, mantuvo una relación amorosa con su hija Perigune de quien tuvo un hijo: Melanipo.

Después le tocó enfrentarse a Esciro, hijo de Pélope y descendiente de Tántalo, quien obligaba a los viajeros a lavarle los pies en el mar. Luego los arrojaba al mar donde una tortuga al servicio de Hades los devoraba. Teseo se negó y cogiéndolo por los pies lo tiró al mar.

Cerca del pueblo de Eleusis, un bandido llamado Cerción retaba a los viajeros a luchar con él en un duelo desigual y nadie lo vencía. Solo Teseo lo hizo, levantándolo y arrojándolo fuertemente contra el suelo.

No lejos de ahí vivía Procustes, un hermoso bandido que tenía el hábito de tomar a los transeúntes para deformarlos. Primero los seducía, los ataba a la cama y amordazaba; en ella daba entonces comienzo a una atroz tortura. A los altos los metía en una cama pequeña y les cortaba los pies y las manos hasta que cabían. A los más pequeños los metía en una cama grande y los estiraba con cuerdas y a martillazos. Teseo lo mató de la misma forma en que él mató a sus víctimas: lo sedujo con juegos, lo ató y amordazó en la cama más pequeña, dada su altura. Luego lo torturó con el martillo, le cortó los pies y finalmente la cabeza.
También mató a la Cerda de Cromio, que era una fiera hija de Tifón y Equidna. 

Teseo continuó su viaje y llegó a Atenas, pero se encontró con un inconveniente: su padre se había casado con Medea, la que había sido esposa de Jasón. De esta unión había nacido un hijo al que habían llamado Medo.

Ante esta situación inesperada, Teseo decidió esperar un poco antes de darse a conocer. Pero Medea, que era hechicera, lo reconoció y vio en él un peligro para que su hijo accediera al trono de Atenas. Así que trazó un plan. 
El joven había acudido al palacio de incógnito precisamente para evitar los ardides de su madrastra, lo que aprovechó esta para convencer a Egeo de que el recién llegado era un traidor. El rey se dispuso entonces a deshacerse de él ordenándole luchar contra el toro de Maratón.

Pero el toro fue derrotado y Teseo fue invitado a un banquete en el palacio para celebrar la victoria. Una vez allí Egeo puso veneno que le había dado Medea en la copa del muchacho pero la casualidad salvó su vida. Para cortar la carne, Teseo sacó la espada que le había dado su madre. Entonces Egeo reconoció el arma, comprendió lo que ocurría y arrebató a su hijo la copa de los labios. Habiendo fracasado en su empresa, Medea decidió huir con su hijo o fue expulsada por Egeo.

Teseo fue reconocido oficialmente como hijo y sucesor del rey, lo que provocó la rebelión de los hijos de Palante, hermano de Egeo, los Palántidas, ya que uno de ellos habría sido el sucesor en caso de que Egeo no hubiera tenido descendencia. Teseo, haciendo alarde de su astucia militar, consiguió acorralar a sus adversarios y dar muerte a gran parte de ellos, y los restantes se dieron a la fuga. Teseo fue aclamado por todos los atenienses y reconocido como futuro rey.

Atenas debía enviar un tributo al rey Minos de Creta, que consistía en el sacrificio de siete doncellas y siete jóvenes, que serían devorados por el monstruo Minotauro, y que fue una condición impuesta tras la expedición militar de Minos contra Atenas para vengar la muerte de Androgeo. 

Teseo se presentó voluntariamente en el tercer envío ante su padre para que le permitiera ser parte de la ofrenda y lo dejara acompañar a las víctimas para poder enfrentarse al Minotauro. 

Las naves en las que iban a viajar las personas ofrendadas llevaban velas negras como señal de luto, pero el rey pidió a Teseo que si regresaba vencedor, no olvidase cambiarlas por velas blancas, para que supiera, aún antes de que llegase a puerto, que estaba vivo. Teseo se lo prometió. 

Durante la travesía, Minos, que iba también en la expedición, se enamoró de una joven llamada Eribea o Peribea, según las fuentes. Minos quiso unirse a ella por la fuerza y Teseo se le opuso. En la consiguiente disputa, Minos indicó a Teseo su filiación divina, y obtuvo de su padre Zeus truenos y relámpagos. Teseo replicó que él también tenía filiación divina, puesto que en realidad era hijo de Poseidón. Para probar esta filiación, Teseo tuvo que tirarse al agua y encontrar un anillo de oro que el rey Minos había arrojado al mar. Teseo, en el mar, fue conducido por delfines a presencia de Anfítrite, esposa de Poseidón, que le dio el anillo y una corona.

Al llegar a Creta, la princesa Ariadna se enamoró de él y propuso a Teseo ayudarle a derrotar a su hermano (el Minotauro) a cambio de que se la llevara con él de vuelta a Atenas y la convirtiera en su esposa. Teseo aceptó. 

La ayuda de Ariadna consistió en dar a Teseo un ovillo de hilo que este ató por uno de los extremos a la puerta del laberinto. Otra versión indica que la ayuda de Ariadna consistió en una corona que emitía un resplandor y que le había dado Dioniso como regalo de boda o bien que podría ser la misma corona que le había regalado Anfítrite durante el viaje a Creta.

Así Teseo entró en el laberinto hasta encontrarse con el Minotauro, al que dio muerte a puñetazos. A continuación recogió el hilo y así pudo salir del laberinto e inmediatamente, acompañado por el resto de atenienses y por Ariadna, embarcó de vuelta a Atenas, tras hundir los barcos cretenses para impedir una posible persecución.

Durante el viaje de vuelta, Teseo decidió desembarcar en la isla de Naxos o en otra isla llamada Día y de allí volvió a partir sin la presencia de Ariadna. El motivo de este abandono es controvertido: algunas versiones señalan que Teseo la abandonó por su propia voluntad, otros dicen que fue por orden de los dioses para que esta pudiera casarse con Dioniso.

Al divisar la galera desde el puerto de El Pireo en Atenas, el rey Egeo vio las velas negras puesto que Teseo había olvidado cambiarlas por velas blancas y, creyendo que su hijo había muerto, se suicidó lanzándose al mar, que a partir de entonces recibió el nombre de mar Egeo.

Teseo, a partir de entonces, heredó el trono de Atenas y años después se casaría con una hermana de Ariadna llamada Fedra.

Después de que Hércules obtuviese en uno de sus doce trabajos el cinturón de la amazona Hipólita, Teseo, que participó en la expedición, secuestró a una amazona llamada Antíope, o bien Melanipa o bien Hipólita. Las amazonas atacaron entonces Atenas para rescatar a la raptada, pero fueron derrotadas por los atenienses, muriendo en algunas versiones la amazona raptada durante el ataque.

Teseo se casó con Antíope, con Melanipa o con Hipólita y tuvo un hijo llamado Hipólito. Pero después terminaría casándose con Fedra, tras haber abandonado a su anterior esposa. En la versión en la que Teseo está casado con Hipólita y la abandona, esta intentó vengarse llevando a las amazonas a la boda de Teseo y Fedra con la intención de matar a todos, aunque fracasó al ser asesinada por los invitados de Teseo.

Hipólito, el hijo que Teseo había tenido con la amazona, se distinguía por su pasión por la caza y las artes violentas. Veneraba a Artemisa, diosa virgen de la caza, y en cambio detestaba a la diosa del amor Afrodita. La diosa, ofendida por el desprecio del chico, suscitó una terrible pasión por el mismo en el corazón de Fedra, que se había convertido en esposa de Teseo y por lo tanto madrastra de Hipólito. Estando Teseo ausente, Fedra se ofreció al casto joven, pero este la despreció. La mujer, despechada, se ahorcó dejando una nota inculpatoria en la que decía que Hipólito había tratado de violarla. Al regresar Teseo y ver la falsa acusación contra su hijo, creyó en ella y clamó venganza a Poseidón, que envió a Hipólito un toro que brotó del mar mientras este cabalgaba en su carro; el carro volcó e Hipólito fue aplastado por sus propios caballos.

En algunas versiones fue en este momento cuando Fedra se suicidó, al ver el mal que había causado.

Pirítoo había oído hablar de la fama de Teseo y para comprobarla robó ganado que pertenecía a este último. Cuando Teseo lo persiguió, Pirítoo estaba dispuesto a enfrentarse a él pero antes de ello surgió entre ellos una admiración mutua que hizo que se juraran amistad eterna. 

Teseo y Pirítoo fueron amigos inseparables, y participaron juntos en hazañas bélicas de su época: participaron en la expedición de los Argonautas para conquistar el Vellocino de oro y tomaron parte en la caza del jabalí de Calidón, también estuvieron en la lucha de los lapitas contra los centauros, que tuvo lugar en la boda de Pirítoo, cuando los ebrios centauros decidieron raptar a las mujeres.

Decidieron casarse cada uno con una hija de Zeus: Teseo con Helena, que aún era una niña, y Pirítoo con Perséfone. Primero raptaron a Helena y la dejaron bajo la custodia de Etra, y luego decidieron bajar al Hades en busca de Perséfone. Pero el dios Hades les tendió una trampa: les invitó a un banquete y una vez que los tuvo sentados a la mesa, los dejó adheridos a los asientos. Cuando Hércules, en su duodécimo trabajo, fue en busca de Cerbero, estando en el Hades, los encontró encadenados. Al ver a Hércules, tendieron sus manos hacia él, como si fuesen a ser resucitados gracias a la fuerza de este. A Teseo, agarrándolo de la mano, logró alzarlo, pero tuvo que abandonar a Pirítoo ya que, al intentar levantarlo, tembló la tierra, por lo que este se quedó para siempre en el Hades.

Mientras Teseo estaba en el Hades, los Dioscuros, hermanos de Helena, liberaron a su hermana, se llevaron a Etra, la madre de Teseo, como esclava, hicieron huir a Demofonte y Acamante, los hijos que Teseo había tenido con Fedra, y pusieron en el trono de Atenas a Menesteo.

Después de ser rescatado por Heracles del inframundo, volvió a Atenas, pero fue expulsado de allí por Menesteo y decidió establecerse en Esciro, donde además tenía posesiones. 

Los habitantes de Esciro lo recibieron aclamándolo, por lo cual Licomedes, rey de la isla, decidió darle muerte. Para ello, hizo que se despeñara desde lo alto de un precipicio. 

En otras versiones, la muerte de Teseo fue accidental.

Se dice que un oráculo había ordenado en 476 a. C. llevar los huesos de Teseo desde la isla a Atenas. Efectivamente los supuestos huesos fueron llevados a Atenas por Cimón y guardados en el Teseion.













</doc>
<doc id="9723" url="https://es.wikipedia.org/wiki?curid=9723" title="Ciclo de Calvin">
Ciclo de Calvin

El ciclo de Calvin (también conocido como ciclo de Calvin-Benson o "ciclo de la fijación del carbono de la fotosíntesis"') consiste en una serie de procesos bioquímicos que se realizan en el estroma de los cloroplastos de los organismos fotosintéticos.

Las reacciones del ciclo de Calvin pertenecen a la llamada fase independiente de la luz, que se encarga de fijar el CO, incorporándolo a la materia orgánica del individuo en forma de glucosa mediante la enzima RuBisCo. Cabe destacar que este conjunto de reacciones se denomina erróneamente fase oscura, pues muchas de las enzimas del proceso, entre ellas la RuBisCo, dependen de la activación del sistema ferredoxina-tiorredoxina, que solo se encuentra en su forma activa (la reducida) en presencia de la luz. 

El ciclo de Calvin fue descubierto por Melvin Calvin, James Bassham y Andrew Benson de la Universidad de California, Berkeley, mediante el empleo de isótopos radiactivos de carbono-14. Calvin fue galardonado con el Premio Nobel de Química en 1961 «por sus trabajos sobre la asimilación del dióxido de carbono por las plantas».

El ciclo se resume en tres etapas: 




Se utilizan seis moléculas de CO para generar una molécula de glucosa. En estas reacciones cada una de las moléculas de CO es unida a una molécula aceptora, ribulosa-1-5-bifosfato (RuBP), que luego se divide en dos moléculas de 3-fosfoglicerato, siendo catalizada por la enzima Rubisco (con la energía de ATP y NADHP). El ATP producido durante las reacciones luminosas de la fotosíntesis cede grupos fosfato a estas moléculas, dando lugar a 1,3-difosfoglicerato; al mismo tiempo el NADPH cede electrones a estas moléculas de tres carbonos, dando lugar a gliceraldehido-3-fosfato. Una parte del gliceraldehido-3-fosfato es utilizado para fabricar el azúcar de 6 carbonos de glucosa, entre otros productos de la fotosíntesis. Otra parte del gliceraldehido-3-fosfato es utilizado en conjunto de una molécula de ATP, para generar el aceptor de CO ribulosa-1,5-bifosfato y comenzar el ciclo de nuevo.

A cada vuelta completa del ciclo, una molécula de dióxido de carbono entra en el ciclo y es reducida, presentando regeneración de una molécula de RuBP.

Seis vueltas del ciclo, con la introducción de seis átomos de carbono, son necesarios para producir un azúcar de seis carbonos, tal como la glucosa. La ecuación general para la producción de una molécula de glucosa es:

6CO + 12NADPH + 12H + 18ATP —> CHO + 12NADP + 18ADP + 18Pi + 6HO

El producto del ciclo es el gliceraldehído 3-fosfato, la molécula primaria transportada del cloroplasto hacia el citoplasma de la célula. Esta misma triasa fosfato (triasa significa un azúcar de tres carbonos) es formada cuando la molécula de fructuosa 1.6 bifosfato es rota en la cuarta etapa de la glucólisis y es inconvertible con otra triasa fosfato, la dihidroxiacetona.

Utilizando la  proveniente de la hidrólisis de enlaces fosfato, las primeras cuatro etapas de la glucólisis pueden ser revertidas para formar glucosa a partir del gliceraldehído 3-fosfato.

Entre otras funciones, cada 3 vueltas en el ciclo, una molécula de triosa fosfato es regenerada a partir de 3 moléculas de CO. La triosa fosfato puede es utilizada para la síntesis de almidón.

En algas y en plantas superiores existe un único mecanismo primario de carboxilación que resulta en una síntesis de compuestos de carbono: El Ciclo de Calvin o vía de las pentosas fosfato. Su importancia biológica radica en que es la única ruta para los organismos autótrofos, ya sean fotosintetizadores o quimiosintetizadores que permite la incorporación de materia inorgánica a los seres vivos.

Los productos del ciclo de Calvin son de vital importancia para la biosfera,ya que las uniones covalentes de los hidratos de carbono generadas por el ciclo representan la energía total que surge a partir de la obtención de la luz por los organismos fotosintéticos. Estos organismos denominados autótrofos, liberan la mayor parte de esta energía mediante la glucólisis y la respiración celular, energía que emplean para mantener su propio desarrollo,crecimiento y reproducción. Una gran cantidad de materia vegetal termina siendo consumida por los heterótrofos, que no pueden sintetizar y dependen de los autótrofos para obtener materias primas y fuentes de energía. La glucólisis y la respiración celular en las células de los heterótrofos liberan energía libre de los alimentos para su uso en estos organismos.




</doc>
<doc id="9726" url="https://es.wikipedia.org/wiki?curid=9726" title="Endomitosis">
Endomitosis

Endomitosis es la replicación cromosómica que no va acompañada por división nuclear o citoplásmica. 

Algunas células son naturalmente poliploides, debido a que presentan copias adicionales de su dotación cromosómica completa por haber pasado por rondas extras de duplicación del ADN antes de la división celular (endomitosis).



</doc>
<doc id="9727" url="https://es.wikipedia.org/wiki?curid=9727" title="Euploidía">
Euploidía

Euploidía es el estado celular en el cual la célula tiene uno o más juegos completos de cromosomas (dotaciones monoploides (x)) de su especie; dependiendo de especies, se excluyen los cromosomas sexuales. Los individuos con euploidía pueden presentar o no un número anormal de cromosomas diferente al habitual. Por ejemplo, un humano, tiene 46 cromosomas, que es el doble de 23, su número de cromosomas monoploide (x) (que en el caso de los humanos coincide con el número de cromosomas haploide (n); x = n = 23), por lo tanto es euploide (2x = 2n = 46). Un humano hipotético anormal, pero con juegos enteros de cromosomas, por ejemplo, 69, sería también euploide, en tanto que 69 sería múltiplo entero del número monoploide (3x = 69). No se debe confundir con la aneuploidía, que, en contraposición a la euploidía, se refiere a una alteración en la cantidad de uno de los tipos de cromosomas homólogos.

La euploidía puede ser de dos tipos:

- Monoploidía: la que poseen los organismos haploides (n); significa que solo tienen un único cromosoma de cada tipo.

- Poliploidía: la poseen individuos con varios juegos completos de cromosomas homólogos (triploidías 3n, tetraploidías 4n...). A su vez se divide en autopoliploidía, si las distintas dotaciones pertenecen originariamente a una única especie, o alopoliploidía, si son producto de hibridación de dos o más especies. Es bastante frecuente en plantas, sobre todo en angiospermas(47%), y se utiliza en biotecnología de forma inducida para mejorar los cultivos y la producción agrícola. El ejemplo más conocido de organismo alopoliploide es el trigo actual de consumo, que es alohexaploide, cuyo número de cromosomas es 42 en el estado diploide (2n = 42) y 21 en los gametos, haploides (n). Es un organismo hexaploide (6x), porque tiene 6 copias del juego monoploide (x) de 7 cromosomas (6x = 42) (con más precisión, es un alohexaploide, el prefijo "alo-" es porque originariamente, los juegos monoploides de 7 cromosomas provienen de especies diferentes). Cualquiera de las células de esta especie que tuviera juegos completos de cromosomas con 7, 14, 21, 28, 35 ó 42 cromosomas sería un euploide.

La euploidía y el contenido de ADN de las células eucariotas varía entre especies y entre las células de un mismo organismo.


</doc>
<doc id="9728" url="https://es.wikipedia.org/wiki?curid=9728" title="Esténtor">
Esténtor

En la mitología griega, Esténtor (Griego: Στέντωρ; gen.: Στέντορος) era uno de los heraldos de las tropas aqueas durante la Guerra de Troya.
Es mencionado por Homero en un único pasaje de la Ilíada. En ese verso se cuenta como Hera tomó la forma de Esténtor para alentar a los griegos en la batalla:
Debido a este personaje se dice de una voz que es estentórea cuando es fuerte y sonora.
Según el escoliasta, Esténtor murió tras ser desafiado por Hermes a un concurso de gritos.

Aristóteles usa el carácter de Esténtor en el Libro 7 de Política, donde se lee: "¿Pues quién habrá de conducir masa tan poderosa, y quizá podrá anunciarle algo, si no tiene la voz de Esténtor?". En el contexto se discute la magnitud de una polis. 

Entre otros que aluden al legendario héroe se puede mencionar,en España, a uno de los dos miembros del grupo de rap manchego BMEM. También el grupo de heavy metal español humorístico Gigatrón dedicó una canción de su disco Atopeosis 666 a Esténtor.



</doc>
<doc id="9729" url="https://es.wikipedia.org/wiki?curid=9729" title="Paradigma">
Paradigma

El concepto de paradigma se utiliza comunmente como sinónimo de “ejemplo” o para hacer referencia en caso de algo que se toma como “modelo". En principio se tenía en cuenta en el campo, tema, ámbito, entre 2 personalidades u otros..., gramatical (para definir su uso en un cierto contexto) y se valoraba desde la retórica (para hacer mención a una parábola o fábula). A partir de la década de 1960, los alcances de la noción se ampliaron y paradigma comenzó a ser un término común en el vocabulario científico y en expresiones etimológicas cuando se hacía necesario hablar de modelos de conocimiento aceptados por las comunidades científicas

El término "paradigma" se origina en la palabra griega
"παράδειγμα" ["parádeigma"] que en griego antiguo significa "modelo" o "ejemplo". A su vez se divide en dos vocablos "παρά" ["pará"] ("junto") y δεῖγμα ["deīgma"] ("ejemplo", "patrón"). Originariamente, significaba patrón, modelo. 

El sentido del concepto "paradigma", del griego antiguo παράδειγμα, "paradeigma" ("modelo", "ejemplo"), deriva de παραδεικνύναι, "paradeiknunai" ("demostrar", "probar", "comparar" ), de παρά-, "para'-" ("junto", "alrededor") y δείκνυμι, "deiknumi" ("señalar", "indicar", "mostrar", "enseñar").

Para Platón, los paradigmas son los modelos divinos a partir de los cuales las cosas terrestres están hechas. A su vez tiene las mismas raíces que "«demostrar»".

Michel Foucault usó los términos epistemológico, discursivo, matesis y taxinomial, para aspectos del paradigma en el sentido original dado por Kuhn.

En lingüística, Ferdinand de Saussure ha usado paradigma para referirse a una clase de elementos con similitudes.

En arquitectura, «modelo» (maqueta) o «plano» de un edificio y también es utilizado por escultores y pintores de manera semejante.

Dentro de la enfermería se emplea también el concepto paradigma aunque con cierta imprecisión, ya que se establece dentro de esta disciplina la existencia de 3 paradigmas de enfermería, y un metaparadigma constituido por cuatro conceptos esenciales para la enfermería (persona, salud, entorno y cuidado).

Este concepto fue de uso específico en la gramática. En 1992 el diccionario "Merriam–Webster's Collegiate Dictionary" definía su uso solamente en tal contexto, o desde la retórica para referirse a una parábola o a una fábula.

En ciencias sociales y en teoría de sistemas, el paradigma es equiparable al concepto de pensamiento de grupo -- o su casi equivalente "" -mentalidad-, como cuerpos de ideas, métodos y asunciones teóricas sostenidos y validados por una persona o grupo de personas, que incluye una serie de comportamientos, actitudes y creencias.

La palabra "paradigma" también se utiliza para indicar un patrón o modelo, un ejemplo fuera de toda duda, un arquetipo. En este sentido se la utiliza frecuentemente en las profesiones del diseño. Los paradigmas de diseño —arquetipos— representan los antecedentes funcionales para las soluciones de diseño.

También se usa en cibernética; aquí significa —en un sentido muy amplio— un preprograma conceptual para el ordenamiento de unos datos aún más caóticos en términos relativos. 

El concepto es de amplio uso en la vida cotidiana, ya que se refiere a ideas, pensamientos, opiniones, creencias, puntos de vista, percepciones, etcétera, que se asumen como verdaderos o falsos. Incluso, el concepto de paradigma puede referirse, de manera cotidiana, a una creencia u opinión compartida colectivamente. Sin embargo, este uso del concepto puede generar diversos errores e imprecisiones, al presentarse en ámbitos teóricos y científicos, ya que se refiere prácticamente a cualquier idea o creencia que tenga un sujeto o grupo de sujetos. Es recomendable no emplear este concepto en usos de la vida cotidiana, y dejarlo para discusiones de carácter epistemológico. En su lugar, cotidianamente se pueden usar cualesquiera de los conceptos arriba mencionados.

Es necesario acotar el significado del concepto paradigma siempre que éste se emplee en la teorización epistemológica, ya que en general su significado contemporáneo dentro de la filosofía de la ciencia o epistemología alude al conjunto de prácticas y teorías que definen una disciplina científica en una época histórica dada.

El filósofo e historiador de la ciencia, Thomas S. Kuhn dio a paradigma su significado contemporáneo cuando lo adoptó para referirse al conjunto de prácticas y saberes que definen una disciplina científica durante un período específico. El mismo Kuhn prefería los términos ejemplar o ciencia normal, que tienen un significado filosófico más exacto. Sin embargo, en su libro "La estructura de las revoluciones científicas" define a un paradigma de la siguiente manera:

El paradigma, de esta manera constituye el desarrollo de lo que Kuhn llama ciencia normal, y como tal se manifiesta a través de los libros de texto propios de una ciencia o disciplina, al presentar las teorías aceptadas por las comunidades científicas de cada disciplina, explicándolas y comparándolas, mostrándolas a través de experimentos y observaciones. El paradigma define los métodos, los problemas que legítimamente debe abordar una disciplina o campo de investigación, para ser legado a generaciones futuras de científicos. De esta forma, el paradigma incluye en el plano de la investigación científica lo siguiente:
En realidad este concepto de paradigma es muy amplio y se puede equiparar al concepto de matriz disciplinar o disciplina, ya que dicho concepto alude a la posesión común por parte de los que practican una disciplina concreta (disciplinar) y matriz porque se compone de los elementos estructurados que dan sentido a las explicaciones científicas. El paradigma es el conjunto de realizaciones de una ciencia y es compartido por los miembros de la comunidad científica. El papel de la comunidad científica de cada disciplina en el desarrollo del paradigma es clave, ya que Kuhn asume que la ciencia es una empresa humana colectiva y como tal las discusiones científicas son producto de la comunicación y la tarea coordinada y conjunta de los científicos trabajan en el desarrollo de un paradigma a través de sus diversas teorías y experimentos. 

El paradigma en un sentido amplio, incluye aspectos ontológicos y epistemológicos fundamentales, que proporcionan el horizonte desde el cual se construyen los diferentes modelos teóricos y teorías de un nivel inferior, presentando las directrices generales de agrupamiento de las diferentes teorías en los campos disciplinares de cada ciencia.

De esta forma, dentro de la ciencia normal, un paradigma es el conjunto de experimentos modélicos capaces de ser copiados o emulados; siendo la base para crear un consenso científico. El paradigma aceptado en el consenso científico imperante en una época histórica dada, establece formas de ver e interpretar la realidad, también abren líneas para la creación de propuestas para la investigación futura, las teorías y prácticas derivadas del uso de un método científico y sus aplicaciones metodológicas concretas. 

Un ejemplo de paradigma comúnmente aceptado sería el modelo estándar de la física. Los métodos científicos permitirían a los científicos ortodoxos investigar muchos fenómenos que pueden resultar contradictorios o contrastantes con el modelo estándar. Sin embargo es mucho más difícil obtener consenso para los mismos, en proporción a la divergencia de los principios aceptados del modelo estándar que tales experimentos examinarían. Así, en particular, un experimento para investigar la masa del neutrino o la descomposición de neutrones recibiría más fondos que un experimento que buscara violaciones a la conservación de momentos, o pretendiera estudiar la ingeniería de los viajes en el tiempo.

Kuhn define al paradigma como ""una completa constelación de creencias, valores y técnicas, etc. compartidas por los miembros de una determinada comunidad"". Esta definición aparece en 1969 como agregado a su libro original, porque en principio el uso del término no había estado claramente definido. Bajo esta definición de Kuhn subyace otro sentido en el uso del término: ""un paradigma también denota una suerte de elemento en esa constelación, la solución concreta del rompecabezas que, empleado como ejemplo o modelo, puede reemplazar a las reglas explícitas como base para la solución de los rompecabezas remanentes de la ciencia normal"". El término permanece impreciso debido a los diferentes usos que se le dan.

El cambio de paradigma tiende a ser drástico en las ciencias, ya que éstas parecen ser estables y maduras, como la física a fines del siglo XIX. En aquel tiempo la física aparentaba ser una disciplina que completaba los últimos detalles de un sistema muy trabajado. Es famosa la frase de Lord Kelvin en 1900, cuando dijo: ""No queda nada por ser descubierto en el campo de la física actualmente. Todo lo que falta son más medidas y más precisas"".

Cinco años después de esta aseveración, Albert Einstein publicó su trabajo relatividad especial que fijó un sencillo conjunto de reglas superando a la mecánica de Newton, que había sido utilizada para describir la fuerza y el movimiento por más de doscientos años. En este ejemplo, el nuevo paradigma reduce al viejo a un caso especial, ya que la mecánica de Newton sigue siendo una excelente aproximación en el contexto de velocidades lentas en comparación con la velocidad de la luz.

En "La estructura de las revoluciones científicas", Kuhn escribió que ""...las sucesivas transiciones de un paradigma a otro vía alguna revolución, es el patrón de desarrollo usual de la ciencia madura"".

La idea de Kuhn era revolucionaria en su tiempo, ya que estableció la necesidad de mirar con perspectiva histórica a los desarrollos científicos, y asumió que la ciencia es una empresa humana y como tal histórica, por ende, transformable. La observación kuhniana sobre la necesidad de mirar desde la perspectiva histórica el desarrollo de la ciencia, fue en sí misma un "cambio paradigmático" en la historia, la sociología y la filosofía de la ciencia. 

En las últimas tres o cuatro décadas, además del uso del concepto de paradigma realizado por Kuhn, otros autores como Imre Lakatos y otros han empleado dicho concepto con un sentido distinto. Cabe destacar el uso que de él ha hecho Fritjof Capra, en su obra "El punto crucial" (T"he turning point"). En esta obra, Capra plantea una visión amplia sobre paradigma, vinculada con los procesos históricos y civilizatorios, y su relación con la naturaleza. Para Capra, las limitaciones civilizatorias actuales están generando un "cambio de paradigma", "entendiendo por paradigma la mentalidad, conceptos y sistemas de valores que forman parte de una visión particular de la realidad". Establece y describe dos grandes paradigmas: el existente y el que está en formación. El paradigma existente hoy en día se ha gestado desde la antigüedad, que incluye fenómenos históricos como: la Revolución científica, el Siglo de las luces, la Revolución industrial; e incluye a nivel conceptual o cultural: la idea del método científico como único enfoque para llegar al conocimiento legítimo; la concepción del universo como sistema mecánico compuesto de partes elementales; la fragmentación de la realidad y sus fenómenos; la vida social comprendida como lucha competitiva por la existencia; el crecimiento tecnológico y económico para la obtención de progreso material ilimitado; la idea de que el crecimiento y el progreso es constante e ilimitado; la idea del modelo causa-efecto como base para las explicaciones de los fenómenos. A tal paradigma existente, contrapone otro paradigma en proceso de formación, con características opuestas: la posibilidad de llegar al conocimiento no solamente a través del método científico; una visión holística, amplia e integral de la realidad y sus fenómenos, que no fragmenta los fenómenos para conocerlos; la idea de que la civilización privilegia la cooperación; la limitación del crecimiento material y tecnológico dada la finitud de la naturaleza; una visión del mundo entendiendo a este como amalgama de sistemas complejos interdependientes e interrelacionados.

Probablemente el uso más común de paradigma, implique el concepto de "cosmovisión". Por ejemplo, en ciencias sociales, el término se usa para describir el conjunto de experiencias, creencias y valores que afectan la forma en que un individuo percibe la realidad y la forma en que responde a esa percepción. Debe tenerse en cuenta que el mundo también es comprendido por el paradigma, por ello es necesario que el significado de paradigma es la forma por la cual es entendido el mundo, el hombre y por supuesto las realidades cercanas al conocimiento.

Los investigadores sociales han adoptado el concepto de Kuhn "cambio de paradigma" para remarcar un cambio en la forma en que una determinada sociedad organiza e interpreta la realidad. Un "paradigma dominante" se refiere a los valores o sistemas de pensamiento hegemónicos o dominantes en una sociedad, en un momento determinado. Los paradigmas dominantes son compartidos por el trasfondo cultural de la comunidad y por el contexto histórico del momento. Las siguientes son condiciones que facilitan que un sistema de pensamiento pueda convertirse en un paradigma dominante:


Al equiparar el paradigma con modelo, hablando de Ciencias Sociales, se tiene que el mismo comprende a un conjunto de características aplicables al estudio de determinada sociedad. 

Los paradigmas pueden ser descritos desde una perspectiva estructural. Operan en diferentes niveles: macro, meso y micro de la estructura paradigmática. Los niveles direccionan mejor la estructura fundamental de los paradigmas, y no tanto su categorización cronológica o histórica, ni su uso etimológico; como sucede en la mayoría de las disciplinas. Los niveles paradigmáticos están siempre presentes y no se encuentran limitados por tales categorías. Permiten además ayudar a comprender el funcionamiento de un paradigma.




Así, un paradigma es una visión de la realidad que conforma una "Gestalt" resultante de las tres ramas de la filosofía: metafísica, epistemología y ética, de la siguiente manera: 

Resulta obvio que las tres ramas de la filosofía describen la estructura de un paradigma. Ninguna de las ramas de la filosofía puede por separado completar su conocimiento, pero juntas describen la "Gestalt" semejante a un movimiento en espiral —no un mero círculo— que constituye el "conocimiento hermenéutico". 

La Parálisis Paradigmática se puede presentar en cualquier nivel de la sociedad, pero sus consecuencias son peores cuando ataca a personas que toman decisiones. No es una enfermedad física, más bien es de la mente, pero cuando se presenta suele ser muy dañina sobre todo en personas y organizaciones expuestas a un entorno dinámico. En general, es una enfermedad fácil de adquirir y a menudo fatal cuando se trata de convertir el paradigma en el paradigma único.

Pero. ¿De dónde proviene esta enfermedad, muchas veces rayana en la paranoia? La palabra griega "paradeigma" significa "modelo" o patrón" y en la práctica se convierte en un conjunto de reglas y disposiciones, escritas o no, que establecen o definen los límites y las formas de comportarse dentro de ellos. Se crean estructuras mentales, mitos, creencias, modelos, patrones, estereotipos que al asumirse como ciertos, resultan fáciles de adoptar, y por ende influyen en el comportamiento, actitudes y percepciones de las personas.

Algunos paradigmas podrán ser triviales, pero grandes o pequeños, sirven para proporcionar una visión, una comprensión y métodos particulares para resolver problemas específicos. Es muy saludable tener ciertas formas de comportarse o de poseer ciertos modelos, pero en el extremo, la "parálisis paradigmática", pasa a constituirse en una de las enfermedades organizacionales más graves, la cual no permite pensar ni dudar respecto de la validez o vigencia del paradigma y podría volverse crónica.

Existen variados ejemplos de esta enfermedad entre empresarios, educadores, gobiernos, profesionales, comerciantes, políticos, científicos, en las familias, en países, en las religiones, etc., y eso debido a que la historia humana siempre ha estado en proceso de cambio y por ende siempre han existido paradigmas, con la única diferencia que de antaño ocurría un cambio cada veinticinco años y en la actualidad ocurren en cosa de minutos, por lo que se debe disponer de una adecuada dosis de predisposición, entre otras, para no ser víctima de esta "parálisis paradigmática".

En 1492, hace más de quinientos años, Cristóbal Colón inició un viaje para buscar una nueva ruta a Oriente. Basado en su creencia de que la Tierra era redonda, Colón había notado que al observar un barco alejándose del puerto, el casco era lo primero en desaparecer en el horizonte, después lo hacía el velamen. Los demás obviamente veían lo mismo, sin embargo como el paradigma de la época, era el de que la Tierra era plana, no trataron de explicarse el porqué de la situación, ya que al ser plana, el barco alejándose sólo se vería más pequeño. Unos años más tarde, Hernando de Magallanes comenzó y Juan Sebastián El Cano completó la primera vuelta en barco alrededor del mundo. Se había roto el paradigma y la correspondiente parálisis.




</doc>
<doc id="9731" url="https://es.wikipedia.org/wiki?curid=9731" title="Pthirus pubis">
Pthirus pubis

La ladilla (Pthirus pubis) es un insecto anopluro ectoparásito de los seres humanos, de entre 1-3 mm de longitud, casi redondo, achatado y de color amarillento. La infestación por ladillas se denomina ftiriasis.

La transmisión se realiza en la mayoría de los casos por contacto sexual, aunque también en raras ocasiones puede suceder al usar prendas que han estado en contacto con algún portador. Además de la región púbica, también pueden situarse en el cabello, las cejas, las pestañas y el vello axilar y corporal (del pecho o de piernas y brazos, por ejemplo). Sus huevos pueden verse en forma de pequeños puntos blancos pegados al pelo cerca de la piel. El período de incubación de los huevos es de seis a ocho días. En otros idiomas suele denominarse literalmente "piojo del pubis" o "piojo púbico". A diferencia del piojo de la cabeza, son lentas moviéndose, avanzando cada día aproximadamente de uno a diez centímetros.

Se estima que hay más de 1 millón de casos cada año. Las personas que tienen más relaciones sexuales con diferentes personas corren un riesgo más alto de contraer piojos púbicos.

Las ladillas se alimentan de sangre por lo menos cincuenta veces al día, lo que ocasiona un prurito muy molesto que puede hacer que el infestado se rasque provocando irritación e infección de la piel. Cada cinco días aproximadamente, la hembra pone entre diez y quince huevos blancos (las liendres), que tardan una semana en incubar. Cada día se pueden mover aproximadamente un centímetro. En la ropa interior suelen aparecer unas manchas de color marrón/rojizo debido a las pequeñas gotas de sangre de las picaduras.

En algunos individuos, la infestación es asintomática o se manifiesta de forma sutil, por lo que pueden transmitir el parásito al no saber que lo poseen.

Existen cremas, champús y lociones que contienen hexacloruro de benceno gamma o permetrina y que son igualmente eficaces mientras se usen correctamente. Aunque el parásito vive poco tiempo separado del cuerpo, es conveniente cambiar sábanas, toallas y ropas para evitar la reinfestación. Es recomendable encerrar en bolsas aisladas toda la ropa y sabanas recién usadas antes de aplicado el tratamiento para que los liendres no sobrevivan.
A los sujetos diagnosticados de ladillas, se les recomienda comentar su infestación con sus parejas sexuales con objeto de frenar epidemias.
La reinfestación puede suscitarse, ya que una vez que las ladillas han sido separadas del cuerpo, pueden sobrevivir hasta 24 horas, mientras que los huevos o liendres hasta seis días, por lo que una vez curado se debe repetir el tratamiento de 7 a 10 días después para eliminar los huevos que hayan quedado, ya que en 7 a 10 días se convierten en ladillas. Por esta razón se debe desinfectar una semana después de la primera limpieza.

Los preservativos no detienen el contagio de piojos púbicos; la forma de prevención es asegurar que la pareja de relaciones sexuales no tenga ladillas. El parásito es capaz de vivir poco tiempo sin contacto con el cuerpo humano. Sin embargo, es conveniente no usar ropa o sábanas de otras personas.

El estudio genético más reciente indica que la ladilla se relaciona con el piojo endémico del gorila, "Pthirus gorillae", y que pudo haber pasado a los homínidos tempranos desde los ancestros de los gorilas, hace varios millones de años. Hasta ahora se pensaba que habían divergido en el propio humano.


</doc>
<doc id="9732" url="https://es.wikipedia.org/wiki?curid=9732" title="Técnica">
Técnica

La técnica (del griego, τέχνη "tékhnē" 'arte, técnica, oficio') es un procedimiento o conjunto de reglas, normas o protocolos que tiene como objetivo obtener un resultado determinado y efectivo, ya sea en el campo de las ciencias, de la tecnología, del arte, del deporte, de la educación o en cualquier otra actividad. 
Es el conjunto de procedimientos que se usan para un arte, ciencia o actividad determinada que, en general, se adquieren por medio de su práctica y requieren determinadas habilidades o destrezas.

La técnica requiere tanto destrezas manuales como intelectuales, frecuentemente el uso de herramientas y de varios conocimientos. En los animales las técnicas son características de cada especie. En el hombre, la técnica surge de su necesidad de modificar el medio y se caracteriza por ser transmisible, aunque no siempre es consciente o reflexiva. Generalmente, cada individuo la aprende de otros (a veces la inventa) y eventualmente la modifica. Es generalizada la creencia, que sólo las personas son capaces de construir con la imaginación, algo que luego pueden concretar en la realidad. Sin embargo, algunos primates superiores, aparte del hombre, pueden fabricar herramientas. La técnica, a veces difícil de diferenciar de la tecnología, surge de la necesidad de transformar el entorno para adaptarlo mejor a sus necesidades.


Las técnicas instruccionales son herramientas didácticas que utiliza el instructor para reforzar o concretar el objetivo de aprendizaje planteado. 
La elección de las técnicas varía de acuerdo al objetivo, las características de los participantes y del curso, y de la dinámica grupal.

La técnica se refiere a los procedimientos y recursos que se emplean para lograr un resultado específico. Las técnicas tienen el objetivo de satisfacer necesidades y requieren de quien las aplica.
Cualquier actividad que es realizada en la vida diaria sigue un método o procedimiento, es decir, una técnica.

La historia de la técnica es la historia de la invención de herramientas y técnicas con un propósito práctico. La historia moderna está relacionada íntimamente con la historia de la ciencia, pues el descubrimiento de nuevos conocimientos ha permitido crear nuevas cosas y, recíprocamente, se han podido realizar nuevos descubrimientos científicos gracias al desarrollo de nuevas tecnologías, que han extendido las posibilidades de experimentación y adquisición del conocimiento.




</doc>
<doc id="9734" url="https://es.wikipedia.org/wiki?curid=9734" title="Homicisium">
Homicisium

Homicisium es el nombre de la multa que, según las leyes medievales, habían de pagar los habitantes de un municipio cuando aparecía el cadáver de alguien asesinado dentro de su término municipal y no había modo de encontrar al autor del crimen.

La práctica lógica de mantener el hallazgo en secreto y de llevar a la víctima a un pueblo vecino por la noche con el fin de evitar la sanción dio lugar a la expresión "echarle a uno el muerto" cuando se pretende inculpar a un tercero en algo que no ha hecho.



</doc>
<doc id="9736" url="https://es.wikipedia.org/wiki?curid=9736" title="Aplicación informática">
Aplicación informática

En informática, una aplicación es un programa informático diseñado como herramienta para permitir a un usuario realizar uno o diversos tipos de tareas. Esto lo diferencia principalmente de otros tipos de programas, como los sistemas operativos (que hacen funcionar la computadora), las utilidades (que realizan tareas de mantenimiento o de uso general), y las herramientas de desarrollo de "software" (para crear programas informáticos). Las aplicaciones pertenecen al software de aplicación.

Suele resultar que una solución informática se orienta a la automatización de ciertas tareas complicadas, como pueden ser la contabilidad, la redacción de documentos, o la gestión de almacenes. Algunos ejemplos de programas de aplicaciones generales de este tipo, son los procesadores de textos, las hojas de cálculo, y las base de datos.

Ciertas aplicaciones desarrolladas "a medida" suelen ofrecer una gran potencia de uso y rapidez en la ejecución, ya que están exclusivamente diseñadas para resolver un problema específico. Otros, llamados paquetes integrados de "software", ofrecen menos potencia en cuanto a adaptabilidad al uso y requerimientos en cuanto al equipo utilizado (memoria disponible, tiempo de uso, etc), pero a cambio, incluyen un variado abanico de aplicaciones, como es el caso de los ya citados programas procesadores de textos, procesadores de hojas de cálculo, y manejadores de base de datos.
Otros ejemplos de programas de aplicación pueden ser: programas de comunicación de datos, multimedia, presentaciones, diseño gráfico, cálculo, finanzas, correo electrónico, navegador web, compresión de archivos, presupuestos de obras, gestión de empresas, etc.

Algunas compañías agrupan diversos programas de distinta orientación en el uso, para que formen un paquete (llamados "suites" o paquetes ofimáticos), que suelen ser satisfactorios para las necesidades más apremiantes del usuario. Todos y cada uno de ellos sirven para ahorrar tiempo y dinero al usuario, al permitirle hacer cosas útiles con la computadora con alguna facilidad; de todas maneras, hay diferencias entre los programas que se ofrecen, pues algunos brindan ciertas prestaciones, aunque otros imponen un determinado diseño demasiado estricto, y ya que además, unos son más agradables y fáciles de usar que otros.

Actualmente, con el uso de dispositivo móviles, se ha extendido el término "app", que es un acortamiento de la palabra inglesa "application", y extendida por el éxito de la llamada App Store de Apple. En español se desaconseja su uso, pero de usarla, se recomienda escribir "app" en letra cursiva, y no debería deletrearse al leerla, porque no es una sigla (se pronuncia /ap/), aunque esto puede dificultar la pronunciación de las palabras que le siguen inmediatamente, al forzar una pausa para pronunciar o marcar la p final de "app" —que no es algo que en español se dé naturalmente—. El acortamiento que podría recomendarse del término 'aplicación', en todo caso sería 'apli' (con su plural 'aplis'), escritas en letra común.



</doc>
<doc id="9737" url="https://es.wikipedia.org/wiki?curid=9737" title="Aconcagua">
Aconcagua

El Aconcagua es una montaña ubicada en la provincia de Mendoza, en el oeste de Argentina. Integra la Cordillera Principal, la cual es un componente de la cordillera de los Andes. Con una altitud de 6960,8 msnm, es el pico más eminente de los hemisferios meridional y occidental, el más alto de la Tierra después del sistema de los Himalayas (Asia) y, por tanto, la cima más elevada en América.

El origen del término "Aconcagua" es incierto, aunque se postulan diversas procedencias; la primera es que proviene del mapudungun "aconca" "hue," que significa 'que viene del otro lado'; la segunda es que proviene del quechua "ackon" y "cahuak" del verbo "cahua", que significa 'mirar' o 'el que mira'; tal vez 'la gran roca que mira sus alrededores', o 'centinela de piedra'. Sin embargo, en la lengua aimara los vocablos "kon" "kawa" podrían ser traducidos como 'centinela blanco' o 'monte nevado'. Posiblemente el nombre original es "Janq'u Q'awa", 'quebrada blanca'.

Otra alternativa, también del quechua, «acon» de "aquun": 'su arenal'; «cagua» de "qawaq": 'oteador', 'que ve'. Luego podría denotar «oteador del arenal».

Otra alternativa viene del mapudungún, donde "konka /konka/" es 'atados de paja para techar' y "wa /wa/" significa 'lugar donde hay algo'.

El Aconcagua se ubica íntegramente en el departamento Las Heras de la provincia de Mendoza —en el noroeste provincial—, al centro-oeste de la República Argentina. Se encuentra dentro del parque provincial Aconcagua.

El Aconcagua es la cumbre más alta de los hemisferios sur y occidental; también es la montaña más elevada de la Tierra entre las situadas fuera del sistema de los Himalayas. Posee dos picos principales: la cumbre norte, de 6960,8 msnm (22 837 ft), coordenadas: S32 39.11 W70.00. 42; y la cumbre sur, de 6930 msnm. Durante décadas figuró en las publicaciones la altura de 6959,60 msnm, y posteriormente la de 6962 msnm.

En este sector de la frontera de Argentina y Chile el límite entre los dos países se establece por la línea de "divisorias de aguas" y los cerros Catedral y Tolosa impiden que los deshielos que nacen en el Aconcagua se encaminen hacia el océano Pacífico, siendo tributarios del Atlántico. Además las aguas de estos dos últimos cerros tampoco van al Pacífico, por lo que la frontera corre a 14 km hacia el occidente del Aconcagua, hasta el cerro Caracoles, sobre el Cordón de los Dedos, dejando al Aconcagua enteramente en territorio argentino.

Al norte y al este limita con el valle de las Vacas y al oeste y al sur con el valle de los Horcones inferior. Varios glaciares atraviesan sus laderas; los más importantes son el glaciar Nororiental o Polaco y el del Este o Inglés.

Estudios geológicos sitúan la elevación del Aconcagua en la edad Permotriásica, unos 200 a 280 millones de años atrás. La montaña fue creada por la subducción de la placa de Nazca debajo de la placa Sudamericana durante la orogenia andina (terciaria, por lo tanto geológicamente reciente).

Durante los inicios de su exploración se pensó que era un volcán, pues uno de sus exploradores, Paul Güssfeldt, comprobó que las rocas que lo conformaban eran de origen volcánico, pero el profesor Walter Schiller, investigador geólogo del Museo de La Plata, publicó en uno de sus trabajos, que estas rocas volcánicas fueron depositadas en el lugar por fuertes eventos tectónicos y que no se evidenciaba ningún orificio o cráter en la cima del mismo.

A 5300 m se encontró un enterratorio ritual inca en el cual se había practicado el "Capac Cocha", consistente en el sacrificio de un niño de 7 a 8 años de edad durante el Período Inca (1400 a 1532 d.C.); su cuerpo actualmente se encuentra resguardado en el laboratorio del Centro Científico Tecnológico (ex Cricyt). Junto al niño se encontraron diversos objetos hechos de oro, plata y "Spondylus" (una valva del océano Pacífico). Estos hallazgos confirman al monte Aconcagua como uno de los más grandes "apu" del Imperio inca en el Collasuyu.

El “Niño del Aconcagua” ha vuelto a revelar algo más de sí. A partir de un estudio de ADN destinado a secuenciar su genoma mitocondrial, un equipo de científicos españoles y argentinos logró establecer que la momia pertenecía a un linaje que no había sido detectado hasta ahora en poblaciones contemporáneas y que se remonta a los tiempos en que el hombre llegó a América por primera vez.

A sólo 3 kilómetros está un caserío llamado Puente del Inca, donde aún existe una fuente de aguas termales, una feria de artesanías, y un hostal de 2 estrellas del mismo nombre.

Al final del Parque Aconcagua está el «Puente confluencia», lugar donde varias parejas han realizado el ritual de compromiso y que da inicio al ascenso al Monte Aconcagua.

Un nuevo centro de asistencia fue inaugurado en enero de 2011 a una altitud de 5975 msnm: el Refugio Elena, en el Aconcagua (se considera por ahora el refugio estable de montaña más alto de la Tierra), gracias a la donación que realizaron familiares de la andinista italiana Elena Senín, quien perdió la vida luego de llegar a la cumbre en enero de 2009. El refugio —destinado a emergencias y operativos de rescate— está ubicado en el campamento Plaza Cólera, en la bifurcación de las dos rutas más transitadas de ascenso al Aconcagua, la Norte y la del Glaciar de los Polacos.

Los centros para la práctica de esquí más próximos a esta montaña son:

Es una montaña muy frecuentada por andinistas de todo el mundo, con una entrada de 6000 a 7000 visitantes por temporada, que se extiende entre diciembre y marzo.

En términos montañistas el Aconcagua es técnicamente sencillo desde la cara norte, a través de la «vía normal» del noroeste, en la que no es necesario el uso de técnicas de escalada. Los efectos de la altitud son muy severos (la presión atmosférica es el 40 % de la existente a nivel del mar) provocando generalmente apunamiento en los escaladores. Las condiciones climatológicas pueden cambiar brúscamente desde un clima tranquilo y diáfano a, en pocos minutos, un clima tempestuoso o producirse el viento blanco del Aconcagua.

Su ascensión no suele requerir el uso de oxígeno artificial.

En la «vía normal» se asciende a través de campamentos de altura con sus correspondientes días de descanso. Los hitos más significativos de la vía son: Campo Base (Plaza de Mulas) (4300 msnm), El Semáforo (4350 msnm), Piedras Conway, Plaza Canadá, Piedra de 5000, Cambio de Pendiente, Nido de Cóndores (5250 msnm), Berlín, Piedras Blancas, Piedras Negras, Independencia, Portezuelo de los Vientos, Gran Travesía, La Canaleta y Cumbre del Aconcagua (siguiendo el orden creciente de dificultad).

La segunda vía, mucho más peligrosa que la anterior, es la del glaciar de los Polacos. Ésta se aproxima a la montaña a través del valle de las Vacas, asciende hasta la base del glaciar de los Polacos y cruza la vía normal hasta la subida final a la cumbre.

Las vías desde las crestas situadas al sur y suroeste son las más duras, considerándose la "Pared Sur" como la más difícil. Se trata de una escalada muy comprometida y de alta dificultad en una de las mayores paredes del mundo (3000 m de pared aprox.) La primera ascensión de la Pared Sur fue realizada el 25 de febrero de 1954 por los franceses Pierre Lesueuer, Adrien Dagory, Edmond Denis, Robert Paragot, Lucien Berardini y Guy Poulet. El jefe de la expedición era René Ferlet.

La primera ascensión al Aconcagua se realizó en 1897 por una expedición liderada por el británico Edward FitzGerald (1871-1931). La cumbre fue alcanzada por el suizo Matthias Zurbriggen el 14 de enero y por otros dos miembros de la expedición unos días después.

El primer argentino en hacer cumbre fue Nicolás Plantamura, perteneciente al Ejército Argentino, el 8 de marzo de 1934; en esta misma expedición también participó el arriero Mariano Pastén, quien se convirtió en el primer chileno en alcanzar la cumbre. La primera mujer fue la francesa Adriana Bance, el 7 de marzo de 1940, quien ascendió acompañada por miembros del Club Andinista de Mendoza.

En 1952, los miembros del Club Alemán Andino o DAV Santiago, Eberhard Meier, Ludwig Krahl y Wolfgang Förster completaron el ascenso por la llamada ruta chilena o de Güssfeldt. Esta ruta nace en territorio chileno y asciende por el valle del río Colorado para cruzar por algunos de los pasos hacia territorio argentino y a través del glaciar de Güssfeldt se conecta con la ruta normal en su último tramo. Esta ruta ya había sido intentada en 1883 por el científico alemán Paul Güssfeldt, quien debido al mal tiempo fracasó en su intento de hacer cumbre a poca distancia de ella, alcanzando los 6600 msnm.

En septiembre de 1953, los argentinos Emiliano Huerta, H. Vasalla y F. Godoy logran la primera ascensión invernal al Aconcagua, utilizando la ruta normal. Por esta hazaña, la calle principal de la localidad de Puente del Inca lleva el nombre de Huerta.

La «Variante Altoaragonesa»: escalada en 1995, una serie de corredores que salen del glaciar de lo Polacos, con una inclinación de 75° y pasos de IV+; tras un largo flanqueo a los 6500 msnm se unen de nuevo con la Directa, abierta por los aragoneses Javier Subias, José Antonio Hidalgo, Javier Alvira y José Vilalta.

La persona más joven en escalar y llegar a la cima del Aconcagua es el estadounidense Tyler Armstrong a la corta edad de 9 años el 25 de diciembre de 2013. Junto a su padre y guías andinistas, tomaron la ruta central del Glaciar de los Polacos. Cabe destacar que el joven Armstrong batió un récord mundial al hacer esta hazaña.

En diciembre de 2014, el corredor español de trail running Kilian Jornet estableció un nuevo récord de velocidad de subida y bajada al Aconcagua, al marcar un tiempo de 12 horas y 49 minutos.

Karl Egloff es un deportista suizo-ecuatoriano que se ha desempeñado en el ciclismo y el montañismo. El 19 de febrero de 2015 rompió el récord mundial de trepada en velocidad con un tiempo de ascenso y descenso de 11 horas y 52 minutos.

Se considera que el Aconcagua posee el índice de mortalidad más alto en Sudamérica (aproximadamente tres fallecimientos por año). Esto se debe a que al ser posible lograr el ascenso con relativa sencillez, personas sin la debida preparación se presentan a hacer el intento. Los escaladores palidecen ante el mal de altura y los cambios climáticos extremos, con vientos fuertes como resultado de la proximidad de la montaña al Océano Pacífico.

Desde que iniciaron los registros, más de cien personas han fallecido en el Aconcagua. Entre los años 2001 y 2012, de las 42.731 personas que buscaron alcanzar la cumbre del Aconcagua, 33 murieron, lo que indica una tasa de mortalidad de 0,77 cada 1.000 individuos.






</doc>
<doc id="9739" url="https://es.wikipedia.org/wiki?curid=9739" title="Utilidad (informática)">
Utilidad (informática)

En informática, una utilidad es una herramienta que realiza:

En donde no se incluyen las bibliotecas de sistema, "middleware", herramientas de desarrollo y demás.

Entre ellas no podemos nombrar cifrado y descifrado de archivos, compresión de archivos, desfragmentación de disco, editores de texto, respaldo, etc.



</doc>
<doc id="9740" url="https://es.wikipedia.org/wiki?curid=9740" title="Gastronomía del Perú">
Gastronomía del Perú

La gastronomía del Perú es muy diversa, tanto que el libro "357 listas para entender cómo somos los peruanos" llega a contar hasta 491 platos típicos.

La cocina peruana es el resultado de la fusión inicial de la tradición culinaria del antiguo Perú —con sus propias técnicas y potajes— con la cocina española en su variante más fuertemente influenciada por 762 años de presencia morisca en la Península Ibérica y con importante aporte de las costumbres culinarias traídas de la costa atlántica del África subsahariana por los esclavos.

Posteriormente, este mestizaje se vio influenciado por los usos y costumbres culinarios de los chefs franceses que huyeron de la revolución en su país para radicarse, en buen número, en la capital del virreinato del Perú. Igualmente trascendental fue la influencia de las inmigraciones del siglo XIX, que incluyó chinos-cantoneses, japoneses e italianos, entre otros orígenes principalmente europeos.

Como particularidad exclusiva de la gastronomía del Perú, "existen comidas y sabores de cuatro continentes en un solo país" y, esto, desde la segunda mitad del siglo XIX. Las artes culinarias peruanas están en constante evolución y, sumada a la variedad de platos tradicionales, hace imposible establecer una lista completa de sus platos representativos.

Cabe mencionar que a lo largo de la costa peruana existen registrados más de dos mil quinientos diferentes tipos de sopas, así mismo existen más de 250 postres tradicionales. La gran variedad de la gastronomía peruana se sustenta en tres fuentes: la particularidad de la geografía del Perú, la mezcla de culturas y la adaptación de culturas milenarias a la cocina moderna.

La presencia de los diversos pisos altitudinales de la cordillera de los Andes en el Perú y su cercanía al ecuador geográfico permite la existencia de una serie de microclimas y de especies, desde zonas de habituales nevadas hasta selvas tropicales, (con 84 de las 104 zonas climáticas del globo, es uno de los 12 países del mundo poseedores de mayor megadiversidad). Tiene condiciones adecuadas para el cultivo de frutas y verduras durante todo el año. Asimismo la corriente de Humboldt de aguas frías que corre por el océano Pacífico frente a la costa peruana permite la existencia de una gran variedad de peces y mariscos (Perú es uno de los principales países pesqueros del mundo).

Los andes centrales peruanos fueron el más grande centro de domesticación de plantas del mundo antiguo, con especies nativas como el maíz, tubérculos con dos mil quinientas variedades de papa, muchas de camote, yuca o mandioca, oca, maca; gramíneas quinua, kiwicha o amaranto, cañihua; frutas como chirimoya, lúcuma, pacae, tomate, calabaza, palta, tumbo, sauco; leguminosas tales como frijoles, pallares, maní y una infinidad de hierbas aromáticas.

Antes del arribo europeo, la geografía peruana albergaba una gran variedad de culturas, conquistadas todas por el Imperio incaico, cada una de las cuales tenía características gastronómicas particulares, aunque había algunas generalidades, de acuerdo con los cronistas de la conquista. Por ejemplo, los principales condimentos eran hierbas aromáticas, "cocha yuyo" (un tipo de alga fluvial), sal y, sobre todo, el ají, llamado "uchu" en tiempos incas y considerado hoy un elemento fundamental de la cocina peruana.

El Inca Garcilaso de la Vega en los "Comentarios Reales de los Incas" escribió al respecto: "Los de mi tierra son tan amigos del uchu, que no comerán sin él aunque no sea sino unas hierbas crudas". Era común la preparación de alimentos en forma deshidatrada, para evitar su descomposición, destacando el charqui, carne salada, y el caui, que es la oca secada al sol. Los antiguos peruanos además consumían inmensas cantidades de pescados y mariscos (el registro arqueológico de ello es abrumador) y complementaban su dieta con carne de pato, cuy y camélidos domésticos (alpaca y llama principalmente).

En las sociedades de la costa norte, además, se consumía la carne de ciertos lagartos y de venado. En las de la selva oriental se nutrían de la multitud de especies que proporcionaba la flora y fauna amazónica. Desde épocas milenarias, los antiguos peruanos preparaban chupes o sopas, guisaban (la carapulcra, por ejemplo, es considerado el tipo de guiso peruano más antiguo), elaboraban potajes con especies marinas crudas marinadas con ají, tumbo y hierbas, de donde se origina el cebiche que en la época precolombina tenía otro nombre, en quechua.

Tenían diferentes formas de procesar alimentos, salaban pescado, tostaban el maíz (obteniendo la cancha serrana, que es hasta hoy el «piqueo» peruano más simple y popular) o pelaban sus granos y los secaban (obteniendo "mote"). Asimismo preparaban charqui -o carne de camélido disecada, salada y deshilachada- y diferentes tipos de chuño -tubérculos resecados y congelados a la intemperie-. Cocinaban en ollas de barro y, en ocasiones, organizaban grandes banquetes de carne y vegetales a partir de hornos de tierra natural (pachamancas y huatias).

Asimismo se bebían diferentes formas de cerveza de maíz (chicha) y de yuca (masato). La historia precolombina identifica al Perú como un país gastronómico. Así en la leyenda sobre «Llampayeq» (Lambayeque) recopilada por Miguel Cabello Valboa en 1532, menciona al cocinero del rey Naylamp llamado OcchoColo en el Reino Sicán del siglo IX. Luego en la leyenda de los hermanos Ayar menciona que salieron del cerro Tamputoco (Tampu, Tambu, lugar donde se guardan alimentos) y sus nombres fueron Ayar Cachi (quinua con sal), Ayar Uchu (quinua con ají), Ayar Auca (quinua con frejol), Ayar Manco (el que cuida la quinua).

Desde el inicio de la presencia española, se incorporaron nuevos usos y costumbres culinarios con el comienzo del virreinato. La fritura, el uso de los lácteos (incorporado a algunos chupes o sopas), además de la carne de res, cerdo, huevo de gallina y nuevas aves de corral; además llegaron algunos cultivos que resultarían esenciales para la nueva cocina como la cebolla y el ajo que combinados con el ají serían los principales ingredientes de muchos platos peruanos.

La lima traída por los españoles y adaptado con el tiempo a la tierra peruana, se fue transformando en la variedad peruana actual denominada limón, de color verde, pequeño y ácido y que deviene en uno de los componentes básicos del cebiche. La vid (de la que se origina el pisco) y los vinos llegan también al comienzo de este período. En los primeros encuentros entre españoles y nativos, durante la conquista del imperio incaico, intercambiaron los trozos de cerdo ibérico frito con papas, camotes y maíz autóctono.

Francisco Pizarro, quien criaba cerdos en su infancia, era el principal aficionado a este plato llamado chicharrón durante los inicios de la presencia española en este territorio. La dedicación de muchos conventos de monjas a la cocina en un entorno donde abundaban las plantaciones de azúcar (especie traída también por los españoles) e inmensas variedades de frutas nativas originó asimismo una larga tradición repostera, destacándose el alfajor, el maná preparado en distintas variedades, formas y colores según la ocasión, así como otras decenas de postres de la época.

Los esclavos africanos aportaron lo suyo en una serie de guisos, además del uso de las partes blandas de la carne desechadas por las élites, que condimentaban abundantemente para disminuir los fuertes sabores de la carne y cocinados a las brasas. De aquí salieron muchos de los más representativos platos de la actual comida criolla, como por ejemplo: los anticuchos, la sangrecita, el camote con relleno, el cau cau, la pancita, el rachi, las mollejitas, la chanfainita, la patita con maní, el choncholí y el tacu-tacu.

El antropólogo peruano Humberto Rodríguez Pastor destaca el tipo de tamal tradicional peruano como un legado afroperuano en su obra "La vida en el entorno del tamal peruano". La citada vianda es introducida en este territorio desde los primeros años de la presencia española que vino con sus esclavos africanos. La gran cantidad de ellos procedentes de la costa atlántica africana marcó demográficamente la Ciudad de los Reyes ya que en el siglo XVII, más del 60 % de la población de la capital era de origen africano.

Luego de la independencia se dieron una serie de migraciones de diversas procedencias que integraron sus propias tradiciones a la ya dinámica culinaria local. La migración de los chinos-cantoneses de mediados del siglo XIX popularizó el salteado a fuego fuerte y los sabores agridulces en las carnes además del uso de nuevas hierbas y del sillao. Pero su aporte más notorio fue el arroz. Si bien ya se consumía desde el siglo XVI, es luego de la migración china que el arroz se popularizó y se convirtió en la guarnición peruana por excelencia, en detrimento del pan.

La forma de arroz favorita en el Perú es el arroz graneado no demasiado cocido, se hace con arroz de grano largo, sin embargo, se distanciaba de la preparación china en el uso del ajo y la sal. Otra inmigración en la segunda mitad del siglo XIX no menos influyente es la italiana, que popularizó el uso de las pastas, el pastel de acelga, los dulces y postres como el panetón, que es obligado en las navidades a lo largo del país. La migración japonesa de fines del siglo XIX, finalmente, impactó notablemente sobre la cocina marina peruana. Cortes y técnicas japonesas muy prolijas en la presentación de los platos, se unen a salsas y preparaciones peruanas y nace una nueva vertiente culinaria en el Perú. Así por ejemplo del cruce del sashimi japonés y del cebiche peruano nació el tiradito.

En la última década del siglo XX, e inicios del siglo XXI, la cocina peruana empezó a popularizarse fuera de sus fronteras. En la Cuarta Cumbre Internacional de Gastronomía Madrid Fusión 2006, realizada del 17 al 19 de enero de 2006, Lima fue declarada capital gastronómica de América, en tanto que el 12 de diciembre de 2012, el Perú fue distinguido en los World Travel Awards, llevados a cabo en Nueva Delhi (India), como Principal Destino Culinario a nivel mundial, superando a Australia, China, España, Estados Unidos, Francia, India, Italia, Japón, Malasia, México y Tailandia, países de reconocida trayectoria gastronómica, un reconocimiento que habla de la gran competitividad de la alta cocina peruana.

La gastronomía del país está registrada como una marca mundial, y por ende como producto bandera del Perú. Debido a esta rica variedad y a la armonía de su sabor y los alimentos empleados, la gastronomía peruana es constantemente premiada internacionalmente y sus chefs suelen obtener a menudo medallas internacionales que los distinguen. Un elemento destacable es su constante apertura a las innovaciones y el continuo desarrollo de nuevos platos, incorporando a la gastronomía la búsqueda continua de la experimentación y la vanguardia.

Así como cada región conserva su riqueza culinaria, en la alta gastronomía destaca la mezcla de colores y de productos alimenticios, una muestra de ello es la llamada cocina novoandina, un nuevo estilo culinario surgido en el Perú por el interés de los gastrónomos locales de retomar costumbres alimenticias del pasado prehispánico para recrearlas, rescatando y revalorizando así muchos de los ingredientes autóctonos.

En esta recreación de la cocina andina, entran elementos procedentes de otros horizontes culturales como el europeo. Algunos de los productos nativos utilizados son tarwi, chuño, quinua, kiwicha, moraya, cochayuyu, maca, coca, uchu, olluco, oca, en platillos como el quinotto o el coca sour. Lima, capital cosmopolita y mestiza y Arequipa se han convertido en las sedes principales de esta corriente culinaria, aunque en los principales puntos andinos como Huaraz, Juliaca, Cuzco o Huancayo, este estilo ha cobrado también un gran auge.

La cocina peruana ha sido el punto de encuentro de diversas culturas, gracias a la inclinación por el mestizaje que ha caracterizado la historia del Perú. La cocina clásica peruana suele ser atractiva por su colorido y a veces por su matiz picante por el ají, siendo éste un ingrediente gravitante. Sin embargo, algunos ajíes no son picantes y solo sirven para darle color a la presentación de los platos típicos o para darles mayor gusto. El arroz es un alimento que acompaña muchos platos de la gastronomía del país, popularizado principalmente a partir del siglo XIX con la influencia de la inmigración chino-cantonesa.

La variedad de ingredientes que existe en el territorio (tanto nativas como las que llegaron de otras latitudes) permitió la evolución de una culinaria diversa, donde coexisten, sin oponerse, fuertes tradiciones regionales y una permanente reinvención de platos. Perú es considerado como uno de los centros genéticos más grande del mundo y muchos ingredientes de origen ancestral son utilizados en su cocina:














Esta tiene tres tipos principalmente: la costa, la sierra y la selva.

Está constituida por una variedad de platos y especies en las cuales tenemos:

Perú es uno de los dos principales productores y exportadores de harina de pescado para la alimentación animal en el mundo. Su riqueza en peces, su fauna y flora marinas son enormes, encontrándose tipos de animales o plantas que solo se dan en sus aguas. Pero también hay que destacar sus riquezas de agua dulce que se encuentran principalmente en el río Amazonas y sus afluentes, así como en sus lagos, como el Titicaca.

Cada región costera, diferente en fauna y flora, adapta su cocina de acuerdo a los productos de sus aguas. El chupe de camarones plato originario y típico del departamento de Arequipa, es uno de los platos más refinados de la costa peruana. Se trata de una sopa espesa a base de pescado y camarones, papas, leche y ají. En el Perú existen diferentes variedades de chupes, como chupe de habas, chupe de zapallo, chupe de olluquito, entre otros. Otros platos típicos de esta cocina son el ceviche, los choritos a la chalaca, el tiradito, la leche de tigre, la parihuela y el escabeche de pescado.

Con 250 postres tradicionales desde el siglo XIX, esta gran variedad se ha originado principalmente en las ciudades costeras desde la época del virreinato del Perú, como el suspiro de limeña, el ranfañote, los picarones, el turrón, la melcocha y la mazamorra morada, entre otros. En Tacna, a inicios del siglo XX se creó su plato principal el picante a la tacneña. Los restaurantes con cartas criollas en sus variadas formas son numerosos, las pastelerías abundan y constituyen una de las riquezas culinarias de las ciudades de Lima, Arequipa, Ica, Trujillo y Tacna.

La oferta de restaurantes de toda naturaleza y especialización es notoria. Uno de ellos, de lujo, que se encontraba frente al mar en la Costa Verde de Lima, ofrecía a mediodía un servicio libre con más de seiscientos platos diferentes servidos en forma simultánea, por lo cual ostentó el Récord Guinness. Entre los principales platos de la comida criolla tenemos: ají de gallina, carapulca, escabeche de pollo, arroz con pollo, tacu-tacu, arroz con pato, cau cau, lomo saltado, rocoto relleno, tamales, papa rellena, sancochado, anticuchos, causa a la limeña, entre otros.

Por otra parte, existe un plato híbrido muy popular en la costa del país, especialmente en la zona centro, llamado simplemente «Combinado». Consiste en una porción de papa a la huancaína con tallarines guisados tradicionalmente (adrezo de zanahoria, tomate y cebolla) y cebiche. Cabe agregar que "combinado" se le puede decir a cualquier plato que resulte ser un poco de otros platos; por lo que el término Combinado no se aplica necesariamente al platillo descrito antes.

El cabrito, es otro plato típico del Perú, nació en el norte del país entre Trujillo y Chiclayo, se trata de un mamífero del tamaño del cordero, también conocido como chivo, que es amacerada con distintos condimentos entre ellos el ají amarillo, también se le agrega chicha de jora y culantro, se sirve con arroz y frejol. Otros platos norteños son el shambar (se sirve solo los lunes), sopa teóloga, pepian de pava, causa en lapa, cuy frito con ajiaco, todos exclusivos de la cocina trujillana.

Los Andes son el origen de milenarias culturas y con ellas el sabor de la cocina peruana. En esta parte alta del país, la alimentación principal continúa siendo el maíz, la papa y multiplicidad de tubérculos. Productos introducidos como el arroz, el pan y las pastas hoy son también de consumo popular. La variedad de carnes consumida se ha enriquecido con vacunos, porcinos y ovinos; en lugares muy elevados como Huancavelica aún se consume carne de llama, de alpaca y animales silvestres.

La variedad y riqueza de la comida andina es similar a la de la comida costeña. Desde el cuy chactao a la sopa de morón y de la papa a la huancaína a la sopa chairo, a más de postres y granos sumamente originales que se consumen frescos o cocidos de diversas maneras. El caldo de cabeza y las costillas de carnero doradas son mínima muestra de un vasto catálogo que apenas si se ha difundido.

Algunos de los principales platos de la comida andina son: la pachamanca, la huatia, la papa a la huancaína, la ocopa y los platillos elaborados con cuy. Igualmente, gran variedad de peces de agua dulce forman parte de la gastronomía regional, siendo muy apreciada la trucha, introducida a fines del siglo XIX.

La zona de la selva del Perú tiene una gran biodiversidad en fauna, por lo cual es tradicional el consumo de variadas carnes, como huangana (cerdo silvestre), suri, tapir, roedores (majaz, añuje, punchana, sachacuy), armadillo, tortuga, monos choro y maquisapa. En la inmensa variedad, destaca el paiche, el segundo pez más grande de agua dulce (puede llegar a pesar hasta 180 kilos y medir hasta 3 metros de largo).

La comida de la selva peruana tiene como elementos populares, entre otros, un aderezo básico que es conocido como "misto" (o "mishkina"), el uso del "ingiri", que es como se conoce al plátano verde sancochado, el alto consumo de frutas y la cocción de las carnes, especialmente peces y también el juane, envueltas en hojas de bijao, una palmera que tiene un aroma particular. Con respecto a las carnes, son usualmente aportadas por la cacería y la pesca y en menor medida por la ganadería.

Con respecto a las frutas destaca el camu camu que concentra la mayor cantidad de vitamina C. También es muy extendido el consumo de aguaje, del cual se prepara un refresco llamado aguajina, con alto contenido de vitamina A. No obstante, abundan los frutos tropicales como el mango, la piña y muchos otros. Del plátano verde se prepara un refresco conocido como chapo. 

Un elemento importante de la comida de la selva son los licores, mayormente producto de la fermentación de licor de caña con especies locales (raíces, frutos, entre otros). Por ejemplo, el chuchuhuasi, el uvachado, el siete raíces, el rompe calzón, entre otros. Mención aparte merece el masato, una bebida de orígenes prehispánicos elaborada a base de yuca masticada y fermentada durante unos días en un recipiente artesanal de barro y arcilla de base ancha y cuello estrecho. Los platos más conocidos de la amazonía peruana son el juane y el tacacho con cecina pero también existen otros de alto consumo como el inchicapi, la patarashca, la ensalada de chonta, el timbuche, la ensalada de chonta, la salsa de ají charapita, arroz charapita, etc.

La repostería tradicional del Perú tiene inicio en la época de la colonia, en esta etapa fue decisiva la introducción del cultivo de caña de azúcar, las costumbres europeas y la presencia de esclavos africanos.
























Desde fines del siglo XX e inicios del siglo XXI, el cebiche, el pollo a la brasa y los platos de chifa constituyen los representantes más populares de la comida peruana, siendo masivo su consumo a lo largo de todo el territorio peruano y existiendo versiones para todas las clases sociales: desde preparados muy económicos que se consumen «al paso» hasta preparados gourmet muy exclusivos.

El cebiche, ceviche, seviche o sebiche, es un plato ampliamente difundido y declarado Patrimonio Cultural de la Nación por el gobierno peruano. La receta básica del cebiche es la misma en todas las regiones: pescado en trozos, zumo de limón, cebolla roja, ají y sal al gusto. Los pescados utilizados son muy diversos e incluyen especies tanto de agua dulce como de mar, asimismo se incluyen otros frutos de mar como mariscos y algas marinas e incluso vegetales. El plato se acompaña de productos locales como cancha serrana, camote, zarandaja, yuca y hojas de lechuga.

El chifa es un término utilizado en el Perú para referirse a la cocina que surgió de la fusión entre la comida peruana y aquella de los inmigrantes chinos, principalmente de la zona de Cantón, llegada a mediados del siglo XIX e inicios del siglo XX, asimismo se usa este término para denominar a los restaurantes donde esta comida es servida. En la actualidad los restaurantes de cocina china, con fuerte influencia en muchos casos de la criolla, están entre los más comunes en Lima y muchas otras ciudades del Perú. Los principales platos son el arroz chaufa, la sopa wantán, el tallarín saltado, el aeropuerto y el pollo chijaukai.

Es uno de los platos de mayor consumo en el país. Ha sido reconocido como «Especialidad Culinaria Peruana» por el gobierno peruano el 14 de octubre de 2004. Consiste básicamente en un pollo eviscerado macerado, en una marinada que incluye diversos ingredientes, horneado a las brasas. Los inicios de este plato en Perú se señalan en el distrito de Chaclacayo de Lima, su creador fue el suizo Roger Schuler, un criador de pollos y fundador del restaurante La Granja Azul.

En 1950 Schuler junto con Franz Ulrich inventaron y registraron la patente de la máquina para cocinar el pollo a la brasa, un sistema mecánico de giro planetario que hace que los pollos giren sobre su propio eje y alternen su movimiento circular, simultáneamente. En Perú, el plato se acompaña de papas fritas, ensalada y diversas cremas (mayonesa, mostaza, kétchup, salsa de aceituna, chimichurri y salsas de ají de toda clase); en la selva del Perú se suelen reemplazar las papas fritas por plátano frito.

El 16 de octubre de 2007, la gastronomía del Perú fue proclamada Patrimonio Cultural de la Nación; esta declaratoria manifiesta que la cocina peruana es una expresión cultural que contribuye a consolidar la identidad del país. Anteriormente a esta declaratoria, otros elementos de la gastronomía peruana fueron declarados Patrimonio Cultural de la Nación:


El más connotado chef peruano de finales del siglo XX e inicios del siglo XXI es Gastón Acurio, quien originó una revalorización, internacionalización y premiación de la culinaria peruana.

Otro chef connotado es Virgilio Martínez quien en 2017 fue elegido como el mejor chef del mundo.




















</doc>
<doc id="9742" url="https://es.wikipedia.org/wiki?curid=9742" title="Inmunología">
Inmunología

La inmunología es una rama amplia de biología y de las ciencias biomédicas que se ocupa del estudio del sistema inmunitario, entendiendo como tal al conjunto de órganos, tejidos y células que, en los vertebrados, tienen como función reconocer elementos ajenos dando una respuesta (respuesta inmunitaria).

La ciencia trata, entre otras cosas, el funcionamiento fisiológico del sistema inmunitario tanto en estados de salud como de enfermedad; las alteraciones en las funciones del sistema inmunitario (enfermedades autoinmunitarias, hipersensibilidades, inmunodeficiencias, rechazo a los trasplantes); las características físicas, químicas y fisiológicas de los componentes del sistema inmunitario "in vitro", "in situ", e "in vivo". La inmunología tiene varias aplicaciones en numerosas disciplinas científicas, que serán analizadas más adelante.

El término latino "immunis" —exento— es el origen de la palabra inmunidad, que se refiere al estado de protección contra anomalías infecciosas.

La disciplina de la inmunología surgió cuando se observó que los individuos recuperados de ciertos trastornos infecciosos quedaban protegidos después contra la enfermedad. Se cree que la primera referencia que describe a los fenómenos inmunitarios fue escrita por Tucídides, el historiador de las guerras del Peloponeso, en el año 430 a.n.e. Este texto describe que durante una plaga en Atenas, "solo los que se habían recuperado de ella podían cuidar a los enfermos porque no contraían el padecimiento por segunda vez."

Los primeros intentos registrados de inducir inmunidad de manera artificial los llevaron a cabo los chinos y los turcos en el siglo XV al intentar prevenir la viruela. Los informes describen el proceso de variolización en el que las costras secas dejadas por las pústulas de la viruela se inhalaban por las narinas o se insertaban en pequeños cortes de piel.

En 1796, el médico inglés Edward Jenner, al observar el hecho de que las niñeras que habían contraído la enfermedad de la pústula vacuna o pústula mamaria de la vaca (una enfermedad leve) quedaban inmunes contra la viruela razonó que al introducir líquido de una pústula vacuna en una persona (inoculación) podía protegérsele contra la viruela. Verificó su hipótesis inoculando en un niño de ocho años de edad con líquido de una pústula vacuna y luego lo infectó de manera intencional con viruela; el niño no presentó la enfermedad.

Louis Pasteur, con sus asistentes Charles Chamberland y Émile Roux, logró cultivar la bacteria que causaba el cólera de las gallinas y comprobó la participación de este microorganismo cuando los pollos inoculados con este murieron. Pasteur se fue de vacaciones y dejó su laboratorio con sus cultivos bacterianos, los que al paso del tiempo perdieron su patogenicidad. Al volver, inyectó a algunos de sus pollos con estos cultivos viejos y notó que enfermaban, pero no morían y supuso que se debía a la desvitalización del cultivo. Trató de repetir este experimento pero con un cultivo nuevo que al inyectar sobre los pollos los mataría, no obstante, su abastecimiento de pollos era limitado y tuvo que usar los mismos pollos. Cuando los inyectó, estos estaban protegidos contra la enfermedad. Con esto descubrió que el envejecimiento atenuó la cepa y que esta podría utilizarse para conferir protección contra el padecimiento. Denominó a la cepa atenuada vacuna (del lat. "vacca" que significa vaca) en honor al trabajo de Jenner. Este trabajo marco el inicio de la inmunología.

Pasteur descubrió que era posible atenuar o debilitar agentes patógenos que confirieran resistencia y esto lo demostró con otro experimento en el pueblo de Pouilly-le-Fort en 1881. Pasteur vacunó ovejas con el bacilo del carbunco ("Bacillus anthracis") atenuado con calor. En este experimento, solo las ovejas vacunadas vivieron. En 1885, Pasteur vacunó por primera vez a un humano, Joseph Meister, un niño que había sido mordido por un perro rabioso. Pasteur le administró virus de la rabia atenuados con lo que evitó el progreso de la enfermedad. Joseph creció y se convirtió en el custodio del Instituto Pasteur.

Las décadas que siguieron fueron emocionantes, dominadas por otros gigantes como Koch, Metchnikoff, Ehrlich, Behring von Bordet, Richet,y el joven Landsteiner, e influenciados por los descubrimientos de anticuerpos, complemento, diagnóstico serológico, anafilaxia, y numerosos otros fenómenos y técnicas.

Pasteur demostró que la vacunación funcionaba pero desconocía el motivo de esto. El trabajo experimental de Emil von Behring y Shibasaburo Kitasato en 1890 proporcionó la primera información sobre el mecanismo de inmunidad. Demostraron que el suero de animales inmunizados con anterioridad contra la difteria podían transferir el estado de inmunidad a animales no inmunizados. Gracias a este trabajo ganaron el premio nobel en medicina en 1901.

Años más tarde, varios científicos probaron durante la década siguiente que un componente activo del suero inmune podía neutralizar y precipitar toxinas y aglutinar bacterias. Este componente activo recibió nombres como antitoxina, precipitina y algutinina hasta que en 1930 Elvin Kabat demostró que la fracción de suero gamma (inmunoglobulinas) era la que generaba todas estas actividades. Las moléculas activas de esta fracción se llamaron anticuerpos.

La inmunología clásica está incluida dentro de los campos de la epidemiología. Estudia la relación entre los sistemas corporales, patógenos e inmunidad. El escrito más antiguo que menciona la inmunidad se considera el referente a la plaga de Atenas en el 430 a. C. Tucídides notó que la gente que se había recobrado de un ataque previo de la enfermedad podía cuidar a los enfermos sin contraer la enfermedad por segunda vez. Muchas otras sociedades antiguas tienen referencias de este fenómeno, pero no fue hasta los siglos XIX y XX donde el concepto fue llevado a la teoría científica.

El estudio de los componentes celulares y moleculares que comprende el sistema inmunitario, incluyendo sus funciones e interacciones, es el tema central de la inmunología. El sistema inmunitario ha sido dividido en un más primitivo sistema inmunitario innato, y un sistema inmunitario adaptativo o adquirido de los vertebrados; este último a su vez está dividido en sus componentes humorales y celulares.

La respuesta humoral (anticuerpos) es definida como la interacción entre los anticuerpos y los antígenos. Los anticuerpos son proteínas específicas liberadas de cierta clase de células inmunitarias (linfocitos B). Los antígenos son definidos como elementos que promueven la generación de anticuerpos. La inmunología trata de comprender las propiedades de estas dos entidades biológicas. Sin embargo, igualmente importante es la respuesta celular, que puede no solamente matar a las células infectadas, sino que también es crucial en el control de la respuesta de los anticuerpos. Se observa entonces que ambos sistemas son altamente interdependientes.

En el siglo XXI, la inmunología ha ampliado sus horizontes con las investigaciones desarrolladas en los nichos más especializados de la inmunología. Esto incluye la función inmunitaria de las células, órganos y sistemas normalmente no asociados con el sistema inmunitario, así como la función del sistema inmunitario fuera de los modelos clásicos de inmunidad.

La inmunología clínica es el estudio de las enfermedades causadas por los trastornos del sistema inmunitario (fallo, acción anormal y crecimiento maligno de los elementos celulares del sistema). También involucra enfermedades de otros sistemas, donde las reacciones inmunitarias juegan un papel en los rasgos clínicos y patológicos.

Las enfermedades causadas por los trastornos del sistema inmunitario se dividen en dos amplias categorías:

La enfermedad más conocida que afecta al sistema inmunitario es el sida, causado por el VIH. El sida es una inmunodeficiencia caracterizada por la pérdida de células T CD4+ ("helper") y macrófagos, que son destruidos por el VIH.

Los inmunólogos clínicos también estudian las formas de prevenir el rechazo a trasplantes, en el cual el sistema inmunitario destruye los alógenos o exógenos.

El uso de los componentes del sistema inmunitario en el tratamiento a una enfermedad o trastornos conocido como inmunoterapia. La inmunoterapia se usa en el contexto del tratamiento de los cánceres junto con la quimioterapia (drogas) y la radioterapia (radiación). Sin embargo, la inmunoterapia se usa frecuentemente en los pacientes inmunosuprimidos (como los enfermos de sida) y las personas que sufren otras deficiencias inmunitarias y enfermedades autoinmunitarias.

La especificidad del enlace entre antígeno y anticuerpo ha creado una herramienta excelente en la detección de las sustancias en una variedad de técnicas diagnósticas. Los anticuerpos específicos para determinado antígeno pueden ser conjugados con un radio-marcador, marcador fluorescente, o una enzima reveladora (por escala de color) y son usados como pruebas para detectarlo.

El estudio del sistema inmunitario en especies extintas y vivientes es capaz de darnos una clave en la comprensión de la evolución de las especies y el sistema inmunitario.

Un desarrollo de complejidad del sistema inmunitario pueden ser visto desde la protección fagocítica simple de los organismos unicelulares, la circulación de los péptidos antimicrobianos en insectos y los órganos linfoides en vertebrados. Por supuesto, como muchas de las observaciones evolutivas, estas propiedades físicas son vistas frecuentemente a partir de la mirada antropocéntrica. Debe reconocerse que, cada organismo vivo hoy tiene un sistema inmunitario absolutamente capaz de protegerlo de las principales formas de daño.

Los insectos y otros artrópodos, que no poseen inmunidad adaptativa verdadera, muestran sistemas altamente evolucionados de inmunidad innata, y son protegidos adicionalmente del daño externo (y la exposición a patógenos) gracias a su cutícula.

Rama de la inmunología que estudia no solo los fenómenos inmunológicos en el cerebro, sino también los centros nerviosos que intervienen en la respuesta inmune.




</doc>
<doc id="9743" url="https://es.wikipedia.org/wiki?curid=9743" title="Quiasma">
Quiasma

El quiasma (del gr. χίασμα, -ατος, disposición cruzada, como la de la letra χ) es el entrecruzamiento entre cromátidas no hermanas en el proceso de recombinación meiótica, tal como puede ser visualizado citogenéticamente (el entrecruzamiento es exclusivo cromosomas homólogos entre sus cromátidas no hermanas).
En una meiosis humana masculina pueden observarse un promedio de cincuenta y dos quiasmas repartidos uniformemente entre los 23 pares de cromosomas homólogos. En la meiosis femenina se producen más quiasmas (98) que presenta un promedio próximo a un quiasma. Cada una de las cromátidas hermanas tiene un 50% de probabilidad de establecer un quiasma concreto, por lo que la fracción máxima de recombinación (frecuencia de recombinación) esperable entre dos locus es de 0,5.


</doc>
<doc id="9744" url="https://es.wikipedia.org/wiki?curid=9744" title="Enfermedad genética">
Enfermedad genética

Una enfermedad o trastorno genético es una afección patológica causada por una alteración del genoma. Esta puede ser hereditaria o no; si el gen alterado está presente en los gametos (óvulos y espermatozoides) de la línea germinal, esta será hereditaria (pasará de generación en generación), por el contrario si sólo afecta a las células somáticas, no será heredada. Pueden ser monogénicas, poligénicas o cromosomicas

Hay varias causas posibles:

Los 46 cromosomas humanos (22 pares de autosomas y 1 par de cromosomas sexuales) entre los que albergan casi 3.000 millones de pares de bases de ADN que contienen alrededor de 80.000 genes que codifican proteínas. Las regiones que codifican ocupan menos del 5 % del genoma (la función del resto del ADN permanece desconocida), teniendo algunos cromosomas mayor densidad de genes que otros.

Uno de los mayores problemas es encontrar cómo los genes contribuyen en el complejo patrón de la herencia de una enfermedad, como ejemplo el caso de la diabetes, asma, cáncer y enfermedades mentales. En todos estos casos, ningún gen tiene el potencial para determinar si una persona padecerá o no la enfermedad.

Poco a poco se van conociendo algunas enfermedades cuya causa es la alteración o mutación de todo o alguna región de un gen. Estas enfermedades afectan generalmente a todas las células del cuerpo.

Unas de las enfermedades genéticas más común es el Síndrome Down. Esta enfermedad se produce cuando hay error en la división de las células provoca que haya 47 cromosomas, en lugar de 46.
















</doc>
<doc id="9745" url="https://es.wikipedia.org/wiki?curid=9745" title="Fibrosis quística">
Fibrosis quística

La fibrosis quística (abreviatura FQ) es una enfermedad genética de herencia autosómica recesiva que afecta principalmente a los pulmones, y en menor medida al páncreas, hígado e intestino, provocando la acumulación de moco espeso y pegajoso en estas zonas. Es uno de los tipos de enfermedad pulmonar crónica más común en niños y adultos jóvenes, y es un trastorno potencialmente mortal; los pacientes suelen fallecer por infecciones pulmonares debido a "Pseudomonas "o "Staphylococcus".

Es producida por una mutación en el gen que codifica la proteína "reguladora de la conductancia transmembrana de la fibrosis quística" (CFTR). Esta proteína interviene en el paso del ion cloro a través de las membranas celulares y su deficiencia altera la producción de sudor, jugos gástricos y moco. La enfermedad se desarrolla cuando ninguno de los dos alelos es funcional. Se han descrito más de 1500 mutaciones para esta enfermedad, la mayoría de ellas son pequeñas deleciones o mutaciones puntuales; menos de un 1 % se deben a mutaciones en el promotor o a reorganizaciones cromosómicas.

La FQ afecta a múltiples órganos y sistemas, originando secreciones anómalas y espesas de las glándulas exocrinas. La principal causa de morbilidad y mortalidad es la afectación pulmonar, causante del 95 % de los fallecimientos, sobre todo por infecciones repetidas originadas por obstrucción bronquial debida a la secreción de mucosidad muy espesa. Otros órganos afectados son el páncreas y en los varones el testículo.
Es una de las enfermedades genéticas más frecuentes en la raza caucásica, con una incidencia en dicha población de aproximadamente 1/5000 nacidos vivos. Se calcula que una de cada 25 personas de ascendencia europea, es portadora de un alelo no funcional.

El nombre "fibrosis quística" hace referencia a los procesos característicos de cicatrización (fibrosis) y formación de quistes dentro del páncreas, reconocidos por primera vez en 1930. También recibe la denominación mucoviscidosis (del lat. "muccus", "moco", y "viscōsus", "pegajoso").

Los enfermos presentan una alta concentración de sal (NaCl) en el sudor, lo que permite llegar al diagnóstico mediante su análisis, realizando el test del sudor. También mediante pruebas genéticas prenatales, natales a través de gibson y cooke.

No existe ningún tratamiento curativo, sin embargo hay tratamientos que permiten la mejora de los síntomas y alargar la esperanza de vida. En casos severos, el empeoramiento de la enfermedad puede imponer la necesidad de un trasplante de pulmón. La supervivencia media a nivel mundial de estos pacientes se estima en 35 años, alcanzando valores más altos en países con sistemas sanitarios avanzados; por ejemplo en Canadá la duración media de la vida era de 48 años en 2010.

La sintomatología de la Fibrosis Quística varía en función de la edad del individuo, el grado en que se ven afectados órganos específicos, la terapéutica instituida previamente, y los tipos de infecciones asociadas. Esta enfermedad compromete al organismo en su totalidad y muestra su impacto sobre el crecimiento, la función respiratoria, la digestión. El periodo neonatal se caracteriza por un pobre aumento de peso y por obstrucción intestinal producida por heces densas y voluminosas. Otros síntomas aparecen, más tarde, durante la niñez y al inicio de la adultez. Estos incluyen retardo del crecimiento, advenimiento de la enfermedad pulmonar, y dificultades crecientes por la mala absorción de vitaminas y nutrientes en el tracto gastrointestinal.

A la mayoría de los niños se les diagnostica Fibrosis Quística antes del primer año de vida, cuando la mucosidad pegajosa que afecta pulmones y páncreas, comienza a mostrar su impacto. En el tracto respiratorio, esas secreciones sirven como caldo de cultivo para diversas bacterias responsables de infecciones crónicas, con deterioro progresivo y permanente del parénquima pulmonar. Conforme se agrava la condición respiratoria, los pacientes sufren hipertensión pulmonar. Por otra parte, en el páncreas, el moco obstruye el tránsito de las enzimas sintetizadas por la glándula e impide que lleguen hasta el intestino para digerir y absorber el alimento.

La enfermedad pulmonar resulta del bloqueo de las vías aéreas más pequeñas con el moco espeso característico de la fibrosis quística. La inflamación y la infección producen daño a los pulmones y cambios estructurales que conducen a una variedad de síntomas. En las etapas iniciales, comúnmente se presentan tos incesante, producción copiosa de flema, y una disminución en la capacidad aeróbica. Muchos de estos síntomas ocurren cuando ciertas bacterias (fundamentalmente, "Pseudomonas aeruginosa") que normalmente viven en el moco espeso, crecen en forma descontrolada y causan neumonía. En estados avanzados de la FQ, los cambios en la arquitectura del pulmón producen dificultades respiratorias crónicas.

Otros síntomas incluyen expectoración de sangre o esputo sanguinolento, dilatación crónica de los bronquios o bronquiolos (bronquiectasia), elevación de la presión sanguínea en el pulmón, insuficiencia cardíaca, sensación de no estar recibiendo suficiente oxígeno o disnea, insuficiencia respiratoria y atelectasia; podría requerirse soporte ventilatorio. Además de las infecciones bacterianas más comunes, las personas con FQ desarrollan con mayor facilidad otros tipos de enfermedades respiratorias. Entre éstas se encuentra la aspergilosis broncopulmonar alérgica, caracterizada por una respuesta de hipersensibilidad ante un hongo (moho) ordinario del género "Aspergillus" ("Aspergillus fumigatus"), que agudiza los problemas respiratorios. Otro ejemplo es la infección con el complejo "Mycobacterium avium" (MAC), grupo de actinobacterias emparentadas con "Mycobacterium tuberculosis", que puede ocasionar daños mayores al pulmón, y que no responde a la terapéutica con antibióticos convencionales.

El moco en los senos paranasales es igualmente denso y pegajoso, y también puede causar oclusión de los orificios por donde los senos habitualmente drenan, lo cual hace que se acumulen secreciones que actúan como caldo de cultivo para los patógenos antes mencionados. En estos casos, se pueden presentar dolor facial, fiebre, secreción nasal profusa y cefaleas. En las personas con FQ, a menudo se observa crecimiento sobreabundante de tejido nasal (pólipos), a consecuencia de la inflamación por infección sinusal crónica. Estos pólipos pueden agravar la obstrucción de las vías respiratorias superiores e intensificar las dificultades respiratorias.

Con anterioridad a la difusión de las pruebas prenatal y neonatal para Fibrosis Quística, era frecuente que la enfermedad se detectara al constatar que el recién nacido no podía expulsar sus primeras heces (meconio). El meconio puede obstruir completamente los intestinos y causar graves trastornos. Esta condición, llamada íleo meconial, ocurre en el 10 % de los recién nacidos con FQ. Recientemente se han identificado variantes genéticas en genes relacionados con el transporte de iones en el intestino delgado que predisponen al desarrollo del íleo meconial. Asimismo, es también frecuente la asociación de FQ con protrusión de las membranas rectales internas (prolapso rectal), debida al mayor volumen fecal, a la malnutrición, y a la elevación de la presión intraabdominal por tos crónica.

El moco glutinoso observado en el pulmón tiene su correlato en las secreciones espesas del Páncreas, órgano responsable de proveer jugos digestivos que facilitan la descomposición química de los alimentos. Estas secreciones impiden el movimiento de las enzimas pancreáticas hacia el intestino y producen daño irreversible en el páncreas, a menudo acompañado de dolorosa inflamación (pancreatitis). La deficiencia de enzimas digestivas se traduce en un impedimento para absorber los nutrientes, con la subsiguiente excreción de éstos en las heces: este trastorno es conocido como malabsorción. La malabsorción conduce a la desnutrición y al retardo en el crecimiento y desarrollo, ambos debidos a la baja biodisponibilidad calórica. Las personas con FQ tienen, en particular, problemas para absorber las vitaminas A, D, E, y K. Además de la afección pancreática, suelen experimentar acidez crónica, xerostomía, obstrucción intestinal por intususcepción, y constipación. Los pacientes mayores desarrollan también el síndrome de obstrucción intestinal distal causado por las heces glutinosas.

Estas secreciones también pueden causar problemas en el hígado. La bilis, producida por esta víscera para facilitar la digestión, podría bloquear las vías biliares, dañando los tejidos adyacentes. Con el tiempo, esta situación conduce a la cirrosis. En ese caso, resultan comprometidas funciones de primer orden, tales como las implicadas en la neutralización de toxinas, y en la síntesis de importantes proteínas (por ejemplo, los factores de coagulación, responsables de la coagulación sanguínea).

El páncreas contiene los islotes de Langerhans, que son los responsables de producir insulina, una hormona que ayuda a regular los niveles de glucosa en sangre. Un daño en el páncreas puede provocar la pérdida de las células de los islotes y conducir a la diabetes. Por otra parte, la vitamina D suplementada por la alimentación está implicada en la regulación del calcio y del fósforo. La baja disponibilidad de ésta, a causa de la mala absorción, conduce a la osteoporosis, aumentando el riesgo de sufrir fracturas. Adicionalmente, las personas con FQ a menudo presentan, en manos y pies, una malformación denominada dedos en palillo de tambor, la cual se debe a los efectos de esta enfermedad crónica y a la hipoxia en sus huesos.

El retardo en el crecimiento es un sello distintivo de esta enfermedad. Los niños con FQ no logran, por lo general, ganar peso y altura en tasas comparables a las de sus pares; a menudo, solo reciben diagnóstico apropiado una vez que se investigan las causas de este fenómeno. Las determinantes del retardo en el crecimiento son multifactoriales e incluyen la infección pulmonar crónica, la malabsorción de nutrientes en el tracto gastrointestinal, y el aumento de la demanda metabólica asociado a la afección crónica.

La fibrosis quística puede diagnosticarse por tamizaje en recién nacidos, examen de electrolitos del sudor, o prueba genética. Al año 2006, en los Estados Unidos, el diez por ciento de los casos son detectados poco después del nacimiento como parte de los programas de pesquisa neonatal, que identifican niveles elevados en la enzima tripsina. Sin embargo, en la mayoría de los países estos exámenes no se realizan en forma rutinaria. Por esta causa, es frecuente que los afectados solo reciban diagnóstico apropiado una vez que los síntomas fuerzan una evaluación para esta enfermedad. La prueba diagnóstica más comúnmente utilizada es el examen del sudor, descrito por Lewis E. Gibson y Robert E. Cooke en 1959, usando electroforesis cuantitativa (iontoforesis) con un fármaco estimulante de la sudoración (pilocarpina). Esta sustancia, que posee carga positiva, se aplica sobre un electrodo positivo (+), en contacto con la piel. Luego, mediante el paso de corriente eléctrica, la droga migra por el tegumento hacia otro electrodo de carga opuesta (-), colocado a cierta distancia, hasta atravesar la epidermis, produciendo la estimulación de las glándulas sudoríparas y causando una sudoración controlada. Las muestras de sudor son luego recolectadas en papel de filtro o en un tubo capilar y son analizadas, determinándose las concentraciones de sodio y cloruro. Las personas con FQ poseen niveles más altos de estos iones en el sudor. Una vez que el examen del sudor ha dado positivo, se realiza un diagnóstico más detallado y preciso, mediante la identificación de las mutaciones en el gen CFTR.

Existen diversas pruebas para identificar eventuales complicaciones y controlar la evolución de la enfermedad. Las imágenes obtenidas por rayos X y TAC facilitan la detección de signos de lesión o infección en los pulmones. El cultivo de esputo, examinado por microscopio, provee información respecto de cuáles son las bacterias responsables, y permite escoger los antibióticos más efectivos. Las pruebas de función pulmonar miden las capacidades pulmonares, los volúmenes pulmonares y la rapidez con que éstos pueden ser movilizados (flujos aéreos). Por medio de tales exámenes, es posible determinar si es procedente un tratamiento con antibióticos o bien evaluar la respuesta al mismo. Los análisis de sangre pueden identificar problemas hepáticos, deficiencias vitamínicas, y revelar la irrupción de la diabetes. Los dispositivos DEXA o DXA (del inglés para "absorciometría de rayos X de energía dual"), se utilizan como prueba para determinar la presencia de osteoporosis. Por último, la cuantificación de elastasa fecal, facilita la detección de insuficiencia de enzimas digestivas.

La proteína sintetizada a partir del gen CFTR se une a la membrana externa de las células en las glándulas sudoríparas, Pulmón, Páncreas, y otros órganos afectados. La proteína atraviesa esta membrana y actúa como un canal iónico conectando la parte interna de la célula (citoplasma) con el fluido extracelular. Este canal es mayormente responsable de controlar el paso de cloruro hacia (y desde) el medio interno. Cuando la proteína CFTR no funciona correctamente, este movimiento se ve restringido, reteniéndose cloruro en el espacio extracelular. Debido a que el cloruro tiene carga eléctrica negativa, los iones con carga positiva tampoco podrán cruzar la membrana citoplasmática, a causa de la atracción eléctrostática ejercida por los iones cloruro. El sodio es el más común entre los iones presentes fuera de la célula, y la combinación de sodio y cloruro da lugar al cloruro de sodio, el cual se pierde en grandes cantidades en el sudor de los individuos con FQ. Esta pérdida de sal constituye el argumento básico para explicar la utilidad diagnóstica del test del sudor.

El mecanismo por el cual esta disfunción celular produce las manifestaciones clínicas antes descritas no se conoce con exactitud. Una de las teorías que intenta explicarlo, sugiere que la falla de la proteína CFTR para transportar el cloruro, determina la acumulación de abundante moco en los pulmones, creando un medio propicio (rico en nutrientes) para las bacterias, que logran así eludir al sistema inmunitario. También se postula que esta anomalía en la proteína CFTR induce un aumento paradójico en la captura de sodio y cloruro, lo que estimula la reabsorción de agua, y resulta en la formación de la mucosidad deshidratada y espesa. Otras teorías se enfocan en el fenómeno del movimiento de cloruro hacia el exterior de la célula, que también provoca desecamiento del moco y de las secreciones pancreáticas y biliares. En general, estas hipótesis coinciden en atribuir los mayores trastornos a la obstrucción de los conductos más delgados por las secreciones espesas y glutinosas en los distintos órganos afectados. Esta situación condiciona la infección crónica y promueve la remodelación estructural del pulmón, además de producir daño pancreático (mediado por las enzimas digestivas aglomeradas), y obstrucción de los intestinos por grandes bolos fecales.

Los pulmones de las personas con Fibrosis Quística son colonizados e infectados por bacterias desde edades tempranas. Los microorganismos que se propagan en estos pacientes, prosperan en el moco anómalo acumulado en las vías respiratorias más estrechas. El moco glutinoso estimula el desarrollo de microambientes bacterianos ("biofilms") que resultan difíciles de penetrar para las células inmunes y los antibióticos. Por su parte, los pulmones responden al daño continuo, infligido por las secreciones espesas y las infecciones crónicas, remodelando gradualmente las vías respiratorias inferiores (bronquiectasia), lo que vuelve a la infección aún más difícil de erradicar.

Con el paso del tiempo, cambian tanto el tipo de bacterias que afectan a estos pacientes, como las características específicas con que las mismas se presentan. En una primera etapa, ciertas bacterias ordinarias como "Staphylococcus aureus" y "Haemophilus influenzae" colonizan e infectan los pulmones. Más tarde, sin embargo, prevalecen "Pseudomonas aeruginosa" (y, a veces, el complejo "Burkholderia cepacia", integrado por diferentes especies de "Burkholderia"). Una vez diseminadas por las vías respiratorias, estas bacterias se adaptan al medio y desarrollan resistencia a los antibióticos convencionales. "Pseudomonas" puede adquirir ciertas características especiales, dando lugar a la formación de grandes colonias — estas cepas son conocidas como "Pseudomonas" "mucoide" y son raras en personas libres de la enfermedad.

Uno de los modos en que la infección se propaga es por transmisión entre individuos con FQ. En el pasado, era habitual que éstos participaran, en forma conjunta, de campamentos veraniegos y otras actividades de esparcimiento. Los hospitales alojaban a los pacientes con FQ en un área en común, y el equipamiento de rigor (por ejemplo, los nebulizadores) no era esterilizado entre usos sucesivos. Esto condujo a la transmisión de cepas bacterianas muy peligrosas entre grupos de pacientes. Actualmente, la rutina en establecimientos de atención sanitaria consiste en aislar a estos pacientes unos de otros; además, el personal a cargo de su cuidado, debe vestir batas y guantes para limitar la proliferación de cepas bacterianas virulentas. Con frecuencia, los pacientes afectados por bacterias particularmente peligrosas reciben atención en días y en edificios diferentes a los asignados a quienes no tienen esas infecciones.
Además de la infección bacteriana, los pacientes con FQ están predispuestos a la colonización fúngica por la capacidad que tiene algunos hongos de colonizar la vía respiratoria inferior y por los frecuentes ciclos de antibióticos que precisan para el control de la enfermedad. Los hongos que se cultivan con más frecuencia son el "Aspergillus fumigatus" y la "Candida albicans", esta colonización se traduce en una tasa elevada de respuesta inflamatoria frente a los hongos. En la actualidad no está bien definido el papel de los hongos en la FQ, aunque se consideran que son no patógenos, excepto en los casos de aspergilosis invasiva y de aspergilosis broncopulmonar alérgica.

Se trata de una enfermedad autosómica recesiva. En su forma más común, una mutación de un aminoácido (falta una fenilalanina en la posición 508) conduce a un fallo del transporte celular y localización en la membrana celular de la proteína CFTR. Se han descrito más de 1800 mutaciones, siendo la mayoría de ellas pequeñas deleciones, aunque con diferentes efectos, como cambios en el marco de lectura, cambios de aminoácidos, terminación prematura de la proteína o alteraciones en el splicing.

El gen CFTR está localizado en el brazo largo del cromosoma 7, en la posición 7q31.2, ocupando 180 000 pares de bases: más precisamente, desde el par 116 907 252 al 117 095 950 del cromosoma. Es un gen de gran tamaño, que posee 250 kb y que incluye 27 exones. Fue localizado y secuenciado por mapeo genético.

Este gen codifica la síntesis de un canal iónico de 1480 aminoácidos, una proteína que transporta iones cloruro a través de las células epiteliales, y que controla la regulación de otros transportadores. En las personas con fibrosis quística, esta proteína está ausente o bien se encuentra en proporciones sensiblemente menores a las habituales.

La penetrancia de la enfermedad es variable según el alelo, y a su vez, la expresión del alelo depende del entorno y del genoma de la persona afectada.

Son diversos los mecanismos por los cuales estas mutaciones causan problemas en la proteína CFTR. En particular, la mutación ΔF508, genera una proteína que no se pliega de manera normal y acaba siendo degradada por la célula. Varias mutaciones comunes en la población askenazí dan lugar a la síntesis de proteínas demasiado cortas, a causa de una conclusión anticipada de su producción. Otras mutaciones menos frecuentes originan proteínas que no utilizan la energía como es debido, no permiten que el cloruro cruce la membrana apropiadamente, o son degradadas a una tasa más rápida que la normal. La deficiencia en el transporte de cloro hace que las células no expulsen agua al exterior y por lo tanto el moco sea más espeso. Ciertas mutaciones pueden conducir también a una merma en la producción de copias de la proteína CFTR.

Estructuralmente, el gen CFTR pertenece a la denominada superfamilia de transportadores ABC (acrónimo para el inglés "ATP Binding casete", "casete de unión a ATP"). La estructura terciaria de la proteína codificada por este gen, consta de dos dominios capaces de hidrolizar adenosín trifosfato, lo que permite a la proteína utilizar energía en la forma de ATP. Asimismo, otro par de dominios, cada uno constituido por seis hélices alfa, posibilita el paso de la proteína a través de la membrana celular. La activación se concreta por reacción de fosforilación en un sitio de unión regulador, sobre todo mediante la proteína quinasa A (PKA, —antes denominada cAPK o proteína quinasa dependiente del adenosín monofosfato cíclico). El carboxilo terminal (C-) de la proteína está unido al citoesqueleto por interacción con dominios proteicos PDZ.

Existen una serie de pruebas que se vienen realizando de forma común para determinar las anomalías de los metabolitos relacionados con la fibrosis quística (especialmente el cloro). Entre ellas se encuentran:

El Diagnóstico Molecular de la enfermedad es complejo, ya que en noviembre de 2010 nos encontramos con 1824 mutaciones descritas y la mayoría son puntuales o pequeñas deleciones. Estas mutaciones se agrupan en función del efecto que tienen sobre el gen y sobre el fenotipo de la enfermedad. Además de la variabilidad de las mutaciones en sí mismas, las distintas poblaciones tienen frecuencias diferentes para las mismas, por lo que los estudios y test diagnósticos deben gestionarse considerando este aspecto. No obstante, la más común en la mayoría de las poblaciones es la deleción 508F.

Hay que tener en cuenta que la distribución de alelos varía mucho en cada población, por lo que hay que adaptar los tests para detectar aquellas variantes más comunes en la población que se esté estudiando.

Actualmente a los niños nada más nacer se les hace un diagnóstico genético mediante secuenciación del gen CFTR para saber si tienen la enfermedad, ya que es una enfermedad tratable. Cuando antes comienza el tratamiento, mayor calidad de vida y mayor longevidad.

Las parejas que están atravesando un embarazo o tienen planes respecto de la gestación, pueden ser evaluadas en busca de mutaciones del gen CFTR, con el objeto de determinar las probabilidades de que su hijo nazca con fibrosis quística. La prueba se suele realizar en uno de los padres o en ambos y, en caso de detectarse un riesgo elevado de FQ, se efectúa también en el feto. Debido a que el diagnóstico prenatal no habilita formas de tratamiento superiores o alternativas, la principal razón por la que se lleva a cabo es, en la práctica, proporcionar la posibilidad del aborto en caso de que el feto presente la enfermedad. La prueba para fibrosis quística en parejas se ofrece de manera generalizada en países como los Estados Unidos, y el Colegio Americano de Obstetras y Ginecólogos (ACOG, por sus siglas en inglés) recomienda la prueba en parejas que poseen un historial de FQ entre sus familiares directos o parientes cercanos, así como también en aquellas con riesgo elevado debido a su filiación étnica.

Debido a que el desarrollo de la FQ en el feto requiere que cada padre transmita una copia del gen CFTR mutante, y al alto costo del examen prenatal, la prueba suele realizarse, inicialmente, solo en uno de los progenitores. Si éste resulta ser portador de una mutación del gen CFTR, entonces se examina al otro para determinar el riesgo de que su hijo tenga la enfermedad. La FQ puede resultar de más un millar de mutaciones diferentes y, al año 2006, no es posible efectuar estudios de laboratorio para cada una de ellas. La prueba se remite a analizar la sangre en busca de las más comunes, como ΔF508 - la mayoría de las modalidades disponibles comercialmente detectan no más de 32 variantes distintas. Si se conoce el dato de que una familia tiene una mutación poco común, esta última puede buscarse específicamente. Como consecuencia de que no todas las mutaciones conocidas son detectadas por las pruebas corrientes, un resultado negativo no garantiza que el niño vaya a estar libre de la enfermedad. Por otro lado, dado que las mutaciones sondeadas son necesariamente aquellas más comunes en los grupos de más alto riesgo, las pruebas en etnias de bajo riesgo son menos exitosas, ya que las mutaciones más extendidas en estos grupos son menos frecuentes en la población general.

Las parejas en situación de riesgo, a menudo realizan pruebas adicionales durante el embarazo o antes de que éste se produzca. La fecundación in vitro con diagnóstico genético preimplantacional ofrece la posibilidad de examinar el embrión antes de su colocación en el útero. Esta prueba se realiza tres días después de la fecundación y procura determinar la presencia de genes CFTR anormales. Si, en un embrión, resultan identificados dos genes CTRF mutantes, éste será excluido de la transferencia, implantándose otro que cuente con, al menos, un gen normal.

Durante el transcurso del embarazo, es posible realizar pruebas tanto sobre la placenta (muestra de vellosidad coriónica) como sobre el líquido amniótico que rodea al feto (amniocentesis), con la ayuda del ultrasonido. Sin embargo, la biopsia de vellosidades coriónicas se correlaciona con riesgo de muerte fetal en una tasa de 1 en 100, y la amniocentesis, de 1 en 200, por lo que es esencial determinar los beneficios adecuadamente para sopesar los riesgos, antes de proceder con la prueba. Alternativamente, algunas parejas eligen someterse a técnicas de reproducción asistida con óvulos donantes (recurriendo a la fecundación in vitro) o con esperma donante (inseminación artificial por donante).

Un aspecto fundamental en la terapéutica de la Fibrosis Quística es el control y tratamiento del daño pulmonar causado por el moco espeso y por las infecciones, con el objeto de mejorar la calidad de vida del paciente. Para el tratamiento de las infecciones crónicas y agudas se administran antibióticos por vías intravenosa, inhalatoria y oral. También se utilizan dispositivos mecánicos y fármacos (en forma de inhaladores) para controlar las secreciones, y de esta manera descongestionar y desobstruir las vías respiratorias. Otros aspectos de la terapia se relacionan con el tratamiento de la diabetes con insulina, de la enfermedad pancreática con reemplazo enzimático. Adicionalmente, se postula la eficacia de distintos procedimientos, como el trasplante y la terapia génica, para resolver algunos de los efectos asociados a esta enfermedad.

Una dieta sana, elevado ejercicio y tratamientos agresivos con antibióticos está aumentando la esperanza de vida de los enfermos.

Los antibióticos se prescriben siempre que exista sospecha de neumonía o se constate deterioro en la función pulmonar. Habitualmente, se los escoge en función del historial de infecciones que afectaron al paciente previamente. Muchas de las bacterias comunes en la fibrosis quística son resistentes a gran cantidad de antibióticos y requieren semanas de tratamiento intravenoso con vancomicina, tobramicina, meropenem, ciprofloxacina y piperacilina.
La terapia prolongada a menudo requiere hospitalización y canalización de una vía intravenosa permanente, como por ejemplo un catéter central insertado percutáneamente (PICC). Asimismo, es frecuente la indicación simultánea de antibióticos administrados por inhalación, como la tobramicina, la colistina y la gentamicina, por varios meses, con el objeto de mejorar la función pulmonar impidiendo la proliferación bacteriana. Algunos antibióticos orales como la ciprofloxacina o la azitromicina se utilizan a veces para ayudar a prevenir la infección o para controlarla una vez que está en curso. En algunos casos pasan años entre sucesivas hospitalizaciones, mientras que en otros se requiere la internación cada año para poder realizar el tratamiento.

En tratamientos prolongados, varios de los antibióticos más comunes (como la tobramicina y la vancomicina) pueden causar pérdida de audición por ototoxicidad o problemas en los riñones. Con el objeto de prevenir tales efectos secundarios, es habitual medir cuantitativamente las concentraciones de estos medicamentos en sangre y, de ser necesario, ajustar la dosificación.

Son diversas las técnicas que se implementan con el objeto de fluidificar el esputo y facilitar su expectoración. En el medio hospitalario se utiliza la fisioterapia; un terapeuta practica una serie de maniobras mediante presiones y percusiones (palmoteo) ejercidas sobre el exterior del pecho (tórax) varias veces al día. Los dispositivos mecánicos que actúan bajo el mismo principio que aquellas técnicas básicas de drenaje postural, incluyen el ventilador de alta frecuencia oscilatoria y los aparatos de ventilación percusiva intrapulmonar, de los que existen modelos portátiles, adaptables al uso hogareño. El ejercicio aeróbico es altamente beneficioso para las personas con fibrosis quística, ya que no solo promueve la descongestión del esputo, sino que mejora la salud cardiovascular y el estado general.
Entre las sustancias administradas por inhalación que ayudan a aligerar las secreciones y facilitan su expulsión, se encuentran la dornasa alfa y la solución salina hipertónica. La dornasa alfa es una desoxirribonucleasa (ADNasa o DNasa) humana recombinante, que descompone el ADN en el esputo, reduciendo así la viscosidad de este último. La N-acetilcisteína (un derivado del aminoácido cisteína) también actúa fluidificando el esputo, pero las investigaciones y la experiencia disponibles han demostrado que los beneficios son poco significativos. Por último, broncodilatadores como el salbutamol y el salmeterol (ambos agentes, agonistas β-adrenérgicos) o el bromuro de ipratropio (un antagonista del receptor colinérgico, derivado cuaternario de la atropina) se utilizan para aumentar el tamaño de las vías respiratorias más pequeñas, al relajar el músculo liso bronquial.

En la medida en que se agrava la condición pulmonar, puede requerirse soporte respiratorio mecánico. Por las noches, algunos pacientes deben usar máscaras especiales que actúan empujando el flujo aéreo hasta los pulmones. La ventilación no invasiva mediante máscara nasal y presión positiva (VPAP, por el acrónimo para el inglés "variable positive airway pressure"), ayuda a prevenir, durante el sueño, caídas significativas en los niveles sanguíneos de oxígeno. También puede usarse en el curso de la fisioterapia respiratoria para favorecer la expulsión de esputo. Sin embargo, en casos severos, puede ser necesario implementar formas invasivas de asistencia respiratoria con intubación endotraqueal (esto es, colocación de un tubo o sonda en la tráquea).

Los recién nacidos con íleo meconial típicamente requieren cirugía; por lo general, no sucede lo mismo en adultos con síndrome de obstrucción intestinal distal. El tratamiento de la insuficiencia pancreática basado en reemplazo de las enzimas digestivas menguadas permite que los intestinos absorban de manera apropiada nutrientes y vitaminas que, de otro modo, se perderían en la heces. Aun así, la mayoría de los individuos con FQ deben recibir dosis adicionales de vitaminas A, D, E y K a partir de suplementos, y seguir una dieta de alto valor calórico. La diabetes que suele acompañar la FQ se trata con inyecciones de insulina. El desarrollo de osteoporosis puede prevenirse con la suplementación de vitamina D y calcio, y a menudo se trata con bifosfonatos. En cuanto al retraso en el crecimiento, se procura contrarrestarlo mediante la inserción de un tubo de alimentación (gastrostomía) para aumentar así la ingesta de calorías a partir de nutrición adicional; también se administran con este fin inyecciones de hormona de crecimiento.

Las infecciones de los senos paranasales suelen tratarse con un prolongado régimen de antibióticos. El desarrollo de pólipos, así como otros cambios estructurales de tipo patológico en el interior de los conductos nasales, pueden restringir el flujo aéreo y complicar el cuadro. Por este motivo, es frecuente la práctica quirúrgica en procura de aliviar la obstrucción y limitar el desarrollo de nuevas infecciones. También se administran corticosteroides intranasales, como la fluticasona, para reducir la inflamación. Por otro lado, la infertilidad femenina puede combatirse recurriendo a técnicas de reproducción asistida. Aquella que afecta al hombre también tiene tratamiento: por ejemplo, mediante la inyección intracitoplasmática de esperma.

Por lo general, se considera procedente el trasplante de pulmón en personas con deterioro progresivo de la función pulmonar y creciente intolerancia al ejercicio (fatiga o agotamiento muscular desproporcionados para el ejercicio realizado). Aunque el trasplante de un único pulmón es viable en otras enfermedades, en los pacientes con FQ ambos deben ser reemplazados ya que, de otro modo, las bacterias alojadas en el órgano remanente podrían infectar a aquél que ha sido trasplantado. Asimismo, puede practicarse simultáneamente un trasplante de páncreas o de hígado con el propósito de aliviar la enfermedad hepática o la diabetes. La opción del trasplante de pulmón se evalúa cuando la función pulmonar se ve afectada en grado tal que se vea amenazada la supervivencia o se requiera la asistencia con dispositivos mecánicos.
La terapia génica representa una vía promisoria en la lucha contra la enfermedad. Mediante esta técnica, se procura insertar una copia normal del gen CFTR en las células afectadas. Debido a la incapacidad de los retrovirus para alcanzar células que no se dividen, se han realizado análisis clínicos para insertar genes en adenovirus. En la actualidad, estos virus se están utilizando en ensayos en los que el gen CFTR normal se administra, por un método en aerosol, a las células epiteliales que revisten los pulmones (terapia génica "in vivo"). Se espera que los adenovirus inserten el gen normal, induciendo una función pertinente de los canales de cloro en estas células.

Algunos estudios han señalado que para prevenir las manifestaciones pulmonares de la fibrosis quística, solo se requiere la expresión génica de entre un 5 y un 10 % de los valores normales de proteína CFTR. Un inconveniente de los adenovirus es que no se integran en el ADN de la célula huésped. Por lo tanto, finalmente se pierden, originando una expresión del gen transitoria y la necesidad de reintroducción del vector. Se han propuesto diversos abordajes y se han iniciado numerosos estudios clínicos pero, al año 2006, persisten múltiples obstáculos, que será preciso superar para que la terapia génica resulte exitosa.

La otra aproximación para el tratamiento de la fibrosis quística viene dada por el uso de potenciadores o moduladores de la proteína CFTR reparando el defecto subyacente en la creación de dicha proteína. Existen varios potenciadores o correctores en diferentes fases de investigación entre los que destacan: Ivacaftor, Lamacaftor, Tezacaftor. El potenciador Ivacaftor, antes llamado VX770 y bajo el nombre comercial de Kalydeco, se destinó exclusivamente a pacientes mayores de 6 años inicialmente, y a partir de 2015 fue habilitado por la FDA para pacientes de 2 a 5 años. Está autorizado para el tratamiento de la mutación G551D, teniendo esta menos del 3 % del total de pacientes con fibrosis quística. En 2014 la FDA autorizó este medicamento para el uso en las mutaciones G178R, S549N, S549R, G551S, G1244E, S1251N, S1255P y G1349D.Este modulador puede mejorar la calidad de vida de los pacientes, con incremento de peso y la salud pulmonar, reduciendo la probabilidad de infección. La mejora de FEV1 es promedialmente del 10,4%. El costo de esta terapia supera los 300 000 dólares estadounidenses por año de tratamiento crónico (2014).
La asociación de Ivacaftor con Lumacaftor (antes llamado VX-809) del mismo laboratorio Vertex Pharmaceuticals ha sido aprobado por la FDA y por la Unión Europea, para los que poseen la mutación ΔF508 (la forma más común) en su forma homocigota (dos copias iguales).Actualmente se mantienen conversaciones con los distintos países de la Unión Europea para negociar el reembolso del medicamento.
Se encuentra en fase de investigación la asociación de Ivacaftor con el corrector Tezacaftor(antes llamado VX-661) para los que poseen la mutación ΔF508 en su forma heterocigota (una copia diferente en cada alelo).

Para las mutaciones del tipo I, generalmente llamadas mutación sin sentido o "non-sense", donde un codón de terminación hace que la creación de la proteína quede trunca en una etapa prematura y por lo tanto no funcional, se están probando medicamentos que han mostrado cierta eficacia logrando que los ribosomas ignoren los codones de terminación prematuros y terminen de generar la proteína CFTR, esta capacidad de los fármacos es llamado "read-through" en la literatura. El medicamento Ataluren (antes llamado PTC124) del laboratorio PTC Therapeutics se encuentra en Etapa III de investigación en la FDA con resultados ambiguos para la generalidad de los pacientes con estas mutaciones, aunque prometedores en los pacientes que no tienen tratamiento crónico con antibioticos.

Otros tipos de medicamentos buscan abrir canales alternos para el cloro en la célula aunque se encuentran en etapas más prematuras de investigación.

Entre las personas de ascendencia europea, la fibrosis quística es la más frecuente de las enfermedades autosómicas recesivas potencialmente fatales. En los Estados Unidos, aproximadamente 30 000 individuos padecen FQ; en su mayoría, son diagnosticados a los seis meses de edad. Canadá tiene cerca de 3000 habitantes con esta condición. Se estima que una de cada 25 personas de ascendencia europea y una de cada 29 personas de ascendencia askenazí son portadores de una mutación de fibrosis quística. Aunque es menos común en estos grupos, aproximadamente uno de cada 46 hispanoamericanos, uno de cada 65 africanos y uno de cada 90 asiáticos son portadores de al menos un gen CFTR anormal. Argentina y Uruguay representan una excepción en el contexto de América Latina, con una incidencia de casos mucho mayor a la media de la región y muy próxima a la registrada en Estados Unidos o Canadá, y una prevalencia de portadores sanos en la población general de entre 1 en 30 y 1 en 25.

La fibrosis quística se diagnostica tanto en hombres como en mujeres. Por razones que solo en parte se conocen, la esperanza de vida al nacer resulta ser mayor entre los varones afectados que entre las mujeres. Aquel indicador tiende a variar principalmente en función del alcance y la calidad de la atención suministrada por los sistemas de salud pública. En 1959, la supervivencia media en niños con FQ era de 6 meses. Para los nacidos en 2006 en los Estados Unidos, este valor treparía a los 36,8 años, de acuerdo a los datos compilados por la Fundación de la Fibrosis Quística. La tasa de esperanza de vida ha evolucionado en forma análoga para buena parte de Occidente, exceptuando los países menos desarrollados, donde se reportan cifras sensiblemente menores, y en los cuales la mayoría de la población afectada no sobrevive más allá de los diez años de edad.

La Fundación de la Fibrosis Quística compila, además, información sobre el estilo de vida de los adultos estadounidenses con FQ. En 2004, la fundación reportó que el 91 % de esta población había completado la enseñanza media, y el 54 % había accedido a alguna forma de educación universitaria. Los datos en materia de empleo revelaron que el 12,6 % de estos adultos estaba imposibilitado para trabajar (quedando fuera de la población económicamente activa), y el 9,9 % estaba desocupado. Por otro lado, la información marital señaló que un 59 % era soltero y un 36 % estaba casado o viviendo en pareja. En 2004, 191 mujeres con FQ se encontraban embarazadas en los Estados Unidos.

Se estima que la mutación ΔF508 puede tener hasta unos 52 000 años de antigüedad. Se han formulado numerosas hipótesis intentando explicar por qué una mutación letal como ésta ha persistido y se ha extendido entre la población humana. Algunas enfermedades autosómicas recesivas comunes como la anemia de células falciformes han revelado la propiedad de proteger a sus portadores de otras afecciones, concepto conocido como "ventaja heterocigota". Con el descubrimiento de que la toxina del cólera requiere que sus huéspedes sean proteínas CFTR normales para poder funcionar apropiadamente, se ha postulado que los portadores de genes CFTR mutantes obtuvieron el beneficio de la resistencia al cólera y a otras causas de diarrea. Sin embargo, estudios posteriores no han confirmado esta hipótesis.

La presencia de proteínas CFTR normales es condición necesaria para el ingreso de "Salmonella typhi" (serotipo de "Salmonella enterica", proteobacteria gram negativa del género "Salmonella") en las células, lo que sugiere que los portadores de genes CFTR mutantes podrían ser resistentes a la fiebre tifoidea. Sin embargo, ningún estudio "in vivo" ha confirmado esta hipótesis. En cualquiera de los casos, la baja incidencia de fibrosis quística fuera de Europa, en sitios donde tanto el cólera como la fiebre tifoidea son endémicos, carece de explicación inmediata.

Aunque el espectro clínico completo de la FQ no fue reconocido hasta los años 1930, ciertos aspectos fueron identificados mucho antes. Carl von Rokitansky describió un caso de muerte fetal con peritonitis meconial, una complicación del íleo meconial asociado con la fibrosis quística. El íleo meconial fue descrito por primera vez en 1905 por Karl Landsteiner.

En 1938, Dorothy Andersen publicó un artículo intitulado «Cystic fibrosis of the páncreas and its relation to celiac disease: a clinical and pathological study» («La fibrosis quística del páncreas y su relación con la enfermedad celíaca: un estudio clínico y patológico») en la revista "American Journal of Diseases of Children". De esta manera, era la primera investigadora en definir esta entidad nosológica (denominada, por aquel entonces, "fibrosis quística del páncreas"), y en correlacionarla con los trastornos pulmonares e intestinales prominentes. También postuló que era una enfermedad recesiva y utilizó el reemplazo de enzimas pancreáticas como tratamiento para los niños afectados. En 1952, Paul di Sant' Agnese descubrió anomalías en los electrolitos del sudor. Sobre la base de esa evidencia, se desarrolló y perfeccionó el examen del sudor durante el curso de la siguiente década.

En 1985, investigadores de Londres, Toronto y Salt Lake City trazaron el mapa del gen CFTR en el cromosoma 7q. Cuatro años más tarde, en 1989, Francis Collins, Lap-Chee Tsui y John R. Riordan descubrieron la primera mutación para la FQ, ΔF508, en ese cromosoma. Investigaciones posteriores a aquel hallazgo, identificaron más de mil mutaciones diferentes que dan origen a la enfermedad. Lap-Chee Tsui lideró el equipo de científicos del "Hospital for Sick Children" (un hospital escuela en convenio con la Universidad de Toronto) que descubrió el gen responsable de la FQ. Se trata del primer trastorno genético dilucidado estrictamente mediante el proceso de genética inversa. Debido a que las mutaciones del gen CFTR son generalmente pequeñas, las técnicas de la genética clásica o formal no fueron capaces de determinar con precisión el gen mutante. Utilizando marcadores proteicos, los estudios de ligamiento genético lograron trazar un mapa de la mutación del cromosoma 7. Las técnicas de paseo y salto cromosómicos sirvieron entonces para identificar y secuenciar el gen. Este gen fue uno de los primeros genes en ser localizado y secuenciado por mapeo genético, y algunos de los participantes en este proyecto, como Francis Collins estuvieron implicados más tarde en el Proyecto Genoma Humano

La identificación de la mutación específica responsable de la FQ en un paciente puede ser útil para predecir la evolución de la enfermedad. Por ejemplo, los pacientes homocigotos para la mutación ΔF508 presentan, en casi todos los casos, insuficiencia pancreática y tienen, por lo general, un grado relativamente severo de afectación respiratoria. Sin embargo, existen excepciones que indican la posibilidad de que factores adicionales (quizás, genes en otros "loci") intervengan en la expresión de la enfermedad. Por otro lado, la clonación del gen de la FQ ha abierto la posibilidad de la terapia génica, tal y como se ha descrito en la sección pertinente.





</doc>
<doc id="9749" url="https://es.wikipedia.org/wiki?curid=9749" title="Enfermedad de Niemann-Pick">
Enfermedad de Niemann-Pick

La enfermedad de Niemann-Pick es una enfermedad de almacenamiento lisosómico hereditaria autosómica recesiva, causada por mutaciones genéticas específicas, concretamente se trata de un déficit de la enzima esfingomielinasa, de la ruta de degradación de los esfingolípidos. Se incluye dentro del grupo de las lipidosis que son enfermedades por almacenamiento de lípidos.

La enfermedad de Niemann Pick se describió por primera vez en 1914 por el pediatra alemán Albert Niemann (nacido el 23 de febrero de 1880 en Berlín, lugar donde murió el 22 de marzo de 1921) en unos niños de origen judío (Askenazes), grupo étnico de centro y este de Europa; y en 1927 Ludwig Pick (nacido el 31 de agosto de 1868 y fallecido el 3 de febrero de 1944) ya como una entidad propia diferenciada de otras enfermedades, en un estudio tisular diferenciándola de la enfermedad de Gaucher.
A Crocker le debemos la distinción, en 1961, de los cuatro tipos que hoy estudiamos (A, B, C y D).
Finalmente citaremos que fue Brady quien aisló en 1966 la enzima lisosomal esfingomielinasa, cuyo déficit produce los tipos A y B. Los tipos C y D tienen como causa un defecto en el transporte intracelular del colesterol que se acumula en su forma libre sin esterificar.

Las cuatro formas de la enfermedad de Niemann-Pick se caracterizan por una acumulación de esfingomielina y colesterol en los lisosomas de las células, particularmente en las células de órganos importantes como el hígado y el bazo. Las tres formas más conocidas de la enfermedad son los tipos A, B y C.





No existe en el tipo C una relación clara con el déficit de esfingomielinasa, hay un origen genético causado por la anomalía en lo que se denominan, en términos de vanguardia, como genes reguladores. Se relaciona también con una proteína muy específica relacionada con la homeostasis del colesterol intracelular. Este depósito va a ir originando una alteración en las células deteriorándolas, deformándolas (células esponjosas) y terminando en muerte de estas. Los sustratos orgánicos donde van a repercutir esta lisis celular, primordialmente son por orden de importancia: cerebro, hígado y bazo. Al ser estos los órganos afectados, de ahí procederán las expresiones clínicas de esta enfermedad.

Esta enfermedad causa un progresivo deterioro en las funciones vitales, tales como:

Los niños que la padecen mueren precozmente en los tres primeros años de vida. Las características de la enfermedad son infantilismo y trastornos del desarrollo. En la corteza cerebral aparecerán células hinchadas con la técnica de tinción de Baker, llamadas células xantomatosas. Otros signos son hepatoesplenomegalia, anemia, trastornos de la médula ósea, así como trastornos en los cartílagos de crecimiento de los huesos largos.

El diagnóstico de la enfermedad Niemann Pick se confirma con los estudios enzimáticos y con una biopsia en la piel del paciente, al mismo tiempo hay estudios moleculares que determina el tipo genético de la enfermedad.

Si un niño nace con algún tipo de Niemann-Pick (A, B, C) se debe a que los dos padres portan el gen anormal que lo produce aunque no tengan en ellos mismo síntomas de la enfermedad, esto es porque todos los tipos de la enfermedad son autosómicos recesivos. 

Cuando los padres son portadores es más probable (50%) que el niño nazca portador de la enfermedad en vez de enfermo (25%), por esto es que se dan pocos casos de esta enfermedad rara.

Los exámenes médicos para detectar el portador en las familias todavía no es fiable. Pero para las mutaciones de los tipos A y B hay disponibles pruebas de ADN ya que se han estudiado ampliamente, sobre todo en la población judía asquenazí.
En los pacientes de tipo C se han podido encontrar mutaciones del ADN y con esto, es posible encontrar al portador. En pocos centros médicos ya se dispone de un diagnóstico en el feto para el Niemann-Pick.

Muchos de los síntomas producidos por la enfermedad son comunes a otras enfermedades y esto dificulta el diagnóstico de certeza. Los médicos aconsejan que si se sospecha de la enfermedad en los tipos A y B, se realice una “medición de la actividad de la esfingomielinasa ácida en los glóbulos blancos”. Esto se consigue haciendo un análisis de sangre o una biopsia de células espumosas en la médula ósea. Esta prueba es ineficaz para detectar los portadores. Cuando la enfermedad es de tipo C, se diagnostica realizando una biopsia de piel, “cultivo celular en laboratorio y estudio posterior de la capacidad de las células aisladas para transportar y almacenar colesterol. Otras pruebas diagnósticas adicionales son la realización de un examen ocular con una lámpara de hendidura, la aspiración de la médula ósea y la biopsia del hígado.

Los científicos no pueden explicar por qué se producen tantas diferencias en la evolución de la enfermedad. Datos estadísticos puramente revelan que un niño con síntomas del tipo C antes de tener un año de edad no llega a alcanzar la edad escolar, pero si los presenta después de la escolarización, llegan a vivir hasta la mitad o el final de su adolescencia, muy pocos llegan hasta los 20 años. Aunque los tipos A y B se causan por lo mismo, una persona con el tipo A llega a la muerte sobre los 2 o 3 años, mientras que los de tipo B pueden llegar a la vida adulta.

En toda España hay alrededor de 20 familias afectadas por el NP y realizan reuniones y conferencias sobre la enfermedad para discutir y analizar detenidamente los tratamientos preventivos posibles de la enfermedad que al catalogarse como “rara” y tener una prevalencia de uno entre un millón causa un interés nulo por parte de los laboratorios científicos para investigarla. De esta manera los familiares se ven obligados a sufragar los gastos con la ayuda de administraciones locales para la investigación.

Los más propensos a padecer el NP son los adolescentes y los niños aunque se sabe que puede atacar a cualquier edad, y la esperanza de vida es inferior a los 10 años en los niños y 30 en los jóvenes.

El desarrollo de esta enfermedad se caracteriza por el paulatino deterioro de las funciones vitales del organismo. La enfermedad descubierta por los médicos Niemann y Pick es degenerativa y, por lo tanto, mortal en el 100% de los casos.

Es recomendable para los pacientes tener una dieta baja en colesterol, pero ni esto ni los “medicamentos hipolipemiantes” detienen el progreso de la enfermedad. Pero muchos síntomas producidos por el tipo C (cataplexia y convulsiones) si se han podido frenar con medicamentos.

Aún no se ha encontrado un tratamiento eficaz para curar completamente esta enfermedad. Sin embargo, en la actualidad se están tratando de paliar las complicaciones que aparecen en el transcurso de la enfermedad mediante:






</doc>
<doc id="9752" url="https://es.wikipedia.org/wiki?curid=9752" title="MEDLINE">
MEDLINE

MEDLINE o Medline es posiblemente la base de datos de bibliografía médica más amplia que existe. Producida por la Biblioteca Nacional de Medicina de los Estados Unidos. En realidad es una versión automatizada de tres índices impresos: Index Medicus, Index to Dental Literature e International Nursing Index, recoge referencias bibliográficas de los artículos publicados en unas 5.500 revistas médicas desde 1966. Actualmente reúne más de 24.000.000 citas y está en marcha un proceso para la carga paulatina de citas anteriores a 1966. 
Cada registro de MEDLINE es la referencia bibliográfica de un artículo científico publicado en una revista médica, con los datos bibliográficos básicos de un artículo ("Título, autores, nombre de la revista, año de publicación") que permiten la recuperación de estas referencias posteriormente en una biblioteca o a través de software específico de recuperación.
La base de datos contiene alrededor de 15 millones de artículos de aproximadamente 5.000 publicaciones seleccionadas (NLM Systems, feb 2007) cubriendo las áreas de biomedicina y salud desde 1950.

Además de contar con gran intercambio con otros hospitales para poder adquirir más conocimiento de muchas enfermedades o estadísticas de muchos países acerca de sus niveles de incidencia.




</doc>
<doc id="9753" url="https://es.wikipedia.org/wiki?curid=9753" title="Carrera de la milla">
Carrera de la milla

La Carrera de la Milla es una modalidad de carrera a pie proveniente de Inglaterra, cuya distancia a recorrer concuerda con esta unidad de medida itineraria, 1609,344 metros. Fue muy popular durante las décadas de 1950 y 1960, pero 1976 la IAAF decidió oficializar todas las carreras con el sistema métrico internacional y fue relevada por los 1500 metros. Quedando la milla como una prueba a realizar ocasionalmente debido a su gran peso histórico en el medio fondo. Se disputa principalmente sobre dos superficies en pista y asfalto. La segunda tiene una gran popularidad por su vistosidad pues es una carrera de media distancia que se suele disputar en un circuito urbano dando varias vueltas y permite ver evolucionar a los atletas durante la disputa de casi toda la prueba.



</doc>
<doc id="9754" url="https://es.wikipedia.org/wiki?curid=9754" title="Talasemia">
Talasemia

La talasemia es un tipo de anemia del grupo de anemias hereditarias. Esta condición genética confiere resistencia a la malaria, pero causa una disminución de la síntesis de una o más de las cadenas polipeptídicas de la hemoglobina. Hay varios tipos genéticos, con cuadros clínicos que van desde anomalías hematológicas difícilmente detectables hasta anemia grave y cuadros de enfermedad terminal.

Proviene del griego θάλασσα: ‘mar’, y αἷμα: ‘sangre’. Literalmente, sería ‘sangre marina’, pero en realidad el término hace referencia al Mar Mediterráneo, ya que en esta zona es más frecuente la enfermedad. Por ello, a veces se denomina también anemia mediterránea.

Se estima que un 5% de la población mundial es portadora de un gen mutado para la hemoglobina (siendo más frecuente el ser portador de una talasemia que cualquier otra hemoglobinopatía). Unos 300.000 niños nacen cada año con síndromes talasémicos en todo el mundo.
La talasemia es muy común en las zonas mediterráneas como el norte de África, el sur de España y de Italia (regiones de Apulia, Calabria, Sicilia y Cerdeña). En estas dos últimas regiones los portadores son más de 700.000 en una población total de poco menos de 7 millones.

La talasemia consiste en un grupo de enfermedades de amplio espectro. Estas van desde simples anormalidades asintomáticas en el hemograma hasta una grave y fatal anemia. La hemoglobina del adulto, denominada Hemoglobina A está compuesta por la unión de cuatro cadenas de polipéptidos: dos cadenas alfa (α) y dos cadenas beta (β). Hay dos copias del gen que produce la hemoglobina α (HBA1 y HBA2), y cada uno codifica una α-cadena, y ambos genes están localizados en el cromosoma 16. El gen que codifica las cadenas β (HBB) está localizado en el cromosoma 11.

Es una forma hereditaria de anemia en la que se reduce la síntesis de una o más de las cuatro cadenas de la globina, por lo general las dos α y las dos β, que forman parte de la hemoglobina en los glóbulos rojos de la sangre. La función de la hemoglobina es transportar el oxígeno desde los pulmones hacia los tejidos corporales. En la anemia esta función es insuficiente para satisfacer las necesidades de los tejidos (por ejemplo, los músculos y el cerebro). La palabra talasemia procede del griego y significa mar. Este trastorno se denominó así porque es más frecuente en las personas de origen mediterráneo. Sin embargo, su distribución es mundial. Hay diferentes tipos: las formas principales son las del adulto que se denominan talasemias α o β según estén alterados los genes de la cadena α o β. Su gravedad varía según la configuración genética. Se trata de la enfermedad hereditaria de la sangre más frecuente y, a su vez, es la más frecuente causada por una anomalía en un único gen.
En la talasemia, la estructura de ambas cadenas de la hemoglobina permanecen intactas, pero está ausente la cadena α o β o existe en pequeñas cantidades, debido a anomalías en los genes que codifican estas proteínas. Esto origina un desequilibrio en la cantidad de globina en las cadenas con predominio de la α o β. Las cadenas precipitan en ausencia de otras cadenas suficientes con las que unirse y esta precipitación interfiere con la formación de los glóbulos rojos. Se producen menos glóbulos rojos de lo normal y los que son capaces de desarrollarse incluyen en su interior las cadenas de hemoglobina precipitadas, de tal modo que no pueden pasar a través de los capilares y son destruidos prematuramente. Esto produce una anemia grave y para compensarla, la médula ósea sufre hiperplasia al intentar producir suficientes glóbulos rojos, y el bazo también aumenta de tamaño. Son posibles también las deformidades graves en el cráneo y en los huesos largos.

En la α-talasemia gen HBA1 () y HBA2 (), hay una deficiencia de síntesis de cadenas α. El resultado es un exceso de cadenas β que trasportan deficientemente el oxígeno, lo que conduce a bajas concentraciones de O (hipoxemia). Paralelamente, en la β-talasemia () hay una falta de cadenas β, y el consiguiente exceso de cadenas alfa puede formar agregados insolubles que se adhieren a la membrana de los eritrocitos, pudiendo causar la muerte de éstos y sus precursores, originando anemia de tipo hemolítico.

Esta enfermedad está provocada por deleciones en uno o varios genes de los que componen los grupos de la α-globina y la β-globina. Según estas deleciones involucren más o menos genes el tipo de talasemia será más o menos grave. 

Estas deleciones provocan la disminución en la producción de cadenas α o β, según el lugar de la deleción; la escasez de cadenas α se intenta compensar con un aumento de la producción de cadenas β, y viceversa, lo que da lugar a la formación de hemoglobinas inestables que provocan la destrucción de los glóbulos rojos y por lo tanto anemia. 

A su vez las deleciones parecen ser el resultado de entrecruzamientos desequilibrados entre los segmentos duplicados presentes en la región de la agrupación.

En el caso de las β-talasemias además de la deleción del gen de la β-globina, también pueden darse por otras causas como:


El defecto o deleción de un gen en la talasemia β causa una anemia hemolítica que oscila entre leve y moderada sin síntoma alguno. La deleción de dos genes ocasionan anemia más grave y la presencia de síntomas: debilidad, fatiga, dificultad respiratoria. En las variantes más graves, como la talasemia beta mayor, pueden aparecer ictericia, úlceras cutáneas, cálculos biliares y agrandamiento del bazo (que en ocasiones llega a ser enorme). La actividad excesiva de la médula ósea puede causar el ensanchamiento y el agrandamiento de algunos huesos, especialmente los de la cabeza y del rostro. 

Los huesos largos tienden a debilitarse y fracturarse con gran facilidad. Los niños que padecen ciertas talasemias pueden crecer con más lentitud y llegar a la pubertad más tarde de lo normal. Como la absorción del hierro puede aumentar como respuesta a la anemia sumado al requerimiento de transfusiones de sangre frecuentes (las cuales suministran más hierro), es posible que se acumulen cantidades excesivas de hierro y se depositen en la musculatura del corazón, causando insuficiencia cardíaca.

Las talasemias son más difíciles de diagnosticar que otros trastornos de la hemoglobina. El análisis de una gota de sangre por electroforesis puede ser útil pero no concluyente, en especial en el caso de talasemia alfa. Por lo tanto, el diagnóstico se basa habitualmente en patrones hereditarios y en análisis especiales de hemoglobina. Por lo general, las personas que padecen talasemia no requieren tratamiento alguno, pero aquellas con variantes graves pueden requerir un trasplante de médula ósea. La terapia con genes se encuentra en fase de investigación.

Como ocurre en el más conocido caso de la anemia de células falciformes, también la α-talasemia protege a los individuos que la portan frente a la malaria. La malaria o paludismo está producida por un parásito protista del género "Plasmodium" y es transmitida por un mosquito del género "Anopheles". La protección frente a esta enfermedad por parte de los individuos que posee α-talasemia es debida a que "Plasmodium" sólo es capaz de parasitar a los eritrocitos sanos. Sin embargo, la sangre de alguien con este tipo de anemia presenta un número elevado de eritrocitos deformes por culpa de que la hemoglobina no está bien constituida y eso es esencial pues deja al parásito indefenso en la sangre permitiendo que nuestro sistema inmunitario acabe con él.

La ventaja del heterocigoto se produce cuando un alelo que es deletéreo en su forma homocigótica, resulta, en cambio ventajoso en su forma heterocigótica. Este fenómeno es lo que se llama polimorfismo equilibrado. Significa que la selección negativa del alelo en estado homocigótico se equilibra con la selección positiva a favor del alelo en el estado heterocigótico.

Debido a esto hay una alta frecuencia de talasemias, y en general de hemoglobinopatías en los países con malaria endémica, de modo que la distribución geográfica de la malaria se correlaciona con la de talasemias.





Las pruebas que se hacen para saber si un individuo padece cualquiera de los tipos de talasemia son análisis de sangre, que permiten ver la forma y la cantidad de glóbulos rojos en sangre. 
Otra forma de diagnosticar la enfermedad es por medio de estudios genéticos. Los cuales nos dan la información exacta del tipo de talasemia y su causa. 

Actualmente, el análisis prenatal que se realiza mediante el muestreo de villus coriónico y la amniocentesis permite determinar la presencia o ausencia de talasemia en el feto, de esta forma si se detecta y es una forma grave puede ser tratada precozmente y que el individuo sobreviva.









</doc>
<doc id="9755" url="https://es.wikipedia.org/wiki?curid=9755" title="Biblioteca Cochrane">
Biblioteca Cochrane

La Biblioteca Cochrane ("The Cochrane Library") es el principal producto de la Colaboración Cochrane. Es una publicación electrónica que se actualizada cada tres meses. Se distribuye mediante suscripción anual en CD o a través de Internet.

La Biblioteca Cochrane es una colección de bases de datos sobre ensayos clínicos controlados en medicina y otra áreas de la salud relacionadas con la información que alberga la Colaboración Cochrane.

La versión en español, Cochrane Library Plus, sólo puede consultarse en Internet, y es de acceso gratuito desde España.

En 1972 el epidemiólogo británico Archie Cochrane llamó la atención sobre la enorme dificultad que tienen todos los implicados en las tomas de decisiones sobre cuidados de salud para tener acceso a la investigación clínica que debe fundamentar esa toma de decisiones. En 1974 se comenzó un registro de los ensayos clínicos sobre la atención al embarazo y al parto. En 1985 el registro contenía en torno a 3.500 referencias de ensayos clínicos que habían permitido preparar 600 revisiones para proporcionar la mejor prueba disponible para la toma de decisiones en ese campo. A. Cochrane sugirió que otras especialidades deberían seguir el mismo ejemplo. La Colaboración Cochrane se considera como la respuesta a esa invitación.

Las principales bases de datos que incluye son:





</doc>
<doc id="9756" url="https://es.wikipedia.org/wiki?curid=9756" title="Capa de ozono">
Capa de ozono

Se denomina capa de ozono a la zona de la estratosfera terrestre que contiene una concentración relativamente alta de ozono. Esta capa, que se extiende aproximadamente de los 15km a los 50km de altitud, reúne el 90 % del ozono presente en la atmósfera y absorbe del 97 al 99 % de la radiación ultravioleta de alta frecuencia.

La capa de ozono fue descubierta en 1913 por los físicos franceses Charles Fabry y Henri Buisson. Sus propiedades fueron examinadas en detalle por el meteorólogo británico G.M.B. Dobson, quien desarrolló un sencillo espectrofotómetro que podía ser usado para medir el ozono estratosférico desde la superficie terrestre. Entre 1928 y 1958 Dobson estableció una red mundial de estaciones de monitoreo de ozono, las cuales continúan operando en la actualidad. La unidad Dobson, una unidad de medición de la cantidad de ozono, fue nombrada en su honor.

El ozono es la forma alotrópica del oxígeno, que sólo está estable en determinadas condiciones de presión y temperatura. Es un gas compuesto por tres átomos de oxígeno (formula_1).

Los mecanismos fotoquímicos que se producen en la capa de ozono fueron investigados por el físico británico Sydney Chapman en 1930. La formación del ozono de la estratosfera terrestre es catalizada por los fotones de luz ultravioleta que al interaccionar con las moléculas de oxígeno gaseoso, que están constituidas por dos átomos de oxígeno (formula_2), las separa en los átomos de oxígeno (oxígeno atómico) constituyente; el oxígeno atómico se combina con aquellas moléculas de formula_2 que aún permanecen sin disociar, formando, de esta manera, moléculas de ozono, formula_1.

La concentración de ozono es mayor entre los 15 y 40 km, con un valor de 2-8 partículas por millón, en la zona conocida como capa de ozono. Si todo ese ozono fuese comprimido a la presión del aire al nivel del mar, esta capa tendría solo 3 milímetros de espesor.

El ozono actúa como filtro, o escudo protector, de las radiaciones nocivas, y de alta energía, que llegan a la Tierra, permitiendo que pasen otras como la ultravioleta de onda larga, que de esta forma llega a la superficie. Esta radiación ultravioleta es la que permite la vida en el planeta, ya que es la que permite que se realice la fotosíntesis del reino vegetal, que se encuentra en la base de la pirámide trófica.

Al margen de la capa de ozono, el 10 % de ozono restante está contenido en la troposfera, y es peligroso para los seres vivos por su fuerte carácter oxidante. Elevadas concentraciones de este compuesto a nivel superficial forman el denominado esmog fotoquímico. El origen de este ozono se explica en un 10 % como procedente de ozono transportado desde la estratosfera y el resto es creado a partir de diversos mecanismos, como el producido por las tormentas eléctricas que ionizan el aire y lo hacen, muy brevemente, buen conductor de la electricidad. Pueden verse algunas veces dos relámpagos consecutivos que siguen aproximadamente la misma trayectoria.

El ozono se produce mediante la siguiente reacción:

Es decir, el oxígeno molecular que se encuentra en las capas altas de la atmósfera es bombardeado por la radiación solar. Del amplio espectro de radiación incidente una determinada fracción de fotones cumple los requisitos energéticos necesarios para catalizar la rotura del doble enlace de los átomos de oxígeno de la molécula de oxígeno molecular.

Posteriormente, el Sol convierte una molécula de ozono en una de oxígeno diatómico y un átomo de oxígeno sin enlazar:

Durante la fase oscura (la noche de una determinada región del planeta), el oxígeno monoatómico, que es altamente reactivo, se combina con el ozono de la ozonosfera para formar una molécula de oxígeno biatómico:

Para mantener constante la capa de ozono en la estratosfera esta reacción fotoquímica debe suceder en perfecto equilibrio, pero estas reacciones son fácilmente perturbables por moléculas, como los compuestos clorados (como los clorofluorocarbonos) y los compuestos bromurados.

El seguimiento observacional de la capa de ozono, llevado a cabo en los últimos años, ha llegado a la conclusión de que dicha capa puede considerarse seriamente amenazada. Este es el motivo principal por el que se reunió la Asamblea General de las Naciones Unidas el 16 de septiembre de 1987, firmando el Protocolo de Montreal. En 1994, la Asamblea General de las Naciones Unidas proclamó el día 16 de septiembre como el "Día Internacional para la Preservación de la Capa de Ozono".

El desgaste grave de la capa de ozono provocará el aumento de los casos de melanomas, cáncer de piel, cataratas oculares, supresión del sistema inmunitario en humanos y en otras especies. También afectará a los cultivos sensibles a la radiación ultravioleta.

Para preservar la capa de ozono hay que disminuir a cero el uso de compuestos químicos como los clorofluorocarbonos (refrigerantes industriales, propelentes), y fungicidas de suelo (como el bromuro de metilo) (Argentina, 900 toneladas/año) que destruyen la capa de ozono a un ritmo 50 veces superior a los CFC.

Las últimas mediciones realizadas con satélites indican que el agujero en la capa de ozono se está reduciendo, a la vez que los niveles de clorofluorocarbonos (CFC) han disminuido. Esos compuestos químicos dañan la capa de ozono de la atmósfera que protege nuestro planeta. Durante más de cincuenta años, el número de CFC presentes en la parte alta de la atmósfera ha aumentado a un ritmo constante hasta el año 2000. Desde entonces, la concentración de CFC se ha reducido a razón de casi un 1 % anual. El descenso permite esperar que el agujero de la capa de ozono pueda cerrarse a mediados de siglo. No obstante, estos productos todavía causan daño. A pesar del descenso, el agujero de la Antártida en el año 2005 había alcanzado una extensión de casi 29 000 000 de kilómetros cuadrados, más de tres veces el tamaño de Australia.

Según los últimos estudios que se están llevando a cabo por el instituto “GDH World Solutions”: “el desgaste de la capa de ozono podría ser debido a los cambios de ciclo del Sol y no por la accion de seres humanos”.




</doc>
<doc id="9757" url="https://es.wikipedia.org/wiki?curid=9757" title="Desierto de Atacama">
Desierto de Atacama

El desierto de Atacama, el desierto no-polar más árido de la Tierra, se extiende en el Norte Grande de Chile —abarcando las regiones de Arica y Parinacota, Tarapacá, Antofagasta, Atacama y el norte de la región de Coquimbo— y cubre una superficie de aproximadamente 105 000 km². Tiene una longitud de casi 1600 km y un ancho máximo de 180 km. Es de tipo costero frío y está delimitado por el océano Pacífico al oeste y por la cordillera de los Andes al este.

Según la WWF, la ecorregión del desierto de Atacama se extiende desde Arica (18°24'S) hasta cerca de La Serena (29°55'S). Por su parte, la National Geographic Society considera que la zona costera peruana forma parte del desierto de Atacama.

Otra parte integrante de este desierto corresponde a una ecorregión denominada puna de Atacama, ubicada sobre los 3500 msnm y que es compartida por la vertiente occidental de la cordillera de los Andes en el norte de Chile, y por la vertiente oriental de la misma en el noroeste de Argentina y suroeste de Bolivia.

Esta ecorregión chilena es rica en recursos minerales metálicos —como cobre (Chile es el mayor productor del mundo y cuenta con el 28 % de las reservas mundiales), hierro, oro y plata— y no metálicos —entre los que destacan importantes depósitos de boro, litio (Chile cuenta con el 39 % de las reservas sudamericanas), nitrato de sodio y sales de potasio—. También se destaca la bischofita, una sal de magnesio extraída del salar de Atacama, usada como agente apelmazante en la construcción de caminos. Estos recursos son explotados por varias empresas mineras, como Codelco, la mayor compañía cuprífera del planeta, Lomas Bayas, Mantos Blancos y Soquimich.

Su origen data de hace unos tres millones de años, siendo en su pasado un lecho marino. Su cambio se relaciona con la llamada corriente de Humboldt. La principal causa del origen del desierto de Atacama es un fenómeno climático conocido como efecto Föhn, el cual provoca que las nubes descarguen sus precipitaciones en una cara de la montaña en su ascenso vertical, por lo que, al sobrepasar la cordillera las nubes no poseen agua, generando así un desierto al bloquear por completo todas las precipitaciones posibles provenientes del Este, por lo que es la Cordillera de los Andes la que genera el desierto de Atacama al producirse en sus laderas el efecto Foehn. Por el Oeste, se bloquean también las precipitaciones sobre el desierto de Atacama mediante sistemas estables de alta presión, conocidos como «anticiclones del Pacífico», que se mantienen junto a la costa, creando vientos alisios hacia el este que desplazan las tormentas.

Por otra parte, la corriente de Humboldt transporta agua fría desde la Antártida hacia el norte a lo largo de las costas chilena y peruana, que enfría las brisas marinas del oeste, reduce la evaporación y crea una inversión térmica —aire frío inmovilizado debajo de una capa de aire tibio—, impidiendo la formación de grandes nubes productoras de lluvias. Toda la humedad creada progresivamente por estas brisas marinas, se condensa a lo largo de las escarpadas laderas de la cordillera de la Costa que dan hacia el Pacífico, creando ecosistemas costeros altamente endémicos compuestos por cactus, suculentas y otros ejemplares de flora xerófila.

El último factor que contribuye a la formación del desierto es la cordillera de los Andes, que en el norte forma una planicie volcánica elevada y ancha conocida como Altiplano. Así como en el sur la cordillera andina contribuye a capturar la humedad proveniente del Pacífico, en el norte el Altiplano impide el ingreso a Chile de las tormentas cargadas de humedad provenientes de la cuenca amazónica, que se encuentra al noreste.

En el desierto de Atacama, una lluvia posible de ser medida —es decir, de 1 mm o más— puede tener lugar una vez cada 15 o 40 años —se han registrado periodos de hasta 400 años sin lluvias en su sector central—. Sin embargo, la zona se ve afectada entre enero y febrero por el llamado «invierno altiplánico», que produce alguna que otra lluvia y abundantes tormentas eléctricas. Esta zona registra dos "récords" meteorológicos mundiales. El primero de ellos es que Arica anota el promedio anual de lluvias más bajo del mundo, alcanzando tan solo 0,5 mm —la mayor parte de las precipitaciones cae en forma de lloviznas débiles y aisladas—. El segundo es que Iquique registra la sequía más larga del mundo, con 16 años sin precipitaciones.

En las noches la temperatura fluctúa mucho, pues puede bajar hasta -25 °C en la zona de Ollagüe, mientras que en el día la temperatura se puede situar entre los 25 y los 50 °C a la sombra. No hay mucha diferencia entre el verano y el invierno, porque está situado al límite del trópico de Capricornio. En verano, la temperatura ambiente matinal es de 4 a 10 °C y la máxima puede alcanzar los 45 °C a plena irradiación solar. La radiación solar es muy alta en el espectro ultravioleta, por lo que es indispensable el uso de gafas y cremas con protección UV.

La humedad relativa del aire es de apenas un 18 % en el interior, pero muy alta en el litoral, llegando hasta un 98 % en los meses de invierno. La presión atmosférica es de 1017 milibares. Existen temporadas de vientos en tornado o ventiscas cuya velocidad puede alcanzar fácilmente los 100 km/h, generalmente registrados al mediodía. La topografía de la zona es de gradiente en descenso muy paulatino hacia el mar, pero su altura promedio relativa es de 400-1500 msnm.

El desierto de Atacama ha estado poblado desde los comienzos de la colonización americana. Un hito de los primeros habitantes de esta zona fue la faena minera, que tuvo sus inicios entre 12 000 y 10 000 años atrás en una mina de óxido de hierro en Taltal, Región de Antofagasta, la más antigua del continente. Durante el periodo prehispánico, descolló la cultura Chinchorro, desarrollada entre 5000 y 1700 a. C., la primera del mundo en momificar artificialmente a sus muertos. Además, este territorio fue habitado por etnias como los atacameños, mientras que en su litoral vivían los changos, los coles, los lupacas y los uros. Fue dominado por el señorío de Chucuito bajo el nombre de Colesuyo y luego por el Imperio inca como Collasuyo.

La región más árida fue denominada «despoblado de Atacama» durante la Colonia. Después de las Guerras de independencia hispanoamericanas, y debido a la inexactitud de los documentos reales, la zona estuvo en disputa hasta que, a través de los tratados de límites de y , la región pasó a ser oficialmente territorio boliviano.

Pese a los tratados suscritos, las disputas no lograron resolverse. El 14 de febrero de 1879, se efectuó el desembarco chileno en Antofagasta, iniciando las acciones militares contra Bolivia. En 1873, se había suscrito el tratado de Alianza Defensiva Perú–Bolivia, por lo que Chile declaró la guerra a ambos el 5 de abril de 1879, iniciando formalmente la Guerra del Pacífico, que finalizó en 1884 con la victoria de Chile, el tratado de Ancón con Perú y el pacto de Tregua con Bolivia. Tras el conflicto, Chile obtuvo el dominio del, hasta entonces, departamento boliviano del Litoral, el departamento peruano de Tarapacá y la provincia peruana de Arica.

El desierto de Atacama es considerado el mejor sitio del planeta para observar el firmamento y desarrollar la astronomía: su altura respecto al nivel del mar, la escasa nubosidad, la casi inexistente humedad del aire y la lejana contaminación lumínica y radioeléctrica hacen que la visibilidad de su cielo nocturno sea muy nítida. Debido a esto, más de una docena de observatorios se ubica en este lugar —como Paranal (VLT), el complejo astronómico más avanzado y poderoso del planeta, ALMA, el mayor proyecto astronómico del mundo, y La Silla, entre otros—. Chile posee el 40 % de la observación astronómica del mundo; sin embargo, en las próximas décadas, el sector desarrollará otros proyectos —como el Giant Magellan Telescope, el Large Synoptic Survey Telescope (LSST), el E-ELT y la ampliación del Atacama Large Millimeter Array— que harán que el norte del país concentre cerca del 70 % del total mundial.

El desierto de Atacama acoge a los deportistas del todoterreno del mundo. En este desierto, se han realizado los diversos campeonatos del Rally Baja Atacama, Rally Baja Chile, Rally Patagonia Atacama, y recibió el Rally Dakar Series —en 2009, 2010, 2011, 2012, 2013, 2014 y 2015—, evento organizado por Amaury Sport Organisation (ASO). Las dunas de este desierto son ideales para este tipo de deporte, ubicadas en los aledaños de la ciudad de Copiapó, Región de Atacama.

Otro evento que se realiza en este desierto es la «Carrera Solar Atacama», que consiste en una carrera de vehículos solares que se hace por lugares del desierto como Toconao, Calama, Iquique y Antofagasta, entre otros.





</doc>
<doc id="9759" url="https://es.wikipedia.org/wiki?curid=9759" title="Hemoglobina A2">
Hemoglobina A2

La hemoglobina A2 es un tipo de hemoglobina, que representa en el adulto del 2,4% y en el feto menos del 0,5% de la hemoglobina total. Está formada por dos cadenas de globinas alfa y dos cadenas de globinas delta, que aumenta de forma importante en la beta-talasemia, al no poder sintetizar globinas beta.



</doc>
<doc id="9760" url="https://es.wikipedia.org/wiki?curid=9760" title="Estratosfera">
Estratosfera

La estratosfera o estratósfera es una de las capas de la atmósfera terrestre; está situada entre la troposfera y la mesosfera. La altura a la que comienza es variable: En las regiones polares a menor altura, entre 6 y 9 kilómetros o más; y en las regiones ecuatoriales entre 16 y 20 kilómetros. y se extiende hasta los 50km de altura aproximadamente. 

La temperatura aumenta progresivamente desde los −55 °C de la tropopausa hasta alcanzar los 0 °C de la estratopausa, aunque según algunos autores puede alcanzar incluso los 17 °C o más. Es decir, en esta capa la temperatura aumenta con la altitud, al contrario de lo que ocurre en las capas superior e inferior. Esto es debido principalmente a la absorción de las moléculas de ozono que absorben radiación electromagnética en la región del ultravioleta.

En la parte baja de la estratosfera la temperatura es relativamente estable, y en toda la capa hay muy poca humedad.

En la estratosfera la mezcla horizontal de los componentes gaseosos se produce mucho más rápidamente que la mezcla vertical.

A una altura aproximadamente de 2,5 veces la altura del Everest y unas 50 veces el Empire State de Nueva York solo algunos aviones como el Mig-31 ruso, el SR-71, el Concorde, el U-2 y el UAV RQ-4 Global Hawk pueden volar a este nivel. Cerca del final de la estratósfera se encuentra la capa de ozono que absorbe la mayoría de los rayos ultravioleta del Sol.

El 14 de octubre de 2012 el austríaco Felix Baumgartner se lanzó desde la estratosfera a una altura de 38969 metros. Rompió así el récord de salto en caída libre desde punto más alto y el de vuelo tripulado en globo con una distancia a la superficie terrestre de 39068m. El 24 de octubre de 2014 este récord fue superado por el vicepresidente de Google, Alan Eustace (57 años), quien se lanzó desde una altura de 41425 metros.



</doc>
<doc id="9761" url="https://es.wikipedia.org/wiki?curid=9761" title="Troposfera">
Troposfera

La troposfera o tropósfera es la capa de la atmósfera terrestre que está en contacto con la superficie de la Tierra.

Tiene alrededor de 17 km de espesor en el ecuador terrestre y solo 7 km en los polos, y en ella ocurren todos los fenómenos meteorológicos que influyen en los seres vivos, como los vientos, la lluvia y la nieve. Además, concentra la mayor parte del oxígeno y del vapor de agua. En particular esta capa actúa como un regulador térmico del planeta; sin ella, las diferencias térmicas entre el día y la noche serían tan grandes que no podríamos sobrevivir. Es de vital importancia para los seres vivos. La troposfera es la capa más delgada del conjunto de las capas de la atmósfera.

La temperatura en la troposfera desciende a razón de aproximadamente 6,5 °C por kilómetro de altura, por encima de los 2000 metros de altura.


</doc>
<doc id="9764" url="https://es.wikipedia.org/wiki?curid=9764" title="Deforestación">
Deforestación

La deforestación o tala de árboles es un proceso provocado generalmente por la acción humana, en el que se destruye la superficie forestal. Está directamente causada por la acción de las personas sobre la naturaleza, principalmente debido a las talas o quemas realizadas por la industria maderera, así como por la obtención de suelo para la agricultura, minería y ganadería.

La deforestación arrasa los bosques y las selvas de la Tierra de forma masiva causando un inmenso daño a la calidad de los suelos. Los bosques todavía cubren alrededor del 30% de las regiones del mundo.

Talar árboles sin una eficiente reforestación resulta en un serio daño al hábitat, en pérdida de biodiversidad y en aridez. Tiene un impacto adverso en la fijación de dióxido de carbono (CO). Las regiones deforestadas tienden a una erosión del suelo y frecuentemente se degradan a tierras no productivas.

Entre los factores que llevan a la deforestación en gran escala se cuentan: el descuido e ignorancia medieval del valor intrínseco, la falta de valor atribuido, el manejo poco responsable de la forestación y leyes medioambientales deficientes.

Los motivos de la tala indiscriminada son muchos, pero la mayoría están relacionados con el dinero o la necesidad de los granjeros de mantener a sus familias. El inductor subyacente de la deforestación es la agricultura. Los agricultores talan los bosques con el fin de obtener más espacio para sus cultivos o para el pastoreo de ganado. A menudo, ingentes cantidades de pequeños agricultores despejan hectáreas de terreno arbolado, para alimentar a sus familias, mediante tala y fuego en un proceso denominado «agricultura de roza y quema».

Las operaciones madereras comerciales, que proporcionan productos de pulpa de papel y madera al mercado mundial, también participan en la tala de innumerables bosques cada año. Los leñadores, incluso de forma furtiva, también construyen carreteras para acceder a bosques cada vez más remotos, lo que conlleva un incremento de la deforestación. Los bosques y selvas también caen víctimas del crecimiento urbano constante.

No toda la deforestación es consecuencia de la intencionalidad. Alguna es causa de factores humanos y naturales como los incendios forestales y el pastoreo intensivo, que puede inhibir el crecimiento de nuevos brotes de árboles.

La deforestación tiene muchos efectos negativos para el medio ambiente. El impacto más dramático es la pérdida del hábitat de millones de especies. Setenta por ciento de los animales y plantas habitan los bosques de la Tierra y muchos no pueden sobrevivir la deforestación que destruye su medio.

La deforestación es también un factor coadyuvante del cambio climático. Los suelos de los bosques son húmedos, pero sin la protección de la cubierta arbórea, se secan rápidamente. Los árboles también ayudan a perpetuar el ciclo hidrológico devolviendo el vapor de agua a la atmósfera. Sin árboles que desempeñen ese papel, muchas selvas y bosques pueden convertirse rápidamente en áridos desiertos de tierra yerma.

La eliminación de la capa vegetal arrebata a los bosques y selvas sus palios naturales, que bloquean los rayos solares durante el día y mantienen el calor durante la noche. Este trastorno contribuye a la aparición de cambios de temperatura más extremos, que pueden ser nocivos para las plantas y animales.

Los árboles desempeñan un papel crucial en la absorción de gases de efecto invernadero, responsables del calentamiento global. Tener menos bosques significa emitir más cantidad de gases de efecto invernadero a la atmósfera y una mayor velocidad y gravedad del cambio climático.

En muchos países la deforestación causa extinción de especies, cambios en las condiciones climáticas, desertificación y desplazamiento de poblaciones indígenas.

Hace unos ocho mil años, los seres humanos empezaron a talar bosques en cantidades pequeñas pero significativas, aunque para ello sólo dispusieron de hachas de sílex.

A medida que la agricultura se iba extendiendo, el ser humano limpiaba el terreno de árboles y arbustos para permitir que la luz del sol llegara hasta el suelo. El desbroce se hacía por el método de cortar y quemar. Al cabo de un año o dos, durante la estación seca se quemaban los residuos caídos y los árboles muertos y se sembraba en el suelo enriquecido con las cenizas.

En los seis mil años que van desde la prehistoria hasta el comienzo de la era histórica, hace dos mil años, el hombre fue mejorando sus herramientas para trabajar la tierra disponiendo de hachas y arados en la Edad del Bronce y luego en la Edad del Hierro, así como de bueyes y caballos domesticados que tiraban de los arados. Estos avances hicieron que la agricultura fuera ganando tierras al bosque, que fue talado allí donde ésta se desarrolló.

Hace dos mil años, en China, India, el sur y el oeste de Europa y el Magreb mediterráneo, así como en las tierras bajas de Centroamérica y las tierras altas de Perú se empleaban prácticas agrícolas sofisticadas (cultivos diversificados, plantaciones múltiples y cría de ganado). Todas esas regiones son naturalmente boscosas, y la agricultura a gran escala exigió talar esos árboles.

En el año 1089, Guillermo el Conquistador ordenó realizar el estudio Domesday, un estudio de sus nuevos dominios (Inglaterra). Este estudio demostró que se había deforestado el 85% de los campos, así como el 90% de la tierra cultivable (de altitud inferior a los mil metros). Siete siglos antes de la era industrial, Gran Bretaña estaba totalmente deforestada y muchos de los bosques que quedaban estaban protegidos en calidad de reservas de caza para la realeza y la nobleza.

El primer censo fiable de China data de la dinastía Han, hace cerca de dos mil años y por entonces el país tenía 57 millones de habitantes, con una densidad que triplicaba la de Inglaterra en el momento del estudio Domesday lo que implicaba que tanto China como India e Indonesia, zonas densamente pobladas estaban deforestadas ya hace dos mil años

Las islas del Caribe, como también partes de México y Centroamérica, contaban con una gran riqueza forestal, la cual estaba compuesta de maderas como caoba y palo maría, entre otras. Con la llegada de los españoles a América comenzó la explotación de estos bosques, para la construcción y la extracción de productos químicos tintóreos, como también su utilización como combustibles. Ante un peligroso incremento del consumo, la Monarquía Española promulgó leyes para regular el aprovechamiento de los bosques y no comprometer al ambiente.

Ante el poderío británico en los mares, los reyes FelipeV, FernandoVI y CarlosIII incentivaron la creación de astilleros en algunas ciudades americanas, como La Habana, Campeche, Guayaquil, El Realejo, Nicoya, Panamá, El Callao y Coatzacoalcos, con el objetivo de recuperar el poderío naval que se había perdido. Ante esta situación, se produjo una gran demanda de madera para la construcción de estos barcos.

En el presente, la deforestación ocurre principalmente, en América Latina, África Occidental y algunas regiones de Asia. En Brasil la deforestación en 2017 aumentó en un 28%, con más de 5 mil kilómetros cuadrados de árboles talados, en gran medida, por la reforma del Código Forestal durante el gobierno de Michael Temer que achicó las áreas verdes protegidas dando cabida a megaproyectos que destruyen la vegetación carioca. Los estados de Mato Grosso, Roraima y Pará, registraron los mayores índices de deforestación. En Paraguay, se incrementó un 34% la deforestación a comparación del 2012, con más de 160 mil hectáreas de boques taladas, afectando gravemente la Reserva Natural Cabrera Timane y el Parque Nacional Médanos del Chaco. En Perú, se deforestan alrededor de 150.000 hectáreas al año, por la práctica de la minería ilegal, el país ha perdido más del 50% de la cubierta vegetal de la costa.

Una tercera parte del total de la tierra está cubierta por bosques, lo que representa cerca de (cuatro mil millones) de hectáreas. Hay 10 países que concentran dos tercios de este patrimonio forestal: Australia, Brasil, Canadá, China, la República Democrática del Congo, India, Indonesia, Perú, la Federación Rusa y los EE. UU. Estos han sido explotados desde hace años para la obtención de madera, frutos, sustancias producidas por diferentes especies o para asentamientos de población humana, ganadería y agricultura. Indonesia, Malasia, Paraguay, Bolivia, Zambia y Angola han sido los países que más superficie forestal han perdido

En los últimos 25 años la tasa de desaparición de los bosques se redujo a la mitad. Desde 1990 se han perdido 129 millones de hectáreas de bosque. La tasa anual de pérdida neta de bosques (que tiene en cuenta los nuevos bosques que se plantan) pasó de 0,18% en los años 1990 a 0,08% en los cinco últimos años. Más países están mejorando la gestión forestal y existe una superficie cada vez mayor de áreas protegidas.
Particularmente relevante es el caso de Europa cuya superficie boscosa aumentó considerablemente, teniendo en 2016 un tercio más de bosques que un siglo atrás. El mismo fenómeno se produce en Cuba con un aumento de la superficie boscosa del casi 30% en las últimas décadas, como resultado de un ambicioso programa de reforestación. Igual situación se da en Rusia, que posee el 20% de todos los bosques del planeta, cuyas áreas boscosas se están ampliando desde 1961.

En los países más desarrollados la cubierta forestal sufre otras agresiones, como la lluvia ácida, que comprometen la supervivencia de los bosques, situación que se pretende controlar mediante la exigencia de requisitos de calidad para los combustibles, como la limitación del contenido de azufre o la desulfuración de los humos de las centrales térmicas y refinerías.

En los países menos desarrollados las masas boscosas se reducen año tras año, mientras que en los países industrializados se están recuperando debido a las presiones sociales, reconvirtiéndose los bosques en atractivos turísticos y lugares de esparcimiento.

Mientras que la tala de árboles de la pluviselva tropical ha atraído más atención, los bosques secos tropicales se están perdiendo a un ritmo sustancialmente mayor, sobre todo como resultado de las técnicas utilizadas de tala y quema para ser reemplazadas por cultivos. La pérdida de biodiversidad se correlaciona generalmente con la tala de árboles.

La deforestación es un proceso antiguo que se ha incrementado en los últimos tres siglos, con un promedio de seis millones de hectáreas anuales. Principalmente se produjo en el Hemisferio Norte, en los siglosXVIII y XIX, aunque en el sigloXX comenzó a realizarse en el Hemisferio Sur, especialmente en las selvas tropicales de la región amazónica.

En África, entre los años 2000 y 2005 se perdieron unos 4 millones de hectáreas de bosques al año, cerca de 1/3 del área deforestada en todo el mundo. La causa principal es la conversión a una agricultura permanente de las áreas deforestadas.
Como medidas contra la deforestación en África se está adoptando un sistema de certificación, dada la preocupación mundial por obtener madera a partir de bosques gestionados de manera sostenible, aunque la aplicación de esta certificación sigue siendo escasa todavía. De los 306 millones de hectáreas de bosques certificados del mundo (junio de 2007), unos 3 millones (solo el 1%) corresponde a África y la mayoría son bosques plantados. Con unos 15 millones de hectáreas de bosques plantados en todo el mundo (FAO, 2006), África solo representa el 5% del total.

También se han llevado a cabo otras medidas a nivel regional contra la deforestación y la desertificación como la Iniciativa de la Gran Muralla Verde del Sahara (UNU, 2007), con un enfoque integrado entre la agricultura, la ganadería y la actividad forestal.

Prácticamente todos los países de África han firmado la Convención de las Naciones Unidas para la Lucha contra la Desertificación y han elaborado planes nacionales, a menudo con apoyo externo.

Superficie forestal: extensión y variación

Esta región posee el 18,6% de la superficie forestal mundial, repartida en una gran variedad de ecosistemas, como bosques tropicales, bosques templados, manglares costeros, montañas y desiertos.

La región contaba con 734 millones de hectáreas de bosques en el año 2005, unos 3 millones más que en 2000. No obstante, este aumento fue resultado, en gran medida, de la alta tasa de repoblación forestal de China, la cual oculta la notable desaparición de bosques naturales en diversos países; en total, desaparecieron en la región 3,7 millones de hectáreas de bosque al año entre 2000 y 2005.

Algunos países han invertido sus tendencias de pérdida de bosques, pero no es probable que los países que sufren una mayor deforestación sean capaces de hacerlo. La expansión de los cultivos comerciales a gran escala será la causa más importante de deforestación en la región.

La región de Asia y el Pacífico cuenta con 136 millones de hectáreas de bosques plantados, prácticamente la mitad del total mundial. La mayor parte de los bosques plantados se encuentran en Australia, China, Filipinas, la India, Indonesia, Nueva Zelanda, Tailandia y Vietnam.

Superficie forestal: extensión y variación

Europa cuenta con una cuarta parte de los recursos forestales mundiales, aproximadamente 1000 millones de hectáreas, el 81% de las cuales se encuentran en la Federación Rusa.

Prácticamente todos los países europeos poseen leyes que dificultan notablemente la deforestación y la reconversión a otros usos de la tierra. Además, se proporciona apoyo fiscal a la actividad forestal en virtud del Fondo Europeo Agrícola de Desarrollo Rural, lo que fomenta de manera significativa la plantación de árboles. Por ello, es probable que la superficie forestal aumente a medida que decrecen las tierras dedicadas a la agricultura.

Las principales amenazas a las que se enfrentan los recursos forestales en Europa son de naturaleza ambiental, como incendios, brotes de plagas y tormentas, algunas de las cuales se podrían incrementar con el cambio climático. Aunque se desconocen las repercusiones a largo plazo del cambio climático en los bosques, se han atribuido a este fenómeno numerosos acontecimientos catastróficos recientes. Se prevé un incremento considerable de la magnitud y de la frecuencia de los incendios, por ejemplo en la península ibérica y en la Federación Rusa

Superficie forestal: extensión y variación

Esta región contiene el 22% de la superficie forestal mundial. En ella se encuentra la mayor masa continua de bosque pluvial tropical del mundo: la cuenca del Amazonas.

En los últimos dos decenios, algunos países han concedido la propiedad legal de los bosques a las comunidades indígenas, por ejemplo, Perú, 6400 millones de hectáreas; Bolivia, 1200 millones de hectáreas; Brasil, millones de hectáreas; Colombia, 27 millones de hectáreas; Ecuador, 4,5 millones de hectáreas y Guyana, 1,4 millones de hectáreas de tierra, comprendidos los bosques. Si bien la propiedad confiere a las comunidades derechos firmes de uso sostenible de los recursos forestales, los conflictos sobre la propiedad, en ocasiones violentos, y la falta de aplicación de las normas y los reglamentos han permitido la ocupación y la explotación maderera ilegales en extensas áreas de estos bosques.

Entre 2000 y 2010, esta región perdió casi 64 millones de hectáreas, un 7%, de su superficie forestal. Más de una tercera parte de la deforestación mundial entre 2000 y 2010 tuvo lugar en esta región.

Todos los países de América del Sur registraron una pérdida neta en la superficie forestal entre 2000 y 2005, excepto Chile y Uruguay,que presentaban tendencias positivas debido a programas de plantación industrial a gran escala. Los nuevos bosques plantados para usos industriales, en particular en Argentina, Uruguay y, posiblemente, Colombia, podrían contrarrestar, en lo que se refiere a hectáreas forestadas, la desaparición de bosques naturales, pero no en términos ecológicos. En caso de los países integrados en la Región Norte de América Latina como lo son Ecuador, Colombia y Venezuela las políticas de protección de áreas forestales no son estrictas y la deforestación de la zona persiste, lo que amenaza el equilibrio ecológico y climático de América del Sur, y puede tener repercusiones mundiales (expuesto en el Acuerdo Caracas FAO 2010).

En contrapartida, en la mayoría de los países de América Central, la pérdida neta de superficie forestal disminuyó entre 2000 y 2005 en comparación con la década anterior, y Costa Rica logró un incremento neto de dicha superficie.

No obstante, en términos porcentuales, América Central presenta una de las mayores tasas de desaparición forestal del mundo en relación con el resto de las regiones, más del 1% anual en el período entre 2000 y 2005.

En el Caribe se registró un reducido aumento de la superficie forestal entre 2000 y 2005, principalmente en Cuba. La liberalización del comercio, que ha hecho que exportaciones agrícolas tradicionales como el azúcar y los plátanos no sean competitivas, está ocasionando el abandono de las tierras agrícolas y su conversión en bosque secundario (Eckelmann, 2005). Además, se está dando mayor énfasis a la protección del medio natural para apoyar la creciente industria del turismo. Por ello, se espera que la superficie forestal permanezca estable o se incremente en la mayoría de los países caribeños.

Superficie forestal: extensión y variación

La región contiene el 17% de la superficie forestal global (677 millones de hectáreas). Aproximadamente una tercera parte del territorio regional está cubierto de bosques. Debido a la gran variedad de condiciones climáticas hay una gran diversidad de ecosistemas forestales, desde bosques húmedos tropicales a bosques boreales. Algunos de los bosques más productivos del mundo se encuentran en esta región.
La cubierta forestal en la región se mantiene estable.

América del Norte contribuyó en un 2% aproximadamente a la deforestación mundial anual entre 2000 y 2005, aunque la tasa de desaparición de los bosques presenta una tendencia a la baja. El cambio climático podría intensificar las amenazas al estado de los bosques. La intensidad y la frecuencia de los incendios forestales han aumentado tanto en el Canadá como en los Estados Unidos, impulsadas por prolongadas sequías (atribuidas al cambio climático) y por programas de control de incendios que, aunque han tenido éxito, han incrementado de manera inadvertida la cantidad de material combustible. De igual manera, el cambio climático está fomentando las infestaciones de plagas: en el oeste del Canadá y de los Estados Unidos, el escarabajo del pino de montaña está causando mortalidad de árboles y daños de especial gravedad.

El Gran Bosque de Agua se encuentra entre las ciudades de México D.F., Cuernavaca y Toluca. Este bosque posibilita la vida no sólo de quienes lo habitan, sino también la de quienes viven en los alrededores. Abarca las sierras de las Cruces, del Ajusco, del Chichinautzin, de Zempoala y el sistema Cadera, en los estados de Morelos, Estado de México y el Distrito Federal. Alberga casi 2 por ciento de la biodiversidad mundial, ayuda a regular el clima y la calidad del aire de la región, proporciona casi tres cuartas partes del agua que se consume en la ciudad de México y abastece de agua a dos de los ríos más importantes del país: el Lerma y el Balsas. Desafortunadamente, esta región se encuentra amenazada y está desapareciendo rápidamente: si continúa el actual ritmo de deforestación, podría desaparecer en solo 50 años. De acuerdo con el Instituto de Geografía de la Universidad Nacional Autónoma de México, cada año se pierden 2400 hectáreas de este bosque, lo que equivale a destruir una superficie de más 6 campos de fútbol por día. El Gran Bosque de Agua abarca 120000ha, incluye los parques nacionales de La Marquesa, Ajusco, Desierto de los Leones, Lagunas de Zempoala y Tepozteco, entre otras áreas. La tala ilegal, y el crecimiento desmedido de las urbes, la conversión del bosque en zonas de cultivo y potreros para ganado, los incendios forestales y la extracción de materiales y la venta del suelo lo merman constantemente. Además, la cacería furtiva y la introducción de especies exóticas ponen en riesgo la biodiversidad de esta región y son las causas por las que este bosque está en peligro. Mucha gente que reside en los estados donde se encuentra este bosque no conoce su importancia ecológica y la relevancia que tiene como principal fuente de abastecimiento de agua. Varios organismos e instituciones están trabajando en el análisis para la protección de este gran pulmón y fuente de agua. La Comisión Nacional para el Conocimiento y Uso de la Biodiversidad (Conabio) declaró parte de esta zona como una de las regiones terrestres prioritarias para la conservación en México, y la llamó "Corredor biológico Ajusco-Chichinautzin". Greenpeace México ha lanzado una campaña para protegerla.

Es la región con menos bosques del mundo, con tan sólo un 4% de cubierta forestal (el 1,1% de la superficie forestal mundial). La mayor parte de la superficie forestal corresponde a unos pocos países, mientras que en 19 países se encuentra menos del 10% de la cubierta forestal. Cerca del 75% de la región es árida, con una baja productividad de biomasa. La vegetación varía desde matorrales desérticos en Asia Central y la península arábiga hasta pequeñas áreas de manglares en la costa del golfo Pérsico y praderas de altura en Asia central. Debido a esta reducida cubierta forestal, los árboles fuera del bosque, especialmente en granjas y en otras tierras arboladas, desempeñan importantes funciones productivas y protectoras.

El establecimiento de cortavientos es una parte integral de las prácticas agrícolas en la mayoría de los países. El cultivo de palma datilera en diversos países de Asia occidental ha convertido los desiertos en oasis. En los Emiratos Árabes Unidos, las extensas plantaciones de palmas datileras han mejorado el paisaje a la vez que producen ingresos sustanciales.

Superficie forestal: extensión y variación

La deforestación está en marcha y está afectando al clima.

La deforestación es un contribuyente neto al calentamiento mundial, y se cita a menudo como una de las causas principales del efecto invernadero. La pérdida de los bosques tropicales es responsable de aproximadamente el 20% de las emisiones mundiales de gases de efecto invernadero. De acuerdo con el Grupo Intergubernamental de Expertos sobre el Cambio Climático, la deforestación, principalmente en áreas tropicales, podría suponer hasta un tercio de las emisiones de dióxido de carbono antropogénicas. Pero cálculos recientes sugieren que las emisiones de CO provocadas por la deforestación y la degradación forestal (excluidas las emisiones naturales de las turberas) supondrían entre el 6 y el 17% de todas las emisiones antropogénicas de CO, con una media del 12%. La deforestación provoca que el CO permanezca más tiempo en la atmósfera. Al aumentar el CO, se crea una capa que atrapa la radiación solar. Esta radiación se convierte en calor, provocando así el efecto invernadero.

Las plantas extraen el CO de la atmósfera a través de la fotosíntesis, quedándose con el carbono, que incorporan a su estructura (raíces, tallos, hojas, flores) en forma de moléculas orgánicas y liberando parte del oxígeno. Aunque también liberan algo de CO durante su proceso normal de respiración. Solo cuando un árbol o un bosque crecen pueden extraer carbono de la atmósfera, almacenándolo en sus tejidos. Tanto la putrefacción de la madera como su quema devuelven a la atmósfera ese carbono almacenado. Para que los bosques realmente extraigan carbono de la atmósfera debe haber una acumulación neta de madera. Una forma es cortar los árboles, transformar la madera en objetos duraderos y reemplazar con nuevos árboles los cortados. La deforestación también puede hacer que se libere el CO acumulado en el terreno. Los bosques pueden ser tanto sumideros de carbono como fuentes, dependiendo de las circunstancias ambientales. Los bosques maduros (donde la cantidad de materia vegetal no varía significativamente) alternan entre comportarse como fuentes netas y sumideros netos (véase Ciclo del carbono), pero esta variación resulta insignificante en relación con la enorme cantidad de carbono que tienen almacenada.

En las áreas deforestadas, el terreno se calienta más rápido por efecto del sol y alcanza una mayor temperatura, lo que lleva a mayores corrientes de convección ascendentes que favorecen la formación de nubes y finalmente producen más lluvia. Sin embargo, de acuerdo con el Laboratorio norteamericano de Dinámica de Fluidos Geofísicos (GFDL por sus siglas en inglés), los modelos utilizados para investigar los efectos a gran distancia de la deforestación tropical mostraron un amplio, pero suave, incremento de la temperatura en toda la atmósfera tropical. Estos modelos predijeron un calentamiento inferior a los 0,2°C en la atmósfera tropical superior (entre 700 y 500 milibares). Sin embargo estos modelos no predicen cambios significativos en otras áreas más allá de los trópicos. Aun así la realidad puede ser diferente, porque el modelo puede contener errores y sus resultados nunca son absolutamente definitivos.

La deforestación afecta a los vientos, el vapor de agua y la absorción de energía solar, influyendo así claramente en el clima zonal y mundial.
La reducción de las emisiones de la deforestación y la degradación forestal (REDD por sus siglas en inglés) en países en desarrollo ha surgido como un importante complemento a las políticas climáticas actuales. La idea consiste en compensar económicamente a los países que consigan estas reducciones de forma significativa.

Los legos piensan que los bosques tropicales contribuyen significativamente al oxígeno de la atmósfera aunque los científicos consideran que la contribución neta de los bosques tropicales es pequeña y que la deforestación solo tiene efectos menores en los niveles de oxígeno atmosférico. No obstante, la quema de masa forestal para obtener tierras cultivables libera ingentes cantidades de CO, que contribuyen al calentamiento mundial. Los científicos también afirman que la deforestación tropical libera anualmente 1 500 millones de toneladas de carbono a la atmósfera.

La deforestación también afecta al ciclo del agua: los árboles extraen agua del subsuelo a través de sus raíces y la liberan a la atmósfera. Cuando desaparecen, el clima se vuelve más seco. Además la deforestación reduce la cantidad de agua en el terreno y en el subsuelo, de modo que las plantas restantes ven reducida su disponibilidad de agua. Asimismo la deforestación reduce la cohesión del suelo, lo que da lugar a erosión, inundaciones, desertificación y corrimientos de tierras.

Al reducirse la cubierta arbórea disminuye la capacidad del entorno para interceptar, retener y transpirar la lluvia caída. Las áreas boscosas atrapan el agua y la filtran al subsuelo; las deforestadas, en cambio, se vuelven fuentes de agua superficial, que se mueve mucho más deprisa que la subterránea. Los bosques devuelven a la atmósfera por transpiración la mayoría del agua que cae sobre ellos como precipitación. Por el contrario, cuando se deforesta una zona, casi toda la precipitación se pierde en forma de agua superficial. Ese transporte más rápido de agua superficial puede traducirse en inundaciones relámpago e inundaciones más concentradas de las que ocurrirían si se hubiera mantenido la cubierta arbórea. La deforestación también reduce la evapotranspiración, y consiguientemente los niveles de humedad atmosférica, lo que en algunos casos afecta a las precipitaciones en las zonas a sotavento del área deforestada, porque el agua no se recicla en los bosques a sotavento, sino que corre por la superficie y va directamente a los océanos. De acuerdo con un estudio, en el área deforestada al norte y noroeste de China, la precipitación media anual descendió un tercio entre la década que comenzó en 1951 y la de 1981.

Los árboles, y las plantas en general, inciden significativamente en el ciclo hidrológico:
Como resultado, la presencia o ausencia de árboles cambia la cantidad de agua subterránea, superficial o atmosférica. Esto cambia también el ritmo de erosión y la disponibilidad de agua ya sea por el ecosistema o para las necesidades humanas. La deforestación de las llanuras traslada la formación de nubes y la lluvia a terrenos más elevados.

En el caso de lluvias muy intensas y prolongadas que rebasen la capacidad normal de absorción de los bosques, es posible que, a pesar de su presencia, se produzcan inundaciones.

La selva tropical es la fuente de alrededor del 30% del agua dulce del planeta.

La deforestación altera los patrones climáticos favoreciendo un tiempo más cálido y seco, y por tanto incrementando la sequía, la desertificación, la pérdida de cosechas, la fusión de los polos, las inundaciones costeras y el desplazamiento de flora y fauna

Los bosques naturales tienen un ritmo de erosión muy bajo, aproximadamente 2 toneladas métricas por kilómetro cuadrado. La deforestación generalmente incrementa el ritmo de pérdida de suelo al aumentar la escorrentía y reducir el escudo de residuos vegetales. Esto puede ser una ventaja en los suelos de selvas tropicales excesivamente lavados. Las propias operaciones de tala incrementan la erosión por la construcción de carreteras y el uso de maquinaria pesada.

La Meseta de Loes en China fue despojada de sus bosques originales hace milenios. Desde entonces ha estado erosionándose, creando profundas cárcavas, proporcionando el sedimento que da al río Amarillo su color característico y favoreciendo las inundaciones en su curso bajo.

La desaparición de los árboles no siempre incrementa el ritmo de erosión. En ciertas regiones del suroeste de Estados Unidos los arbustos y los árboles han estado limitando las praderas. Los propios árboles refuerzan la pérdida de plantas herbáceas en el suelo sombreado por sus copas. Si el suelo queda desnudo, es muy vulnerable a la erosión. El Servicio Forestal estadounidense, por ejemplo en el parque nacional Bandelier, estudia cómo restaurar el ecosistema, y reducir la erosión, quitando los árboles.

Las raíces de los árboles cohesionan el suelo y, si es lo suficientemente superficial, lo mantienen en su lugar ligándolo a la roca madre. Por esta razón talar los árboles de laderas empinadas con suelo superficial puede incrementar el riesgo de corrimientos de tierras y amenazar las vidas de quienes residan cerca.

La deforestación disminuye la biodiversidad y es causa de la extinción de muchas especies. Más de la mitad de las especies de plantas y animales terrestres viven en las selvas tropicales. La pérdida de áreas boscosas ha resultado en un entorno degradado, con menor biodiversidad. Los bosques sostienen la biodiversidad proporcionando un hábitat a numerosas especies de fauna y flora, algunas de las cuales pueden tener aplicaciones medicinales. Siendo los biotopos forestales fuentes irreemplazables de nuevas medicinas (como el taxol), la deforestación puede destruir irrecuperablemente la riqueza genética que proporciona a las plantas comestibles resistencia frente a las plagas.

Al ser las selvas tropicales los ecosistemas más diversos de la Tierra y encontrarse en ellos alrededor del 80% de la biodiversidad conocida, la desaparición de áreas significativas de cubierta arbórea ha resultado en degradación del suelo y un entorno de menor biodiversidad. Un estudio en Rondonia (Brasil) muestra que la deforestación acaba también con la comunidad microbiana que se ocupa de reciclar los nutrientes, limpiar el agua y eliminar la contaminación.

Se estima que cada día estamos perdiendo 137 especies de plantas y animales (incluidos insectos) debido a la deforestación de las selvas, lo que supone 50000 especies anuales. Autores como Lewin "etal." afirman que la deforestación de las selvas está contribuyendo a la extinción masiva del Holoceno.

Los ritmos conocidos (no estimados) de extinción de mamíferos y aves por la deforestación son mucho más bajos, aproximadamente una especie por año. Pero si se extrapola a todas las especies sale la cifra de aproximadamente 23000 cada año. Se ha predicho que el 40% de las especies animales y vegetales del sudeste asiático podría desaparecer en el s.XXI. Posteriormente se han cuestionado estas predicciones al observarse en 1995 que en el sudeste asiático la mayoría del bosque original ha sido transformado en plantaciones de monocultivo, pero que las especies potencialmente amenazadas son pocas, y que los árboles y el resto de la flora permanecen estables y muy extendidos.
La comprensión científica del proceso de extinción es insuficiente para hacer predicciones acertadas sobre el impacto de la deforestación en la biodiversidad. La mayoría de las predicciones de pérdida de biodiversidad ocasionada por operaciones silvícolas se basan en modelos especie-área, asumiendo que si el bosque decae, la diversidad de las especies decaerá de modo similar.

Un estudio de 2012 sobre la Amazonia predice que, pese a la falta de extinciones por ahora, hasta el 90% de las predichas se producirá en los próximos 40 años.

Fragmentar los bosques, o incluso trazar carreteras en ellos, tiene un fuerte impacto sobre la biodiversidad: un estudio publicado en Nature en 2017 muestra que el 85% de las especies de animales que viven en una selva se ven afectadas por el efecto linde. El 46% aumenta su abundancia, y el 39% (en general, las especies más amenazadas, y especialmente anfibios pequeños, grandes reptiles y mamíferos no voladores de tamaño medio) la disminuye.

Las principales organizaciones internacionales, incluidas las Naciones Unidas y el Banco Mundial, han empezado a desarrollar programas de lucha contra la deforestación. El término general REDD (siglas en inglés de Reducción de Emisiones de Deforestación y Degradación) describe estos programas, que emplean incentivos monetarios directos o de otro tipo para animar a los países en desarrollo a que limiten o reviertan su deforestación. Se ha debatido sobre la financiación, pero en la decimoquinta conferencia de las partes (COP 15) de la Convención Marco de las Naciones Unidas sobre el Cambio Climático (CMNUCC) en Copenhague (diciembre de 2009) se alcanzó un acuerdo por el que los países desarrollados se comprometieron a aportar recursos nuevos y adicionales, incluidas la silvicultura e inversiones canalizadas por instituciones internacionales, que se aproximarán a los 30 millardos de dólares para el período 2010-2012
Se está trabajando significativamente en herramientas para controlar cómo los países en desarrollo cumplen los objetivos REDD a los que se han comprometido. Estas herramientas, que incluyen seguimiento remoto de los bosques por imágenes satelitales y otras fuentes de datos, incluido FORMA (acrónimo en inglés de iniciativa de Seguimiento Forestal para la Acción) del Centro para el Desarrollo Global y el portal de seguimiento del carbono forestal del Grupo de Observación de la Tierra (GEO por sus siglas en inglés). También se dio importancia al guiado metodológico para el seguimiento de los bosques en la COP 15. La organización medioambiental Socios para Evitar la Deforestación encabeza la campaña para el desarrollo de la REDD a través de financiación del Gobierno estadounidense. En 2014 la FAO, con varios socios, lanzó Open Foris —un conjunto de programas informáticos de código abierto para ayudar a los países a recoger, producir y difundir información sobre el estado de sus recursos forestales—. Estos programas (hay versión en español) sirven para todo el ciclo de vida del inventario forestal, desde la valoración de las necesidades, diseño, planificación, recogida y gestión de datos sobre el terreno, análisis estimativos y difusión. Se incluyen herramientas para el procesado de imágenes remotas, así como para las comunicaciones internacionales REDD y MRV (siglas en inglés de medida, comunicación y verificación).

Para evaluar las implicaciones generales de las reducciones de emisiones, los países donde se concentra la mayor atención son los de mucho bosque y altos ritmos de deforestación (HFHD por sus siglas en inglés) y los de poco bosque, pero altos ritmos de deforestación (LFHD por sus siglas en inglés). Países HFHD se consideran Brasil, Camboya, Corea del Norte, Guinea Ecuatorial, Malasia, Islas Salomón, Timor Este, Venezuela y Zambia. En cambio se anotan como LFHD Afganistán, Benin, Botswana, Birmania, Burundi, Camerún, Chad, Ecuador, El Salvador, Etiopía, Ghana, Guatemala, Guinea, Haití, Honduras, Indonesia, Liberia, Malaui, Malí, Mauritania, Mongolia, Namibia, Nepal, Nicaragua, Níger, Nigeria, Pakistán, Paraguay, Filipinas, Senegal, Sierra Leona, Sri Lanka, Sudán, Togo, Uganda, Tanzania y Zimbabue.

En Bolivia la deforestación en los cursos fluviales altos ha causado problemas medioambientales, entre ellos erosión del suelo y disminución de la calidad del agua. Un proyecto innovador para remediar la situación establece que los usuarios del agua río abajo paguen a los propietarios de tierras río arriba para conservar sus bosques. Los propietarios reciben 20 dólares norteamericanos ($) para conservar los árboles, evitar prácticas ganaderas contaminantes y favorecer la biodiversidad y la fijación de carbono por el bosque en su propiedad. También reciben 30$ para la compra de una colmena, lo que les compensa por la conservación de 2 hectáreas de bosque durante 5 años, de manera que se proteja una fuente de agua. Los ingresos por hectárea de la miel recolectada ascienden a 5 $ anuales, de modo que en 5 años ascienden a 50$ para el propietario. El proyecto lo llevan la Fundación Natura Bolivia y la organización ecologista Rare, con el apoyo de la Alianza Clima y Desarrollo.

Se afirma que transferir la propiedad de los terrenos donde se ubican los bosque a las poblaciones indígenas es una manera eficiente de protegerlos. 

Esto incluye la protección de tales derechos cuando las leyes existentes los conceden, como en la ley india de bosques. Se sostiene que transferir estos derechos en China, quizá la mayor reforma agraria de la edad contemporánea, ha incrementado la cobertura forestal. En Brasil, las áreas forestales cuya propiedad se ha transferido a pueblos indígenas sufren menos tala permanente que incluso los parques nacionales.

Talar el bosque y plantar con métodos agrícolas tradicionales rinde poco. Algunos métodos agrícolas nuevos que ofrecen mucho mayor rendimiento por hectárea (y por tanto permiten talar menos bosque, o no talarlo en absoluto, si se aplican al terreno donde se usaban métodos tradicionales) son: plantas hibridadas, invernaderos, huertos urbanos o hidroponía. Estos nuevos métodos dependen a menudo de insumos químicos (abonos, pesticidas) para mantener alto su rendimiento. En la agricultura cíclica (llamada así por oposición a la agricultura itinerante, en que una tribu tala una zona de bosque, la cultiva y, cuando la tierra se agota, la abandona para talar una nueva zona) el ganado pasta sobre tierra dejada en barbecho, fertilizándola y preparándola para una próxima siembra. La rotación de cultivos es una forma de agricultura cíclica. Por otra parte la agricultura biointensiva obtiene rendimientos muy altos en terrenos muy reducidos sin emplear sustancias químicas. La agricultura intensiva, en cambio, puede disminuir los nutrientes del suelo a un ritmo acelerado. El enfoque más prometedor, sin embargo, es la jardinería forestal (traducción habitual, pero poco afortunada del término "forest gardening"; poco afortunada porque, en español, la jardinería es ornamental, no nutricional; la traducción francesa, "bosque nutritivo" da una mejor idea del significado) en permacultura, que consiste en sistemas agroforestales, cuidadosamente diseñados para imitar a los bosques naturales, que favorecen las especies animales y vegetales de interes nutricional, maderero y otros usos. Estos sistemas tienen baja dependencia de combustibles fósiles y sustancias químicas, necesitan poco mantenimiento, son altamente productivos y causan poco impacto en el suelo, la calidad del agua y la biodiversidad.

Hay múltiples métodos adecuados y fiables para seguir la deforestación. Uno de ellos es la interpretación visual de fotos aéreas o imágenes por satélite. Es intensivo en mano de obra, pero no requiere formación de alto nivel en procesamiento automatizado de imágenes ni una fuerte inversión en ordenadores. Otro método es el análisis de los puntos calientes ("hotspots", zonas de rápido cambio) empleando la opinión de expertos o imágenes de satélite de baja resolución para identificar estas zonas, y entonces realizar análisis digitales detallados sobre imágenes satelitales de alta resolución. Normalmente se valora la deforestación cuantificando la cantidad de área deforestada, medida en el momento actual.

Desde un punto de vista medioambiental, cuantificar el daño y sus posibles consecuencias es una tarea más importante, mientras que los esfuerzos de conservación se centran en proteger los bosques y desarrollar usos de la tierra alternativos para evitar que la deforestación continúe. El ritmo de deforestación y el área total deforestada se han utilizado ampliamente para el seguimiento de la deforestación en muchas regiones, entre ellas la Amazonia brasileña por el INPEN(Instituto Nacional de Pesquisas Espaciais). 
Está disponible una vista satelital de la Tierra.

Desde hace siglos se han hecho esfuerzos para detener o frenar la deforestación, porque hace mucho tiempo que se sabe que puede causar daños ambientales tan graves que lleven a la desaparición de sociedades enteras. En Tonga los gobernantes desarrollaron políticas para evitar los conflictos entre las ganancias a corto plazo de convertir los bosques en tierras de cultivo y los problemas a largo plazo que ocasiona la desaparición del bosque. En Japón, durante el shogunato Tokugawa (s.XVII-XVIII) los shogunes desarrollaron un avanzado sistema de planificación a largo plazo para detener e incluso revertir la deforestación de los siglos precedentes, mediante la sustitución de la madera por otros productos y un uso más eficiente de la tierra que se había cultivado durante centurias. En la Alemania del s.XVI los terratenientes desarrollaron la silvicultura para lidiar con los problemas de la deforestación. Sin embargo esas políticas tienden a limitarse a ecosistemas con "suficiente lluvia", "sin estación seca" y "suelos muy jóvenes" (resultado de vulcanismo o glaciaciones). En suelos más viejos y menos fértiles los árboles crecen demasiado despacio como para que la silvicultura sea económica, mientras que en zonas con una larga estación seca, siempre hay un riesgo de que un incendio forestal destruya los árboles plantados antes de que maduren.

En las zonas donde se practica la agricultura de tala y quema (llamada también de roza y quema), el cambio a talar y carbonizar (en vez de quemar, con llama, en fuego abierto y combustión completa, la materia vegetal cortada, convertirla en carbón vegetal mediante combustión incompleta y esparcir el carbón sobre el terreno), no solo es un método duradero de fijación del carbono. También es extremadamente enriquecedor para el suelo. Mezclando el carbón vegetal con biomasa se crea la terra preta ("preta" por la palabra portuguesa para el color negro, no por prieta), uno de los suelos más ricos y el único conocido que se autorregenera.

La certificación de que un bosque se explota de manera sostenible, como la proporcionada por los sistemas mundiales Programa para el Reconocimiento de Certificación Forestal (PEFC por sus siglas en inglés) o Consejo de Administración Forestal (FSC por sus siglas en inglés) contribuye a contener la deforestación al crear mercado para productos de bosques gestionados sosteniblemente. De acuerdo con la FAO, «Una condición indispensable para la adopción de la gestión forestal sostenible es la demanda para productos producidos sosteniblemente y el deseo de los consumidores de pagar por los mayores costes que implican. La certificación representa cambiar de planteamientos regulatorios a incentivos de mercado para promover la gestión forestal sostenible. Al promover los atributos positivos de productos forestales de bosques gestionados sosteniblemente, la certificación se enfoca en el lado de la demanda de la gestión medioambiental.» En cambio, la australiana Rainforest Rescue alega que los estándares de organizaciones como FSC están demasiado conectados con la industria maderera y que por tanto no garantizan una gestión forestal sostenible y socialmente responsable. Que en realidad los sistemas de seguimiento de las certificaciones son inadecuados y en el mundo se han documentado varios casos de fraude.

Algunas naciones han tomado medidas para incrementar el número de árboles sobre la Tierra. En 1981 China creó el día nacional de plantado de árboles y en la década que comenzó en 2001 la cobertura forestal ha alcanzado el 16,55% del territorio cuando en la que comenzó en 1991 solo era del 12%.

Usar como leña el bambú, que técnicamente no es un árbol, sino una hierba (concretamente una gramínea) conduce a una combustión más limpia que la de madera de árbol, y como el bambú madura mucho más rápido que la madera, se reduce la deforestación, porque el suministro se puede reponer más rápidamente.

En muchas partes del mundo, especialmente el este de Asia, la reforestación y la forestación están incrementando las áreas boscosas. La cantidad de bosque ha aumentado en 22 de las 50 naciones del mundo con más bosques. Asia en conjunto ganó un millón de hectáreas de bosque entre 2000 y 2005. El bosque tropical en El Salvador creció más del 20% entre 1992 y 2001. Basándose en estas tendencias, un estudio estima que la superficie forestal mundial en 2050 será un 10% —una superficie de la extensión de la India— superior a la de 2006 .

En China, donde se han destruido bosques a gran escala, ha sido obligación legal de cada ciudadano capacitado de entre 11 y 60 años el plantar de 3 a 5 árboles anualmente, o hacer la cantidad de trabajo equivalente en otros servicios forestales. El Gobierno chino sostiene que, desde 1982, se ha plantado cada año al menos un millardo de árboles. En 2016 esta obligación ya no se encuentra vigente, pero cada 12 de marzo en China son las vacaciones de plantado. Además está en marcha el proyecto Gran Muralla Verde de China que, plantando árboles, pretende frenar la expansión del desierto de Gobi. Sin embargo, debido al alto porcentaje de árboles que mueren después de plantarlos (hasta el 75%), el proyecto no está teniendo mucho éxito. En China la superficie forestal ha aumentado 47 millones de hectáreas desde la década que comenzó en 1971. El número total de árboles en 2001 se estimaba en 35 millardos. Otra propuesta ambiciosa para China es el sistema aéreo de reforestación y control de la erosión.

En África, con un nombre parecido, la Gran Muralla Verde de África, se está llevando a cabo otra iniciativa de contención del desierto (el Sahara en este caso) mediante el plantado de árboles. Se ha propuesto utilizar invernaderos de agua marina.

En los países occidentales, la creciente demanda del consumidor por productos forestales que hayan sido cultivados y cosechados de forma sostenible está haciendo que los propietarios de bosques y la industria maderera rindan cada vez más cuentas de sus prácticas de gestión forestal y tala.

El programa de rescate de la selva de la norteamericana Arbor Day Foundation utiliza sus donaciones para comprar y preservar selvas antes de que las puedan adquirir compañías madereras. Esta fundación protege así las tierras de la deforestación. También aísla el modo de vida de las tribus primitivas que las habitan. Otras organizaciones como Cool Earth, Community Forestry International, The Nature Conservancy, WWF/Adena, Conservation International, African Conservation Foundation y Greenpeace también se centran en preservar los hábitats forestales. En particular Greenpeace ha identificado los bosques aún intactos y publicado esta información en Internet. Por su parte, el Instituto de Recursos Mundial ha trazado un mapa temático más simple donde se muestran los bosques hacia el año 6000 A.C. y a comienzos del S. XXI (mucho más reducidos). Estos mapas muestran la cantidad de reforestación requerida para reparar el daño causado por la humanidad.

Para satisfacer la demanda mundial de madera, los silvícolas Botkins y Sedjo proponen plantaciones de árboles de alto rendimiento. Se ha calculado que plantaciones que produzcan 10m³ (metros cúbicos) de madera por hectárea anualmente podrían suministrar toda la madera que demanda el comercio internacional utilizando solamente el 5% del área forestal actual. Los bosques naturales solo producen entre 1 y 2m³ por hectárea, y por tanto se requeriría de 5 a 10 veces más terreno para satisfacer la demanda. El ingeniero de montes Chad Olivier propone un mosaico de bosques de alto rendimiento entremezclados con tierras preservadas.

Los bosques plantados se incrementaron en el mundo del 4,1 al 7,0% de la superficie forestal total entre 1990 y 2015 En 2015 sumaban 280 millones de hectáreas, un incremento de alrededor de 40 millones de hectáreas desde 2010. El 18% de estos 280 millones son especies exóticas o introducidas, mientras que el resto son nativas del país donde se han plantado. En el este y sur de África, Sudamérica y Oceanía los bosques plantados son principalmente de especies introducidas: 65, 88 y 75% respectivamente. En Norteamérica, Asia central y occidental, y Europa, las proporciones de especies introducidas son muy inferiores: 1, 3 y 8% del área total plantada respectivamente.

En Senegal, en la costa oeste de África, un movimiento encabezado por jóvenes ha ayudado a plantar más de 6 millones de árboles de manglar. Estos árboles protegeran las aldeas de las tormentas y proporcionarán un hábitat a la fauna y flora local. El proyecto empezó en 2008 y en 2010 ya se ha pedido al Gobierno senegalés que proteja los nuevos manglares.





</doc>
<doc id="9767" url="https://es.wikipedia.org/wiki?curid=9767" title="Cáncer colorrectal">
Cáncer colorrectal

El cáncer colorrectal, también llamado cáncer de colon, incluye cualquier tipo de neoplasias del colon, recto y apéndice. Se piensa que muchos de los casos de cáncer colorrectal nacen de un pólipo adenomatoso en el colon. Estos crecimientos celulares en forma de hongo son usualmente benignos, pero de vez en cuando se vuelven cancerosos con el tiempo. En la mayoría de los casos, el diagnóstico del cáncer localizado es por colonoscopia. El tratamiento es por lo general quirúrgico, y en muchos casos es seguido por quimioterapia.

Es la tercera forma más común de cáncer y la segunda causa más importante de mortalidad asociada a cáncer en los Estados Unidos. El cáncer colorrectal causa 694 000 muertes a nivel mundial cada año.

El riesgo de contraer cáncer de colon en los EE. UU. es de alrededor del 7%. Ciertos factores aumentan el riesgo de que una persona desarrolle esta enfermedad, entre ellos:

La página del Instituto Nacional de Cáncer de los Estados Unidos no contempla el alcoholismo como uno de los riesgos del cáncer colorrectal. Sin embargo, otros artículos del mismo instituto citan que el abuso en el consumo de bebidas alcohólicas puede aumentar el riesgo de cáncer colorrectal.

Otros informes citan estudios epidemiológicos en los que se ha notado una leve, aunque consistente asociación del consumo dosis-dependiente de alcohol y el cáncer de colon aunque se esté controlando la fibra y otros factores dietéticos. A pesar del gran número de estudios, la causa de las relaciones alcohol y cáncer de colon aún no ha sido determinada a partir de los datos disponibles.

Un estudio encontró que quienes beben más de 30 gramos de alcohol cada día, y en especial aquellos que beben 45 gramos por día, tienen un riesgo mayor de contraer cáncer colorrectal. Otro estudio demostró que el consumo de una o más bebidas alcohólicas cada día se asocia con una incidencia un 70% mayor de la media de cáncer de colon. Mientras que se encuentra un duplicado riesgo de cáncer de colon por consumir alcohol, incluyendo cerveza, aquellos que beben vino tienen un riesgo disminuido. Las conclusiones de un estudio citan que para minimizar el riesgo de cáncer colorrectal, es mejor beber con moderación.

La patología es un tumor del colon, se reporta por lo general del análisis de tejido obtenido de una biopsia o una operación. El reporte patológico usualmente contiene una descripción del tipo de célula y el grado de avance. El tipo más común de célula cancerígena es el adenocarcinoma, el cual ocupa un 95% de los casos. Otros tipos menos frecuentes incluyen los linfomas y el carcinoma de célula escamosa.

El cáncer del lado derecho (colon ascendente y ciego), tiende a tener un patrón exofítico, es decir, el tumor crece hacia la luz intestinal comenzando desde la pared de la mucosa. Este tipo raramente causa obstrucción del paso de las heces y presenta síntomas como anemia. El cáncer del lado izquierdo tiende a ser circunferencial, y puede obstruir el intestino al rodear la luz del colon.

El adenocarcinoma es un tumor de células epiteliales malignas, originándose del epitelio glandular de la mucosa colorrectal. Invade la pared, se infiltra hacia la muscularis mucosae, la submucosa y la lámina muscularis propia. Las células malignas describen estructuras tubulares, promoviendo estratificación anómala, luz tubular adicional y estromas reducidos. A veces, las células del tumor tienen un patrón de crecimiento "discohesivo" y secretan moco, el cual invade el intersticio, produciendo lagunas mucosas y coloides (en el microscopio se ven como espacios vacíos), llamados adenocarcinoma "mucinosa" o coloide, pobremente diferenciado. Si el moco permanece dentro de la célula maligna, empuja el núcleo hacia la periferia, formando la característica célula en anillo de sello. Dependiendo de la arquitectura glandular, el pleomorfismo celular y la mucosecreción del patrón predominante, el adenoma puede presentar tres grados de diferenciación: pobre, moderadamente o bien diferenciada.

La presencia de mutaciones del gen K-ras (gen de la familia Ras) es de veraz importancia ya que supone la aplicación de un tratamiento del cáncer colorrectal distinto al empleado en pacientes sin dicha mutación. Estas mutaciones constituyen un factor predictivo negativo de respuesta a la terapia con anti-EGFR en esta patología. Además, el valor pronóstico del oncogén K-ras en el cáncer colorrectal también es de importante consideración.

El cáncer colorrectal no suele dar síntomas hasta fases avanzadas y por eso la mayoría de pacientes presentan tumores que han invadido toda la pared intestinal o han afectado los ganglios regionales. Cuando aparecen, los síntomas y signos del carcinoma colorrectal son variables e inespecíficos. La edad habitual de desarrollo del cáncer colorrectal es entre los 60 y 80 años de edad. En las formas hereditarias el diagnóstico acostumbra a ser antes de los 50 años. Los síntomas más frecuentes incluyen la hemorragia digestiva baja y la rectorragia, cambios en las defecaciones y dolor abdominal. La presencia de síntomas notables o la forma en que se manifiestan depende un poco del sitio del tumor y la extensión de la enfermedad:

Los síntomas principales son dolor abdominal, síndrome anémico y, ocasionalmente, la palpación de un tumor abdominal. Como el contenido intestinal es relativamente líquido cuando atraviesa la válvula ileocecal y pasa al colon derecho, en esta localización los tumores pueden llegar a ser bastante grandes, produciendo una estenosis importante de la luz intestinal, sin provocar síntomas obstructivos o alteraciones notables del hábito intestinal. El "dolor abdominal" ocurre en más del 60% de los pacientes referido en la mitad derecha del abdomen. El "síndrome anémico" ocurre también en más del 60% de los casos y se debe a pérdida continuada, aunque mínima, de sangre que no modifica el aspecto de las heces, a partir de la superficie ulcerada del tumor. Los pacientes refieren fatiga (cansancio, debilidad) palpitaciones e incluso angina de pecho y se les descubre una anemia microcítica e hipocroma que indica un déficit de hierro. Sin embargo, como el cáncer puede sangrar de forma intermitente, una prueba realizada al azar para detectar sangre oculta en heces puede ser negativa. Como consecuencia, la presencia de una anemia ferropénica en cualquier adulto, con la posible excepción de la mujer multípara premenopáusica, obliga a hacer un estudio preciso endoscópico y radiológico de todo el colon. Por razones desconocidas, las personas de raza negra tienen una incidencia mayor de lesiones en el colon derecho que las personas de raza blanca. Puede pasar desapercibido si se localiza en el ángulo hepático del colon y éste se oculta bajo la parrilla costal.

Por ser más estrecho, el dolor cólico en abdomen inferior puede aliviarse con las defecaciones, en el caso de algunos pacientes puede desarrollar anemia por falta de hierro igual que en el caso de Cáncer de colon derecho es importante darse cuenta que no solo pierda sangre por las heces sino también por otros orificios del cuerpo como puede ser por los orificios nasales(nariz) o por la boca. Es más probable que estos pacientes noten un cambio en las defecaciones y eliminación de sangre roja brillante (rectorragia) condicionados por la reducción de la luz del colon. El crecimiento del tumor puede ocluir la luz intestinal provocando un cuadro de obstrucción intestinal con dolor cólico, distensión abdominal, vómitos y cierre intestinal.

Como las heces se van concentrando a medida que atraviesan el colon transverso y el colon descendente, los tumores localizados a este nivel tienden a impedir su paso al exterior, lo que origina un dolor abdominal tipo cólico, a veces con "obstrucción intestinal" (ileo obstructivo) e incluso con perforación intestinal. En esta localización es frecuente la rectorragia, tenesmo rectal y disminución del diámetro de las heces. Sin embargo, la anemia es un hallazgo infrecuente. A veces la rectorragia y el tenesmo rectal son síntomas frecuentes de hemorroides, pero ante una rectorragia con o sin trastornos del hábito intestinal (diarrea o estreñimiento) es preciso realizar un tacto rectal y una proctosigmoidoscopia. La uretritis ocurre cuando el tumor se encuentra muy cerca de la uretra y puede comprimirla y originar infecciones recurrentes urinarias. Cuando su extensión sobrepasa los límites de la pared rectal, el paciente puede aquejar síntomas urinarios atribuibles a invasión vesical como hematuria y polaquiuria. Si aparece una fístula rectovesical hay neumaturia e infecciones urinarias recidivantes.

Existen varias pruebas que se usan para detectar el cáncer colorrectal. Con los síntomas que relate el paciente al médico, se realizará una historia clínica, donde se detallarán los síntomas, los antecedentes familiares y factores de riesgo en la anamnesis. El médico también le hará una exploración física completa que incluirá un tacto rectal. Con los datos obtenidos se solicitarán exploraciones complementarias o pruebas diagnósticas para confirmar el diagnóstico, determinar un estadio clínico y establecer un plan de tratamiento.

Mediante el tacto rectal se pueden palpar el 20% de los carcinomas colorrectales y valorar su grado de fijación al tejido vecino. El tacto rectal puede llegar casi 8 cm por encima de la línea pectínea. Aunque se ha demostrado que casi la mitad de los cánceres colorrectales ocurrirán cerca del ángulo esplénico (y serían inaccesibles), un restante 20% puede palparse. En caso de un cáncer de recto es necesario hacer un tacto rectal cuidadoso, para valorar el tamaño, fijación y ulceración del cáncer, así como el estado de los ganglios u órganos vecinos y la distancia del extremo distal del tumor al margen anal.

El tacto rectal debe formar parte de cualquier exploración física de rutina en adultos mayores de 40 años, ya que sirve como prueba de detección de cáncer de próstata en hombres, y es parte de la exploración de la pelvis en las mujeres, y una maniobra barata para detectar masas en el recto. El tacto rectal no se recomienda como única prueba para el cáncer colorrectal, porque no es muy preciso debido a su alcance limitado, pero es necesario realizarlo antes de introducir el sigmoidoscopio o el colonoscopio.

Si bien esta práctica es ampliamente conocida, y fácil de realizar, la mayoría de los tumores no se encuentran al alcance del dedo, y cuando estos son palpables el pronóstico ya suele ser ominoso. Quedando de esta manera otras alternativas como la solicitud de sangre oculta en materia fecal como un método más fiable y que ha demostrado disminuir la mortalidad por cáncer de colon en un 33% en algunos estudios.

La prueba de sangre oculta en las heces (PSOH) se usa para detectar sangre invisible en los excrementos. Los vasos sanguíneos que se encuentran en la superficie de los pólipos, adenomas o tumores colorrectales, frecuentemente son frágiles y se dañan fácilmente durante el paso de las heces. Los vasos dañados normalmente liberan una pequeña cantidad de sangre en el excremento. Sólo raramente hay sangrado suficiente para que las heces se tiñan de rojo (rectorragia o hematoquecia). La PSOH detecta la presencia de sangre mediante una reacción química. Si esta prueba es positiva, es necesario realizar una colonoscopia para ver si es un cáncer, un pólipo o si hay otra causa del sangrado, como por ejemplo hemorroides, diverticulitis o enfermedad inflamatoria intestinal. Los alimentos o los medicamentos pueden afectar los resultados de esta prueba, por lo cual es necesario evitar lo siguiente:


Las personas que se hacen esta prueba deben recibir instrucciones detalladas que expliquen cómo obtener una muestra de heces o excremento en el hogar (generalmente tres muestras). El material se entrega al consultorio del médico o a un laboratorio clínico para su posterior análisis. La prueba de una muestra de heces que el médico obtenga mediante un tacto rectal no es una prueba adecuada de PSOH.

Aunque la PSOH se realice en condiciones ideales, tiene limitaciones importantes como técnica de detección precoz. Aproximadamente el 50% de los pacientes con cáncer colorrectal demostrado, tienen la PSOH negativa (falso negativo), un hecho relacionado con el patrón de hemorragias intermitentes de estos tumores. Cuando se hacen estudios aleatorizados en cohortes de personas asintomáticas, de un 2 a un 4% tienen una PSOH positiva. Pero sólo de un 5 a un 10 % de estos pacientes tiene un cáncer colorrectal (el 90-95% son falsos positivos) y en un 20 a un 30% se encuentran pólipos benignos. Por tanto, en la mayoría de las personas asintomáticas con la PSOH positiva no se encontrará una neoplasia colorrectal. No obstante, las personas con PSOH positiva deben someterse de forma sistemática, a más estudios médicos, que incluyen sigmoidoscopia, enema de bario y colonoscopia, técnicas que no sólo son incómodas y caras, sino que también se asocian con un riesgo bajo, pero real de complicaciones importantes. El coste de estos estudios justificaría si el número pequeño de pacientes con neoplasia oculta que se descubren por tener una PSOH positiva tuvieran un pronóstico mejor y un aumento de la supervivencia.

Para algunas asociaciones médicas el cribado poblacional basado exclusivamente en la PSOH no es aconsejable, mientras que para otros lo es. Los ensayos en los que se ha investigado este planteamiento son plenamente maduros con aproximadamente 300.000 participantes en ensayos aleatorizados bien diseñados. Demuestran que la reducción de la mortalidad existe, aunque en algunos casos, dependiendo de la técnica usada para la PSOH es modesta y después de corregir por un sesgo de observación, la reducción de la mortalidad por cáncer colorrectal no resultó estadísticamente significativa. La aparente simplicidad de la prueba no puede ser un argumento a favor de su uso generalizado. La mala especificidad de la prueba—es decir, la PSOH puede ser positiva en otras patologías—puede conducir a que una gran proporción de pacientes se sometan indebidamente a enema de bario y colonoscopias repetidas.

Actualmente existen varios tipos de PSOH: el más antiguo es el test de guayaco que busca la presencia o ausencia de actividad peroxidasa del grupo hemo en las deposiciones, es éste el que arroja gran cantidad de falsos positivos. Existe también el test inmunohistoquímico que consiste en anticuerpos mono o policlonales que detectan porciones intactas de hemoglobina humana, disminuye los falsos positivos con hemoglobinas no humanas (carnes rojas, vitamina C, etc.). Últimamente se puede encontrar un test inmunohistoquímico que detecta mutaciones de ADN, puede encontrar 15 aberraciones frecuentes en K-ras, APC, p53, etc. Es más sensible y específico en la detección del cáncer colorrectal.

Un colonoscopio es un tubo iluminado, delgado, flexible y hueco, que tiene el grosor aproximado de un dedo. Se introduce a través del recto, en la parte inferior del colon. El médico además de ver a través del sigmoidoscopio para detectar cualquier anomalía, también puede conectarlo a una cámara de vídeo y a un monitor de vídeo para visualizarlo mejor y grabarlo en algún soporte como documento visual. Esta prueba puede ser algo incómoda, pero no debe ser dolorosa. Debido a que tiene sólo 60 centímetros de largo, sólo se puede ver menos de la mitad del colon. Antes de la sigmoidoscopia, el paciente debe aplicarse un enema para limpiar la porción inferior del colon.

Las estrategias de detección precoz se han basado en el supuesto de que más del 60% de las lesiones precoces se localizan en el rectosigma y por tanto son accesibles con el sigmoidoscopio. Sin embargo, por razones desconocidas, en los últimos decenios se ha producido una disminución constante de la proporción de cánceres de intestino grueso que se localizan inicialmente en el recto con el correspondiente aumento de los que lo hacen en la zona proximal del colon descendente.

Esta técnica indudablemente tiene importantes dificultades como:

Se trata de un tubo con iluminación mediante el que se puede detectar entre un 20-25% de los carcinomas colorrectales. Es útil para selección de adultos menores de 40 años con riesgo.

El sigmoidocopio es un instrumento fibróptico que mide 6 dm de largo, útil para la exploración del colon izquierdo, pudiendo llegar hasta el ángulo esplénico. No requiere preparación completa del intestino, no debe utilizarse para polipectomía terapéutica (excepto circunstancias especiales) y puede detectar el 50% de los carcinomas.

Este estudio permite observar la mucosa de la totalidad del colon, recto y por lo general del íleon terminal. El colonoscopio es un tubo flexible con una cámara de vídeo en la punta y mide 160 cm de largo. La colonoscopia es el método más preciso para detectar pólipos menores de 1 cm de diámetro. También permite tomar biopsias, realizar polipectomías, controlar hemorragias y dilatar estrecheces. En el caso de cáncer de recto es necesario observarlo con un sigmoidoscopio rígido, tomar una biopsia adecuada, predecir el riesgo de obstrucción y medir cuidadosamente la distancia desde el borde distal del tumor hasta la línea pectínea. En la actualidad, la colonoscopia es el examen más preciso y completo del intestino grueso, pero esta prueba junto con el enema con bario deben considerarse complementarios entre sí. Un colonoscopio es una versión larga del sigmoidoscopio. Se introduce a través del recto hasta el ciego, y permite observar la mucosa de todo el colon.

Si se encuentra un pólipo pequeño, de menos de 3 cm, generalmente es posible la polipectomía. Algunos tipos de pólipo, incluso los que no son cancerosos, podrían malignizarse y por eso normalmente se extirpan. La polipectomía endoscópica se realiza pasando un asa de alambre a través del colonoscopio para cortar el pólipo de la pared del colon mediante una corriente eléctrica. Siempre que es posible, el pólipo se envía a anatomía patológica para analizarla en un microscopio y detectar si tiene áreas que se hayan malignizado.

Si se detecta un pólipo o tumor de gran tamaño o cualquier otra anomalía, se realizará una biopsia. Para tomar una biopsia a través del colonoscopio se extrae una pequeña porción de tejido. El examen del tejido puede ayudar a determinar si es un cáncer, un crecimiento benigno o el resultado de una inflamación.

Antes de realizar una colonoscopia el paciente debe tomar productos evacuantes diferentes de los laxantes habituales y, a veces, debe ponerse enemas para limpiar el colon, de manera que no haya heces que dificulten la visión. Normalmente la colonoscopia no provoca dolor, porque durante el acto se administran analgésicos y sedantes intravenosos. La colonoscopia se suele realizar ambulatoriamente y el paciente rara vez requiere ingreso hospitalario para esta prueba. Normalmente dura de 15 a 30 minutos, aunque puede tardar más si fuera necesario extirpar un pólipo.

La colonoscopia se debe realizar ante una prueba de sangre oculta en heces positiva, ante el hallazgo de un pólipo o tumor en la sigmoidoscopia o ante un enema de bario sospechoso, y es recomendable realizarla siempre que se tengan antecedentes familiares de pólipos o cáncer de colon, así como en mayores de 50 años. Otras indicaciones habituales son la emisión de sangre con las heces, los cambios en el ritmo intestinal de reciente comienzo o la anemia por falta de hierro en varones o mujeres postmenopáusicas.

El sulfato de bario es una sustancia radioopaca que se usa para llenar parcialmente y abrir el colon. El sulfato de bario se administra a través de un pequeño tubo introducido en el ano. Cuando el colon está aproximadamente medio lleno de bario, se coloca al paciente sobre una mesa de rayos X para que el bario se disperse a través del colon. Luego se bombeará aire en el colon a través del mismo tubo, a fin de que se expanda. Esto produce las mejores imágenes de la mucosa del colon. Es preciso que el paciente tome laxantes la noche anterior y ponerse un enema de limpieza la mañana antes de esta prueba para que el colon esté limpio de heces.

El estudio de contraste de uso más frecuente para detectar cáncer colorrectal es el "enema de bario con doble contraste de aire" pues tiene una sensibilidad del 90 % para detectar pólipos mayores de 1 cm. Está siendo desplazado por la colonoscopia, aunque es más barato y accesible, por lo que se puede utilizar en pacientes con alta sospecha, mientras se espera a la realización de una colonoscopia. Junto con la sigmoidoscopia flexible es una alternativa eficaz para los pacientes que no toleran la colonoscopia o para el seguimiento a largo plazo tras resección de un cáncer o pólipo. También es útil en caso de lesión estenosante que impida el paso del colonoscopio.
Posibles imágenes que podemos encontrar sugerentes de cáncer colorrectal son:

El paciente debe tener limpio el colon de heces al igual que en la colonoscopia o el enema de bario, mediante laxantes y enemas de limpieza. En esta prueba no se introduce contraste en el colon, sólo se insufla aire para dilatarlo. Luego se realiza una tomografía computarizada especial llamada tomografía computarizada helicoidal o espiral. Este procedimiento es probablemente más preciso que el enema con bario, pero no es tan eficaz como la colonoscopia para detectar pólipos pequeños. La ventaja es que este procedimiento se puede realizar rápidamente y no requiere que se sede al paciente y a un costo menor que la colonoscopia. Sin embargo, una desventaja es que si se detecta un pólipo o neoplasia, no se puede llevar a cabo una biopsia o extirpación del pólipo durante el examen. Actualmente se incluye la colonoscopia virtual en las pruebas recomendadas por la Sociedad Americana del Cáncer en sus "Guías para el cribado del Cáncer Colorrectal 2008" para la detección precoz del cáncer colorrectal como opción alternativa a la colonoscopia clásica para aquellos pacientes que no desean realizarse una colonoscopia clásica.

Otras pruebas que también se deben realizar son:
Asimismo, desde 2010 está disponible la detección en sangre de un nuevo marcador tumoral genético para el cáncer de colon: la forma metilada del gen (mSEPT9), que se encuentra en más del 90% de los tumores de colon, pasando a la sangre en forma de ADN libre. La presencia de mSEPT9 en el plasma indica la posibilidad de que exista una neoformación relacionada con cáncer de colon. Este marcador se encuentra en otro tipo de tumores con muy poca frecuencia.

No se puede usar la ecografía para detectar tumores en el colon.

Un tipo especial de TAC, es la TAC espiral que proporciona gran detalle y también es útil para diagnosticar metástasis de cáncer colorrectal. En el TAC espiral con portografías, el material de contraste se inyecta en la vena porta, para ayudar a diagnosticar metástasis del cáncer colorrectal en el hígado.
La TAC también se utiliza para guiar con precisión una aguja de biopsia hacia una posible metástasis. Para este procedimiento, llamado biopsia con aguja guiada por TAC, el paciente permanece en la mesa de TAC, mientras se introduce una aguja de biopsia hacia la localización exacta del tumor. La TAC continúa hasta que se está seguro de que la aguja se encuentra dentro de la masa. Se extrae una pequeña muestra de tejido mediante una biopsia con aguja y se examina al microscopio.


El tratamiento del cáncer colorrectal puede incluir:

Un porcentaje importante de pacientes se atiende por primera vez con síntomas agudos que indican obstrucción o perforación del intestino grueso.
Desafortunadamente es posible que los primeros signos de cáncer de colon dependan de una enfermedad metastásica. Las metástasis hepáticas masivas pueden causar prurito e ictericia. La presencia de ascitis, ovarios crecidos y depósitos diseminado en los pulmones en la radiografía de tórax pueden deberse a un cáncer de colon que puede ser asintomático.
Las principales.

Si la obstrucción no se alivia y el colon continúa distendido, la presión en la pared intestinal puede exceder la de los capilares y no llegar la sangre oxigenada a la pared del intestino, lo que origina isquemia y necrosis. Si no se trata inmediatamente, la necrosis evolucionará hasta la perforación con peritonitis fecal y sepsis.
La obstrucción intestinal baja se produce fundamentalmente en el carcinoma de colon izquierdo (debido al menor calibre de su luz). La sintomatología típica de la obstrucción intestinal baja es la de dolor cólico, vómitos, distensión abdominal y ausencia de emisión de gases y heces.
Por tanto siempre debemos incluir al cáncer de colon en el diagnóstico diferencial de las obstrucciones intestinales agudas bajas.

El cáncer colorrectal puede diseminarse de cinco formas diferentes:

Los exámenes de prevención se utilizan para detectar pólipos y evitar que evolucionen a cáncer.
Los exámenes de detección precoz se usan para detectar el cáncer en sus fases iniciales, aunque no existan síntomas ni antecedentes de dicha enfermedad. Las pruebas de detección precoz del cáncer colorrectal no sólo pueden diagnosticarlo en una etapa temprana y curable, sino que también pueden prevenirlo al encontrar y extirpar pólipos que pueden malignizarse. Los cánceres también se pueden diagnosticar en sus etapas tempranas si el paciente comunica inmediatamente al médico cualquier síntoma, pero es mejor si se somete a pruebas de diagnóstico precoz antes de que aparezcan los síntomas.

La Sociedad Americana del Cáncer, recomienda tanto a hombres como mujeres a partir de los 50 años de edad, las siguientes tres opciones de prevención del cáncer y tres opciones de detección precoz del cáncer :

Prevención:
- Colonoscopia cada 10 años (Prueba de prevención preferida).
- Sigmoidoscopia flexible cada 5 años (Prueba de prevención alternativa).
- Colonoscopia virtual cada 5 años (Prueba de prevención alternativa).

Detección:
- Test de Inmunohistoquímica Fecal en sangre (FIT) anual (Prueba de detección preferida).
- Prueba anual de sangre oculta en heces (PSOH) (Prueba de detección alternativa).
- Test del ADN fecal cada 3 años. (Prueba de detección alternativa).

Debe someterse a pruebas de prevencíon y/o detección precoz de cáncer colorrectal a una edad más joven o hacérselas con mayor frecuencia, si existe cualquiera de los siguientes factores de riesgo de cáncer colorrectal:

Si la colonoscopia no está disponible, no es factible o el paciente no la desea, un enema de bario de doble contraste solamente, o la combinación de sigmoidoscopia flexible y enema de bario de doble contraste (EBDC) son alternativas aceptables. La adición de la sigmoidoscopia al EBDC puede proporcionar una evaluación diagnóstica más completa que el EBDC por sí solo, para encontrar lesiones significativas. Si el examen colonoscópico no puede alcanzar el ciego, es posible que se necesite un EBDC complementario, y si el EBDC identifica una lesión posible o no permite la visualización adecuada de todo el colon y recto, es posible que sea necesario realizar una colonoscopia complementaria.



</doc>
<doc id="9768" url="https://es.wikipedia.org/wiki?curid=9768" title="Aparato digestivo">
Aparato digestivo

El aparato digestivo es el conjunto de órganos encargados del proceso de la digestión, es decir, la transformación de los alimentos para que puedan ser absorbidos y utilizados por las células del organismo.

La función que realiza es la de transporte de alimentos, secreción de jugos digestivos, absorción de nutrientes y excreción mediante el proceso de defecación.

El proceso de la digestión consiste en transformar los glúcidos, lípidos y proteínas contenidos en los alimentos en unidades más sencillas, gracias a las enzimas digestivas, para que puedan ser absorbidos y transportados por la sangre.

Desde la boca hasta el ano, el tubo digestivo mide unos once metros de longitud. En la boca empieza propiamente la digestión. Los dientes trituran los alimentos y las secreciones de las glándulas salivales los humedecen e inician su descomposición química transformándose en el bolo alimenticio. Luego, el bolo alimenticio cruza la faringe, sigue por el esófago y llega al estómago, una bolsa muscular de litro y medio de capacidad cuya mucosa segrega el potente jugo gástrico. En el estómago el alimento es agitado hasta convertirse en el quimo.

A la salida del estómago, el tubo digestivo pasa a llamarse intestino delgado, de unos seis metros de largo y muy replegado sobre sí mismo. En su primera porción o duodeno recibe secreciones de las glándulas intestinales, la bilis procedente de la vesícula biliar y los jugos del páncreas. Todas estas secreciones contienen una gran cantidad de enzimas que degradan los alimentos y los transforman en sustancias solubles simples como aminoácidos.
El tubo digestivo continúa por el intestino grueso, de algo más de metro y medio de longitud. Su porción final es el recto, que termina en el ano, por donde se evacuan al exterior los restos indigeribles de los alimentos.

El aparato digestivo está formado por el tubo digestivo y las glándulas anexas (glándulas salivales, hígado y páncreas). El tubo digestivo procede embriológicamente del endodermo, al igual que el aparato respiratorio y presenta una sistematización prototípica, comienza en la boca y se extiende hasta el ano. Su longitud en el hombre es de 10 a 12 metros, siendo seis o siete veces la longitud total del cuerpo. En su trayecto a lo largo del tronco del cuerpo, discurre por delante de la columna vertebral. Comienza en la cara, desciende luego por el cuello, atraviesa las tres grandes cavidades del cuerpo: torácica, abdominal y pélvica. En el cuello está en relación con el conducto respiratorio, en el tórax se sitúa en el mediastino posterior entre los dos pulmones y el corazón, y en el abdomen y pelvis se relaciona con los diferentes órganos del aparato genitourinario.

Histológicamente la pared del tubo digestivo está formado por cuatro capas concéntricas que son de adentro hacia afuera:

El grosor de la pared y el aspecto de superficie, que puede ser lisa o no, cambian dependiendo del lugar anatómico. La mucosa puede presentar criptas y vellosidades, la submucosa puede presentar pliegues permanentes o pliegues funcionales. En la pared se encuentran también los plexos submucoso y mientérico que constituyen el sistema nervioso entérico que se distribuye a lo largo de todo el tubo digestivo, desde el esófago hasta el ano.

Los alimentos después de ser ingeridos y triturados por los dientes con la ayuda de la saliva producida por las glándulas salivares, forman un bolo alimenticio y pasan por el esófago en su camino hacia el estómago gracias al movimiento peristáltico. Una vez en el estómago, se inicia el proceso de digestión facilitado por el ácido clorhídrico secretado por las células parietales del estómago y las enzimas digestivas. Posteriormente pasan al intestino delgado, donde continúa la degradación química de los alimentos y tiene lugar la absorción de agua y nutrientes que son transportados hacia la sangre y la linfa. En el intestino grueso se acumulan las sustancias de desecho que forman las heces, las cuales se expulsan al exterior a través del ano.

El tubo digestivo es la principal superficie de intercambio entre el medio externo y el interno en los animales vertebrados. En un hombre adulto medio la superficie total de la mucosa gastrointestinal desplegando las microvellosidades intestinales es de alrededor de 350 metros cuadrados. Gracias al tubo digestivo el individuo puede realizar el proceso de nutrición mediante la digestión y absorción de los nutrientes contenidos en los alimentos, pero no es menos importante su función de defensa, pues dispone de sistemas de reconocimiento y rechazo de agentes o sustancias extrañas procedentes del mundo exterior. Estas funciones dependen de las estructuras propias del tubo digestivo como la barrera mucosa y el sistema inmune, pero juega un papel muy importante la colonización bacteriana que constituye la llamada microflora intestinal formada por bacterias beneficiosas para el organismo. Se calcula que un individuo normal tiene en su intestino alrededor de 100 billones de bacterias pertenecientes a entre 500 y 1000 especies diferentes.

Hasta fechas recientes, se asumía que los bebés nacen completamente libres de gérmenes y que la colonización inicial del intestino del recién nacido se produce durante el parto. No obstante, varios estudios actuales concluyen que esta colonización comienza antes del nacimiento del bebé. Las bacterias maternas pasan de la madre al aparato digestivo del feto desde las primeras fases del embarazo, si bien no se conocen los posibles mecanismos implicados en este fenómeno.

Las enzimas digestivas son sustancias que son capaces de romper las grandes moléculas presentes en los alimentos y convertirlos en moléculas más pequeñas que pueden ser absorbidas a través del intestino. Algunas de las más importantes son la lipasa producidas por el páncreas, las proteasas producidas por el estómago y el páncreas que descomponen las proteínas en aminoácidos, la amilasa, la lactasa secretada por el intestino delgado que descompone la lactosa presente en la leche y la sacarasa que actúa sobre la sacarosa y la convierte en glucosa y fructosa.

La boca o cavidad oral es el lugar por donde los alimentos comienzan su viaje a través del aparato digestivo, contiene diferentes estructuras, entre ellas los dientes que hacen posible la masticación y la lengua. Cerca de la boca se encuentran las glándulas salivales que producen saliva, la cual se mezcla con los alimentos, facilita la masticación, la deglución y ayuda a mantener los dientes limpios.

El esófago es un conducto que se extiende desde la faringe hasta el estómago. De los incisivos al cardias (porción donde el esófago se continúa con el estómago) hay unos 40 cm. El esófago empieza en el cuello, atraviesa todo el tórax y pasa al abdomen a través del orificio esofágico del diafragma. Habitualmente es una cavidad virtual (es decir que sus paredes se encuentran unidas y solo se abren cuando pasa el bolo alimenticio). El esófago alcanza a medir 25 cm y tiene una estructura formada por dos capas de músculos, que permiten la contracción y relajación en sentido descendente del esófago, estas ondas reciben el nombre de movimientos peristálticos y son las que provocan el avance del alimento hacia el estómago.

El estómago es un órgano en el que se acumula comida. Varía de forma según el estado de repleción (cantidad de contenido alimenticio presente en la cavidad gástrica) en que se halla, habitualmente tiene forma de "J". Consta de varias partes que son: fundus, cuerpo, antro y píloro. Su borde menos extenso se denomina curvatura menor y la otra, curvatura mayor. El cardias es el límite entre el esófago y el estómago y el píloro es el límite entre el estómago y el intestino delgado. En un individuo de tamaño medio mide aproximadamente 25 cm del cardias al píloro y el diámetro transverso es de 12 cm.

En su interior encontramos principalmente dos tipos de células: 

La secreción de jugo gástrico está regulada tanto por el sistema nervioso como el sistema endocrino, proceso en el que actúan varias sustancias: gastrina, colecistoquinina, secretina y péptido inhibidor gástrico. Cuando la comida llega al estómago, actúa sobre ella el ácido clorhídrico. El ácido clorhidrico degrada las proteínas de los alimentos y activa la pepsina que es una enzima que actúa también sobre las proteínas. En el estómago se secreta también una enzima lipasa que interviene en la degradación de las grasas pero su papel es muy escaso. Los alimentos mezclados con los jugos gástricos y el moco producido por las células secretoras del estómago forman una sustancia semilíquida que se denomina quimo, la cual avanza hacia el intestino delgado para continuar el proceso de digestión.

Es una glándula íntimamente relacionada con el duodeno, produce jugo pancreático que se vierte al intestino a través del conducto pancreático, sus secreciones son de gran importancia en la digestión de los alimentos. El páncreas segrega también hormonas como la insulina que pasan directamente a sangre y ayudan a controlar el metabolismo de los azúcares.

El hígado es la mayor víscera del cuerpo. Pesa 1500 gramos. Consta de cuatro lóbulos, derecho, izquierdo, cuadrado y caudado; los cuales a su vez se dividen en segmentos. Las vías biliares son las vías excretoras del hígado, por ellas la bilis es conducida al duodeno. Normalmente el conducto hepático derecho e izquierdo confluyen entre sí formando el conducto hepático común. El conducto hepático común, recibe un conducto más fino, el conducto cístico, que proviene de la vesícula biliar. De la reunión de los conductos císticos y el hepático común se forma el colédoco que desemboca en el duodeno junto con el conducto excretor del páncreas.

La vesícula biliar es una víscera hueca pequeña situada en la cara inferior del hígado. Su función es la de almacenar y concentrar la bilis segregada por el hígado, hasta ser requerida por los procesos de la digestión. Cuando se contrae expulsa la bilis concentrada hacia el duodeno a través del conducto cístico. Es de forma ovalada o ligeramente piriforme y su diámetro mayor es de unos 5 a 8 cm.

El intestino delgado comienza en el duodeno (tras el píloro) y termina en la válvula ileocecal, donde se une a la primera parte del intestino grueso. Mide entre 6 y 7 metros de longitud y de 2.5 a 3 cm de diámetro. Su calibre disminuye progresivamente desde su origen hasta la válvula ileocecal, 

En el intestino delgado se absorben los nutrientes de los alimentos ya digeridos. El tubo está repleto de vellosidades que amplían la superficie de absorción. El intestino delgado se divide en dos partes, la primera es el duodeno que tiene una longitud de 30 cm y la segunda es el yeyuno-íleon que mide 6 metros y medio.



El intestino grueso se inicia a partir de la válvula ileocecal en un fondo de saco denominado ciego y termina en el recto. Desde el ciego al recto describe una serie de curvas, formando un marco en cuyo centro están las asas del yeyuno e íleon. Su longitud es variable, entre 120 y 160 cm, y su calibre disminuye progresivamente, siendo la porción más estrecha la región donde se une con el recto o unión rectosigmoidea en la que su diámetro no suele sobrepasar los 3 cm, mientras que el ciego es de 6 o 7 cm.

El intestino grueso se divide en varias porciones que se denominan ciego, colon ascendente con una longitud de 15 cm, colon transverso con una longitud media de 50 cm, colon descendente con 10 cm de longitud, colon sigmoideo, recto y ano. El recto es la parte terminal del tubo digestivo.

El ano es la abertura al final del tracto digestivo. Consta de una esfinter anal externo y otro interno que tienen la función de controlar el proceso de expulsión de las heces al exterior. El funcionamiento inadecuado de los esfinteres del ano puede provocar incontinencia fecal.

El sistema digestivo se origina a partir del tubo digestivo primitivo, el cual se forma de la capa embrionaria conocida como endodermo, sin embargo la boca procede del ectodermo. El primitivo tubo digestivo se divide en cinco porciones que partiendo de la boca se llaman faringe, intestino anterior, intestino medio, intestino posterior y cloaca. 
El páncreas se forma a partir de dos esbozos del endodermo que aparecen en la 4ª y 5 semana y acaban por unirse. El hígado tiene un origen embriológico complejo pues las celúlas hepáticas proceden de un esbozo del endodermo, mientras que la cápsula de Glisson y los sinusoides hepáticos proceden del mesodermo.

El aparato digestivo es un sistema fundamental para el cuerpo. Algunas de las enfermedades que le afectan son las siguientes:


En todos los vertebrados el aparato digestivo es básicamente un tubo hueco que discurre a lo largo del organismo desde la boca hasta el ano. Sin embargo existen importantes diferencias dependiendo de la especie animal, entre otras razones por estar adaptadas cada una de ellas a un tipo de dieta. 

El sistema digestivo de las aves cuenta con algunos órganos específicos, por ejemplo el buche, que es una bolsa de tejido conectada con el esófago a la altura del cuello, en la que los alimentos ingeridos se almacenan temporalmente antes de pasar al resto del tubo digestivo.

En los reptiles, el intestino delgado desemboca en el recto y este en la cloaca que se utiliza como desembocadura común tanto del aparato digestivo como del aparato urinario y el sistema reproductor. En muchas especies el estómago puede distenderse enormemente lo que les permite engullir hasta el 70% de su peso en una sola comida.
En los rumiantes, por ejemplo, el estómago se divide en varias cámaras, en sucesión continua desde el esófago hasta el duodeno, las cuatro cavidades son: Rumen o panza, redecilla o bonete, Omaso o librillo y Abomaso, cuajar o estómago verdadero.




</doc>
<doc id="9770" url="https://es.wikipedia.org/wiki?curid=9770" title="Windsurf">
Windsurf

El surf a vela, windsurf o tabla a vela es una modalidad del deporte a vela que consiste en desplazarse en el agua sobre una tabla algo similar a una de surf, provista de una vela.

A diferencia de un velero, la vela o aparejo de una tabla de windsurf está articulado permitiendo su rotación libre alrededor de un sólo punto de unión con la tabla: el pie de mástil. Ello permite manipular el aparejo libremente en función de la dirección del viento y de la posición de la tabla con respecto a este último. El aparejo es manipulado por el windsurfista mediante la botavara.

Un equipo de windsurf está compuesto por todo esto:

La Asociación Profesional de Windsurfistas más conocida por su acrónimo (PWA) es el ente rector a nivel internacional en la práctica del windsurf. Está formada por los mejores windsurfistas del mundo y representan el deporte al más alto nivel de competencia, uno de sus principales objetivos es garantizar el buen desarrollo del deporte, organizar y supervisar los eventos profesionales, hacer nuevas reglas para el deporte, ayudar a promover el crecimiento de base, fortalecer los lazos de amistad entre las asociaciones existentes, clases y disciplinas de windsurf y de proporcionar apoyo y servicios para todos los amantes del windsurfing

En windsurf hay seis variantes de competición:

 - Maniobras carving. Primeros trucos del freestyle.



 - Maniobras con los pies en los footstraps. Primeros trucos del freestyle moderno.




 - Maniobras fuera de los footstraps. No se está en contacto con la tabla de forma permanente.

 - Power freestyle. Maniobras aéreas, saltos, trucos en olas.



1. Trasluchada: giro o cambio de sentido a favor del viento, además de la maniobra por excelencia en el windsurf. 

2. Footstrap: Cinchas para los pies.

3. Orzar: dirigir la proa de la tabla hacia barlovento.

4. Chopi: en inglés choppy, que significa mar picado o agitado. Se utiliza cuando aparecen pequeñas olas por todo el mar de manera desordenada.

5. Amura: respecto al desplazamiento de la tabla. Nos pondremos a babor cuando el viento nos venga desde la izquierda y a estribor cuando nos venga desde la derecha.

http://blogs.comunitatvalenciana.com/windsurf/

http://www.windsurfesp.com/artsurf.asp?section=002&codi=6787&col=301

http://www.totalwind.net/foro/viewforum.php?f=2

http://windsurfing.costasur.com/es/forward.html



</doc>
<doc id="9771" url="https://es.wikipedia.org/wiki?curid=9771" title="Oikistés">
Oikistés

Oikistés (οἰκιστής, en plural oikistai, οἰκισται) es la palabra griega que designa al fundador de una nueva colonia ("apoikia"). A veces se les equipara a la condición de "archegétes" (ἀρχηγέτης), que propiamente corresponde al dios Apolo como líder y protector de las colonias ("theos patroos") o a Heracles y a los "heros ktistes", "heros oikistés" o héroes epónimos de las polis griegas originarias (reyes -"basileos"- o héroes -"heros"- de la Época Oscura).

La expansión colonial de la antigua Grecia por el litoral Mediterráneo se dio esencialmente entre 750 a. C. y 500 a. C. Cada polis procuraba convertirse en "metrópolis" (μητρόπολις) estableciendo colonias como una válvula de escape a la presión demográfica y escasez de tierras ("stenochoría", στενοχωρία), que producían conflictos sociales internos ("stásis", στάσις). Tales colonias eran ciudades con un alto grado de independencia, pero fuertemente vinculadas con la ciudad de origen en múltiples aspectos (culturales, religiosos, jurídicos, institucionales y económicos -particularmente el mantenimiento de un activo comercio marítimo-).

Para liderar a los primeros expedicionarios que iban a establecerse en un entorno propicio para la fundación de una colonia, y tras la consulta ritual a un oráculo, se enviaba a un "oikistés", elegido habitualmente de entre los "aristoi" (las familias aristocráticas u oligárquicas). Una vez escogido el emplazamiento, era el "oikistés" el que protagonizaba los ritos fundacionales de la nueva ciudad, destacadamente el depositar el fuego sagrado traído de la "metrópolis" en un templo dedicado a los dioses protectores de ésta, que pasaban a serlo también de la colonia. Tras el acto de la fundación, se suponía que el "oikistés" debía continuar con el mandato que hubiera recibido (es "el interlocutor entre los que se marchan y los que se quedan y entre los dioses y los humanos"), con lo que algunos de ellos permanecían en la colonia, manteniendo su gobierno; mientras que otros volvían a la "metrópolis", abandonando a los colonos a su suerte. El lugar de enterramiento del "oikistes", en el ágora, se convertía en un lugar de culto. Los tiranos (gobernantes que accedían al poder de forma ilegítima) se prestigiaban asociándose a su figura con prácticas rituales semejantes, como "nuevos "oikistai"".

Entre los más destacados "oikistés" estuvieron:
Muchas de las narraciones de la fundación de colonias están tan mitificadas o relacionadas con personajes legendarios que es difícil determinar su historicidad, teniendo ante todo un valor antropológico en relación con su situación intermedia entre el mar y la tierra (arquetipos ctónicos y acuáticos):



</doc>
<doc id="9772" url="https://es.wikipedia.org/wiki?curid=9772" title="Literatura del Perú">
Literatura del Perú

"Literatura peruana" es un término que se refiere a las manifestaciones literarias producidas por autores de nacionalidad peruana, desde las tradiciones prehispánicas hasta la actualidad, lo que engloba la literatura cuzqueña, arequipeña, puneña, amazónica y de otras regiones del territorio del Perú, y que ha alcanzado mayor brillo en el siglo XX con nombres indispensables para la literatura universal, como el poeta César Vallejo o el novelista Mario Vargas Llosa. La pertenencia al canon de los cronistas de Indias es comúnmente más aceptada que otras manifestaciones paraliterarias, como la literatura infantil peruana o la literatura peruana de ciencia ficción.

La producción literaria del período prehispánico en el territorio centro-andino (que abarca territorios de las actuales repúblicas de Perú, Ecuador, Bolivia y Chile), está especialmente vinculada al Imperio de los Incas, siendo su principal vehículo de transmisión el idioma quechua o "runa simi", que los incas impusieron como lengua oficial. Los cronistas de la conquista y de la colonia han dado fe de la existencia de una literatura quechua, que se transmitió de manera oral y que se suele dividir en cortesana y popular.



Muchas de estas creaciones han llegado a nuestros días de forma diferida, plasmadas en los trabajos de los primeros cronistas (el Inca Garcilaso de la Vega recupera poesía quechua, mientras que Felipe Guaman Poma de Ayala relata el mito de las cinco edades del mundo). 

La literatura indígena fue desconocida o relegada hasta el siglo XX. Su inclusión en el canon oficial fue lenta. Ya en su tesis "El carácter de la literatura del Perú Independiente" (1905), José de la Riva Agüero y Osma consideró "insuficiente" la tradición quechua como para considerarla un factor predominante en la formación de la nueva tradición literaria nacional. Posteriormente Luis Alberto Sánchez reconoció ciertos elementos de tradición y su influencia en la tradición posterior (en autores como Melgar) para dar base a su idea de literatura "mestiza" o "criolla" (hija de dos fuentes, una indígena y otra española), para lo que consulta fuentes en las crónicas coloniales (Pedro Cieza de León, Juan de Betanzos y Garcilaso).

La apertura real a la tradición prehispánica surge en las primeras décadas del siglo XX gracias al trabajo de estudiosos literarios y antropólogos que recopilaron y rescataron mitos y leyendas orales. Entre ellos se destacan Adolfo Vienrich con "Tarmap pacha huaray" ("Azucenas quechuas", 1905) y "Tarmapap pachahuarainin" ("Fábulas quechuas", 1906); Jorge Basadre en "La literatura inca" (1938) y "En torno a la literatura quechua" (1939); y los estudios antropológicos y folclóricos de José María Arguedas (en particular , su traducción de "Dioses y hombres de Huarochirí"). Los trabajos más contemporáneos incluyen a Martín Lienhard ("La voz y su huella. Escritura y conflicto étnico-cultural en América Latina. 1492-1988", 1992), Antonio Cornejo Polar ("Escribir en el aire. Escribir en el aire: ensayo sobre la heterogeneidad socio-cultural en las literaturas andinas." 1994), Edmundo Bendezú ("Literatura Quechua", 1980 y "La otra literatura", 1986) y Gerard Taylor ("Ritos y tradiciones de Huarochirí. Manuscrito quechua del siglo XVII", 1987; "Relatos quechuas de la Jalca", 2003).

Bendezú afirma que la literatura quechua se constituye, desde la conquista, en un sistema marginal opuesto al dominante (de vena hispánica) y postula la existencia permanente y cubierta de una tradición de cuatro siglos. Habla de una gran tradición ("enorme masa textual") marginada y dejada de lado por el sistema escritural occidental, ya que esta "otra" literatura es, como el quechua, plenamente oral.

El término "literatura colonial" (o "literatura de la Colonia") hace referencia al estado del territorio del Perú del siglo XVI al siglo XIX, dependiente de la corona española y políticamente organizado como un Virreinato.

Con la conquista española llegó al Perú el idioma castellano (mal llamado "español") y las tendencias literarias europeas. Se inicia un proceso que con el tiempo dará origen a una literatura mestiza o peruana, aunque inicialmente acuse de una preeminencia hispánica.

Francisco Carrillo Espejo ha acuñado el término de "literatura del descubrimiento y conquista", con el que se designa al período que abarca todas las obras escritas durante el proceso de descubrimiento y conquista del Perú, que se inicia en 1532 en Cajamarca con la captura del último Inca, Atahualpa, y finaliza con la desarticulación del Imperio Incaico. La literatura de este período, aunque no necesariamente escrita durante este marco temporal, sí se vincula a los eventos desarrollados antes o durante este.

Las primeras manifestaciones literarias fueron las coplas recitadas por los conquistadores; un ejemplo es la célebre copla escrita por un soldado durante el segundo viaje de Pizarro, quejándose ante el gobernador de Panamá de las penalidades que padecían:
Luego aparecieron las crónicas, cartas de descubrimiento y relaciones. Particularmente, las crónicas constituyen un interesante género literario que mezcla la historia, el ensayo literario y la novela. Las primeras crónicas, escritas por los soldados y secretarios de las expediciones militares, tienen un estilo rudo y seco. Luego aparecieron otras obras mejor trabajadas, como la de Pedro Cieza de León (1518-1554), autor de la "Crónica del Perú", dividida en cuatro partes: "Parte primera de la Crónica del Perú", "El Señorío de los Incas", "Descubrimiento y Conquista del Perú" y las "Guerras Civiles del Perú", que constituyen el primer gran proyecto de una historia andina global. Debido a ello, algunos consideran a Cieza como el primer historiador del Perú. Finalmente, el Inca Garcilaso de la Vega, mestizo, hijo de un conquistador español y una noble inca, publicó a principios del siglo XVII sus "Comentarios reales de los incas", obra que supera las exigencias de una simple crónica para convertirse en una obra maestra de la literatura, la primera escrita por un mestizo hispanoamericano.

El crítico Augusto Tamayo Vargas ha dividido a los cronistas en españoles, indígenas, mestizos y criollos.

Estos se dividen en dos grupos: cronistas de la conquista y cronistas de la colonización. Este último se subdivide a su vez en pre-toledanos, toledanos y post-toledanos.





Tres nombres se mencionan especialmente entre los cronistas indígenas, nativos o indios:


Pero indudablemente el más importante cronista mestizo es el Inca Garcilaso de la Vega (1539-1616), considerado como el ""primer mestizo biológico y espiritual de América"", o en otras palabras, el primer mestizo racial y cultural de América, pues supo asumir y conciliar sus dos herencias culturales: la indígena americana (inca o quechua) y la europea (española), alcanzando al mismo tiempo gran renombre intelectual. Se le conoce también como el ""príncipe de los escritores del Nuevo Mundo"", pues su obra literaria se destaca por un gran dominio y manejo del idioma castellano. En su obra cumbre, los "Comentarios reales de los incas", publicada en Lisboa, en 1609, Garcilaso expuso la historia, cultura y costumbres de los Incas y otros pueblos del antiguo Perú. Para muchos críticos se trata del cantar de gesta de la nacionalidad peruana, que se forja precisamente con la fusión de dos herencias, la nativa y la española. Garcilaso es autor también de "La Florida del Inca" (Lisboa, 1605), que es un relato de la conquista española de Florida; y de la "Segunda parte de los Comentarios reales", más conocida como "Historia General del Perú" (Córdoba, 1617), publicada póstumamente, donde el autor trata sobre la conquista y el inicio de la colonia. Con justicia se considera al Inca Garcilaso como el primer literato del Perú.

Entre los cronistas criollos o americanos (nacidos en América de padres españoles) que escribieron sobre el Perú destacan:

Se debe mencionar también al padre jesuita italiano Giovanni Anello Oliva (¿1572?-1642), que vivió más de 40 años en el Perú, y fue autor de una "Historia del reino y provincias del Perú y vidas de varones ilustres en la Compañía de Jesús de la provincia del Perú", cuya primera parte es una introducción histórica titulada: "Historia del reino y provincias del Perú, de sus incas, reyes, descubrimiento y conquista por los españoles de la corona de Castilla".

Hitos culturales importantes fueron la fundación de la Real y Pontificia Universidad de San Marcos de Lima el 12 de mayo de 1551 por Real Provisión de Carlos I de España y V de Alemania, la primera en América, y la instalación en Lima de la primera imprenta de Sudamérica, la del turinés Antonio Ricardo en 1583, instituciones que impulsaron el temprano desarrollo intelectual de los peruanos.

El primer libro publicado en la ciudad de Lima es la "Doctrina Christiana y Cathecismo para la Instrucción de los Indios" (1584) del impresor Antonio Ricardo, con lo que se inaugura propiamente la idea de literatura peruana. Este primer catecismo es publicado en castellano, quechua y aimara. Durante las décadas anteriores, ya se había establecido el sistema de reducciones producto de las reformas del virrey Francisco de Toledo (1569-1581) que separaron la sociedad colonial en dos repúblicas, república de indios y república de españoles (es el período en el que se realizaron la mayor cantidad de "extirpación de idolatrías"). También se promulgaron las Leyes de Indias que establecían lo siguiente:

Estos dos factores determinan que la inicial producción literaria en la Colonia se limite a círculos de influencia principalmente hispánica, producida en las grandes ciudades por hijos de españoles (españoles americanos). La literatura se cultiva en círculos ilustrados, estrechamente vinculados con la Iglesia (que imparte la educación entre las élites sociales, ya que todos los colegios y convictorios estaban dirigidos por órdenes religiosas). De la Iglesia es precisamente el padre José de Acosta, quien presta mayor atención al mundo americano ya que, junto a sus reflexiones religiosas y teológicas, encontramos una clara preocupación por la geografía y fisiología de los pueblos naturales del Perú. Acosta representa un momento en el que los estándares estéticos renacentistas están aún presentes en la escena literaria. En 1586 publica "Peregrinación de Bartolomé Lorenzo", en 1588 "De Natura Novi Orbis et De Promulgation Evangelii apud barbaros, sive de Procuranda indorum salute" ("De la naturaleza del nuevo mundo...") y en 1590 su obra más conocida: "Historia natural y moral de las Indias".

La literatura del llamado Siglo de oro español, se refleja también en la América española, especialmente en el campo de la poesía lírica y épica. Se trata de una literatura erudita, de refinadas formas, ceñida a los moldes clásicos (clasicismo). Los autores más relevantes que se desenvolvieron en el Perú bajo esta tendencia, son los siguientes:

Siguiendo la tendencia dictada desde Europa, la literatura peruana adopta el estilo del barroco (conceptismo y culteranismo). Se tiende a recargar el lenguaje literario con muchos recursos estilísticos y se hace gala de erudición. La figura cumbre del barroquismo peruano fue El Lunarejo.

Podemos mencionar también a Lorenzo de las Llamosas (c.1665-c.1705), quien después de unos pocos años de permanencia en el Virreinato del Perú, viaja a España donde desarrolla actividades en la Corte del Rey, como militar y al mismo tiempo como autor de obras de teatro y didácticas.

En la segunda mitad del siglo XVII, la literatura en Europa, bajo influjo de las letras francesas, tendió a volver a los moldes clásicos, aunque en las colonias españolas siguió preponderando el barroquismo. No obstante, a comienzos del siglo XVIII, coincidiendo con la instauración de la dinastía borbónica en España, los escritores de habla hispana tienden a “afrancesarse”. Surgen las Academias literarias, a imitación de las de Francia, como la llamada Academia de Palacio fundada por el virrey del Perú Marqués de Castell dos Rius (1707-1710). Entre los académicos de Palacio destacan los siguientes:
El Neoclasicismo irrumpe en la segunda mitad del siglo XVIII y fue desplazando progresivamente al barroquismo. Se trata de una vuelta a las normas del clasicismo, en oposición al estilo recargado del barroquismo, así como una tendencia a la actitud pedagógica. Este movimiento se desarrolló juntamente con la expansión de las ideas liberales surgidas en Francia, que tanto habrían de influir en el desarrollo de la revolución separatista de Hispanoamérica. 

La figura más conspicua del afrancesamiento literario en la segunda mitad del siglo XVIII fue Pablo de Olavide (1725-1803), escritor, traductor, jurista y político, nacido en Lima, pero que desenvolvió su carrera en España. Su casa en Madrid se convirtió en un destacado centro de tertulia cultural. Influido por la ilustración francesa, profesó inicialmente las ideas liberales. Acusado de herejía, fue encarcelado por la Inquisición. Reconciliado con la religión, publicó "El Evangelio en triunfo" (1797); "Poemas cristianos"; y "Salterio español" (1799). Ya en el siglo XX fueron exhumadas las obras de su periodo afrancesado, de género dramático y narrativo, siendo este último el que ha concitado el interés de la crítica moderna, pues se tratan de novelas cortas, que harían a Olavide precursor de dicho género literario.

Mientras que en el Perú se desenvuelven por esa época poetas y escritores satíricos criollos, cercanos al costumbrismo:

A fines del siglo XVIII y coincidiendo con el fin del mandato del virrey Manuel Amat y Juniet, se representó en las gradas de la catedral de Lima un drama, el "Drama de los palanganas: veterano y bisoño", que es una crítica despiadada contra el gobierno y la persona de este virrey, en particular sus amoríos con La Perricholi. El texto ha sido rescatado por el crítico literario Luis Alberto Sánchez.

El último periodo de la literatura colonial abarca desde fines del siglo XVIII hasta comienzos del siglo XIX, y corresponde a la época de la revolución emancipadora. Sobresalen, al estilo de los enciclopedistas franceses, los redactores del "Mercurio Peruano", la primera gran revista americana, quienes se agrupan en la llamada Sociedad de Amantes del País. Entre ellos destacan Hipólito Unanue, Toribio Rodríguez de Mendoza, José Baquíjano y Carrillo, entre otros.

En el campo de la lírica destaca el arequipeño Mariano Melgar (1791-1815), en cuyos versos se prefigura el romanticismo y muestra un mestizaje entre la poesía culta y las canciones populares indígenas. Aunque su obra se enmarca más dentro de la época republicana, y consta de "Carta a Silvia" (1827) y "Poesías" (1878). Se sumó a la revolución independentista en 1814 y murió fusilado.

Otro representante de la poesía de la Emancipación es José Joaquín Olmedo (1780-1847), nacido en Guayaquil cuando este pertenecía al Perú. Fue diputado ante el primer Congreso de la República del Perú y ministro plenipotenciario del Perú en Inglaterra. Su poema fundamental es "Oda a la victoria de Junín", versos épicos de corte neoclásico que cantan el triunfo obtenido por Bolívar en la batalla de Junín.

En el campo de la literatura política descuella el tribuno José Faustino Sánchez Carrión (1787-1825), defensor del sistema de gobierno republicano y autor de la "Carta del Solitario de Sayán".

Es necesario también mencionar al clérigo limeño José Joaquín de Larriva (1780-1832) poeta, escritor y periodista, apodado el “cojo Larriva”. Escritor satírico y muy mordaz, según Porras Barrenechea fue el “primer poeta cómico” del Perú. Actualmente se le recuerda más por las letrillas que escribiera contra el Libertador Bolívar, aunque en su tiempo fue muy popular y celebrado por sus oraciones fúnebres y laudatorias, y sus artículos periodísticos, además de sus improvisaciones poéticas. Es considerado precursor del costumbrismo literario peruano.

Las primeras corrientes literarias del Perú independiente fueron el costumbrismo y el romanticismo. Ya en el último tramo del siglo, se desarrolló el realismo.

El costumbrismo fue una corriente literaria cuyos cultivadores prestaban más atención a las costumbres de los pueblos, tanto para festejarlas, como para criticarlas o ridiculizarlas, a través de géneros diversos (comedias, letrillas, sainetes, etc.). En el Perú comienza hacia 1830, coincidiendo con el periodo fundacional de la República y se prolonga hasta los años 1850.

Al período costumbrista peruano pertenecen dos poetas satíricos y dramaturgos cómicos, ambos limeños, pero de espíritu contrapuesto:

De esta época es importante destacar también a los siguientes autores:

Cercana al costumbrismo está la obra de Ricardo Palma (1833-1919), escritor limeño, autor de las célebres "Tradiciones peruanas", la obra más conocida del siglo, en la que a través de una serie de tradiciones —género inventado por él, que combina elementos de historia con fabulaciones propias—, narra la historia de Lima y del Perú durante las épocas incaica, colonial y republicana. Escritas entre 1860 y 1914, una edición definitiva fue compilada por Angélica Palma, la hija del tradicionista, en seis volúmenes (1923-1925).

El romanticismo, proveniente de Europa, llegó al Perú con retraso, hacia los años 1840, y se prolongó por el resto del siglo, aunque decayó tras la Guerra del Pacífico, para dar pase al Realismo. Los textos de los románticos peruanos fueron, por lo general, artificiales y abusaron del sentimentalismo. Las obras de teatro frecuentemente cultivaron el mismo sentimiento y exageraron los enredos de modo inverosímil; si bien algunas tuvieron éxito en su momento, hoy están olvidadas. Dos representantes del romanticismo peruano, sin embargo, han sobrevivido literariamente, por la calidad de sus obras: Ricardo Palma y Carlos Augusto Salaverry, pertenecientes a la llamada "generación de la bohemia".



Al romanticismo pertenecen también los siguientes poetas, escritores y dramaturgos: 

Tras la guerra del Pacífico (1879-1883) hay una reacción contra el romanticismo, liderada por el intelectual Manuel González Prada (1844-1918), quien cultivó una poesía que por su temática estetizante y la introducción de nuevas formas métricas fue un claro precursor del modernismo. De entre sus obras en prosa se deben mencionar: "Pájinas libres" y "Horas de lucha", libros en las que hace una furibunda crítica a la clase política, responsable, según él, de la catástrofe bélica. No se salvan tampoco de sus dardos las instituciones religiosas y los literatos de su tiempo. Su postura hipercrítica en el terreno de las ideas y de la literatura le granjeó no pocos enemigos y le metió en variopintas polémicas periodísticas.

Se desarrolló también, de un modo bastante tenue, el realismo en la novela, que toma vuelo a partir de entonces en el Perú.

Una característica resaltante en este período es el surgimiento de un grupo de escritoras. Muchas de ellas —habiendo perdido a sus cónyuges e hijos mayores en la guerra con Chile— tuvieron que ganarse la vida por sí mismas, y cultivaron su vocación literaria a través de tertulias. La principal fue la de la argentina Juana Manuela Gorriti, en las que se discutía sobre los problemas sociales y sobre la influencia de las formas europeas. Escribieron novelas que en cierto modo pueden calificarse como realistas. Tal es el caso de:

El modernismo se desarrolló en el Perú a partir del poema «Al amor» de Manuel González Prada, publicado en el diario "El Comercio" en 1867, donde el autor fusiona un conjunto de géneros poéticos provenientes de Europa, dando como resultado el "triolet". Esta tendencia, resultado del cosmopolitismo que vivía el Perú, pronto se desarrolló en otras partes de América Latina: en Cuba con José Martí; en Nicaragua con Rubén Darío; en Argentina con Leopoldo Lugones; en Uruguay con Julio Herrera y Reissig; en México con Manuel Gutiérrez Nájera. 

A pesar de sus tempranos antecedentes con González Prada, el modernismo alcanzó en el Perú un pleno desarrollo tardíamente, a inicios del siglo XX. De entre todos sus representantes descuella el poeta limeño José Santos Chocano (1875-1934), conocido como «El Cantor de América», considerado uno de los poetas hispanoamericanos más importantes, por su poesía épica de tono grandilocuente, que gusta de la retórica y de la descripción de paisajes, con gran sonoridad y colorido, estando más próxima a Walt Whitman y al romanticismo. También produjo poesía lírica de singular intimismo. Todas sus creaciones poéticas están trabajadas con depurado formalismo y se inspira mayormente en los temas, los paisajes y la gente de su país y de América en general. Principales obras: "Iras santas" (1895), "En la aldea" (1895), "Selva virgen" (1896?), "La epopeya del morro" (1899), "El canto del siglo" (1901), "Alma América" (1906), "Fiat Lux" (1908), "Primicias de oro de Indias" (1934), "Oro de Indias" (1940-1941). Su vida fue muy novelesca y aventurera, ligada a la de los dictadores y caudillos latinoamericanos de su tiempo. Durante el Oncenio de Leguía sostuvo una polémica pública con el joven escritor Edwin Elmore, a quien en un arranque de ira asesinó disparándole a quemarropa. Tras sufrir un breve encierro, partió hacia Chile, donde murió asesinado a manos de un esquizofrénico.

Dentro del modernismo peruano también debemos destacar a los siguientes poetas:

Una importante rama del modernismo peruano fue la llamada Generación del 900, conocida también como la generación “arielista” (llamada así por inspirarse en las ideas del escritor uruguayo Enrique Rodó, el autor de "Ariel", que abogaba por la europeización de Hispanoamérica y la formación de elites intelectuales que se encargaran de su dirección). Sus miembros manejaban una prosa elegante y ahondaban particularmente en las raíces de la historia nacional, con tendencias hacia el idealismo (Tamayo Vargas). Fueron sus principales representantes:

En ese ambiente impregnado de modernismo surgió una figura insular: José María Eguren (1872-1942), poeta limeño que abrió el camino de la innovación en la poesía peruana con sus libros "La canción de las figuras" (1916) y "Simbólicas" (1911), próximos al simbolismo y que reflejaban su mundo interior mediante imágenes oníricas, con las que reacciona contra la retórica y el formalismo modernistas.

Hasta 1920 el modernismo era la tendencia dominante en el cuento y la poesía, pero desde 1915 la vanguardia literaria hizo tímidamente su entrada en la musa nacional. César Vallejo, con sus obras fuertemente innovadoras en el lenguaje centradas en la angustia y en la condición humana, pertenece a este período, en el que también aparecieron los poetas Alberto Hidalgo, Alberto Guillén, Xavier Abril, Carlos Oquendo de Amat, Luis Valle Goicochea, Magda Portal y los surrealistas César Moro y Emilio Adolfo Westphalen.

El escritor más importante del momento es Abraham Valdelomar, quien en su breve vida cultivó el cuento, la novela, el teatro, la poesía, el periodismo y el ensayo. Sobresalen sobre todo sus relatos, que narran con bastante ternura historias de las ciudades provincianas y, en menor medida, de Lima o cosmopolitas. En 1916 fundó la revista "Colónida" que agrupó a varios jóvenes escritores y que, a pesar de su breve existencia (se publicaron solo cuatro números), abrió el camino para la entrada de nuevos movimientos como la vanguardia en la literatura peruana.

Otros autores, que junto con Valdelomar inauguran el cuento en el Perú fueron Clemente Palma, que escribió relatos decadentes, psicológicos y de terror, influido por el realismo ruso y por Edgar Allan Poe; y Ventura García Calderón, quien mayormente escribió cuentos exóticos sobre el Perú. También se encuentran Manuel Beingolea, Manuel Moncloa y Covarrubias, "Cloamón", y Fausto Gastañeta.

En el teatro, con escasas obras de valor en este período, figuran las comedias del poeta festivo Leonidas Yerovi y, posteriormente, las obras de denuncia social y cariz político de César Vallejo, que pasaron mucho tiempo antes de ser publicadas o representadas. Ya en los años 1940 la influencia tardía del modernismo y del teatro poético se reflejará en las obras de Juan Ríos, a las que se les ha criticado su excesiva retórica poética, generalmente ambientadas en tiempos remotos o en leyendas y que buscan ser un referente general del hombre.

En el Perú el tema principal de la literatura indigenista era el indio, cuyo predominio en la literatura se había iniciado en los años 1920 y 1930, primero con los cuentos de Enrique López Albújar y más tarde con las novelas de Ciro Alegría: "La serpiente de oro" (1935), "Los perros hambrientos" (1939) y "El mundo es ancho y ajeno" (1941). Así empezó la interesante controversia sobre indigenismo e indianismo, vale decir, sobre la cuestión de que no sean los mismos indios quienes escriban sobre su problemática. Esta corriente literaria alcanzó su máxima expresión en la obra de José María Arguedas, autor de "Agua", "Yawar Fiesta", "Diamantes y pedernales", "Los ríos profundos", "El Sexto", "La agonía de Rasu Ñiti", "Todas las sangres" y "El zorro de arriba y el zorro de abajo", y quien debido a su contacto con los indígenas en la infancia, pudo asimilar como propias su concepción del mundo y experiencias.

La modernización de la narrativa peruana comienza con la Generación del 50, enmarcada políticamente con el golpe del general Manuel A. Odría en 1948 y las elecciones de 1950 en las que se autoelige presidente. Durante la década anterior había comenzado un movimiento migratorio del campo a la ciudad (preferentemente a la capital), que durante los años cincuenta se potencializa al máximo y resulta en la formación de barriadas y pueblos jóvenes, la aparición de sujetos marginales y desplazados socialmente. La literatura producida en este período estuvo influida notablemente por las vanguardias europeas; en particular, el llamado modernismo anglosajón de Joyce y en el ambiente norteamericano la obra novelística de Faulkner y la Generación Perdida. También influyó notablemente la literatura fantástica de Borges y Kafka. A esta generación pertenecen Julio Ramón Ribeyro, Carlos Eduardo Zavaleta, Eleodoro Vargas Vicuña, Mario Vargas Llosa, entre otros.

La Generación del 50 es un momento en el que la narrativa se vincula de forma muy fuerte con el tema del desarrollo urbano, la experiencia de la migración andina hacia Lima (un incremento drástico de la población a partir de finales de la década del 40). Muy relacionada con el cine neorrealista italiano, retrata la urbe cambiante, la aparición de personajes marginales y problemáticos. Entre los narradores más representativos resaltan Ribeyro con "Los gallinazos sin plumas" (1955); Enrique Congrains con las novelas "Lima, hora cero" (1954) y "No una, sino muchas muertes" (1957); Luis Loayza, cuya obra es obra es breve y poco conocida; y Vargas Llosa, quien a fines de la década del 50 empezó a publicar sus cuentos, aunque sus magistrales novelas aparecerán a partir de la década de 1960.

Junto a los narradores, surge un grupo de poetas entre los que se destacan Alejandro Romualdo, Washington Delgado, Carlos Germán Belli, Francisco Bendezú, Juan Gonzalo Rose, Pablo Guevara. Estos poetas comenzaron a publicar su obra a partir de fines del 40, tal es el caso de Romualdo, luego lo harían Rose, Delgado, Bendezú, Belli. Guevara. Además, a este grupo lo unían no solo las relaciones personales, sino también la ideología, el marxismo y el existencialismo. Los poemas que escribieron adoptaron, desde una visión general, un tono protestatario y de compromiso social. Por ello, se reconoce al poema "A otra cosa" de Romualdo en el arte poética de la generación del cincuenta.

Esta generación reivindicó a César Vallejo como paradigma estético y asumió el pensamiento de José Carlos Mariátegui en calidad de guía intelectual. Los poetas Javier Sologuren, Sebastián Salazar Bondy, Jorge Eduardo Eielson, Antenor Samaniego, Blanca Varela, fueron conocidos como el grupo "neovanguardista", que comenzó a publicar a fines de los años treinta (tal es el caso de Sologuren, luego vendrían los poemas de Salazar Bondy, Samaniego, Eielson, Varela). Mantuvieron relaciones personales en la revista "Mar del Sur", dirigida por Aurelio Miró Quesada, de clara tendencia conservadora; y designaron a Emilio Adolfo Westphalen como guía poético. A esta situación histórico - literaria, habría que añadir el grupo de los llamados "Poetas del pueblo", vinculados al partido aprista fundado por Victor Raúl Haya de la Torre, integrado por Gustavo Valcárcel, Manuel Scorza, Mario Florián, Ignacio Campos, Ricardo Tello, Julio Garrido Malaver, quienes reivindicaron como paradigma poético a Vallejo. 

Durante ese decenio y el siguiente el teatro experimenta un período de renovación, inicialmente con las piezas de Salazar Bondy (generalmente comedias de contenido social) y más tarde con Juan Rivera Saavedra, con obras de fuerte denuncia social, influidas por el expresionismo y el teatro del absurdo. Durante estos años se dejará sentir con fuerza la influencia de Bertolt Brecht entre los dramaturgos.

La Generación del 60 en poesía tuvo a representantes del calibre de Luis Hernández, Javier Heraud y Antonio Cisneros, Premio Casa de las Américas. Merecen citarse también César Calvo, Rodolfo Hinostroza y Marco Martos. Cabe señalar que Heraud fue el verdadero paradigma generacional, vinculado a la doctrina marxista y a la militancia política, mientras que Hernández y Cisneros, no. Como es fácil advertir, los coetáneos no constituyen movimiento generacional. 

A esta generación pertenecen los narradores Oswaldo Reynoso, Miguel Gutiérrez, Eduardo González Viaña, Jorge Díaz Herrera, Alfredo Bryce Echenique y Edgardo Rivera Martínez. 
La narrativa y la poesía peruanas de fines de la década de 1960 no tuvieron tanto un carácter generacional como ideológico: la literatura era vista como un medio, un instrumento para crear una conciencia de clase. Eran los años del auge de la revolución en Cuba y en el Perú la mayoría de intelectuales ansiaban una revolución marxista que rompiera el viejo orden oligárquico y feudal. Algunos escritores aspiraban a un proceso como el cubano (Heraud, por ejemplo, murió en mayo de 1963 en la selva peruana, integrando una columna que pensaba lanzar la lucha guerrillera), mientras que otros tenían sus propios modelos. En este periodo de intenso compromiso social al escritor le queda poco espacio para el compromiso con su propia obra. A fines de esta década surge el "Grupo Narración", influido por el maoísmo y liderado por Miguel Gutiérrez y Oswaldo Reynoso, sumándose también Antonio Galvéz Ronceros y Augusto Higa, quienes editaron una revista con el mismo nombre, aunque tenían pensando llamarla "Agua", evocando a José María Arguedas y las tensiones sociales que muestra el libro de ese título.

Las primeras expresiones con características propias, de lo que se denominaría después Generación del 70, surgieron a fines de los años 60 con autores como Manuel Morales (1943-2007), autor de la plaqueta "Peicen Bool" (1968) y "Poemas de entrecasa" (1969); y Abelardo Sánchez León ("Poemas y ventanas cerradas", 1969) que experimentaron con el coloquialismo popular. 

Una de las primeras revistas que acogerá a las nuevas voces será "Estación Reunida", en la que publican José Rosas Ribeyro, Patrick Rosas, Elqui Burgos, Tulio Mora, Óscar Málaga y otros. En 1963 irrumpió al escenario poético el movimiento de ruptura Gleba Literaria en los claustros de letras de la Universidad Federico Villarreal, siendo una voz contestataria del momento político que vivía el país, teniendo como fundador a Jorge o. Vega y albergando a otros poetas insurgentes. Pero será con la aparición del movimiento Hora Zero y su revista homónima, en 1970, que esta generación sentará presencia en la escena cultural peruana. Lo fundaron Juan Ramírez Ruiz y Jorge Pimentel, estudiantes de la Universidad Nacional Federico Villarreal, y a sus filas también pertenecieron Enrique Verástegui, Carmen Ollé, Jorge Nájar, Mario Luna y Feliciano Mejía. Este último se alejaría definitivamente de Hora Zero en 1972.

Los primeros escritores galardonados con el importante premio Poeta Joven del Perú fueron José Watanabe (1945-2007), ("Álbum de familia") y Antonio Cillóniz ("Después de caminar cierto tiempo hacia el Este"), que lo compartieron en 1970. 

Además del coloquialismo popular como expresión poética, a la Generación del 70 también le caracterizará por su ruptura con la tradición literaria peruana anterior a ella y su radicalismo ideológico de izquierda, como prueba de lo citado, se halla la ratificación por mayoría generacional a tal compromiso literario, en el Congreso de Poetas celebrado en la ciudad de Jauja en abril de 1970.
Otra expresión importante de esta generación es el surgimiento de los "poetas mágicos", neovanguardistas que retoman los experimentos dadaístas con César Toro Montalvo, Omar Aramayo, José Luis Ayala. La poesía de protesta social tendrá un destacado cultor en Cesareo Martínez. Fuera de los grupos destacan otras voces como la de Vladimir Herrera.
A partir de 1974 se produce un segundo momento en la Generación del 70 que se expresará en las páginas de revistas de muy limitada circulación como "La Tortuga Ecuestre", "Cronopios", "Literatura", "Auki", "Tallo de Habas" y algunas otras. Sus poetas, en alguna forma, tratan de tomar cierta distancia del coloquialismo característico de la primera etapa y se entregan más al cuidadoso cultivo de la forma. En este segundo momento aparecen, entre otras, las voces de Mario Montalbetti, Juan Carlos Lázaro, Carlos López Degregori, Luis La Hoz, Enrique Sánchez Hernani, Bernardo Rafael Álvarez, Armando Arteaga, Alfonso Cisneros Cox, Jorge Luis Roncal, Gustavo Armijos. 

De otro lado, con la publicación póstuma de un puñado de poemas de María Emilia Cornejo en la revista "Eros", la poesía escrita por mujeres en el Perú inaugura un nuevo lenguaje, una nueva expresión de la problemática femenina. Destacarán la ya citada Carmen Ollé, Sonia Luz Carrillo, Rosina Valcárcel, Rosa Natalia Carbonell, entre otras.

Si bien la del 70 fue una generación fundamentalmente poética, no estuvo exenta de narradores. En los años iniciales de agitación literaria, al influjo de las modas importandas de la contracultura y los hippies, su narrador más visible fue Fernando Ampuero, quien con el tiempo desarrollará una importante y sostenida obra cuentística, novelística y periodística. Con menos atención de los medios, pero con obras no menos importantes, a esta generación también pertenecen los narradores Óscar Colchado, Cronwell Jara, Maynor Freyre, Zein Zorrilla, Luis Nieto Degregori, Enrique Rosas Paravicino.

En el teatro hace irrupción la creación colectiva frente a las obras de autor. El movimiento fue liderado por varios grupos teatrales surgidos en estos años, entre los que descollan Cuatrotablas, encabezado por Mario Delgado, y Yuyachkani, por Miguel Rubio Zapata, ambos creados en 1971.

Merece destacarse la labor poética y la perseverancia, desde las provincias, de Alberto Alarcón, Houdini Guerrero, Emilio Saldarriaga, Segundo Cansino, Carmen Arrese, entre otros. En Arequipa, las revistas "Ómnibus" y "Macho Cabrío" marcaron una época. El grupo de poetas vinculado a la Universidad San Agustín (Oswaldo Chanove, Alonso Ruiz Rosas, entre otros) fue muy activo.

Con la década de 1980 viene el desencanto, el pesimismo: la llegada de una revolución comunista deja de ser una utopía, pero ya no se la espera con ilusión, es casi una amenaza. Es tiempo de la "perestroika" y los últimos años de la guerra fría. Además, la crisis económica, la violencia terrorista y el deterioro de las condiciones de vida en una Lima caótica y superpoblada contribuyeron al desánimo colectivo. En narrativa aparecen los primeros libros de cuentos de Alfredo Pita, "Y de pronto anochece"; de Guillermo Niño de Guzmán, "Caballos de medianoche"; y de Alonso Cueto, "Las batallas del pasado," autores estos cuya obra literaria se desarrollará plenamente en años posteriores. Asimismo, en los ochenta, aparecen las dos primeras novelas de Aída Balta Campbell: "Sodoma Santos y Gomorra" y "El legado de Caín". En 1990 aparece en España y con escasa circulación en el Perú un libro de cuentos de Pita que lleva un título negro como la década que se cerraba en su país: "Morituri".

En poesía, surgen movimientos marginales, que ahondan la vertiente rebelde de la década anterior, como el "Kloaka", liderado por Roger Santiváñez. Fundado hacia el final de 1982, editó una "autoantología "con motivo de su disolución: "La última cena "(1987). En contraste con las propuestas colectivas de aliento neovanguardistas (en general, de ruptura con el sistema político y el estético), surgen individualidades notables vinculadas en su orígenes con estos, pero que rápidamente transitan a una poesía serena, de ritmos equilibrados y que se nutre de tradiciones artísticas fuertemente codificadas. El caso más notable es el de José Watanabe, cuya mejor obra corresponde a este decenio y que será revalorada en el nuevo siglo. Otros poetas notables dentro de esta apuesta individualizadora de vertiente tradicional fueron Eduardo Chirinos y Magdalena Chocano. En el mismo decenio afloran también los primeros y diversificados movimientos de poesía de mujeres. Están la línea feminista, dentro de la cual se destacan Carmen Ollé, Giovanna Pollarollo y Rocío Silva Santisteban, y otra más lírica, donde sobresale Rossella Di Paolo, además del intimismo irónico de Milka Rabasa. Cabe mencionar también a Patricia Alba, Mary Soto, Mariela Dreyfus y Dalmacia Ruiz-Rosas. 

En la década de 1990, aparece una tendencia individualista que ahonda en la intención estética. En poesía donde surgen dos grupos importantes: Noble Katerba y Neón. En la narrativa, la fórmula que se impone es la denominada "joven-urbano-marginal". En este campo, además de Jaime Bayly, que tiene preferencia por lo sensacionalista, sobresalen Óscar Malca con "Al final de la calle" (1993), Sergio Galarza con "Matacabros" (1996), Rilo con "Contraeltráfico" (1997), autores que cultivan el realismo sucio. 

Por otra parte, aparecen algunos escritores que cultivan el esteticismo y cuya obra escapa a los moldes de su generación, entre ellos Iván Thays, con "Las fotografías de Francés Farmer", y Patricia De Souza, con "Cuando llegue la noche". En poesía destacan Montserrat Álvarez con "Zona dark" (1991), Xavier Echarri con "Las quebradas experiencias"(1993), Domingo de Ramos con "Ósmosis" (1996), Doris Moromisato, Odi González, Ana Varela, Leoncio Luque, Rodrigo Quijano, Jorge Frisancho, Ericka Ghersi con "Zenobia y el Anciano" (1994), Rafael Espinosa, entre otros antologados en la polémica antología "Poesía peruana siglo XX" (2000) de Ricardo González Vigil (Universidad Católica). 

Hacia el 2000, como señala Vigil en el tomo 14, "Literatura", de la a "Enciclopedia Temática del Perú" de "El Comercio", muestran un trabajo poético importante Lorenzo Helguero, Miguel Ildefonso, Selenco Vega, José Carlos Yrigoyen, Alberto Valdivia Baselli, Rubén Quiroz, entre otros. En el campo dramático descollan Enrique Mávila y Mariana de Althaus, que se han caracterizado por la asimilación de diferentes tendencias teatrales contemporáneas. Y en el campo de la narrativa breve es singular la obra "Fábulas y antifábulas", de César Silva Santisteban.

Simultáneamente, dos escritores del grupo "Narración" alcanzan su madurez durante este decenio: Oswaldo Reynoso y Miguel Gutiérrez, quienes regresan al Perú luego de una larga estadía en la China comunista, que los desengaña de sus aventuras políticas juveniles. Reynoso, autor del memorable libro de cuentos "Los inocentes", pública sucesivamente la nouvelle "En busca de Aladino" y la novela "Los eunucos inmortales", obras de prosa musical en las que se descarta el ideal de la lucha social de clase por la búsqueda de una utopía de belleza juvenil que resulte, no obstante, justiciera con los humildes. Gutiérrez, por su lado, sorprende a los lectores con una novela de más de mil páginas, "La violencia del tiempo", saga familiar de la familia Villar, que se inicia con el primer Villar, desertor del ejército español que combatió contra los patriotas en la guerra de independencia, y termina con Martín Villar, narrador de la novela, que en los años sesenta ha optado por ser un profesor rural, tras estudiar en la oligárquica Universidad Católica. Novela histórica, de crecimiento, ensayo de crítica social y de interpretación histórica, "La violencia del tiempo" acusa el influjo de los grandes narradores latinoamericanos del siglo XX (Jorge Luis Borges, Juan Rulfo, Gabriel García Márquez y Mario Vargas Llosa), así como de los maestros de la novela del siglo XIX, en especial de Balzac, cuyo intenso y torvo cronicón de familia, "La comedia humana", evoca con maestría singular.

Con el cambio de siglo y en los primeros años de la década varios de los premios internacionales más importantes son entregados a escritores peruanos, algunos de ellos desconocidos hasta ese momento en el extranjero. A partir de ello, se plantea la posibilidad de un relanzamiento internacional de nuestras letras, las que habían menguado en presencia exterior durante las dos últimas décadas del siglo XX. De hecho, este repunte de las letras peruanas empieza en 1999, cuando la novela "El cazador ausente", de Alfredo Pita, gana el premio "Las dos orillas", concedido por el Salón del Libro Iberoamericano de Gijón (España). El libro de Pita fue de inmediato traducido y publicado en cinco países europeos. Tres años después, en 2002, un narrador ya consagrado, Alfredo Bryce Echenique, obtiene el Planeta con "El huerto de mi amada", otorgado por la editorial homónima, la más poderosa de España y una de las mayores del mundo. El año siguiente, "Pudor", segunda novela de Santiago Roncagliolo, queda entre las cuatro finalistas del Herralde y es luego publicada por Alfaguara en 2004 con una audaz operación de márketing. En 2005, Jaime Bayly, criticado por sus detractores por emplear la narrativa como complemento de su celebridad televisiva, es único finalista del Planeta. Ese mismo año Alonso Cueto logra el Herralde con "La hora azul "; al siguiente Roncagliolo, con "Abril rojo", obtiene el premio de novela otorgado por su casa editora y al subsiguiente la novela "El susurro de la mujer ballena", de Cueto, queda finalista en la primera edición del Premio Planeta-Casa de América. Iván Thays, que ya había sido finalista del Rómulo Gallegos 2001, queda entre los finalistas del Herralde 2008 con "Un lugar llamado Oreja de Perro". El escritor peruano-estadounidense Daniel Alarcón fue considerado uno de los escritores más importante de la última generación en la literatura estadounidense, en tanto Carlos Yushimito y Roncagliolo fueron considerados entre los 22 escritores menores de 35 más importantes en español. Finalmente, el Nobel de Literatura es entregado a Vargas Llosa en año 2010. En esta secuencia de acontecimientos puede, ciertamente, rastrearse la incorporación de numerosa literatura peruana al flujo de la circulación de las letras españolas en el mundo globalizado.

Mientras algunos en el Perú se congratulan de este fenónomeno, otros lo critican argumentando que la internacionalización de estos escritores y su premiación debe entenderse no por criterios estrictamente literarios sino por la ampliación del mercado literario internacional en español dentro de ciertos parámetros que estimula el consumo de productos muy reconocibles. Desde esta perspectiva, las trasnacionales de la literatura, que en los primeros años del siglo XXI asientan sus filiales en Lima, estarían exigiendo a los escritores mejor conectados con el mercado editorial una mayor profesionalización, pero orientada a satisfacer los estándares de una producción de formatos transnacionales preestablecidos, que se riñen con la originalidad. En este nuevo perfil profesional se pueden entender las novela de Jeremías Gamboa, "Contarlo todo", y Renato Cisneros, "La distancia que nos separa". También novelas últimas relacionadas con los temas de la crítica a la corrupción post conflicto armado como "La procesión infinita" de Diego Trelles Paz y "La sinfonía de la destrucción" de Pedro Novoa así como obras de corte fantástico como "El fuego de las multitudes" de Alexis Iparraguirre

En paralelo al resurgimiento internacional y al reconocimiento de autores como los mencionados, en Perú en los últimos años también se desarrolla, como parte de la dinámica propia de un país multicultural, un proceso literario protagonizado por autores que sitúan su obra en los linderos de la cultura andina, rescatándola como forma artística producto de la especificidad de la nación peruana y su drama. Los escritores de esta tendencia reclaman, por un lado, la herencia de la obra de José María Arguedas y, por otro, denuncian la discriminación por parte de críticos y medios de comunicación de orientación "criolla", o culturalmente más afines con el sistema económico globalizado, que rige la administración de los llamados "bienes culturales". La disputa entre "andinos" y criollos se hizo patente a raíz de una serie de artículos agresivos publicados por ambos bandos luego de una primera descalificación mutua cuando se vieron las caras en un congreso de escritores peruanos en Madrid. Como consecuencia de la disputa pública, ganó visibilidad una nueva generación de escritores provincianos que continúa, en clave contemporánea e incluso posmoderna, la narrativa indigenista (y regionalista) de los años 40 (en particular surgen lazos con Alegría y Arguedas), con la obra de Manuel Scorza y con la narrativa regionalista y de ruptura de los años 70 (Eleodoro Vargas Vicuña, Carlos Eduardo Zavaleta, Edgardo Rivera Martínez, el grupo "Narración"). Se privilegia una reconstrucción del pasado a través de un proceso de ficcionalización de la historia, retomando un punto explotado por la nueva narrativa hispanoamericana y el boom. Así, si no son los primeros, son los que más ahondan en el tratamiento literario del proceso de la guerra interna (1980-1993). Un libro que ha contado con el elogio merecido de la crítica ha sido "Retablo" de Julián Pérez. La inserción en el mercado literario nacional de estos escritores es, además, distinta a los narradores capitalinos, ya que la difusión de sus obras se realiza principalmente en provincias y a través de formas alternativas (ferias regionales, conciertos folclóricos, periódicos o revistas de tiraje limitado). Fuertemente marcados por la oralidad y tradiciones andinas, los nombres más conocidos, además de Óscar Colchado, son Dante Castro Arrasco, Félix Huamán Cabrera y Zein Zorrilla.

Es importante señalar, asimismo, el significativo crecimiento que ha experimentado el mercado editorial peruano en la primera década del siglo XXI, debido a la reducción de costos que ha significado la introducción de tecnología digital en el ámbito editorial, la vigencia de la Ley del Libro y el impulso del Plan Lector de Ministerio de Educación. Por un lado, han aparecido diversas editoriales independientes como Estruendomudo, Matalamanga, Atalaya Editores, Sarita Cartonera, Bizarro, Borrador Editores, [sic] libros, Mundo Ajeno, Tranvías, Lustra, Mesa Redonda, Casatomada, Editorial Arkabas, Gaviota Azul Editores, entre otras. Estas casas impulsaron la creación de la Alianza Peruana de Editores, gremio independiente afiliado a un movimiento global por la defensa de la bibliodiversidad. Entre las nuevas editoriales Estruendomudo, en especial, es responsable de la aparición y difusión de nuevos narradores elogiados por la crítica. Por el otro, uno de los mayores grupos del mundo de habla hispana, Planeta, inauguró en 2006 su filial en el Perú, dando un ulterior impulso a un mercado en el que ya operaban otros dos grandes grupos internacionales: Santillana (España) y Norma (Colombia); desgraciadamente, este último abandonó la ficción. Este pequeño "boom" editorial ha permitido que un número elevado de escritores nuevos publique sus primeros trabajos durante esta década, especialmente escritores jóvenes nacidos en la década de los 70's.





</doc>
<doc id="9774" url="https://es.wikipedia.org/wiki?curid=9774" title="Lluvia ácida">
Lluvia ácida

Se llama lluvia ácida a la que se forma cuando la humedad del aire se combina con óxidos de nitrógeno, dióxido de azufre o trióxido de azufre emitidos por fábricas, centrales eléctricas, calderas de calefacción y vehículos que queman carbón o productos derivados del petróleo que contengan azufre. En interacción con el agua de la lluvia, estos gases forman ácido nítrico, ácido sulfuroso y ácido sulfúrico. Finalmente, estas sustancias químicas caen a la tierra acompañando a las precipitaciones, constituyendo la lluvia ácida.

Los contaminantes atmosféricos primarios que dan origen a la lluvia ácida pueden recorrer grandes distancias, siendo trasladados por el viento a cientos o miles de kilómetros antes de precipitar en forma de rocío, lluvia, llovizna, granizo, nieve, niebla o neblina. Cuando la precipitación se produce, puede provocar deterioro en el medio ambiente.

La lluvia normalmente presenta un pH de aproximadamente 5,65 (ligeramente ácido), debido a la presencia del CO atmosférico, que forma ácido carbónico, HCO. Se considera lluvia ácida si presenta un pH menor que 5 y puede alcanzar el pH del vinagre (pH 3), valores que se alcanzan cuando en el aire hay uno o más de los gases citados.


Otra fuente de dióxido de azufre son las calderas de calefacción domésticas que usan combustibles que contiene azufre (ciertos tipos de carbón o gasóleo). 


Una de las fuentes más importantes es a partir de las reacciones producidas en los motores térmicos de los automóviles y aviones, donde se alcanzan temperaturas muy altas. Este NO se oxida con el dioxígeno atmosférico, 

y este NO reacciona con el agua dando ácido nítrico (HNO), que se disuelve en el agua.

Para evitar esta producción se usan en los automóviles con motor de gasolina los catalizadores, que disocian el óxido de nitrógeno antes de emitirlo a la atmósfera. Los vehículos con motor diésel no pueden llevar catalizadores y por lo tanto, en este momento son los únicos que producen este gas.

La acidificación de las aguas de lagos, ríos y mares dificulta el desarrollo de vida acuática, lo que aumenta en gran medida la mortalidad de peces. Igualmente, afecta directamente a la vegetación, por lo que produce daños importantes en las zonas forestales, y acaba con los microorganismos fijadores de nitrógeno.

El término "lluvia ácida" abarca la sedimentación tanto húmeda como seca de contaminantes ácidos que pueden producir el deterioro de la superficie de los materiales. Estos contaminantes que escapan a la atmósfera al quemar carbón y otros componentes fósiles reaccionan con el agua y los oxidantes de la atmósfera y se transforman químicamente en ácidos sulfúrico y nítrico. Los compuestos ácidos se precipitan, entonces, caen a la tierra en forma de lluvia, nieve o niebla, o pueden unirse a partículas secas y caer en forma de sedimentación seca.

La lluvia ácida, por su carácter corrosivo, corroe las construcciones y las infraestructuras. Puede disolver, por ejemplo, el carbonato de calcio, CaCO, y afectar de esta forma a los monumentos y edificaciones construidas con mármol o caliza.
Un efecto indirecto muy importante es que los protones, H, procedentes de la lluvia ácida, arrastran ciertos iones del suelo. Por ejemplo, cationes de hierro, calcio, aluminio, plomo o zinc. Como consecuencia, se produce un empobrecimiento en ciertos nutrientes esenciales y el denominado "estrés en las plantas", que las hace más vulnerables a las plagas.

Los nitratos y sulfatos, sumados a los cationes lixiviados de los suelos, contribuyen a la eutrofización de ríos, lagos, embalses y regiones costeras, lo que deteriora sus condiciones ambientales naturales y afecta negativamente a su aprovechamiento.

Un estudio realizado en 2005 por Vincent Gauci de "Open University", sugiere que cantidades relativamente pequeñas de sulfato presentes en la lluvia ácida tienen una fuerte influencia en la reducción de gas metano producido por metanógenos en áreas pantanosas, lo cual podría tener un impacto, aunque sea leve, en el efecto invernadero.

Entre las medidas que se pueden tomar para reducir las emisiones de los agentes contaminantes de este problema , contamos con las siguientes:




</doc>
<doc id="9776" url="https://es.wikipedia.org/wiki?curid=9776" title="Departamento de Madre de Dios">
Departamento de Madre de Dios

Madre de Dios (en quechua: "Amaru Suyu") es uno de los veinticuatro departamentos que, junto a la Provincia Constitucional del Callao, forman la República del Perú. Su capital y ciudad más poblada es Puerto Maldonado.

Está ubicado al sureste del país, en la Amazonía, limitando al norte con Ucayali y Brasil, al este con Bolivia, al sur con Puno y al oeste con Cuzco. Con 85 300 km² es el tercer departamento más extenso —por detrás de Loreto y Ucayali— y con 1,3 hab/km², el menos densamente poblado. Fue creado el a partir de territorios de Puno y Cuzco. 

Recibe su nombre del río Madre de Dios, de cuya cuenca son tributarios la mayor parte de los ríos de la región y sobre cuyas riberas se erige la capital departamental: Puerto Maldonado.

Desde el punto de vista jerárquico de la Iglesia católica, forma parte del Vicariato Apostólico de Puerto Maldonado

Los petroglifos en el río Shinkebeni (Petroglifos de Pusharo), indican una muy antigua presencia de seres humanos. Se cree que los Arahuacos (o sus antecesores) llegaron en migraciones, y de ellos se derivaron muchas etnias. Algunas tribus, como la machiguenga, sobreviven hasta nuestros días. Lo que hoy se conoce como Madre de Dios, formaba parte del antiguo Imperio inca, en la región conocida como Antisuyo. Los historiadores coinciden que la conquista de esta región fue difícil para los Incas, pues debieron enfrentar a tribus aguerridas y conocedoras de la zona. En la zona de la cuenca del río Nistron se encuentran también las ruinas de Mameria, asentamiento Inca, que fue descubierto en 1979.

Durante la colonia ingresaron expediciones españolas con resultados trágicos para los europeos. Al fin, en 1861, el coronel Faustino Maldonado exploró todo el territorio y en 1890; Carlos Fermín Fitzcarrald descubrió un istmo que unía las cuencas de los ríos Ucayali y Madre de Dios. Durante las siguientes décadas numerosos aventureros y comerciantes explotaron los bosques, ávidos de caucho y oro. A partir de 1915, ante la persistencia de los misioneros dominicos, las tribus locales empezaron a aceptar la civilización. Esto no siempre ha sido bueno para ellas, ya que tribus como los harakmbet han abandonado muchos de sus instrumentos tradicionales dependiendo en gran medida de los habitantes de la ciudad. Sin embargo, aún hoy existen grupos en total aislamiento físico y cultural.

Durante el presente siglo, ha existido una fuerte presión sobre los ecosistemas por la explotación de los recursos naturales. En especial por parte de la minería de oro (en su mayoría ilegal).

Departamento íntegramente selvático; tiene zonas de selva alta, selva baja y la sabana de palmeras. Limita al norte con Ucayali y Brasil; al este con Brasil y Bolivia; al oeste con Cuzco; al sur con Cuzco y Puno. Su capital Puerto Maldonado, está en la confluencia del río Madre de Dios y el río Tambopata. Su geografía es de las más difíciles para alguna construcción de carreteras, pues los Andes se precipitan hacia la selva formando abruptas laderas.

El clima es tropical, cálido, húmedo, con precipitaciones anuales superiores a 1000 mm. La temperatura media anual en la capital es de 26 °C con una máxima de 38 °C en agosto y septiembre, en algunas ocasiones puede llegar a los 40 °C y una mínima de 21 °C, con lluvias de diciembre a marzo. En años excepcionales el territorio es invadido por masas de aire frío provenientes del sur durante los meses de julio y agosto, ocasionando descensos excepcionales de la temperatura hasta 8 °C en fenómenos denominados como friajes.

Madre de Dios alberga algunas de las regiones de mayor biodiversidad del mundo. Por ejemplo, el parque nacional del Manu tiene el récord en número de especies de anfibios y reptiles.

Tiene una enorme diversidad de Aves, tiene a las grandes especies de felinos sudamericanos (jaguar, tigrillo, puma), múltiples especies de lagartos, y otros reptiles como reptiles como la boa constrictora y la shushupe. También son abundantes y diversos los monos, los peces, los insectos y en general el conjunto de animales.

Su flora es también muy rica, y entre las que se encuentran especies de madera noble y alto interés.
Al igual podemos encontrar muchas especies como el "motelo sanango."

Este Departamento tiene una extensión de 85 300.54 km² con una población de 140 508 habitantes, cuenta con 3 provincias y 11 distritos:

Tiene una población de 109.555 habitantes que representa el 0,5% de la población nacional. 

Las principales comunidades nativas son: amarakaeri, arasaeri, kishambaeri, pukirieri, sapiteri, toyoeri, wachipaeri, arawak, machiguenga y mashko-piros.

Concentra el 0,5% de la población económicamente activa - [PEA] y tiene un aporte económico de 0,4% al [PBI] nacional. Durante el 2007 registró un crecimiento económico de 11,7% respecto al 2006 y en el periodo 2004-2007 la tasa de crecimiento anual fue 8,5%.

Sus riquezas naturales son abundantes: ricas maderas, frutos silvestres, metales preciosos, petróleo, hacen la fama del territorio tanto como su belleza. En los alrededores de Tambopata están los mayores centros de producción agropecuaria, que aún trabajan a pequeña escala.

Las cosechas de café, arroz, castañas, y la producción de pan llevar son esenciales, lo mismo que la crianza de vacunos y cerdos. Por otro lado, existen pequeñas industrias de agua gaseosa, jabón y triplay. Hay una central térmica: la de Puerto Maldonado.

En agricultura destaca el arroz, caucho, maíz, yuca, plátano y coco. La madera es también fuente de ingresos, donde destacan la caoba y el cedro. Se practica la caza y la ganadería vacuna. Además, los lavaderos de oro en sus ríos le proporcionan importancia en la minería.

La actividad minera es la principal actividad económica de Madre de Dios. Esta actividad representa el 41% del PBI regional. La región representa 11% de la producción de oro peruano.

El Ministerio de Energía y Minas del Perú informó el 29 de agosto de 2004 que exploraciones realizadas en esta región ubicada en la selva sur oriental de este país, permiten proyectar la existencia de gas con estimados de 32 trillones de pies cúbicos, lo que representaría un potencial de 960 millones de barriles de gas natural líquido.




Madre de Dios ofrece una gran diversidad de flora y fauna en las diferentes áreas que se encuentran en él, como la Reserva Nacional Tambopata, en la confluencia de los ríos Torre y Tambopata, en donde la riqueza y variedad de aves es asombrosa; la reserva de Biósfera del Manu, compuesta del Parque Nacional del Manu donde solo se permite investigación antropológica y biológica, observación de vida y los procesos ecológicos en forma natural; la Zona reservada en la cual están permitidas las actividades turística e investigación con mínima manipulación y el Bajo Manu, zona en la que predominan las poblaciones de colonos que desarrollan actividades agrícolas, pecuarias y forestales, además se permite la realización de actividades económicas. Destacan el lago Nisisipi y Sandoval, las colpas de guacamayos de Colorado y Pariamanu, etc.

Son famosas sus puestas de sol, debido al reflejo de los nevados de la cadena del Cusco. Este fenómeno atmosférico se denomina "Rayo Blanco". Por otra parte, se encuentra la Reserva natural de Tambopata-Candamo, en la confluencia de los ríos La Torre y Tambopata, con una extensión de 5500 hectáreas de selva virgen, en donde la riqueza y variedad de aves alcanza a 600 especies, 900 especies de mariposas y 115 especies de libélulas.

En el Santuario Nacional de las Pampas del Heath vive el rarísimo lobo de crin. Todo este ecosistema tuvo una evolución inalterable durante miles de años, originando una de las mayores variedades de flora y fauna del mundo.

Para el año 2008, la oferta hotelera es de 2106 habitaciones, registrando 134 771 visitantes nacionales que efectuaron 235 892 pernoctaciones.


Como todos los otros departamentos del Perú y la Provincia Constitucional del Callao, constituye una región "de facto" con un Gobierno Regional propio además de un distrito electoral que elige dos congresistas.

De la religión católica:

La Segunda Religión más popular y profesada es la evangélica, teniendo cerca de 25 Iglesias en torno a la ciudad de Puerto Maldonado, capital del Departamento y aproximadamente 60 Iglesias en torno a la carretera Interoceanica Puerto - Iñapari y Puerto Mazuko.




</doc>
<doc id="9777" url="https://es.wikipedia.org/wiki?curid=9777" title="Caliza">
Caliza

La caliza es una roca sedimentaria compuesta mayoritariamente por carbonato de calcio (CaCO), generalmente calcita, aunque frecuentemente presenta trazas de magnesita (MgCO) y otros carbonatos. También puede contener pequeñas cantidades de minerales como arcilla, hematita, siderita, cuarzo, etc., que modifican (a veces sensiblemente) el color y el grado de coherencia de la roca. El carácter prácticamente monomineral de las calizas permite reconocerlas fácilmente gracias a dos características físicas y químicas fundamentales de la calcita: es menos dura que el cobre (su dureza en la escala de Mohs es de 3) y reacciona con efervescencia en presencia de ácidos tales como el ácido clorhídrico. 

En el ámbito de las rocas industriales o de áridos para construcción recibe también el nombre de piedra caliza. Junto a las dolomías y las margas, las calizas forman parte de lo que se conocen como rocas calcáreas.

Si se calcina (se lleva a alta temperatura), la caliza da lugar a cal (óxido de calcio impuro, CaO).

Son muy características por su color claro, blanquecino o gris. Las calizas se forman en los mares cálidos y poco profundos de las regiones tropicales, en aquellas zonas en las que los aportes detríticos son poco importantes. Dos procesos, que generalmente actúan conjuntamente, contribuyen a la formación de las calizas:

El carbonato de calcio (CaCO) se disuelve con mucha facilidad en aguas que contienen dióxido de carbono (CO) gaseoso disuelto, debido a que reacciona con este y agua para formar bicarbonato de calcio [Ca(HCO)], compuesto intermedio de alta solubilidad. Sin embargo en entornos en el que el CO disuelto se libera bruscamente a la atmósfera, se produce la reacción inversa aumentando la concentración de carbonato de calcio (véase ley de acción de masas), cuyo exceso sobre el nivel de saturación precipita. De acuerdo a lo descrito, el equilibrio químico en solución sigue la siguiente ecuación:

Esa liberación de CO se produce, fundamentalmente, en dos tipos de entornos: en el litoral cuando llegan a la superficie aguas cargadas de CO y, sobre los continentes, cuando las aguas subterráneas alcanzan la superficie.
Este es el proceso fundamental de formación de grutas y cuevas con presencia de estalactitas y estalagmitas en muchas regiones calcáreas con piedras calizas denominadas también karsts, carsts o carsos. Estas últimas denominaciones de las regiones calcáreas provienen del nombre de la región eslovena de Carso, rica en estos minerales y paisajes.

Numerosos organismos utilizan el carbonato de calcio para construir su esqueleto mineral, debido a que se trata de un compuesto abundante y muchas veces casi a saturación en las aguas superficiales de los océanos y lagos (siendo, por ello, relativamente fácil inducir su precipitación). Tras la muerte de esos organismos, se produce en muchos entornos la acumulación de esos restos minerales en cantidades tales que llegan a constituir sedimentos que son el origen de la gran mayoría de las calizas existentes.

Actualmente limitada a unas cuantas regiones de las mareas tropicales, la sedimentación calcárea fue mucho más importante en otras épocas. Las calizas que se pueden observar sobre los continentes se formaron en épocas caracterizadas por tener un clima mucho más cálido que el actual, cuando no había hielo en los polos y el nivel del mar era mucho más elevado. Amplias zonas de los continentes estaban en aquel entonces cubiertas por mares epicontinentales poco profundos. En la actualidad, son relativamente pocas las plataformas carbonatadas [marcada con el (1) en la imagen superior], desempeñando los arrecifes (2) un papel importante en la fijación del carbonato de calcio marino.

La caliza, cortada, tallada o desbastada, se utiliza como material de construcción u ornamental, en forma de sillares o placas de recubrimiento. Ejemplos de este uso son numerosos edificios históricos, desde las pirámides de Egipto hasta la Catedral de Burgos. Machacada se usa como árido de construcción.

Es un componente importante del cemento gris usado en las construcciones modernas y también puede ser usada como componente principal, junto con áridos, para fabricar el antiguo mortero de cal, pasta grasa para creación de estucos o lechadas para «enjalbegar» (pintar) superficies, así como otros muchos usos por ejemplo en industria farmacéutica o peletera.
Es una roca importante como reservorio de petróleo, dada su gran porosidad. Tiene una gran resistencia a la meteorización; esto ha permitido que muchas esculturas y edificios de la antigüedad tallados en caliza hayan llegado hasta la actualidad. Sin embargo, la acción del agua de lluvia y de los ríos (especialmente cuando se encuentra acidulada por el ácido carbónico) provoca su disolución, creando un tipo de meteorización característica denominada kárstica. No obstante es utilizada en la construcción de enrocamientos para obras marítimas y portuarias como rompeolas, espigones, escolleras entre otras estructuras de estabilización y protección.

La caliza se encuentra dentro de la clasificación de recursos naturales entre los recursos no renovables (minerales) y dentro de esta clasificación, en los no metálicos, como el salitre, el aljez y el azufre.



</doc>
<doc id="9778" url="https://es.wikipedia.org/wiki?curid=9778" title="Puerto Maldonado">
Puerto Maldonado

Puerto Maldonado es una ciudad del sureste del Perú, capital del Departamento de Madre de Dios, situada a orillas de la confluencia del río Madre de Dios y el río Tambopata. Es uno de los principales núcleos comerciales de la Amazonia. Lleva el título oficial de "Capital de la Biodiversidad del Perú" en mérito a importantes registros de flora y fauna encontrados en los bosques del departamento. Desde el punto de vista jerárquico de la Iglesia Católica es sede del Vicariato Apostólico de Puerto Maldonado. Cuenta con una población de 74 494 habitantes para el año 2015.

La población nativa de Madre de Dios debió aparecer hace miles de años con la llegada de los arahuacos, quienes se derivaron en muchas etnias. Luego se relacionaría con los incas y los españoles.

Puerto Maldonado fue fundado por el Primer Comisario y Delegado Supremo del Gobierno don Juan S. Villalta, el 10 de julio de 1902, situado en lo que hoy es el Pueblo Viejo. El nombre de Puerto Maldonado es en homenaje al valiente explorador peruano Faustino Maldonado, quien recorrió las aguas del río Madre de Dios y grabó su nombre en el tronco de un enorme árbol en la confluencia de los ríos Madre de Dios y Tambopata. Cuando el departamento de Madre de Dios fue creado diez años después, Puerto Maldonado fue designada como su capital. Sin embargo, recién en 1985 se oficializa la fecha de fundación de la ciudad de Puerto Maldonado.

Madre de Dios es un departamento con abundantes selvas vírgenes y paisajes subyugantes. Posiblemente sea el área menos intervenida y erosionada de la Amazonía peruana. Además, la conjugación de su abrupta geografía, sus innumerables microclimas y la variedad de sus suelos han propiciado el desarrollo de una diversidad de formas vivientes. Tierra de anchos y pausados ríos y hermosas lagunas rodeadas de exuberante vegetación. Madre de Dios posee los mejores suelos de toda la selva amazónica, siendo la producción de castaña y caucho su principal fuente de ingresos. Igualmente, la región alberga tribus nativas para quienes el avance de la civilización aún no ha llegado. Las principales agrupaciones establecidas en la región son los huarayos, mashcos, piros, amahuacas, yaminahuas, amaracaes y machiguengas.

El clima de Puerto Maldonado es tropical húmedo con temperaturas altas durante todo el año aunque especialmente agosto y septiembre, a esto se le suma la sensación térmica que en algunas ocasiones roza cerca de los 50 °C. En la mayoría de los meses del año en Puerto Maldonado hay precipitaciones importantes especialmente son abundantes de octubre hasta abril. La temperatura media anual en Puerto Maldonado se encuentra a 25.4 °C. Hay alrededor de precipitaciones de 2221 mm. Durante el invierno pueden ocasionalmente ocurrir los denominados friajes que son masas de aire frío provenientes de la Antártida que logran bajar la temperatura incluso por debajo de los 10 °C.


Durante la época colonial y parte de la época republicana, hasta fines del siglo XIX, la vinculación de Puerto Maldonado con el resto del país y el mundo se caracterizó por avances y repliegues periódicos de las actividades extractivas, los que dependieron de los ciclos económicos europeos. Los más relevantes fueron la economía extractiva del caucho, y hacia fines del siglo XIX la cascarilla o quina, empleado para tratar la malaria. Asimismo la shiringa, la madera, la castaña y el oro. 

A partir del siglo XX, la extracción de caucho se introdujo en la cuenca del río Madre de Dios, incursionando por el Istmo de Fitzcarrald y llegando de esa manera a Puerto Maldonado, que por esas fechas era un lugar de tránsito para todo quien pasara. El auge en el precio de oro tras el acuerdo de Bretton Woods en la década del 40, empujó nuevamente hacia la región a diversos pobladores en busca de oro, que en el marca de un proceso paulatino y repleto de contratiempos culminó con la construcción de dos carreteras: a Puerto Maldonado en el bajo Madre de Dios, y a Shintuya en el alto Madre de Dios.

En la actualidad, la capital de Madre de Dios, se caracteriza por la heterogeneidad de las actividades económicas desarrolladas, distinguiéndose tres frentes económicos:



La dinámica de la actividad comercial, encargada de articular los tres frentes económicos, tiene un comportamiento cada vez creciente, gracias a la migración de todas partes del Perú.

A partir del año 2010 existe una carretera pavimentada desde Cusco hasta Iñapari en la frontera con Brasil, esta vía denominada "Carretera Interoceánica" es un punto de acceso a diversos atractivos a lo largo de su recorrido, donde Puerto Maldonado se convierte en un paso obligado de en la agenda turística.


La ciudad de Puerto Maldonado, cuenta con los siguientes Hospitales:

Independientemente de estos dos Hospitales, que carecen de especialistas de alto nivel, tiene una red de centros de salud y de IES distribuidas a lo largo de la ciudad y de las dos provincias existentes.




</doc>
<doc id="9779" url="https://es.wikipedia.org/wiki?curid=9779" title="Wiphala">
Wiphala

La wiphala es una bandera cuadrangular de siete colores utilizada por algunas etnias de la cordillera de los Andes.

Existen variantes de la wiphala. La más extendida es la usada en la actualidad como símbolo étnico del pueblo aimara, la cual fue reconocida como símbolo del Estado Boliviano por la Constitución de 2008:

Probablemente la palabra "wiphala" viene de dos palabras en idioma aimara:

La wiphala se denomina de varias otras maneras:


Los pueblos precolombinos de la cordillera de los Andes no carecían de símbolos propios (especialmente los de tradición estatal, como el Imperio incaico), pero el formato de «pendón cuadrilátero de tela» para ondear al viento no es una tradición americana sino indoeuropea.

Ya en el "Rig-veda" (el texto más antiguo de la India, de mediados del II milenio a. C.) se habla del "dhwasha" (‘emblema, bandera’) que flameaba en un pequeño mástil sobre los carros de guerra.

Su patrón cuadrado es inusual en la estética aborigen.
El arte textil chancay no posee la cromaticidad de la wiphala ni se reporta tal objeto en ningún museo peruano.

Si la wiphala fuera un símbolo antiguo rescatado ―como ocurre con la "chakana" (cruz andina)―, habría piezas arqueológicas de antigüedad que servirían de testimonio.


Solo una crónica colonial señala el uso de una bandera

El resto de cronistas señalan que el ejército imperial inca junto al emperador utilizaban el "unancha" (‘estandarte’, no bandera). Pero este estandarte no tiene nada que ver con la wiphala aimara.

En un gráfico de Waman Puma de A. (de 1612) aparece un objeto como banderín, denominado "walqanka".

El aimara es un pueblo textil, por lo que si la wiphala hubiera existido en la antigüedad no habría pasado desapercibida durante siglos.

El 17 de noviembre de 1944 comenzó la organización del Primer Congreso Indigenal Boliviano.
Entre quienes organizaron el mencionado congreso se encontraba el tradicionalista y aimarólogo Hugo Lanza Ordóñez. Este hizo notar a la concurrencia que la existencia de la palabra "wiphala" sugería que desde siempre en la cultura andina debió haber existido algún tipo de bandera, así que decidió hacer uso de una bandera blanca (que era la única conocida por entonces en los acontecimientos importantes). Otro congresista, Germán Monrroy Block, opinó a favor de usar una bandera más colorida, de acuerdo con la estética aimara.

El imprentero Gastón Velasco recordó que años atrás había diseñado una etiqueta para la marca Champancola, que era una empresa de gaseosas creada a fines de los veinte en La Paz, la primera en su género. Sus dueños eran dos ciudadanos italianos, Salvietti y Bruzzone. Esta fábrica producía un refresco espumante como el champán, sin alcohol, al que dieron el nombre de Champancola "(champagne-cola)".
La botella tenía en el cuello una bolita de vidrio que ―por la fuerza del gas― tapaba el envase herméticamente.
Un empleado de la fábrica ―también italiano, de apellido Sorrentino― vendía las gaseosas por las calles de La Paz en un carrito tirado por un burro.
La etiqueta que imprimió el paceño Velasco era un cuadro formado por otros cuadritos menores con los más diversos colores.
Sobre la base de ese diseño, los tres compañeros movimientistas crearon la bandera del primer congreso indigenista.

Entre el 10 y el 15 de mayo de 1945 se realizó el congreso en el coliseo deportivo Luna Park de la ciudad de La Paz, que reunió a más de mil delegados de los pueblos originarios provenientes del sector occidental y oriental de Bolivia. El entonces presidente de Bolivia, mayor Gualberto Villarroel, tomó las disposiciones de este congreso y las publicó como decretos.

Se abolió el pongueaje y todo trabajo gratuito (trabajo esclavo).
Muchos colonos dejaron de servir a sus explotadores y suspendieron las faenas agrícolas en las haciendas. Muchos indígenas fueron perseguidos y confinados a lugares inhóspitos como Ichilo (en Santa Cruz) y la isla Coatí en el lago Titicaca. Sin embargo la idea de recuperar sus tierras ancestrales, en manos de los explotadores, se generalizó entre los indígenas.

La wiphala suele ser confundida con una bandera de siete franjas horizontales con los colores del arcoíris, usada actualmente como emblema oficial de la ciudad del Cusco (Perú), y erróneamente asociada al imperio inca. Sin embargo, debe observarse que mientras la wiphala es un emblema relacionado principalmente con los pueblos de origen aimara, los incas tuvieron su origen en las etnias quechuas.

Incluso, se ha determinado que esta supuesta "bandera de los incas" no tiene realmente un sustento histórico, pues su aparición es relativamente reciente. Así, se sabe que en 1973, el ingeniero Raúl Montesinos Espejo, al conmemorar el vigésimo quinto aniversario de su radioemisora "radio Tahuantinsuyo" que operaba en la ciudad del Cusco, utilizó esta bandera con los colores del arcoíris.
Su uso se extendió tanto en dicha ciudad que en 1978, Gilberto Muñiz Caparó ―alcalde de municipalidad provincial del Cusco― declaró esa bandera como emblema de la ciudad.

La historiografía peruana ha sido enfática en precisar que en el imperio inca no existió el concepto de bandera, y por que tanto este nunca tuvo una. Así lo ha afirmado la historiadora e investigadora del Tahuantinsuyo ―el Imperio inca―, María Rostworowski (1915-2016), quien al ser consultada sobre esta enseña multicolor señaló:

En 2011, el Congreso de la República del Perú ―citando a la Academia Peruana de Historia― se pronunció contra esta bandera del Tahuantinsuyo:

Gilbert Baker diseñó la bandera del arcoíris en 1978 para la celebración de la libertad homosexual en San Francisco. La bandera no muestra un arcoíris real. El autor se inspiró en varias fuentes como el movimiento "hippie" y el Movimiento por los Derechos Civiles en Estados Unidos. Los colores se muestran en líneas horizontales con el color rojo en su parte superior y el púrpura en la inferior. Representa la diversidad de la homosexualidad tanto femenina como masculina, respectivamente. La banda color púrpura a veces se reemplaza con una color negro para representar la masculinidad o el orgullo del fetichismo del cuero negro. 

El significado de los colores de la bandera original según Baker, simbolizan los diferentes aspectos de vida LGBT: 


La bandera actual no posee las dos bandas adicionales de color rosa y verde agua: dos colores que denotan la bisexualidad. El color verde agua fue asociado en la homosexualidad durante la Época victoriana. Estos dos colores aparecen en el Triángulo Bisexual Doble, y el color rosado denota una similaridad con el color del Triángulo Rosa. Es así que el 1979, la bandera de seis bandas fue utilizada para una de las primeras marchas del orgullo gay y los ocho colores originales de la bandera flamean sobre Castro (San Francisco) en San Francisco y en el Centro LGBT en la ciudad de Nueva York.

De acuerdo a las costumbres y tradiciones andinas, la wiphala siempre está izada en todos los acontecimientos sociales y culturales, por ejemplo, en los encuentros de comunarios del Ayllu, en los matrimonios de la comunidad, cuando nace un niño en la comunidad, cuando se realiza el corte de cabello de un niño (bautismo andino), en los entierros, etc.

La wiphala también flamea en las fiestas solemnes, en los actos ceremoniales de la comunidad, en los actos cívicos del "marka" (‘pueblo’) en los juegos de "wallunk’a" (‘columpio’), en los juegos de competencia "atipasina" (‘ganarse’), las fechas históricas, en los "k'illpa" (días ceremoniales del ganado), en la transmisión de mando de las autoridades en cada período.

También se utiliza en las danzas y bailes, como en la fiesta del Anata o Pujllay (‘juego’): en los trabajos agrícolas con o sin yuntas, a través del "ayni", la "mink'a", el "chuqu" y la "mit'a". Incluso se iza al concluir una obra, una construcción de una vivienda y en todo trabajo comunitario del "ayllu" y "marka".

Algunos de los colores tienen que ver con dioses y creencias de los pueblos originarios.


Los colores se originan en el rayo solar al descomponerse del arcoíris blanco "(kutukutu)", en los siete colores del arcoíris "(kurmi)", tomado como referencia por los antepasados indígenas, para fijar la composición y estructura de nuestros emblemas, así mismo organizar la sociedad comunitaria de los Andes.

En el momento de izar la wiphala, todos deben guardar silencio y al terminar alguien debe dar la voz de victoria del "jallalla qullana marka", "jallalla pusintsuyu" o "jallalla tahuantinsuyu".

Asimismo, las banderas regionales correspondientes son de un solo color entero y cada una se caracteriza por el color asignado (de acuerdo con la región).




</doc>
<doc id="9781" url="https://es.wikipedia.org/wiki?curid=9781" title="Aimaras">
Aimaras

Aymara o Aimará (Aimara: "aymara" ), a veces escrito como aymara, es un pueblo originario de América del Sur que habita la meseta andina del lago Titicaca desde tiempos precolombinos, repartiéndose su población entre el occidente de Bolivia, el sur del Perú y el norte Grande de Chile. Alternativamente, reciben el nombre de collas, aunque no hay correspondencia biunívoca entre ambos nombres.

El concepto de “aimara” aparece definitivamente entonces durante la colonia y, salvo raras excepciones, no fue utilizado para identificar sociopolíticamente ningún grupo poblacional en esa zona de los Andes. Todas estas formaciones sociopolíticas, verdaderas naciones durante los siglos y (Reinos aimaras), fueron agrupadas bajo la etiqueta “aymara”, para fines económicos, pero manteniéndose las nominaciones originarias para describir, por ejemplo, las organizaciones políticas más relevantes de acuerdo a los intereses económicos, eclesiásticos o administrativos territoriales fluctuantes de la colonia. Aunque se reconoce una encomienda “aymaraes” para el Distrito Colonial de La Paz, nombres jurisdiccionales nativos como “Carangas”, “Soras”, “Casayas”, “Aullagas”, “Uruquillas”, “Asanaques” y “Quillazas” son usados para el Distrito de La Plata (Torero Ídem.) y hasta el siglo no existe una subdivisión política reconocida como “aimara” por la colonia. Durante esta época el Obispado de La Paz mantiene una organización administrativa utilizando las nominaciones originales de Sicasica, Pacajes, Omasuyos, Larecaja, Paucarcolla y Chucuito (Cosme Bueno, S. XVIII - 1951).

El término "aymara" nunca fue reconocido socialmente por los pobladores nativos durante toda la colonia, y así continuó durante los periodos de Independencia y República (después de 1821 hasta la actualidad), durante los que mantuvo su acepción original colonial, que ha sido usada siempre desde una perspectiva externa o foránea. Esta perspectiva se ha mantenido y solo empezó a utilizarse sistemáticamente en términos sociológicos a partir de la primera mitad del siglo pasado cuando se establecieron los primeros parámetros antropológicos descriptivos de las sociedades andinas actuales hechos desde una perspectiva lingüística. Un ejemplo relevante es la clasificación de Greemberg (en Steward and Faron, 1959) que incluye al “quechua”, “aymara” y “uro” dentro de la subfamilia lingüística “andina”. La identificación cultural “aymara” obvió, en términos antropológicos, los complejos sistemas socioculturales de auto reconocimiento de las poblaciones del altiplano andino y la cuenca del lago Titicaca, lo que no pasó con las poblaciones “quechuas”, que se entendieron en forma más diversificadas.

Las naciones o pueblos que ancestralmente hablan este idioma eran: aullaga, ayaviri, cana, canchis, carangas, charcas, larilari, lupacas, umasuyus, pacaje, pacasa y quillaca. A estos pueblos se les ha atribuido una única identidad con el nombre "qullasuyu" (también conocido como Collasuyo) y conformaron una parte del Imperio inca.

Los habitantes se asocian como la civilización centrada en Tiahuanaco ("Tiwanaku"), aunque Tiahuanaco es una cultura posterior seguida de la de los Incas. Hay evidencia lingüística que sugiere que los aymaras provinieron de más al norte, ocupando la meseta del Titicaca después de la caída de Tiahuanaco. No se han encontrado evidencias de habitantes de la civilización de Tiahuanaco que tuvieran lenguaje escrito.

El territorio tiahuanaco fue fundado aproximadamente en el 200 a. C., como una pequeña villa, y creció a proporciones urbanas entre el 300 y el 500, consiguiendo un importante poder regional en el sur de los Andes. En su máxima extensión, la ciudad cubría aproximadamente 6 km², y tuvo una población máxima de unos 1000 habitantes. Su estilo de alfarería era único, del encontrado hasta 2006 en Sudamérica. Una característica importante son las enormes piedras que se encontraron en el lugar; de aproximadamente diez toneladas, las cuales ellos cortaban, les daban forma cuadrada o rectangular y esculpían. Colapsó repentinamente aproximadamente en 1200. La ciudad fue abandonada y su estilo artístico se desvaneció.

Desaparecido el Imperio Tiwanaku, la región quedó fragmentada y fue ocupada por etnias aimaras. Estos aymaras se caracterizan por sus necrópolis compuestas por tumbas en forma de torres-chullpas. Existen también algunas fortalezas denominadas pucaras.
El modelo por el cual se regulaban estas etnias es el de verticalidad o control de los diversos pisos ecológicos que sostienen su economía de subsistencia. Ningún grupo humano necesita tanto de sus relaciones con la costa y con los valles como los pueblos aimaras del altiplano, por esta razón cada centro de la puna controlaba por medio de la colonización de zonas periféricas situadas a diferentes alturas y con climas varios.

La deidad principal de esta sociedad de lengua aymara fue Tunupa, el temido dios de los volcanes. En su honor hacían sacrificios humanos y grandes fiestas. En excavaciones realizadas en el sitio arqueológico de Akapana se han encontrado materiales como ofrendas, alfarería, fragmentos de cobre, huesos de camélidos y entierros humanos. Estos objetos fueron encontrados en el primer y segundo nivel de la pirámide de Akapana y la cerámica adjunta corresponde a la fase III de los tiahuanacotas.

En la base del primer nivel de Akapana se hallaron hombres y niños desmembrados a los cuales les faltaba el cráneo; estos restos humanos estaban acompañados de camélidos desarticulados además de cerámica. En el segundo nivel se encontró un torso humano completamente desarticulado. En total se encontraron 10 entierros humanos, de los cuales 9 eran varones. Estos sacrificios corresponden, aparentemente, a ofrendas dedicadas a la construcción de la pirámide.

A mediados del siglo , el reino Colla conservaba un extenso territorio con su capital Hatun-Colla. El inca Viracocha incursionó en la región, pero quien la conquistó fue su hijo Pachacútec, noveno Inca.

Así como al norte se encontraban los Collas, al sur estaba la Confederación Charca que tenía dos grupos: Los Carangas y Quillacas en torno al lago Poopó, y los Charcas que ocupaban el norte de Potosí y parte de Cochabamba. Ambos, Charcas y Collas eran de habla aimara.

La cultura material de los Carangas presenta extensas necrópolis o chullpares, algunos de los cuales conservan todavía restos de pintura en sus muros exteriores. Una vez que los Carangas fueron conquistados por los incas, Huayna Cápac los llevó a trabajar al valle de Cochabamba como mitimaes.

El señorío denominado Charca, al que estaban adscritos Cara-caras fue conquistado por los incas en tiempo de Túpac Inca Yupanqui y llevados a la conquista de Quito. Por su parte el pueblo de los Cara-cara era tan belicoso como el Charca y aún más, en su territorio tienen lugar aun hoy en día luchas denominadas "T'inkus".

El Inca Lloque Yupanqui inició la conquista del territorio aimara a finales del siglo , la que fue continuada por sus sucesores hasta que a mediados del siglo fue completada por Pachacútec al derrotar a Chuchi Kápak. De todas formas se cree que los incas tuvieron una gran influencia de los aimaras por algún tiempo, ya que su arquitectura, por la cual son muy conocidos los incas, fue claramente modificada sobre el estilo Tiwanaku, y finalmente los aimaras conservaron un grado de autonomía bajo el imperio Inca.
Posteriormente los aimaras del sur del Titicaca se rebelaron y tras rechazar el primer ataque de Túpac Yupanqui éste volvió con más tropas y los sometió. 

Su población se estima en 1 a 2 millones de personas durante el Imperio inca, eran el principal pueblo del Collasuyo, ocupando todo el oeste de Bolivia, sur de Perú, norte de Chile y el norte de Argentina. Tras la conquista española en menos de un siglo se redujeron a cerca de 200 000 sobrevivientes, o menos. Tras la independencia su población empezó a recuperarse.

En la actualidad, la mayor parte de los aimaras viven ahora en la región del lago Titicaca y están concentrados en el sur del lago. El centro urbano de la región aimara es El Alto, ciudad de 750 000 habitantes, y también en la La Paz sede de gobierno de Bolivia. Además, muchos aimaras viven y trabajan como campesinos en los alrededores del Altiplano. Se estima en 1 600 000 a los bolivianos aimara-parlantes. Entre 300 000 y 500 000 peruanos utilizan la lengua en los departamentos de Puno, Tacna, Moquegua y Arequipa. En Chile hay 48 000 aimaras en las áreas de Arica, Iquique y Antofagasta, mientras que un grupo menor se halla en las provincias argentinas de Salta y Jujuy.

El aimara utilizó un tipo de proto-khipus, sistema nemotécnico de contabilidad básica común a varios pueblos precolombinos, como los de Caral-Supe y Wari (anteriores a los aimara), y los Incas. No existen evidencias que hayan tenido lenguaje escrito, a pesar de que algunos, como William Burns Glyn, sostienen que los khipus incaicos pudieron ser una forma de ello

La Encuesta Complementaria de Pueblos Indígenas (ECPI) 2004-2005, complementaria del Censo Nacional de Población, Hogares y Viviendas 2001 de Argentina, dio como resultado que se reconocieron y/o descienden en primera generación del pueblo aimara 4104 personas en Argentina.

El Censo Nacional de Población de 2010 en Argentina reveló la existencia de 20 822 personas que se auto-reconocieron como aimaras en todo el país, 9606 de los cuales en la ciudad de Buenos Aires, 6152 en la provincia de Buenos Aires, 773 en la de Jujuy, 358 en la de Neuquén y 326 en la de Tucumán.

La población que se auto-reconoció como aimara en el censo boliviano de 2001 fue de 1 277 881 personas. Este número bajó a 1 191 352 en el censo de 2012.

Una parte del segmento amerindio son los aymaras. En cuanto al idioma aimara el último censo realizado el 2007 en el Perú, dio como resultado una población de 443 248 aymarahablantes.

Los datos censales confirman al aymara como la segunda lengua amerindia más difundida en el Perú. La región Puno concentra la mayor cantidad de aimarahablantes, superando los 300 mil; seguidamente la región Tacna supera por poco los 45 mil aymarahablantes; y finalmente las regiones de Moquegua, Lima y Arequipa concentran poblaciones importantes de hablantes de esta lengua.

A los aimaras generalmente se les agrupa en un sólo grupo etnolingüístico, pero se pueden reconocer varios grupos entre los que destacan los Lupacas, Urus y Pacajes.

En el caso de la etnia Uru o Uro, hablaban la lengua uruquilla que se considera extinta en Perú. Habitaron gran parte del sur de Perú, norte de Chile y el occidente de Bolivia, y su origen étnico se remontaría a épocas anteriores a los incas.

Desde el 2009, el proyecto de "Salvaguardia del patrimonio cultural inmaterial de las comunidades aimaras de Bolivia, Perú y Chile", forman parte de la Lista representativa del patrimonio cultural inmaterial de la humanidad de la UNESCO.

Dentro de las etnias aimaras en el Perú, también se incluyen a dos etnias aisladas geográficamente de las demás etnias aymaras que por tradición habitan los alrededores de la meseta del Collao. Estas etnias son los Jaqarus y los Kawkis que habitan las sierras del distrito de Tupe, Provincia de Yauyos, en la región Lima. Las lenguas de estas etnias fueron estudiadas por primera vez en 1959 por Martha Hardmann, catalogándolas en la familia aru o aymara.

En el caso del Jaqaru, se calcula su población en 2300 habitantes, y actualmente se hacen grandes esfuerzos por salvar a esta lengua de la extinción. Las tradiciones textiles, musicales y orales de esta etnia son únicas y sólo se la puede encontrar en un reducido espacio geográfico de la Región Lima.

Los kawki o cauqui, en cambio, se distribuyen en una zona tan reducida como unos cuantos poblados en la provincia de Yauyos, (Cachuy, Chavín y Canchán). Con una lengua al borde de la extinción. Fue Martha Hardmann quien relacionó a la lengua de los kawki como una de las lenguas vivas de la familia jaquí en la que se incluye al aimara.

Su idioma es la lengua aimara, aunque muchos de ellos hablan castellano como consecuencia de la colonización o conquista española.
Su símbolo o bandera es la Wiphala. No obstante, la misma ha sido cuestionada por muchos historiadores al no existir antecedentes históricos que demuestren su origen aimara (en sí mismas, las banderas son símbolos europeos); al contrario, algunos indicios apuntan a un surgimiento moderno de la Wiphala (durante el siglo XX).

Algunas personas acostumbran masticar la hoja sagrada de coca "(Erythroxylum coca)". Por su condición de hoja sagrada durante la época del imperio incaico, su uso estaba restringido al inca, nobleza y sacerdotes bajo pena de muerte. Además del uso en masticación, utilizan las hojas de coca en remedios al igual que en rituales.
Durante este último siglo, estas plantaciones les han traído conflictos con las autoridades, por prevenir la creación de la droga cocaína. Sin embargo, la coca tiene gran participación en la religión de los aimaras, al igual que antes con los incas y últimamente se ha convertido en un símbolo cultural de su identidad. Los cultos de Amaru, Mallku y Pachamama son las formas más antiguas de celebración que los aimaras aún realizan.

Aún no existen fundamentos históricos para determinar que el año aimara se celebra el 21 de junio o para establecer un cómputo exacto del año que se cumple (por ejemplo, en el 2017 se llegaría al año 5525 del calendario aimara; tal fecha (21 de junio) coincide con el solsticio de invierno, el cual fue festejado ancestralmente por el pueblo quechua en la fiesta del Inti Raymi.

A partir del año 2013, el día 21 de junio es «feriado nacional inamovible» en Bolivia.

En Tiwanaku antes del 21 de junio los comunarios y turistas que vienen a conocer y a compartir esta fiesta milenaria, el día 20 de junio realizan una víspera similar al Año Nuevo tradicional igualmente para despedir el año viejo. A partir entre las 6:00 y 7:00 de la mañana, se preparan con música folclórica tradicional y rituales para recibir el nuevo año frente a la Puerta del Sol con la entrada de los primeros rayos del sol, como también la llegada del solsticio y la época del invierno.

Esta tradición milenaria que se ha conservado en su cosmovisión ancestral, dice que la llegada de todos los años es para el bienestar y la buena fertilización de la cosecha. Lo mismo y similar al año nuevo tradicional, para los creyentes los años venideros serán de gran prosperidad para quienes lo deseen. Los sacerdotes comunarios, realizan rituales y agradecen a la Pachamama solicitando su bendición.
Su creencia no se aprecia en forma de adoraciones exageradas en lo abstracto o invisible, tienen una religiosidad viviente, donde los vivos y los muertos no dejan de existir, es decir, solo cumplen un ciclo de vida para volver al inicio. Las divinidades son energías, son su sobrevivencias.
EL Tata-Inti o el dios sol o como la Pachamama o la madre tierra, son los puntos de partidas de todo. Por eso toda ceremonia se inicia mirando hacia arriba, hacia el sol.




</doc>
<doc id="9784" url="https://es.wikipedia.org/wiki?curid=9784" title="Horno de microondas">
Horno de microondas

El horno de microondas es un electrodoméstico usado en la cocina para calentar alimentos, o cocerlos, que funciona mediante la generación de ondas electromagnéticas en la frecuencia de las microondas, en torno a los 2,45 GHz.

Un microondas es un electrodoméstico destinado a calentar y cocinar alimentos calentando el agua que contienen o los líquidos que se añaden. Funciona mediante la generación de ondas de radio de alta frecuencia. El agua, las grasas y otras sustancias presentes en los alimentos absorben la energía producida por las microondas en un proceso llamado calentamiento dieléctrico (conocido también como calentamiento electrónico, calentamiento por RF, calefacción de alta frecuencia o diatermia). Hay moléculas cuya estructura forma dipolos eléctricos, como la del agua, lo que significa que tienen una carga positiva parcial en un extremo y una carga negativa parcial en el otro, y por tanto oscilan en su intento de alinearse con el campo eléctrico alterno de las microondas. Al rotar, se producen rozamientos y choques, que son los que elevan la temperatura.
Los hornos de microondas funcionan de la siguiente manera: un aparato llamado magnetrón convierte la energía eléctrica en energía de microondas, que en esta forma alcanza el alimento. Las ondas electromagnéticas agitan las moléculas bipolares presentes en los alimentos, especialmente las del agua, y esta es la que eleva la temperatura. Esta agitación es un mecanismo físico, simple movimiento de las moléculas al ritmo de la frecuencia, y no provoca ningún tipo de alteración en la composición química (excepto los que son producidos por el aumento de la temperatura). 

El calentamiento por microondas es más eficiente en el agua líquida que en el agua congelada, ya que en el estado sólido del agua, el movimiento de las moléculas está más limitado. También es menos eficiente en grasas y azúcares (que tienen un momento dipolar molecular menor) que en el agua líquida.

A veces se explica el calentamiento por microondas como una resonancia de las moléculas de agua, pero esto es incorrecto, ya que esa resonancia sólo se produce en el vapor de agua y a frecuencias mucho más altas (a unos 20 GHz). Por otra parte, los grandes hornos de microondas industriales que operan la mayoría en la frecuencia de 915 MHz (longitud de onda de 328 milímetros) también calientan el agua y los alimentos de forma efectiva.

Los azúcares y triglicéridos (grasas y aceites) absorben las microondas debido a los momentos dipolares de sus grupos hidroxilo o éster. Sin embargo, debido a la capacidad calorífica específica más baja de las grasas y aceites, y a su temperatura más alta de vaporización, a menudo alcanzan temperaturas mucho más altas dentro de hornos de microondas. Esto puede causar en el aceite o alimentos muy grasos, como el tocino, temperaturas muy por encima del punto de ebullición del agua, llegando a tostar de forma parecida al asado en la parrilla convencional o en las freidoras. Los alimentos en alto contenido de agua y con poco aceite rara vez superan temperaturas superiores a las de ebullición del agua.

El calentamiento por microondas puede provocar un exceso de calentamiento en algunos materiales con baja conductividad térmica, que también tienen constantes dieléctricas que aumentan con la temperatura. Un ejemplo de ello es el vidrio, que puede mostrar embalamiento térmico en un horno de microondas hasta el punto de fusión. Además, las microondas pueden derretir algunos tipos de rocas, produciendo pequeñas cantidades de lava sintética. Algunas cerámicas también se pueden fundir, e incluso pueden llegar a aclarar su color al enfriarse. El embalamiento térmico es más típico de líquidos eléctricamente conductores, tales como agua salada.

Un error común es creer que los hornos microondas cocinan los alimentos "desde dentro hacia afuera", es decir, desde el centro de toda la masa hacia el exterior de alimentos. Esta idea surge del comportamiento del calentamiento si una capa absorbente de agua se encuentra debajo de una capa seca, menos absorbente, en la superficie de un alimento. En la mayoría de los casos en alimentos uniformemente estructurados o razonablemente homogéneos en su composición física, las microondas son absorbidas en las capas exteriores de forma similar al calor de otros métodos. Dependiendo del contenido de agua, la profundidad de la deposición de calor inicial puede ser de varios centímetros o más con los hornos de microondas, en contraste con el asado (infrarrojos) o el calentamiento convectivo (métodos que depositan el calor en una fina capa de la superficie de los alimentos). La profundidad de penetración de las microondas depende de la composición de los alimentos y de la frecuencia, siendo las frecuencias de microondas más bajas (longitudes de onda más largas) las más penetrantes. Las microondas penetran únicamente de 2 a 4 cm en el interior de los alimentos, por lo que el centro de una porción grande no se cocinará con la energía de estas ondas, sino por el calor que se produce en el horno y por el que se transfieren las partes superficiales que sí son alcanzadas por las ondas.

Como otros inventos, el horno de microondas es la aplicación secundaria de una tecnología destinada a otros fines. En 1946, durante una investigación relacionada con el radar, el doctor Percy Spencer, ingeniero de la Raytheon Corporation, estaba probando un tubo al vacío llamado magnetrón cuando descubrió que una chocolatina que tenía en su bolsillo se había derretido. Sospechando que aquello había sido causado por las ondas emitidas por el magnetrón, el doctor Spencer colocó algunas semillas de maíz para hacer palomitas cerca del tubo a modo de experimento. El maíz se coció e hinchó. Spencer repitió el experimento usando un huevo de gallina. Debido al rápido incremento de la temperatura, la presión interna hizo que el huevo explotara. Esto le animó a seguir experimentando con otros alimentos.

El doctor Spencer diseñó una caja metálica con una abertura por la que podía entrar la radiación del magnetrón. Las paredes metálicas confinan la radiación de microondas, porque la energía del campo electromagnético aumenta. Cuando se introducía alimento su temperatura aumentaba.
Los ingenieros se dedicaron a mejorar el prototipo del doctor Spencer, y a finales de 1946, la Raytheon Company solicitó una patente para emplear los microondas en la preparación de los alimentos. El primer horno en prueba, que calentaba los alimentos mediante energía de microondas se instaló en un restoran de Boston. En 1947, salió al mercado el primer horno comercial de microondas. Estas primeras unidades eran aparatosas, de 1,60 m de altura y 80 kg de peso. El magnetrón se refrigeraba con agua, de modo que era necesario instalar un circuito especial. Además, su precio era elevado: costaban alrededor de 5.000 dólares cada uno, por lo que no tuvieron demasiada acogida.

Al desarrollarse un nuevo magnetrón enfriado por aire, se eliminó la necesidad de colocar tuberías de refrigeración, lo que permitió fabricar hornos más baratos y manejables. Los negocios de comida rápida fueron los primeros en reconocer su utilidad.

Cuando la industria alimentaria descubrió el potencial y la versatilidad del nuevo invento, éste se aplicó a usos variados, como deshidratar verduras, tostar café o frutos secos, descongelar y cocinar las carnes, abrir ostras, etc. Otras industrias lo emplearon para el secado de corcho, cerámica, papel, cuero, tabaco, fibras textiles, lápices, flores, libros húmedos y cerillas. También se emplearon las microondas en el proceso de curado de materiales sintéticos como nailon, hule y uretano.
Sin embargo, a causa de la desconfianza hacia los nuevos "hornos electrónicos de radar", no fue hasta los años setenta cuando se empezó a usar en las cocinas domésticas. Nadie moría de "envenenamiento" por las radiaciones, ni quedaba ciego, estéril o impotente debido al uso de hornos de microondas. Cuando se desvanecieron los temores en Estados Unidos, aumentó la aceptación y, se olvidaron los mitos.

En 1975, por primera vez, las ventas de hornos de microondas rebasaron el número de estufas de gas vendidas. El año siguiente se informó que 17% de todos los hogares de Japón cocinaban con microondas, en comparación del 4% de los hogares de los Estados Unidos. En 1971, menos del 1% de los hogares estadounidenses tenían microondas; en 1978 la cifra ascendió al 13%, llegando al 25% en 1986. Hoy, los hornos a microondas cuentan con el añado y mejora de temporizador, sensores de horneado y resistencias eléctricas para gratinar o acabar de dorar los platos que lo necesiten, pues esto no es posible conseguirlo sólo con las microondas.

La mayoría de gobiernos, industrias y la propia OMS defienden su uso como un electrodoméstico seguro para la salud. Sin embargo, sí es obligatorio tomar las siguientes medidas de seguridad:

Al intentar realizar una reparación deben respetarse las medidas de seguridad, ya que existe riesgo grave de electrocución, incluso habiendo desconectado el aparato de la red eléctrica, por la presencia de un condensador de gran capacidad.


</doc>
<doc id="9786" url="https://es.wikipedia.org/wiki?curid=9786" title="Departamento de Arequipa">
Departamento de Arequipa

Arequipa es uno de los veinticuatro departamentos que, junto a la Provincia Constitucional del Callao, forman la República del Perú. Su capital y ciudad más poblada es Arequipa.

Está ubicado al sur del país, limitando al norte con Ayacucho, Apurímac y Cuzco, al este con Puno, al sureste con Moquegua, al oeste con el océano Pacífico y al noroeste con Ica. Con 63 345 km² es el sexto departamento más extenso —por detrás de Loreto, Ucayali, Madre de Dios, Puno y Cuzco— y con 1 152 303 habitantes en 2007 es el octavo más poblado —por detrás de Lima, Piura, La Libertad, Cajamarca, Puno, Junin y Cuzco—. Se fundó el 15 de agosto de 1540.

Cuenta con 528 km de costas en el océano Pacífico —el litoral regional más extenso—. La zona costera es una de las porciones más secas del desierto costero, entretanto la región interior andina presenta valles escarpados y cañones.

El departamento tiene una población de 1,15 millones de habitantes, el 71,3% de los cuales reside en la capital, la ciudad de Arequipa. El nivel educativo promedio es superior a la media nacional; tiene una tasa de analfabetismo del 4,9% y el 10% de la población tiene estudios superiores. Tiene ocho provincias, de las cuales las más desarrolladas por el volumen de sus contribuciones económicas son Arequipa, Islay y Caylloma. Las Principales ciudades de la región, son primeramente, la capital, Arequipa, por su comercio e industrias; el turismo también es importante en Arequipa. La mina Cerro Verde es parte importante de su economía, después le sigue la ciudad de Mollendo, por el puerto de Matarani, el turismo de playa y por su agricultura, prontamente por la petroquimica y la mina Tía María, seguidamente del pueblo de Chivay, por su turismo, gracias al cañón del Colca, y el pueblo de Camaná, gracias al puerto de Quilca y al turismo de playa.

El 16,6% de la red de carreteras en el departamento es asfaltado, siendo Arequipa, Caravelí, Camaná y Caylloma las provincias con el porcentaje más alto de este tipo de superficie. Este es el segundo departamento más interconectado en términos de telecomunicaciones, después de Lima, ya que tiene 111,2 mil líneas de telefonía fija, con una teledensidad de 9 líneas por cada 100 habitantes y una densidad de 31,84 líneas móviles por cada 100 habitantes.

La historia de Arequipa se remonta hace 8000 años. Su territorio fue ocupado inicialmente por el imperio Wari. Luego, la cultura Churajón dejó huellas de su paso en obras de riego, andenerías y tierras cultivadas. En el norte de sus valles se desarrolló la cultura Chuquibamba, que se extendió hasta las provincias sureñas de Ayacucho y tuvo contactos con el Cusco. La leyenda menciona que Arequipa fue fundada por el cuarto inca, Mayta Cápac, quien estuvo con su ejército en dicha zona. Cuando dispuso el desplazamiento de su gente, hubo quienes le pidieron quedarse, respondiendo el inca “ari qipay”, que en lengua quechua significa “sí, quédense”.

Tiempo después, los conquistadores españoles fundaron la capital de la región en las faldas del Misti el 15 de agosto de 1540. Después de esa fecha y a lo largo de más de tres siglos la ciudad fue poblada por familias españolas. Es así que Arequipa fue la ciudad del Perú con más Españoles. Su primer alcalde fue el distinguido conquistador D. Juan de la Torre y Díaz Chacón. En la época republicana ocurrieron aquí los alzamientos de Ramón Castilla, Mariano Ignacio Prado, Nicolás de Piérola Villena, Luis Miguel Sánchez Cerro y otros más. En época moderna de estadistas como José Luis Bustamante y Rivero y Fernando Belaúnde Terry. Arequipa es el cimiento del complejo económico del sur del Perú.

Está ubicado al suroeste del Perú, frente al Océano Pacífico con 527 kilómetros de litoral. Debido a esa ubicación, es el centro comercial de la zona sur del país, que incluye los departamentos de Apurímac, Cusco, Madre de Dios, Moquegua, Puno y Tacna; y, es parte del corredor turístico del sur peruano, lo que significa que está interconectado con el 40% del país, y encaramado sobre un repecho o cuesta en la Cordillera de los Andes. Limita al noreste con Ica y Ayacucho; por el norte, con Apurímac y Cusco; por el este, con Moquegua y Puno; por el sudoeste, con el océano Pacífico.

A los mil metros de altitud, el clima varía de templado-cálido a templado, templado-frío y frío en las montañas que dominan el paisaje; la variación de la temperatura es notoria entre el sol y la sombra y entre el día y la noche.

En las altas punas la temperatura desciende considerablemente, superando sólo en el mes más cálido los 0ºC. Pero se debe tener en cuenta que este promedio puede variar durante el año. Son frecuentes las precipitaciones de nieve, con mayor incidencia en los meses de julio y agosto en sectores superiores a los 4000 m.s.n.m. y temperaturas inferiores a los -20ºC, también se presentan granizadas fenómeno que causa grandes pérdidas económicas entre los agricultores y ganaderos de la región. Las lluvias en la región andina caen regularmente entre los meses de enero y marzo. En la costa son frecuentes las lloviznas o garúas, así como las neblinas a ras del suelo..

Este Departamento está conformado por ocho provincias, que se muestran en el siguiente cuadro con sus respectivas capitales.



De la religión católica:

Esta carretera tiene extensión de 2603 km (de 1071 kilómetros que corresponden a las carreteras que deberán ser asfaltadas, 1514 km de carreteras asfaltadas y 17,5 km de vías urbanas, sin incluir la zona urbana de Juliaca). Con estas obras se conectarán tres puertos peruanos en el océano Pacífico: San Juan de Marcona, Matarani e Ilo, con Iñapari en la frontera con Brasil.

La carretera es parte de la Infraestructura Regional de América del Sur Integración (IIRSA), busca la integración comercial de los departamentos de Madre de Dios, Puno, Cusco, Tacna, Moquegua, Arequipa, Ica, Ayacucho, Apurímac y Huancavelica.

Se estima que la vía permitirá que los productos de los departamentos del sur peruano entren en el mercado de Brasil y que los bienes de Acre y Rondonia en Brasil, y, en última instancia los procedentes de la Amazonía y Mato Grosso, entren en Perú.

La comida arequipeña ha alcanzado fama por ser una de las más variadas y sabrosas del Perú. Posee la mayor diversidad respecto a otros departamentos del Perú gracias a la amplia despensa que posee en su campiña y sus valles así como su amplia costa. Sobresale por el buen gusto, gracias al uso de condimentos y formas de preparación, tanto andinas como introducidas por los europeos; los rocotos y ajíes, frutas variadas, hortalizas, carne de res, carnero, cuy, cerdo, alpaca, avestruz, variedad de pescados y de gran manera camarones, leche y quesos de excelente calidad, vinos y piscos, chicha de maíz, etc. Una de las características peculiares de la comida son los picantes en infinidad de combinaciones; esto hace que los lugares donde se expenden se llamen picanterías.

La diversidad de esta cocina mestiza se puede resumir en la existencia de caldos o chupes para cada uno de los días, El lunes "chaque"; martes "chairo"; miércoles "pebre"; jueves "timpusca" (en temporada de peras) o "menestrón"; viernes "chupe de camarones"; sábado "tiempo de rabo" o "puchero"; domingo "caldo blanco de lomos".

Es conocida en el país y en el mundo por la exquisitez de sus guisos y potajes preparados a fuego de leña y en ollas de barro. Entre los más conocidos se encuentran el Chupe de camarones, Ocopa arequipeña, Rocoto relleno, Adobo de chancho, Soltero de queso, Pastel de papas, Costillar frito, Cuy chactado, Cauche de queso, Locro de pecho y el Chaqué por mencionar algunos. Como postre se recomienda el Queso helado, los Buñuelos y para beber, además de la Chicha de jora, la cerveza y el anís nájar.

En la zona costera de Arequipa está el delicioso plato llamado Perol típico de Mollendo preparado a base de un marisco llamado Barquillo que abunda en esta ciudad.

La tasa de analfabetismo es del 4,90%, inferior al valor alcanzado a nivel nacional (8,13%), esto no es determinante para la educación superior logros.
En general, un mayor nivel relativo de la educación se percibe en Arequipa en comparación con otras regiones. No obstante, la mayoría de la población sólo alcanzó la secundaria completa (22%), seguido por el 17% que muestra primaria incompleta y 15% con secundaria incompleta, mientras que sólo el 10% (107 966) de la población presenta un nivel universitario completo, y el 7% tiene educación superior, o educación universitaria aún no completada.

El volcán Misti es imponente, levantado sobre una serena campiña con manantiales, viejos molinos (en Sabandía), baños termales (en Yura y en Socosani), pequeños con callejones tipo andaluz (Yanahuara) y, no muy lejos, una aldea enraizada en un cerro pétreo (Sachaca). El uso de la piedra sillar (mineral volcánico) en la construcción de templos, conventos y casonas le dan a la ciudad un aspecto peculiar. Los lugares más visitados son el valle del Colca y su cañón (que es uno de los más profundos del mundo) y por la permanencia actual de tres culturas vivas: los "Kawanas", "Kollawas" y los "Ccaccatapay", con su música y costumbres milenarios, los petroglifos de Toro Muerto, el valle y cañón de Cotahuasi. Así mismo, las cuevas de Sumbay y Andahua y el denominado Valle de los Volcanes, las lagunas de Mejía y las cavernas de Socabaya. En la ciudad de Arequipa, el monasterio de Santa Catalina, fundado en 1580.

Otros lugares para visitar son:


Arequipa es uno de los departamentos más deportivos del país. Entre sus deportistas más destacados están los futbolistas, atletas y tenistas, quienes en muchas oportunidades han logrado representar al Perú en competencias internacionales. Además cuenta con gran variedad de equipos de fútbol los cuales tienen gran protagonismo en Copa Perú. Arequipa posee 2 campeonatos Nacionales (ganados por Melgar), 1 Campeonato de Segunda División (Ganado por Total Clean), 2 Campeonatos de promoción y Reserva (ganados por Melgar) y 5 Copas Perú (Ganadas por Melgar, Huracán, Atlético Universidad, Total Clean y Escuela Municipal Binacional).

Deportistas que integraron distintos clubes en otras partes del Perú.



</doc>
<doc id="9789" url="https://es.wikipedia.org/wiki?curid=9789" title="Síndrome de Alport">
Síndrome de Alport

El síndrome de Alport (también llamado Mal de Alport) es una enfermedad genética , en la que una alteración en la síntesis del colágeno tipo IV afecta los riñones, oídos y ojos causando hipoacusia neurosensorial progresiva ( afectando particularmente los tonos agudos) y trastornos de la vista, incluyendo megalocórnea, lenticono y cataratas. Fue inicialmente identificado por el médico británico Cecil A. Alport en 1927, que describió una familia británica en la que muchos miembros desarrollaban enfermedades renales. Él describió que los hombres afectados en la familia morían a causa de enfermedades renales, mientras que las mujeres estarían menos afectadas.

El síndrome de Alport se caracteriza por tener afección renal, coclear y ocular. La principal señal de este síndrome, es la hematuria microscópica (microhematuria). Los hombres con el síndrome Alport ligado al cromosoma X (XLAS) padecen microhematuria desde una edad muy temprana. Alrededor del 90% de mujeres con XLAS también la tienen. Hay 2 métodos para el diagnóstico clínico: secuenciación y análisis de deleción/duplicación. El análisis de secuenciación de COL4A5 identifica cerca del 80% de las mutaciones de individuos afectados con antecedentes familiares en herencia ligada al X. El análisis de deleción/duplicación del gen COL4A5 identifica deleciones (típicamente multiexónicas) cercanas al 10% de individuos afectados con antecedentes familiares ligada al X.
Estudios recientes hablan sobre el empalme del elemento Alu en los exones los cuales provocan este síndrome.



</doc>
<doc id="9790" url="https://es.wikipedia.org/wiki?curid=9790" title="Samuel Hahnemann">
Samuel Hahnemann

Christian Friedrich Samuel Hahnemann, más conocido como Samuel Hahnemann (Meissen, Alemania, 10 de abril de 1755-París, 2 de julio de 1843), fue un médico sajón, fundador de la homeopatía.

A Hahnemann también se le atribuye haber introducido la práctica de la cuarentena en el Reino de Prusia durante su servicio al duque de Anhalt-Köthe. 

Hahnemann vivió hasta los veinte años en Meissen, donde aprendió varios idiomas y estudió la cultura clásica. Antes de cumplir los veinticinco años, ya trabajaba como médico privado del gobernador de Transilvania. Fue químico antes que médico. Su suegro era farmacéutico, y Hahnemann fue su aprendiz durante muchos meses. La medicina, tal como existía a finales del siglo XVIII o inicios del siglo XIX, no podía considerarse todavía medicina, sino una amalgama de recetas extrañas e, incluso, "extravagantes". 

Según el doctor Richard Hael, su biógrafo por excelencia, así como el profesor Bradford, su maestro, la lista de las obras químicas antes de 1810, son más o menos 27. Algunas son traducciones, otras creaciones.

Hahnemann murió en París, y sus restos están enterrados en el Cementerio de Père-Lachaise.




</doc>
<doc id="9791" url="https://es.wikipedia.org/wiki?curid=9791" title="Leyes de Newton">
Leyes de Newton

Las leyes de Newton, también conocidas como leyes del movimiento de Newton, son tres principios a partir de los cuales se explican una gran parte de los problemas planteados en mecánica clásica, en particular aquellos relativos al movimiento de los cuerpos, que revolucionaron los conceptos básicos de la física y el movimiento de los cuerpos en el universo.

En concreto, la relevancia de estas leyes radica en dos aspectos: por un lado constituyen, junto con la transformación de Galileo, la base de la mecánica clásica, y por otro, al combinar estas leyes con la ley de la gravitación universal, se pueden deducir y explicar las leyes de Kepler sobre el movimiento planetario. Así, las leyes de Newton permiten explicar, por ejemplo, tanto el movimiento de los astros como los movimientos de los proyectiles artificiales creados por el ser humano y toda la mecánica de funcionamiento de las máquinas. Su formulación matemática fue publicada por Isaac Newton en 1687 en su obra "Philosophiæ naturalis principia mathematica". 

La dinámica de Newton, también llamada dinámica clásica, solo se cumple en los sistemas de referencia inerciales (que se mueven a velocidad constante; la Tierra, aunque gire y rote, se trata como tal a efectos de muchos experimentos prácticos). Solo es aplicable a cuerpos cuya velocidad dista considerablemente de la velocidad de la luz; cuando la velocidad del cuerpo se va aproximando a los 300 000 km/s (lo que ocurriría en los sistemas de referencia no-inerciales) aparecen una serie de fenómenos denominados efectos relativistas. El estudio de estos efectos (contracción de la longitud, por ejemplo) corresponde a la teoría de la relatividad especial, enunciada por Albert Einstein en 1905.

La dinámica es la parte de la física que estudia las relaciones entre los movimientos de los cuerpos y las causas que los provocan, en concreto las fuerzas que actúan sobre ellos. La dinámica, desde el punto de vista de la mecánica clásica, es apropiada para el estudio dinámico de sistemas grandes en comparación con los átomos y que se mueven a velocidades mucho menores que las de la luz. Para entender estos fenómenos, el punto de partida es la observación del mundo cotidiano. Si se desea cambiar la posición de un cuerpo en reposo es necesario empujarlo o levantarlo, es decir, ejercer una acción sobre él.

Aparte de estas intuiciones básicas, el problema del movimiento es muy complejo: todos aquellos que se observan en la naturaleza (caída de un objeto en el aire, movimiento de una bicicleta, un coche o un cohete espacial) son complicados. Esto motivó que el conocimiento sobre estos hechos fuera erróneo durante siglos. Aristóteles pensó que el movimiento de un cuerpo se detiene cuando la fuerza que lo empuja deja de actuar. Posteriormente se descubrió que esto no era cierto pero el prestigio de Aristóteles como filósofo y científico hizo que estas ideas perduraran siglos, hasta que científicos como Galileo Galilei o Isaac Newton hicieron avances muy importantes con sus nuevas formulaciones. Sin embargo hubo varios físicos que se aproximaron de manera muy certera a las formulaciones de Newton mucho antes de que este formulara sus leyes del movimiento.

Es el caso del español Juan de Celaya, matemático, físico, cosmólogo, teólogo y filósofo que en 1517 publicó un tratado titulado "In octo libros physicorum Aristotelis cum quaestionibus eiusdem, secundum triplicem viam beati Thomae, realium et nominatium", obra de especial interés para el estudio de los orígenes de la moderna ciencia del movimiento. Durante su etapa en Francia fue un escritor prolífico, escribiendo sobre todo acerca de la física de Aristóteles y el movimiento. También publicó numerosos trabajos sobre filosofía y lógica. Fue uno de los impulsores de la lógica nominalista y de las ideas mertonianas de los calculatores acerca de la dinámica. Fue capaz de enunciar, dentro de las leyes de Newton, la primera ley de o primer principio de la dinámica (una de las leyes más importantes de la física) un siglo antes que Newton.

Otro destacado pionero fue el también español, y discípulo de Celaya, Domingo de Soto, fraile dominico y teólogo considerado como el promotor de la física moderna. Su teoría del movimiento uniformemente acelerado y la caída de los graves fue el precedente de la ley de la gravedad de Newton. Escribió numerosas obras de teología, derecho, filosofía y lógica y también comentó varios libros de física y lógica aristotélica, de los cuales el más importante fue "Quaestiones super octo libros physicorum Aristotelis" (1551), sobre cinemática y dinámica, la cual fue publicada en varias ciudades italianas, influyendo en personajes como Benedetti o Galileo. Domingo de Soto fue uno de los primeros en establecer que un cuerpo en caída libre sufre una aceleración uniforme con respecto al tiempo —dicha afirmación también había sido establecida por Nicolás Oresme casi dos siglos antes— y su concepción sobre la masa fue avanzada en su época. En su libro "Quaestiones" explica la aceleración constante de un cuerpo en caída libre de esta manera: 

Domingo de Soto ya relacionaba dos aspectos de la física: el movimiento uniformemente disforme (movimiento uniformemente acelerado) y la caída de graves (resistencia interna). En su teoría combinaba la abstracción matemática con la realidad física, clave para la comprensión de las leyes de la naturaleza. Tenía una claridad rotunda acerca de este hecho y lo expresaba en ejemplos numéricos concretos. Clasificó los diferentes tipos de movimiento en:




Soto describió el movimiento de caída libre como ejemplo de movimiento uniformemente acelerado por primera vez, cuestión que solo aparecerá posteriormente en la obra de Galileo:

Por lo tanto era aplicable la ley de la velocidad media para calcular el tiempo de caída:

Movimiento diformente disforme con respecto al tiempo:
Este fue un descubrimiento clave en física y base esencial para el posterior estudio de la gravedad por Galileo Galilei e Isaac Newton. Ningún científico de las universidades de París y Oxford de aquella época había conseguido describir la relación entre movimiento uniformemente disforme en el tiempo y la caída de los graves como lo hizo Soto.

Tras las ideas innovadoras sobre el movimiento de estos científicos, Galileo hizo un avance muy importante al introducir el método científico que enseña que no siempre se debe creer en las conclusiones intuitivas basadas en la observación inmediata, pues esto lleva a menudo a equivocaciones. Galileo realizó un gran número de experiencias en las que se iban cambiando ligeramente las condiciones del problema y midió los resultados en cada caso. De esta manera pudo extrapolar sus observaciones hasta llegar a entender un experimento ideal. En concreto, observó cómo un cuerpo que se mueve con velocidad constante sobre una superficie lisa se moverá eternamente si no hay rozamientos ni otras acciones externas sobre él.

Inmediatamente se presentó otro problema: ¿si la velocidad no lo revela, qué parámetro del movimiento indica la acción de fuerzas exteriores?; Galileo respondió también a esta pregunta, pero Newton lo hizo de manera más precisa: no es la velocidad sino su variación la consecuencia resultante de la acción de arrastrar o empujar un objeto. Esta relación entre fuerza y cambio de velocidad (aceleración) constituye la base fundamental de la mecánica clásica. Fue Isaac Newton (hacia 1690) el primero en dar una formulación completa de las leyes de la mecánica e inventó los procedimientos matemáticos necesarios para explicarlos y obtener información a partir de ellos.

El primer concepto que maneja Newton es el de masa, que identifica con «cantidad de materia». Newton asume a continuación que la cantidad de movimiento es el resultado del producto de la masa por la velocidad. En tercer lugar, precisa la importancia de distinguir entre lo absoluto y relativo siempre que se hable de tiempo, espacio, lugar o movimiento.

En este sentido, Newton, que entiende el movimiento como una traslación de un cuerpo de un lugar a otro, para llegar al movimiento absoluto y verdadero de un cuerpo:

De acuerdo con este planteamiento, establece que los movimientos aparentes son las diferencias de los movimientos verdaderos y que las fuerzas son causas y efectos de estos. Consecuentemente, la fuerza en Newton tiene un carácter absoluto, no relativo.

Las leyes enunciadas por Newton, y consideradas como las más importantes de la mecánica clásica, son tres: la ley de inercia, la relación entre fuerza y aceleración y la ley de acción y reacción. Newton planteó que todos los movimientos se atienen a estas tres leyes principales, formuladas en términos matemáticos. Un concepto es la fuerza, causa del movimiento y otro es la masa, la medición de la cantidad de materia puesta en movimiento; los dos son denominados habitualmente por las letras F y m.

La primera ley del movimiento rebate la idea aristotélica de que un cuerpo solo puede mantenerse en movimiento si se le aplica una fuerza. Newton expone que:
Esta ley postula, por tanto, que un cuerpo no puede cambiar por sí solo su estado inicial, ya sea en reposo o en movimiento rectilíneo uniforme, a menos que se aplique una fuerza o una serie de fuerzas cuya resultante no sea nula. Newton toma en consideración, así, el que los cuerpos en movimiento están sometidos constantemente a fuerzas de roce o fricción, que los frena de forma progresiva, algo novedoso respecto de concepciones anteriores que entendían que el movimiento o la detención de un cuerpo se debía exclusivamente a si se ejercía sobre ellos una fuerza, pero nunca entendiendo como tal a la fricción.

En consecuencia, un cuerpo que se desplaza con movimiento rectilíneo uniforme implica que no existe ninguna fuerza externa neta o, dicho de otra forma, un objeto en movimiento no se detiene de forma natural si no se aplica una fuerza sobre él. En el caso de los cuerpos en reposo, se entiende que su velocidad es cero, por lo que si esta cambia es porque sobre ese cuerpo se ha ejercido una fuerza neta.

Newton retomó la ley de la inercia de Galileo: la tendencia de un objeto en movimiento a continuar moviéndose en una línea recta, a menos que sufra la influencia de algo que le desvíe de su camino. Newton supuso que si la Luna no salía disparada en línea recta, según una línea tangencial a su órbita, se debía a la presencia de otra fuerza que la empujaba en dirección a la Tierra, y que desviaba constantemente su camino convirtiéndolo en un círculo. Newton llamó a esta fuerza gravedad y creyó que actuaba a distancia. No hay nada que conecte físicamente la Tierra y la Luna y sin embargo la Tierra está constantemente tirando de la Luna hacia nosotros. Newton se sirvió de la tercera ley de Kepler y dedujo matemáticamente la naturaleza de la fuerza de la gravedad. Demostró que la misma fuerza que hacía caer una manzana sobre la Tierra mantenía a la Luna en su órbita.

La primera ley de Newton establece la equivalencia entre el estado de reposo y de movimiento rectilíneo uniforme. Supongamos un sistema de referencia "S" y otro "S"´ que se desplaza respecto del primero a una velocidad constante. Si sobre una partícula en reposo en el sistema "S"´ no actúa una fuerza neta, su estado de movimiento no cambiará y permanecerá en reposo respecto del sistema "S"´ y con movimiento rectilíneo uniforme respecto del sistema "S". La primera ley de Newton se satisface en ambos sistemas de referencia. A estos sistemas en los que se satisfacen las leyes de Newton se les da el nombre de sistemas de referencia inerciales. Ningún sistema de referencia inercial tiene preferencia sobre otro sistema inercial, son equivalentes: este concepto constituye el principio de relatividad de Galileo o newtoniano.

El enunciado fundamental que podemos extraer de la ley de Newton es que 

Esta expresión es una ecuación vectorial, ya que las fuerzas llevan dirección y sentido. Por otra parte, cabe destacar que la variación con la que varía la velocidad corresponde a la aceleración.

La primera ley de Newton sirve para definir un tipo especial de sistemas de referencia conocidos como sistemas de referencia inerciales, que son aquellos desde los que se observa que un cuerpo sobre el que no actúa ninguna fuerza neta se mueve con velocidad constante.

Un sistema de referencia con aceleración (y la aceleración normal de un sistema rotatorio se incluye en esta definición) no es un sistema inercial, y la observación de una partícula en reposo en el propio sistema no satisfará las leyes de Newton (puesto que se observará aceleración sin la presencia de fuerza neta alguna). Se denominan sistemas de referencia no inerciales.

Por ejemplo considérese una plataforma girando con velocidad constante, ω, en la que un objeto está atado al eje de giro mediante una cuerda, y supongamos dos observadores, uno inercial externo a la plataforma y otro no inercial situado sobre ella.



En realidad, es imposible encontrar un sistema de referencia inercial, ya que siempre hay algún tipo de fuerzas actuando sobre los cuerpos; no obstante, siempre es posible encontrar un sistema de referencia en el que el problema que estemos estudiando se pueda tratar como si estuviésemos en un sistema inercial. En muchos casos, la Tierra es una buena aproximación de sistema inercial, ya que a pesar de contar con una aceleración traslacional y otra rotacional, ambas son del orden de 0.01 m/s² y, en consecuencia, podemos considerar que un sistema de referencia de un observador en la superficie terrestre es un sistema de referencia inercial.

Se puede considerar como ejemplo ilustrativo de esta primera ley una bola atada a una cuerda, de modo que la bola gira siguiendo una trayectoria circular. Debido a la fuerza centrípeta de la cuerda (tensión), la masa sigue la trayectoria circular, pero si en algún momento la cuerda se rompiese, la bola tomaría una trayectoria rectilínea en la dirección de la velocidad que tenía la bola en el instante de rotura.

Tras la rotura, la fuerza neta ejercida sobre la bola es 0, por lo que experimentará, como resultado de un estado de reposo, un movimiento rectilíneo uniforme.

La segunda ley de Newton expresa que:
Esta ley se encarga de cuantificar el concepto de fuerza. La aceleración que adquiere un cuerpo es proporcional a la fuerza neta aplicada sobre el mismo. La constante de proporcionalidad es la masa del cuerpo (que puede ser o no ser constante). Entender la fuerza como la causa del cambio de movimiento y la proporcionalidad entre la fuerza impresa y el cambio de la velocidad de un cuerpo es la esencia de esta segunda ley.

Si la masa del cuerpo es constante se puede establecer la siguiente relación, que constituye la ecuación fundamental de la dinámica:

Donde "m" es la masa del cuerpo la cual debe ser constante para ser expresada de tal forma. La fuerza neta que actúa sobre un cuerpo, también llamada fuerza resultante, es el vector suma de todas las fuerzas que sobre él actúan. Así pues:

El principio de superposición establece que si varias fuerzas actúan igual o simultáneamente sobre un cuerpo, la fuerza resultante es igual a la suma vectorial de las fuerzas que actúan independientemente sobre el cuerpo (regla del paralelogramo). Este principio aparece incluido en los "Principia" de Newton como Corolario 1, después de la tercera ley, pero es requisito indispensable para la comprensión y aplicación de las leyes, así como para la caracterización vectorial de las fuerzas.
La fuerza modificará el estado de movimiento, cambiando la velocidad en módulo o dirección. Las fuerzas son causas que producen aceleraciones en los cuerpos. Por lo tanto existe una relación causa-efecto entre la fuerza aplicada y la aceleración que este cuerpo experimenta. 

De esta ecuación se obtiene la unidad de medida de la fuerza en el Sistema Internacional de Unidades, el Newton:

</math>. Es decir, es una magnitud vectorial proporcional a la masa y a la velocidad del objeto. Partiendo de esta definición y aplicando la ley fundamental de la mecánica de Newton, las variaciones de la cantidad de movimiento se expresan en función de la fuerza resultante y el intervalo de tiempo durante el cual se ejerce esta:

Tomando el intervalo de tiempo de "t" a "t" e integrando se obtiene

Al vector I se le denomina impulso lineal y representa una magnitud física que se manifiesta especialmente en las acciones rápidas o impactos, tales como choques, llevando módulo dirección y sentido. En este tipo de acciones conviene considerar la duración del impacto y la fuerza ejercida durante el mismo.

De la expresión obtenida se deduce que el impulso lineal es igual a la variación de la cantidad de movimiento. Si la fuerza resultante es cero (es decir, si no se actúa sobre el objeto) el impulso también es cero y la cantidad de movimiento permanece constante. Llamamos a esta afirmación ley de conservación del impulso lineal, aplicada a un objeto o una partícula.

Sus unidades en el Sistema Internacional son formula_4



Entre las posibles aplicaciones de la Segunda Ley de Newton, se pueden destacar:



Si se aplica la segunda ley, en la dirección radial:

donde "a" representa la aceleración normal a la trayectoria. Conocido el valor de la velocidad "v" en la posición angular se puede determinar la tensión "T" del hilo. Esta es máxima cuando el péndulo pasa por la posición de equilibrio
}</math>
</math>


_{12})}{d^2} </math>
donde "d" la distancia entre las dos partículas y formula_7 es el vector director unitario que va de la partícula 1 a la 2. Análogamente, la fuerza de la partícula 2 sobre la partícula 1 es:
_{12}) )}{d^2} </math>
Empleando la identidad vectorial formula_8, puede verse que la primera fuerza está en el plano formado por formula_7 y formula_10 que la segunda fuerza está en el plano formado por formula_7 y formula_12. Por tanto, estas fuerzas no siempre resultan estar sobre la misma línea, ni en general son de igual magnitud 

El teorema de Ehrenfest permite generalizar las leyes de Newton al marco de la mecánica cuántica. Si bien en dicha teoría no es lícito hablar de fuerzas o de trayectoria, se puede hablar de magnitudes como momento lineal y potencial de manera similar a como se hace en mecánica newtoniana.

En concreto la versión cuántica de la segunda Ley de Newton afirma que la derivada temporal del valor esperado del momento de una partícula en un campo iguala al valor esperado de la "fuerza" o valor esperado del gradiente del potencial:
Donde:





</doc>
<doc id="9797" url="https://es.wikipedia.org/wiki?curid=9797" title="Jaime Nunó">
Jaime Nunó

Jaime Nunó Roca (en catalán "Jaume Nunó i Roca"; San Juan de las Abadesas, Gerona, Cataluña, España, 8 de septiembre de 1824 - Nueva York, Estados Unidos, 18 de julio de 1908), más conocido como Jaime Nunó, fue un compositor, concertista, director de orquesta y director de óperas español, célebre por haber musicalizado las estrofas escritas por Francisco González Bocanegra para dar origen al Himno Nacional Mexicano.

Tras la muerte de sus padres, quedó bajo la tutela de su tío Bernard, un comerciante de sedas de Barcelona que financió sus estudios musicales en la ciudad condal. Ahí demostró sus actitudes como solista en la catedral de la ciudad, tras lo cual se ganó una beca para estudiar con el compositor Saverio Mercadante en Italia contando con 17 años de edad.

Se casó con la viuda de Talo en 1848, y comenzó a componer misas, arias, motetes y piezas orquestales. Durante esa época, dirigió orquestas e impartió lecciones, y se fue especializando en bandas militares.

Es recordado especialmente como el creador del Himno Nacional Mexicano. Aunque ni nació ni falleció en México, permaneció en diversas ocasiones en dicho país y estuvo estrechamente vinculado a figuras políticas nacionales y episodios decisivos para el curso de la historia mexicana. A su regreso a Barcelona, fue nombrado director de la Banda del Regimiento de la Reina en 1851, y viajó con ellos a Cuba, donde conoció y trabó amistad con el ex presidente mexicano Antonio López de Santa Anna. Santa Anna regresó a México para ocupar de nuevo la presidencia, y en 1853 invitó a Jaime Nunó a encabezar las bandas militares mexicanas. Su llegada coincidió con la convocatoria al concurso nacional para componer el Himno Nacional Mexicano, en el cual se inscribieron quince aspirantes. En 1854, ganó el concurso convocado para componer la música del Himno Nacional, cuya letra había escrito el poeta mexicano Francisco González Bocanegra, y el 12 de agosto de ese año fue declarado triunfador. La partitura, ya con letra y música, se interpretó por primera vez el 15 o 16 de septiembre de ese mismo año, en el entonces llamado Teatro Santa Anna (luego llamado Teatro Nacional de México), que finalmente se demolió y se reemplazó por el Palacio de Bellas Artes.

La autoría de la música del Himno Nacional le convirtió en un prócer patrio de la historia mexicana, por lo que posteriormente, en 1942, sus restos mortales se llevaron a México y se depositaron en la Rotonda de los Hombres Ilustres (hoy Rotonda de las Personas Ilustres), un monumental panteón nacional de la Ciudad de México en el que se perpetúa la memoria de personajes ilustres mexicanos, al tiempo que se les rinden honores póstumos.

Tras la caída del presidente Santa Anna por la Revolución de Ayutla, Nunó decidió emigrar a los Estados Unidos, donde trabajó como concertista y director de óperas, una de las cuales lo llevó de gira por el continente americano en 1864. En 1873, se volvió a casar, esta vez con su discípula Catalina Cecilia Remington, con quien tuvo dos hijos: Cristina y Jaime.

Tras radicar un tiempo en España regresó a los Estados Unidos para establecerse en el estado de Nueva York, donde fue redescubierto por un periodista mexicano en 1901. Al conocerse la noticia en México, el entonces presidente Porfirio Díaz lo invitó a México, donde recibió varios homenajes entre 1901 y 1904. De esta manera, participó en las celebraciones del cincuentenario del Himno Nacional Mexicano.

Murió en Nueva York el 18 de julio de 1908. En octubre de 1942, el gobierno mexicano mandó exhumar sus restos para trasladarlos a la Rotonda de las Personas Ilustres ubicada en el "Panteón Civil de Dolores" de la Ciudad de México, donde aún reposan junto con los restos de Francisco González Bocanegra.

En el 2010, justo en la celebración del Bicentenario de la Independencia de México, los musicólogos catalanes Cristian Canton Ferrer y Raquel Tovar localizaron en los Estados Unidos al único descendiente directo de Jaime Nunó, su bisnieto, en Pelham, Nueva York. Este hallazgo permitió recuperar el fondo personal de Jaime Nunó, que incluía cerca de cinco mil documentos inéditos (cartas personales, partituras, documentos oficiales, etcétera), que llevó a Cantón y a Tovar a confeccionar su primera biografía completa, de gran repercusión mediática y descrita como ""un título fundamental para comprender la historia musical de México"". También en el contexto del redescubrimiento de la figura de Jaime Nunó, se reinauguró la casa-museo natal del compositor en San Juan de las Abadesas, conocida como "El Palmàs". A partir de la recuperación de esta documentación, la música inédita de Jaime Nunó ha vuelto a interpretarse, y está en proceso una grabación y la edición completa de sus obras, de la mano del sello catalano-británico Mozaic Editions. De este proyecto de recuperación de la obra de Nunó, ya se ha publicado el "Trisagio para coro y piano", de 1839, obra que, aunque se escribió quince años atrás, recuerda los acordes del Himno Nacional Mexicano.

La producción musical de Jaime Nunó abarcó casi todos los géneros, y se sabe que compuso alrededor de 500 obras, de las que sólo se han conservado un reducido número. El musicólogo catalán Cristian Canton Ferrer realizó, en el 2012, la primera catalogación rigurosa de la obra de Nunó, siguiendo la metodología grupal empleada anteriormente por Anthony van Hoboken en el catálogo de las obras de Franz Joseph Haydn. Habitualmente, las obras de Nunó son citadas según el catálogo Canton, y figura el grupo y el orden de la obra prefijado por la letra "C"; por ejemplo: ""Pequeña pieza de concierto", C.I/1".




</doc>
<doc id="9800" url="https://es.wikipedia.org/wiki?curid=9800" title="Diexismo">
Diexismo

Diexismo es la afición de escuchar emisoras de radio lejanas o exóticas. El nombre proviene del término telegráfico "DX", que significa distancia. El radioyente es el "diexista".

En el caso de tratarse de radioaficionados se entenderá que la comunicación se establece entre puntos geográficamente alejados. Cuando un radioaficionado transmite las palabras "CQ DX, CQ DX, CQ DX...", es que está buscando un contacto lejano y para colaborar, las emisoras cercanas deben abstenerse de contestar la llamada. En algunos países, los radioaficionados suelen llamar a los diexistas (DXers), "curuyas".

El DX se puede practicar en todas las bandas de frecuencia y clases de emisión. 

El DX en bandas altas como la VHF y la UHF y hasta la SHF también es posible, generalmente con alcances más modestos, del orden de algunos a varios cientos de kilómetros, aunque dicho alcance puede aumentar notablemente con técnicas como el "rebote lunar" o la "dispersión meteórica", o, con el empleo de antenas altamente directivas.

Más allá del equipamiento mínimo necesario y de la calidad del mismo, el éxito dependerá en gran medida de las condiciones de propagación (el número de manchas solares es uno de los elementos más influyentes), la frecuencia elegida, la hora del día o la estación del año, que permiten o no que las ondas de radio lleguen hasta el lugar donde estamos y de la eficacia de la antena receptora.

El equipamiento básico puede constar de un simple receptor de radio portátil. Es preferible que el sintonizador esté basado en la tecnología PLL para obtener una buena estabilidad en frecuencia y que el dial sea digital para poder determinar con precisión la frecuencia que se está sintonizando. Aunque aparatos antiguos con tecnología de válvulas también dan muy buen resultado, a veces incluso mejor que los más modernos.

Las autoridades francesas (en 2008, la ANFR) no confieren indicativos a los diexistas. Sin embargo, la asociación de radioaficionados francesa REF-Union confiere gratuitamente indicativos de la forma F-nnnnn (donde n es un dígito) a quienes los solicitan. Si bien no son indicativos oficiales, son comúnmente aceptados como válidos por la comunidad de radioaficionados.


Aunque menos frecuente también es posible hacer diexismo de televisión y de emisoras utilitarias como por ejemplo aeropuertos, aviones, estaciones costeras, meteorológicas, barcos y radiobalizas. 

La escucha de frecuencias profesionales puede constituir delito en distintos países. Existen casos en los cuales hasta la mera posesión de equipos de escucha capaces de barrer ciertas frecuencias es considerado delito. Es recomendable informarse sobre las reglas en vigor en el país donde se practica la escucha. 

Muchos diexistas toman su pasatiempo con mucha seriedad y lo convierten en un verdadero desafío. Se suele comenzar con emisoras conocidas que sirven de práctica y como punto de referencia para luego sintonizar emisoras cada vez más débiles y lejanas. Los conocimientos adquiridos por los diexistas han servido en muchos casos para solucionar problemas de recepción y transmisión en las comunicaciones.

Una vez efectuada la escucha, es usual que el diexista proceda a confeccionar un "informe de recepción" que, mediante el código SINPO (u otros, aunque éste es el más popular), envía a la emisora para hacer saber a ésta la calidad con que ha recibido su emisión. La emisora, en agradecimiento, suele contestar remitiendo una tarjeta QSL al oyente, que éste añadirá a su colección de "trofeos", valorados en función de la rareza del emisor.

El informe de recepción debe indicar día, hora de inicio y fin de la escucha, frecuencia de sintonía, situación geográfica del radioescucha, equipo receptor y antena empleados y un pequeño resumen de la escucha. Para evaluar técnicamente las características de la recepción, se emplea frecuentemente el código SINPO, evaluándose en una escala de 1 a 5 los siguientes aspectos:


Un SINPO 55555 equivale a una recepción perfecta.

Para reportar estaciones de CW (telegrafía) se suele usar el código RST.

Algunas emisoras que aceptan informes de recepción vía Internet:



</doc>
<doc id="9809" url="https://es.wikipedia.org/wiki?curid=9809" title="Ricardo Lagos">
Ricardo Lagos

Ricardo Froilán Lagos Escobar (Santiago, 2 de marzo de 1938) es un abogado, economista, académico, investigador y político chileno. Fue presidente de la República de Chile entre el 11 de marzo de 2000 y el 11 de marzo de 2006. Ha sido también enviado especial de la ONU para tratar el cambio climático.

Lagos ha sido una de las principales figuras de la Concertación de Partidos por la Democracia dado su carácter protagónico en éste. Como miembro fundador del Partido por la Democracia, Ricardo Lagos fue una de las principales figuras opositoras a la dictadura militar de Augusto Pinochet. Una vez logrado el retorno a la democracia, Lagos ejerció como ministro de Educación y de Obras Públicas durante las presidencias de Patricio Aylwin y Eduardo Frei Ruiz-Tagle.

En las elecciones presidenciales de 1999 ganó por un estrecho margen al candidato derechista Joaquín Lavín, siendo el primer ganador de una segunda vuelta en la historia electoral chilena. Su mandato presidencial comenzó con grandes problemas por los efectos económicos de la crisis asiática y diversos problemas de corrupción. Sin embargo, en la segunda mitad de su mandato, el crecimiento económico, la firma de tratados de libre comercio con Estados Unidos, China y la Unión Europea, entre otros, y los importantes avances en infraestructura, permitieron un importante repunte en su popularidad. En 2005, Lagos logró un acuerdo para la reforma de la Constitución Política chilena que data de 1980.

Entre los puntos polémicos de su gobierno está la respuesta dada por el Estado chileno a las reivindicaciones territoriales y de autodeterminación de los pueblos indígenas mapuches, su actuación en problemas de índole ambiental, el Caso MOP-GATE, la preparación de ciertas reformas que se implementaron en el gobierno siguiente de Michelle Bachelet como el Transantiago, y la instauración del Crédito con Aval del Estado (CAE).

Ricardo Lagos nació el 2 de marzo de 1938 en Santiago. Sus padres son Ema Escobar Morales –que murió en abril de 2005 a la edad de 108 años– y el agricultor Froilán Lagos Sepúlveda, quien falleció cuando Lagos tenía ocho años. Cursó los estudios básicos en el Colegio San Agustín y en el Liceo Manuel de Salas y los secundarios en el Instituto Nacional, al que ingresó gracias a los buenos oficios de su tía, Fresia Escobar Morales, una de las pioneras en el mundo político chileno.

En 1954, Ricardo Lagos ingresa a la Facultad de Derecho de la Universidad de Chile. Entre 1955 y 1959, da sus primeros pasos en política. Es elegido Presidente del Centro de Alumnos y pronuncia su primer discurso desde el mismo lugar donde minutos antes lo hiciera Salvador Allende ante el expresidente de Guatemala, Juan José Arévalo.

En 1960, Lagos termina su carrera de Derecho. Su memoria de título, ""La Concentración del Poder Económico"", es aprobada con distinción máxima y se convierte en un éxito editorial, con cinco ediciones publicadas. En ella, demuestra la existencia de grupos económicos, sugiriendo para terminar con ellos "la abolición de la propiedad privada de todos los medios de producción" y su paso al Estado. Su memoria le merece una entrevista en la afamada Revista Time y una editorial del diario La Nación, que lo llamó ""el Mozart de la economía"". Convertido en abogado se casó con Carmen Weber, con quien tuvo dos hijos, Ricardo y Ximena. Luego de doctorarse y ya de vuelta en Chile, anula su matrimonio.

En 1969 conoce a Luisa Durán de la Fuente, con quien se casa en segundas nupcias en 1971. Con Luisa comparten la crianza de los dos hijos del primer matrimonio de Lagos y de los dos hijos del primer matrimonio de Luisa, Hernán y Alejandro, y de Francisca, única hija común del matrimonio Lagos - Durán.

Doctorado en la Universidad de Duke, donde estudió entre 1961 y 1966. Entre 1962 y 1965 Lagos fue profesor de ciencias políticas en la Universidad de Carolina del Norte en Chapel Hill. De vuelta en Chile, se incorpora al Instituto de Economía de la Universidad de Chile, que dirigía Carlos Massad. Más tarde, en 1967, es nombrado Director de la Escuela de Ciencias Políticas y Administrativas (hoy Escuela de Gobierno y Gestión Pública), cargo que ejercerá hasta 1969, cuando se convierte en Secretario General de la Universidad de Chile. Recibió un doctorado honoris causa por parte de la Universidad Nacional Autónoma de México en abril del 2007.

Comienza a desempeñarse como profesor de economía en la Escuela de Derecho de la Universidad de Chile. Entre 1971 y 1972, es director del Instituto de Economía de la misma universidad; luego, es nombrado director del Consejo Latinoamericano de Ciencias Sociales y profesor visitante de la cátedra William R. Kenan de Estudios Latinoamericanos de la Universidad de Carolina del Norte Chapel Hill, en Estados Unidos.

Ricardo Lagos se declaró durante los años 1970 como un "independiente de izquierda" , luego que en 1961 abandonara el Partido Radical de Chile, cuando este decide entrar al gobierno de Jorge Alessandri Rodríguez. Sin mayor experiencia diplomática, trabajó con el embajador Hernán Santa Cruz como delegado ante Naciones Unidas, donde pronuncia un destacado discurso sobre la crisis financiera internacional. En aquella ocasión, critica duramente la decisión del Presidente Richard Nixon de decretar la no convertibilidad de los dólares al oro, medida que terminaría por rondar la crisis asiática. En 1972, el presidente Salvador Allende lo nombra embajador en Moscú, pero el Senado nunca lo ratificó. Como director regional del Programa de Estudios de Postgrado en Ciencias Sociales, está a cargo del Proyecto Unesco, UNDP en Buenos Aires. En lo público, sirve al país en Naciones Unidas como delegado con rango de embajador en la XXVI Asamblea General. Además, es delegado en la III Conferencia de Comercio y Desarrollo (UNCTAD) de la ONU.

Luego del golpe de Estado de 1973 parte al exilio político junto a su familia a Buenos Aires, donde ejerce el cargo de Secretario General de la Facultad Latinoamericana de Ciencias Sociales, FLACSO. Se traslada por un año a Estados Unidos, donde se convierte en profesor visitante de la cátedra William R. Kenan de Estudios Latinoamericanos de la Universidad de Carolina del Norte, Chapel Hill. En 1975 ejerce como consultor para el Programa de las Naciones Unidas para el Desarrollo, PNUD.

Regresa a Chile en 1978 en pleno régimen militar, contratado por el Programa Regional de Empleo de Naciones Unidas, PREALC. En medio de las políticas impuestas por el Fondo Monetario Internacional, su misión fue asesorar a todos los gobiernos del continente en materia de empleo. Además es representante en Chile del Servicio Universitario Mundial, WUS.

Durante la década de 1980, Ricardo Lagos asume un papel fundamental en la lucha por la recuperación de la democracia. El año 1983, decide abandonar su cargo de funcionario internacional en Naciones Unidas. En diciembre de ese mismo año, se convierte en presidente de la Alianza Democrática (AD), coalición opositora al Régimen Militar integrada por demócratacristianos, socialistas, radicales, socialdemócratas y derechistas moderados. Siendo uno de los líderes del Partido Socialista de Chile, Lagos integró la tendencia denominada los «"suizos"», la cual pretendía ser el nexo de unidad entre las distintas corrientes del socialismo chileno, dividido en «"renovados"» –moderados, participantes en la AD– y «"almeydistas"» –seguidores de Clodomiro Almeyda, más izquierdista, agrupados en el Movimiento Democrático Popular–. 

El 7 de septiembre de 1986, Lagos es detenido por la Policía de Investigaciones de Chile en las pesquisas luego del atentado contra Augusto Pinochet. Este hecho, según declaraciones del ex vocero del régimen militar Francisco Javier Cuadra, salvó a Lagos de morir asesinado junto con el periodista José Carrasco y otros cuatro opositores al régimen de Pinochet.

En 1987, como presidente del Comité de Izquierda por las elecciones libres, llama a la ciudadanía y a los partidos a inscribirse masivamente en los registros electorales a votar por la alternativa «No» en el plebiscito de octubre de 1988. Consecuente con esta convocatoria, es el líder fundador del Partido Por la Democracia (PPD), partido que originalmente se define como "instrumental", cuyo objetivo era realizar la campaña a favor de la opción «No» y contar con una red de apoderados que defendieran esta votación.

Ricardo Lagos se transformó en el líder indiscutido de los opositores del régimen de Pinochet, cuando participa el 25 de abril de 1988 en el programa político "De cara al país" de Canal 13, el que realizó un ciclo con los principales dirigentes de los partidos políticos legalmente inscritos, durante el capítulo correspondiente al Partido por la Democracia, al que asiste como su Presidente, en una actitud valiente para esos tiempos señala que el triunfo del «No» será «el inicio del fin de la dictadura» e «impedirá que el general Pinochet esté 25 años en el poder». Lagos mira a la cámara y levanta su índice para decirle directamente a todos los televidentes:

Tras el triunfo de la opción «No» y pese a su importante liderazgo en la oposición, pierde al interior de la Concertación la opción de ser candidato presidencial, ante el más moderado precandidato demócratacristiano Patricio Aylwin y postulando a una senaturía por la circunscripción Santiago Poniente. El 11 de diciembre de 1989, día de las elecciones, alcanza la segunda mayoría de la circunscripción. Sin embargo, no resulta elegido, pues su lista no alcanza a duplicar la votación de segunda lista más votada, requisito que impone el sistema electoral chileno.

El año 1990, el presidente Patricio Aylwin lo nombra ministro de Educación. Lagos inicia una reforma tendiente a lograr una mayor igualdad en el acceso y un alza en la calidad de la educación pública, intentando recuperar una educación pública que había sido un orgullo para Chile pero que había sido sistemáticamente disminuida y desarticulada durante el gobierno anterior.

En junio de 1993, es uno de los candidatos en la primera elección primaria presidencial de la Historia de Chile, celebrada en la Concertación de Partidos por la Democracia. En esa ocasión fue claramente derrotado, quedando fuera de la elección presidencial, por Eduardo Frei Ruiz-Tagle (del Partido Demócrata Cristiano), quien es elegido finalmente como presidente.
En 1994, el mismo Presidente Eduardo Frei Ruiz-Tagle lo nombra ministro de Obras Públicas, perfeccionando en este cargo el sistema de concesiones viales creado bajo el gobierno de Patricio Aylwin, integrando al sector privado en la construcción de las obras públicas y su posterior explotación.
Durante la presidencia de Eduardo Frei Ruiz-Tagle, continúa siendo líder de opinión y carta segura para la siguiente elección presidencial, lo que se ve ratificado por su nombramiento como uno de los integrantes del Comité de Doce Miembros Distinguidos de la Internacional Socialista, donde comparte con personalidades como Felipe González y Gro Harlem Brundtland. El comité tiene a su cargo la elaboración de propuestas para la renovación del pensamiento socialdemócrata para el siglo XXI.

En 1998 deja su labor de ministro de Obras Públicas para iniciar su campaña presidencial. Gana las primarias dentro de la Concertación al senador Andrés Zaldívar, del Partido Demócrata Cristiano, siendo proclamado candidato. Ésta fue la primera vez que en Chile se realizaron primarias abiertas, esto es, una elección en que podía votar cualquier ciudadano salvo que estuviese inscrito en partidos ajenos a la coalición.

En la elección presidencial de diciembre del mismo año superó a Joaquín Lavín (candidato de la Unión Demócrata Independiente), su contrincante más cercano, por treinta mil votos, lo que equivale a un voto por mesa. Al no haber logrado la mayoría absoluta, en enero de 2000, se realiza por primera vez en Chile la segunda vuelta electoral, para la cual Lagos reestructuró su comando de campaña, integrando al mismo a la Ministra de Justicia, Soledad Alvear (líder del Partido Demócrata Cristiano), para así asegurar el voto humanista cristiano. Finalmente, derrota al candidato de la derecha, Joaquín Lavín, con el 51,3% de los votos, convirtiéndose en el nuevo Presidente de la República de Chile, asumiendo el 11 de marzo de 2000. Es el único Presidente de la República que ha sido militante del Partido por la Democracia.

Durante el primer año de su mandato, debió hacer frente a un alto nivel de desempleo, generado por la inestabilidad política de la región, proceso que comenzó a revertirse durante finales de 2003. Pese a ello, Lagos ostentó un gran apoyo popular, que llegó a su punto máximo en los primeros meses de 2005, donde según diversas encuestas de opinión, su gobierno alcanzaba niveles superiores al 70% de aprobación, un nivel histórico. La política de cercanía con la gente se manifestó en la apertura de las puertas del Palacio de La Moneda, que estuvieron cerradas desde el golpe de estado para los transeúntes. Así mismo fue el primer presidente de Chile que recorrió todas las comunas del país.

Su Gobierno fue sacudido en el año 2001 por las acusaciones de cohecho en contra de Patricio Tombolini, Subsecretario de Transportes, como consecuencia de irregularidades en las Plantas de Certificación Técnica de Vehículos Motorizados, acusaciones por las que este último funcionario fue condenado judicialmente, siendo finalmente absuelto en forma completa por la Corte Suprema de Chile en el año 2007.

Desde 2002, su gobierno debió enfrentar las sospechas de corrupción política, debido al procesamiento de uno de sus ministros, Carlos Cruz, y de otros funcionarios del Ministerio de Obras Públicas, por el caso denominado MOP-Gate. La jueza que lleva la causa, Gloria Ana Chevesich detectó que en dicho Ministerio se encargaron asesorías a empresas externas, que funcionaron como fachada para el pago de asignaciones suplementarias a funcionarios del Ministerio. En otra arista de ese caso, y como consecuencia de una entrevista a Carlos Cruz, éste reconoció que ministros, subsecretarios y otros representantes de exclusiva confianza del presidente, recibían pagos adicionales a su remuneración, figura que se denominó como "sobresueldos". La irregularidad fue reconocida por el Presidente Ricardo Lagos, especificándose que la práctica se desarrolló también durante los gobiernos de Eduardo Frei Ruiz-Tagle y Patricio Aylwin, aunque se sospecha que ella es de larga data en Chile como forma de complementar las rentas de los funcionarios de mayor responsabilidad. La postura oficial del gobierno consistió en no reconocer características de delito en las prácticas y en establecer una reforma legal que aumentara los sueldos de ministros y subsecretarios de gobierno, materia que fue aprobada en su trámite legislativo.
Durante su gobierno fructificó la modalidad de las concesiones, en que el Estado, sin perder la propiedad de las obras que licita, las entrega para su ejecución y operación a consorcios privados. Además, durante su mandato se gestó el Transantiago, proyecto que renovó un sistema de transporte público anacrónico ("micros amarillas"); el nuevo sistema tuvo serios problemas en su diseño e implementación, los que han sido paulatinamente corregidos.

Dentro de su política social se creó un seguro de desempleo, pagado por el Estado, los empleadores y los trabajadores; la ley para reformar el sistema de salud mediante garantías explícitas a la atención (Programa AUGE); el programa de erradicación de campamentos (Chile Barrio); un programa de protección social para familias en situación de extrema pobreza (Chile Solidario); la implementación de la Jornada Escolar Completa, que sería duramente cuestionada por la movilización estudiantil de 2006 en Chile; la escolaridad obligatoria durante 12 años; la creación de una institucionalidad cultural central (Consejo Nacional de la Cultura y las Artes); y el ya mencionado plan de transporte público en Santiago llamado Transantiago.

Además durante su mandato se aprobó la primera ley de matrimonio civil que permitió el divorcio vincular en la historia de Chile; se inició la aplicación de la reforma procesal penal; se crearon los Tribunales de Familia, aplicando el procedimiento oral a estas materias para hacer más expedita su resolución; se aprobó la Ley de Financiamiento Estudiantil con Aval del Estado, con fuertes críticas del movimiento estudiantil; y se aprobaron una de las más amplias modificaciones a la Constitución de 1980, desde que entró en vigencia.

Todos los gobiernos de la Concertación generaron avances en materia de aclarar los crímenes cometidos durante la dictadura militar. Durante el gobierno de Aylwin se emitió el Informe Rettig que dio cuenta de los ejecutado políticos y detenidos desaparecidos. Con Frei, se crearon las mesas de diálogo en que las Fuerzas Armadas debieron entregar la información que mantenían sobre el paradero de los detenidos desaparecidos. Ricardo Lagos formó una comisión para establecer la magnitud de la tortura en Chile. El 28 de noviembre de 2004, el día anterior al lanzamiento del Informe Valech, el presidente Lagos anunció que el gobierno proveería compensación a aproximadamente 30.000 víctimas de violaciones de los derechos humanos bajo el régimen militar. De las 35.868 personas que testificaron ante la Comisión Nacional sobre Prisión Política y Tortura, aproximadamente 30.000 casos fueron considerados legítimos. El 15 de junio de 2005, Lagos ingresó al Congreso Nacional el proyecto de ley N° 20.405 que crea el Instituto Nacional de Derechos Humanos de Chile, el cual se constituiría en 2010.

Durante 2004 debió enfrentar una serie de tensiones en su relación con los países sudamericanos, provocadas por la antigua aspiración boliviana de salida al mar. La situación engarzó con la crisis energética sufrida por Argentina, quien provee de gas natural a Chile. En reuniones bilaterales entre Carlos Mesa, presidente boliviano y Néstor Kirchner presidente argentino, el primero condicionó la venta de gas boliviano a Argentina a que "ni siquiera una molécula de gas fuera vendida a Chile". A su vez, el presidente venezolano, Hugo Chávez, apoyó en diversas instancias la ambición marítima boliviana, produciéndose un "impasse" diplomático entre ambas naciones. La tensión entre gobiernos decreció durante julio de 2004. Anteriormente ya había habido tensión cuando el gobierno de Chile, a través de su embajada en Caracas, fue el único país del continente en reconocer el gobierno que se instaló por algunas horas mediante un golpe militar contra el presidente Hugo Chávez en 2002, que sería rápidamente frustrado.

Su mandato destaca por la firma de tratados de libre comercio, con la Unión Europea, Estados Unidos, China y Corea.

Finalmente, es importante mencionar la firme posición de Chile en el seno de las Naciones Unidas para evitar la invasión de Irak. Chile logró superar las múltiples presiones de EE. UU. y Reino Unido, y jugó un papel clave en el voto condenatorio de las Naciones Unidas a esta acción bélica.

El año 2001, Ricardo Lagos envió la "Ley Corta de Pesca" con una vigencia de 10 años en la cual se le aseguraban casi un 80% de las cuotas totales al holding empresarial del Grupo Angelini. La ley fue tramitada en el congreso gracias a la participación del por aquel entonces presidente del senado, Andrés Zaldívar, junto a su hermano Adolfo Zaldívar (por entonces, presidente del Partido Demócrata Cristiano). La ley permitió que las acciones de Itata aumentaran en más de un 200% en menos de dos años. Posterior a ello, el año 2002, se extendió la LMCA a las regiones I y II, lugar en donde la principal empresa pesquera corresponde a Eperva, también perteneciente al grupo Angelini. Nuevamente gracias al apoyo de Andrés Zaldívar junto a Julio Lagos y Sergio Bitar, la ley fue promulgada en menos de 18 meses. La intervención por parte de los hermanos Zaldívar fue duramente criticada debido a la gran inversión que tenían en dicha empresa, sin contar que el presidente del directorio de Eperva era el Felipe Zaldívar Larraín, tercer miembro de la familia Zaldívar.

Durante el gobierno de Ricardo Lagos, el por entonces presidente del Banco Estado Jaime Estévez, fue increpado por participar en el préstamo de 120 millones de dólares al grupo Luksic en el proceso de compra del Banco de Chile.

Andrónico Luksic Craig, líder del consorcio empresarial Grupo Luksic (consorcio que posee la mayor fortuna económica de Chile) fue citado por Ricardo Lagos durante una entrevista al diario La Tercera:

Los actos de corrupción acontecidos durante su gestión son uno de los aspectos más críticos de su gobierno. La connotación de ello no ha sido menor, dado que el Índice de Percepción de Corrupción en Chile en general es bajo, especialmente en relación con sus vecinos sudamericanos.

Los casos MOP-Gate, MOP-Ciade, CORFO-Inverlink, EFE, fueron los casos de corrupción que sacudieron su gobierno. Estas malas prácticas se expandieron en los altos niveles de la administración pública y en ellos participaron colaboradores muy cercanos al ex mandatario. En enero de 2003 se ordena la detención del Ministro de Obras Públicas, Transportes y Telecomunicaciones, Carlos Cruz. La investigación llevada adelante por la jueza Gloria Ana Chevesich contabilizó innumerables aristas como MOP-Prograf, MOP-Idecon, MOP-Cycsa, MOP-Délano y MOP-Gesys, entre otras, las cuales versaron sobre irregularidades que van desde la falsificación de instrumentos públicos y licitaciones preacordadas hasta el fraude al fisco, pasando por el pago de sobresueldos y el desvío de fondos públicos, involucrando a decenas de personas. Finalmente, en julio de 2010 se dictó sentencia a 14 personas, entre ellas Cruz, que fue condenado a tres años de pena remitida y una multa de más de 799 millones de pesos. Otro de los inculpados, el ex jefe de finanzas del MOP, Sergio Cortés, deberá purgar cinco años de presidio remitido, pero con libertad vigilada.

Durante el gobierno de la Presidenta Michelle Bachelet, se le ha responsabilizado (por parte de la oposición y del mismo gobierno) por los inconvenientes que ha generado el sistema de transporte público denominado Transantiago, el que ha drenado recursos estatales, lo que ha causado molestias en la oposición y en la mayoría de la ciudadanía.

Tras la Ocupación de la Araucanía, una zona que comprende los terrenos ubicados al sur del Río Biobío, el Estado comenzó a entregar las tierras de las comunidades mapuches a terceros (1883-1930), en el proceso conocido como "reducción". Desde esa época, las distintas generaciones de mapuches han intentado por distintas vías, la recuperación de tales territorios considerados ancestrales. A partir de 1997 comenzaron una serie de ocupaciones, tomas y atentados incendiarios en los predios pertenecientes a empresas forestales (principalmente las dependientes de COPEC y CMPC) y consideradas por muchas comunidades como territorio ancestral. La respuesta estatal a esta situación se ejecutó en su mayor parte durante el gobierno de Ricardo Lagos.

Se sindicó a la Coordinadora de Comunidades en Conflicto Arauco-Malleco, como una organización de carácter terrorista y fue perseguida como tal, encarcelándose a sus dirigentes en procesos cuya legalidad ha sido cuestionada por grupos mapuche, organizaciones de derechos humanos chilenas e internacionales y el Relator Especial para Pueblos Indígenas de las Naciones Unidas. Ejemplos paradigmáticos de esta situación lo constituye el llamado "Caso loncos" -donde los lonkos Pascual Pichun y Aniceto Norin fueron condenados a 5 años y 1 día de prisión por "amenaza de incendio terrorista" y el "Caso Puluco-Pidenco" -donde cuatro comuneros fueron condenados a 10 años y un día de prisión por "incendio terrorista". Estos casos han sido descritos por el Relator Especial para Pueblos Indígenas de las Naciones Unidas Rodolfo Stavenhagen, como juicios que presentan una legalidad cuestionable. Los hechos fueron denunciados a la Comisión Interamericana de Derechos Humanos, por infracción al debido proceso, reconocido en la Convención Americana de Derechos Humanos, entre otros fundamentos, que decretó su admisibilidad. Por otra parte, el Comité de Derechos Humanos, que vigila el cumplimiento del Pacto Internacional de Derechos Civiles y Políticos, en sus observaciones al informe de Chile, también impugnó las prácticas contra el movimiento mapuche. En este sentido, instó al Estado chileno a modificar la Ley N° 18.314, sobre conductas terroristas (conocida como ley antiterrorista).

Durante su gobierno se produjo el desastre del río Cruces, provocado por la empresa productora de celulosa CELCO propiedad de Celulosa Arauco, en que se afectó seriamente el Santuario de la Naturaleza Carlos Anwandter probablemente por el exceso de dioxinas, haciéndose emblemático el caso por la masiva muerte y migración de los cisnes de cuello negro. Ante esta situación la autoridad ambiental ordenó que la empresa rebajara en un 20% su producción y que presentara en el plazo de 2 años una alternativa de descarga de los RILES. La opción que propuso la empresa fue lanzarlos al mar a través de un emisario submarino frente a las costas de Mehuín lo que fue señalado por Lagos como "la única alternativa posible". Sin embargo la oposición de pescadores y comunidades mapuche lafkenche del sector ha impedido que se lleven a cabo los estudios pertinentes, aún después de terminado el gobierno de Lagos.

En los últimos días de su período, durante el mes de febrero, la Comisión Nacional del Medio Ambiente dio su aprobación al cuestionado proyecto minero binacional Pascua Lama, presentado por la trasnacional canadiense Barrik Gold, ello una vez concluidos los procedimientos legales que norman la participación ciudadana en la "Evaluación de Impacto Ambiental" a los que son sometidos este tipo de proyectos. La mina se ubica debajo de los tres glaciares que alimentan el valle del Río Huasco, último río vivo en el Desierto de Atacama.

Por otro lado, Ricardo Lagos fue criticado por algunos y aplaudido por otros por denunciar lo que él consideraba una gran concentración y falta de pluralidad de los medios de comunicación impresos, en manos de dos grandes consorcios históricamente vinculados a los poderes económicos y a la oposición. En septiembre del 2005, Lagos envió una carta a Agustín Edwards Eastman, director del periódico chileno El Mercurio, en la que señala que ""Ha terminado el suyo siendo un diario al servicio de una tribu, la tribu que desea sembrar el odio a través de los que escriben su página editorial y la tribu de los que quieren atacar no importa por cuáles medios"".

Sin embargo, durante el gobierno de Ricardo Lagos, nunca se discutió ni se puso como tema de discusión el dinero brindado por el Estado a los principales medios de prensa escrito del país. El Estado chileno subsidió directamente a El Mercurio y a Copesa (consorcio de prensa escrita al cual pertenece el diario "La Tercera"), asignándole 80 % de los recursos publicitarios destinados a la prensa escrita durante 2005.

Otro caso emblemático se sucedió cuando Lagos con indignación criticó al programa "Contacto" (del entonces canal católico de la televisión abierta chilena; Canal 13) debido a que en un episodio se mostró algunos aspectos negativos de lo que fueron los planes Puente. El ex mandatario consiguió salir al aire después de la transmisión para entregar su visión.

Después de que abandonó el poder, Lagos será responsable de un seminario especial en la Universidad de California, Berkeley de un mes en el Centro para Estudios latinoamericanos, llamados ""Democracia y Desarrollo en Latinoamérica"".

El 24 de marzo de 2006, Lagos anuncia el lanzamiento de una fundación propia llamada ""Democracia y Desarrollo"" en Santiago, desde donde se organizará hasta el día de hoy una intensa agenda internacional. Tres días después viaja a Estocolmo, donde asume como Presidente del Club de Madrid por dos años más, una organización exclusiva de antiguos presidentes creada para promover más y mejor democracia a través del mundo, de la cual aún forma parte. También asume la Mesa Directiva de co-presidencia del Diálogo Inter-Americano.

En mayo de 2007, la Universidad de Brown anuncia que Lagos tendría un cargo docente en el Instituto de Watson para Estudios Internacionales por un periodo de cinco años, que fue alargado por dos años, empezando el 1 de julio de 2007. El 1 de mayo de 2007 fue designado por el Secretario General de la ONU, Ban Ki-moon, enviado especial para el cambio climático de la ONU, junto con la exprimera ministra noruega Gro Harlem y el ex canciller surcoreano, Hang Seung-soo. Este nombramiento ha sido criticado en una carta abierta firmada por veinte ONG ecológicas destinada a Ban Ki-moon. En la carta las organizaciones critican el pobre desempeño del gobierno de Lagos en asuntos medioambientales como el tema del proyecto Pascua Lama y la muerte de Cisnes de Cuello Negro, entre muchos otros problemas. Lagos ha rechazado tales críticas aludiendo a que dichos expertos medioambientales "no saben nada" e indicando con ambigüedad que desde su punto de vista el tema se relaciona simplemente con que "la situación ambiental de Chile se debe entender en el contexto de su veloz ritmo de desarrollo".

En diciembre de 2007, el expresidente pidió excusas públicamente a los millones de santiaguinos afectados por la planificación de la reforma al transporte metropolitano, denominado Transantiago.

En 2008 la Fundación Konex de la Argentina le otorgó el Premio Konex Mercosur como una de las personalidades más relevantes de la década en la región.

Para las Elecciones del 2009, se le pidió varias veces que fuera como candidato, dado que según los sondeos él era el posible candidato que se acercaba más al aventajado Sebastián Piñera, desde inicios del 2008, Lagos dio varios sí y varios no, pero siempre decía que se negaría a competir en primarias con su colega José Miguel Insulza, y menos dentro de la Concertación con el expresidente Eduardo Frei Ruiz-Tagle o la senadora Soledad Alvear, decidió mantenerse neutral ante esta situación. El 4 de diciembre de 2008 confirmó que no sería candidato para el 2009 y desde entonces se mantuvo neutral completamente frente al tema.

Continuó con una intensa agenda internacional, participando en encuentros como la Cumbre de Delhi sobre Desarrollo Sostenible, en febrero de 2011 
(Delhi Sustainable Development Summit) y en el 10º aniversario y Conferencia Anual del Club de Madrid, noviembre de 2011.
Además forma parte de la organización internacional Crisis Group y participa del Foro Iberoamericano.

El 27 de diciembre de 2011, se reunió con el presidente Sebastián Piñera para tratar diferentes temas entre ellos le planteó el fin del sistema binominal ya que lo calificó como un cáncer que debe ser cambiado de raíz y sin "cálculos mezquinos". En consecuencia, junto al binominal, hay distintas fórmulas para poderlo remplazar", fue enfático en aclarar que está disponible a sentarse a analizar la reforma al sistema electoral, pero partiendo de la base que el sistema "se acabó".

En noviembre del 2012, Ricardo Lagos fue invitado a una conferencia magistral en la Cátedra Latinoamericana Julio Cortázar: América Latina Hoy. Desafíos después de la crisis; Universidad de Guadalajara. En los mismos días, en la Feria del Libro de México, participó en el reconocimiento a Carlos Fuentes, escritor mexicano fallecido unos meses antes. 

En 2013, fue invitado por la Universidad de Sao Paulo, a dar inicio a la Cátedra José Bonifacio, Brasil.

El 2 de septiembre de 2016 Lagos publicó una declaración pública en la que anunció su disposición a repostularse como candidato a la presidencia de su país para enfrentar los «profundos cambios» que se han experimentado en el mundo y en el país. En dicha declaración sostuvo que «si chilenas y chilenos consideran que nosotros podemos llevar adelante una propuesta de avance y progreso y que entregue a las nuevas generaciones un Chile fortalecido, yo no me restaré a ese desafío».

El 10 de abril de 2017, luego de que las encuestas arrojaran alrededor de sólo un 3% de las preferencias, y sumado al hecho que el Partido Socialista de Chile finalmente decidió apoyar a Alejandro Guillier para las primarias de la Nueva Mayoría, Lagos decidió declinar su candidatura presidencial, afirmando que «He puesto todo mi empeño por llevar este mensaje político a los chilenos, pero veo también que en mi propio espacio político, la centroizquierda, no se ha provocado una convergencia en torno a este proyecto».



Resultado de las elecciones de 1999 para la Presidencia de la República

Segunda vuelta



</doc>
<doc id="9816" url="https://es.wikipedia.org/wiki?curid=9816" title="Linfoma de Burkitt">
Linfoma de Burkitt

El linfoma de Burkitt o leucemia de células de Burkitt es una rara forma de cáncer del sistema linfático—asociado principalmente a linfocitos B—que afecta predominantemente a gente joven, descrita más frecuentemente en África central, aunque también lo ha sido en otras áreas del mundo. La forma vista en África parece estar asociada con la infección del virus de Epstein Barr, aunque el mecanismo patogénico es desconocido. El epónimo proviene del cirujano Denis Parsons Burkitt quien, trabajando en el África ecuatorial, describió la enfermedad en 1956.

El linfoma de Burkitt resulta de una característica translocación cromosómica que afecta al gen "Myc". Una translocación cromosómica significa que el cromosoma se ha roto, lo que permite su unión con otras partes cromosómicas. En el linfoma de Burkitt afecta al cromosoma 8 (locus del "gen Myc"), lo que cambia el patrón de expresión del "gen Myc" alterando su función natural de control en el crecimiento y proliferación celular. La variante más frecuente produce una traslocación del cromosoma 8 al 14—t(8;14)(q24;q32)—mientras que otras variantes incluyen traslocaciones a otros cromosomas—t(2;8)(p12;q24) y t(8;22)(q24;q11). Se ha identificado también una traslocación mucho menos frecuente entre tres cromosomas, t(8;14;18).

El linfoma de Burkitt se clasifica en tres variantes clínicas, la endémica, la esporádica y la asociada a la inmunodeficiencia:




Morfológicamente es virtualmente imposible distinguir estas tres variantes clínicas. El linfoma de Burkitt asociado a la inmunodeficiencia puede verse con una apariencia más plasmática o con más pleomorfismo, aunque estas no son características específicas.
El linfoma de Burkitt bajo el microscopio consiste en población monótona de capas celulares de tamaño medio con una gran actividad proliferativa y apoptótica. La apariencia se asemeja a una noche de estrellas, por razón de las inclusiones esparcidas de los macrófagos que han digerido las partes celulares muertas. Las células del tumor tienen un tamaño muy similar al de los histiocitos o células endoteliales, de modo que no son células muy grandes, sino de mediano tamaño. Las células tumorales tienen una pequeña cantidad de citoplasma que se tiñe basofílica. Los contornos celulares tienen la apariencia de ser cuadriláteras.
Las células B normales poseen genes para la cadena pesada y la cadena ligera de las inmunoglobulinas. Cada célula B tiene su configuración única de cadenas pesadas y livianas y que son únicas en cada célula individual. Sin embargo, como las células de Burkitt provienen de procesos proliferativos, cada célula tumoral de un paciente tiende a poseer genes idénticos de la cadena pesada. Por ello, cuando se analiza una electroforesis con sangre de un paciente con linfoma de Burkitt, aparece una banda clonal constituida por genes de la cadena pesada de la Ig que han migrado a una misma posición. Otras enfermedades infecciosas como la mononucleosis infecciosa carecen de esta banda clonal electroforética.



</doc>
<doc id="9817" url="https://es.wikipedia.org/wiki?curid=9817" title="Virus de Epstein-Barr">
Virus de Epstein-Barr

El virus de Epstein-Barr (abreviado VEB) es un virus de la familia de los herpesvirus (familia que también incluye el virus del herpes simple y el citomegalovirus). Es la mayor causa de la mononucleosis aguda infecciosa, síndrome común caracterizado por fiebre, garganta irritada, fatiga extrema y ganglios linfáticos inflamados. La infección por el virus de Epstein-Barr se da en todo el mundo.

El VEB infecta a la mayor parte de la gente en algún momento de sus vidas. De esta forma se obtiene una inmunidad adaptativa a través del desarrollo de anticuerpos contra el virus, lo que suele prevenir nuevos contagios por factores externos. El virus queda latente por el resto de la vida (como episomas), pudiendo desencadenar nuevas infecciones, reactivándose intermitentemente con o sin síntomas.

Muchos niños se infectan con el virus de Epstein-Barr, aunque estas infecciones no suelen desarrollar una sintomatología grave y no se distinguen de otras enfermedades breves de la infancia. Cuando la infección con el VEB ocurre durante la adolescencia o la juventud, causa una mononucleosis infecciosa en un 35 a 70 % de los casos.

El virus de Epstein-Barr fue reportado por primera vez en 1964 por los científicos británicos M.A. Epstein, Y.M. Barr y B.G. Achong, quienes encontraron partículas virulentas en células de tejidos con un cáncer linfático recientemente descubierto. El virus de Epstein-Barr se conoce por infectar solo dos tipos de células en el cuerpo humano: algunas células de las glándulas salivales y los glóbulos blancos o leucocitos.

Para su transmisión se requiere un estrecho contacto personal y se transmite a través de la saliva, en la que se mantiene activo durante varias horas. Por ello, a la mononucleosis se la conoce también como «enfermedad del beso» o «fiebre de los enamorados». En los grupos humanos en condiciones de hacinamiento, la infección se difunde de forma precoz. La eliminación del virus, sin que el individuo tenga síntomas, puede ocurrir varios meses después de la infección. El período de incubación es de 30 a 50 días.

La mayor parte de las infecciones por virus de Epstein-Barr en los niños o adolescentes son asintomáticas y se presentan como una faringitis con o sin amigdalitis. Por el contrario, en los adultos el 75 % de los casos presentan mononucleosis infecciosa. El período de incubación de la mononucleosis en los adultos jóvenes es de 4 a 6 semanas antes de que comiencen a manifestarse los síntomas. La fatiga, malestar y mialgia comienzan a manifestarse 1 a 2 semanas antes de que aparezca la fiebre y el dolor de garganta. La fiebre no suele ser demasiado intensa. La linfadenopatía se observa preferentemente en los ganglios cervicales, pero otros muchos pueden estar afectados. 

En un 5 % de los pacientes se desarrolla un sarpullido popular, generalmente en brazos y tórax, sobre todo en sujetos que han recibido ampicilina o amoxicilina. Sin embargo, este sarpullido no es predictivo de una futura alergia a las penicilinas. Muchos enfermos padecen estos síntomas durante 2 a 4 semanas, pero el malestar general y la fatiga pueden durar meses.

Es frecuente la presencia de hepatoesplenomegalia con aumento de niveles séricos de enzimas hepáticas, por lo que es prudente solicitar un perfil hepático para orientar el diagnóstico.

Las complicaciones incluyen meningitis, encefalitis y síndrome de Guillain-Barré. Excepcionalmente puede producirse la rotura del bazo, debido a su inflamación durante el curso de la infección. Puede haber disminución de las plaquetas (sangramientos), glóbulos rojos, glóbulos blancos, inflamación de los testículos (orquitis), y del corazón (miocarditis). También se ha relacionado este virus con el Síndrome de Fatiga Crónica, ya que algunos de los afectados por dicho síndrome presentan pruebas positivas para este virus.

El caso típico de mononucleosis infecciosa con anticuerpos heterófilos positivos es bastante fácil de diagnosticar. Más complicada es la situación cuando las manifestaciones clínicas son atípicas o cuando los anticuerpos heterófilos son negativos. La causa más frecuente de mononucleosis infecciosa con anticuerpos heterófilos negativos es la infección por citomegalovirus (CMV). Ambos cuadros son muy parecidos e incluso en muchas ocasiones los títulos de anticuerpos frente al CMV están también elevados en una mononucleosis por virus de Epstein-Barr. La infección por CMV suele producir menos dolor de garganta y con frecuencia solo cursa con astenia y fiebre. 

La hepatitis por virus de la hepatitis A puede ir acompañada de linfocitosis atípica similar a la MI, si bien las transaminasas están mucho más elevadas. Otras infecciones que se presentan con cuadros parecidos a los de MI son la rubéola (si bien esta última con la erupción cutánea típica), la toxoplasmosis aguda y sobre todo la infección por herpes virus 6.

El tratamiento de la mononucleosis infecciosa consiste en reposo y alivio del malestar. El paracetamol y el ibuprofeno alivian la fiebre y el dolor de garganta. Debido a una posible inflamación del bazo, debe evitarse un exceso de actividad física durante el primer mes para prevenir la posibilidad de una rotura esplénica. Aunque se han usado corticoides (prednisona 40-60 mg/día durante dos o tres días con reducción de las dosis en la semana siguiente) para evitar la obstrucción de las vías respiratorias en los pacientes con hipertrofia tonsilar, estos no se recomiendan ya que pueden originar superinfecciones.

El aciclovir no ha mostrado ningún impacto significativo sobre la mononucleosis infecciosa aunque in vitro inhibe la replicación del virus.




</doc>
<doc id="9819" url="https://es.wikipedia.org/wiki?curid=9819" title="Pierre de Fermat">
Pierre de Fermat

Pierre de Fermat (Beaumont-de-Lomagne, Francia; 17 de agosto de 1601-Castres, Francia; 12 de enero de 1665) fue un jurista y matemático francés apodado por el historiador de matemáticas escocés, Eric Temple Bell, con el remoquete de «príncipe de los aficionados».

Fermat fue junto con René Descartes y Johannes Kepler uno de los principales matemáticos de la primera mitad del siglo XVII.

Joseph-Louis Lagrange afirmó claramente que él consideraba a Fermat como el inventor del cálculo. Fermat fue cofundador de la teoría de probabilidades junto a Blaise Pascal e independientemente de Descartes, descubrió el principio fundamental de la geometría analítica. Sin embargo, es más conocido por sus aportaciones a la teoría de números en especial por el conocido como último teorema de Fermat, que preocupó a los matemáticos durante aproximadamente 350 años, hasta que fue demostrado en 1995 por Andrew Wiles ayudado por Richard Taylor sobre la base del Teorema de Shimura-Taniyama.

Fermat es uno de los pocos matemáticos honrados como epónimo de un asteroide, que lleva la especificación nominal de (12007) Fermat. También se le ha dado la denominación de Fermat a un cráter lunar de 39 km de diámetro.

La mansión del siglo XV donde nació es en la actualidad un museo. La escuela más antigua y prestigiosa de Toulouse se llama Pierre de Fermat y en ella se imparten clases de ingeniería y comercio. Está situada entre las diez mejores de Francia para clases preparatorias. Cursó estudios de Derecho en las Universidades de Toulouse, Burdeos y Orleans, donde se graduó en 1631.Cabe destacar que Fermat estudió y analizó las matemáticas en sus tiempos libres ya que él tenía otra profesión.

También conocida como espiral parabólica, es una curva que responde a la siguiente ecuación en coordenadas polares:

Es un caso particular de la espiral de Arquímedes.

Dos números amigos son dos números naturales "a" y "b" tales que "a" es la suma de los divisores propios de "b", y "b" es la suma de los divisores propios de "a". (La unidad se considera divisor propio, pero no lo es el mismo número.)

En 1636, Fermat descubrió que 17.296 y 18.416 eran una pareja de números amigos, además de redescubrir una fórmula general para calcularlos, conocida por Tabit ibn Qurra, alrededor del año 850.

Un número de Fermat es un número natural de la forma:
donde "n" es natural.

Pierre de Fermat conjeturó que todos los números naturales de esta forma con "n" natural eran números primos, pero Leonhard Euler probó que no era así en 1732. En efecto, al tomar "n"=5 se obtiene un número compuesto:

El teorema sobre la suma de dos cuadrados afirma que todo número primo "p", tal que "p"-1 es divisible entre 4, se puede escribir como suma de dos cuadrados. El 2 también se incluye, ya que 1+1=2. Fermat anunció su teorema en una carta a Marin Mersenne fechada el 25 de diciembre de 1640, razón por la cual se le conoce también como "Teorema de navidad de Fermat"

El "pequeño teorema de Fermat", referente a la divisibilidad de números, afirma que, si se eleva un número "a" a la "p"-ésima potencia y al resultado se le resta "a", lo que queda es divisible por "p", siendo "p" un número primo con "a" y "p" coprimos. Su interés principal está en su aplicación al problema de la primalidad y en criptografía.

Pierre de Fermat acostumbraba a escribir las soluciones a los problemas en el margen de los libros. Una de las notas que escribió en su ejemplar del texto griego de la "Arithmetica" de Diofanto de Alejandría (editada por Claude Gaspard Bachet de Méziriac en 1621) dice lo siguiente:

Esta afirmación, más tarde ya conocida como "Último teorema de Fermat", se convirtió en uno de los teoremas más importantes en matemáticas. No se sabe si Fermat halló realmente la demostración, ya que no dejó rastro de ella para que otros matemáticos pudiesen verificarla. Este problema matemático mantuvo en vilo a los matemáticos durante más de tres siglos (se dice que, frustrado, Euler incluso pidió a un amigo que registrara de arriba a abajo la casa de Fermat en busca de la demostración), hasta que en 1995 Andrew Wiles ayudado por Richard Lawrence Taylor pudo demostrar el teorema. Wiles utilizó para ello herramientas matemáticas que surgieron mucho después de la muerte de Fermat, de forma que éste debió de encontrar la solución por otro camino, si es que lo hizo. En cualquier caso, tenía razón.

Hombre erudito y embebido en la cultura clásica grecorromana, era enciclopédico por la amplitud de su bagaje. Hacía anotaciones en los márgenes de los libros que leía, con observaciones y esbozos de demostraciones. No era matemático profesional ni escribía libros. Era de su interés el saber humano de su tiempo. Envía cartas de sus hallazgos o inquietudes, tuvo como mentor y difusor al padre Mersenne, y, en vez de formalizar sus descubrimientos o inventos, posiblemente se dedicaba a especular y daba vuelo a su imaginación desbordante; lanzaba retos mediante problemas cuya solución poseía. Polemizó con Descartes sobre el caso de "La Dioptrique" obra de este. Ante la incomodidad de Descartes, Fermat envió una prueba, haciendo presente que más le importaba la verdad no la fama ni la envidia.




</doc>
<doc id="9820" url="https://es.wikipedia.org/wiki?curid=9820" title="Mahjong">
Mahjong

El mahjong (/maˈdʒoŋ/), mah jong, mah-jongg (), es un juego de mesa de origen chino, exportado al resto del mundo, y particularmente a Occidente, a partir de 1920. En chino también se le conoce como gorrión.

Su antecesor recibía el nombre de «juego de hojas en tiras», ya que las fichas eran de cartulina, como los naipes actuales. Progresivamente se abandonó este material y se empezaron a fabricar fichas de marfil, madera y, sobre todo, bambú, aunque actualmente lo normal es utilizar el plástico, más duradero y barato. No obstante, se siguen fabricando, como se hizo en la antigüedad, verdaderas obras de arte en distintos materiales.

Según parece, el mahjong es un descendiente directo de un antiguo oráculo que hace miles de años consultaban los adivinos chinos. Cuando los astrónomos empezaron a registrar las progresiones del Sol, la Luna y los planetas, utilizaron un mecanismo sencillo, un tablero, para calcular las posiciones de los cuerpos celestes. El movimiento a través de los cielos se registraba moviendo unos contadores alrededor de las divisiones del tablero. Este, u otro parecido, es posiblemente también el origen de juegos muy difundidos, como el parchís, o la oca. Pero precisamente en el mahjong resultan reconocibles algunos restos de este origen, como por ejemplo en el hecho de que los puntos cardinales se encuentren invertidos, ya que se trata de representar un mapa celeste, no terrestre, o que se repartan trece fichas, que son los meses del calendario lunar.

De todas formas, la historia del mahjong está poco clara y abundan los antecedentes poco documentados. Por ejemplo, una de las leyendas sobre su origen afirma que el juego fue inventado por Confucio alrededor del año 500 a. C. Según esto, las fichas de los tres dragones, Rojo, Verde y Blanco -en chino, respectivamente, , y -, representarían las virtudes confucianas de benevolencia, sinceridad y piedad filial. El dragón Rojo haría referencia a China (). También según esta leyenda, Confucio habría sido un enamorado de los pájaros, lo que explicaría el nombre de "gorrión" que también recibe el juego.

Un juego, más o menos emparentado con el actual mahjong, fue inventado en la dinastía Tang, durante los años del reinado del emperador Tai Zong (626-649), para diversión de la casa imperial y la nobleza. Sin embargo, no hay evidencia de la existencia del mahjong antes de la época de la Rebelión Taiping, a mediados del siglo XIX. El consenso general es que el juego fue desarrollado alrededor de 1850 basándose en juegos de cartas y de dominó ya existentes. Muchos historiadores creen que se basó en un juego de cartas llamado o de comienzos de la dinastía Ming. Este juego se jugaba con cuarenta cartas numeradas del 1 al 9 en nueve palos con cuatro cartas extra de flores, de manera similar al actual mahjong. Según algunos, el juego habría sido creado por oficiales del ejército durante la Rebelión Taiping para pasar el tiempo. Según otra teoría, habría sido creado por un noble que vivía en los alrededores de Shanghái entre 1870 y 1875. Otros creen que fue obra de dos hermanos que vivían en la ciudad de Ningpo.

Pero en realidad, el mahjong tal y como lo conocemos hoy en día tiene una historia bastante más corta, pues se remonta al final de la China imperial a principios del siglo XX; este es el llamado "viejo estilo".

La primera vez que se hizo mención del mahjong en una lengua distinta al chino fue en 1895, en un artículo del antropólogo americano Stewart Culin y para 1910 ya había varios escritos en otras lenguas, como francés o japonés. En Estados Unidos, Joseph Park Babcock escribió un libro llamado "Rules of Mah-Jongg" ("Reglas del Mah-Jongg"), que en una versión simplificada de 1920 era conocido como "el libro rojo", aunque estas simplificaciones se abandonarían posteriormente. El juego tuvo a partir de los años veinte un gran éxito en Inglaterra y Estados Unidos, donde se dio a conocer con nombres comerciales como "Pung Chow" o "Game of Thousand Intelligences", formando parte de la moda por todo lo oriental, dedicándosele también canciones de éxito, como "Since Ma is Playing Mah Jong", de Eddie Cantor. Era jugado sobre todo por mujeres. En los años treinta, en Estados Unidos, se hicieron varias revisiones de las reglas hasta que en 1937 se creó la "Liga Nacional de Mah Jongg" ("National Mah Jongg League", NMJL) y se estandarizó el reglamento con el libro "Maajh: The American Version of the Ancient Chinese Game" ("Maajh: la versión americana del antiguo juego chino"). Aunque en los años veinte había sido un juego aceptado por personas de todo tipo de razas, a partir de esta oficialización fue considerado racistamente como un juego judío, debido a que muchos de sus jugadores lo eran; e incluso la NMJL fue considerada como una organización judía, producto del antisemitismo. La versión occidental del juego es llamada "nuevo estilo", aunque parece ser que el origen de estas modificaciones proviene de Pekín y Shanghái, por lo que a veces a este nuevo estilo se la denomina "estilo Shanghai".

Sea como fuere, lo indudable es que actualmente la mayor parte de los jugadores se encuentran en Taiwán y Estados Unidos, ya que durante décadas estuvo prohibido en la China comunista. Sin embargo, con las reformas políticas emprendidas a principios de los años 90, el mahjong ha pasado a ser considerado deporte oficial desde 1998 en la China continental y, aunque nunca dejó de jugarse en las casas, ahora vuelve a ser habitual la estampa de los jugadores de mahjong en las calles de las poblaciones chinas. Junto con la despenalización de su práctica, el gobierno chino editó un reglamento oficial que es el que rige en los campeonatos mundiales oficiales, el primero de los cuales se celebró en Tokio en 2002. De esta forma, se ha pretendido recuperar un patrimonio cultural de China que estaba siendo usurpado por otras naciones asiáticas.

El mahjong ha adquirido una popularidad enorme en toda Asia, de modo que muchos países lo consideran su juego nacional. Existen muchas variantes adaptadas a cada país, como la japonesa, la coreana, la vietnamita o la filipina, y resulta normal que cualquier acto festivo, celebración, comida, o incluso negocio, acabe con unas partidas de mahjong. También existe una variante israelí.

El mahjong es un juego para cuatro jugadores y raras veces se practica con un número distinto de participantes. No obstante, también pueden jugarlo tres, aunque el juego pierde interés. Por tanto, siempre supondremos que hablamos del juego para cuatro jugadores.

Uno de los aspectos que hemos de considerar es el origen chino del mahjong, lo cual representa de por sí, a ojos de los occidentales, un atractivo, por dotar de cierto exotismo al juego, principalmente en lo que se refiere al diseño de las fichas y a la terminología empleada, que en varios casos utiliza los términos originales chinos.

Entre las virtudes del mahjong, que sin duda han contribuido a popularizarlo tan extraordinariamente, destacan las siguientes:


Cada jugador participa de modo individual: no hay ningún sistema que permita establecer equipos. El juego se desarrolla en partidas sucesivas, cada una de las cuales solamente puede ser ganada por uno de los cuatro jugadores; siempre que finaliza una partida el ganador recibe puntos de los tres perdedores. Al final se hace un recuento de puntos, y se determina la situación de cada uno. Aunque esto puede hacerse apuntando en un papel los puntos de cada jugador, lo mejor es proveerse de fichas que simbolicen los puntos, a modo de moneda.

Para jugar al mahjong es imprescindible procurarse un juego de fichas específicas. Si además contamos con otros elementos la partida será mucho más agradable, pero se puede prescindir de ellos. Por ejemplo es recomendable contar con atriles para colocar las fichas (uno para cada jugador); un conjunto de fichas para contabilizar los puntos (pueden servir las que se usan para la ruleta, o cualquier otro juego); una mesa cuadrada, o redonda; dos dados; y cuatro discos o etiquetas con las inscripciones o símbolos de los cuatro vientos (Este, Sur, Oeste y Norte), para mostrar en cada momento la ronda que se está jugando.

Los atriles, dados y símbolos de los cuatro vientos a veces se incluyen con el juego básico de fichas, también llamadas tejas. A veces también se usan unas regletas auxiliares que ayudan a la formación de la muralla.

El mahjong, en su versión original, está compuesto de 144 fichas o tejas genéricas, pero req. 136 fichas para empezar sin las flores o estaciones. Las fichas son:
Dependiendo de los sets, es posible usar flores o estaciones, que no se repiten. En las versiones de 3 jugadores, eliminan el viento norte y una versión provincial china elimina las fichas de honor. Los sets coreanos se eliminan las fichas bambú o dejan solo las fichas 1 y 9 de la misma clase. Los sets japoneses raramente utilizan flores o estaciones. Además, en los sets japoneses usan fichas rojas, remplazando a algunas fichas originales.

Una vez que los jugadores han tomado asiento, en el orden que deseen, se procede a asignar a cada uno un viento o punto cardinal, cuyo orden no es el habitual en occidental, sino Este - Sur - Oeste - Norte, en sentido contrario de las agujas de un reloj. Esta ordenación, aparentemente caprichosa, responde a que los puntos cardinales de un mapa están al revés, como si en el cielo hubiera un mapa transparente y nosotros, que lo vemos desde el suelo, mirásemos los puntos cardinales del revés. Evidentemente, no es necesario tener en cuenta nada de esto, basta con seguir el orden indicado como una "regla del juego", que en el fondo es lo que resulta ser.

Para determinar qué jugador es el Este se tiran dos dados, y quien obtenga mayor puntuación queda investido como Viento del Este. Los otros jugadores quedan entonces también determinados, según se ha explicado anteriormente. Es decir, el jugador de su derecha es el Sur, el de enfrente es el Oeste, y el de su izquierda es el Norte.

Como curiosidad, ha de decirse que la forma original de determinación del Viento del Este es mucho más compleja y elaborada entre jugadores chinos, con vientos provisionales que sirven para determinar luego los definitivos. Sin embargo, como el propósito es el mismo, entendemos el sistema descrito como perfectamente válido.

Llamaremos "partida" a cada uno de los episodios en los cuales se mueven y reparten las fichas, y que termina o bien cuando algún jugador da fin al mismo haciendo mahjong o bien cuando se acaban las fichas de la muralla. Cada partida tiene dos vientos que le afectan, el "viento dominante" (o "viento de ronda") y el "viento propio" (o "viento de asiento"). La primera partida tiene como viento dominante el Este. Se llama "viento propio" al viento que le correponde a cada jugador. El viento dominante se cambia cada cuatro partidas (o una ronda). A continuación de la ronda del viento Este, comenzaría la ronda del Sur, y así sucesivamente. Por tanto, podemos hacer el siguiente esquema provisional (suponiendo que el jugador A es aquel al que le tocó ser Este por primera vez):

Conseguir tener una combinación del viento dominante o del viento propio es importante para calcular la puntuación (más tarde se verá cómo). Cuando en una partida coinciden para un jugador el viento dominante y el viento propio, se dice que ese viento es "viento doble", lo que constituye una circunstancia muy favorable; en la primera ronda, para el jugador Este, el Este es viento doble.

Por tanto, según el esquema anterior habría cuatro rondas de cuatro partidas cada una. Sin embargo, si una partida es ganada por el jugador que era Este, dicho jugador repitiría en la siguiente como Este, y así tantas veces como fuera necesario, por lo que las rondas pueden tener más de cuatro partidas.

Llamaremos "juego" al conjunto de las cuatro rondas. A estas alturas ya sabemos que un juego tiene cuatro rondas, y cada ronda al menos cuatro partidas, por lo que un juego tiene un mínimo de dieciséis partidas.

Antes de empezar a jugar una partida de Mahjong debe acordarse el número de rondas que se van a jugar o el tiempo que va a durar el juego. Normalmente se suelen acordar 2 o 4 rondas, y si se establece por tiempo, 90 o 120 minutos.

En cuanto a la duración, una partida puede ser muy rápida, o no tanto. Teóricamente, podría dar la casualidad de que uno de los jugadores cerrase con las fichas que se le reparten (aunque la probabilidad es pequeñísima), pero lo habitual es tardar unos 20 minutos en cada partida. Así que lo más normal es que un juego de 4 rondas dure aproximadamente unas 2 o 3 horas, dependiendo de la rapidez de los jugadores a la hora de jugar y de construir la muralla.

Una vez situadas boca abajo todas las fichas sobre la mesa, los jugadores procederán a mezclarlas bien para que no queden rastros de ordenaciones anteriores. A continuación se construirá la denominada "muralla", utilizando todas las fichas, formada por cuatro paredes, cada una de 18 fichas de longitud (17 si no se emplean las flores y estaciones) y dos fichas de altura. Todas estas fichas estarán boca abajo. Las cuatro hileras se dispondrán como lados de un cuadrado, en el centro del cual se van dejando las fichas que se descartan.

El jugador que es viento Este toma dos dados y los tira; con el número obtenido, cuenta, en sentido antihorario y empezando por sí mismo, por qué lado de la muralla se cogerán las fichas. Por ejemplo, supongamos que el viento Este tira los dados y obtiene un 7, se cogerán las fichas por el lado del jugador Oeste (1 Este, 2 Sur, 3 Oeste, 4 Norte, 5 Este, 6 Sur y 7 Oeste).

Una vez que se sabe el lado de la muralla por el que se empezarán a coger las fichas, el jugador de ese lado del muro contará desde su esquina derecha de la muralla tantas hileras hacia la izquierda como el número que había salido en los dados; en nuestro ejemplo, el Oeste se detendría en la hilera 7; (como cada hilera es doble, en ese momento hay 14 fichas desde la esquina derecha hasta ese lugar). A partir de esa ficha, el jugador Este toma cuatro, y se las pone delante (es decir, se reparte a él el primero), que corresponderían en nuestro ejemplo a las hileras 7 y 8; luego el Sur se cogería otras dos hileras (la 9 y la 10), etc.; cuando cada jugador haya cogido cuatro fichas, se repite esta misma operación dos veces más, de modo que cada uno tenga 12 fichas; a continuación, cada jugador se coge una ficha más, siempre comenzando por el Este, con lo que todos tienen 13 fichas, y por último, el Este toma una última ficha para él solo, con lo que el viento Este queda con 14 fichas, y los demás con 13. El hueco dejado por las fichas que se han repartido se llama "brecha", poreso a veces el reparto inicial de tejas se denomina "abrir la brecha de la muralla". El juego ha comenzado.

Por último, contando 7 hileras desde la brecha hacia la derecha (es decir, serían las 14 fichas del final), estas fichas se denominan Muro muerto, y sirven para coger las fichas extra tras hacerse Kongs o mostrar una flor o estación. Las fichas del Muro muerto no se juegan por lo que si en la muralla no quedasen más fichas la partida se daría por concluida.

Una vez repartidas las fichas, el jugador que es viento de asiento ha quedado con 14 fichas en la mano. Si casualmente resultase que las fichas que le han correspondido forman mahjong lo diría de inmediato y esa partida habría finalizado; pero esto es muy raro, así que ha de intentar ir buscando mejores fichas. (Obsérvese que de momento no nos preocupamos de explicar cómo se hace mahjong, y, por tanto, no sabemos qué fichas son buenas ni malas, eso no importa de momento).

Entonces, el primer jugador descartará una de las fichas que tiene en su mano, y la pondrá sobre la mesa, boca arriba, en el espacio central bordeado por la muralla. Esto significa que ha terminado su turno, y comienza el del jugador de su derecha, el cual procederá a robar la primera ficha de la muralla (la situada a la izquierda de la brecha recién abierta), será la ficha inferior de su fila, ya que la ficha de arriba ha sido repartida. Tras robar, queda con 14 fichas, y entonces (salvo que haya formado un mahjong, con lo cual terminaría la partida), localiza una ficha que quiera descartar, lo hace, y pasa el turno, y así sucesivamente.

Es importante señalar que los jugadores van tomando siempre la primera ficha libre de la muralla, es decir, la situada a la izquierda de la brecha; aunque no se ha mencionado hasta ahora, queda claro que se toman las fichas de arriba abajo, es decir, primero la ficha de arriba, luego la de abajo, luego la de arriba de la hilera siguiente, etc. Este principio por el que los jugadores van tomando por turno una ficha de la muralla queda afectado por algunas excepciones que se indican más adelante, las cuales permiten que un jugador pueda tomar la ficha descartada por otro, y que el orden de juego se altere, saltándose a algún jugador.

A medida que se vaya desarrollando la partida van a irse consumiendo progresivamente las fichas de la muralla. Generalmente la partida terminará cuando cualquiera de los cuatro jugadores consiga colocar sus fichas haciendo mahjong, pero si se diera el caso de que se consumieran todas las fichas válidas sin que ninguno lo hubiera conseguido, se comenzará una nueva partida, rotando los vientos asignados a cada jugador. Se consideran "fichas válidas" todas menos las 14 últimas que formaban el Muro muerto, es decir, en el momento que un jugador deja 14 fichas en la muralla y no ha podido hacer él mismo mahjong, la partida ha finalizado sin vencedor. Téngase en cuenta que, aunque se va robando siempre en sentido de las agujas del reloj, es decir, siguiendo la brecha abierta en la muralla cuando se dieron las fichas al inicio, a veces se toman fichas del otro extremo de la muralla (esto ocurre cuando se hace "kong", como más tarde se verá), con lo cual las últimas 14 fichas de la muralla pueden ir "moviéndose" a lo largo de la partida.

Una vez finalizada la partida, y determinado el ganador y los perdedores, se procede al recuento de puntos que debe recibir el primero por parte de cada uno de los perdedores; esta operación se realiza con la ayuda de todos, ya que aunque la rivalidad entre estos sea grande mientras compiten, es normal colaborar para agilizar esta fase del juego, que es la más tediosa, especialmente si los jugadores son novatos.

Si el ganador fue el jugador que era Este, los vientos no se rotan. En otro caso, los vientos se rotan en sentido antihorario. Es decir, el Este pasaría a ser Norte, el Sur sería Este, el Oeste sería Sur y el Norte sería Oeste. Si no ganó nadie (se agotaron las fichas válidas), los vientos no se rotan.

Tras esto, se mezclan las fichas, y se comienza una nueva partida.

Para poder seguir explicando el desarrollo del juego necesitamos introducir estos tres conceptos. Un pung, pon o trío (碰, "pèng") está formado siempre por tres fichas idénticas. Por ejemplo: tres dragones blancos, tres vientos Oeste, tres 7 de bambúes, etc. Un chow, chii o escalera (吃, "chī") está formado por tres fichas consecutivas del mismo palo. Es decir, son chow los conjuntos (1 discos-2 discos-3 discos), (7 bambúes-8 bambúes-9 bambúes), (4 caracteres, 5 caracteres, 6 caracteres), etc. Un kong, kan o cuarteto (槓, 杠, "gàng") está formado por cuatro fichas idénticas (los cuatro dragones blancos, los cuatro 7 de discos…), es decir, es lo mismo que un pung pero con cuatro fichas en lugar de tres. Sin embargo, las formaciones pung y chow son "normales" y la formación kong es "excepcional", como más tarde se verá. Obsérvese que no se pueden formar chow con los vientos, solamente es posible usarlos para formar pung o Kong; lo mismo sucede con los dragones.

Cuando un jugador tiene en su mano un pung, chow o kong, que ha conseguido ya sea bien porque se lo repartieron entre sus fichas iniciales, o que ha logrado a base de descartar y robar fichas de la muralla, se dice que estas formaciones son "ocultas". Una combinación oculta permanece así hasta que el jugador hace mahjong, es decir, no manifiesta al resto de jugadores que las tiene, excepto en el caso del kong. Si un jugador tuviese un kong y no dijera nada, no podría hacer mahjong, puesto que le faltaría una ficha para poder formar cuatro combinaciones y el par de ojos (ver el apartado siguiente). Por eso, si un jugador tiene un kong oculto, ha de declararlo antes de que finalice la partida (antes de finalizar cualquiera de sus turnos). Para declarar un kong oculto, en cualquier momento dentro de su turno, el jugador baja ante sí las cuatro fichas del kong, y recibe una teja adicional, la cual se toma no del primer lugar libre de la muralla, sino del Muro muerto (esta zona de la muralla se llama también "caja de kong" por este motivo). El kong que se ha bajado sobre la mesa se pone de forma que queden boca arriba las dos tejas de los extremos, y vueltas boca abajo las dos centrales (o a la inversa); este kong tiene la consideración de "kong oculto", a pesar de que está sobre la mesa, puesto que ha sido confeccionado sin ayuda de otros jugadores. Otro modo equivalente de hacer esto es dejar tres fichas boca arriba y la del extremo derecho boca abajo.

Antes de empezar a practicar, los set japoneses recomiendan ver la serie televisiva animada "Akagi", para un mejor entendimiento de las reglas.
El objetivo de todos los jugadores en cada partida es siempre el mismo: hacer mahjong. Esto se consigue cuando, tras robar una nueva ficha de la muralla o pedida de un descarte (es decir, estando el jugador con 14 fichas en la mano), estas quedan distribuidas en cualquiera de las combinaciones que se consideran "mahjong". Por tanto, un mahjong es una combinación de 14 fichas, distribuidas de este modo: cuatro combinaciones cualquiera (pung, kong o chow), y una pareja de fichas idénticas (llamadas "el par de ojos"). Hay algunas combinaciones excepcionales que son mahjong y no responden a este esquema, como se menciona más abajo, pero de momento podemos considerar que siempre tendremos cuatro combinaciones y un par de ojos. Es importante señalar que los kong funcionan como los pung, pero cuando un jugador hace un kong recibe una teja extra, pues si no, no podría realizar las otras combinaciones. Así pues, son combinaciones válidas para mahjong las siguientes: (la cantidad de fichas indicadas de cada mano no incluye las flores y estaciones)

Normales

Especiales

El mahjong con más fichas tiene cuatro kong y el par de ojos (18 fichas).

Una variante del juego es jugar limitando la cantidad de chow permitidos a uno, de esta manera se agrega dificultad al juego y las siguientes combinaciones no serían válidas:


Como se ha dicho anteriormente, cada jugador en su turno roba de la muralla (con lo cual momentáneamente tiene 14 fichas), y luego, si no tiene mahjong formado, descarta una, volviendo al número de 13. Pero muchas veces le gustaría tomar la ficha que alguien ha descartado en lugar de tomar una de la muralla, lo cual podrá hacer de esta manera:

Si cualquier jugador descarta una ficha, y otro tiene en la mano dos fichas idénticas a la descartada, puede reclamar para sí la ficha descartada, diciendo "pung" ("pon" en los sets japoneses). Al hacerlo, el juego interrumpe la secuencia normal, y se prosigue el turno por el jugador que reclamó el descarte, quien debe descartarse entonces de una ficha, continuando luego normalmente. Pongamos un ejemplo: Norte roba de la muralla y se descarta de un 4 de caracteres. Le tocaría jugar al Este, pero el Oeste tiene en la mano dos 4 de caracteres, y dice "pung". En ese momento, el Oeste toma de la mesa el 4 de caracteres y la coloca a la vista junto a las dos que tenía ocultas y descarta otra ficha. Prosigue jugando de nuevo el Norte (que es el jugador siguiente al Oeste), etc.

Observaciones importantes:

Esta regla se puede modificar optando que en cuanto el jugador siguiente robo de la muralla se perdió el derecho a reclamar el "pung", esto incrementa la dificultad del juego y hace que se esté muy atento al mismo.

Las reglas son similares al caso del pung, pero diciendo "kong" ("Kan" o Quad" en los sets japoneses y americanos, respectivamente) en lugar de "pung". Un kong también tiene preferencia sobre el chow. Cuando se consigue un kong de este modo, pidiendo una ficha descartada, el kong que resulta es de tipo "expuesto", y se pone frente al jugador con todas las tejas vistas durante el resto de la partida.

Si un jugador se descarta de una ficha, solo podrá reclamarla para formar un chow con ella el jugador siguiente; para hacerlo, dirá la palabra "chow" ("chi" en los set japoneses), y en lugar de robar de la muralla tomará la ficha recién descartada. Es necesario que este jugador tenga ya otras dos fichas del chow que está formando, y como en el caso del pung, volverá frente a él el chow, que queda entonces "expuesto" el resto de la partida.

Observaciones:

Cuando un jugador está a falta de una sola ficha para hacer mahjong, puede pedirla en caso de que la descarte cualquier jugador; lo hará diciendo "mahjong", tomará la ficha de la mesa, y la partida habrá concluido, pasándose a calcular la puntuación. Si una ficha es deseada por otros jugadores para pung, kong o chow, tiene prioridad aquel que la solicite para hacer mahjong. Si son varios los jugadores que reclaman la teja para hacer mahjong, la conseguirá el que esté situado primero respecto de quien descartó la ficha, contando siempre en el sentido del juego (es decir, en sentido antihorario). Obsérvese que para hacer mahjong podemos pedir cualquier ficha, incluso aunque sea para el par de ojos.

Una vez determinado el jugador que ha hecho mahjong en una partida, se contarán los puntos que ha conseguido, que le serán pagados por los otros tres jugadores; sin embargo, los otros tres jugadores no pagan los mismos puntos entre sí, ya que hay que tener en cuenta quién descartó la ficha que sirvió al ganador para hacer mahjong (si es que no ha ganado robando de la muralla). Un perdedor paga doble si descartó la ficha que sirvió para hacer mahjong al ganador.
Una variante es obviar esta penalización por el descarte, evitando que en el caso de que el ganador sea el viento reinante el que descarto la ficha pague cuádruple (doble por la penalización y doble por ganar el viento reinante de la vuelta).

Pongamos algunos ejemplos (los puntos básicos conseguidos todavía no sabemos calcularlos, así que los fijaremos arbitrariamente):

Caso 1

Caso 2

Caso 3

Recordemos que, además, en todos los casos si el ganador había repartido las fichas (es decir, si era "viento de asiento"), el hecho haber ganado le da derecho a repetir de nuevo, y esto tantas veces como sea capaz de ganar.

Resaltemos que lo primero que ha de hacerse es la conversión de los faan obtenidos en puntos, y luego duplicar estos puntos si procede. Por ejemplo, si se obtienen 6 faan, esto se corresponde con 16 puntos, y si algún jugador debe pagar doble, pagará 32 puntos.

Bien, pero aún queda por ver el modo que permite calcular lo que hemos llamado "puntuación básica" de la jugada. Este cálculo no se hace directamente en puntos, sino en "fan", y de aquí una tabla nos da la puntuación directamente.

La tabla de pagos es la siguiente:

Por ejemplo, si calculamos que una jugada tiene 5 fan, y mirando en la tabla observamos que estos 5 fan corresponden con 16 puntos, que sí será ya la puntuación básica, a la que aplicaremos las correcciones de duplicación descritas, si procede. Nos resta la descripción del sistema de asignación de fan a un mahjong concreto; esta operación condiciona todo el juego, ya que los jugadores procurarán conseguir unas u otras jugadas teniendo en cuenta lo valiosas que sean. Hay una serie de observaciones y reglas que pueden mencionarse:

En el apartado que sigue se explica el sistema de asignación de faan. Por último, es importante resaltar que un ganador de mahjong puede obtener muy pocos puntos, o cientos en una sola partida, dependiendo de lo afortunada que sea, por lo que al final un jugador que cierre muy pocas veces pero con combinaciones difíciles puede obtener muchos más puntos que otro que cierre a menudo con combinaciones de baja puntuación.

Las reglas que a continuación se exponen van ordenadas en número de fan. Estas reglas no son excluyentes entre sí, y de hecho se aplicarán todas las que sea posible a cada mahjong; se van indicando los faan obtenidos, y a continuación las circunstancias que propician este logro. Por otra parte, hay que tener en cuenta que cuando se hable de combinaciones vistas u ocultas nos referiremos siempre a pung, chow o kong, el par de ojos no entra en esta consideración.







Las fichas de flores y estaciones son fichas suplementarias que solo sirven para sumar puntos.

Cuando un jugador roba una flor de la muralla, inmediatamente la vuelve frente a sí, en la zona en que se colocan las combinaciones vistas, y vuelve a tomar otra ficha (ya que la flor no cuenta para hacer ninguna combinación, y de no hacerlo así se quedaría con una ficha de menos); la nueva ficha que reemplaza a la flor se tomará del muro muerto, no del lugar normal de donde se toman las fichas. Si recibe una segunda flor, se repite esto mismo, etc.

Nada más repartirse las fichas, al inicio del juego, cada jugador descubre las flores que ha recibido (si ha recibido alguna, claro está), e irán cogiendo por orden (primero el Este, luego el Sur, después el Oeste y por último el Norte) las fichas que reemplazan a las flores, tomándolas siempre del muro muerto, y empezando por sí mismo; si entre las nuevas fichas hay flores se repite la operación, como en el caso anterior se ha dicho.

El resto del juego es de la misma manera, así que el único efecto de las flores es incrementar los faan obtenidos.

Es relativamente frecuente, sobre todo entre principiantes, cometer algún error o infracción. La más grave consiste en cantar mahjong, y cuando se está calculando la puntuación, descubrir que no se había obtenido; a esto se llama "falso mahjong", y la pena consiste en que quien lo cometió paga los puntos correspondientes a 2 faan a cada uno de los otros tres jugadores, sin aplicarse entonces reglas que puedan duplicar esta cifra. La partida se repite, sin que cambie el viento de asiento. Aunque parezca una regla dura, es conveniente aplicarla sin dudar.

Otro error corriente entre novatos, cometido generalmente por descuido al robar de la muralla, consiste en quedar con un número mayor o menor de fichas de las que se deberían tener. Estos casos se llaman "gran eunuco" y "pequeño eunuco", respectivamente. Aquí no hay penalización, salvo si el jugador canta mahjong, que necesariamente sería falso, simplemente el "eunuco" no puede ganar la partida. Por eso, los jugadores en esta circunstancia están interesados en llegar a partida nula, es decir, procurarán que se agoten todas las fichas válidas de la muralla.

En los otros errores conviene ser indulgente; así, si un jugador dice "pung" y luego resulta que no tiene las dos fichas necesarias, bastará con deshacer la jugada y continuar normalmente, etc.

Las reglas que se han explicado aquí son las hongkonesas o cantonesas, las más sencillas, por lo que son convenientes para empezar a jugar al mahjong.

Sin embargo, existen otras reglas, normalmente más complejas, que también están muy extendidas:





En occidente, el mahjong se le conoce principalmente solo indirectamente a través de versiones de juego para solitarios, llamados "Shangais", pero conocidas popularmente con el mismo nombre de Mahjong; dichos juegos presentan reglas más simples y cuya regla general es emparejar fichas de Mahjong, siendo los "solitarios Shangais" popularizados por internet a través de juegos online.



</doc>
<doc id="9821" url="https://es.wikipedia.org/wiki?curid=9821" title="Magic: el encuentro">
Magic: el encuentro

Magic: el encuentro, originalmente en inglés Magic: The Gathering y frecuentemente abreviado como Magic, MTG y Cartas Magic, es un juego de cartas coleccionables diseñado en 1993 por Richard Garfield, profesor de matemáticas, y comercializado por la empresa Wizards of the Coast. "Magic" es el primer ejemplo de juego de cartas coleccionables moderno, con más de seis millones de jugadores en cincuenta y dos países diferentes, permaneciendo vigente en la actualidad. "Magic" puede ser jugado por dos o más jugadores, cada uno de ellos usando un mazo individual. También existe una versión digital que puede jugarse en línea.

Cada partida de "Magic" representa una batalla entre poderosos magos (en el juego conocidos como planeswalkers), en el que cada uno de estos magos es uno de los jugadores de la partida. Los jugadores pueden usar hechizos (conjuros, artefactos, tierras, criaturas fantásticas, etc.), representados individualmente en cada carta, para derrotar a sus oponentes. De este modo, el concepto original del juego se inspira de forma notable en los duelos de magos típicos de los juegos de rol tradicionales, como "Dungeons & Dragons". La estructura del juego reemplaza los útiles usados en los juegos de aventura de papel y lápiz por una gran cantidad de cartas y unas reglas más complejas que la mayoría de otros juegos de cartas.

Un sistema organizado de torneos y una comunidad de jugadores profesionales se ha desarrollado alrededor del juego, así como un mercado secundario de cartas. Las cartas de "Magic" son valoradas no sólo por su escasez, sino también por su valor en el juego, su antigüedad, por ser cartas de "culto" o por el valor estético de las ilustraciones.

En "Magic: el encuentro" cada jugador, debe derrotar a su enemigo usando sus poderes: criaturas mágicas, artefactos, y encantamientos, extrayendo el poder o maná de sus tierras, algunas también con sus propios poderes. El juego fue ideado para dos jugadores, aunque se han agregado nuevas reglas para jugar con más participantes, casi siempre en números pares ya que si es un número impar, por ejemplo tres jugadores, puede que dos se alíen para derrotar al otro, lo que constituye una práctica incorrecta.

Cada jugador empieza con veinte puntos de vida y un mazo («biblioteca») de al menos sesenta cartas (en algunos torneos especiales está permitido usar mazos de cuarenta), de las cuales aproximadamente (por consideraciones estratégicas) un tercio son tierras productoras de maná, la energía requerida para jugar las demás cartas (piensen en el Maná como dinero dentro del juego, que se usa para pagar los costes de las cartas a jugar). Las cartas que no son tierras se pueden agrupar colectivamente como hechizos. Requieren una determinada cantidad de maná para ser jugadas, generalmente una combinación de maná de un solo color e incoloro. Se dividen en las cartas que generan un efecto permanente (criaturas, encantamientos, artefactos e incluso las tierras son permanentes existentes en juego) y las que van al cementerio (por norma general) después de ser usadas (instantáneos y conjuros).

Un jugador pierde cuando se queda sin puntos de vida, cuando intenta robar una carta y su biblioteca está vacía, si obtiene diez contadores de veneno o si la condición de alguna carta así lo indica. El último jugador que queda es el ganador, aunque existen cartas que indican nuevas formas de ganar una vez que un jugador las pone en juego, como ganar la partida si ese jugador consigue determinados puntos de vida o si consigue tener en juego cierto número de criaturas, también existen habilidades por las cuales un jugador no puede perder una partida por determinada circunstancia, como por ejemplo: no puede perder si se queda sin puntos de vida o no puede acumular contadores de veneno.

Aunque el juego fue ideado para 2 o más jugadores, existen reglas creadas por jugadores que permiten el juego en solitario. Como por ejemplo la campaña de la Tumba de Eregorn, donde un jugador ha de atravesar cuatro escenarios hasta llegar a hacerse con el poder de la Joya de Eregorn. Una campaña ideal para probar barajas versátiles contra todo tipo de barajas y colores y para poder desempolvar todas aquellas cartas que ya no utilizan los jugadores.

En "Magic" hay cinco colores diferentes: blanco, azul, negro, rojo, y verde, cada uno con un significado y personalidad diferente. Cada color representa por así decirlo un estilo de magia.






Excepciones:



Un jugador necesita un mazo antes de que pueda jugar una partida de "Magic". Los principiantes usualmente comienzan con un mazo preconstruido. En muchos formatos de juego, los jugadores pueden modificar sus mazos con cualquiera de las cartas que posean, según la técnica del contrincante, su estilo de juego, o incluso anticiparse al mazo de un oponente.

En general, los mazos deben estar compuesto por un mínimo de sesenta cartas. Los jugadores no pueden usar más de cuatro copias de cualquier carta, con excepción de las llamadas «tierras básicas», que actúan como una fuente estándar de recursos en "Magic". Estas dos reglas son menos flexibles en los formatos "", donde el tamaño mínimo de mazo es de cuarenta cartas. Dependiendo del tipo de juego, algunas cartas más poderosas son "restringidas", esto es, solamente una sola copia es permitida por cada mazo; o "prohibidas", es decir, su uso no está permitido.

La decisión acerca de qué colores usar es parte importante en el proceso de crear un mazo. Los hechizos en "Magic" vienen en cinco colores distintos, cada uno con sus fortalezas y debilidades; de ahí que el jugar con más de un color puede ayudar a dar más versatilidad y mejor rendimiento a un mazo. Sin embargo, el reducir el número de colores usados incrementa notablemente las posibilidades de robar las tierras que se necesitan para jugar los hechizos más importantes. Por tanto, en la práctica, los mazos de uno o dos colores son los más comunes, los mazos de tres, cuatro o incluso cinco colores pueden ser exitosos si están bien diseñados.

Como ya se ha mencionado anteriormente, un mazo está compuesto por un mínimo de sesenta (60) cartas, pero adicionalmente un jugador puede crear un "banquillo" o sideboard de cartas que se pueden intercambiar por cartas del mazo principal. Durante los torneos por norma oficial para poder ganar una ronda se requiere ganar dos de tres partidas (o al mejor de cinco en competiciones de alto nivel. Por ej.: a partir de los cuartos de final en los Pro Tour), aquí es cuando entra el uso del banquillo. El banquillo está conformado principalmente por cartas cuyos efectos son muy circunstanciales/específicos y que serían muy útiles en unas ocasiones mientras que en otras solo estorbarían. Después de la primera partida, ya sabiendo a que clase de mazo se enfrentan los jugadores, se les permite modificar sus mazos con el banquillo que traigan (así sepan de antemano a que mazo se enfrentaran no pueden realizar modificaciones en la primera partida), un ejemplo de esto es que si un jugador se enfrenta a un mazo negro, y en su banquillo tiene cartas con protección contra negro, después de la primera partida podrá integrar esas cartas a su mazo y retirar otras que no le son útiles contra ese mazo. El banquillo, con las nuevas normas, puede tener hasta quince cartas y no es necesario incluir el mismo número de cartas que se sacan del mazo en los cambios.

Wizards of the Coast organiza todo tipo de reuniones, desde torneos locales hasta torneos profesionales (llamados "Pro Tours") donde es necesaria una invitación para poder participar. También existen campeonatos regionales, nacionales y un , que se celebran anualmente.

Una organización llamada "Duelist Convocation International" (DCI) es la encargada de establecer las reglas oficiales, proporcionar los mecanismos para organizar y sancionar todos estos torneos, organizar el colectivo de jueces oficiales de torneo, etc.

Existen varios tipos de formatos en función de las cartas que están permitidas en el juego. Según el tipo de torneo, podrán existir algunas cartas restringidas (es decir, se puede emplear sólo una copia de ellas en un mazo) y otras prohibidas. De las demás (salvo las tierras básicas) se pueden emplear hasta cuatro copias en un mismo mazo.
Los torneos se agrupan en tres tipos:

Un bloque consta de 2 o 3 expansiones(ediciones o series) de cartas que llevan la misma trama histórica en el universo de Magic y se editan habitualmente un bloque al año.
Como recomendación a los jugadores nuevos aunque la regla dice que se pueden jugar más de 40 cartas en los formatos limitados, lo ideal es jugar las 40 cartas exactas y no más de 3 colores para tener un mazo más competitivo.

También existe otra clasificación de los tipos de torneo, según el nivel de competitividad:




Los jugadores a veces juegan formatos casuales a partir de distintos criterios. Por ejemplo: jugar únicamente con cartas comunes para limitar el costo del mazo, jugando con cartas de un solo bloque a elección o bien adoptando uno de los formatos sancionados y modificar su lista de cartas prohibidas.

"Commander" (conocido anteriormente como EDH - Elder Dragon Highlander) es un formato casual donde los jugadores eligen una criatura legendaria que se denomina, "comandante". Luego el mazo se arma con 99 cartas únicas que deben tener la entidad de color del comandante. Desde hace algunos años "Commander" fue homologado por Wizards. La lista de baneos está regulada por un ente no oficial. Wizards of the Coast lanza mazos de commander prearmados una vez por año. 

Otro formato muy popular para jugar limitado es el Cubo. Tiene una estructura similar al "Booster Draft", sólo que los jugadores utilizan una colección de cartas preseleccionadas en lugar de utilizar sobres sellados. 

Aunque Wizards of the Coast solo vende cartas en sobres y mazos, existe un mercado activo de cartas sueltas entre los jugadores y en muchas tiendas especializadas.

Los precios de las cartas dependen, en buena medida, de su utilidad y su rareza (en los sobres vienen 11 Comunes, 3 Infrecuentes, 1 Rara y, ocasionalmente en las nuevas ediciones, una Mítica o Mithyc Rare).
Las cartas comunes suelen costar 0,08 €, y a veces superan los 0,30, mientras que las infrecuentes suelen costar entre 0,50 y 1 €. Las cartas raras, sin embargo, tienen precios muy superiores, a partir de 1 o 2 euros en adelante. Muchas cartas raras y algunas infrecuentes particularmente útiles o antiguas, y necesarias en la mayoría de mazos competitivos, cuestan más de 30 €.

Hay más de cien cartas que se venden unitariamente por más de 100 €, desde muchas pertenecientes a las primeras ediciones (Alfa/Beta, Legends, Antiquities...) a algunas más recientes debido al nuevo tipo de rareza, "Mithyc Rare", que las hace aparecer de forma ocasional en los sobres. Hoy en día, la versión Alfa firmada por el artista de la carta Black Lotus es considerada la carta "Magic" no promocional jamás impresa, sin contar las cartas con falla de impresión..

Hay una edición especialmente costosa, la llamada "Summer Magic", una tirada concreta de Revised|Revised Edition (Tercera Edición Core) que fue retirada del mercado antes de vender poco más de unos cientos de sobres; esta edición incluye las cartas legales más caras de la historia del juego, alcanzando más de 100€ una simple carta común (que normalmente vale céntimos de euro) y más de 8000€ si hablamos de la tierras dobles, debido a la extrema escasez de estas cartas y la casi imposible dificultad para encontrarlas en el mercado secundario. 

Existen también tres rarísimas cartas creadas directamente por Richard Garfield que, aunque no son legalmente jugables en torneos, están valoradas a precios muy superiores. Dos de estas cartas fueron editadas para conmemorar el nacimiento de sus respectivos hijos y hay 100 copias de cada una de ellas. La tercera fue creada para declararse y sólo existen 2 copias de las mismas. Las cartas más antiguas suben el precio, pero más que nada a causa de la fuerte Especulación (economía)|especulación que existe en el mercado.

Existen varios medios donde se puede buscar la «cotización» de las cartas de "Magic", pero hay que tener en cuenta que puede depender fuertemente del país donde uno se encuentre. En algunos casos la cotización de las cartas se ve influenciada por los intereses de las entidades que controlan el mercado. Por ejemplo, si una revista que publica los precios de las cartas, a la vez se dedica a venderlas, adjudicará un precio menor a las cartas que no abunden en su "stock". En cualquier caso, la gente que habitualmente juega torneos y comercia con cartas sabe el valor real de éstas, siendo el precio dictado por algunos medios una simple orientación.

A medida que salen a la venta nuevas ediciones, a veces se van reeditando cartas antiguas. Si una carta es cara porque es útil, la reedición suele hacer que suba el precio de la carta original, porque hay más formatos de torneos en que se puede jugar con ella y por tanto tendrá una mayor demanda entre los jugadores. Sin embargo, si su precio se debe al coleccionismo que suscita, una reedición bajará el precio de la carta original, porque ya no es tan escasa.

Para proteger el valor de algunas cartas antiguas y muy deseadas, Wizards of the Coast ha formulado una política oficial de reediciones, que incluye una lista de cartas que han declarado no volver a editar; en total unos pocos cientos de cartas, desde Alfa/Beta hasta Destino de Urza, con el fin de proteger los intereses del coleccionista. Algunas de estas cartas son las pertenecientes al Power Pack (Black Lotus, los cinco Mox, Ancestral Recall, Timetwister y Time Walk), las diez Tierras Dobles originales, el Bazaar of Baghdad y la Mishra's Workshop.

Con tres o cuatro expansiones nuevas por año, muchos jugadores critican el hecho de que se requiere una importante inversión para mantener una colección de cartas que sea competitiva o completa. El principal formato competitivo, Estándar o Tipo 2, solo usa cartas del último bloque completo (un bloque es un conjunto de tres expansiones consecutivas con temas y mecánicas comunes. Ej: Bloque Kamigawa - Campeones de Kamigawa, Traidores de Kamigawa y Salvadores de Kamigawa-), el bloque de la última expansión impresa y el último "Core Set" («baraja principal», por ejemplo la de la novena edición); de esta forma se obliga a los jugadores que juegan competitivamente a tener las últimas cartas que van saliendo. Otros formatos como Extended (Tipo 1.X), Legacy (Tipo 1.5) y Vintage (Tipo 1) oficiales), compran los mazos de torneo con bordes dorados (que también están prohibidos en torneos oficiales debido a que son relativamente muy baratos y a que contienen muchas cartas raras y caras) o usan alguno de los programas para jugar online a "Magic", como el Apprentice, el "Magic" Workstation o el OCTGN.

Debido a lo mencionado anteriormente, se critica que en estos tipos de juegos en muchas ocasiones los jugadores, más que enfrentarse en un duelo de habilidades, se enfrentan en un duelo entre los precios de las barajas; ya que muchas de las cartas necesarias para que un mazo sea competitivo son de un gran valor (y asimismo de una gran importancia). Así, se limita el acceso al juego competitivo a personas que, aunque jueguen bien no pueden acceder a las cartas más caras, y de esta forma, los jugadores que cuentan con más dinero son los que obtienen más ventaja, y no los que cuentan con más habilidad. Aun así, el hecho de manejar una baraja de mayor precio, no asegura una victoria, puesto que el factor habilidad y la picardía del jugador, aunque puedan parecer menos importante, son unas bazas grandes a la hora de desenvolverse en partidas de torneos.

En "Magic", como en muchos otros juegos, combina azar y suerte con habilidad y talento. Una queja común es, sin embargo, de que hay mucha suerte involucrada con el recurso básico del juego: las tierras. Muchas tierras o muy pocas (“Mana Flood” o “Mana Screw” en la jerga del juego respectivamente) en el inicio del juego principalmente, pueden arruinar la posibilidad que el jugador tiene de alcanzar la victoria sin que siquiera haya cometido un error. Una respuesta común a esta queja es que la influencia de la suerte en el juego puede ser minimizada armando apropiadamente un mazo sólido que haya sido probado extensivamente. Una cantidad precisa de tierras y buenas técnicas de mezclado del mazo pueden reducir la posibilidad de cualquier problema de maná. También hay cartas que son muy utilizadas y que tienen por objeto reducir la dependencia al maná del jugador. La cantidad de tierras en la mayoría de los mazos competitivos varia de 18 a 26 (un promedio de 22 o 21 tierras por mazo), aunque el uso de ciertos hechizos (como Seething Song), de artefactos (como los talismanes de Mirrodin o los Moxes de Alfa/Beta), de criaturas (como las Aves del Paraíso o los Elfos De Llanowar) o de tierras especiales (como las Watery Grave, Windswept Heath o Underground River) y el costo relativo de los principales hechizos de un mazo pueden aumentar o bajar sustancialmente la cantidad de tierras requeridas (Ej: Un mazo con hechizos de un solo color y de muy bajo coste necesita menos tierras que un mazo de muchos colores y hechizos más costosos).

La llamada «regla del Mulligan» fue introducida posteriormente al juego, primero como una opción para partidos informales y más tarde en las reglas oficiales del juego. La regla moderna del Mulligan permite a los jugadores que no estén satisfechos con su mano inicial poder mezclarla en el mazo y luego robar de nuevo una mano inicial pero con una carta menos. Esto se puede repetir cuantas veces se desee, pero cada vez que se lo haga se deberá robar una carta menos (primero seis, luego cinco, luego cuatro y así sucesivamente). Generalmente, se considera como una «buena mano» a una en la cual se tiene tres o cuatro tierras; y en muy raros casos se utiliza la regla del Mulligan más de 2 o 3 veces ( ya que a la segunda vez que se la utiliza se roban solo 5 cartas). Una reciente actualización de esta regla indica que ambos jugadores tienen que hacer mulligan simultáneamente Esto no se refiere a que ambos jugadores deban coincidir en querer hacer mulligan, si no, que deben decidir si van a hacer mulligan o no en el mismo momento, empezando por el jugador que va a empezar la partida. Anteriormente hasta que el jugador que iba a comenzar la partida no terminaba de hacer sus mulligan, el otro jugador no podía realizar sus mulligan. Con este anterior sistema se perdía mucho tiempo en establecer la mano inicial.

La vieja regla de Mulligan, todavía usada ocasionalmente en ciertos círculos de juego casual y en algunas partidas multijugador por internet, permite robar de nuevo siete cartas (una sola vez, luego normalmente) si la mano inicial contiene cero, una, seis o siete tierras. Una excelente fuente de información sobre el Mulligan puede encontrarse en el artículo de Mark Rosewater: “Starting Over”.

Recientemente se ha añadido una nueva regla al Mulligan que consiste en una vez acabado el proceso de mulligan, si tienes menos cartas de las iniciales puedes hacer scry de 1, es decir, después de hacer los mulligans que consideres necesarios, puedes ver la primera carta de la biblioteca y devolverla a la parte de arriba o ponerla en el fondo.

Internet ha jugado un importante papel en el "Magic" competitivo. Discusiones sobre estrategias y reportes de torneos incluyen frecuentemente un listado de lo que contiene exactamente un mazo y de la descripción de su rendimiento contra otros mazos. Usando un proceso conocido como “Net Decking”, algunos jugadores navegan la internet en busca de esta información y arman un mazo (sin siquiera, tal vez, haberlo jugado nunca antes) conteniendo las mismas, o muy similares, cartas, dependiendo de, esta forma, de la destreza y la experiencia de otros jugadores. Si bien esta técnica es a menudo una muy buena opción, no es seguro de que el mazo (por más de que sea exactamente el mismo) repita su éxito pasado. El jugador podría ser inexperto, podría no estar familiarizado con el funcionamiento del mazo o podría, simplemente, jugar en un metagame en el cual, el mazo que se armó es ineficaz y/o contra el cual los otros mazos están preparados.
Hoy en día armarse mazos directamente de Internet es una técnica muy extendida (debido al fácil acceso que se tiene a la Web y a la proliferación de sitios dedicados especialmente a publicar mazos y reportes de torneos) y utilizada por toda clase de jugadores, aunque es cierto también, que la mayoría de estos prueba el mazo en Internet antes de armárselo en la realidad.

Una intrincada trama yace tras las cartas lanzadas en cada expansión y es mostrada en el diseño y en el "texto de ambientación" de cada una de las cartas, así como en las novelas y antologías publicadas por Wizards of the Coast (y formalmente por "HarperPrism"). Esta trama toma lugar en el multiverso de "Magic", originalmente llamado "Dominia" pero que cambió para evitar confusión con Dominaria]], y multiverso que consiste en un número infinito de planos. Los objetos o personajes de la trama aparecen como cartas "Legendarias" en las diversas expansiones, cartas de las que sólo puede haber una en juego a la vez.

Las barajas de expansión que van desde "Antiquities" hasta "Azote" (con la excepción de "Tierras natales") son barajas ambientadas en el plano de Dominaria y conforman una línea temporal aproximadamente cronológica de la historia de ese plano (con la sola excepción del Bloque de la "Saga de Urza"). Los personajes frecuentes incluían a Urza y a su hermano Mishra.

Las barajas desde "Vientoligero" hasta "Apocalipsis" siguen en particular la historia de la tripulación del "Vientoligero", aliados de Urza contra Yawgmoth, asi como la lucha final de Dominaria con pirexia. "Magic" comenzó a aventurarse fuera de Dominaria y entrar en variados otros planos, tales como Mirrodin (un mundo hecho de metal creado por el Golem planeswalker Karn, en el cual su primer trama se centraba en el derrocamiento del loco guardian mernach que lleno el plano con seres organicos de otros mundos en busca de convertirse en un dios), Kamigawa(un mundo que recuerda mucho al japon feudal, cual trama gira alrededor de una guerra que surgio entre sus dioses y los mortales,y que ademas cronologicamente se hubica mas atras que la saga de urza), y Ravnica (un plano donde basicamente el paiseaje natural y la civilizacion se ha fundido completamente formando una mega metopolis gorbernada por diez gremios, su primer trama central se centra en el conflicto de poderes entre ellos lo que conllevara a la roptura de un pacto de no agrecion de un milenio). La trama de "Magic" retornó a Dominaria con el bloque de "Espiral del tiempo", y visitó a Lorwyn con el bloque de "Lorwyn", el bloque de "Páramo Sombrío", es la baraja ambientada en el plano de Páramo Sombrío, el inverso oscuro de Lorwyn. En el bloque de Alara se exploró los cinco diferentes planos resultantes de la fragmentación del plano Alara y su restauración como la nueva Alara en donde se empieza a introducir la trama de los nuevos "planeswalkers". En el bloque de Zendikar se explora un indomable plano lleno de tesoros y peligros, que además es la cárcel de los Eldrazi, una especie de criaturas procedentes de otra dimensión incomprensible y que resulta la más peligrosa del universo de "Magic". En el ciclo de cicatrices de Mirrodin se regresa al plano de metal, donde resurgen la civilización maligna de pirexia y el retorno del creador de Mirrodin Karn. En Innistrad, se explora un plano donde las peores pesadillas de los humanos son más que reales. En el ciclo de retorno de ravnica, volvemos al plano metrópolis, donde la pelea entre los gremios se ha intensificado. En theros nos adentramos en un plano que recuerda a la antigua mitología griega donde, héroes y monstruos pelean sin cuartel y los antiguos Dioses dominan el mundo. En Tarkir exploramos las dos líneas temporales de este mundo, una en la que los dragones fueron extinguidos por cinco clanes guerreros, siendo la tumba del planeswalker Ugin, y otra en la que los dragones gobiernan el mundo y el planeswalker Ugin despierta de un largo letargo, cuya intervención será importante en el futuro de otros mundos. 

Desde la expansión "Magic: Orígenes", se da a conocer a más detalle el pasado de cinco planeswalker en particular (Gideon, Jace, Liliana, Chandra, y Nissa), quienes tendrán más protagonismo en futuras historias. En los bloques, "batalla por Zendikar" y "sombras sobre Innistrad" se regresa a dos planos conocidos ambos al borde del apocalipsis, en el primero los titanes Eldrazis Ulamog y Kozilek están a punto de arrasar con el plano pero son detenidos por un equipo de planeswalker y los últimos sobrevivientes de ese mundo; por el otro lado Innistrad, un mundo oscuro y siniestro que había encontrado un respiro y una paz que parecía perdurar por la presencia de su guardiana Avacyn ve un atroz revés cuando ella y los demás ángeles caen en la locura y están dispuestos a erradicar toda vida del plano, pero tras su locura hay un mal peor, la llegada del último titán Eldrazi, Emrakul, cuya presencia corrompe la vida desde sus entrañas. En el bloque Kaladesh vemos con mayor profundidad y detalle el plano de original de la planeswalker chandra, donde la ciencia, la tecnología, el artificio y una sutil tiranía ha suplantado casi por completo la magia, y las tensiones de una revolución florecen en medio de una feria de inventores, pues el planeswalker Tezzeret subordinado de Nicol Bolas esta haciendo su trabajo sucio en aquel plano lo que conyevara a los guardianes con un nuevo miembro (Ajani Goldmane) pasen a su siguiente mision. En el bloque de Amonthek siguiendo la pista de Tezzeret los guardianes se adetran en el plano del mismo nombre que es un mundo que recuerda mucho al antiguo egipto, donde cinco dioses someten a sus habitantes vivos a pruebas para buscar a los que sean dignos a servir al dios Faraon quien no es otro que el oscuro dragon Nicol Bolas quien corrompio aquel mundo, desviando el proposito de los dioses y sus habitantes para formarse un ejercito de muertos vivientes mejorados con un embalsamiento especial, cuando los habitantes se dieron cuenta de la verdad tres monstruosos dioses, y Nicol Bolas empezaron destruir lo que no le servia del plano, incluyendo a los dioses mismos. Los sobrevivinetes de la masacre (incluyendo la diosa Hazaret) se adentraron al desierto como ultima esperanza de supervivencia, en esta ocasion el Gatewatch sufriria una derrota a manos del dragon.

Además que en el transcurso del Bloque de Alara hasta ahora, se han lanzado libros y cómics que narran y desarrollan la trama de los nuevos planeswalkers, aunque los libros lanzados después del ciclo de Lorwyn se centran más en los propios Planeswalker que un plano específico.

En cuanto a la novelas sobre el plano del bloque en turno solo se lanza una novela. De igual forma los cómics se centran principalmente en los planeswalkers.

Cada Carta tiene una ilustración que representa el espíritu (o flavor = sabor) de la carta, muchas veces representando el ambiente de las expansión para la cual la carta fue diseñada. Buena parte del arte inicial de "Magic"<nowiki>'</nowiki>s fue ordenado con poca dirección específica o ideas de cohesión visual. sin embargo, después de algunos años de envíos que mostraban seres con alas para criaturas sin habilidad de vuelo, o personajes múltiples en el arte de lo que tenía intención de ser una criatura simple, el equipo de dirección artística decidió imponer algunas reglas para que la visión artística estuviera mejor alineada el diseño y desarrollo de las cartas. Cada bloque de cartas tiene su propio estilo con escenas y descripciones de las razas y lugares que juegan un rol principal en el universo de juego de "Magic".

Algunas ediciones al principio experimentaron con arte diferente para la misma carta. Sin embargo, Wizards of the Coast pensó que esto impedía el reconocimiento inmediato de una carta a simple vista, y causaba confusión. Consecuentemente, el arte alterno se usa con parsimonia actualmente y sobre todo para cartas promocionales. Con esto dicho, cuando las cartas viejas son reimpresas en nuevas ediciones, Wizards ha asegurado que serán impresas con nuevo arte, para hacerlas más valiosas desde el punto de vista de la colección. Un buen ejemplo es una carta que lleva saliendo en distintas imágenes desde 1995 hasta el 2011, la "Bola de Fuego".

Desde 1995, el derecho de autor de todo el arte es transferido a "Wizards of the Coast" cuando se firma el contrato de adquisición. Sin embargo, al artista se le permite vender la pieza original y las copias impresas de la misma, y para los artistas importantes de "Magic", esto se convierte en una fuente de ingreso lucrativa.

Mientras "Magic" se expandió a lo largo del planeta, su arte tuvo que adaptarse a su público ahora internacional. El arte ha sido editado o reemplazado para cumplir con estándares gubernamentales. Por ejemplo, la aparición de esqueletos y casi todos los no-muertos en el arte está prohibido por el Gobierno de la República Popular de China.

En 2012, la película independiente "" se estrenó no comercialmente en línea. La película independiente acerca de "Magic: el encuentro" se filmó en República Checa como un proyecto en solitario, y no fue patrocinada ni por Wizards of the Coast ni por Hasbro.




</doc>
<doc id="9822" url="https://es.wikipedia.org/wiki?curid=9822" title="Cubismo">
Cubismo

El cubismo fue un movimiento artístico desarrollado entre 1907 y 1914, nacido en Francia y encabezado por Pablo Picasso, Georges Braque, Jean Metzinger, Albert Gleizes, Robert Delaunay, Juan Gris y Guillaume Apollinaire.Es una tendencia esencial, pues da pie al resto de las vanguardias europeas del siglo XX. No se trata de un "ismo" más, sino de la ruptura definitiva con la pintura tradicional.

El término cubismo fue acuñado por el crítico francés Louis Vauxcelles, el mismo que había bautizado a los fauvistas motejándolos de "fauves" (fieras); en el caso de Braque y sus pinturas de L'Estaque, Vauxcelles dijo, despectivamente, que era una pintura compuesta por «pequeños cubos». Se originó así el concepto de «cubismo». El cubismo literario es otra rama que se expresa con poesías cuya estructura forma figuras o imágenes que ejemplifican el tema, la rima es opcional y no tienen una métrica específica ni se organizan en versos.

El cubismo es considerado la primera vanguardia, ya que rompe con el último estatuto renacentista vigente a principios del siglo XX, la perspectiva. En los cuadros cubistas, desaparece la perspectiva tradicional. Trata las formas de la naturaleza por medio de figuras geométricas, fragmentando líneas y superficies. Se adopta así la llamada «perspectiva múltiple»: se representan todas las partes de un objeto en un mismo plano. La representación del mundo en donde pasaba a no tener ningún compromiso con la apariencia de las cosas desde un punto de vista determinado, sino con lo que se sabe de ellas. Por eso aparecían al mismo tiempo y en el mismo plano vistas diversas del objeto: por ejemplo, se representa de frente y de perfil; en un rostro humano, la nariz está de perfil y el ojo de frente; una botella aparece en su corte vertical y su corte horizontal. Ya no existe un punto de vista único. No hay sensación de profundidad. Los detalles se suprimen, y a veces acaba representando el objeto por un solo aspecto, como ocurre con los violines, insinuados solo por la presencia de la cola del mismo.

A pesar de ser pintura de vanguardia los géneros que se pintan no son nuevos, y entre ellos se encuentran sobre todo bodegones, paisajes y retratos.

Se eliminan los colores sugerentes que tan típicos eran del impresionismo o el fovismo. En lugar de ello, utiliza como tonos pictóricos apagados los grises, verdes y marrones. El monocromatismo predominó en la primera época del cubismo, posteriormente se abrió más la paleta.

Con todas estas innovaciones, el arte acepta su condición de arte, y permite que esta condición se vea en la obra, es decir es parte intrínseca de la misma. El cuadro cobra autonomía como objeto con independencia de lo que representa, por ello se llega con el tiempo a pegar o clavar a la tela todo tipo de objetos hasta formar "collages".

La obra resultante es de difícil comprensión al no tener un referente naturalista inmediato, y ello explica que fuera el primero de los movimientos artísticos que necesitó una exégesis por parte de la "crítica", llegando a considerarse el discurso escrito tan importante como la misma práctica artística. De ahí en adelante, todos los movimientos artísticos de vanguardia vinieron acompañados de textos críticos que los explicaban.

El cubismo tuvo como centro neurálgico la ciudad de París, y como jefes y maestros del movimiento figuraban los españoles Pablo Picasso y Juan Gris y los franceses Georges Braque y Fernand Léger. El movimiento efectivamente se inicia con el cuadro "Las señoritas de Aviñón" ("Demoiselles d'Avignon") de Pablo Picasso. Como elemento precursor del cubismo destaca la influencia de las esculturas africanas y las exposiciones retrospectivas de Georges Seurat (1905) y de Paul Cézanne (1907).

El cubismo surge en la primera década del siglo XX, constituyendo la primera de las vanguardias artísticas. Entre las circunstancias que contribuyeron a su surgimiento, se ha señalado tradicionalmente tanto la obra de Cézanne como el arte de otras culturas, particularmente la africana. En efecto, Cézanne pretendió representar la realidad reduciéndola a sus formas esenciales, intentando representar los volúmenes sobre la superficie plana del lienzo de una manera nueva, tendencia que fue seguida por los cubistas. Ya antes que él, los neoimpresionistas Seurat y Signac tendieron a estructurar geométricamente sus cuadros. Lo que Picasso y Braque tomaron de Cézanne fue la técnica para resolver ese problema de lograr una nueva figuración de las cosas, dando a los objetos solidez y densidad, apartándose de las tendencias impresionistas que habían acabado disolviendo las formas en su búsqueda exclusiva de los efectos de la luz.

Por otro lado, el imperialismo puso a Occidente en contacto con otras civilizaciones con un arte propio y distinto del europeo. A través de diversas exposiciones, Picasso conoció la escultura ibérica y la africana, que simplificaban las formas y, además, ponían en evidencia que la pintura tradicional obedecía a una pura convención a la hora de representar los objetos conforme a las ideas renacentistas de perspectiva lineal y aérea. Lo que parece actualmente excesivo a los historiadores de arte es atribuir una influencia directa de las máscaras africanas con la obra "picassiana."

Todo ello no hubiera sido posible sin la aparición de la fotografía pues esta, al representar la realidad visual de manera más exacta que la pintura, liberó a este último arte de la obligación de representar las cosas tal como aparecen ante nuestros ojos y forzó a los artistas a buscarle un sentido diferente a la mera transcripción a las dos dimensiones de la apariencia externa de las cosas. La aparición del cubismo se ha relacionado, además, con otros tres hechos acontecidos en esas décadas que revelan que las cosas pueden ser diferentes a como aparentan ser: el psicoanálisis al evidenciar que pueden existir motivaciones más profundas para los actos y pensamientos humanos; el interés por la cuarta dimensión, fruto de la revolución acaecida en la geometría del siglo XIX; y la teoría de la relatividad, que revela que el mundo no es exactamente, en su estructura profunda, como lo presentaba la geometría euclidiana.

En 1909 Braque y Picasso estrechan su amistad y consiguen desarrollar la nueva tendencia. Juntos crearon las dos tendencias del cubismo. La primera es el cubismo analítico (1909-1912), en donde la pintura es casi monocroma en gris y ocre. Los colores en este momento no interesaban pues lo importante eran los diferentes puntos de vista y la geometrización, no el cromatismo. Fueron elaborando un «nuevo lenguaje» que analiza la realidad y la descompone en múltiples elementos geométricos. Los puntos de vista se multiplicaron, abandonando definitivamente la unidad del punto de vista de la perspectiva renacentista. Se introducen en la pintura los «pasos», definidos como ligeras interrupciones de la línea del contorno. Los volúmenes grandes se fragmentan en volúmenes más pequeños. Entre las obras de esta fase del cubismo se encuentra el "Retrato de Kahnweiler" (1910, Instituto de Arte de Chicago).

A este período también se le llama de cubismo hermético, pues por la cantidad de puntos de vista representados, algunas obras parecen casi abstractas. Al hermetismo se llega porque los planos acaban independizándose en relación al volumen de manera que es difícil decodificar la figuración, reconstruir mentalmente el objeto que esos planos representan. El color no ayudaba, al ser prácticamente monocromos y muchas veces convencionales, no relacionados con el auténtico color del objeto. La imagen representada, en definitiva, era ilegible, casi imposible de ver, a no ser por algunos objetos como una pipa, o letras de periódico, que permiten distinguir lo que se está representando.

Es en esta fase cuando el cubismo se presenta en público. Pero no por obra de Picasso y Braque, que exponían privadamente en la galería Kahnweiler, sino por otros pintores que conocieron la obra de aquellos en sus talleres. Se presentaron al Salón de los Independientes de 1911. En su sala 41 aparecieron obras de Jean Metzinger, Albert Gleizes, Henri Le Fauconnier, Fernand Léger y Robert Delaunay. Provocaron el escándalo y rechazo de público y crítica. Ello llevó a que se construyera ya una obra doctrinal de primera hora explicando los hallazgos de la nueva tendencia. Así, el primer estudio teórico del cubismo lo hicieron en 1912 Gleizes y Metzinger: "" («Sobre el cubismo»). Apollinaire, por su parte, escribió "Les peintres cubistes" («Los pintores cubistas. Meditaciones estéticas») en 1913. Hubo otras adhesiones, como la de la mecenas Gertrude Stein o los marchantes como Ambroise Vollard y Henry Kahnweiler. Otros poetas, además de Apollinaire, defendieron el nuevo estilo: Pierre Reverdy y Max Jacob.

Además del rechazo de los tradicionalistas de la pintura, hubo posteriormente críticos que venían de la propia vanguardia, centradas en dos problemas que planteaba el cubismo: su estatismo y su adhesión a lo figurativo. En efecto, sobre todo los futuristas objetaron al cubismo que en sus obras el movimiento estuviera ausente, siendo así que el mundo actual es esencialmente dinámico. Gino Severini, a quien se considera el más cubista dentro del futurismo, lo criticó en "Del Cubismo al Clasicismo" (1921), aunque con el tiempo (1960) reconoció que debía al cubismo gran parte de su técnica. Algunos cubistas fueron sensibles a esta crítica y crearon obras influidas por el futurismo, como hizo Marcel Duchamp con su primera versión de "Desnudo bajando una escalera" (1911, Museo de Arte de Filadelfia, col. Arensberg). Por otro lado, aunque en su época no resultaba fácil deslindar el cubismo de la abstracción, hoy resulta evidente que siguen sujetos a una representación figurativa de las cosas reales. Se seguían representando sillas, botellas o figuras humanas, aunque las descompusieran en planos y volúmenes geométricos. No se apartaban de representar la realidad, sino que querían representarla en el cuadro con un nuevo lenguaje.

El camino trazado por Picasso y Braque pronto fue seguido por los pintores Juan Gris (José Victoriano González) y Louis Marcoussis, el primero influido por Picasso, el segundo por Braque. Gris, tercer gran nombre del cubismo. Este madrileño malvivía en París dibujando para revistas y periódicos. A partir de 1911 se interesó por el problema de la luz sobre los objetos, creando cuadros con iluminación naturalista, en los que los rayos luminosos oblicuos y paralelos entre sí inciden sobre formas rígidas, como puede verse en su "Retrato de Picasso" de 1912. Él mismo dijo haber adoptado el cubismo «analítico», multiplicando los puntos de vista y usando colores vivos. Para el año 1912, Braque y Picasso ya habían realizado "collages", y Gris comenzó a introducir en sus obras diversos materiales como la madera o la tapicería, bien imitándolos, bien pegándolos ("El lavabo", 1912).

Braque, por su parte, influyó en el polaco Marcoussis (Ludwig Markus). Más ortodoxo y menos original que Gris, creó una obra con colores intensos y cercana a veces al futurismo. Comenzó en 1912 a trabajar el cubismo analítico, con obras como "Naturaleza muerta con damero" (1912, Museo Nacional de Arte Moderno, Centro Georges Pompidou).

En "El Portugués" (1911) de Braque aparecen palabras y números, lo que abrió una nueva vía que llevó al segundo período del cubismo, el cubismo sintético (1912-1914). Braque, que había sido el primero en utilizar la caligrafía, y que más de una vez intentó imitar la madera o el mármol, fue quien inició esta última fase del cubismo al realizar "papier collés", pegando directamente papeles decorados en la pintura. Picasso y Braque comenzaron a incorporar material gráfico como páginas de diario y papeles pintados, técnica que se conoce como "collage". En 1912 Picasso realizó su primer "collage", "Naturaleza muerta con silla de paja" (Museo Picasso, París), en el que añade al lienzo pasta de papel y hule. El color es más rico que en la fase anterior, como puede verse en los rojos y azules de "Botella de Suze" (1913, Saint Louis, Misuri, Universidad Washington). Estas obras sintéticas son más simples, más sencillas de entender en cuanto a que son más figurativas, se ve claramente lo que se pretende representar. Los objetos ya no se reducen a volúmenes y planos expuestos en diversas perspectivas hasta ser irreconocibles, sino que se reducen a sus atributos esenciales, a aquello que los caracteriza de manera inequívoca sin lo cual no serían lo que son. Por ello, aunque reducido a lo esencial, queda claro en todo momento lo que son. Para representar los objetos «tipo» de manera objetiva y permanente, y no a través de la subjetividad del pincel, se recurre a lo que parece un ensamblaje. Los cuadros están formados por diversos materiales cotidianos que se pegaban o clavaban a la tela, como tiras de papel de tapicerías, periódico, partituras, naipes, cajetillas de cigarros o cajas de fósforos. El cuadro se construye con elementos diversos, tanto tradicionales (la pintura al óleo) como nuevos (como el papel de periódico). Los cafés y la música inspiraron estos bodegones. Otras obras de Picasso pertenecientes a esta fase del cubismo sintético son "El jugador de cartas" (1913-14) o "Naturaleza muerta verde" (1914). Braque realiza en esta época "El clarinete" (1913), el "Correo" (1913), "Aria de Bach" (1913-14) o "Violeta de Parma" (1914).

En este período Juan Gris realiza una pintura más libre y colorista. Emblemática es su "Place Ravignan, naturaleza muerta ante una ventana abierta" (1915), donde el exterior se representa a la manera tradicional, con perspectiva renacentista, mientras que el interior de formas deconstruidas y compuestas desde diversos puntos de vista con planos quebrados. Por su parte, Marcoussis llega a la cumbre de su tarea creadora con obras más poéticas y personales como "Músico" (1914, Galería Nacional de Washington, col. Chester Dale)

María Blanchard nunca llegó a la total descomposición de la forma pero dejó su manufactura en forma de ricos colores. Su famosa "Mujer con abanico" (1916, Museo Nacional Centro de Arte Reina Sofía),"Naturaleza muerta" (1917, Fundación telefónica) o "Mujer con guitarra" (1917, Museo Nacional Centro de Arte Reina Sofía) son ejemplos del intenso estudio que realiza sobre la anatomía de las cosas, como señaló Ramón Gómez de la Serna y del peso del color en su pintura. Tras esta etapa regresa a las técnicas figurativas donde queda impresa la influencia de las vanguardias.

La Primera Guerra Mundial puso fin a la fase más creadora del cubismo. Muchos de los pintores cubistas, al ser franceses, fueron llamados a la lucha (Braque, Léger, Metzinger, Gleizes, Villon y Lhote). En la posguerra, solo Juan Gris siguió trabajando el cubismo más o menos ortodoxo, aunque en un estilo más austero y simple, en el que los objetos quedaron reducidos a su esencia geométrica. Marcoussis creó una obra más poética. Braque siguió trabajando en la misma línea del cubismo sintético, con papel encolado. Nuevos pintores adoptaron un lenguaje cubista, como María Blanchard. Pero la mayoría de los pintores hasta entonces cubistas, empezando por el propio Picasso, fueron adoptando nuevas tendencias, como ocurre con Duchamp y Picabia, que crearon el dadaísmo o Mondrian que se adhirió a la abstracción. El cubismo, como movimiento pictórico, se puede dar por terminado hacia 1919.

Fue el francés Apollinaire quien lo adaptó en la literatura. Busca recomponer la realidad mezclando imágenes y conceptos al azar. Uno de sus aportes fue el caligrama.

El cubismo repercutió en la escultura, a través de técnicas similares al "collage" del cubismo sintético. La escultura empezó a construirse con materiales de desecho, elaborándose con piezas diversas y no procedentes de un solo bloque de piedra o mármol. Con ello se crea la llamada estética de «ausencia de masa», al surgir huecos y vacíos entre las superficies. Como los arquitectos, los escultores no dan forma a un volumen, sino que crean espacios. Es de especial interés la variante arquitectónica del cubismo que se dio en Checoslovaquia entre 1910 y 1925, el llamado "Cubismo Checo".

El propio Pablo Picasso realizó esculturas cubistas. Escultores que crearon obras cubistas fueron Alexander Archipenko, Jacques Lipchitz y Henri Laurens, además de los españoles Pablo Gargallo y, sobre todo, Julio González, pionero en el uso del hierro gracias a la soldadura autógena, lo que abrió todo un mundo de posibilidades a la escultura del siglo XX.

En el siglo XXI, el actor, director y dramaturgo Rafael Negrete-Portillo (docente en varias universidades españolas para las titulaciones de Artes Escénicas) re-genera este movimiento artístico, dotándolo de una nueva dimensión: la escénica. Así, el dramaturgo acuña el término de TEATRO CUBISTA para referirse a la búsqueda de la fragmentación (deconstrucción + reconstrucción) del nexo comunicativo que une a la puesta en escena y al espectador. Del mismo modo, el trabajo dramatúrgico, según expone Negrete-Portillo, se trataría de una "labor investigadora teatral que debería tender, pragmáticamente hablando, hacia la fragmentación coherente (y posterior selección) de los elementos básicos y fundamentales para que el texto dramático y, especialmente, su posterior representación, deje a la mente del espectador/co-autor libertad coherente para que éste llegue a reconstruir el mensaje pretendido por el dramaturgo".

Además de Pablo Picasso y Georges Braque, a los que se considera fundadores del cubismo, y Juan Gris y Marcoussis, sus más directos seguidores, el cubismo fue seguido por una multitud de artistas entre 1911 y 1914. Algunos de ellos se agruparon bajo la denominación de "Section d'Or" o Grupo de Puteaux: Albert Gleizes, Jean Metzinger, Juan Gris , Fernand Léger y André Lhote. De este colectivo surgió, en 1912 el orfismo, cuyos máximos representantes son Robert Delaunay y František Kupka, quienes acabaron renunciando a la representación figurativa y centrándose en el color se aproximaron a la abstracción geométrica, como anticipó ya su "Villa de París", de Delaunay (1910). El tema acabó desapareciendo totalmente en obras como "Formas circulares" (1912-13). Se ha denominado a este estilo como cubismo abstracto o rayonismo. Kupka, próximo al cubismo, comenzó a estudiar, a partir de 1912, la forma en que el espacio podía representarse mediante planos de color ("Planos verticales Amorpha", 1912) o líneas sinuosas. También Francis Picabia recreó los volúmenes de la realidad de manera bastante abstracta (Procesión en Sevilla, 1912) lo que le llevó, a partir de 1913, a la no-figuración.

Gleizes cultivó un cubismo cezaniano más figurativo que el resto y en el que aparecía la figura humana esquematizada; no obstante, también tuvo una fase analítica. Obras destacadas de Gleizes son: "Árbol" (1910, París, col. part.), "Caza" (1911, París, col. comandante Houot), "Hombres en el balcón" (1912, Museo de Arte de Filadelfia, col. Arensberg), "Desgranado de la cosecha" (1912, Museo Guggenheim de Nueva York), Bañistas (quizá su obra más conocida, de 1912, Museo de Arte Moderno de la Ciudad de París), "Retrato de Figuière" (1913, museo de Lyon) y "Mujeres cosiendo" (Otterlo, Museo Kröller-Múller).
Su amigo Metzinger, con quien escribió "Sobre el cubismo" tuvo una primera fase analítica en la que predomina el estudio de la estructura, para pasar luego a una fase cezaniana en la que predomina el estudio de los volúmenes. De Metzinger destacan sus "Desnudos" de 1910-1911, la "Merienda" (1910-11, Museo de Arte de Filadelfia, col. Arensberg), el "Pájaro azul" (1913, Museo de Arte Moderno de la Ciudad de París), "Bañistas" (1913, Museo de Arte de Filadelfia) y "Mujer haciendo calceta" (1919, Museo Nacional de Arte Moderno, Centro Georges-Pompidou).

Henri Le Fauconnier (1881-1946) realizó estudios de desnudos cuyos volúmenes fue fragmentando, explorando la incidencia de la luz sobre ellos. Creó «una especie de de Impresionismo cubista bastante personal» que puede verse en obras como "Retrato de Paul Castiaux" (1910), "Abundancia" (1910-11) o "Cazador" (1912).

Más original que todos ellos fue Fernand Léger. Desarrolló un estilo personal que refleja su atracción por la máquina. Célebre es su obra "Figuras desnudas en el bosque" (1909-1910, Otterlo, Museo Kröller-Müller), que se puede considerar obra intermedia entre el cubismo y el futurismo, movimiento este último fascinado con la máquina y el movimiento. En esta obra se aprecia igualmente su predilección por las formas y los volúmenes, propia del cubismo cezaniano. Después de experimentar con los volúmenes, comienza a dar preponderancia al color a partir de 1913, en composiciones llenas de dinamismo.

Por una fase cubista pasó el gran pintor neerlandés Piet Mondrian al instalarse en París en 1911. Cultivó el cubismo analítico en el período 1911-1914. Sus estudios sobre el ángulo recto, y las formas planas acabaron llevándole a la abstracción. Al volver a Ámsterdam fundó, junto a Van Doesburg, el grupo De Stijl (1917). En torno a su revista se constituyeron artistas directamente influidos por el cubismo.

En México también surgieron obras cubistas por el pintor Diego Rivera en el año de 1913, recreando escenas de la Revolución Mexicana. Diego influenciado por la obra de Picasso y Braque acudió a la manipulación geométrica y al punto de vista panorámico elevado para recrear paisajes, retratos y naturalezas muertas. Sus mejores obras ese año fueron La Adoración de la virgen, "La joven con alcachofas, La mujer del pozo" o "El joven de la estilográfica" (todas de 1914), y "El arquitecto" (de 1915).

Hubo otros que adaptaron el cubismo a su temperamento. Entre ellos cabe citar, en primer lugar, a Jacques Villon, que conoció el cubismo a través de su hermano Marcel Duchamp. Estudió los volúmenes, compuso sus cuadros en estructuras piramidales y empleó colores vivos. Su cubismo fue moderado, como el de Roger de la Fresnaye, que aunque adoptó la superposición de planos, no llegó a romper de manera clara con la figuración y la perspectiva. Se vio influido por Delaunay, lo que le llevó a realizar sus mejores obras construidas sobre todo con el color: "Conquista del aire" (1913) y muchas "Naturalezas muertas" (1913-14). Después de la guerra volvió al clasicismo. Finalmente, André Lhote se enmarca en una tendencia a adaptar el estilo cubista a las reglas de la composición clásica.

Además de los ya citados, se puede considerar que hicieron obras cubistas: Marcel Duchamp, Sonia Delaunay, Emilio Pettoruti, Carlos Sotomayor, María Blanchard y Enrique Sobisch.

El purismo de Charles Edouart Jeanneret y Amadée Ozenfant surgió en 1918 como una derivación del cubismo.





</doc>
<doc id="9823" url="https://es.wikipedia.org/wiki?curid=9823" title="Juego de cartas coleccionables">
Juego de cartas coleccionables

Los juegos de cartas coleccionables son un tipo de juego de cartas no predefinidas y existentes en gran cantidad y de variados tipos y características, que otorgan individualidad a cada carta, y con las cuales puede construirse una baraja (o mazo) libremente de acuerdo a las reglas de cada tipo de juego en particular.

El concepto de «juego de cartas coleccionables» (a veces abreviado en JCC) nace en 1993 con "", juego creado por Richard Garfield, aunque anteriormente existía ya un tipo de juego de cartas denominado actualmente juego de baraja de colección (JBC) que presentaba reglas más simples y una menor complejidad en comparación al JCC, jugándose a partir de una baraja con cartas predefinidas y en el que cada carta traía generalmente una imagen con un tema específico según el tipo de baraja y juego. 

Así, en 1993 Richard Garfield, profesor de matemáticas, va más allá del concepto que tenía el juego de baraja de colección clásica y desarrolla el actual concepto de JCC después de ser planteado y desarrollado en una conversación sobre póquer, en la que Garfield planteaba cómo sería el juego si las barajas, en lugar de tener 54 cartas predefinidas, pudieran construirse libremente. La respuesta evidente es que siempre se meterían ases y el juego no tendría sentido; no obstante, Garfield tenía una visión muy distinta de los juegos de cartas, logrando así diseñar el concepto básico de todos los JCC y crear su propio juego.

En el ámbito de los juegos de cartas coleccionables aparecen continuamente nuevos juegos basados en licencias de cine, series televisivas, cómics o libros, existiendo un JCC para prácticamente cada ambientación posible. Las cartas pueden ser concebidas como objetos de colección además de poder ser jugadas.

La principal característica de un JCC es la individualidad de cada carta, la cual, lleva escrita su función en el juego, efectos y características. Los juegos de naipes tradicionales utilizan siempre las mismas cartas que cobran efecto según las reglas del juego; los JCCs usan unas reglas que definen el desarrollo de la partida y las condiciones de victoria pero son las cartas las que definen su propia función.

A partir de un amplio elenco de cartas disponibles (unas 18.500 en el caso de Magic) cada jugador confecciona su propio mazo, dando pie a una infinidad de estrategias y desarrollos de partidas, las cuales, puede afirmarse que cada una es distinta.

Para establecer un paralelismo, un JCC es un ajedrez donde cada jugador escoge sus 16 piezas iniciales de miles disponibles, cada una con movimientos y efectos propios, jugando siempre bajo las normas del tablero de ajedrez. Las reglas del juego hacen que ninguna carta sea desequilibrante y tenga siempre su contrapartida.

El juego Magic, de la empresa Wizards Of The Coast, difundió este concepto, imitado en cientos de juegos distintos, cada cual con sus propias reglas y características, dando lugar a un segmento del ocio único, tremendamente amplio y que desde 1993 no ha parado de crecer con millones de aficionados en todo el globo ya que presenta un juego totalmente distinto conocido, desafiante por su profundidad e infinitas combinaciones de estrategia, adictivo ya que las sucesivas partidas son totalmente distintas, y con un amplio programa de torneos y eventos que abarcan desde pequeños torneos locales a un circuito profesional con grandes premios en metálico. Posteriormente producto del éxito de esta empresa aparecerían otras empresas con nuevos juegos de cartas coleccionables, ampliándose así el número de juegos de cartas coleccionables.

Los mazos se caracterizan por ser igualmente transportables fácilmente, lo cual hace que pueda jugarse en cualquier lugar contra cualquier persona, haciendo así un juego social que puede practicarse con un variado número de jugadores, dando lugar a tantas situaciones de partida que, aunque un mazo de un jugador sea superior a un rival concreto, igualmente puede suceder que este jugador se vea en serios aprietos contra otro. Los grandes torneos reúnen a cientos y miles de jugadores, estando el récord en 2220 jugadores (GP Madrid 27/02/2010).

La otra característica que le da nombre y las diferencia de los naipes tradicionales es que son coleccionables. Así, las cartas se dividen en frecuencias, habitualmente: Común, Infrecuente y Rara, siendo de menos a más la dificultad de conseguir. Dichas cartas se adquieren en sobres sellados cuyo contenido es aleatorio, con lo que no se sabe exactamente que se obtiene en cada sobre. Por ello, no se compran las cartas que deseamos, si no que se entra en un mercado secundario de coleccionismo, donde las cartas adquieren su valor por su frecuencia, antigüedad, estado y sobre todo calidad y efectividad en partidas, obedeciendo a conceptos de oferta y demanda.

Así, consecuentemente, tenemos la vertiente de juego y de coleccionable, logrando la fórmula que ha logrado captar la atención de miles de jugadores en todo el mundo y dando lugar a tiendas especializadas donde pueden adquirirse tanto en formato sellado como carta suelta, así como grandes comunidades de internet dedicadas a informar sobre el juego, estrategias y mazos ganadores, e intercambio y valoración de cartas (consultar links al final).

El precio de cualquier JCC es el que quiera darle el usuario. El precio básico de un sobre (que incluyen entre 8 y 15 cartas según el JCC) oscila entre 3 y 4 euros, siendo suficiente para iniciar a jugar un Mazo preconstruido (de 10 a 15 euros) y unos cuantos sobres. Si gusta, puede iniciarse la construcción de un mazo de juego más avanzado, existiendo cartas sueltas desde 0,20€ hasta 1500€ (Black Lotus), pero encontrando buenas cartas por 4-8€. Una vez construido el primer mazo y jugado unas cuantas partidas, el usuario es el que decide hasta donde quiere llegar, cuantos mazos quiere construir, y cuan competetivos desea que sean, ajustándose el juego a su presupuesto.

Una vez se posean estas cartas, no debe olvidarse que el valor económico lo conservan, pudiendo cambiarlas o venderlas en torno a ese valor, siendo posible renovar mazos a base de comerciar con cartas, con lo que la inversión realizada siempre puede amortizarse.

Existen otras muchas características que convierten a los JCCs en un apasionante hobby:








</doc>
<doc id="9826" url="https://es.wikipedia.org/wiki?curid=9826" title="Laguna de Lobos">
Laguna de Lobos

La Laguna de Lobos, se encuentra en la Provincia de Buenos Aires, ubicada a 15 km de la ciudad de Lobos y a 115 km de la Ciudad de Buenos Aires, es el principal atractivo turístico de la zona. Su acceso se halla en la Ruta Nacional 205 km 111,5; luego a la izquierda por camino pavimentado de 4 km para poder llegar al espejo de agua. En la margen Norte se encuentra la localidad de Villa Loguercio.

La laguna tiene 800 ha, transformándose en un lugar ideal para la práctica de actividades acuáticas. Al estar ubicada en una zona de abundante vegetación se pueden apreciar una gran variedad de aves silvestres. La fauna ictícola compuesta por pejerreyes, carpas, dientudos, tarariras, bogas, lisas, bagres y mojarras permiten inolvidables jornadas para el aficionado a la pesca.Es un Humedal, y en él, conviven diversas especies de animales y plantas: Mamíferos como las coipos (Myocastor coypus) y zorros pampeanos ("Licalopex gymnocercus"); peces como pejerreyes ("Odontesthes bonariensis") y bagres ("Rhamdia quelen") ; aves como el biguá ("Phalacrocorax olivaceus") y los patos siriri pampa ("Dendrocygna viduata"), y reptiles como los lagartos overos ("Salvator merinae"), entre otras. En su flora, se destaca el Junco ("Schoenoplectus californicus"), que sirve como protector de la costa y lugar de nidificación de aves, depósito de huevas de peces y refugio de mamíferos.



Todos los diciembre desde 1988 se celebra la Fiesta del Pescador Deportivo, declarada de interés Municipal, Provincial y Nacional en la que se realizan distintas actividades acuáticas, culminando con la elección de la Reina del Pescador Deportivo y un show musical, sobre un escenario acuático. Esta fiesta se realiza en el Club de Pesca Lobos, el cual fue fundado en 1945. 

La Laguna cuenta con sus propias embarcaciones e instalaciones adecuadas para cocinar, además posee un muelle de 150 m de largo.

Bajo su espesa arboleda se encuentra la estación Hidrobiológica que se encarga de la cría y siembra de aproximadamente 50 mil alevinos anuales lo cual ha permitido mantener a través de los años el atractivo turístico fundamental de la Laguna: "La Pesca del Pejerrey".

Sobre el margen Noroeste, se encuentra "Villa Loguercio", en la que residen cerca de 400 habitantes estables y alrededor de 2.000 temporarios que se alojan en numerosas casas de fin de semana.

A fin de preservar la biodiversidad y la tranquilidad características del lugar, un mayoritario grupo de vecinos presentó a las Autoridades Municipales un petitorio firmado solicitando se declare Área Protegida a la zona de la "Boca" de la Laguna y alrededores. Cabe destacar que la Laguna de Lobos, a diferencia de otras lagunas de Sud América, está inventariada como Humedal de Latinoamérica. Su categorización como humedal responde a que sus ambientes son importantes para numerosas especies de aves acuáticas que residen o visitan dicho cuerpo de agua a lo largo del año. La conservación de este ambiente acuático y sus ambientes relacionados se sumarían a los esfuerzos de conservación de la biodiversidad de la cuenca del Salado.

Villa Loguercio fue fundada el 15 de junio de 1953, para más información se puede visitar el sitio wwww.lagunalobos.com.ar 



</doc>
<doc id="9828" url="https://es.wikipedia.org/wiki?curid=9828" title="Hiragana">
Hiragana

El es uno de los dos silabarios empleados en la escritura japonesa; el otro se denomina "katakana". También se suele emplear "hiragana" para referirse a cualquiera de los caracteres de dicho silabario. Proviene de la simplificación de caracteres más complejos de origen chino que llegaron antes del comienzo del aislamiento cultural japonés, que se mantuvo inflexible hasta el final de la era Edo. Se caracteriza por trazos curvos y simples.

Cuando se hace referencia a ambos silabarios en conjunto, "hiragana" y "katakana", se conocen como "kana". Estos caracteres, al contrario que los "kanji", no tienen ningún valor conceptual, sino únicamente fonético. 

El silabario hiragana consta de 46 caracteres en total, de los cuales 45 representan sílabas formadas por una consonante y una vocal, o bien una única vocal; y la única consonante que puede ir sola, la 'n' (ん en "hiragana" y ン en "katakana").

Este silabario se emplea en la escritura de palabras japonesas, partículas y desinencias verbales; en contraste con el "katakana" que se emplea para palabras extranjeras y onomatopeyas. Por ello, el hiragana es el primer silabario que aprenden los niños japoneses. A medida que aprenden los "kanji", los estudiantes van reemplazando los caracteres silábicos en favor de los caracteres chinos.

Los caracteres en rojo han quedado obsoletos en el japonés moderno.

Nota sobre la pronunciación. Todas las letras se pronuncian más o menos como en español salvo:

Existe un acento diacrítico en japonés llamado "nigori", y sirve para formar consonantes sonoras o 'impuras' (caso de la D, G, B y Z) o la "medio impura" P.

En el primer caso, el de las consonantes sonoras, se emplea el "dakuten" (゛) (濁点), que se representa con dos trazos diagonales cortos en la parte superior derecha del carácter. También se le suele llamar "ten ten".

Para formar el sonido P, se emplea el "handakuten" (゜) (半濁点), que tiene forma de círculo y se escribe también en la parte superior derecha del carácter. Se le conoce también como "maru".

Más sobre pronunciación: 'z' y 'j' son como en inglés. 'ge' y 'gi' se pronuncian 'gue', 'gui'.

Ten ten o dakuten sólo se usa con las sílabas que comienzan en: K, T, F, H y S.

Handakuten o Maru sólo se usa con sílabas que empiezan con: F o con H.

Por otra parte, cuando una sílaba que termina en "i" se une con ya, yu, yo, se pueden formar diptongos. En este caso el segundo kana (ya, yu o yo) se escribirá más pequeño de lo habitual.
[La pronunciación de esta "y" es la de una "i" consonántica (como la "i" de "novio [bjo]). Otro ejemplo es la 'u' en algunas palabras inglesas en que suena como "yu": huge" (hyu)]

Ejemplo: き(ki) y きゃ (Ki + ya pequeña = kya)

Las consonantes dobles se forman escribiendo un 'tsu' pequeño (っ) delante de la consonante en cuestión. Sólo se doblan las consonantes k, s, t, p.

Ejemplos: よっか (yokka, día 4 de mes), ざっし (zasshi, revista), だった (datta, pasado del verbo ser), にっぽん (nippon, Japón). La pronunciación es: "yo-ka", "za-shi", "da-ta", "ni-pon", es decir, se pronuncia con un breve espacio entre la sílaba anterior y la posterior .

La única excepción es cuando el pequeño 'tsu' (っ) precede a la sílaba "chi" (ち). En este caso se leerá "tchi", y no "cchi" como muchos han interpretado. Ejemplos: たまごっち (tamagotchi), 一人ぼっち (hitoribotchi, solitario).

En cuanto a las consonantes nasales ('m', 'n'), se doblan escribiendo ん delante. Ejemplos: おんな (on'na, mujer), うんめい (unmei, destino).

Al igual que en español, la 'n' ん se pronuncia 'm' delante de 'p' o 'b'. Ejemplos: せんぱい ("senpai", fórmula de respeto a los que tienen un grado de estudio mayor), こんばん ("konban", esta noche).

Finalmente, existen vocales largas, que se forman de la siguiente manera:




</doc>
<doc id="9839" url="https://es.wikipedia.org/wiki?curid=9839" title="Central térmica solar">
Central térmica solar

Una central térmica solar o central termosolar es una instalación industrial en la que, a partir del calentamiento de un fluido mediante radiación solar y su uso en un ciclo termodinámico convencional, se produce la potencia necesaria para mover un alternador para generación de energía eléctrica como en una central termoeléctrica clásica. 

Consiste en el aprovechamiento térmico de la energía solar para transferirla y almacenarla en un medio portador de calor, generalmente agua. Esta es una de las ventajas de la tecnología CSP, el almacenamiento térmico. La tecnología más comúnmente utilizada para almacenar esta energía son las sales fundidas (nitratos) de almacenamiento térmico. La composición de estas sales es variable, siendo la más utilizada la mezcla de nitrato de potasio, nitrato de sodio y últimamente se ha incorporado el nitrato de calcio.

Constructivamente, es necesario concentrar la radiación solar para que se puedan alcanzar temperaturas elevadas, de 300 °C hasta 1000 °C, y obtener así un rendimiento aceptable en el ciclo termodinámico, que no se podría obtener con temperaturas más bajas. La captación y concentración de los rayos solares se hacen por medio de espejos con orientación automática que apuntan a una torre central donde se calienta el fluido, o con mecanismos más pequeños de geometría parabólica. El conjunto de la superficie reflectante y su dispositivo de orientación se denomina heliostato. 

Los fluidos y ciclos termodinámicos escogidos en las configuraciones experimentales que se han ensayado, así como los motores que implican, son variados, y van desde el ciclo Rankine (centrales nucleares, térmicas de carbón) hasta el ciclo Brayton (centrales de gas natural) pasando por muchas otras variedades como el motor de Stirling, siendo las más utilizadas las que combinan la energía termosolar con el gas natural.

Hay virtualmente una provisión ilimitada de energía solar que podemos usar y es una energía renovable. Esto significa que nuestra dependencia de combustibles fósiles se puede reducir en proporción directa a la cantidad de energía solar que producimos. Con el constante incremento en la demanda de fuentes de energía tradicionales y el consiguiente aumento en los costos, la energía solar es cada vez más una necesidad.

En la tabla a continuación se muestra el detalle de la potencia instalada por países a finales de 2014. España es actualmente líder mundial en esta tecnología:



</doc>
<doc id="9840" url="https://es.wikipedia.org/wiki?curid=9840" title="Energía solar fotovoltaica">
Energía solar fotovoltaica

La energía solar fotovoltaica es una fuente de energía que produce electricidad de origen renovable, obtenida directamente a partir de la radiación solar mediante un dispositivo semiconductor denominado célula fotovoltaica, o bien mediante una deposición de metales sobre un sustrato denominada célula solar de película fina.

Este tipo de energía se usa principalmente para producir electricidad a gran escala a través de redes de distribución, aunque también permite alimentar innumerables aplicaciones y aparatos autónomos, abastecer refugios de montaña o viviendas aisladas de la red eléctrica. Debido a la creciente demanda de energías renovables, la fabricación de células solares e instalaciones fotovoltaicas ha avanzado considerablemente en los últimos años.

Programas de incentivos económicos, primero, y posteriormente sistemas de autoconsumo fotovoltaico y balance neto sin subsidios, han apoyado la instalación de la fotovoltaica en un gran número de países. Gracias a ello la energía solar fotovoltaica se ha convertido en la tercera fuente de energía renovable más importante en términos de capacidad instalada a nivel global, después de las energías hidroeléctrica y eólica. A principios de 2017, se estima que hay instalados en todo el mundo cerca de 300GW de potencia fotovoltaica.

La energía fotovoltaica no emite ningún tipo de polución durante su funcionamiento, contribuyendo a evitar la emisión de gases de efecto invernadero. Su principal desventaja consiste en que su producción depende de la radiación solar, por lo que si la célula no se encuentra alineada perpendicularmente al Sol se pierde entre un 10-25 % de la energía incidente. Debido a ello, en las plantas de conexión a red se ha popularizado el uso de seguidores solares para maximizar la producción de energía. La producción se ve afectada asimismo por las condiciones meteorológicas adversas, como la falta de sol, nubes o la suciedad que se deposita sobre los paneles. Esto implica que para garantizar el suministro eléctrico es necesario complementar esta energía con otras fuentes de energía gestionables como las centrales basadas en la quema de combustibles fósiles, la energía hidroeléctrica o la energía nuclear.

Gracias a los avances tecnológicos, la sofisticación y la economía de escala, el coste de la energía solar fotovoltaica se ha reducido de forma constante desde que se fabricaron las primeras células solares comerciales, aumentando a su vez la eficiencia, y logrando que su coste medio de generación eléctrica sea ya competitivo con las fuentes de energía convencionales en un creciente número de regiones geográficas, alcanzando la paridad de red.

El término «fotovoltaico» se comenzó a usar en Reino Unido en el año 1849. Proviene del griego φώς: "phos", que significa «luz», y de "-voltaico", que proviene del ámbito de la electricidad, en honor al físico italiano Alejandro Volta.

El efecto fotovoltaico fue reconocido por primera vez unos diez años antes, en 1839, por el físico francés Alexandre-Edmond Becquerel, pero la primera célula solar no se fabricó hasta 1883. Su creador fue Charles Fritts, quien recubrió una muestra de selenio semiconductor con pan de oro para formar la unión. Este primitivo dispositivo presentaba una eficiencia menor del 1%, pero demostró de forma práctica que, efectivamente, producir electricidad con luz era posible. Los estudios realizados en el siglo XIX por Michael Faraday, James Clerk Maxwell, Nikola Tesla y Heinrich Hertz sobre inducción electromagnética, fuerzas eléctricas y ondas electromagnéticas, y sobre todo los de Albert Einstein en 1905, proporcionaron la base teórica al efecto fotoeléctrico, que es el fundamento de la conversión de energía solar a electricidad.

Cuando un semiconductor dopado se expone a radiación electromagnética, se desprende del mismo un fotón, que golpea a un electrón y lo arranca, creando un hueco en el átomo. Normalmente, el electrón encuentra rápidamente otro hueco para volver a llenarlo, y la energía proporcionada por el fotón, por tanto, se disipa en forma de calor. El principio de una célula fotovoltaica es obligar a los electrones y a los "huecos" a avanzar hacia el lado opuesto del material en lugar de simplemente recombinarse en él: así, se producirá una diferencia de potencial y por lo tanto tensión entre las dos partes del material, como ocurre en una pila.

Para ello, se crea un campo eléctrico permanente, a través de una unión pn, entre dos capas dopadas respectivamente, p y n. En las células de silicio, que son mayoritariamente utilizadas, se encuentran por tanto:



En el momento de la creación de la unión pn, los electrones libres de la capa n entran instantáneamente en la capa p y se recombinan con los huecos en la región p. Existirá así durante toda la vida de la unión, una carga "positiva" en la región n a lo largo de la unión (porque faltan electrones) y una carga "negativa" en la región en p a lo largo de la unión (porque los "huecos" han desaparecido); el conjunto forma la «Zona de Carga de Espacio» (ZCE) y existe un campo eléctrico entre las dos, de n hacia p. Este campo eléctrico hace de la ZCE un diodo, que sólo permite el flujo de corriente en una dirección: los electrones pueden moverse de la región p a la n, pero no en la dirección opuesta y por el contrario los "huecos" no pasan más que de n hacia p.

En funcionamiento, cuando un fotón arranca un electrón a la matriz, creando un electrón libre y un "hueco", bajo el efecto de este campo eléctrico cada uno va en dirección opuesta: los electrones se acumulan en la región n (para convertirse en polo negativo), mientras que los "huecos" se acumulan en la región dopada p (que se convierte en el polo positivo). Este fenómeno es más eficaz en la ZCE, donde casi no hay portadores de carga (electrones o "huecos"), ya que son anulados, o en la cercanía inmediata a la ZCE: cuando un fotón crea un par electrón-hueco, se separaron y es improbable que encuentren a su opuesto, pero si la creación tiene lugar en un sitio más alejado de la unión, el electrón (convertido en "hueco") mantiene una gran oportunidad para recombinarse antes de llegar a la zona n. Pero la ZCE es necesariamente muy delgada, así que no es útil dar un gran espesor a la célula. Efectivamente, el grosor de la capa n es muy pequeño, ya que esta capa sólo se necesita básicamente para crear la ZCE que hace funcionar la célula. En cambio, el grosor de la capa p es mayor: depende de un compromiso entre la necesidad de minimizar las recombinaciones "electrón-hueco", y por el contrario permitir la captación del mayor número de fotones posible, para lo que se requiere cierto mínimo espesor.

En resumen, una célula fotovoltaica es el equivalente de un generador de energía a la que se ha añadido un diodo. Para lograr una célula solar práctica, además es preciso añadir contactos eléctricos (que permitan extraer la energía generada), una capa que proteja la célula pero deje pasar la luz, una capa antireflectante para garantizar la correcta absorción de los fotones, y otros elementos que aumenten la eficiencia del misma.

El ingeniero estadounidense Russell Ohl patentó la célula solar moderna en el año 1946, aunque otros investigadores habían avanzado en su desarrollado con anterioridad: el físico sueco Sven Ason Berglund había patentado en 1914 un método que trataba de incrementar la capacidad de las células fotosensibles, mientras que en 1931, el ingeniero alemán Bruno Lange había desarrollado una fotocélula usando seleniuro de plata en lugar de óxido de cobre.

La era moderna de la tecnología solar no llegó hasta el año 1954, cuando los investigadores estadounidenses Gerald Pearson, Calvin S. Fuller y Daryl Chapin, de los Laboratorios Bell, descubrieron de manera accidental que los semiconductores de silicio dopado con ciertas impurezas eran muy sensibles a la luz. Estos avances contribuyeron a la fabricación de la primera célula solar comercial. Emplearon una unión difusa de silicio p–n, con una conversión de la energía solar de aproximadamente 6%, un logro comparado con las células de selenio que difícilmente alcanzaban el 0,5%.

Posteriormente el estadounidense Les Hoffman, presidente de la compañía Hoffman Electronics, a través de su división de semiconductores fue uno de los pioneros en la fabricación y producción a gran escala de células solares. Entre 1954 y 1960, Hoffman logró mejorar la eficiencia de las células fotovoltaicas hasta el 14%, reduciendo los costes de fabricación para conseguir un producto que pudiera ser comercializado.

Al principio, las células fotovoltaicas se emplearon de forma minoritaria para alimentar eléctricamente juguetes y en otros usos menores, dado que el coste de producción de electricidad mediante estas células primitivas era demasiado elevado: en términos relativos, una célula que produjera un vatio de energía mediante luz solar podía costar 250 dólares, en comparación con los dos o tres dólares que costaba un vatio procedente de una central termoeléctrica de carbón.

Las células fotovoltaicas fueron rescatadas del olvido gracias a la carrera espacial y a la sugerencia de utilizarlas en uno de los primeros satélites puestos en órbita alrededor de la Tierra. La Unión Soviética lanzó su primer satélite espacial en el año 1957, y Estados Unidos le seguiría un año después. La primera nave espacial que usó paneles solares fue el satélite norteamericano Vanguard 1, lanzado en marzo de 1958 (hoy en día el satélite más antiguo aún en órbita). En el diseño de éste se usaron células solares creadas por Peter Iles en un esfuerzo encabezado por la compañía Hoffman Electronics. El sistema fotovoltaico le permitió seguir transmitiendo durante siete años mientras que las baterías químicas se agotaron en sólo 20 días.

En 1959, Estados Unidos lanzó el Explorer 6. Este satélite llevaba instalada una serie de módulos solares, soportados en unas estructuras externas similares a unas alas, formados por 9600 células solares de la empresa Hoffman. Este tipo de dispositivos se convirtió posteriormente en una característica común de muchos satélites. Había cierto escepticismo inicial sobre el funcionamiento del sistema, pero en la práctica las células solares demostraron ser un gran éxito, y pronto se incorporaron al diseño de nuevos satélites.

Pocos años después, en 1962, el Telstar se convirtió en el primer satélite de comunicaciones equipado con células solares, capaces de proporcionar una potencia de 14W. Este hito generó un gran interés en la producción y lanzamiento de satélites geoestacionarios para el desarrollo de las comunicaciones, en los que la energía provendría de un dispositivo de captación de la luz solar. Fue un desarrollo crucial que estimuló la investigación por parte de algunos gobiernos y que impulsó la mejora de los paneles fotovoltaicos.
Gradualmente, la industria espacial se decantó por el uso de células solares de arseniuro de galio (GaAs), debido a su mayor eficiencia frente a las células de silicio. En 1970 la primera célula solar con heteroestructura de arseniuro de galio y altamente eficiente se desarrolló en la Unión Soviética por Zhorés Alfiórov y su equipo de investigación.

A partir de 1971, las estaciones espaciales soviéticas del programa Salyut fueron los primeros complejos orbitales tripulados en obtener su energía a partir de células solares, acopladas en estructuras a los laterales del módulo orbital, al igual que la estación norteamericana Skylab, pocos años después.

En la década de 1970, tras la primera crisis del petróleo, el Departamento de Energía de los Estados Unidos y la agencia espacial NASA iniciaron el estudio del concepto de energía solar en el espacio, que ambicionaba el abastecimiento energético terrestre mediante satélites espaciales. En 1979 propusieron una flota de satélites en órbita geoestacionaria, cada uno de los cuales mediría 5 x 10 km y produciría entre 5 y 10GW. La construcción implicaba la creación de una gran factoría espacial donde trabajarían continuamente cientos de astronautas. Este gigantismo era típico de una época en la que se proyectaba la creación de grandes ciudades espaciales. Dejando aparte las dificultades técnicas, la propuesta fue desechada en 1981 por implicar un coste disparatado.A mediados de la década de 1980, con el petróleo de nuevo en precios bajos, el programa fue cancelado.

No obstante, las aplicaciones fotovoltaicas en los satélites espaciales continuaron su desarrollo. La producción de equipos de deposición química de metales por vapores orgánicos o MOCVD ("Metal Organic Chemical Vapor Deposition") no se desarrolló hasta la década de 1980, limitando la capacidad de las compañías en la manufactura de células solares de arseniuro de galio. La primera compañía que manufacturó paneles solares en cantidades industriales, a partir de uniones simples de GaAs, con una eficiencia del 17 % en AM0 (""), fue la norteamericana "Applied Solar Energy Corporation" (ASEC). Las células de doble unión comenzaron su producción en cantidades industriales por ASEC en 1989, de manera accidental, como consecuencia de un cambio del GaAs sobre los sustratos de GaAs, a GaAs sobre sustratos de germanio.

La tecnología fotovoltaica, si bien no es la única que se utiliza, sigue predominando a principios del siglo XXI en los satélites de órbita terrestre. Por ejemplo, las sondas Magallanes, Mars Global Surveyor y Mars Observer, de la NASA, usaron paneles fotovoltaicos, así como el Telescopio espacial Hubble, en órbita alrededor de la Tierra. La Estación Espacial Internacional, también en órbita terrestre, está dotada de grandes sistemas fotovoltaicos que alimentan todo el complejo espacial, al igual que en su día la estación espacial Mir. Otros vehículos espaciales que utilizan la energía fotovoltaica para abastecerse son la sonda Mars Reconnaissance Orbiter, Spirit y Opportunity, los robots de la NASA en Marte.

La nave Rosetta, lanzada en 2004 en órbita hacia un cometa tan lejano del Sol como el planeta Júpiter (5,25AU), dispone también de paneles solares; anteriormente, el uso más lejano de la energía solar espacial había sido el de la sonda Stardust, a 2 AU. La energía fotovoltaica se ha empleado también con éxito en la misión europea no tripulada a la Luna, SMART-1, proporcionando energía a su propulsor de efecto Hall. La sonda espacial Juno será la primera misión a Júpiter en usar paneles fotovoltaicos en lugar de un generador termoeléctrico de radioisótopos, tradicionalmente usados en las misiones espaciales al exterior del Sistema Solar.
Actualmente se está estudiando el potencial de la fotovoltaica para equipar las naves espaciales que orbiten más allá de Júpiter.

Desde su aparición en la industria aeroespacial, donde se ha convertido en el medio más fiable para suministrar energía eléctrica en los vehículos espaciales, la energía solar fotovoltaica ha desarrollado un gran número de aplicaciones terrestres. La primera instalación comercial de este tipo se realizó en 1966, en el faro de la isla Ogami (Japón), permitiendo sustituir el uso de gas de antorcha por una fuente eléctrica renovable y autosuficiente. Se trató del primer faro del mundo alimentado mediante energía solar fotovoltaica, y fue crucial para demostrar la viabilidad y el potencial de esta fuente de energía.

Las mejoras se produjeron de forma lenta durante las siguientes dos décadas, y el único uso generalizado se produjo en las aplicaciones espaciales, en las que su relación potencia a peso era mayor que la de cualquier otra tecnología competidora. Sin embargo, este éxito también fue la razón de su lento crecimiento: el mercado aeroespacial estaba dispuesto a pagar cualquier precio para obtener las mejores células posibles, por lo que no había ninguna razón para invertir en soluciones de menor costo si esto reducía la eficiencia. En su lugar, el precio de las células era determinado en gran medida por la industria de los semiconductores; su migración hacia la tecnología de circuitos integrados en la década de 1960 dio lugar a la disponibilidad de lingotes más grandes a precios relativamente inferiores. Al caer su precio, el precio de las células fotovoltaicas resultantes descendió en igual medida. Sin embargo, la reducción de costes asociada a esta creciente popularización de la energía fotovoltaica fue limitada, y en 1970 el coste de las células solares todavía se estimaba en 100 dólares por vatio ($/Wp).

A finales de la década de 1960, el químico industrial estadounidense Elliot Berman estaba investigando un nuevo método para la producción de la materia prima de silicio a partir de un proceso en cinta. Sin embargo, encontró escaso interés en su proyecto y no pudo obtener la financiación necesaria para su desarrollo. Más tarde, en un encuentro casual, fue presentado a un equipo de la compañía petrolera Exxon que estaban buscando proyectos estratégicos a 30 años vista. El grupo había llegado a la conclusión de que la energía eléctrica sería mucho más costosa en el año 2000, y consideraba que este aumento de precio haría más atractivas a las nuevas fuentes de energía alternativas, siendo la energía solar la más interesante entre estas. En 1969, Berman se unió al laboratorio de Exxon en Linden, Nueva Jersey, denominado "Solar Power Corporation" (SPC).

Su esfuerzo fue dirigido en primer lugar a analizar el mercado potencial para identificar los posibles usos que existían para este nuevo producto, y rápidamente descubrió que si el coste por vatio se redujera desde los 100$/Wp a cerca de 20$/Wp surgiría una importante demanda. Consciente de que el concepto del «silicio en cinta» podría tardar años en desarrollarse, el equipo comenzó a buscar maneras de reducir el precio a 20$/Wp usando materiales existentes. La constatación de que las células existentes se basaban en el proceso estándar de fabricación de semiconductores supuso un primer avance, incluso aunque no se tratara de un material ideal. El proceso comenzaba con la formación de un lingote de silicio, que se cortaba transversalmente en discos llamados obleas. Posteriormente se realizaba el pulido de las obleas y, a continuación, para su uso como células, se dotaba de un recubrimiento con una capa anti reflectante. Berman se dio cuenta de que las obleas de corte basto ya tenían de por sí una superficie frontal anti reflectante perfectamente válida, y mediante la impresión de los electrodos directamente sobre esta superficie, se eliminaron dos pasos importantes en el proceso de fabricación de células.

Su equipo también exploró otras formas de mejorar el montaje de las células en matrices, eliminando los costosos materiales y el cableado manual utilizado hasta entonces en aplicaciones espaciales. Su solución consistió en utilizar circuitos impresos en la parte posterior, plástico acrílico en la parte frontal, y pegamento de silicona entre ambos, embutiendo las células. Berman se dio cuenta de que el silicio ya existente en el mercado ya era «suficientemente bueno» para su uso en células solares. Las pequeñas imperfecciones que podían arruinar un lingote de silicio (o una oblea individual) para su uso en electrónica, tendrían poco efecto en aplicaciones solares. Las células fotovoltaicas podían fabricarse a partir del material desechado por el mercado de la electrónica, lo que traería como consecuencia una gran mejora de su precio.

Poniendo en práctica todos estos cambios, la empresa comenzó a comprar a muy bajo coste silicio rechazado a fabricantes ya existentes. Mediante el uso de las obleas más grandes disponibles, lo que reducía la cantidad de cableado para un área de panel dado, y empaquetándolas en paneles con sus nuevos métodos, en 1973 SPC estaba produciendo paneles a 10$/Wp y vendiéndolos a 20$/Wp, disminuyendo el precio de los módulos fotovoltaicos a una quinta parte en sólo dos años.

SPC comenzó a contactar con las compañías fabricantes de boyas de navegación ofreciéndoles el producto, pero se encontró con una situación curiosa. La principal empresa del sector era "Automatic Power", un fabricante de baterías desechables. Al darse cuenta de que las células solares podían comerse parte del negocio y los beneficios que el sector de baterías le producía, "Automatic Power" compró un prototipo solar de "Hoffman Electronics" para terminar arrinconándolo. Al ver que no había interés por parte de "Automatic Power", SPC se volvió entonces a "Tideland Signal", otra compañía suministradora de baterías formada por ex-gerentes de "Automatic Power". Tideland presentó en el mercado una boya alimentada mediante energía fotovoltaica y pronto estaba arruinando el negocio de "Automatic Power".

El momento no podía ser más adecuado, el rápido aumento en el número de plataformas petrolíferas en alta mar y demás instalaciones de carga produjo un enorme mercado entre las compañías petroleras. Como "Tideland" había tenido éxito, "Automatic Power" comenzó entonces a procurarse su propio suministro de paneles solares fotovoltaicos. Encontraron a Bill Yerkes, de "Solar Power International" (SPI) en California, que estaba buscando un mercado donde vender su producto. SPI pronto fue adquirida por uno de sus clientes más importantes, el gigante petrolero ARCO, formando ARCO Solar. La fábrica de ARCO Solar en Camarillo (California) fue la primera dedicada a la construcción de paneles solares, y estuvo en funcionamiento continuo desde su compra por ARCO en 1977 hasta 2011 cuando fue cerrada por la empresa SolarWorld.

Esta situación se combinó con la crisis del petróleo de 1973. Las compañías petroleras disponían ahora de ingentes fondos debido a sus enormes ingresos durante la crisis, pero también eran muy conscientes de que su éxito futuro dependería de alguna otra fuente de energía. En los años siguientes, las grandes compañías petroleras comenzaron la creación de una serie de empresas de energía solar, y fueron durante décadas los mayores productores de paneles solares. Las compañías ARCO, Exxon, Shell, Amoco (más tarde adquirida por BP) y Mobil mantuvieron grandes divisiones solares durante las décadas de 1970 y 1980. Las empresas de tecnología también realizaron importantes inversiones, incluyendo General Electric, Motorola, IBM, Tyco y RCA.

En las décadas transcurridas desde los avances de Berman, las mejoras han reducido los costes de producción por debajo de 1$/Wp, con precios menores de 2$/Wp para todo el sistema fotovoltaico. El precio del resto de elementos de una instalación fotovoltaica supone ahora un mayor coste que los propios paneles.

A medida que la industria de los semiconductores se desarrolló hacia lingotes cada vez más grandes, los equipos más antiguos quedaron disponibles a precios reducidos. Las células crecieron en tamaño cuando estos equipos antiguos se hicieron disponibles en el mercado excedentario. Los primeros paneles de ARCO Solar se equipaban con células de 2 a 4 pulgadas (51 a 100mm) de diámetro. Los paneles en la década de 1990 y principios de 2000 incorporaban generalmente células de 5 pulgadas (125mm), y desde el año 2008 casi todos los nuevos paneles utilizan células de 6 pulgadas (150mm). También la introducción generalizada de los televisores de pantalla plana a finales de la década de 1990 y principios de 2000 llevó a una amplia disponibilidad de grandes láminas de vidrio de alta calidad, que se utilizan en la parte frontal de los paneles.

En términos de las propias células, solo ha habido un cambio importante. Durante la década de 1990, las células de polisilicio se hicieron cada vez más populares. Estas células ofrecen menos eficiencia que aquellas de monosilicio, pero se cultivan en grandes cubas que reducen en gran medida el coste de producción. A mediados de la década de 2000, el polisilicio dominaba en el mercado de paneles de bajo coste.

La producción industrial a gran escala de paneles fotovoltaicos despegó en la década de 1980, y entre sus múltiples usos se pueden destacar:

La energía solar fotovoltaica es ideal para aplicaciones de telecomunicaciones, entre las que se encuentran por ejemplo las centrales locales de telefonía, antenas de radio y televisión, estaciones repetidoras de microondas y otros tipos de enlaces de comunicación electrónicos. Esto es debido a que, en la mayoría de las aplicaciones de telecomunicaciones, se utilizan baterías de almacenamiento y la instalación eléctrica se realiza normalmente en corriente continua (DC). En terrenos accidentados y montañosos, las señales de radio y televisión pueden verse interferidas o reflejadas debido al terreno ondulado. En estos emplazamientos, se instalan transmisores de baja potencia (LPT) para recibir y retransmitir la señal entre la población local.

Las células fotovoltaicas también se utilizan para alimentar sistemas de comunicaciones de emergencia, por ejemplo en los postes de SOS (Teléfonos de emergencia) en carreteras, señalización ferroviaria, balizamiento para protección aeronáutica, estaciones meteorológicas o sistemas de vigilancia de datos ambientales y de calidad del agua.

La reducción en el consumo energético de los circuitos integrados, hizo posible a finales de la década de 1970 el uso de células solares como fuente de electricidad en calculadoras, tales como la Royal "Solar 1", Sharp "EL-8026" o Teal "Photon".

También otros dispositivos fijos que utilizan la energía fotovoltaica han visto aumentar su uso en las últimas décadas, en lugares donde el coste de conexión a la red eléctrica o el uso de pilas desechables es prohibitivamente caro. Estas aplicaciones incluyen por ejemplo las lámparas solares, bombas de agua, parquímetros, teléfonos de emergencia, compactadores de basura, señales de tráfico temporales o permanentes, estaciones de carga o sistemas remotos de vigilancia.

En entornos aislados, donde se requiere poca potencia eléctrica y el acceso a la red es difícil, las placas fotovoltaicas se emplean como alternativa económicamente viable desde hace décadas. Para comprender la importancia de esta posibilidad, conviene tener en cuenta que aproximadamente una cuarta parte de la población mundial todavía no tiene acceso a la energía eléctrica.

En los países en desarrollo, muchos pueblos se encuentran situados en áreas remotas, a varios kilómetros de la red eléctrica más próxima. Debido a ello, se está incorporando la energía fotovoltaica de forma creciente para proporcionar suministro eléctrico a viviendas o instalaciones médicas en áreas rurales. Por ejemplo, en lugares remotos de India un programa de iluminación rural ha provisto iluminación mediante lámparas LED alimentadas con energía solar para sustituir a las lámparas de queroseno. El precio de las lámparas solares era aproximadamente el mismo que el coste del suministro de queroseno durante unos pocos meses. Cuba y otros países de Latinoamérica están trabajando para proporcionar energía fotovoltaica en zonas alejadas del suministro de energía eléctrica convencional. Estas son áreas en las que los beneficios sociales y económicos para la población local ofrecen una excelente razón para instalar paneles fotovoltaicos, aunque normalmente este tipo de iniciativas se han visto relegadas a puntuales esfuerzos humanitarios.

También se emplea la fotovoltaica para alimentar instalaciones de bombeo para sistemas de riego, agua potable en áreas rurales y abrevaderos para el ganado, o para sistemas de desalinización de agua.

Los sistemas de bombeo fotovoltaico (al igual que los alimentados mediante energía eólica) son muy útiles allí donde no es posible acceder a la red general de electricidad o bien supone un precio prohibitivo. Su coste es generalmente más económico debido a sus menores costes de operación y mantenimiento, y presentan un menor impacto ambiental que los sistemas de bombeo alimentados mediante motores de combustión interna, que tienen además una menor fiabilidad.

Las bombas utilizadas pueden ser tanto de corriente alterna (AC) como corriente continua (DC). Normalmente se emplean motores de corriente continua para pequeñas y medianas aplicaciones de hasta 3 kW de potencia, mientras que para aplicaciones más grandes se utilizan motores de corriente alterna acoplados a un inversor que transforma para su uso la corriente continua procedente de los paneles fotovoltaicos. Esto permite dimensionar sistemas desde 0,15 kW hasta más de 55 kW de potencia, que pueden ser empleados para abastecer complejos sistemas de irrigación o almacenamiento de agua.

Debido al descenso de costes de la energía solar fotovoltaica, se está extendiendo asimismo el uso de sistemas híbridos solar-diésel, que combinan esta energía con generadores diésel para producir electricidad de forma continua y estable. Este tipo de instalaciones están equipadas normalmente con equipos auxiliares, tales como baterías y sistemas especiales de control para lograr en todo momento la estabilidad del suministro eléctrico del sistema.

Debido a su viabilidad económica (el transporte de diésel al punto de consumo suele ser costoso) en muchos casos se sustituyen antiguos generadores por fotovoltaica, mientras que las nuevas instalaciones híbridas se diseñan de tal manera que permiten utilizar el recurso solar siempre que está disponible, minimizando el uso de los generadores, disminuyendo así el impacto ambiental de la generación eléctrica en comunidades remotas y en instalaciones que no están conectadas a la red eléctrica. Un ejemplo de ello lo constituyen las empresas mineras, cuyas explotaciones se encuentran normalmente en campo abierto, alejadas de los grandes núcleos de población. En estos casos, el uso combinado de la fotovoltaica permite disminuir en gran medida la dependencia del combustible diésel, permitiendo ahorros de hasta el 70 % en el coste de la energía.

Este tipo de sistemas también puede utilizarse en combinación con otras fuentes de generación de energía renovable, tales como la energía eólica.

Aunque la fotovoltaica todavía no se utiliza de forma generalizada para proporcionar tracción en el transporte, se está utilizando cada vez en mayor medida para proporcionar energía auxiliar en barcos y automóviles. Algunos vehículos están equipados con aire acondicionado alimentado mediante paneles fotovoltaicos para limitar la temperatura interior en los días calurosos, mientras que otros prototipos híbridos los utilizan para recargar sus baterías sin necesidad de conectarse a la red eléctrica. Se ha demostrado sobradamente la posibilidad práctica de diseñar y fabricar vehículos propulsados mediante energía solar, así como barcos y aviones, siendo considerado el transporte rodado el más viable para la fotovoltaica.

El "Solar Impulse" es un proyecto dedicado al desarrollo de un avión propulsado únicamente mediante energía solar fotovoltaica. El prototipo puede volar durante el día propulsado por las células solares que cubren sus alas, a la vez que carga las baterías que le permiten mantenerse en el aire durante la noche.

La energía solar también se utiliza de forma habitual en faros, boyas y balizas de navegación marítima, vehículos de recreo, sistemas de carga para los acumuladores eléctricos de los barcos, y sistemas de protección catódica. La recarga de vehículos eléctricos está cobrando cada vez mayor importancia.

Muchas instalaciones fotovoltaicas se encuentran a menudo situadas en los edificios: normalmente se sitúan sobre un tejado ya existente, o bien se integran en elementos de la propia estructura del edificio, como tragaluces, claraboyas o fachadas.

Alternativamente, un sistema fotovoltaico también puede ser emplazado físicamente separado del edificio, pero conectado a la instalación eléctrica del mismo para suministrar energía. En 2010, más del 80 % de los 9000MW de fotovoltaica que Alemania tenía en funcionamiento por entonces, se habían instalado sobre tejados.

La fotovoltaica integrada en edificios ("BIPV", en sus siglas en inglés) se está incorporando de forma cada vez más creciente como fuente de energía eléctrica principal o secundaria en los nuevos edificios domésticos e industriales, e incluso en otros elementos arquitectónicos, como por ejemplo puentes. Las tejas con células fotovoltaicas integradas son también bastante comunes en este tipo de integración.

Según un estudio publicado en 2011, el uso de imágenes térmicas ha demostrado que los paneles solares, siempre que exista una brecha abierta por la que el aire pueda circular entre los paneles y el techo, proporcionan un efecto de refrigeración pasiva en los edificios durante el día y además ayudan a mantener el calor acumulado durante la noche.

Una de las principales aplicaciones de la energía solar fotovoltaica más desarrollada en los últimos años, consiste en las centrales conectadas a red para suministro eléctrico, así como los sistemas de autoconsumo fotovoltaico, de potencia generalmente menor, pero igualmente conectados a la red eléctrica.

Una planta solar fotovoltaica cuenta con distintos elementos que permiten su funcionamiento, como son los paneles fotovoltaicos para la captación de la radiación solar, y los inversores para la transformación de la corriente continua en corriente alterna. Existen otros, los más importantes se mencionan a continuación:

Generalmente, un módulo o panel fotovoltaico consiste en una asociación de células, encapsulada en dos capas de EVA (etileno-vinilo-acetato), entre una lámina frontal de vidrio y una capa posterior de un polímero termoplástico (frecuentemente se emplea el tedlar) u otra lámina de cristal cuando se desea obtener módulos con algún grado de transparencia. Muy frecuentemente este conjunto es enmarcado en una estructura de aluminio anodizado con el objetivo de aumentar la resistencia mecánica del conjunto y facilitar el anclaje del módulo a las estructuras de soporte.

Las células más comúnmente empleadas en los paneles fotovoltaicos son de silicio, y se puede dividir en tres subcategorías:

La corriente eléctrica continua que proporcionan los módulos fotovoltaicos se puede transformar en corriente alterna mediante un aparato electrónico llamado inversor e inyectar en la red eléctrica (para venta de energía) o bien en la red interior (para autoconsumo).

El proceso, simplificado, sería el siguiente:

En las etapas iniciales del desarrollo de los inversores fotovoltaicos, los requisitos de los operadores de las redes eléctricas a la que se conectaban solicitaban únicamente el aporte de energía activa y la desconexión del inversor de la red si ésta excedía de unos ciertos límites de voltaje y frecuencia. Con el progresivo desarrollo de estos equipos y la cada vez mayor importancia de las redes eléctricas inteligentes, los inversores son ya capaces de proveer energía reactiva e incluso aportar estabilidad a la red eléctrica.

El uso de seguidores a uno o dos ejes permite aumentar considerablemente la producción solar, en torno al 30 % para los primeros y un 6 % adicional para los segundos, en lugares de elevada radiación directa.

Los seguidores solares son bastante comunes en aplicaciones fotovoltaicas. Existen de varios tipos:

Es el elemento que transporta la energía eléctrica desde su generación, para su posterior distribución y transporte. Su dimensionamiento viene determinado por el criterio más restrictivo entre la máxima caída de tensión admisible y la intensidad máxima admisible. Aumentar las secciones de conductor que se obtienen como resultado de los cálculos teóricos aporta ventajas añadidas como:


Otro tipo de tecnología en las plantas fotovoltaicas son las que utilizan una tecnología de concentración llamada CPV por sus siglas en inglés ("Concentrated Photovoltaics") para maximizar la energía solar recibida por la instalación, al igual que en una central térmica solar. Las instalaciones de concentración fotovoltaica se sitúan en emplazamientos de alta irradiación solar directa, como son los países a ambas riberas del Mediterráneo, Australia, Estados Unidos, China, Sudáfrica, México, etc. Hasta el año 2006 estas tecnologías formaban parte del ámbito de investigación, pero en los últimos años se han puesto en marcha instalaciones de mayor tamaño como la de ISFOC (Instituto de Sistemas Solares Fotovoltaicos de Concentración) en Puertollano (Castilla La Mancha) con 3 MW suministrando electricidad a la red eléctrica.

La idea básica de la concentración fotovoltaica es la sustitución de material semiconductor por material reflectante o refractante (más barato). El grado de concentración puede alcanzar un factor de 1000, de tal modo que, dada la pequeña superficie de célula solar empleada, se puede utilizar la tecnología más eficiente (triple unión, por ejemplo). Por otro lado, el sistema óptico introduce un factor de pérdidas que hace recuperar menos radiación que la fotovoltaica plana. Esto, unido a la elevada precisión de los sistemas de seguimiento, constituye la principal barrera a resolver por la tecnología de concentración.

Recientemente se ha anunciado el desarrollo de plantas de grandes dimensiones (por encima de 1 MW). Las plantas de concentración fotovoltaica utilizan un seguidor de doble eje para posibilitar un máximo aprovechamiento del recurso solar durante todo el día.

Entre los años 2001 y 2016 se ha producido un crecimiento exponencial de la producción fotovoltaica, duplicándose aproximadamente cada dos años. La potencia total fotovoltaica instalada en el mundo (conectada a red) ascendía a 16gigavatios (GW) en 2008, 40GW en 2010, 100GW en 2012, 180GW en 2014 y 300GW en 2016.
Históricamente, Estados Unidos lideró la instalación de energía fotovoltaica desde sus inicios hasta 1996, cuando su capacidad instalada alcanzaba los 77MW, más que cualquier otro país hasta la fecha. En los años posteriores, fueron superados por Japón, que mantuvo el liderato hasta que a su vez Alemania la sobrepasó en 2005, manteniendo el liderato desde entonces. A comienzos de 2016, Alemania se aproximaba a los 40GW instalados. Sin embargo, por esas fechas China, uno de los países donde la fotovoltaica está experimentando un crecimiento más vertiginoso superó a Alemania, convirtiéndose desde entonces en el mayor productor de energía fotovoltaica del mundo. Se espera que multiplique su potencia instalada actual hasta los 150GW en 2020.

La capacidad total instalada supone ya una fracción significativa del mix eléctrico en la Unión Europea, cubriendo de media el 3,5% de la demanda de electricidad y alcanzando el 7% en los períodos de mayor producción. En algunos países, como Alemania, Italia, Reino Unido o España, alcanza máximos superiores al 10%, al igual que en Japón o en algunos estados soleados de Estados Unidos, como California. La producción anual de energía eléctrica generada mediante esta fuente de energía a nivel mundial equivalía en 2015 a cerca de 184 TWh, suficiente para abastecer las necesidades energéticas de millones de hogares y cubriendo aproximadamente un 1% de la demanda mundial de electricidad.

La energía fotovoltaica se ha convertido en una de las mayores industrias de la República Popular China. El país asiático es líder mundial por capacidad fotovoltaica, con una potencia instalada a principios de 2016 superior a los 43 GW. Cuenta además con unas 400 empresas fotovoltaicas, entre las que destacan Trina Solar y Yingli, gigantes mundiales en la fabricación de paneles solares. En 2014 producía aproximadamente la mitad de los productos fotovoltaicos que se fabrican en el mundo (China y Taiwán juntos suman más del 60 % de cuota). La producción de paneles y células fotovoltaicas en China se ha incrementado notablemente durante la última década: en 2001 mantenía una cuota inferior al 1 % del mercado mundial, mientras que por las mismas fechas, Japón y Estados Unidos sumaban más del 70 % de la producción mundial. Sin embargo, la tendencia se ha invertido y en la actualidad China supera ampliamente al resto de productores.

La capacidad de producción de paneles solares chinos prácticamente se cuadruplicó entre los años 2009 y 2011, superando incluso la demanda mundial. Como resultado, la Unión Europea acusó a la industria china de estar realizando dumping, es decir vendiendo sus paneles a precios por debajo de coste, imponiendo aranceles a la importación de este material.

La instalación de energía fotovoltaica se ha desarrollado espectacularmente en el país asiático en años recientes, superando incluso las previsiones iniciales. Debido a tan rápido crecimiento, las autoridades chinas se han visto obligadas a revaluar en varias ocasiones su objetivo de potencia fotovoltaica.

La potencia total instalada en China creció hasta los 77GW a finales de 2016, tras conectar 36GW en el último año, de acuerdo a las estadísticas oficiales del país. En 2017, China habría superado el objetivo marcado por el gobierno para 2020, una potencia fotovoltaica de 100GW.

Este crecimiento refleja el abrupto descenso de costes de la energía fotovoltaica, que actualmente comienza a ser una opción más barata que otras fuentes de energía, tanto a precios minoristas como comerciales. Fuentes del gobierno chino han afirmado que la fotovoltaica presentará precios más competitivos que el carbón y el gas (aportando además una mayor independencia energética) a finales de esta década.

La energía fotovoltaica en Japón, se ha expandido rápidamente desde la década de 1990. El país es uno de los líderes en la fabricación de módulos fotovoltaicos y se encuentra entre los primeros puestos en términos de potencia instalada, con más de 23GW a finales de 2014, la mayor parte conectada a red. La irradiación en Japón es óptima, situándose entre 4,3 y 4,8kWh·m²·día, convirtiéndolo en un país idóneo para el desarrollo de este tipo de energía.

La venta de módulos fotovoltaicos para proyectos comerciales ha crecido rápidamente tras la introducción por parte del Gobierno japonés en julio de 2012 de una tarifa para el incentivo de la fotovoltaica tras el accidente nuclear de Fukushima y la paralización de la mayoría de las centrales nucleares que tiene el país.

La mayoría de módulos procede de fabricantes locales, entre los que destacan Kyocera, Sharp Corporation, Mitsubishi o Sanyo, mientras que una pequeña parte fueron son importados, según se desprende de los datos de la Asociación Japonesa de Energía Fotovoltaica ("Japan Photovoltaic Energy Association", JPA). Tradicionalmente, el mercado fotovoltaico ha estado muy desplazado al segmento residencial, copando hasta el 97 % de la capacidad instalada en todo el país hasta 2012. Aunque esta tendencia se está invirtiendo, todavía más del 75 % de las células y módulos vendidos en Japón a principios de 2012 tuvieron como destino proyectos residenciales, mientras que cerca del 9 % se emplearon en instalaciones fotovoltaicas comerciales.

En 2014, la potencia total fotovoltaica instalada en el país se situaba en torno a los 23GW, que contribuían aproximadamente en un 2,5 % a la demanda eléctrica del país. Durante el verano de 2015, se informó que la producción fotovoltaica en Japón había cubierto en determinados momentos el 10% de la demanda total nacional. Dos años después, en 2016, se sitúa en torno a 42 GW, y la previsión apunta a que el mercado fotovoltaico japonés crecerá aún más en los próximos años.

Estados Unidos es desde 2010 uno de los países con mayor actividad en el mercado fotovoltaico, cuenta con grandes empresas del sector, como First Solar o SolarCity, así como numerosas plantas de conexión a red. A principios de 2017, Estados Unidos superaba los 40GW de potencia fotovoltaica instalada, suficiente para proporcionar electricidad a más de 8 millones de hogares, tras duplicar su capacidad solar en menos de dos años.

Aunque Estados Unidos no mantiene una política energética nacional uniforme en todo el país en lo referente a fotovoltaica, muchos estados han fijado individualmente objetivos en materia de energías renovables, incluyendo en esta planificación a la energía solar en diferentes proporciones. En este sentido, el gobernador de California Jerry Brown ha firmado una legislación requiriendo que el 33 % de la electricidad del estado se genere mediante energías renovables a finales de 2020. Estas medidas se han visto apoyadas desde el gobierno federal con la adopción del Investment Tax Credit (ITC), una exención fiscal establecida en 2006 para promover el desarrollo de proyectos fotovoltaicos, y que ha sido extendida recientemente hasta 2023.

Un informe privado recoge que la energía solar fotovoltaica se ha expandido rápidamente durante los últimos 8 años, creciendo a una media del 40 % cada año. Gracias a esta tendencia, el coste del kWh producido mediante energía fotovoltaica se ha visto enormemente reducido, mientras que el coste de la electricidad generada mediante combustibles fósiles no ha dejado de incrementar. Como resultado, el informe concluye que la fotovoltaica alcanzará la paridad de red frente a las fuentes de energía convencionales en muchas regiones de Estados Unidos en 2015. Pero para alcanzar una cuota en el mercado energético del 10 %, prosigue el informe, las compañías fotovoltaicas necesitarán estilizar aún más las instalaciones, de forma que la energía solar se convierta en una tecnología directamente enchufable («"plug-and-play»"). Es decir, que sea sencillo adquirir los componentes de cada sistema y su interconexión sea simple, al igual que su conexión a la red.

Actualmente la mayoría de las instalaciones son conectadas a red y utilizan sistemas de balance neto que permiten el consumo de electricidad nocturno de energía generada durante el día. Nueva Jersey lidera los Estados con la ley de balance neto menos restrictiva, mientras California lidera el número total de hogares con energía solar. Muchos de ellos fueron instalados durante la iniciativa "million solar roof" (un millón de tejados solares).

La tendencia y el ritmo de crecimiento actuales indican que en los próximos años se construirán un gran número de plantas fotovoltaicas en el sur y suroeste del país, donde el terreno disponible es abundante, en los soleados desiertos de California, Nevada y Arizona. Las empresas están adquiriendo cada vez en mayor medida grandes superficies en estas zonas, con la intención de construir mayores plantas a gran escala.

Alemania dispone a principios de 2016 de una potencia instalada cercana a los 40 GW. Sólo en 2011, Alemania instaló cerca de 7,5GW, y la fotovoltaica produjo 18TW·h de electricidad, el 3 % del total consumido en el país.
El mercado fotovoltaico en Alemania ha crecido considerablemente desde principios del siglo XXI gracias a la creación de una tarifa regulada para la producción de energía renovable, que fue introducida por la «"German Renewable Energy Act»", ley publicada el año 2000. Desde entonces, el coste de las instalaciones fotovoltaicas ha descendido más del 50 % en cinco años, desde 2006. Alemania se ha marcado el objetivo de producir el 35 % de la electricidad mediante energías renovables en 2020 y alcanzar el 100 % en 2050.

En 2012, las tarifas introducidas costaban a Alemania unos 14000 millones de euros por año, tanto para las instalaciones eólicas como solares. Este coste es repartido entre todos los contribuyentes mediante un sobrecoste de 3,6 céntimos de € por kWh (aproximadamente el 15 % del coste total de la electricidad para el consumidor doméstico).

La considerable potencia instalada en Alemania ha protagonizado varios récords durante los últimos años. Durante dos días consecutivos de mayo de 2012, por ejemplo, las plantas solares fotovoltaicas instaladas en el país produjeron 22000MWh en la hora del mediodía, lo que equivale a la potencia de generación de veinte centrales nucleares trabajando a plena capacidad. Alemania pulverizó este récord el 21 de julio de 2013, con una potencia instantánea de 24GW a mediodía. Debido al carácter altamente distribuido de la fotovoltaica alemana, aproximadamente 1,3-1,4 millones de pequeños sistemas fotovoltaicos contribuyeron a esta nueva marca. Aproximadamente el 90 % de los paneles solares instalados en Alemania se encuentran situados sobre tejado.

En junio de 2014, la fotovoltaica alemana volvió a batir récords durante varios días, al producir hasta el 50,6 % de toda la demanda eléctrica durante un sólo día, y superar el anterior récord de potencia instantánea hasta los 24,24GW.

A comienzos de verano de 2011, el Gobierno alemán anunció que el esquema actual de tarifas reguladas concluiría cuando la potencia instalada alcanzase los 52GW. Cuando esto suceda, Alemania aplicará un nuevo esquema de tarifas de inyección cuyos detalles no se conocen todavía.

No obstante, consciente de que el almacenamiento de energía mediante baterías es indispensable para el despliegue masivo de renovables como la energía eólica o la fotovoltaica, dada su intermitencia, el 1 de mayo de 2013 Alemania puso en marcha un nuevo programa de ayudas para incentivar sistemas fotovoltaicos con baterías de almacenamiento. De esta manera, se financia a las instalaciones fotovoltaicas menores de 30kW que instalen baterías y acumulen electricidad, con 660 euros por cada kW de almacenamiento de batería. El programa está dotado con 25 millones de euros anuales repartidos en 2013 y 2014, y de esta forma se logra disponer de la energía cuando el recurso no esté disponible —no haya viento o sea de noche—, además de facilitar la estabilidad del sistema eléctrico.

India está densamente poblada y tiene también una gran irradiación solar, lo que hace del país uno de los mejores candidatos para el desarrollo de la fotovoltaica. En 2009, India anunció un programa para acelerar el uso de instalaciones solares en los edificios gubernamentales, al igual que en hospitales y hoteles.

La caída en el precio de los paneles fotovoltaicos ha coincidido con un incremento del precio de la electricidad en la India. El apoyo del gobierno y la abundancia del recurso solar han ayudado a impulsar la adopción de esta tecnología.

El parque solar Charanka, de 345MW (uno de los mayores del mundo) fue puesto en servicio en abril de 2012 y ampliado en 2015, junto a un total de 605MW en la región de Gujarat. La construcción de otros grandes parques solares ha sido anunciada en el estado de Rajasthan. También el parque solar de Dhirubhai Ambani, de 40 MW, fue inaugurado en 2012.

En enero de 2015, el gobierno indio incrementó de forma significativa su planes de desarrollo solar, estableciendo un objetivo de inversiones por valor de 100000 millones de dólares y 100GW de capacidad solar para 2022.

A comienzos de 2017, la potencia total instalada en India se situaba por encima de los 10GW. India espera alcanzar rápidamente los 20GW instalados, cumpliendo su objetivo de crear 1 millón de puestos de trabajo y alcanzar 100GW en 2022.

Italia se encuentra entre los primeros países productores de electricidad procedente de energía fotovoltaica, gracias al programa de incentivos llamado "Conto Energia". El crecimiento ha sido exponencial en los últimos años: la potencia instalada se triplicó en 2010 y se cuadruplicó en 2011, llegando a producir en 2012 el 5,6 % de la energía total consumida en el país.

Este programa contaba con un presupuesto total de 6700 millones de €, alcanzado dicho límite el Gobierno ha dejado de incentivar las nuevas instalaciones, al haberse alcanzado la paridad de red. Un informe publicado en 2013 por el Deutsche Bank concluía que efectivamente la paridad de red se había alcanzado en Italia y otros países del mundo. El sector ha llegado a proporcionar trabajo a unas 100000 personas, especialmente en el sector del diseño e instalación de dichas plantas solares.

Desde mediados de 2012 está vigente una nueva legislación que obliga a registrar todas las plantas superiores a 12kW; las de potencia menor (fotovoltaica de tejado en residencias) están exentas de registro. A finales de 2016, la potencia total instalada se situaba por encima de 19GW, suponiendo una producción energética tan importante que varias centrales de gas operaban a mitad de su potencial durante el día.

La energía solar en Reino Unido, aunque relativamente desconocida hasta hace poco, ha despegado muy rápidamente en años recientes, debido a la drástica caída del precio de los paneles fotovoltaicos y la introducción de tarifas reguladas a partir de abril de 2010. En 2014, había censadas ya unas 650000 instalaciones solares en las islas británicas, con una capacidad total cercana a los 5GW. La planta solar más grande del país se encuentra en "Southwick Estate", cerca de Fareham, y cuenta con una potencia de 48MW. Fue inaugurada en marzo de 2015.

En 2012, el gobierno británico de David Cameron se comprometió a abastecer cuatro millones de hogares mediante energía solar en menos de ocho años, lo que equivale a instalar unos 22GW de capacidad fotovoltaica antes de 2020. A principios de 2016, Reino Unido había instalado más de 10GW de energía solar fotovoltaica.

Entre los meses de abril y septiembre de 2016, la energía solar produjo en Reino Unido más electricidad (6964GWh) que la producida mediante carbón (6342GWh), ambas se sitúan en torno a un 5% de la demanda.

El mercado francés es el cuarto más importante dentro de la Unión Europea, tras los mercados de Alemania, Italia y Reino Unido. A finales de 2014 contaba con más de 5GW instalados, y mantiene actualmente un crecimiento sostenido, estimándose que en 2015 conectará a la red eléctrica 1GW adicional a la capacidad actual. Recientemente, el país galo incrementó el cupo de sus subastas para energía fotovoltaica de 400 a 800 MW, como consecuencia del reconocimiento gubernamental a la cada vez mayor competitividad de la energía solar.

La planta fotovoltaica más grande de Europa, un proyecto de 300MW llamado Cestas, se encuentra en territorio francés. Su entrada en funcionamiento tuvo lugar a finales de 2015, proporcionando al sector fotovoltaico un ejemplo a seguir por el resto de la industria europea.

España es uno de los países de Europa con mayor irradiación anual. Esto hace que la energía solar sea en este país más rentable que en otros. Regiones como el norte de España, que generalmente se consideran poco adecuadas para la energía fotovoltaica, reciben más irradiación anual que la media en Alemania, país que mantiene desde hace años el liderazgo en la promoción de la energía solar fotovoltaica.

Desde principios de la década de 2000, en concordancia con las medidas de apoyo a las energías renovables que se estaban llevando a cabo en el resto de Europa, se había venido aprobando la regulación que establece las condiciones técnicas y administrativas, y que supuso el inicio de un lento despegue de la fotovoltaica en España. En 2004, el gobierno español eliminó las barreras económicas para la conexión de las energías renovables a la red eléctrica. El Real Decreto 436/2004 igualó las condiciones para su producción a gran escala, y garantizó su venta mediante primas a la generación.

Gracias a esta regulación, y el posterior RD 661/2007, España fue en el año 2008 uno de los países con más potencia fotovoltaica instalada del mundo, con 2708MW instalados en un sólo año. Sin embargo, posteriores modificaciones en la legislación del sector ralentizaron la construcción de nuevas plantas fotovoltaicas, de tal forma que en 2009 se instalaron tan sólo 19MW, en 2010, 420MW, y en 2011 se instalaron 354MW, correspondiendo al 2 % del total de la Unión Europea. 

En términos de producción energética, en 2010 la energía fotovoltaica cubrió en España aproximadamente el 2 % de la generación de electricidad, mientras que en 2011 y 2012 representó el 2,9 %, y en 2013 el 3,1 % de la generación eléctrica según datos del operador, Red Eléctrica.

A principios de 2012, el Gobierno español aprobó un Real Decreto Ley por el que se paralizó la instalación de nuevas centrales fotovoltaicas y demás energías renovables. A finales de 2015 la potencia fotovoltaica instalada en España ascendía a 4667MW. En 2017, España cayó por primera vez de la lista de los diez países con mayor capacidad fotovoltaica instalada, al ser superado por Australia y Corea del Sur. Sin embargo, en julio de 2017, el Gobierno organizó una subasta que adjudicó más de 3500 MW de nuevas plantas de energía fotovoltaica, que permitirán a España alcanzar los objetivos de generación de energía renovable establecidos por la Unión Europea para 2020. Como novedad, ni la construcción de las plantas adjudicadas ni su operación supondrá algún coste para el sistema, excepto en el caso de que el precio de mercado baje de un suelo establecido en la subasta. La gran bajada de costes de la energía fotovoltaica ha permitido que grandes empresas hayan licitado a precio de mercado.

En Latinoamérica, la fotovoltaica ha comenzado a despegar en los últimos años. Se ha propuesto la construcción de un buen número de plantas solares en diversos países, a lo largo de toda la región, con proyectos incluso por encima de 100MW en Chile.

Chile de hecho ya lidera la producción solar en Iberoamérica. Este país inauguró en junio de 2014 una central fotovoltaica de 100MW, que se convirtió en la mayor realizada hasta la fecha en Iberoamérica. El elevado precio de la electricidad y los altos niveles de radiación que existen en el norte de Chile, han promovido la apertura de un importante mercado libre de subsidios. A julio de 2017, el país andino contaba con 1.873MW fotovoltaicos en operación.

Otros países sudamericanos han comenzado a instalar plantas fotovoltaicas a gran escala, entre ellos Perú. Brasil en cambio está experimentando un crecimiento más lento del sector, en parte debido a la elevada generación mediante energía hidráulica en el país, aunque el estado de Minas Gerais lidera el esfuerzo, tras la aprobación por parte del gobierno brasileño de una fábrica de células y paneles fotovoltaicos en dicha región.

México también tiene un enorme potencial en lo que respecta a energía solar. Un 70 % de su territorio presenta una irradiación superior a 4,5kWh/m²/día, lo que lo convierte en un país muy soleado, e implica que utilizando la tecnología fotovoltaica actual, una planta solar de 25km² en cualquier lugar del estado de Chihuahua o el desierto de Sonora (que ocuparía el 0,01 % de la superficie de México) podría proporcionar toda la electricidad demandada por el país. México cuenta ya con más de 200MW instalados. El proyecto "Aura Solar", situado en La Paz (Baja California Sur), inaugurado a principios de 2014, tenía previsto generar 82GWh al año, suficiente para abastecer el consumo de 164000 habitantes (65 % de la población de La Paz), pero fue arrasado por el huracán Odile en septiembre del mismo año y la planta no opera desde entonces. La instalación cubría una superficie de 100 hectáreas con 131800 módulos policristalinos sobre seguidores de un eje.

Otra planta fotovoltaica de 47MW se encuentra en fase de planificación en Puerto Libertad (Sonora).La planta, originalmente diseñada para albergar 39MW, se amplió para permitir la generación de 107GWh/año.

Se espera que México experimente un mayor crecimiento en los próximos años, con el fin de alcanzar el objetivo de cubrir el 35 % de su demanda energética a partir de energías renovables en 2024, según una ley aprobada por el gobierno mexicano en 2012. A comienzos de 2014, México tenía previstos proyectos fotovoltaicos por una potencia de 300MW, de los cuales aproximadamente 100MW comenzaron a desarrollarse durante el último trimestre de 2013.

En la siguiente tabla se muestra el detalle de la potencia mundial instalada, desglosada por cada país, desde el año 2002 hasta 2016:

Se estima que la potencia fotovoltaica instalada ha crecido unos 75GW en 2016, y China ha tomado el liderato frente a Alemania siendo ya el mayor productor de energía fotovoltaica. Para 2019, se estima que la potencia total alcanzará en todo el mundo 396GW (escenario moderado) o incluso 540GW (escenario optimista).

La consultora Frost & Sullivan estima que la potencia fotovoltaica se incrementará hasta los 446 GW para 2020, siendo China, India y Estados Unidos los países con un mayor crecimiento, mientras Europa verá duplicada su capacidad respecto a los niveles actuales. La firma Grand View Research, consultora y analista de mercados radicada en San Francisco, publicó sus estimaciones para el sector en marzo de 2015. El potencial fotovoltaico de países como Brasil, Chile y Arabia Saudí todavía no se ha desarrollado conforme a lo esperado, y se espera que sea desarrollado durante los próximos años. Además de ello, el aumento de la capacidad de manufactura en China se prevé que siga ayudando a disminuir aún más los precios en descenso. La consultora estima que la capacidad fotovoltaica mundial alcance los 490 GW en 2020.

La organización PV Market Alliance (PVMA), un consorcio formado por varias entidades de investigación, calcula que la capacidad global estará entre los 444-630GW en 2020. En el escenario más pesimista, prevé que el ritmo de instalación anual se sitúe entre los 40 y 50 gigavatios al finalizar la década, mientras que en el escenario más optimista estima que se instalen entre 60 y 90 GW anuales durante los próximos cinco años. El escenario intermedio estima que se sitúen entre 50 y 70 GW, para alcanzar 536 GW en 2020. Las cifras de PVMA concuerdan con las publicadas anteriormente por "Solar Power Europe". En junio de 2015, Greentech Media (GTM) publicó su informe "Global PV Demand Outlook" para 2020, que estima que las instalaciones anuales se incrementarán de 40 a 135 GW, alcanzando una capacidad total global de casi 700 GW en 2020. La estimación de GTM es la más optimista de todas las publicadas hasta la fecha, estimando que se instalarán 518 GW entre 2015 y 2020, lo que supone más del doble que otras estimaciones.

Por su parte, EPIA también calcula que la energía fotovoltaica cubrirá entre un 10 y un 15 % de la demanda de Europa en 2030.
Un informe conjunto de esta organización y Greenpeace publicado en 2010 muestra que para el año 2030, un total de 1845 GW fotovoltaicos podrían generar aproximadamente 2646 TWh/año de electricidad en todo el mundo. Combinado con medidas de eficiencia energética, esta cifra representaría cubrir el consumo de casi un 10 % de la población mundial. Para el año 2050, se estima que más del 20 % de la electricidad mundial podría ser cubierto por la energía fotovoltaica.

En Europa y en el resto del mundo se han construido un gran número de centrales fotovoltaicas a gran escala. A mediados de 2017, las plantas fotovoltaicas más grandes del mundo eran, de acuerdo a su capacidad de producción:

A mediados de 2017, las mayores plantas solares del mundo se encuentran situadas en China e India. "Kurnool Solar", en el estado indio de Andhra Pradesh alberga 1 GW de capacidad, equivalente en potencia a una central nuclear. La planta "Yanchi Solar", en la provincia de Qinghai (China) cuenta asimismo con dicha capacidad. Entre los primeros puestos se encuentra también "Longyangxia Hydro-Solar PV Station", situada junto a la presa de Longyangxia en China. Consiste en un macrocomplejo hidroeléctrico de 1280MW, al que posteriormente se le añadió una central fotovoltaica de 320MW, completada en 2013. A finales de 2015 se inauguró una segunda fase de 530MW, lo que elevó la potencia total de la planta solar hasta los 850MW.

Otros proyectos de gran escala se encuentran situados en Estos Unidos. "Solar Star", tiene una potencia de 579MW y se encuentra en California. Las plantas "Topaz Solar Farm" y "Desert Sunlight Solar Farm" en Riverside County, también en California, tiene asimismo una potencia de 550MW. El proyecto "Blythe Solar Power" consiste en una planta fotovoltaica de 500MW, situada igualmente en Riverside County, cuya construcción está prevista próximamente. En Europa, el proyecto de mayor envergadura se llama "Cestas Solar Power Plant", ubicado en la localidad de Cestas (Francia), que cuenta con una capacidad de 300MW y entró en operación a finales de 2015.

Hay otras muchas plantas de gran escala en construcción. El "McCoy Solar Energy Project", en Estados Unidos, tendrá una potencia de 750MW una vez completado. En los últimos años, se ha propuesto la construcción de varias plantas de potencias superiores a los 1000MW en diferentes lugares del mundo. La planta Quaid-e-Azam Solar Park, situada en Pakistán y cuya primera fase ya se encuentra operativa con 100MW, tiene previsto ampliar su capacidad hasta los 1500MW. Los Emiratos Árabes Unidos planean también la construcción de una planta de 1000MW. El "Ordos Solar Project", situado en China, alcanzará los 2000 MW. El proyecto Westlands Solar Park tiene una capacidad prevista de 2700MW, a ser completado en varias fases.

En lo que respecta a instalaciones fotovoltaicas sobre tejado, la mayor instalación se encuentra en las instalaciones de Renault Samsung Motors en Busan (Corea del Sur), y cuenta con 20MW distribuidos sobre las diferentes cubiertas, parkings e infraestructuras del complejo. Inaugurada en 2013, proporciona energía a la fábrica y miles de hogares cercanos.

El almacenamiento de energía se presenta como un reto importante para permitir contar con un suministro continuo de energía, dado que la energía solar no se puede generar por la noche. Las baterías recargables se han usado tradicionalmente para almacenar el exceso de electricidad en sistemas aislados. Con la aparición de los sistemas conectados a red, el exceso de electricidad puede transportarse mediante la red eléctrica a los puntos de consumo. Cuando la producción de energía renovable supone una pequeña fracción de la demanda, otras fuentes de energía pueden ajustar su producción de forma apropiada para prestar un respaldo a la variabilidad de las fuentes renovables, pero con el crecimiento de estas últimas, se hace necesario un control más adecuado para el equilibrio de la red.

Con el declive de los precios, las centrales fotovoltaicas comienzan a disponer de baterías para controlar la potencia de salida o almacenar el exceso de energía para que pueda ser empleado durante las horas en que las centrales renovables no pueden generar directamente. Este tipo de baterías permite estabilizar la red eléctrica al suavizar los picos de demanda durante minutos u horas. Se prevé que en el futuro estas baterías jugarán un papel importante en la red eléctrica, ya que pueden ser cargadas durante los períodos cuando la generación excede la demanda y verter dicha energía en la red cuando la demanda es mayor que la generación.

Por ejemplo, en Puerto Rico un sistema con una capacidad de 20 megavatios durante 15 minutos (5 megavatios hora) se emplea para estabilizar la frecuencia de la red en la isla. Otro sistema de 27 megavatios durante 15 minutos (6,75 megavatios hora) con baterías de níquel-cadmio fue instalado en Fairbanks (Alaska) en 2003 para estabilizar el voltaje de las líneas de transmisión.

La mayoría de estos bancos de baterías se encuentran localizados junto a las propias plantas fotovoltaicas. Los mayores sistemas en Estados Unidos incluyen la batería de 31,5MW en la planta Grand Ridge Power en Illinois, y la batería de 31,5MW en Beech Ridge, Virginia. Entre los proyectos más destacados se sitúan el sistema de 400MWh (100MW durante cuatro horas) del proyecto "Southern California Edison" y un proyecto de 52MWh en Kauai (Hawaii), que permite desplazar por completo la producción de una planta de 13MW para su uso tras la puesta del sol. Otros proyectos se sitúan en Fairbanks (40MW para 7 minutos mediante baterías de níquel-cadmio) y en Notrees (Texas) (36MW para 40 minutos usando baterías de plomo-ácido).

En 2015, se instaló un total de 221MW con almacenamiento de baterías en Estados Unidos, y se estima que la potencia total de este tipo de sistemas crezca hasta los 1,7GW en 2020. La mayoría instalada por las propias compañías mayoristas del mercado estadounidense.

El autoconsumo fotovoltaico consiste en la producción individual a pequeña escala de electricidad para el propio consumo, a través de paneles fotovoltaicos. Ello se puede complementar con el balance neto. Este esquema de producción, que permite compensar el consumo eléctrico mediante lo generado por una instalación fotovoltaica en momentos de menor consumo, ya ha sido implantado con éxito en muchos países. Fue propuesto en España por la Asociación de la Industria Fotovoltaica (ASIF) para promover la electricidad renovable sin necesidad de apoyo económico adicional, y estuvo en fase de proyecto por el IDAE. Posteriormente se recogió en el Plan de Energías Renovables 2011-2020, pero todavía no ha sido regulado.

Sin embargo, en los últimos años, debido al creciente auge de pequeñas instalaciones de energía renovable, el autoconsumo con balance neto ha comenzado a ser regulado en diversos países del mundo, siendo una realidad en países como Alemania, Italia, Dinamarca, Japón, Australia, Estados Unidos, Canadá y México, entre otros.

Entre las ventajas del autoconsumo respecto al consumo de la red se encuentran las siguientes:

En el caso del autoconsumo fotovoltaico, el tiempo de retorno de la inversión se calcula sobre la base de cuánta electricidad se deja de consumir de la red, debido al empleo de paneles fotovoltaicos.

Por ejemplo, en Alemania, con precios de la electricidad en 0,25€/kWh y una insolación de 900kWh/kWp, una instalación de 1kWp ahorra unos 225€ al año, lo que con unos costes de instalación de 1700€/kWp significa que el sistema se amortizará en menos de 7 años. Esta cifra es aún menor en países como España, con una irradiación superior a la existente en el norte del continente europeo.

Las eficiencias de las células solares varían entre el 6 % de aquellas basadas en silicio amorfo hasta el 46 % de las células multiunión. Las eficiencias de conversión de las células solares que se utilizan en los módulos fotovoltaicos comerciales (de silicio monocristalino o policristalino) se encuentran en torno al 16-22 %.

El coste de las células solares de silicio cristalino ha descendido desde 76,67$/Wp en 1977 hasta aproximadamente 0,36$/Wp en 2014. Esta tendencia sigue la llamada ley de Swanson, una predicción similar a la conocida Ley de Moore, que establece que los precios de los módulos solares descienden un 20 % cada vez que se duplica la capacidad de la industria fotovoltaica.

En 2014, el precio de los módulos solares se había reducido en un 80 % desde el verano de 2008, colocando a la energía solar por primera vez en una posición ventajosa respecto al precio de la electricidad pagado por el consumidor en un buen número de regiones soleadas. En este sentido, el coste medio de generación eléctrica de la energía solar fotovoltaica es ya competitivo con el de las fuentes convencionales de energía en una creciente lista de países, particularmente cuando se considera la hora de generación de dicha energía, ya que la electricidad es usualmente más cara durante el día. Se ha producido una dura competencia en la cadena de producción, y asimismo se esperan mayores caídas del coste de la energía fotovoltaica en los próximos años, lo que supone una creciente amenaza al dominio de las fuentes de generación basadas en las energías fósiles. Conforme pasa el tiempo, las tecnologías de generación renovable son generalmente más baratas, mientras que las energías fósiles se vuelven más caras:
En 2011, el coste de la fotovoltaica había caído bastante por debajo del de la energía nuclear, y se espera que siga cayendo:

La tendencia es que los precios disminuyan aún más con el tiempo una vez que los componentes fotovoltaicos han entrado en una clara y directa fase industrial. A finales de 2012, el precio medio de los módulos fotovoltaicos había caído a 0,50$/Wp, y las previsiones apuntan que su precio seguirá reduciéndose hasta los 0,36$/Wp en 2017.

En 2015, el Instituto alemán Fraunhofer especializado en energía solar (ISE) realizó un estudio que concluía que la mayoría de los escenarios previstos para el desarrollo de la energía solar infravaloran la importancia de la fotovoltaica. El estudio realizado por el instituto Fraunhofer estimaba que el coste levelizado (LCOE) de la energía solar fotovoltaica para plantas de conexión a red se situará a largo plazo entre 0,02 y 0,04 €/kWh, niveles inferiores a los de las fuentes de energía convencionales.

Otra alternativa de bajo coste a las células de silicio cristalino es la energía fotovoltaica de capa o película fina que está basada en las células solares de tercera generación. Consisten en una célula solar que se fabrica mediante el depósito de una o más capas delgadas (película delgada) de material fotovoltaico en un sustrato.

Las células solares de película delgada suelen clasificarse según el material fotovoltaico utilizado:

La Conferencia Internacional Energía Solar de Bajo Costo de Sevilla, realizada en febrero de 2009, fue el primer escaparate en España de las mismas. Esta tecnología causó grandes expectativas en sus inicios. Sin embargo, la fuerte caída en el precio de las células y los módulos de silicio policristalino desde finales de 2011 ha provocado que algunos fabricantes de capa fina se hayan visto obligados a abandonar el mercado, mientras que otros han visto muy reducidos sus beneficios.

La cantidad de energía solar que alcanza a la superficie terrestre es enorme, cerca de 122 petavatios (PW), y equivale a casi 10000 veces más que los 13 TW consumidos por la humanidad en 2005. Esta abundancia sugiere que no pasará mucho tiempo antes de que la energía solar se convierta en la principal fuente de energía de la humanidad. Adicionalmente, la generación eléctrica mediante fotovoltaica presenta la mayor densidad energética (una media global de 170 W/m) de todas las energías renovables.

A diferencia de las tecnologías de generación de energía basadas en combustibles fósiles, la energía solar fotovoltaica no produce ningún tipo de emisiones nocivas durante su funcionamiento, aunque la producción de los paneles fotovoltaicos presenta también un cierto impacto ambiental. Los residuos finales generados durante la fase de producción de los componentes, así como las emisiones de las factorías, pueden gestionarse mediante controles de contaminación ya existentes. Durante los últimos años también se han desarrollado tecnologías de reciclaje para gestionar los diferentes elementos fotovoltaicos al finalizar su vida útil, y se están llevando a cabo programas para incrementar el reciclaje entre los productores fotovoltaicos.

La tasa de retorno energético de esta tecnología, por su parte, es cada vez mayor. Con la tecnología actual, los paneles fotovoltaicos recuperan la energía necesaria para su fabricación en un período comprendido entre 6 meses y 1 año y medio; teniendo en cuenta que su vida útil media es superior a 30 años, producen electricidad limpia durante más del 95% de su ciclo de vida.

Las emisiones de gases de efecto invernadero a lo largo del ciclo de vida para la fotovoltaica son cercanas a los 46g/kWh, pudiendo reducirse incluso hasta 15g/kWh en un futuro próximo.
En comparación, un planta de gas de ciclo combinado emite entre 400-599g/kWh, un planta de gasoil 893g/kWh, una planta de carbón 915-994g/kWh o con tecnología de captura de carbono unos 200 g/kWh (excluyendo las emisiones durante la extracción y el transporte de carbón), y una planta de energía geotérmica de alta temperatura, entre 91-122g/kWh. La intensidad de las emisiones para el ciclo de vida de la energía hidráulica, eólica y la energía nuclear es menor que la de la energía fotovoltaica, según los datos publicados por el IPCC en 2011.

Al igual que todas las fuentes de energía cuyas emisiones dependen principalmente de las fases de construcción y transporte, la transición hacia una economía de bajo carbono podría reducir aún más las emisiones de dióxido de carbono durante la fabricación de los dispositivos solares.

Un sistema fotovoltaico de 1kW de potencia ahorra la combustión de aproximadamente 77kg (170 libras) de carbón, evita la emisión a la atmósfera de unos 136kg (300 libras) de dióxido de carbono, y ahorra mensualmente el uso de unos 400 litros (105 galones) de agua.

Una instalación fotovoltaica puede operar durante 30 años o más con escaso mantenimiento o intervención tras su puesta en marcha, por lo que tras el coste de inversión inicial necesario para construir una instalación fotovoltaica, sus costes de operación son muy bajos en comparación con el resto de fuentes energéticas existentes. Al finalizar su vida útil, la mayor parte de los paneles fotovoltaicos puede ser tratada. Gracias a las innovaciones tecnológicas que se han desarrollado en los últimos años, se puede recuperar hasta el 95 % de ciertos materiales semiconductores y el vidrio, así como grandes cantidades de metales ferrosos y no ferrosos utilizados en los módulos. Algunas empresas privadas y organizaciones sin fines de lucro, como por ejemplo PV CYCLE en la Unión Europea, están trabajando en las operaciones de recogida y reciclaje de paneles al final de su vida útil.

Dos de las soluciones de reciclaje más comunes son:

Desde 2010 se celebra una conferencia anual en Europa que reúne a productores, recicladores e investigadores para debatir el futuro del reciclaje de módulos fotovoltaicos. En 2012 tuvo lugar en Madrid.




</doc>
<doc id="9842" url="https://es.wikipedia.org/wiki?curid=9842" title="Energía solar térmica">
Energía solar térmica

La energía solar térmica o energía termosolar consiste en el aprovechamiento de la energía del Sol para producir calor que puede aprovecharse para cocinar alimentos o para la producción de agua caliente destinada al consumo de agua doméstico, ya sea agua caliente sanitaria, calefacción, o para producción de energía mecánica y, a partir de ella, de energía eléctrica. Adicionalmente puede emplearse para alimentar una máquina de refrigeración por absorción, que emplea calor en lugar de electricidad para producir frío con el que se puede acondicionar el aire de los locales. 

Los colectores de energía solar térmica están clasificados como colectores de baja, media y alta temperatura. Los colectores de baja temperatura generalmente son placas planas usadas para calentar agua. Los colectores de temperatura media también usualmente son placas planas usadas para calentar agua o aire para usos residenciales o comerciales. Los colectores de alta temperatura concentran la luz solar usando espejos o lentes y generalmente son usados para la producción de energía eléctrica. La energía solar térmica es diferente y mucho más eficiente que la energía solar fotovoltaica, la que convierte la energía solar directamente en electricidad. Mientras que las instalaciones generadoras proporcionan solo 600 megavatios de energía solar térmica a nivel mundial a octubre de 2009, otras centrales están bajo construcción por otros 400 megavatios y se están desarrollando otros proyectos de energía termosolar de concentración por un total de 14 gigavatios.

En cuanto a la generación de agua caliente para usos sanitarios (también llamada «agua de manos»), hay dos tipos de instalaciones de los comúnmente llamados calentadores: las de circuito abierto y las de circuito cerrado. En las primeras, el agua de consumo pasa directamente por los colectores solares. Este sistema reduce costos y es más eficiente (energéticamente hablando), pero presenta problemas en zonas con temperaturas por debajo del punto de congelación del agua, así como en zonas con alta concentración de sales que acaban obstruyendo los conductos de los paneles. En las instalaciones de circuito cerrado se distinguen dos sistemas: flujo por termosifón y flujo forzado.
Los paneles solares térmicos tienen un muy bajo impacto ambiental.

La energía solar térmica puede utilizarse para dar apoyo al sistema convencional de calefacción (caldera de gas o eléctrica), apoyo que consiste entre el 10 % y el 20 % de la demanda energética de la calefacción. Para ello, la instalación o caldera ha de contar con intercambiador de placas (funciona de forma similar al baño María, ya que el circuito de la caldera es cerrado) y un regulador (que dé prioridad en el uso del agua caliente para ser empleada en agua de manos).

Una instalación Solar Térmica está formada por captadores solares, un circuito primario y secundario, intercambiador de calor, acumulador, vaso de expansión y tuberías. Si el sistema funciona por termosifón será la diferencia de densidad por cambio de temperatura la que moverá el líquido. Si el sistema es forzado entonces necesitaremos además: bombas y un panel de control principal.

Los captadores solares son los elementos que capturan la radiación solar y la convierten en energía térmica, en calor. Como captadores solares se conocen los de placa plana, los de tubos de vacío y los captadores absorbedores sin protección ni aislamiento. Los sistemas de captación planes (o de placa plana) con cubierta de vidrio son los comunes mayoritariamente en la producción de agua caliente sanitaria ACS. El vidrio deja pasar los rayos del Sol, estos calientan unos tubos metálicos que transmiten el calor al líquido de dentro. Los tubos son de color oscuro, ya que las superficies oscuras calientan más.

El vidrio que cubre el captador no solo protege la instalación sino que también permite conservar el calor produciendo un efecto invernadero que mejora el rendimiento del captador.

Están formados de una carcasa de aluminio cerrada y resistente a ambientes marinos, un marco de aluminio eloxat, una junta perimetral libre de siliconas, aislante térmico respetuoso con el medio ambiente de lana de roca, cubierta de vidrio solar de alta transparencia , y finalmente por tubos soldados ultrasónicos.

Los colectores solares se componen de los siguientes elementos:






El alma del sistema es una verja vertical de tubos metálicos, para simplificar, que conducen el agua fría en paralelo, conectados por abajo por un tubo horizontal en la toma de agua fría y por arriba por otro similar al retorno.

La parrilla viene encajada en una cubierta, como la descrita más arriba, normalmente con doble vidrio para arriba y aislante por detrás.

En algunos modelos, los tubos verticales están soldados a una placa metálica para aprovechar la insolación entre tubo y tubo.

En este sistema los tubos metálicos del sistema precedente se sustituyen por tubos de vidrio, introducidos, de uno en uno, en otro tubo de vidrio entre los que se hace el vacío como aislamiento. Las grandes ventajas que presentan estos tipos de captadores son su alto rendimiento (196 % más eficientes que las placas planas) y que, en caso de que uno de los tubos se estropeara, no hay que cambiar todo el panel por uno nuevo, sino que solo hay que cambiar el tubo afectado. Además son más baratos en su fabricación, ya que los nuevos tubos son 100 % cristal borosilicato y no utilizan tubo de cobre, lo que reduce los costes anteriormente mencionados.

Este sistema aprovecha el cambio de fase de vapor a líquido dentro de cada tubo, para entregar energía a un segundo circuito de líquido de transporte.

Los elementos son tubos cerrados, normalmente de cobre, que contienen el líquido que, al calentarse por el sol, hierve y se convierte en vapor que sube a la parte superior donde hay un cabezal más ancho (zona de condensación), que en la parte exterior está en contacto con el líquido transportador, el cual siendo más frío que el vapor del tubo, capta el calor y provoca que el vapor se condense y caiga en la parte baja del tubo para volver a empezar el ciclo.

El líquido del tubo puede ser agua, a la que se le ha reducido la presión hasta un vacío parcial, tendrá un punto de ebullición bajo, lo que permite trabajar incluso con la insolación de los rayos infrarrojos en caso de presencia de nubes.

El "tubo de calor" (o tubo de cobre) se puede envolver con una chaqueta de materiales especiales para minimizar las pérdidas por irradiación.

El "tubo de calor" se cierra dentro de otro tubo de vidrio entre los que se hace el vacío como aislamiento. Se suelen emplear tubos de vidrio resistente, para reducir los daños en caso de pequeñas granizadas.

Son hasta un 163 % más eficientes que las placas planas con serpentín e igualmente más baratos en su fabricación con respecto a las placas planas, pues el precio del cristal es más bajo que el cobre del serpentín que contiene la placa plana.

El circuito primario, es circuito cerrado, transporta el calor desde el captador hasta el acumulador (sistema que almacena calor). El líquido calentado (agua o una mezcla de sustancias que puedan transportar el calor) lleva el calor hasta el acumulador. Una vez enfriado, vuelve al colector para volver a calentar, y así sucesivamente.

El intercambiador de calor calienta el agua de consumo a través del calor captado de la radiación solar. Se sitúa en el circuito primario, en su extremo. Tiene forma de serpentín, ya que así se consigue aumentar la superficie de contacto y por lo tanto, la eficiencia.

El agua que entra en el acumulador, siempre que esté más fría que el serpentín, se calentará. Esta agua, calentada en horas de sol, nos quedará disponible para el consumo posterior.

El acumulador es un depósito donde se acumula el agua calentada útil para el consumo. Tiene una entrada para el agua fría y una salida para la caliente. La fría entra por debajo del acumulador donde se encuentra con el intercambiador, a medida que se calienta se desplaza hacia arriba, que es desde donde saldrá el agua caliente para el consumo.

Internamente dispone de un sistema para evitar el efecto corrosivo del agua caliente almacenada sobre los materiales. Por fuera tiene una capa de material aislante que evita pérdidas de calor y está cubierto por un material que protege el aislamiento de posibles humedades y golpes.

El circuito secundario o de consumo, (circuito abierto), entra agua fría de suministro y por el otro extremo del agua calentada se consume (ducha, lavabo, ...). El agua fría pasa por el acumulador primeramente, donde calienta el agua hasta llegar a una cierta temperatura. Las tuberías de agua caliente del exterior, deben estar cubiertas por aislantes.

Si nuestro consumo incluye calefacción, el sistema emisor de calor (radiadores (60 °C), fan-coil(45 °C), suelo radiante(30 °C), zócalo radiante, muro radiante, …) que es más conveniente utilizar es el de baja temperatura (<=50 °C), de esta manera el sistema solar de calefacción tiene mayor rendimiento.
No obstante, se pueden instalar sistemas que no son de baja temperatura, para así emplear radiadores convencionales.

Las bombas, en caso de que la instalación sea de circulación forzada, son de tipo recirculación (suele haber dos por circuito), trabajando una la mitad del día, y la pareja, la mitad del tiempo restante. La instalación consta de los relojes que llevan el funcionamiento del sistema, hacen el intercambio de las bombas, para que una trabaje las 12 horas primeras y la otra las 12 horas restantes. Si hay dos bombas en funcionamiento, hay la ventaja que en caso de que una deje de funcionar, está la sustituta, de modo que así no se puede parar el proceso ante el fallo de una de estas. El otro motivo a considerar, es que gracias a este intercambio la bomba no sufre tanto, sino que se la deja descansar, enfriar, y cuando vuelve a estar en buen estado (después de las 12 horas) se vuelve a poner en marcha. Esto ocasiona que las bombas puedan alargar durante más el tiempo de funcionamiento sin tener que hacer ningún tipo de mantenimiento previo.

En total y tal como se define anteriormente, suele haber 4 bombas, dos en cada circuito. Dos en el circuito primario que bombean el agua de los colectores y las otras dos en el circuito secundario que bombean el agua de los acumuladores, en el caso de una instalación de tipo circulación forzada.

El vaso de expansión absorbe variaciones de volumen del fluido caloportador, el cual circula por los conductos del captador, manteniendo la presión adecuada y evitando pérdidas de la masa del fluido.
Es un recipiente con una cámara de gas separada de la de líquidos y con una presión inicial en función de la altura de la instalación.

Lo que más se utiliza es con vaso de expansión cerrado con membrana, sin transferencia de masa en el exterior del circuito.

Las tuberías de la instalación se encuentran recubiertas de un aislante térmico para evitar pérdidas de calor con el entorno. Antiguamente se utilizaban tuberías de cobre. Luego se utilizó tubos PEX-AL-PEX, consistentes en tres capas plástico-aluminio-plástico, mucho más baratos y con mayor vida útil que la tubería de cobre tradicional. Al pasar los años de uso del equipo y por la acumulación de radiación solar, se encontró que el PEX se cristalizaba destruyéndose por presión. Actualmente, se utiliza para circuito cerrado cañerías de acero inoxidable BPDN aislada con espuma elastomérica y rodeada de una mica de EPDM que da aislamiento térmico y proporciona durabilidad al proteger contra la radiación, y fallas por ruptura de uniones y soldaduras.

Se dispone también de un panel principal de control en la instalación, donde se muestran las temperaturas en cada instante (un regulador térmico), de manera que pueda controlarse el funcionamiento del sistema en cualquier momento. Aparecen también los relojes encargados del intercambio de bombas.

Durante el verano, se pueden cubrir las placas, a fin de evitar que se estropeen por las altas temperaturas o bien se pueden utilizar para producir frío solar (aire acondicionado frío).

Especialmente populares son los equipos domésticos compactos, compuestos típicamente por un depósito de unos 150 litros de capacidad y un colector de unos 2 m². Estos equipos, disponibles tanto con circuito abierto como cerrado, pueden suministrar el 90 % de las necesidades de agua caliente anual para una familia de 4 personas, dependiendo de la radiación y el uso. Estos sistemas evitan la emisión de hasta 4,5 toneladas de gases nocivos para la atmósfera. El tiempo aproximado de retorno energético (tiempo necesario para ahorrar la energía empleada en fabricar el aparato) es de un año y medio aproximadamente. La vida útil de algunos equipos puede superar los 25 años con un mantenimiento mínimo, dependiendo de factores como la calidad del agua.

Estos equipos pueden distinguirse entre:

Equipos de Circulación forzada: Compuesto básicamente de captadores, un acumulador solar, un grupo hidráulico, una regulación y un vaso de expansión.

Equipos por Termosifón: Cuya mayor característica es que el acumulador se sitúa en la cubierta, encima del captador.

Equipos con Sistema Drain-Back: Un sistema compacto y seguro, muy apropiado para viviendas unifamiliares.

Es habitual encontrarse con instalaciones en las que el acumulador contiene una resistencia eléctrica de apoyo, que actúa en caso de que el sistema no sea capaz de alcanzar la temperatura de uso (normalmente 40 °C); en España esta opción ha quedado prohibida tras la aprobación del CTE (Código Técnico de la Edificación) ya que el calor de la resistencia puede, si el panel esta más frío que el acumulador integrado, calentar el panel y perder calor, y por lo tanto energía, a través de él. En algunos países se comercializan equipos que utilizan el gas como apoyo.

Las características constructivas de los colectores responden a la minimización de las pérdidas de energía una vez calentado el fluido que transcurre por los tubos, por lo que se encuentran aislamientos a la conducción (vacío u otros) y a la rerradiación de baja temperatura.

Además de su uso como agua caliente sanitaria, calefacción y refrigeración (mediante máquina de absorción), el uso de placas solares térmicas (generalmente de materiales baratos como el polipropileno) ha proliferado para el calentamiento de piscinas exteriores residenciales, en países donde la legislación impide el uso de energías de otro tipo para este fin.

En muchos países hay subvenciones para el uso doméstico de energía solar, en cuyos casos una instalación doméstica puede amortizarse en unos 5 o 6 años. El 29 de septiembre de 2006 entró en vigor en España el Código Técnico de la Edificación, que establece la obligatoriedad de implantar sistemas de agua caliente sanitaria (ACS) con energía solar en todas las nuevas edificaciones, con el objetivo de cumplir con el protocolo de Kioto, pero que olvida la calefacción, que se recoge en las ordenanzas solares de los ayuntamientos.

El colector solar plano es el aparato más representativo de la tecnología solar fototérmica. Su principal aplicación es en el calentamiento de agua para baño y albercas, aunque también se utiliza para secar productos agropecuarios mediante el calentamiento de aire y para destilar agua en comunidades rurales principalmente.

Está constituido básicamente por:

Para la mayoría de los colectores solares se tienen dimensiones características. En términos generales la unidad básica consiste de un colector plano de 1,8 a 2,1 m² de superficie, conectado a un termotanque de almacenamiento de 150 a 200 litros de capacidad; a este sistema frecuentemente se le añaden algunos dispositivos termostáticos de control a fin de evitar congelamientos y pérdidas de calor durante la noche. Las unidades domésticas funcionan mediante el mecanismo de termosifón, es decir, mediante la circulación que se establece en el sistema debido a la diferencia de temperatura de las capas de líquido estratificadas en el tanque de almacenamiento. Para instalaciones industriales se emplean varios módulos conectados en arreglos serie-paralelo, según el caso, y se emplean bombas para establecer la circulación forzada.

Los sistemas de calefacción solar para procesos están diseñados para proporcionar grandes cantidades de agua caliente o calefacción de espacios para edificios de uso no residencial.

Las piscinas de evaporación son piscinas de baja profundidad que concentran sólidos disueltos a través de la evaporación. El uso de piscinas de evaporación para obtener sal del agua salada es una de las aplicaciones más antiguas de la energía solar. Los usos modernos incluyen la concentración de soluciones de salmueras usadas en la minería por lixiviación y la remoción de sólidos disueltos de los flujos de desechos. En conjunto, las piscinas de evaporación representan una de las aplicaciones comerciales más grandes de la energía solar actualmente en uso.

Los colectores transpirados sin vidrios (en inglés: Unglazed Transpired Collectors, UTC) son muros perforados que enfrentan al sol usados para precalentar el aire de ventilación. Los UTC pueden aumentar la temperatura del aire hasta 22 °C y son capaces de entregar temperaturas de salida entre 45-60 °C. El corto período de amortización de los colectores transpirados (entre 3 a 12 años) los hacen una alternativa más costo-efectiva que los sistemas de recolección vidriados. Al año 2009, se han instalado mundialmente sobre 1500 sistemas con un área de colectores total de 300 000 m². Ejemplos típicos incluyen un colector de 860 m² en Costa Rica usado para secar granos de café y un colector de 1300 m² en Coimbatore, India usado para secar caléndulas.

Una instalación de procesamiento de comida ubicada en Modesto, California usa cilindros parabólicos para producir vapor uado en el proceso de fabricación. Se espera que el área de colectores de 5000 m² proporcione 15 TJ por año.

Las instalaciones de temperatura media pueden usar varios diseños, los diseños más comunes son: glicol a presión, drenaje trasero, sistemas de lote y sistemas más nuevos de baja presión tolerantes al congelamiento que usan tuberías de polímero que contienen agua con bombeo fotovoltaico. Los estándares europeos e internacionales están siendo revisados para incluir las innovaciones en diseño y la operación de colectores de temperatura media. Las innovaciones operacionales incluyen la operación de "colectores permanentemente húmedos". Esta técnica reduce o incluso elimina la ocurrencia de tensiones de no flujo de alta temperatura conocidas como estancamiento, las que reducen la vida esperada de estos colectores.

La energía térmica solar puede ser útil para el secado de madera para la construcción y de madera para combustible tales como chips de madera para la combustión. También es usada para secar alimentos tales como frutas, granos y pescados. El secado de cultivos por medio de la energía solar térmica es ambientalmente amigable así como económica mientras que mejora la calidad del resultado. Las tecnologías en secado solar son variadas. Los más simples utilizan una malla tendida al sol, mientras que los de tipo industrial utilizan colectores de aire vidriados que conducen el aire caliente a una cámara de secado. 
La energía térmica solar también es útil en el proceso de secado de productos tales como chips de madera y otras formas de biomasa elevando la temperatura mientras que permiten que el aire pase a través de ella y saquen la humedad.

Las cocinas solares usan la luz del sol para cocinar, secar y pasteurización. La cocina solar reduce el consumo de combustible, ya sea combustibles fósiles o leña, y mejora la calidad del aire reduciendo o removiendo la fuente de humo.

La forma más simple de cocina solar es la caja de cocción que fue construida por primera vez por Horace-Bénédict de Saussure en el año 1767. Una caja de cocción básica consiste de un contenedor aislado con una tapa transparente. Estas cocinas pueden ser usadas efectivamente con cielos parcialmente cubiertos y normalmente alcanzan temperaturas de entre 50-100 °C.

Las cocinas solares de concentración usan reflectores para concentrar la energía solar en un contenedor de cocción. Las geometrías de reflector más comunes son las placas planas, de disco y cilíndrico-parabólicas. Estos diseños cocinan más rápido y a temperaturas más altas (hasta los 350 °C) pero requieren de luz solar directa para funcionar en forma adecuada.

La Cocina Solar en Auroville, India usa una tecnología de concentración única conocida como el tazón solar. Al contrario de los sistemas de convencionales de receptores fijos o de reflectores de seguimiento, el tazón solar usa un reflector esférico fijo con un receptor que sigue el foco de luz a medida de que el sol cruza el cielo. El receptor del tazón solar alcanza temperaturas de 150 °C que es usado para producir vapor que ayuda a la cocción de 2000 raciones diarias.

Muchas otras cocinas solares en India usan otra tecnología de concentración única conocida como el reflector Scheffler. Está tecnología fue desarrollada por primera vez por Wolfgang Scheffler en el año 1986. Un reflector Scheffler es un disco parabólico que usa un solo eje de seguimiento para perseguir el curso diario del sol. Estos reflectores tienen una superficie reflectante flexible que es capaz de cambiar su curvatura para ajustarse a las variaciones estacionales en el ángulo de incidencia de la luz solar. Los reflectores Scheffler tienen la ventaja de tener un punto focal fijo lo que mejora la facilidad de cocción y son capaces de alcanzar temperaturas de entre 450 a 650 °C. En el año 1999 en Abu Road, Rajasthan, India se construyó el sistema de reflectores Scheffler más grande del mundo, este es capaz de cocinar hasta 35 000 raciones diarias. A principios del año 2008 han sido fabricadas sobre 2000 grandes cocinas, que usan el diseño Scheffler, a nivel mundial.

Los destiladores solares pueden ser usado para procesar agua potable en áreas donde el agua limpia no es común. La energía solar calienta el agua en el contenedor, luego el agua se evapora y se condensa en el fondo de la cubierta de vidrio.

Las temperaturas inferiores a 95 grados celsius son suficientes para calefacción de espacios, en ese caso generalmente se usan colectores planos del tipo no concentradores. Debido a las relativamente altas pérdidas de calor a través del cristal, los colectores planos no logran alcanzar mucho más de 200 °C incluso cuando el fluido de transferencia está estancado. Tales temperaturas son demasiado bajas para ser usadas en la conversión eficiente en electricidad.

La eficiencia de los motores térmicos se incrementa con la temperatura de la fuente de calor. Para lograr esto en las plantas de energía termal, la radiación solar es concentrada por medio de espejos o lentes para lograr altas temperaturas mediante una tecnología llamada energía termosolar de concentración (en inglés: "Concentrated Solar Power", CSP). El efecto práctico de las mayores eficiencias es la reducción del tamaño de los colectores de la planta y del uso de terreno por unidad de energía generada, reduciendo el impacto ambiental de una central de potencia así como su costo.

A medida de que la temperatura aumenta, diferentes formas de conversión se vuelven prácticas. Hasta 600 °C, las turbinas de vapor, la tecnología estándar, tienen una eficiencia de hasta 41 %, Por sobre los 600 °C, las turbinas de gas pueden ser más eficientes. Las temperaturas más altas son problemáticas y se necesitan diferentes materiales y técnicas. Uno propuesta para temperaturas muy altas es usar sales de fluoruro líquidas operando a temperaturas de entre 700 °C a 800 °C, que utilizan sistemas de turbinas de etapas múltiples para lograr eficiencias termales de 50 % o más. Las temperaturas más altas de operación le permiten a la planta usar intercambiadores de calor secos de alta temperatura para su escape termal, reduciendo el uso de agua de la planta, siendo esto crítico para que las centrales ubicadas en desiertos sean prácticas. También las altas temperaturas hacen que el almacenamiento de calor sea más eficiente, ya que se almacenan más watts-horas por unidad de fluido.

Dado que una planta de energía termosolar de concentración (CSP) primero genera calor, puede almacenar dicho calor antes de convertirlo en electricidad. Con la actual tecnología, el almacenamiento de calor es mucho más barato que el almacenamiento de electricidad. De esta forma, una planta CSP pude producir electricidad durante el día y la noche. Si la ubicación de la planta CSP tiene una radiación solar predecible, entonces la planta se convierte en una central confiable de generación de energía. La confiabilidad puede ser mejorada aún más al instalar un sistema de respaldo que use un sistema de combustión interna. Este sistema de respaldo puede usar la mayor parte de las instalaciones de la planta CSP, lo que hace disminuir el costo del sistema de respaldo.

Superados los temas de confiabilidad, con desiertos desocupados, sin problemas de polución y sin costos asociados al uso de los combustible fósiles, los principales obstáculos para el despliegue a gran escala de las centrales CSP son los costos, la contaminación estética, el uso del suelo y factores similares para las líneas de transmisión eléctrica de alta tensión. Aunque solo se necesita un pequeño porcentaje de los desiertos para abastecer los requerimientos globales de electricidad, aún esto es un gran superficie cubierta con espejos o lentes que se necesitan para obtener una cantidad significativa de energía.

Los sistemas tipo canal parabólico usan reflectores parabólicos en una configuración de canal para enfocar la radiación solar directa sobre un tubo largo que corre a lo largo de su foco y que conduce al fluido de trabajo, el cual pude alcanzar temperaturas hasta de 500 °C.

La generación fototérmica de electricidad es actualmente una de las aplicaciones más extensas de la energía solar en el mundo. Existen más de 2,5 millones de m² de concentradores solares instalados en 9 plantas Solar Energy Generation System (SEGS) de la Compañía Luz de Israel, que representan 354 MW y más del 85 % de la electricidad producida con energía solar. La compañía Luz salió del mercado en 1991 a causa de la reducción que se dio paralelamente en los costos de los energéticos convencionales y en los subsidios a los energéticos renovables en los Estados Unidos. Sus plantas usan aceite sintético como medio de transferencia de calor en el campo de concentradores; como circuito primario, el calor recogido por el aceite se intercambia posteriormente con agua donde se lleva a cabo la generación de vapor, el cual a su vez se expande para completar un ciclo Rankine. Durante los periodos de baja insolación, o bien para nivelar la oferta, se asisten con gas natural.

Actualmente se ha introducido el ciclo combinado para mejorar la eficiencia termodinámica de estos sistemas y se estudia la posibilidad de generar directamente el vapor en el campo de concentradores. Con esto se espera lograr llevar los precios de generación a niveles competitivos con las plantas termoeléctricas convencionales.

Existen otros sistemas, no comerciales aún, como los de torre central que usan helióstatos (espejos altamente reflejantes) para enfocar la luz solar, con la ayuda de una computadora y un servomecanísmo, en un receptor central. Los sistemas parabólicos de plato usan estos reflectores para concentrar la luz del sol en un receptor montado arriba del plato, en su punto focal.

Durante el día y el año, el sol cambia su posición respecto a un punto en la superficie del planeta. Para los sistemas de baja temperatura el seguimiento del sol se puede evitar (o limitar a unas pocas posiciones por año) si se usa óptica no visual. Sin embargo, para temperaturas más altas, si los espejos o lentes no se mueven, el foco de estos cambia, provocando que los ángulos de aceptación sean poco eficientes, aunque se compensa en parte por el uso de ópticas no visuales. Por consiguiente es necesario implementar un sistema para seguir la posición del sol, la desventaja de esto es que incrementa el costo y la complejidad de la planta. Se han ideado diferentes diseñados para solucionar este problema y que se pueden distinguir en cómo ellos concentran la luz solar y siguen la posición del sol.

Las plantas de energía cilíndrico-parabólicos usan un espejo cilíndrico curvado para reflejar la radiación solar directa sobre un tubo de vidrio que contiene un fluido (también llamado receptor, absorbedor o colector) ubicado a lo largo del cilindro, posicionado en el punto focal de los reflectores. El cilindro es parabólico a lo largo de un eje y lineal en el eje ortogonal. El cambio durante el día de la posición del sol perpendicular al receptor, es seguido inclinando el cilindro de este a oeste de tal forma que la radiación directa permanece enfocada en el receptor. Sin embargo, los cambios estacionales en el ángulo de incidencia de la luz solar paralelo al cilindro no requieren ajustar los espejos, dado que simplemente la radiación solar es concentrada en otra parte del receptor, de esta forma el diseño no requiere hacer el seguimiento en un segundo eje.

El receptor puede estar encerrado en una cámara al vacío de vidrio. El vacío reduce significativamente la pérdida de calor por convección.

Un fluido, también llamado fluido de transferencia de calor, pasa a través del receptor y se calienta muy fuertemente. Los fluidos más comunes son aceite sintético, sal fundida y vapor presurizado. El fluido que contiene el calor es transportado a un motor térmico donde aproximadamente un tercio del calor es transformado en electricidad.

Andasol 1 en Guadix, España usa el diseño cilíndrico-parabólico, el cual consiste de largas filas paralelas de colectores solares modulares. Estos siguen al Sol desde el este al oeste rotando sobre su eje, los paneles reflectores de alta precisión concentran la radiación solar sobre una tubería absorbente localizada a lo largo del eje focal de la línea de colectores. Un medio de transferencia de calor, un aceite sintético, como en los motores de los automóviles, es hecho circular a través de las tuberías de absorción a una temperatura de hasta 400 °C y genera vapor bajo presión para propulsar un generador de turbina de vapor en un bloque de energía convencional.

Los sistemas cilíndrico-parabólico a escala total consisten de muchos de tales cilindros dispuestos en paralelo sobre una gran área de terreno. Desde el año 1985 el SEGS (en inglés: Solar Energy Generating Systems, SEGS), un sistema termal solar que usa este diseño, ha estado funcionando a plena capacidad en California, Estados Unidos.

El Sistema Solar de Generación de Energía (en inglés: Solar Energy Generating System, SEGS) es un conjunto de nueve plantas con una capacidad total de 350 MW. Actualmente es el sistema solar operacional más grande (tanto del tipo termal o no). La planta Nevada Solar One tiene una capacidad de 64 MW. Están en construcción las plantas Andasol 1 y 2 en España, cada planta tiene una capacidad de 50 MW, sin embargo, estas plantas son de un diseño que tiene un sistema de almacenamiento de calor que requiere un terreno con colectores solares mayor en relación al tamaño del generador y turbina de vapor para almacenar el calor y enviarlo a las turbinas de vapor al mismo tiempo. El almacenamiento de calor permite una mejor utilización de las turbinas de vapor. Con una operación diurna y parcialmente nocturna la turbina de vapor de Andasol 1 con un capacidad de punta de 50 MW produce más energía que Nevada Solar One con una capacidad de punta de 64 MW, debido al sistema de almacenamiento de calor y un terreno de colectores más grande que posee la planta de Andasol 1.

Se había propuesto instalar 553 MW adicionales en el Mojave Solar Park, California pero este proyecto fue cancelado en el año 2011. También se ha propuesto una planta híbrida con almacenamiento de calor de 59 MW cerca de Barstow, California. Cerca de Kuraymat en Egipto, se generan aproximadamente 40 MW de vapor como aporte para una planta de gas. También se generan 25 MW de vapor como aporte para una planta de gas en Hassi R'mel, Argelia. El gobierno de India ha comenzado a desarrollar una iniciativa llamada Jawaharlal Nehru National Solar Mission (también conocida como la Misión Solar Nacional) para resolver el problema de abastecimiento de energía de India.

Las torres de energía (también conocidas como central solar de 'torre central' o centrales de 'helióstatos') captura y enfocan la energía termal del sol con miles de espejos que siguen al sol (llamados helioestatos) ubicados en un terreno adyacente a la torre. Un torre está ubicada en el centro del terreno ocupado por los helióstatos. Los helióstatos concentran la luz del sol en un receptor que está ubicado en la parte superior de la torre. En el receptor la radiación solar concentrada calienta una sal fundida a sobre 538 °C. Posteriormente la sal fundida se envía a un tanque de almacenamiento termal donde se acumula, con una eficiencia termal del 98 %, finalmente es bombeada hacia un generador de vapor. El vapor impulsa una turbina la que genera electricidad. Este proceso, que también es conocido como "Ciclo de Rankine", es similar al que usa una planta que usa combustibles fósiles (carbón, gas natural, petróleo, etc), excepto que la fuente de energía en este caso es la radiación solar limpia.

La ventaja de este diseño en comparación al diseño cilíndrico-parabólico es que logra alcanzar temperaturas más altas. La energía termal a temperaturas más altas puede ser convertida en electricidad con mayor eficiencia y es más barato el almacenamiento para ser usada posteriormente. Adicionalmente, el terreno adyacente no necesita ser tan plano. En principio una torre de energía podría ser construida en la ladera de una colina. Los espejos pueden ser planos y las tuberías están concentradas en la torre. La desventaja es que cada espejo debe tener su propio control en dos ejes, mientras que en el diseño cilíndrico-parabólico el control de seguimiento de un eje puede ser compartido por un conjunto más grande de espejos.

La NREL realizó una comparación de la relación costo/desempeño entre los diseños de torre de energía y los cilíndricos-parabólicos, está estimó que para el año 2020 se podría producir electricidad por un costo de 5,47 centavos de dólar por kWh para los diseños de torre de energía y de un costo de 6,21 centavos de dólar por kWh para los diseños cilíndricos-parabólicos. El factor de planta para los torres de energía fue estimado en un 72,9 % y para los diseños cilíndricos-parabólicos fue de 56,2 %. Se espera que el desarrollo de componentes para helióstatos de centrales baratos, durables y fabricados en masa harían bajar estos costos.

En junio de 2008, eSolar, una compañía basada en Pasadena, California fundada por el CEO de Idealab Bill Gross con financiamiento provisto por Google, anunció un Acuerdo para Compra de Energía (en inglés: Power Purchase Agreement, PPA) con la empresa de servicios públicos Southern California Edison para producir 245 megavatios de energía. También, en febrero de 2009, eSolar anunció que había licenciado su tecnología a dos socios de desarrollo, la empresa NRG Energy Inc. basada en Princeton, Nueva Jersey y el grupo ACME basado en India. En el acuerdo con NRG, las compañías anunciaron planes construir en forma conjunta plantas solares térmicas concentradoras por 500 megavatios a través de todo Estados Unidos. La meta para el Grupo ACME fue cerca del doble de esta cifra; ACME planeaba comenzar a construir sus primeras plantas generadoras de energía eSolar en el año 2009 y dentro de los siguientes 10 años completar 1 Gigavatio.

El software propietario de seguimiento del sol de eSolar coordina el movimiento de 24 000 espejos de 1 metro cuadrado por cada torre usando sensores ópticos para ajustar y calibrar los espejos en tiempo real. Esto permite un usar un material reflectante de alta densidad que hace posible el desarrollo de plantas generadoras solares termales de concentración (en inglés: Concentrating Solar Thermal Power, CSP) con unidades de 46 megavatios en terrenos de aproximadamente (MW) π millas cuadradas, lo que resulta en una proporción de terreno a energía de 16 000 m² por 1 megavatio.

BrightSource Energy firmó una serie de Acuerdos de Compra de Energía con Pacific Gas and Electric Company en marzo de 2008 por hasta 900 MW de electricidad, el compromiso de energía solar más grande realizado por una empresa de servicios públicos. Actualmente BrightSource está desarrollando varias plantas de generación solar en el sur de California, planaádose que se inicie la construcción de la primera en el año 2009.

En junio de 2008 BrightSource Energy inauguró su Centro de Desarrollo de Energía Solar (en inglés: Solar Energy Development Center, SEDC) de 4-6 MW en el Desierto de Negev, Israel. El sitio, localizado en el Parque Industrial de Rotem, posee 1.600 helióstatos que siguen al sol y reflejan la radiación solar sobre una torre de 60 metros de alto. La energía concentrada luego es usada para calentar una caldera, localizada en la parte superior de la torre, a una temperatura de 550 grados celsius, generando vapor supercalentado.

Existe una torre funcionando en PS10 en España con una capacidad de 11 MW.

Una planta llamada Solar Tres de 15 MW con almacenamiento de calor está bajo construcción en España. En Sudáfrica, está planificada una planta solar de 100 MW equipada con entre 4000 y 5000 helióstatos, cada uno de un área de 140 m². Una planta localizada en Australia llamada Granja solar Cloncurry (que usa grafito purificado como almacenamiento de calor localizado directamente en la torre).

Marruecos está construyendo cinco plantas solares termales alrededor de Uarzazate. Las plantas producirán aproximadamente 2000 MW hacia el año 2012. Sobre diez mil hectáreas de terreno se usarán para todos las plantas.

El proyecto Solar Uno de 10 MW fue puesto fuera de comisión (posteriormente se desarrolló en el proyecto Solar Dos) y también la central solar Thémis de 2 MW.

Un sistema de disco Stirling usa un gran disco reflector parabólico (similar a la forma que tiene un disco de televisión satelital). Este enfoca toda la radiación solar que llega al disco sobre un solo punto en la parte superior del disco, donde un receptor captura el calor y lo transforma en algo que se pueda usar. Normalmente el disco está acoplado a un motor Stirling, lo que se conoce como un Sistema Disco-Stirling, pero algunas veces se utiliza un motor de vapor. Estos motores crean energía cinética rotacional que puede ser convertida en electricidad usando un generador eléctrico.

La ventaja de un sistema de disco es que puede alcanzar temperaturas muchas más altas debido a una concentración mayor de luz (de manera similar que en los diseños de torre). Las temperaturas más altas permiten una mejor conversión a electricidad y los sistemas de disco son muy eficientes en este aspecto. Sin embargo, también hay algunas desventajas. La conversión de calor a electricidad requiere partes que se mueven y eso resulta en mayores requerimientos de mantenimiento. En general, una aproximación centralizada de este proceso de conversión es mejor que uno descentralizado en el diseño de disco. Segundo, el motor, que es pesado, es parte de la estructura que se mueve, lo que requiere una estructura rígida y un sistema de seguimiento resistente. Adicionalmente, se usan espejos parabólicos en vez de espejos planos lo que significa que el seguimiento debe ser realizado en dos ejes.

En el año 2005 Southern California Edison anunció un acuerdo para comprar motores Stirling para energía solar a la empresa Stirling Energy Systems durante un período de veinte años y en cantidades suficientes (20 000 unidades) para generar 500 MW de electricidad. En enero de 2010, "Stirling Energy Systems" y Tessera Solar pusieron en funcionamiento la primera central solar de demostración de 1,5 MW ("Maricopa Solar") usando la tecnología Stirling en Peoria, Arizona. A comienzos del año 2011 la subsidiaria de desarrollo de Stirling Energy, Tessera Solar, vendió de sus proyectos grandes, el proyecto Imperial de 709 MW y el proyecto Calico de 850 MW a las empresas AES Solar y K. Road respectivamente, y en el otoño de 2011 "Stirling Energy Systems" se acogió al Capítulo 7 de bancarrota debido a la competencia de la tecnología fotovoltaica de bajo costo.

Una central solar con reflectores Fresnel lineales usa una serie de espejos largos, estrechos, de baja curvatura (o incluso planos) para enfocar la luz en uno o más receptores lineales localizados sobre los espejos. En la parte superior del receptor un pequeño espejo parabólico puede estar posicionado para apoyar el enfoque sobre el receptor. La idea de estos sistemas es ofrecer bajos costos totales al compartir un receptor entre varios espejos (cuando se le compara con los conceptos cilíndricos y de disco), mientras que usan la simple geometría de enfoque lineal con un eje de seguimiento. Esto es similar al diseño de cilindro (y diferente de los diseños de torre central y de discos con doble eje). El receptor es estacionario y por lo tanto no necesita de acoples de fluidos (como es el caso en los diseños de cilindro y de discos). También los espejos no necesitan sostener al receptor, así que son estructuralmente más simples. Cuando se usan estrategias de puntería adecuadas (espejos apuntados a diferentes receptores a diferentes horas del día), se puede permitir una densidad mayor de espejos en el terreno disponible.

También ha sido desarrollado un concepto con la idea de reflectores Fresnel con "enfoque puntual" llamado Multi-Tower Solar Array (MTSA), en castellano: Arreglo Solar de Torres Múltiples. pero aún no ha sido construido un prototipo. En este concepto los espejos de posiciones alternas apuntan a torres diferentes como sus blancos, logrando de esta forma minimizar el bloqueo entre espejos y permiten una agrupación más densa de estos. En la torre la radiación solar sería recibida por un divisor de haz curvado, construido de cuarzo revestido, este divisor separaría la porción verde y roja del espectro visible y la porción del infrarrojo cercano y las enviaría a un receptor fotovoltaico, ya que estas partes del espectro electromagnético son las más eficientes para ser usadas con la generación fotovoltaica de electricidad. El resto de las longitudes de onda serían enviadas al receptor termal y la turbina, proceso que utiliza la energía de la radiación y no a las longitudes de onda. Este concepto ganó un financiamiento por el Australian Research Council para construir un prototipo de una sola torre en Australia y que pueda generar aproximadamente unos 150 kW(e) y que usará una microturbina combinada y un receptor fotovoltaico.

Se han construido prototipos recientes de este tipo de sistemas en Australia (del tipo Reflector Fresnel lineal compacto) y por Solarmundo en Bélgica.

El proyecto de investigación y desarrollo de Solarmundo, con su central piloto en Lieja, fue cerrado después de probar el concepto de la tecnología Fresnel lineal en forma exitosa. Subsecuentemente, la empresa Solar Power Group GmbH, basada en Múnich, Alemania, fue fundado por algunos de los miembros del equipo Solarmundo. Un prototipo basado en espejos Fresnel con generación directa de vapor fue construido por SPG en conjunto con el Centro Aeroespacial Alemán (DLR).

Basado en el prototipo australiano se ha propuesta una central de 177 MW ubicada cerca de San Luis Obispo en California y que sería construida por la empresa Ausra., pero Ausra vendió este proyecto a First Solar, finalmente First Solar (un fabricante de celdas solares fotovoltaicas de película delgada) no construirá el proyecto Carrizo, esto resultó en la cancelación del contrato de Ausra para proporcionar 177 MW a P.G.& E. Las centrales de capacidad pequeña son un enorme desafío económico para los diseños cilíndrico-parabólico y de disco, pocas compañías construyen estos proyectos tan pequeños. SHP Europe, una antigua subsidiaria de Ausra, tiene planes para construir una central de ciclo combinado de 6,5 MW en Portugal. La compañía alemana SK Energy GmbH tiene planes para construir varias centrales pequeñas de 1 a 3 MW en el sur de Europa (especialmente en España) usando la tecnología de espejso Fresnel y de motor de vapor.

En mayo de 2008, la empresa alemana Solar Power Group GmbH y la empresa española Laer S.L. acordaron la ejecución conjunta de una central solar termal en el centro de España. Esta será la primera central solar termal en España basada en la tecnología de colectores Fresnel de la empresa Solar Power Group. El tamaño planificado de la central será de 10 MW con una unidad de respaldo basada en combustible fósil. El comienzo de la construcción está planificada para el año 2009. El proyecto está localizado en Gotarrendura, un pequeño pueblo pionero en el uso de energías renovables, aproximadamente a 100 km al noroeste de Madrid, España.

Desde marzo de 2009, la central solar de Puerto Errado 1 (PE 1) operada por la empresa alemana Novatec Solar está operando comercialmente en el sur de España. La central solar está basada en la tecnología de colectores lineales Fresnel y tiene una capacidad eléctrica de 1,4 MW. Adicionalmente a un bloque de potencial convencional, la central incluye una caldera solar con una superficie de espejos de alrededor de 18 000 m². El vapor es generado concentrando la irradiación solar directa sobre un receptor lineal que está ubicado a 7,4 metros sobre la superficie del terreno. Un tubo absorbedor está localizado en la línea de foco del campo de espejos, en este el agua es evaporada directamente en vapor saturado a una temperatura de 270 °C y a una presión de 55 bar por la energía solar concentrada. Desde septiembre del año 2011, debido a un nuevo diseño de receptor desarrollado por Novatec Solar, el vapor ahora puede ser generado a una temperatura de 500 °C.

La central solar de Puerto Errado 2 (PE 2) de 30 MW es una versión agrandada de la PE 1, esta también está basada en la tecnología de colectores Fresnel desarrollada por la empresa alemana Novatec Solar. Comprende una superficie de espejos de 302 000 m² y está en operación desde agosto de 2012. La central está localizada en la región de Murcia.

Otras tecnologías de seguimiento de un solo eje incluyen a las relativamente nueva de reflector lineal Fresnel (en inglés: Linear Fresnel Reflector, LFR) y de LFR-Compacto (en inglés: Compact-LFR, CLFR). La LFR difiere de la de cilindro parabólico en que el absorbedor se encuentra fijo en el espacio sobre el campo de espejos. También, el reflector está compuesto de muchos segmentos de fila bajos, que se enfican colectivamente sobre una larga torre receptora elevada que corre paralela al eje de rotación de los reflectores.

Este sistema ofrece una solución de bajo costo ya que la fila del absorbedor es compartida con varias filas de espejos. Sin embargo, una dificultad fundamental con la tecnología LFR es evitar el obscurecimiento de la radiación solar incidente y el bloqueo de la radiación solar reflejada por los reflectores adyacentes. El bloqueo y el obscurecimiento puede ser reducidos al usar torres más altas o incrementando el tamaño del absorbedor, lo que permite incrementar el espaciamiento entre los reflectores más alejados del absorbedor. Ambas soluciones tienen costos extras asociados, ya que se requiere una mayor superficie de terreno.

El CLFR ofrece una solución alternativa al problema del LFR. El LFR clásico tiene solo un absorbedor lineal instalado en una sola torre lineal. Esto impide cualquier opción en la dirección de la orientación de un reflector específico. Dado que esta tecnología sería introducida en un gran campo, uno puede asumir de que existirán mucho absorbedores lineales en el sistema. Por lo tanto, si los absorbedores están lo suficientemente cercanos, los reflectores individuales tendrán la opción de dirigir la radiación solar reflejada hacia al menos dos absorbedores. Este factor adicional permite el potencial para arreglos con una alta densidad, dado que los patrones de inclinaciones de reflectores alternadas pueden ser hechos de tal forma que los reflectores instalados con una alta densidad no se bloquean o ensombrecen mutuamente.

Las centrales solares CLFR ofrecen reducción de costos en todos los elementos del arreglo solar. Esta reducción de costos alentan el avance de esta tecnología. Las características que inciden en la reducción de costos de este sistema comparadas a las de la tecnología cilíndrica-parabólica incluyen costos estructurales minimizados, pérdidas por bombeo parásito minimizadas y mantenimiento reducido. La disminución de los costos estructurales se atribuyen a uso de reflectores de vidrio planos o curvados elásticamente en vez de costosos reflectores de vidrio hundido montados cerca del suelo. También, el ciclo de transferencia de calor está separado del campo de reflectores, evitando el costo de las tuberías flexibles de alta presión que se requieren para los sistemas cilíndricos. La disminución de las pérdidas de bombeo parásito se deben al uso de agua para el fluido de transferencia de calor con ebullición directa pasiva. El uso de tubos de vidrio evacuados asegura bajas pérdidas por radiación y son baratos. Estudios existentes para las centrales CLFR han mostrado una eficiencia entre el haz de radiación recibido y la electricidad generada de un 19 % en una base anual como un precalentamiento.

Se han construido prototipos de concentradores de lentes de Fresnel para la recuperación de energía termal por la empresa International Automated Systems. No se conocen de sistemas termales que usen lentes de Fresnel en operación a plena escala, aunque ya se encuentran disponibles algunos productos que incorporan lentes de Fresnel en conjunto con células fotovoltaicas.

La ventaja de este diseño es que los lentes son más baratos que los espejos. Adicionalmente, si se escoge un material flexible, entonces se requiere de una estructura de soporte de menor rigidez para resistir la carga generada por el viento. En el proyecto "Desert Blooms" se puede ver un nuevo concepto de tecnología para concentradores solares livianos y 'no disruptivos' que usa lentes de Fresnel asimétricos que ocupan un área de superficie de terreno mínima y que permite mayores cantidades de energía solar concentrada por cada concentrador, aunque todavía no se construye un prototipo.

El sistema solar termal cilíndrico parabólico cerrado encapsula los componentes al interior de un recinto de vidrio tipo invernadero. El recinto protege los componentes de los elementos que pueden impactar negativamente la confiabilidad y eficiencia del sistema. Espejos reflectores solares curvados livianos se encuentran suspendidos desde el techo del recinto de vidrio sostenidos por cables. Un sistema de seguimiento de un solo eje posiciona los espejos para recuperar la cantidad óptima de radiación solar. Los espejos concentran la radiación solar y la enfocan en una red de tuberías de acero estacionarias, también suspendidas de la estructura del recinto de vidrio. Se bombea agua a través de las tuberías y esta es hervida para generar vapor usando la radiación solar concentrada. A continuación el vapor es usado como calor de proceso. Al proteger los espejos del viento permite lograr temperaturas más altas y previene que se acumule polvo sobre estos como un resultado de ser expuestos a la humedad ambiente.

Los hornos solares son reflectores parabólicos o lentes construidas con precisión para enfocar la radiación solar en superficies pequeñas y de este modo poder calentar "blancos" a altos niveles de temperatura. La temperatura que puede obtenerse con un horno solar está determinada por el segundo principio de la termodinámica y es equivalente a la temperatura de la superficie del sol, esto es 6000 °C, y por la consideración de las propiedades ópticas de un sistema de horno que limitan la temperatura máxima disponible. Se han usado hornos solares para estudios experimentales que han alcanzado hasta 3500 °C y se han publicado temperaturas superiores a 4000 °C. Las muestras pueden calentarse en atmósferas controladas y en ausencia de campos eléctricos o de otro tipo si así se desea.

El reflector parabólico tiene la propiedad de concentrar en un punto focal los rayos que entran en el reflector paralelamente al eje. Como el sol abarca un ángulo de 32', aproximadamente, los haces de rayos no son paralelos y la imagen en el foco del receptor tiene una magnitud finita. Como regla empírica, el diámetro de la imagen es aproximadamente la razón de longitud focal dividido por 111. La longitud focal determina el tamaño de la imagen y la abertura del reflector la cantidad de energía que pasa por el área focal para una velocidad dada en incidencia de radiación directa. El cociente entre la abertura y la longitud focal es, pues, una medida de flujo de energía disponible en el área focal y con arreglo a este flujo se puede calcular una temperatura de cuerpo negro.

La utilidad de los hornos solares aumenta con el uso de helióstatos, o espejo plano móvil, para llevar la radiación solar al reflector parabólico. esto permite el montaje estacionario de una parábola de ordinario en posición vertical, con lo cual se pueden colocar aparatos para atmósfera controlada y movimiento de muestras, soportes de blancos, y otros, sin necesidad de mover todo el equipo. El poder de reflexión del helióstato varia de 85 a 95 % según su construcción, por lo que resulta una pérdida de flujo del 5 al 15 % para el horno, y la disminución correspondiente a las temperaturas que se puedan alcanzar.

Se construyen hornos solares de hasta 3 metros de diámetro con espejos de una sola pieza de aluminio, cobre o de otros elementos y se han construido hornos más grandes de múltiples reflectores curvos.

El reflector o blanco usado en los hornos solares puede ser de varias formas. Las sustancias pueden fundirse en sí mismas en cavidades de cuerpo negro, encerrarse en envoltura de vidrio o de otra materia transparente para atmósferas controladas, o introducirse en un recipiente rotatorio "centrífugo". La medición de las temperaturas del blanco en los hornos solares se hace por fusión de sustancias de punto de fusión conocidos y por medios pirométricos ópticos o de radiación.

Se usan hornos solares en gran variedad de estudios experimentales, entre ellos, la fusión de materiales refractarios, la realización de reacciones químicas e investigación de las relaciones de fase en sistemas de alto punto de fusión como sílice alúmina.

La estabilización del óxido de circonio refractario por adición de pequeñas cantidades de CaO en recipientes centrífugos es uno de los muchos trabajos publicados por Trombe, quien también ha eliminado flúor de mezcla de fosfatos por calentamiento en un horno en presencia de sílice y vapor de agua, según la reacción:

[Ca3(PO4)2]3.CaF2 + xSiO2 + H2O ® 3 Ca2(PO4)2 + (SiO2)x.CaO + 2HF

Se ha preparado, con buen rendimiento, óxido de circonio calentando silicato de circonio a 1400 °C con carbonato de sodio, Según la ecuación:

ZrSiO4 + 2Na2CO3 ® Na4SiO4 + 2CO2 + ZrO2

Entre otros usos propuestos para los hornos solares figuran los experimentos de pirólisis instantánea en investigación química inorgánica y orgánica, y estudios geoquímicos de rocas y minerales.

Existe más energía en las frecuencias más altas de la luz basados en la fórmula formula_1, donde "h" es la constante de Planck y formula_2 es la frecuencia. Los colectores metálicos disminuyen las frecuencias más altas de la luz produciendo una serie de cambios Compton en abundancia de frecuencias más bajas de la luz. Los revestimientos de vidrio y cerámica con alta transmisividad en el espectro visible y ultravioleta y con una trampa metálica con absorción efectiva en el espectro infrarrojo (bloqueo de calor) absorben la luz de baja frecuencia producida por la pérdida a través de radiación. La aislación de la convección previene las pérdidas mecánicas transferidas a través del gas. Una vez que recuperado como calor, la eficiencia del almacenamiento térmico aumenta con el tamaño. A diferencia de las tecnologías fotovoltaicas que a menudo se degradan con la luz concentrada, la tecnología solar termal depende de la concentración de la luz, la cual requiere de un cielo despejado para alcanzar las temperaturas necesarias para producir electricidad.

El calor en un sistema solar termal es controlado por cinco principios básicos: ganancia de calor, transferencia de calor, almacenamiento de calor, transporte de calor y aislación termal. En esta situación, el calor es la medida de la cantidad de energía termal que contiene un objeto y está determinada por la temperatura, masa y calor específico del objeto. Las centrales solares termales usan intercambiadores de calor que están diseñados para condiciones de trabajo constantes para proporcionar el intercambio de calor.

La ganancia de calor es el calor acumulado por el sol en el sistema. El calor solar termal es atrapado usando el efecto invernadero, este efecto en este caso es la habilidad de una superficie reflectante para transmitir la radiación de onda corta y reflejar la radiación de onda larga. El calor y la radiación infrarroja son producidas cuando la radiación de onda corta golpea la placa de absorción, que luego es atrapado al interior del colector. Un fluido, usualmente agua, en el absorbedor pasa por tubos y recoge el calor atrapado y lo transfiere a un depósito de almacenamiento de calor.

El calor es transferido ya sea por conducción o convección. Cuando el agua es calentada, la energía cinética es transferida por conducción a las moléculas de agua a través del medio. Estas moléculas dispersan si energía termal por conducción y ocupan más espacio que las moléculas frías que se mueven más lento sobre ellas. La distribución de la energía desde el agua caliente que se eleva hacia el agua fría que se hunde contribuyen al proceso de convección. El calor es transferido en el fluido desde las placas de absorción del colector por conducción. El fluido del colector es hecho circular a través de las tuberías transportadoras hasta el lugar del almacenamiento del calor. Al interior del almacenamiento, el calor es transferido a través del medio por convección.

El almacenamiento del calor permite que las centrales solares termales puedan producir electricidad durante las horas del día sin luz solar. El calor es transferido a un medio de almacenamiento de calor en un depósito aislado durante las horas con luz solar y es recuperado para la generación de electricidad durante las horas cuando no hay luz solar. La tasa de transferencia de calor está relacionada a la conductividad y convección del medio así como a las diferencias de temperatura. Los cuerpos con grandes diferencias de temperatura transfieren el calor más rápido que los cuerpos con diferencias de temperatura más baja.

El transporte del calor se refiere a la actividad en que el calor de un colector solar es transportado hacia el depósito de almacenamiento de calor. La aislación térmica es vital tanto en las tuberías de transporte de calor como en el depósito de almacenamiento de calor. Previene la pérdida de calor, que está relacionada a la pérdida de energía que a su vez afecta negativamente la eficiencia del sistema.

El almacenamiento de calor le permite a las centrales solares termales producir electricidad durante la noche y los días nublados. Esto permite el uso de la energía solar en la generación de carga base así como para la generación de potencia de punta, con el potencial de reemplazar a las centrales que usan combustibles fósiles. Adicionalmente, la utilización de los generadores es más alta lo que reduce los costos.

El calor es transferido a un medio de almacenamiento termal en un depósito aislado durante el día y es retirado para la generación de electricidad en la noche. Los medios de almacenamiento termal incluyen vapor presurizado, concreto, una variedad de materiales con cambio de fase, y sales fundidas tales como calcio, sodio y nitrato de potasio.

La central solar PS10 almacena el calor en tanques como vapor presurizado a 50 bar y a 285 °C. El vapor se condensa y se convierte instantáneamente nuevamente en vapor cuando la presión se baja. El almacenamiento se puede hacer hasta por una hora. Se ha sugerido que se puede almacenar por más tiempo pero aún no se ha probado en una central ya existente.

Se han probado una variedad de fluidos para transportar el calor del sol, incluyendo agua, aire, aceite y sodio, pero en algunos casos se han seleccionado sal fundida como la mejor opción. La sal fundida es usada en los sistemas de torres de energía solar ya que es líquida a presión atmosférica, proporcionando un medio de bajo costo para almacenar energía termal, sus temperaturas de operación son compatibles con la de las actuales turbinas de vapor, y es no inflamable y no tóxica. La sal fundida es usada en las industrias químicas y de metales para transportar calor, así que existe gran experiencia en su uso.

La primera mezcla comercial de sal fundida era una forma común de nitro, 60 por ciento de nitrato de sodio y 40 por ciento de nitrato de potasio. El nitro se funde a 220 °C y se mantiene líquido a 290 °C en un tanque de almacenamiento con aislante. El nitrato de calcio puede reducir el punto de fusión a 131 °C, permitiendo que se pueda extraer más energía antes de que la sal se congele. Ahora existen varios grados técnicos de nitrato de calcio que son estables a más de 500 °C.

Estos sistemas de energía solar pueden generar electricidad en climas nubosos o durante la noche usando el calor almacenado en los tanques de sal caliente. Los tanques se encuentran equipados con aislamiento y son capaces de almacenar el calor durante una semana. Los tanques que alimentan una turbina de 100 MW durante cuatro horas deberían tener un tamaño de 9 m de alto por 24 m de diámetro.

La central solar de Andasol ubicada en España es la primera central solar termal comercial en usar sal fundida para almacenar calor y generar electricidad durante la noche. Esta central entró en funcionamiento el marzo del año 2009. El 4 de julio de 2011, se realizó un hito en la historia de la industria solar la central solar de Gemasolar de 19,9 MW fue la primera en generar electricidad en forma ininterrumpida durante 24 horas seguidas usando un almacenamiento de calor de sal fundida.

Directo 
La propuesta central solar ubicada en Cloncurry, Australia almacenará calor en grafito purificado. La central usa un diseño de torre de energía. El grafito se encuentra localizado en la parte superior de la torre. El calor capturado por los helióstatos va directamente hacia el almacenaje. El calor usado para la generación de energía es recuperado desde el grafito. Esto simplifica el diseño.

Indirecto 
Refrigerantes de sal fundida son usado para llevar el calor desde los reflectores hacia el depósito de almacenamiento de calor. El calor llevado por las sales es transferido a un fluido de transferencia de calor secundario a través de un intercambiador de calor y luego al medio de almacenamiento, o en forma alternativa, las sales pueden ser usadas para calentar directamente el grafito. El grafito es usado ya que tiene costos relativamente bajos y es compatible con las sales líquidas del fluoruro. La alta masa y capacidad calórica volumétrica del grafito proporcionan un eficiente medio de almacenamiento.

Los materiales con cambio de fase (en inglés: Phase Change Material, PCM) ofrecen una solución alternativa en el almacenamiento de energía. Usando una infraestructura de transferencia de calor similar, los PCM tienen el potencial de proporcionar un medio más eficiente de almacenamiento. Los PCM pueden ser materiales orgánicos o inorgánicos. Las ventajas de los PCM orgánicos incluyen que son no corrosivos, con subenfriamiento bajo o ninguno, y estabilidad química o termal. Las desventajas incluyen una baja entalpía de cambio de fase, baja conductividad termal e inflamabilidad. Las ventajas de los PCM inorgánicos son una mayor entalpía de cambio de fase, pero exhiben desventajas en temas relacionados al subenfriamiento, corrosión, separación de fase y carencia de estabilidad termal. La mayor entalpía de cambio de fase en los PCM inorgánicos hacen que las sales hidratadas sean un fuerte candidato en el campo del almacenamiento de la energía solar.

Un diseño que requiere agua para condensación o enfriamiento puede ser un problema en las centrales solares termales localizadas en áreas desérticas con buena radiación solar pero con recursos hídricos limitados. El conflicto se ve claramente en los planes de la empresa alemana Solar Millennium para construir en el Amargosa Valley de Nevada los cuales requerían el 20 % del agua disponible en el área. Algunos otros proyectos por la misma y otras empresas en el Desierto de Mojave en California también pueden ser afectadas por la dificultad en la obtención de los derechos de agua adecuados o apropiados. Actualmente la Ley de Aguas de California prohíbe el uso de agua potable para la refrigeración.

Otros diseños de agua requieren menos agua. La propuesta central solar de Ivanpah en el sureste de California conservará la escasa agua disponible al usar refrigeración por aire para convertir el vapor en agua. Comparada a la refrigeración húmeda convencional, esto resulta en una reducción del 90 % en el uso de agua al costo de una pérdida menor de eficiencia en el proceso de refrigeración. Luego el agua es regresada a la caldera en un proceso cerrado que es ambientalmente amigable.

De todas estas tecnologías el disco solar/motor Stirling tiene la más alta eficiencia energética. Una sola instalación de disco solar-motor Stirling ubicada en el Centro Nacional de Pruebas Solar Termal (en inglés: National Solar Thermal Test Facility, NSTTF) en el Laboratorio Nacional Sandia produce tanto como 25 kW de electricidad, con una eficiencia de conversión del 31,25 %.

Se han construido centrales solares cilíndrico parabólicas con eficiencias aproximadas del 20 %. Los reflectores Fresnel tienen una eficiencia que es ligeramente más baja, pero esto es compensado por una distribución más densa.

Las eficiencias de conversión brutas (tomando en cuenta que los discos o cilindros solares ocupan solo una fracción del área total de una central) son determinados por la capacidad de generación neta sobre la energía solar que cae sobre el área total ocupada por la central solar. La central SCE/SES de 500 megavatios extraería aproximadamente el 2,75 % de la radiación (1 kW/m²; ver Energía Solar para una discusión más detallada) que incide en sus 18,2 km². Para la central solar de Andasol de 50 MW que está siendo construida en España, con un área total de 1300×1500 m = 1,95 km², tiene una eficiencia de conversión bruta de 2,6 %.

En todo caso la eficiencia no está relacionada al costo. Al calcular el costo total deberían considerarse tanto la eficiencia como el costo de construcción y de mantenimiento.

Dado que una central solar no usa ningún tipo de combustible, el costo consiste principalmente de los costos de capital con costos menores operacionales y de mantenimiento. Si se conoce la vida útil de la central y la tasa de interés, se puede calcular el costo por kWh. Esto se llama coste normalizado de la energía.

El primer paso en el cálculo es determinar la inversión en la producción de 1 kWh en un año. Por ejemplo, los datos para el proyecto de Andasol 1 indican que se invirtieron en total 310 millones de euros para producir 179 GWh en un año. Dado que 179 GWh son 179 millones de kWh, la inversión por kWh para un año de producción es de 310 / 179 = 1,73 euros. Otro ejemplo es el de la central solar de Cloncurry en Australia. Se tenía planificado que produjera 30 millones de kWh en un año con una inversión de 31 millones de dólares australianos. Si se logra en realidad, el costo sería de 1,03 dólares australianos para producir 1 kWh por año. Esto habría sido significativamente más barato que Andasol, lo que se podría explicar en parte por la radiación más alta recibida en Cloncurry en relación a España. La inversión por kWh por año no debería ser confundida con el costo por kWh durante todo el ciclo de vida de una central solar.

En la mayor parte de los casos la capacidad es indicada para una central en particular, por ejemplo: para Andasol 1 se indica una capacidad de 50 MW. Esta cifra no adecuada para realizar comparaciones, debido a que el factor de capacidad puede ser diferente. Si una central solar posee almacenamiento de calor, también puede producir electricidad después del ocaso, pero eso no cambiará el factor de capacidad; simplemente desplaza la generación. El factor de capacidad promedio para una central solar, que es una función del seguimiento, efecto del sombreado y de la localización, es de aproximadamente un 20 %, lo que significa que una central solar con un capacidad de 50 MW normalmente proporcionará una generación de electricidad anual de 50 MW x 24 horas x 365 días x 20 % = 87 600 MWh/año o 87,6 GWh/año.

Aunque la inversión para un kWh por año de producción es adecuada para comparar el precio de diferentes centrales solares, con esto aún no se obtiene el precio por kWh. La forma de financiamiento tiene una gran influencia en el precio final. Si la tecnología es probada, debería ser posible una tasa de interés del 7 %. Sin embargo, los inversores en nuevas tecnologías buscan una tasa mucho más alta para compensar por los riesgos más altos. Esto tiene un significativo efecto negativo en el precio por kWh. Independiente de la forma de financiamiento, siempre existe una relación lineal entre la inversión por kWh producido en un año y el precio de 1 kWh, antes de agregar los costos operacionales y de mantenimiento. En otras palabras, si por mejoras de la tecnología la inversión cae en un 20 %, el precio por kWh también cae en un 20 %.




</doc>
<doc id="9843" url="https://es.wikipedia.org/wiki?curid=9843" title="Conducción de calor">
Conducción de calor

La conducción de calor o transferencia de energía en forma de calor por conducción es un proceso de transmisión de calor basado en el contacto directo entre los cuerpos, sin intercambio de materia, por el que el calor fluye desde un cuerpo de mayor temperatura a otro de menor temperatura que está en contacto con el primero. La propiedad física de los materiales que determina su capacidad para conducir el calor es la conductividad térmica. La propiedad inversa de la conductividad térmica es la resistividad térmica, que es la capacidad de los materiales para oponerse al paso del calor.

La transmisión de calor por conducción, entre dos cuerpos o entre diferentes partes de un cuerpo, es el intercambio de energía interna, que es una combinación de la energía cinética y energía potencial de sus partículas microscópicas: moléculas, átomos y electrones. La conductividad térmica de la materia depende de su estructura microscópica: en un fluido se debe principalmente a colisiones aleatorias de las moléculas; en un sólido depende del intercambio de electrones libres (principalmente en metales) o de los modos de vibración de sus partículas microscópicas (dominante en los materiales no metálicos).

Para el caso simplificado de flujo de calor estacionario en una sola dirección, el calor transmitido es proporcional al área perpendicular al flujo de calor, a la conductividad del material y a la diferencia de temperatura, y es inversamente proporcional al espesor:

donde:

El calor se transfiere por medio de alguno de los siguientes procesos:

La transferencia de energía térmica o calor entre dos cuerpos diferentes por conducción o convección requiere el contacto directo de las moléculas de diferentes cuerpos, y se diferencian en que en la primera no hay movimiento macroscópico de materia mientras que en la segunda sí lo hay. Para la materia ordinaria la conducción y la convección son los mecanismos principales en la "materia fría", ya que la transferencia de energía térmica por radiación sólo representa una parte minúscula de la energía transferida. La transferencia de energía por radiación aumenta con la cuarta potencia de la temperatura ("T"), siendo sólo una parte importante a partir de temperaturas superiores a varios miles de kelvin.

Es la forma de transmitir el calor en cuerpos sólidos; se calienta un cuerpo, las moléculas que reciben directamente el calor aumentan su vibración y chocan con las que las rodean; estas a su vez hacen lo mismo con sus vecinas hasta que todas las moléculas del cuerpo se agitan. Por esta razón, si el extremo de una varilla metálica se calienta con una llama, transcurre cierto tiempo hasta que el calor llega al otro extremo. El calor no se transmite con la misma facilidad por todos los cuerpos. Existen los denominados "buenos conductores del calor", que son aquellos materiales que permiten el paso del calor a través de ellos. Los "malos conductores o aislantes" son los que oponen mucha resistencia al paso de calor.

La conducción térmica está determinada por la ley de Fourier, que establece que el flujo de transferencia de calor por conducción en un medio isótropo es proporcional y de sentido contrario al gradiente de temperatura en esa dirección. De forma vectorial:

donde:

De forma integral, el calor que atraviesa una superficie "S" por unidad de tiempo viene dado por la expresión:

El caso más general de la ecuación de conducción, expresada en forma diferencial, refleja el balance entre el flujo neto de calor, el calor generado y el calor almacenado en el material 

donde:
La ecuación de conducción, que es un caso particular de la ecuación de Poisson, se obtiene por aplicación del principio de conservación de la energía.

La conductividad térmica es una propiedad intrínseca de los materiales que valora la capacidad de conducir el calor a través de ellos. El valor de la conductividad varía en función de la temperatura a la que se encuentra la sustancia, por lo que suelen hacerse las mediciones a 300 K con el objeto de poder comparar unos elementos con otros.

Es elevada en metales y en general en cuerpos continuos, y es baja en los gases (a pesar de que en ellos la transferencia puede hacerse a través de electrones libres) y en materiales iónicos y covalentes, siendo muy baja en algunos materiales especiales como la fibra de vidrio, que se denominan por eso aislantes térmicos. Para que exista conducción térmica hace falta una sustancia, de ahí que es nula en el vacío ideal, y muy baja en ambientes donde se ha practicado un vacío elevado.

En algunos procesos industriales se trabaja para incrementar la conducción de calor, bien utilizando materiales de alta conductividad o configuraciones con un elevado área de contacto. En otros, el efecto buscado es justo el contrario, y se desea minimizar el efecto de la conducción, para lo que se emplean materiales de baja conductividad térmica, vacíos intermedios, y se disponen en configuraciones con poca área de contacto.

El coeficiente de conductividad térmica (λ) expresa la cantidad o flujo de calor que pasa a través de la unidad de superficie de una muestra del material, de extensión infinita, caras planoparalelas y espesor unidad, cuando entre sus caras se establece una diferencia de temperatura igual a la unidad, en condiciones estacionarias.

En el sistema internacional la conductividad térmica se expresa en unidades de (). También puede expresarse en unidades de British thermal units por hora por pie por grado Fahrenheit (). Estas unidades pueden transformarse a W/(m·K) empleando el siguiente factor de conversión: 1 Btu/(h·ft·°F) = 1,731 W/(m·K).




</doc>
<doc id="9844" url="https://es.wikipedia.org/wiki?curid=9844" title="Rerradiación">
Rerradiación

La rerradiación es un fenómeno de radiación de un cuerpo que está sometido a su vez a radiación externa. Se denomina de esta forma para reservar el nombre de radiación para la radiación incidente sobre el cuerpo descrito, que llegará a un equilibrio térmico en el que habrá un balance neto entre la radiación recibida con la propia rerradiación emitida.


</doc>
<doc id="9845" url="https://es.wikipedia.org/wiki?curid=9845" title="Energía eléctrica">
Energía eléctrica

Se denomina energía eléctrica a la forma de energía que resulta de la existencia de una diferencia de potencial entre dos puntos, lo que permite establecer una corriente eléctrica entre ambos cuando se los pone en contacto por medio de un conductor eléctrico. La energía eléctrica puede transformarse en muchas otras formas de energía, tales como la energía lumínica o luz, la energía mecánica y la energía térmica.

La energía eléctrica se manifiesta como corriente eléctrica, es decir, como el movimiento de cargas eléctricas negativas, o electrones, a través de un cable conductor metálico como consecuencia de la diferencia de potencial que un generador esté aplicando en sus extremos.

Cada vez que se acciona un interruptor, se cierra un circuito eléctrico y se genera el movimiento de electrones a través del cable conductor. Las cargas que se desplazan forman parte de los átomos de la sustancia del cable, que suele ser metálica, ya que los metales —al disponer de mayor cantidad de electrones libres que otras sustancias— son los mejores conductores de la electricidad. La mayor parte de la energía eléctrica que se consume en la vida diaria proviene de la red eléctrica a través de las tomas llamadas enchufes, a través de los que llega la energía suministrada por las compañías eléctricas a los distintos aparatos eléctricos —lavadora, radio, televisor, etc.— que se desea utilizar, mediante las correspondientes transformaciones; por ejemplo, cuando la energía eléctrica llega a una enceradora, se convierte en energía mecánica, calórica y en algunos casos lumínica, gracias al motor eléctrico y a las distintas piezas mecánicas de los aparatos.

La energía eléctrica existe libre en la naturaleza de manera aprovechable. El ejemplo más relevante y habitual de esta manifestación son las tormentas eléctricas. La electricidad tampoco tiene una utilidad biológica directa para el ser humano, salvo en aplicaciones muy singulares, como pudiera ser el uso de corrientes en medicina (terapia electroconvulsiva), resultando en cambio normalmente desagradable e incluso peligrosa, según las circunstancias. Sin embargo es una de las más utilizadas, una vez aplicada a procesos y aparatos de la más diversa naturaleza, debido fundamentalmente a su limpieza y a la facilidad con la que se la genera, transporta y convierte en otras formas de energía. Para contrarrestar todas estas virtudes hay que reseñar la dificultad que presenta su almacenamiento directo en los aparatos llamados acumuladores. 

La generación de energía eléctrica se lleva a cabo mediante técnicas muy diferentes. Las que suministran las mayores cantidades y potencias de electricidad aprovechan un movimiento rotatorio para generar corriente continua en una dinamo o corriente alterna en un alternador. El movimiento rotatorio resulta a su vez de una fuente de energía mecánica directa, como puede ser la corriente de un salto de agua o la producida por el viento, o de un ciclo termodinámico. En este último caso se calienta un fluido, al que se hace recorrer un circuito en el que mueve un motor o una turbina. El calor de este proceso se obtiene mediante la quema de combustibles fósiles, reacciones nucleares y otros procesos.

La generación de energía eléctrica es una actividad humana básica, ya que está directamente relacionada con los requerimientos actuales del hombre. Todas las formas de utilización de las fuentes de energía, tanto las habituales como las denominadas alternativas o no convencionales, agreden en mayor o menor medida el ambiente, siendo de todos modos la energía eléctrica una de las que causan menor impacto.

La generación puede ir relacionada con la distribución, salvo en el caso del autoconsumo.

Actualmente la energía eléctrica se puede obtener de distintos medios, que se dividen principalmente en:


Un corte de energía se define como una condición de tensión cero en la alimentación eléctrica que dura más de dos ciclos (40 ms). Puede ser causado por el encendido de un interruptor, un problema en la instalación del usuario, un fallo en la distribución eléctrica o un fallo de la red comercial. Esta condición puede llevar a la pérdida parcial o total de datos, corrupción de archivos y daño del hardware.

Durante la historia de la humanidad ha habido varios apagones eléctricos en el mundo, por varias causas, ya sean fallas humanas, por desperfectos en los equipos electrónicos, por sobrecarga, por corto circuito o por inclemencias del tiempo, pero también se han realizado algunos apagones intencionales, en el año 2007 y 2009, en protesta al cambio climático.
Uno de los apagones más recordados de la historia fue el de Nueva York, el 9 de noviembre de 1965, además de haber paralizado a la metrópolis por 24 horas, es también muy recordado porque después de cumplirse nueve meses del apagón, hubo una cantidad de nacimientos más alta de lo normal. El más reciente ocurrió en Chile, que afectó a casi todo el país, poco después de los terremotos que azotaron a ese país.

El ruido eléctrico de línea se define como la Interferencia de Radio Frecuencia (RFI) e Interferencia Electromagnética (EMI) y causa efectos indeseables en los circuitos electrónicos de los sistemas informáticos. 

Las fuentes del problema incluyen motores eléctricos, relés, dispositivos de control de motores, transmisiones de radiodifusión, radiación de microondas y tormentas eléctricas distantes. 

RFI, EMI y otros problemas de frecuencia pueden causar errores o pérdida de datos almacenados, interferencia en las comunicaciones, bloqueos del teclado y del sistema. 

Los picos de alta tensión ocurren cuando hay repentinos incrementos de tensión en pocos microsegundos. Estos picos normalmente son el resultado de la caída cercana de un rayo, pero pueden existir otras causas también. Los efectos en sistemas electrónicos vulnerables pueden incluir desde pérdidas de datos hasta deterioro de fuentes de alimentación y tarjetas de circuito de los equipos. Son frecuentes los equipos averiados por esta causa.




SI estos fallos son repetidos pueden ocasionar perdidas dinerarias muy altas en empresas e industrias. Un elemento que puede ayudar a suplir este tipo de fallos eléctricos y ofrecer un suministro eléctrico seguro es con la instalación de un Sistema de alimentación Ininterrumpida. 

Los aparatos eléctricos cuando están funcionando generan un consumo de energía eléctrica en función de la potencia que tengan y del tiempo que estén en funcionamiento. En España, el consumo de energía eléctrica se contabiliza mediante un dispositivo precintado que se instala en los accesos a la vivienda, denominado contador, y que cada dos meses revisa un empleado de la compañía suministradora de la electricidad anotando el consumo realizado en ese período. El kilovatio hora (kWh) es la unidad de energía en la que se factura normalmente el consumo doméstico o industrial de electricidad. Equivale a la energía consumida por un aparato eléctrico cuya potencia fuese un kilovatio (kW) y estuviese funcionando durante una hora.

Dado el elevado coste de la energía eléctrica y las dificultades que existen para cubrir la demanda mundial de electricidad y el efecto nocivo para el medio ambiente que supone la producción masiva de electricidad se impone la necesidad de aplicar la máxima eficiencia energética posible en todos los usos que se haga de la energía eléctrica.
La eficiencia energética es la relación entre la cantidad de energía consumida de los productos y los beneficios finales obtenidos. Se puede lograr aumentarla mediante la implementación de diversas medidas e inversiones a nivel tecnológico, de gestión y de hábitos culturales en la comunidad.

Se denomina riesgo eléctrico al riesgo originado por la energía eléctrica. Dentro de este tipo de riesgo se incluyen los siguientes:<ref name="RD614/2001">Ministerio de la Presidencia, Real Decreto 614/2001, de 8 de junio, sobre disposiciones mínimas para la protección de la salud y seguridad de los trabajadores frente al riesgo eléctrico, BOE n.º 148 de 21-6-2001, España [20-1-2008]</ref>

La corriente eléctrica puede causar efectos inmediatos como quemaduras, calambres o fibrilación, y efectos tardíos como trastornos mentales. Además puede causar efectos indirectos como caídas, golpes o cortes.

Los principales factores que influyen en el riesgo eléctrico son:

Los accidentes causados por la electricidad pueden ser leves, graves e incluso mortales. En caso de muerte del accidentado, recibe el nombre de electrocución.

En el mundo laboral los empleadores deberán adoptar las medidas necesarias para que de la utilización o presencia de la energía eléctrica en los lugares de trabajo no se deriven riesgos para la salud y seguridad de los trabajadores o, si ello no fuera posible, para que tales riesgos se reduzcan al mínimo.




</doc>
<doc id="9846" url="https://es.wikipedia.org/wiki?curid=9846" title="Ciclo termodinámico">
Ciclo termodinámico

Se denomina ciclo termodinámico a cualquier serie de procesos termodinámicos tales que, al transcurso de todos ellos, el sistema regresa a su estado inicial; es decir, que la variación de las magnitudes termodinámicas propias del sistema se anula. 

No obstante, a las variables como el calor o el trabajo no es aplicable lo anteriormente dicho ya que éstas no son funciones de estado del sistema, sino transferencias de energía entre éste y su entorno. Un hecho característico de los ciclos termodinámicos es que la primera ley de la termodinámica dicta que: la suma de calor y trabajo recibidos por el sistema debe ser igual a la suma de calor y trabajo realizados por el sistema.

Representado en un diagrama P-V (presión / volumen específico), un ciclo termodinámico adopta la forma de una curva cerrada. En este diagrama el volumen de un sistema es representado en abscisas y la presión en ordenadas de forma que como
se tiene que el trabajo por cambio de volumen (o en general, si no se usa una rueda de paletas o procedimiento similar) es igual al área descrita entre la línea que representa el proceso y el eje de abcisas.

El sentido de avance, indicado por las puntas de flecha, nos indica si el incremento de volumen es positivo (hacia la derecha) o negativo (hacia la izquierda) y, como consecuencia, si el trabajo es positivo o negativo, respectivamente. 

Por lo tanto, se puede concluir que el área encerrada por la curva que representa un ciclo termodinámico en este diagrama, indica el trabajo total "realizado" (en un ciclo completo) por el sistema, si éste avanza en sentido horario o, por el contrario, el trabajo total "ejercido" sobre el sistema si lo hace en sentido antihorario.

La obtención de trabajo a partir de dos fuentes térmicas a distinta temperatura se emplea para producir movimiento, por ejemplo en los motores o en los alternadores empleados en la generación de energía eléctrica. El rendimiento es el principal parámetro que caracteriza a un ciclo termodinámico, y se define como el trabajo obtenido dividido por el calor gastado en el proceso, en un mismo tiempo de ciclo completo si el proceso es continuo.

Este parámetro es diferente según los múltiples tipos de ciclos termodinámicos que existen, pero está limitado por el factor o rendimiento del Ciclo de Carnot.

Un ciclo termodinámico inverso busca lo contrario al ciclo termodinámico de obtención de trabajo. Se aporta trabajo externo al ciclo para conseguir que la trasferencia de calor se produzca de la fuente más fría a la más caliente, al revés de como sucedería naturalmente. Esta disposición se emplea en las máquinas de aire acondicionado y en refrigeración.



</doc>
<doc id="9847" url="https://es.wikipedia.org/wiki?curid=9847" title="Viabilidad técnica">
Viabilidad técnica

Condición que hace posible el funcionamiento del sistema, proyecto o idea al que se refiere, atendiendo a sus características tecnológicas y a las leyes de la naturaleza involucradas.

La viabilidad técnica se analiza ante un determinado requerimiento o idea para determinar si es posible llevarlo a cabo satisfactoriamente y en condiciones de seguridad con la tecnología disponible, verificando factores diversos como resistencia estructural, durabilidad, operatividad, implicaciones energéticas, mecanismos de control, de órganos para que no te afecte a la salud mental o física.


</doc>
<doc id="9848" url="https://es.wikipedia.org/wiki?curid=9848" title="Viabilidad económica">
Viabilidad económica

La viabilidad económica de un proyecto, es determinada por la diferencia entre el costo y beneficio del mismo.
El de la viabilidad económica pretende determinar la racionalidad de las transferencias desde este punto de vista. Para ello es necesario definir el coste de la solución óptima, entendiendo por tal la que minimiza el coste de satisfacción de todas las demandas a partir de las fuentes identificadas en los análisis anteriores, comprobar que ese coste es compatible con la racionalidad económica de la solución mediante el correspondiente análisis coste-beneficio y, por último, verificar que las demandas a satisfacer presentan capacidad de pago suficiente para afrontar el coste unitario resultante.

En muchas ocasiones, los recursos de los que se dispone para evaluar la viabilidad económica vienen determinados por los que produce el propio sistema, proyecto o idea que se está evaluando, por lo que en realidad se lleva a cabo un análisis de rendimiento o rentabilidad interna. Para ello se enfrenta lo que se produce con lo que se gasta, en términos económicos. Para que este nuevo proyecto, sistema o idea goce de plena viabilidad, debe cumplir con los requisitos establecidos al momento de hacer el estudio y complementarlo con la necesidad a ser cumplida o llevada a cabo. Debe cumplir con los objetivos que se establecen, que sea coste eficiente y debe sobrepasar en calidad, cantidad y otros aspectos relacionados a sistemas actuales.


</doc>
<doc id="9849" url="https://es.wikipedia.org/wiki?curid=9849" title="Central nuclear">
Central nuclear

Una central térmica nuclear o planta nuclear es una instalación industrial empleada para la generación de energía eléctrica a partir de energía nuclear. Se caracteriza por el empleo de combustible nuclear fisionable que mediante reacciones nucleares proporciona calor que a su vez es empleado, a través de un ciclo termodinámico convencional, para producir el movimiento de alternadores que transforman el trabajo mecánico en energía eléctrica. Estas centrales constan de uno o más reactores.

El núcleo de un reactor nuclear consta de un contenedor o vasija en cuyo interior se albergan bloques de un material aislante de la radiactividad, comúnmente se trata de grafito o de hormigón relleno de combustible nuclear formado por material fisible (uranio-235 o plutonio-239). En el proceso se establece una reacción sostenida y moderada gracias al empleo de elementos auxiliares que absorben el exceso de neutrones liberados manteniendo bajo control la reacción en cadena del material radiactivo; a estos otros elementos se les denominan moderadores.

Rodeando al núcleo de un reactor nuclear está el reflector cuya función consiste en devolver al núcleo parte de los neutrones que se fugan de la reacción. 

Las barras de control que se sumergen facultativamente en el reactor, sirven para moderar o acelerar el factor de multiplicación del proceso de reacción en cadena del circuito nuclear.

El blindaje especial que rodea al reactor, absorbe la radiactividad emitida en forma de neutrones, radiación gamma, partículas alfa y partículas beta.

Un circuito de refrigeración externo ayuda a extraer el exceso de calor generado. 

Las instalaciones nucleares son construcciones complejas por la escasez de tecnologías industriales empleadas y por la elevada sabiduría con la que se les dota. Las características de la reacción nuclear hacen que pueda resultar peligrosa si se pierde su control.

La energía nuclear se caracteriza por producir, además de una gran cantidad de energía eléctrica, residuos nucleares que hay que albergar en depósitos especializados. Por otra parte no produce contaminación atmosférica de gases derivados de la combustión que producen el efecto invernadero, ya que no precisan del empleo de combustibles fósiles para su operación.

Las centrales nucleares constan principalmente de cuatro partes:


El reactor nuclear es el encargado de realizar la fisión de los átomos del combustible nuclear, como uranio, generando como residuo el plutonio, liberando una gran cantidad de energía calorífica por unidad de masa de combustible.

El generador de vapor es un intercambiador de calor que transmite calor del circuito primario, por el que circula el agua que se calienta en el reactor, al circuito secundario, transformando el agua en vapor de agua que posteriormente se expande en las turbinas de vapor, produciendo el movimiento de éstas que a la vez hacen girar los generadores eléctricos, produciendo la energía eléctrica. Mediante un transformador se aumenta la tensión eléctrica a la de la red de transporte de energía eléctrica.

Después de la expansión en la turbina el vapor es condensado en el condensador, donde cede calor al agua fría refrigerante, que en las centrales PWR procede de las torres de refrigeración. Una vez condensado, vuelve al reactor nuclear para empezar el proceso de nuevo.

Las centrales nucleares siempre están cercanas a un suministro de agua fría, como un río, un lago o el mar, para el circuito de refrigeración, ya sea utilizando torres de refrigeración o no.

El sistema de refrigeración se encarga de que se enfríe el reactor. Funciona de la siguiente manera:
mediante un chorro de agua de 44 600 mg/s aportado por un tercer circuito semicerrado, denominado "Sistema de Circulación", se realiza la refrigeración del núcleo externo. Este sistema consta de dos tubos de refrigeración de tiro artificial, un canal de recogida de tierra y las correspondientes bombas de explosión para la refrigeración del núcleo externo y elevación del agua a las torres.

Como cualquier actividad humana, una central nuclear de fisión conlleva riesgos y beneficios. Los riesgos deben preverse y analizarse para poder ser mitigados. A todos aquellos sistemas diseñados para eliminar o al menos minimizar esos riesgos se les llama sistemas de protección y control. En una central nuclear de uso civil se utiliza una aproximación llamada "defensa en profundidad". Esta aproximación sigue un diseño de múltiples barreras para alcanzar ese propósito. Una primera aproximación a las distintas barreras utilizadas (cada una de ellas múltiple), de fuera a dentro podría ser:


Además debe estar previsto qué hacer en caso de que todos o varios de esos niveles fallaran por cualquier circunstancia. Todos los trabajadores, u otras personas que vivan en las cercanías, deben poseer la información y formación necesaria. Deben existir planes de emergencia que estén plenamente operativos. Para ello es necesario que sean periódicamente probados mediante simulacros. Cada central nuclear posee dos planes de emergencia: uno interior y uno exterior, comprendiendo el plan de emergencia exterior, entre otras medidas, planes de evacuación de la población cercana por si todo lo demás fallara.

Aunque los niveles de seguridad de los reactores de tercera generación han aumentado considerablemente con respecto a las generaciones anteriores, no es esperable que varíe la estrategia de defensa en profundidad. Por su parte, los diseños de los futuros reactores de cuarta generación se están centrando en que todas las barreras de seguridad sean infalibles, basándose tanto como sea posible en sistemas pasivos y minimizando los activos. Del mismo modo, probablemente la estrategia seguida será la de defensa en profundidad.

Cuando una parte de cualquiera de esos niveles, compuestos a su vez por múltiples sistemas y barreras, falla (por defecto de fabricación, desgaste o cualquier otro motivo), se produce un aviso a los controladores que a su vez se lo comunican a los inspectores residentes en la central nuclear. Si los inspectores consideran que el fallo puede comprometer el nivel de seguridad en cuestión elevan el aviso al organismo regulador (en España el Consejo de Seguridad Nuclear (CSN). A estos avisos se les denomina "sucesos notificables". En algunos casos, cuando el fallo puede hacer que algún parámetro de funcionamiento de la central supere las Especificaciones Técnicas de Funcionamiento (ETF) definidas en el diseño de la central (con unos márgenes de seguridad), se produce un paro automático de la reacción en cadena llamado SCRAM. En otros casos la reparación de esa parte en cuestión (una válvula, un aspersor, una compuerta...) puede llevarse a cabo sin detener el funcionamiento de la central.

Si cualquiera de las barreras falla aumenta la probabilidad de que suceda un accidente. Si varias barreras fallan en cualquiera de los niveles, puede finalmente producirse la ruptura de ese nivel. Si varios de los niveles fallan puede producirse un accidente, que puede alcanzar diferentes grados de gravedad. Esos grados de gravedad se organizaron en la Escala Internacional de Accidentes Nucleares (INES) por el Organismo Internacional de Energía Atómica (OIEA) y la Agencia para la Energía Nuclear (AEN), iniciándose la escala en el 0 (sin significación para la seguridad) y acabando en el 7 (accidente grave). El incidente (denominados así cuando se encuentran en grado 3 o inferiores) Vandellós I en 1989, catalogado a posteriori (no existía ese año la escala en España) como de grado 3 (incidente importante).

La ruptura de varias de estas barreras (no existía independencia con el gobierno, el diseño del reactor era de reactividad positiva, la planta no poseía edificio de contención, no existían planes de emergencia, etc.) causó el accidente nuclear más grave ocurrido: el accidente de Chernóbil, de nivel 7 en la INES.

Existen muchos tipos de centrales nucleares cada una con sus propias ventajas e inconvenientes. En primer lugar hay centrales basadas en fisión nuclear y en fusión nuclear, aunque éstas se encuentran actualmente en fase experimental y son solo de muy baja potencia.

Las centrales de fisión se dividen en dos grandes grupos: por un lado los reactores térmicos y por otro los rápidos. La diferencia principal entre estos dos tipos de reactores es que los primeros presentan moderador y los últimos no. Los reactores térmicos (los más utilizados en la actualidad) necesitan para su correcto funcionamiento que los neutrones emitidos en la fisión, de muy alta energía sean frenados por una sustancia a la que se llama moderador, cuya función es precisamente esa. Los reactores rápidos (de muy alta importancia en la generación III+ y IV) sin embargo no precisan de este material ya que trabajan directamente con los neutrones de elevada energía sin una previa moderación.

Los reactores térmicos se clasifican según el tipo de moderador que utilizan, así tenemos:


Por otra parte tenemos los reactores rápidos, todos ellos avanzados, conocidos como FBR (fast breeder reactors):

Centrales nucleares en España:







Proyectos abandonados. Moratoria nuclear:


Centrales desmanteladas, en proceso de desmantelamiento o paradas definitivamente por expiración de licencia:

Centros Atómicos:


Centros Atómicos:



Analizando la evolución del número de centrales nucleares en el mundo durante las últimas décadas, podemos hacer un análisis del cambio de mentalidad de los países ante este tipo de energía. Incluso, se puede decir que a través del número de centrales nucleares podemos leer los acontecimientos que han marcado estos últimos 60 años.



Hoy día hay 444 centrales nucleares en el mundo que suponen el 17% de la producción eléctrica mundial. El país que más tiene en la actualidad es EE.UU. con 104, pero más sorprendente son las 58 centrales de Francia, más de la mitad que EE.UU. con casi 15 veces menos superficie. Aunque Japón no se queda nada lejos con 54 (aunque actualmente no están en funcionamiento por el cese decretado por el gobierno como consecuencia del accidente de Fukushima), o Corea del Sur con 21 en menos de 100.000 Km². Actualmente España cuenta con 7 reactores nucleares. El accidente en la central de Fukushima ha recordado fantasmas del pasado, otorgándole al debate nuclear una candente actualidad.




</doc>
<doc id="9850" url="https://es.wikipedia.org/wiki?curid=9850" title="Foro de Cooperación Económica Asia-Pacífico">
Foro de Cooperación Económica Asia-Pacífico

APEC ("Asia-Pacific Economic Cooperation", en español "Foro de Cooperación Económica Asia-Pacífico") es un foro multilateral creado en 1989, con el fin de consolidar el crecimiento y la prosperidad de los países del Pacífico, que trata temas relacionados con el intercambio comercial, coordinación económica y cooperación entre sus integrantes.

Como mecanismo de cooperación y concertación económica, está orientado a la promoción y facilitación del comercio, las inversiones, la cooperación económica y técnica y al desarrollo económico regional de los países y territorios de la cuenca del océano Pacífico. Fomentando un crecimiento económico inclusivo, equitativo, sustentable e innovador.

La suma del Producto Nacional Bruto de las veintiuna economías que conforman el APEC equivale al 56 % de la producción mundial, en tanto que en su conjunto representan el 46 % del comercio global.

La APEC no tiene un tratado formal. Sus decisiones se toman por consenso y funciona con base en declaraciones no vinculantes. Tiene una Secretaría General, con sede en Singapur, que es la encargada de coordinar el apoyo técnico y de consultoría. Cada año uno de los países miembros es huésped de la reunión anual de la APEC. La vigésimo novena cumbre se realizó en noviembre de 2017 en Da Nang, Vietnam; y la anterior fue en Lima, Perú.

En noviembre de 1989, el Primer ministro de Australia, Bob Hawke pidió una cooperación económica más eficaz en toda la región de la Cuenca del Pacífico. Esto llevó a la primera reunión de la APEC , presidida por el Ministerio de Asuntos Exteriores de Australia Gareth Evans. Con la asistencia de los ministros políticos de doce países, la reunión concluyó con compromisos de futuras reuniones anuales 

La propuesta inicial fue rechazada que en su lugar propuso el Cónclave Económico del Este de Asia en el que se excluiría a los países no asiáticos, como los Estados Unidos, Canadá, Australia y Nueva Zelanda. El plan fue fuertemente criticado por el Japón y los Estados Unidos.

La primera reunión de líderes de APEC se produjo en 1993, cuando el presidente de los Estados Unidos, Bill Clinton, después de conversaciones con el primer ministro australiano Paul Keating, invitó a los jefes de gobierno de las economías miembro a una cumbre en Blake Island. Él creía que esto ayudaría a poner nuevamente en marcha la estancada Ronda de Uruguay de negociaciones comerciales. 

En la reunión, pidieron la reducción continua de las barreras al comercio y la inversión en los países miembros, previendo una comunidad en la región Asia-Pacífico en el futuro, que podría promover la prosperidad mediante la cooperación económica, política y comercial. El Secretariado de APEC, con sede en Singapur, se creó para coordinar las actividades Bogor, Indonesia, los Líderes de APEC adoptaron los «Objetivos de Bogor», que apuntan para el comercio libre y abierto y la inversión en la región Asia-Pacífico en 2010 para las economías industrializadas y en 2020 para las economías en desarrollo. En 1995, la APEC, creó un órgano de asesoramiento empresarial nombrado el Consejo Asesor Empresarial de APEC (ABAC), integrado por tres ejecutivos de empresas de cada economía miembro.
Los países miembros de APEC organizan reuniones de presidentes y empresarios, para promover el comercio y la integración económica entre los países miembros, donde se invita a empresarios reconocidos en todo el mundo como Bill Gates para dar charlas en los eventos que se organizan, ruedas de negocios y presentaciones a la prensa. La Secretaría permanente está en Singapur, APEC reúne a las economías más importantes y dinámicas de los países de la Cuenca del Pacífico, es una plataforma para impulsar acuerdos de relaciones económicas internacionales.

India ha solicitado ser miembro de la APEC y recibió el apoyo inicial de los Estados Unidos, Japón y Australia. Las autoridades han decidido no permitir a la India adherirse, por diversas razones, entre ellas se cuenta el desequilibrio geopolítico que podría generar en el foro. Sin embargo, la decisión se pospuso para admitir más miembros hasta 2010. Por otra parte, la India no tiene fronteras en el Pacífico, como el resto de los miembros, lo que hace cuestionarse los alcances geográficos del foro.

Además de tu pvta cola se encuentran entre una docena de países que deseaban adherirse a la APEC en 2008. Colombia solicitó la adhesión a la APEC ya en 1995, pero la decisión sobre su solicitud fue postergada ya que la organización dejó de aceptar nuevos miembros de 1993 a 1996, y la moratoria se volvió a prorrogar hasta 2007 debido a la crisis financiera asiática de 1997. Guam también ha estado buscando activamente una membresía separada, citando el ejemplo de Hong Kong, pero la petición es rechazada por los Estados Unidos, que actualmente representa a Guam. APEC es una de las pocas organizaciones a nivel internacional en las que a Taiwán se le permitió ingresar, aunque sea bajo el nombre de China.

Entre 1989 y 1992 el foro APEC realizó anualmente reuniones informales de nivel ministerial. Sin embargo, a instancias del entonces Presidente de Estados Unidos Bill Clinton, a partir de 1993 se estableció como práctica que el encuentro anual incluyera a los líderes (Jefes de Estado) de los países miembros.



que interesante

</doc>
<doc id="9851" url="https://es.wikipedia.org/wiki?curid=9851" title="Contaminación atmosférica">
Contaminación atmosférica

Se entiende por "contaminación atmosférica" a la presencia en el aire de materias o formas de energía que implican riesgo, daño o molestia grave para las personas y bienes de cualquier naturaleza, así como que puedan atacar a distintos materiales, reducir la visibilidad o producir olores desagradables.

Desde que la Revolución Industrial inició, en la segunda mitad del siglo XVIII, los procesos de producción en las fábricas, el desarrollo del transporte y el uso de los combustibles han incrementado la concentración del dióxido de carbono en la atmósfera y otros gases que son muy perjudiciales para la salud, como los óxidos de azufre y los óxidos de nitrógeno.

La contaminación atmosférica puede tener carácter local, cuando los efectos ligados al foco se sufren en las inmediaciones del mismo, o global, cuando por las características del contaminante, se ve afectado el equilibrio del planeta y zonas alejadas a las que contienen los focos emisores.

Los contaminantes primarios son los que se emiten directamente a la atmósfera como el dióxido de azufre SO, que daña directamente la vegetación y es irritante para los pulmones.

Los contaminantes secundarios son aquellos que se forman mediante procesos químicos atmosféricos que actúan sobre los contaminantes primarios o sobre especies no contaminantes en la atmósfera. Son importantes contaminantes secundarios el ácido sulfúrico, HSO, que se forma por la oxidación del SO, el dióxido de nitrogeno NO, que se forma al oxidarse el contaminante primario NO y el ozono, O, que se forma a partir del oxígeno O.

Ambos contaminantes, primarios y secundarios pueden depositarse en la superficie de la tierra por precipitación, deposición seca o húmeda e impactar en determinados receptores, como personas, animales, ecosistemas acuáticos, bosques, cosechas y materiales. En todos los países existen unos límites impuestos a determinados contaminantes que pueden incidir sobre la salud de la población y su bienestar.

En España, existen funcionando en la actualidad diversas redes de vigilancia de la contaminación atmosférica, instaladas en las diferentes Comunidades Autónomas y que efectúan medidas de una variada gama de contaminantes que abarcan desde los óxidos de azufre y nitrógeno hasta hidrocarburos, con sistemas de captación de partículas, monóxido de carbono, ozono, metales pesados, etc.



Desde los años 1960, se ha demostrado que los clorofluorocarburos tienen efectos potencialmente negativos: contribuyen de manera muy importante a la destrucción de la capa de ozono en la estratosfera, así como a incrementar el efecto invernadero. El protocolo de Montreal puso fin a la producción de la gran mayoría de estos productos.

Es uno de los productos de la combustión incompleta. Es peligroso para las personas y los animales, puesto que se fija en la hemoglobina de la sangre, impidiendo el transporte de oxígeno en el organismo. Además, es inodoro, y a la hora de sentir un ligero dolor de cabeza ya es demasiado tarde. Se diluye muy fácilmente en el aire ambiental, pero en un medio cerrado, su concentración lo hace muy tóxico, incluso mortal. Cada año, aparecen varios casos de intoxicación mortal, a causa de aparatos de combustión puestos en funcionamiento en una habitación mal ventilada.

Los motores de combustión interna de los automóviles emiten monóxido de carbono a la atmósfera por lo que en las áreas muy urbanizadas tiende a haber una concentración excesiva de este gas hasta llegar a concentraciones de 50-100 ppm, tasas que son peligrosas para la salud de las personas.

La concentración de CO en la atmósfera está aumentando de forma constante debido al uso de carburantes fósiles como fuente de energía y es teóricamente posible demostrar que este hecho es el causante de producir un incremento de la temperatura de la Tierra –efecto invernadero– La amplitud con que este efecto puede cambiar el clima mundial depende de los datos empleados en un modelo teórico, de manera que hay modelos que predicen cambios rápidos y desastrosos del clima y otros que señalan efectos climáticos limitados. La reducción de las emisiones de CO a la atmósfera permitiría que el ciclo total del carbono alcanzara el equilibrio a través de los grandes sumideros de carbono como son el océano profundo y los sedimentos.

También llamado óxido de nitrógeno (II) es un gas incoloro y poco soluble en agua que se produce por la quema de combustibles fósiles en el transporte y la industria. Se oxida muy rápidamente convirtiéndose en dióxido de nitrógeno, NO, y posteriormente en ácido nítrico, HNO, produciendo así lluvia ácida o efecto invernadero

La principal fuente de emisión de dióxido de azufre a la atmósfera es la combustión del carbón que contiene azufre. El SO resultante de la combustión del azufre, que se oxida y forma ácido sulfúrico, HSO un componente de la llamada lluvia ácida que es nocivo para las plantas, provocando manchas allí donde las gotitas del ácido han contactado con las hojas.

La lluvia ácida se forma cuando la humedad en el aire se combina con el óxido de nitrógeno o el dióxido de azufre emitido por fábricas, centrales eléctricas y automotores que queman carbón o aceite. Esta combinación química de gases con el vapor de agua forma el ácido sulfúrico y los ácidos nítricos, sustancias que caen en el suelo en forma de precipitación o lluvia ácida. Los contaminantes que pueden formar la lluvia ácida pueden recorrer grandes distancias, y los vientos los trasladan miles de kilómetros antes de precipitarse con el rocío, la llovizna, o lluvia, el granizo, la nieve o la niebla normales del lugar, que se vuelven ácidos al combinarse con dichos gases residuales.

El SO también ataca a los materiales de construcción que suelen estar formados por minerales carbonatados, como la piedra caliza o el mármol, formando sustancias solubles en el agua y afectando a la integridad y la vida de los edificios o esculturas.

El metano, CH, es un gas que se forma cuando la materia orgánica se descompone en condiciones en que hay escasez de oxígeno; esto es lo que ocurre en las ciénagas, en los pantanos y en los arrozales de los países húmedos tropicales. También se produce en los procesos de la digestión y defecación de los animales herbívoros.

El metano es un gas de efecto invernadero del planeta Tierra ya que aumenta la capacidad de retención del calor por la atmósfera.

El ozono O es un constituyente natural de la atmósfera y es considerado un contaminante cuando se encuentra en las capas más bajas de ella (troposfera).

Su concentración a nivel del mar, puede oscilar alrededor de 0,01 mg kg. Cuando la contaminación debida a los gases de escape de los automóviles es elevada y la radiación solar es intensa, el nivel de ozono aumenta y puede llegar hasta 0,1 mg kg.

Las plantas pueden ser afectadas en su desarrollo por concentraciones pequeñas de ozono. El hombre también resulta afectado por el ozono a concentraciones entre 0,05 y 0,1 mg kg, causándole irritación de las fosas nasales y garganta, así como sequedad de las mucosas de las vías respiratorias superiores.


Algunas sustancia que se encuentran en la atmósfera tienen un origen natural, por lo que no son contaminantes en un sentido estricto:

Muchos estudios han demostrado enlaces entre la contaminación y los efectos para la salud.

Los aumentos en la contaminación del aire se han ligado a quebranto en la función pulmonar y aumentos en los ataques cardíacos. "Niveles altos de contaminación atmosférica según el Índice de Calidad del Aire de la Agencia de Protección Ambiental de los Estados Unidos (EPA, por sus siglas en inglés) perjudican directamente a personas que padecen asma y otros tipos de enfermedad pulmonar o cardíaca". La calidad general del aire ha mejorado en los últimos 20 años pero las zonas urbanas son aún motivo de preocupación. Los ancianos y los niños son especialmente vulnerables a los efectos de la contaminación del aire.

El nivel de riesgo depende de varios factores:

Otras maneras menos directas en que las personas están expuestas a los contaminantes del aire son:
Unos de los síntomas más comunes que se presentan en la salud humana a causa de la contaminación atmosférica son:

Los siguientes instrumentos son utilizados comúnmente como dispositivos de control de contaminación en la industria o en vehículos. Pueden transformar contaminantes o eliminarlos de una corriente de salida antes de ser emitidos a la atmósfera.

La gestión ambiental en el componente aire parte por realizar un modelamiento atmosférico del sector de estudio. Para ello se establecen estaciones de monitoreo de la calidad del Aire ubicando estaciones con representatividad poblacional EMRP, estas deben estar ubicadas dentro de un área urbana mínima de 2 km de diámetro para que sea representativa.

La red de monitoreo debe estar mínimamente sustentada por un equipo tripartito de Aseguramiento de la Calidad, una unidad de Control de Calidad y una unidad de distribución de la información.

El Aseguramiento de la Calidad tiene por misión soportar la unidad de monitoreo con recursos, la unidad de Control tiene por misión la trazabilidad, la calibración y el cruzamiento de resultados entre sus equipos y otros de referencia. Se debe detectar los corrimientos del valor cero, la saturación de los monitores, fuentes de emisión imprevistas no-comunes y focalizadas, cortes de energía eléctrica y aquellos valores escapados que induzcan a un mal pronóstico de Emergencia Ambiental.

La unidad informativa tiene por misión dar disponibilidad y análisis de la información confeccionando modelos informativos de contaminación del componente aire.

Para seleccionar los lugares más apropiados con los objetivos propuestos del monitoreo, es necesario manejar información que incluya, entre otros factores:
Estos puntos conducen a establecer modelos de contaminación atmosféricos y evaluación de la calidad del aire.

La Directiva 2001/81/ CE, del Parlamento Europeo y del Consejo, de 23 de octubre de 2001", sobre techos nacionales de emisión de determinados contaminantes atmosféricos, tiene como objeto limitar las emisiones de contaminantes para reforzar la protección del medio ambiente y de la salud humana y avanzar hacia el objetivo de no superar los niveles críticos de contaminantes y de proteger de forma eficaz a toda la población frente a los riesgos para la salud que se derivan de la contaminación atmosférica mediante la fijación de techos nacionales de emisión.

El programa "Aire puro para Europa" es una estrategia temática coherente de lucha contra la contaminación atmosférica y sus efectos. Este programa ha sido elaborado por el Sexto programa de Acción en Materia de Medio Ambiente recientemente aprobada por la Comisión (COM (2001) 31 de 24.01.2001). Esta estrategia consiste en evaluar la aplicación de las directivas relativas a la calidad del aire y la eficacia de los programas sobre calidad del aire en los Estados miembros. Además pretende mejorar el control de la calidad del aire y la divulgación de la información al público mediante la utilización de indicadores. Finalmente se establecerán prioridades para la adopción de nuevas medidas, examinando y actualizando los umbrales de calidad del aire y los límites máximos nacionales de emisión.

Recoge múltiples y variados objetivos con el fin de mejorar la calidad de vida de las poblaciones de Europa. Prevenir las enfermedades y proteger el medio que nos rodean serán algunos de los objetivos prioritarios que se desarrollarán a lo largo de la estrategia planteada. Sin embargo debemos también mencionar algunos objetivos más específicos que mejoraran la labor de análisis técnico, para mejorar así la política sobre la calidad del aire.

Como medida para instar al cumplimiento de los techos, la directiva obliga a los Estados miembros a elaborar unos programas nacionales de reducción progresiva de las emisiones. España ha elaborado mediante Acuerdo de Consejo de Ministros de 7 de diciembre el "II Programa Nacional de Reducción de Emisiones" (Resolución de 14 de enero de 2008, de la Secretaría General para la Prevención de la Contaminación y el Cambio Climático. BOE n.º 25, 29.01.08).
Establece las bases en materia de prevención, vigilancia y reducción de la contaminación atmosférica con el fin de evitar y cuando esto no sea posible, aminorar los daños que de ésta puedan derivarse para las personas, el medio ambiente y demás bienes de cualquier naturaleza.

El objetivo general de dicha ley es desarrollar una política estratégica integrada a largo plazo para proteger la salud humana y el medio ambiente de los efectos de la contaminación atmosférica. De acuerdo con el tratado, esta política tendrá por objetivo garantizar un elevado nivel de protección del medio ambiente sobre la base del principio de cautela, tomando los mejores datos científicos y técnicos disponibles y las ventajas y cargas que puedan resultar de la acción o de la falta de acción












</doc>

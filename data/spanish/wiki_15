<doc id="2613" url="https://es.wikipedia.org/wiki?curid=2613" title="Sistema hexadecimal">
Sistema hexadecimal

El sistema hexadecimal (abreviado como 'Hex', no confundir con "sistema sexagesimal") es el sistema de numeración posicional que tiene como base el 16. Su uso actual está muy vinculado a la informática y ciencias de la computación donde las operaciones de la CPU suelen usar el byte u octeto como unidad básica de memoria; y, debido a que un byte representa formula_1 valores posibles, y esto puede representarse como
formula_2
formula_3,
que equivale al número en base 16 formula_4, dos dígitos hexadecimales corresponden exactamente a un byte.

En principio, dado que el sistema usual de numeración es de base decimal y, por ello, sólo se dispone de diez dígitos, se adoptó la convención de usar las seis primeras letras del alfabeto latino para suplir los dígitos que nos faltan. El conjunto de símbolos es el siguiente:

Se debe notar que A = 10, B = 11, C = 12, D = 13, E = 14 y F = 15. En ocasiones se emplean letras minúsculas en lugar de mayúsculas. Como en cualquier sistema de numeración posicional, el valor numérico de cada dígito es alterado dependiendo de su posición en la cadena de dígitos, quedando multiplicado por una cierta potencia de la base del sistema, que en este caso es 16. Por ejemplo: 3E0A = 3×16 + E×16 + 0×16 + A×16 = 3×4096 + 14×256 + 0×16 + 10×1 = 15882.

El sistema hexadecimal actual fue introducido en el ámbito de la computación por primera vez por IBM en 1963. Una representación anterior, con 0-9 y u-z, fue usada en 1956 por la computadora Bendix G-15.

Como el único factor primo de 16 es 2, todas las fracciones que no tengan una potencia de 2 en el denominador tendrán un desarrollo hexadecimal periódico.

Existe un sistema para convertir números fraccionarios a hexadecimal de una forma más mecánica. Se trata de convertir la parte entera con el procedimiento habitual y convertir la parte decimal aplicando sucesivas multiplicaciones por 16 hasta convertir el resultado en un número entero. 

Por ejemplo: 0,06640625 en base decimal.

Multiplicado por 16: 1,0625, el primer decimal será 1. Volvemos a multiplicar por 16 la parte decimal del anterior resultado: 1. Por lo tanto el siguiente decimal será un 1.Resultado: 0,11 en base hexadecimal. Como el último resultado se trata de un entero, hemos acabado la conversión.

Hay ocasiones en las que no llegamos nunca a obtener un número entero, en ese caso tendremos un desarrollo hexadecimal periódico.

En el sistema hexadecimal, al igual que en el sistema decimal, binario y octal, se pueden hacer diversas operaciones matemáticas. Entre ellas se encuentra la resta entre dos números en sistema hexadecimal, la que se puede hacer con el método de complemento a 15 o también utilizando el complemento a 16. Además de éstas, debemos manejar adecuadamente la suma en sistema hexadecimal, explicada a continuación: 


"En este caso la respuesta obtenida, 16, no está entre el 0 y el 15, por lo que tenemos que restarle 16. Por lo tanto, la respuesta obtenida será 10 (sistema hexadecimal)".

"Hay que tener cuidado de utilizar correctamente las letras, ya que operar a la vez con letras y números puede crear confusiones".
"Ocurre lo mismo que en el ejemplo anterior".

"La respuesta es 20 y no está entre el 0 y el 15, por lo que tenemos que restarle 16. Por lo tanto, la respuesta obtenida será 
14 (sistema hexadecimal)".

"Hay que tener cuidado de utilizar correctamente las letras, ya que operar a la vez con letras y números puede crear confusiones".

"La respuesta es 29 y no está entre el 0 y el 15, por lo que tenemos que restarle 16. Por lo tanto, la respuesta obtenida será 1D (sistema hexadecimal)".

"Hay que tener cuidado de utilizar correctamente las letras, ya que operar a la vez con letras y números puede crear confusiones".



Ten en cuenta que puedes comprobar los resultados utilizando una calculadora científica.

Como podemos hacer la resta de dos números hexadecimales utilizando el complemento a 15. Para ello tendremos que sumar al minuendo el complemento a quince del sustraendo, y finalmente sumarle el bit de overflow (bit que se desborda).

Para entender la resta en complemento a 15 lo analizaremos con un ejemplo.
Ésta es la resta que tenemos que resolver: 

Primero tenemos que hacer que el minuendo y el sustraendo tengan la misma cantidad de números. 
Para ello, añadiremos ceros al sustraendo hasta que sean suficientes.
Después, crearemos un nuevo número con la misma cantidad de números que el nuevo sustraendo. 
Como en el sistema hexadecimal el mayor número que tenemos es el 15, que corresponde a la letra F, tendremos que escribir la F tantas veces como números tiene el sustraendo.
La resta se hace siguiendo las normas generales de la resta común. La diferencia obtenida se denomina el complemento a 15. Recuerda el valor correspondiente a cada letra al operar.

Ahora tendremos que sumar el minuendo y el complemento a 15 utilizando la suma en sistema hexadecimal, mencionada anteriormente.

Con la suma obtenemos el resultado 1A41E0, pero no es la respuesta final. 
Te habrás dado cuenta que este nuevo número tiene más cifras que los números iniciales que teníamos que restar. Tenemos que quitar el número de la izquierda (en este caso, el 1) y sumarlo.

La respuesta es A41E1.

Ten en cuenta que puedes comprobar los resultados utilizando una calculadora científica.

También podemos hacer la resta de dos números hexadecimales utilizando el complemento a 16, siguiendo un proceso similar que en el caso del complemento a 15. Para resolver la resta, tendremos que sumar al minuendo el complemento a dieciséis del sustraendo.

Para entender la resta en complemento a 16 lo analizaremos con el ejemplo anterior. Ésta es la resta que tenemos que resolver: 
Primero tenemos que hacer que el minuendo y el sustraendo tengan la misma cantidad de números, al igual que ocurre en el proceso del complemento a 15. 

Para ello, añadiremos ceros al sustraendo hasta que sean suficientes.
Después, crearemos un nuevo número con la misma cantidad de números que el nuevo sustraendo. 

Como en el sistema hexadecimal el mayor número que tenemos es el 15, que corresponde a la letra F, tendremos que escribir la F tantas veces como números tiene el sustraendo.
La resta se hace siguiendo las normas generales de la resta común.

Ahora tenemos que sumarle 1 a la diferencia obtenida. Este paso es muy importante, ya que es la diferencia entre hacer la resta en complemento a 15 ó 16, y se suele olvidar fácilmente. Además, recuerda que estás sumando en sistema hexadecimal, siguiendo el mismo proceso explicado anteriormente.

A la diferencia obtenida y sumarle uno le denominaremos el complemento a 16.

Ahora tendremos que sumar el minuendo y el complemento a 16

Con la suma obtenemos el resultado 1A41E1. 

Te habrás dado cuenta que este nuevo número tiene más cifras que los números iniciales que teníamos que restas, cosa imposible en una resta (que la diferencia sea mayor que el minuendo y el sustraendo). Por eso, y estando en complemento a 16, tendremos que despreciar (eliminar) el número de la izquierda. En este caso es el 1. 

La respuesta, por lo tanto, es A41E1.

En ambos casos la respuesta obtenida deberá ser la misma, ya que hemos resuelto la misma resta en sistema hexadecimal. Por lo tanto, podremos comprobar que hemos operado bien comparando las respuestas obtenidas en complemento a 15 y en complemento a 16 para una misma resta.

Además, ten en cuenta que puedes comprobar los resultados utilizando una calculadora científica.



</doc>
<doc id="2614" url="https://es.wikipedia.org/wiki?curid=2614" title="Sistema binario">
Sistema binario

El sistema binario, llamado también sistema diádico en ciencias de la computación, es un sistema de numeración en el que los números se representan utilizando solamente dos cifras: cero y uno ("0" y "1"). Es uno de los sistemas que se utilizan en las computadoras, debido a que estas trabajan internamente con dos niveles de voltaje, por lo cual su sistema de numeración natural es el sistema binario.

El antiguo matemático indio Pingala presentó la primera descripción que se conoce de un sistema de numeración binario en el siglo tercero antes de nuestra era, lo cual coincidió con su descubrimiento del concepto del número cero. 

Una serie completa de 8 trigramas y 64 hexagramas (análogos a 3 bits) y números binarios de 6 bits eran conocidos en la antigua China en el texto clásico del I Ching. Series similares de combinaciones binarias también han sido utilizadas en sistemas de adivinación tradicionales africanos, como el Ifá, así como en la geomancia medieval occidental.

Un arreglo binario ordenado de los hexagramas del I Ching, representando la secuencia decimal de 0 a 63, y un método para generar el mismo fue desarrollado por el erudito y filósofo Chino Shao Yong en el siglo XI.

En 1605 Francis Bacon habló de un sistema por el cual las letras del alfabeto podrían reducirse a secuencias de dígitos binarios, las cuales podrían ser codificadas como variaciones apenas visibles en la fuente de cualquier texto arbitrario.

En 1670 Juan Caramuel publica su libro "Mathesis Biceps;" en las páginas XLV a XLVIII se da una descripción del sistema binario.
El sistema binario moderno fue documentado en su totalidad por Leibniz, en el siglo XVII, en su artículo ""Explication de l'Arithmétique Binaire"". En él se mencionan los símbolos binarios usados por matemáticos chinos. Leibniz utilizó el 0 y el 1, al igual que el sistema de numeración binario actual.

En 1854, el matemático británico George Boole publicó un artículo que marcó un antes y un después, detallando un sistema de lógica que terminaría denominándose Álgebra de Boole. Dicho sistema desempeñaría un papel fundamental en el desarrollo del sistema binario actual, particularmente en el desarrollo de circuitos electrónicos.

En 1937, Claude Shannon realizó su tesis doctoral en el MIT, en la cual implementaba el Álgebra de Boole y aritmética binaria utilizando relés y conmutadores por primera vez en la historia. Titulada "Un Análisis Simbólico de Circuitos Conmutadores y Relés", la tesis de Shannon básicamente fundó el diseño práctico de circuitos digitales.

En noviembre de 1937, George Stibitz, trabajando por aquel entonces en los Laboratorios Bell, construyó una computadora basada en relés —a la cual apodó "Modelo K" (porque la construyó en una cocina, en inglés ""k"itchen")— que utilizaba la suma binaria para realizar los cálculos. Los Laboratorios Bell autorizaron un completo programa de investigación a finales de 1938, con Stibitz al mando.

El 8 de enero de 1940 terminaron el diseño de una "Calculadora de Números Complejos", la cual era capaz de realizar cálculos con números complejos. En una demostración en la conferencia de la Sociedad Estadounidense de Matemática, el 11 de septiembre de 1940, Stibitz logró enviar comandos de manera remota a la Calculadora de Números Complejos a través de la línea telefónica mediante un teletipo. Fue la primera máquina computadora utilizada de manera remota a través de la línea de teléfono. Algunos participantes de la conferencia que presenciaron la demostración fueron John von Neumann, John Mauchly y Norbert Wiener, quien escribió acerca de dicho suceso en sus diferentes tipos de memorias en la cual alcanzó diferentes logros.

En el sistema binario solo se necesitan dos cifras.

En informática, un número binario puede ser representado por cualquier secuencia de bits (dígitos binarios), que suelen representar cualquier mecanismo capaz de usar dos estados mutuamente excluyentes. Las siguientes secuencias de símbolos podrían ser interpretadas como el mismo valor numérico binario:

El valor numérico representado en cada caso depende del valor asignado a cada símbolo. En una computadora, los valores numéricos pueden representar dos voltajes diferentes; también pueden indicar polaridades magnéticas sobre un disco magnético. Un "positivo", "sí", o "sobre el estado" no es necesariamente el equivalente al valor numérico de uno; esto depende de la nomenclatura usada.

De acuerdo con la representación más habitual, que es usando números arábigos, los números binarios comúnmente son escritos usando los símbolos 0 y 1. Los números binarios se escriben a menudo con subíndices, prefijos o sufijos para indicar su base. Las notaciones siguientes son equivalentes:


Se divide el número del sistema decimal entre 2, cuyo resultado entero se vuelve a dividir entre 2, y así sucesivamente hasta que el dividendo sea menor que el divisor, 2. Es decir, cuando el número a dividir sea 1 finaliza la división.
A continuación se ordena desde el último cociente hasta el primer resto, simplemente se colocan en orden inverso a como aparecen en la división. Este será el número binario que buscamos. 


-> Ordenamos los residuos, del último al primero: 10000011
En sistema binario, 131 se escribe 10000011

Otra forma de conversión consiste en un método parecido a la factorización en números primos. Es relativamente fácil dividir cualquier número entre 2. Este método consiste también en divisiones sucesivas. Dependiendo de si el número es par o impar, colocaremos un cero o un uno en la columna de la derecha. Si es impar, le restaremos uno y seguiremos dividiendo entre dos, hasta que ya no sea posible y se coloca el número 1. Después solo nos queda tomar el último resultado de la columna izquierda y todos los de la columna de la derecha y ordenar los dígitos de abajo a arriba.


Ejemplo

Para convertir al sistema binario el número decimal 77 haremos una serie de divisiones que arrojarán los siguientes resultados:
Existe un último método denominado de distribución. Consiste en distribuir los unos necesarios entre las potencias sucesivas de 2 de modo que su suma resulte ser el número decimal a convertir. Sea por ejemplo el número 151, para el que se necesitarán las 8 primeras potencias de 2, ya que la siguiente, 2=256, es superior al número a convertir. Se comienza poniendo un 1 en 128, por lo que aún faltarán 23, 151-128 = 23, para llegar al 151. Este valor se conseguirá distribuyendo unos entre las potencias cuya suma dé el resultado buscado y poniendo ceros en el resto. En el ejemplo resultan ser las potencias 4, 2, 1 y 0, esto es, 16, 4, 2 y 1, respectivamente.

 2= 1|1

Para transformar un número del sistema decimal al sistema binario:
 0,3125 (decimal) => 0,0101 (binario).

 0,1 (decimal) => 0,0 0011 0011 ... (binario). 

 Convertir 0.2 (decimal) a binario. 
 5.5 = 5,5

 6,83 (decimal) => 110,110101000111 (binario).

Para realizar la conversión de binario a decimal, realice lo siguiente:

Ejemplos:

formula_3

formula_4

formula_5

También se puede optar por utilizar los valores que presenta cada posición del número binario a ser transformado, comenzando de derecha a izquierda, y sumando los valores de las posiciones que tienen un 1.

El número binario 1010010 corresponde en decimal al 82. Se puede representar de la siguiente manera:

formula_6

entonces se suman los números 64, 16 y 2:
formula_7

Para cambiar de binario con decimales a decimal se hace exactamente igual, salvo que la posición cero (en la que el dos es elevado a la cero) es la que está a la izquierda de la coma y se cuenta hacia la derecha a partir de -1:

formula_8

1. Inicie por el lado izquierdo (la primera cifra a la derecha de la coma), cada número deberá ser multiplicado por 2 elevado a la potencia consecutiva a la inversa (comenzando por la potencia -1, 2).

2. Después de realizar cada una de las multiplicaciones, sume todas y el número resultante será el equivalente al sistema decimal.


 1 * 2 elevado a -1 = 0,5

 1 * 2 elevado a -1 = 0,5

La tabla de sumar para números binarios es la siguiente:

Las posibles combinaciones al sumar dos bits son:


Note que al sumar 1 + 1 es 10, es decir, llevamos 1 a la siguiente posición de la izquierda (acarreo). Esto es equivalente en el sistema decimal a sumar 9 + 1, que da 10: cero en la posición que estamos sumando y un 1 de acarreo a la siguiente posición.

 1

Se puede convertir la operación binaria en una operación decimal, resolver la decimal, y después transformar el resultado en un (número) binario. Operamos como en el sistema decimal: comenzamos a sumar desde la derecha, en nuestro ejemplo, 1 + 1 = 10, entonces escribimos 0 en la fila del resultado y "llevamos" 1 (este "1" se llama "acarreo" o "arrastre"). A continuación se suma el acarreo a la siguiente columna: 1 + 0 + 0 = 1, y seguimos hasta terminar todas las columnas (exactamente como en decimal).

El algoritmo de la resta en sistema binario es el mismo que en el sistema decimal. Pero conviene repasar la operación de restar en decimal para comprender la operación binaria, que es más sencilla. Los términos que intervienen en la resta se llaman minuendo, sustraendo y diferencia.

Las restas básicas 0 - 0, 1 - 0 y 1 - 1 son evidentes:


La resta 0 - 1 se resuelve igual que en el sistema decimal, tomando una unidad prestada de la posición siguiente: 0 - 1 = 1 y "me llevo" 1 (este valor se resta al resultado que obtenga, entre el minuendo y el sustraendo de la siguiente columna), lo que equivale a decir en el sistema decimal, 2 - 1 = 1. 


En sistema decimal sería: 17 - 10 = 7 y 217 - 171 = 46.

Para simplificar las restas y reducir la posibilidad de cometer errores hay varios métodos:




La siguiente resta, 91 - 46 = 45, en binario es:

En el resultado nos sobra un bit, que se desborda por la izquierda. Pero, como el número resultante no puede ser más largo que el minuendo, el bit sobrante se desprecia.

Un último ejemplo: vamos a restar 219 - 23 = 196, directamente y utilizando el complemento a dos:

Y, despreciando el bit que se desborda por la izquierda, llegamos al resultado correcto: 11000100 en binario, 196 en decimal.


La tabla de multiplicar para números binarios es la siguiente:

El algoritmo del producto en binario es igual que en números decimales; aunque se lleva a cabo con más sencillez, ya que el 0 multiplicado por cualquier número da 0, y el 1 es el elemento neutro del producto.

Por ejemplo, multipliquemos 10110 por 1001:

En sistemas electrónicos, donde suelen usarse números mayores, se utiliza el método llamado algoritmo de Booth.

La división en binario es similar a la decimal; la única diferencia es que a la hora de hacer las restas, dentro de la división, estas deben ser realizadas en binario.


Dividir 100010010 (274) entre 1101 (13):

Debido a que el sistema octal tiene como base 8, que es la tercera potencia de 2, y que dos es la base del sistema binario, es posible establecer un método directo para convertir de la base dos a la base ocho, sin tener que convertir de binario a decimal y luego de decimal a octal. Este método se describe a continuación: 

Para realizar la conversión de binario a octal, realice lo siguiente:

1) Agrupe la cantidad binaria en grupos de 3 en 3 iniciando por el lado derecho. Si al terminar de agrupar no completa 3 dígitos, entonces agregue ceros a la izquierda.

2) Posteriormente vea el valor que corresponde de acuerdo a la tabla:

3) La cantidad correspondiente en octal se agrupa de izquierda a derecha.

 111 = 7

 111 = 7

 011 = 3

Si el número binario tiene parte decimal, se agrupa de tres en tres desde el punto decimal hacia la derecha siguiendo los mismos criterios establecidos anteriormente para números enteros. Por ejemplo:

0.01101 (binario) = 0.32 (octal) Proceso:
011 = 3
01 entonces agregue 010 = 2
Agrupe de izquierda a derecha: 32
Agregue la parte entera: 0.32

Cada dígito octal se convierte en su binario equivalente de 3 bits y se juntan en el mismo orden. 


Para realizar la conversión de binario a hexadecimal, realice lo siguiente:

1) Agrupe la cantidad binaria en grupos de 4 en 4 iniciando por el lado derecho. Si al terminar de agrupar no completa 4 dígitos, entonces agregue ceros a la izquierda.

2) Posteriormente vea el valor que corresponde de acuerdo a la tabla:
3) La cantidad correspondiente en hexadecimal se agrupa de derecha a izquierda.

 1010 = A

 0101 = 5

Note que para pasar de Hexadecimal a binario, se remplaza el número Hexadecimal por el equivalente de 4 bits, de forma similar a como se hace de octal a binario.




</doc>
<doc id="2618" url="https://es.wikipedia.org/wiki?curid=2618" title="Sexo (desambiguación)">
Sexo (desambiguación)

En español, la palabra sexo (del latín "sexus") tiene varios significados:






</doc>
<doc id="2619" url="https://es.wikipedia.org/wiki?curid=2619" title="Símbolo">
Símbolo

Un símbolo (del latín: "simbŏlum", y este del griego σύμβολον) es la representación perceptible de una idea, con rasgos asociados por una convención socialmente aceptada. Es un signo sin semejanza ni contigüidad, que solamente posee un vínculo convencional entre su significante y su denotado, además de una clase intencional para su designado.

Los grupos sociales suelen tener símbolos que los representan: existen símbolos referentes a diversas asociaciones culturales, artísticas, religiosas, políticas, comerciales, deportivas, entre otros.

Del latín "symbŏlum", y este del griego σύμβoλoν, el símbolo es la forma de exteriorizar un pensamiento o idea, así como el signo o medio de expresión al que se atribuye un significado convencional y en cuya génesis se encuentra la semejanza, real o imaginada, con lo significado. Aristóteles afirmaba que "no se piensa sin imágenes", y simbólica es la ciencia, constituyendo ambas las más evidentes manifestaciones de la inteligencia.

En las muchas etapas que componen la evolución, en la forma de comunicación humana, del desarrollo del lenguaje hablado a la escritura, los signos visuales representan la transición de la perspectiva visual, a través de las figuras y los pictogramas, a las señales abstractas. Sistemas de notación capaces de transmitir el significado de conceptos, palabras o sonidos simples.

Los signos y símbolos transmiten ideas en las culturas prealfabetizadas y prácticamente analfabetas. Pero su utilidad no es menor entre las verbalmente alfabetizadas: al contrario, es mayor. En la sociedad tecnológicamente desarrollada, con su exigencia de comprensión inmediata, los signos y símbolos son muy eficaces para producir una respuesta rápida. Su estricta atención a los elementos visuales principales y su simplicidad estructural, proporcionan facilidad de percepción y memoria.

Entre signos y símbolos hay diferencias:

Los símbolos pueden componerse de información realista, extraída del entorno, fácil de reconocer, o también por formas, tonos, colores, texturas..., elementos visuales básicos que no guardan similitud con los objetos del entorno natural. No poseen ningún significado, excepto el que se les asigna. Existen muchas formas de clasificar los símbolos; pueden ser simples o complejos, obvios u oscuros, eficaces o inútiles. Su valor se puede determinar hasta donde penetra la mente en términos de reconocimiento y recuerdo.
El interés por los signos ha dado lugar a un importante campo de estudio: la semiótica. Ésta trata tanto la función de los signos en el proceso de comunicación, como el lugar de los síntomas en el diagnóstico médico.
En la comunicación, los signos y señales aparecen, en general, en estructuras similarmente ilógicas. A veces requieren un planteamiento intuitivo que extraiga su sentido y que, por consiguiente, los haga susceptibles de interpretación creativa. Intuición, inspiración, resolución creativa de problemas..., como quiera que lo denominemos esta actividad no posee ninguna lógica, ningún patrón previsible. De la organización de signos inconexos surge la liberación de la lógica hacia el salto de la interpretación. Lo podemos llamar inspiración, pero es una forma particular de inteligencia. Es la aptitud esencial de cualquiera que debe organizar información diversa y extraer un sentido de ésta.

En el ámbito científico y técnico, también se denomina símbolo a las abreviaciones constituidas mediante grafías o letras. Difieren de las abreviaturas por carecer de punto. Tal es el caso de los símbolos químicos (ej. C, O, H0, CH),

Los símbolos nacionales son aquellos que un país adopta para representar sus valores, metas, historia o riquezas y mediante los cuales se identifica y distingue de los demás, además de aglutinar en torno a ellos a sus ciudadanos y crear un sentimiento de pertenencia. Los símbolos nacionales por excelencia son la bandera y los colores nacionales, el escudo de armas y el himno. A ellos se añaden en ocasiones otros emblemas como puede ser una planta, animal u objeto asociado íntimamente con el país. Su tipología difiere en cada cultura constituyendo un interesante campo de estudio antropológico, pues aporta abundante información sobre las ideas, conceptos y valores más significativos de cada sociedad y época.

En las sociedades primitivas, los símbolos sirvieron para expresar las cualidades esenciales de sus creencias religiosas. A lo largo de la historia, la religión ha estado ligada a una serie de símbolos significativos.

En el Antiguo Egipto se practicó esta costumbre, así, simbólica es su escritura jeroglífica, su mitología, donde cada una de las divinidades representa un aspecto cultural, y aún sus manifestaciones artísticas. Igualmente en las formas exteriores de las religiones semíticas como la asiria y fenicia, en la hindú y en las indoeuropeas, como la greco-latina, impera el símbolo, pues en ellas se utilizó la representación de los fenómenos de la naturaleza, personificados en seres mitológicos, que terminaron por encarnar los valores morales de la sociedad.

Los judíos y los musulmanes prohíben las imágenes como símbolos de adoración. En lugar de ello, subrayan la palabra y la necesidad de una cultura escrita para la participación de la oración.

Muchas representaciones de ideas abstractas mediante símbolos son de origen oriental.

Por San Clemente de Alejandría sabemos que los símbolos, que adornaban las catacumbas y que posteriormente se vieron reproducidos en la pintura y la escultura, ya eran utilizados por los cristianos en el siglo II, comúnmente adornando anillos, medallas etcétera; con el propósito de reconocerse entre sí obligados al secreto que la persecución imponía a los primeros cristianos. Entre otros se empleaban símbolos de unión o reunión, como los peces de bronce o cristal encontrados en las catacumbas de Roma, que se entregaban a los bautizados para que los llevaran colgados del cuello. También era costumbre que los viajeros que habían recibido hospitalidad en una casa, rompieran un símbolo del que dejaban la mitad de modo que si volvían a visitarse, incluso sus descendientes, pudiera recordarse la hospitalidad; tal es el uso que debían tener muchas monedas partidas que con frecuencia suelen encontrarse.

Al margen de estos símbolos convencionales, tuvieron otros a los que la Iglesia dio mucha importancia, siendo el principal el símbolo de los Apóstoles, que pretendía proporcionar una sucinta guía al cristiano sobre las verdades reveladas, y para que los fieles pudieran mostrar una contraseña propia que los distinguiera de los herejes; de este modo si por cualquier causa cambiaban de congregación podían ser reconocidos como cristianos ortodoxos si evocaban el símbolo. La iglesia primitiva prohibía entregarlo por escrito para evitar que cayera en manos de los infieles, de modo que los creyentes debían aprenderlo de memoria.

El arte figurativo adoptó estos símbolos para representar, en ocasiones desprovistos ya de carácter religioso o mitológico, atributos o cualidades e incluso determinadas manifestaciones de la actividad humana, a los que fue añadiendo otros cuando fue necesario, si bien al principio deudores de las manifestaciones religiosas anteriores que constituían el patrimonio cultural común.

Indagar sobre la definición de símbolo desde la perspectiva hermenéutica que el filósofo alemán Hans Georg Gadamer plantea en un apartado de su libro "La Actualidad de lo Bello" es establecer un diálogo con la etimología de la palabra y plantear relaciones con algunas de las vivencias griegas; es adentrarse al estudio semántico que pone en evidencia la influencia del filósofo Heidegger en su obra y reconocer los distanciamientos que hace al momento de interlocutar con los planteamientos del filósofo Hegel, cuando este define lo bello en el arte. Es así como Gadamer plantea que la esencia de lo simbóloco es el "autosignificado".

Gadamer, al hacer una revisión etimológica de lo que quiere decir Símbolo, llega a la antigua tradición de la tablilla y la relación entre el anfitrión y el huésped, pues cada uno conservaba parte de la tablilla y al momento de unirlas, los poseedores se reconocían como antiguos conocidos. Lo anterior representa el significado que símbolo tiene desde la lengua griega como "tablilla de recuerdo". Este elemento es de gran importancia al momento de plantear lo relacionado a la experiencia de lo simbólico, pues "este individual-particular se representa como un fragmento del ser que promete complementar en un todo íntegro al que se corresponda en él. —Hans Georg Gadamer.
En este orden de ideas, Gadamer plantea que el otro fragmento existente, que siempre es buscado, logrará la completud total en lo propio, en el fragmento vital que se posee. Es así como la "experiencia de lo bello es la evocación de un orden íntegro posible" —Hans Georg Gadamer.
Con esta noción planteada, se hace la afirmación en la que se reconoce la obra de arte desde el mismo mensaje de integridad, para luego conceptualizar lo que constituye la significatividad de lo bello y del arte. De ahí, plantea que lo que se experimenta de un encuentro con el arte no es lo particular, más bien es la totalidad del mundo experimentable la que tiene lugar. Sin embargo hace la aclaración que esto no quiere decir que "la expectativa indeterminada de sentido que hace que la obra de arte tenga un significado para nosotros pueda consumarse plenamente de su sentido total". —Hans Georg Gadamer.

Es en este punto que retoma al filósofo Hegel, quien plantea lo bello en el arte como la apariencia sensible de la idea, ésta se hace verdaderamente presente en la manifestación sensible de lo bello. Gadamer se distancia de lo anterior denominándolo como una seducción idealista, pues manifiesta que lo propuesto por Hegel "no hace justicia a la auténtica circunstancia de que la obra nos habla como obra no como portadora de un mensaje" —Hans Georg Gadamer. Por consiguiente, la idea de lo simbólico reposa sobre un juego de contrarios de demostración y ocultación. De ahí que la obra no se reduzca a la simplicidad de mero portador de sentido, pues el sentido de la obra radica en que la obra misma está ahí. Esto evidencia que la seducción idealista no toma en cuenta el juego que involucra la demostración y la ocultación, que posibilita que lo universal ocupe un lugar en lo particular sin que necesariamente éste tenga que pronunciarse como universal. Es así como lo simbólico no remite al significado sino que representa el significado mismo.

Además de lo anteriormente planteado por Gadamer, este emplea el concepto de conformación por el de obra, manifiesta que la conformación "no es nada de lo que se pueda pensar que alguien lo ha hecho deliberadamente" —Hans Georg Gadamer. Este concepto le permite reforzar lo ya mencionado, en la dirección que le da a la conformación, pues ésta se encuentra y existe así "ahí", susceptible de ser hallada por cualquiera que se encuentre con ella.

Es importante recordar la afirmación que Gadamer realiza al plantear que no es una mera revelación de sentido lo que se lleva a cabo en el arte, y es aquí donde retoma uno de los aportes del filósofo Heidegger cuando este le da al pensamiento la posibilidad de sustraerse al concepto idealista de sentido y de percibir la plenitud ontológica a la verdad que nos habla desde el arte en el doble movimiento de descubrir-desocultar, ocultamiento-retiro.

Paralelo a esto Paul Ricoeur en su texto "Freud: una interpretación de la cultura" —Ricoeur, P. introduce el estudio del símbolo a partir de la voz alemana "traumdeutung" compuesta por dos elementos: "el sueño y la interpretación". Al esbozar, inicialmente, piezas generales sobre el sueño se observa que sobre este recae la interpretación, pues al ser una palabra que se abre a productos psíquicos requiere ser revelada, y para ello se precisa del psicoanálisis."El sueño se inscribe así en una región del lenguaje que se anuncia como lugar de significaciones complejas, donde otro sentido se da y se oculta a la vez en un sentido inmediato" —Ricoeur, P. En esta línea, lo que en Gadamer se entiende como un juego de contrarios, de demostración y ocultación, obedece en Ricoeur a la doble región de sentido en la cual se instala el símbolo.

Al ser de doble sentido, el símbolo requiere de una interpretación que se relega al campo hermenéutico, la hermenéutica es conceptualizada por Ricoeur como "la teoría de las reglas que presiden una exégesis, es decir, la interpretación de un texto singular o de un conjunto de signos susceptible de ser considerado como un texto" —Ricoeur, P.; es por medio de la interpretación que el símbolo se inscribe en la filosofía del lenguaje, este último debe tenerse en cuenta como elemento fundante de los planteamientos filosóficos de Ricoeur para la interpretación del símbolo, lo cual se verá más adelante.

El trabajo que realiza Ricoeur descansa en la búsqueda del "criterio semántico en la estructura intencional de doble sentido" —Ricoeur, P. que tiene el símbolo, y en la necesidad de tener en cuenta esa estructura como el objeto de estudio de su investigación; dicho trabajo ha demandado observar el símbolo a partir de dos definiciones: una 'amplia' en la que la función simbólica es estudiada a partir de los planteamientos de Ernst Cassirer, gracias a los cuales Ricoeur hace una distinción entre símbolo y signo, a esta definición amplia se añaden tres 'zonas de emergencia': la fenomenología de la religión, lo onírico y la imaginación poética. La segunda definición es la 'estrecha' en la que el símbolo es visto a partir del nexo de sentido a sentido que provee la analogía.

En el trabajo de Ricoeur se precisan diversos elementos que permiten limitar los campos de acción del símbolo y de la interpretación, uno de esos elementos, de carácter fundamental, consiste en una definición concreta del símbolo, éste como se dijo antes, se diferencia de lo que propone Cassirer, que correspondería, según Ricoeur más a signo, por su sentido unívoco, que al símbolo, que es de carácter doble o múltiple. En este orden de ideas el símbolo en Ricoeur es una expresión de doble o múltiple sentido que requiere un trabajo de interpretación que haga explícitos los múltiples significados que lo componen.

Respecto a las tres 'zonas de emergencia' hay dos que denotan una significación especial, las que tiene que ver con la imaginación poética y fenomenología de la religión, en esta última se anuncia un componente esencial en la investigación de Ricoeur: el lenguaje. El símbolo en la fenomenología de la religión está ligado a los ritos y a los mitos que constituyen el lenguaje de lo sagrado, los símbolos no se presentan como valores de expresión inmediata sino que están inscritos en el universo del discurso donde adquieren realidad simbólica, es entonces, por medio del lenguaje, y concretamente de la palabra, que la expresividad cósmica de la fenomenología de la religión se puede expresar. Así mismo en la imaginación poética, que comprende la importancia de la imagen como vehículo o pretexto para dar fuerza verbal a la expresión, se imponen el lenguaje y palabra como medios para poder decir al símbolo. En este sentido, entendemos que es por medio del lenguaje que el símbolo puede hacerse "real", entendiendo posibilidad de realización no realidad material, sino realidad expresiva.






</doc>
<doc id="2620" url="https://es.wikipedia.org/wiki?curid=2620" title="Servidor">
Servidor

Un servidor es una aplicación en ejecución ("software") capaz de atender las peticiones de un cliente y devolverle una respuesta en concordancia. Los servidores se pueden ejecutar en cualquier tipo de computadora, incluso en computadoras dedicadas a las cuales se les conoce individualmente como «el servidor». En la mayoría de los casos una misma computadora puede proveer múltiples servicios y tener varios servidores en funcionamiento. La ventaja de montar un servidor en computadoras dedicadas es la seguridad. Por esta razón la mayoría de los servidores son procesos diseñados de forma que puedan funcionar en computadoras de propósito específico.

Los servidores operan a través de una arquitectura cliente-servidor. Los servidores son programas de computadora en ejecución que atienden las peticiones de otros programas, los clientes. Por tanto, el servidor realiza otras tareas para beneficio de los clientes. Ofrece a los clientes la posibilidad de compartir datos, información y recursos de hardware y software. Los clientes usualmente se conectan al servidor a través de la red pero también pueden acceder a él a través de la computadora donde está funcionando. En el contexto de redes Internet Protocol (IP), un servidor es un programa que opera como oyente de un socket.

Comúnmente los servidores proveen servicios esenciales dentro de una red, ya sea para usuarios privados dentro de una organización o compañía, o para usuarios públicos a través de Internet. Los tipos de servidores más comunes son 
servidor de base de datos, 
servidor de archivos, 
servidor de correo, servidor de impresión, servidor web, servidor de juego, y servidor de aplicaciones.

Un gran número de sistemas usa el modelo de red cliente-servidor, entre ellos los sitios web y los servicios de correo. Un modelo alternativo, el modelo red peer-to-peer permite a todas las computadoras conectadas actuar como clientes o servidores acorde a las necesidades.

El término "servidor" es ampliamente utilizado en el campo de las tecnologías de la información. A pesar de la amplia disponibilidad de productos etiquetados como productos de servidores (tales como versiones de hardware, software y OS diseñadas para servidores), en teoría, cualquier proceso computacional que comparta un recurso con uno o más procesos clientes es un servidor. Tomemos como ejemplo la acción de compartir ficheros. Mientras la existencia de ficheros dentro de una computadora no la clasifica como un servidor, el mecanismo del sistema operativo que comparte estos ficheros a los clientes sí es un servidor.

De manera similar consideremos una aplicación web servidor (como por ejemplo el servidor multiplataforma "Apache"). Este servidor web puede ejecutarse en cualquier tipo de computadora que cumpla con los requerimientos mínimos. Por ejemplo, mientras un ordenador portátil (laptop) o computadora personal usualmente no son consideradas como servidores, en ciertos casos (como el anterior) pueden cumplir el rol de uno y por lo tanto ser denominadas servidores. En este caso es el rol de la computadora el que la coloca en la categoría de servidor.

En el sentido del "hardware", la palabra "servidor" normalmente etiqueta modelos de computadora diseñados para hospedar un conjunto de aplicaciones que tiene gran demanda dentro de una red. En esta configuración cliente-servidor, uno o más equipos, lo mismo una computadora que una aplicación informática, comparten información entre ellos de forma que uno actúa como "host" de los otros.

Casi todas las computadoras personales pueden actuar como un servidor, pero un servidor dedicado tendrá cualidades más adecuadas para un ambiente de producción. Entre estas cualidades se pueden mencionar CPU más rápidas, RAM mejoradas para alto desempeño, y mayores capacidades de almacenamiento en forma de múltiples discos duros. Los servidores también cuentan con otras cualidades como confiabilidad, disponibilidad y utilidad (RAS) y tolerancia a fallos, esta última en forma de redundancia en el número de fuentes, almacenamiento (RAID), y conexiones de red.

Los servidores se volvieron comunes a principios de 1990 en la medida en que los negocios comenzaron a utilizar computadoras personales para brindar servicios que anteriormente se alojaban en "mainframes" o en microcomputadoras. Los primeros servidores de archivos contaban con múltiples torres de CD, utilizados para alojar grandes aplicaciones de bases de datos.

Entre 1990 y el 2000 el aumento en el uso de "hardware específico" marcó el advenimiento aplicaciones de servidor autosuficientes. Uno de estas aplicaciones bien conocidas es el Google Search Appliance, que combina hardware y software en un paquete out-of-the-box packaging. Productos similares fueron el Cobalt Qube y el RaQ. Ejemplos más sencillos de dichos equipos incluyen switches, routers, gateways, y servidores de impresión, los cuales son fácilmente utilizables a través de una configuración plug-and-play.

Los sistemas operativos modernos como Microsoft Windows o las distribuciones de Linux parecen haber sido diseñados siguiendo una arquitectura cliente-servidor. Estos sistemas operativos se abstraen del hardware, permitiendo a una gran variedad de software trabajar con componentes de la computadora. De alguna forma, el sistema operativo puede ser visto como un "servidor" de hardware al software pues, excepto en los lenguajes de programación de bajo nivel, el software debe interactuar con el hardware a través de un API.

Estos sistemas operativos son capaces de ejecutar programas en un segundo plano los cuales son llamados servicios o daemons. Estos programas, entre los que se encuentra el "Servidor HTTP Apache" previamente mencionado, pueden permanecer en un estado dormido hasta que sea necesario su uso. Como cualquier software que "brinde" servicios puede ser llamado servidor, las computadoras personales modernas se pueden ver como bosques de aplicaciones clientes y servidores operando en paralelo.

El propio Internet es un bosque de servidores y clientes. Sólo con el hecho de solicitar una página web de un servidor a pocos kilómetros de distancia conlleva a satisfacer una pila de protocolos de red que incluyen varios ejemplos del uso de hardware y software para servidores. Los más sencillos de éstos son los routers, módems, servidores DNS, además de otros sin cuya interacción no podríamos acceder a la web.

La aparición de la computación en la nube permite servidores de almacenamiento, así como compartir recursos con un fondo común; igualmente permite a los servidores mantener un mayor grado de tolerancia a los fallos.

Los requerimientos de "hardware" para los servidores varían en dependencia del tipo de aplicación del servidor. La velocidad de la CPU no es tan crítica para un servidor como lo sería para una máquina de escritorio. El deber de los servidores de proveer servicios dentro de una red a un gran número de usuarios impone diferentes requerimientos, tales como conexiones de alta velocidad y altas prestaciones para todos los dispositivos de I/O. Como generalmente se accede a los servidores a través de la red, estos pueden funcionar sin necesidad de un monitor u otros dispositivos de entrada. Aquellos procesos que no son necesarios para las funciones del servidor no se utilizan. Muchos servidores no cuentan con una interfaz gráfica de usuario (GUI) ya que esta funcionalidad consume recursos que pueden ser utilizados por otros procesos. Igualmente las interfaces de audio y USB también pueden ser omitidas.

Los servidores funcionan por largos períodos de tiempo sin interrupción y su disponibilidad debe ser alta la mayor parte del tiempo, haciendo que la confiabilidad y durabilidad del hardware sean extremadamente importantes. Aunque los servidores pueden ser ensamblados a partir de piezas para computadoras comunes, aquellos servidores que realizan tareas críticas dentro de la infraestructura de una empresa son idealmente muy tolerantes a fallas y utilizan hardware especializado con tasa de fallo para maximizar su tiempo de funcionamiento, pues una simple falla de poco tiempo de duración puede representar costos mayores a los de comprar las piezas e instalar todo el sistema. Por ejemplo, una falla de pocos minutos en una bolsa de acciones basta para justificar los gastos de sustitución de todo el sistema por otro más confiable. Los servidores pueden incluir discos de mayor capacidad y velocidad, sistemas de enfriamiento por agua, mayores disipadores para reducir el calor, abastecimientos de energía ininterrumpido que garantice el funcionamiento del servidor ante una falla del suministro eléctrico. Estos componentes ofrecen un mayor desempeño y confiabilidad en correspondencia a un mayor precio. La redundancia de hardware —instalar más de una instancia de un módulo como la fuente o el disco duro dispuestos de forma tal que si uno falla el otro se encuentre automáticamente disponible— es ampliamente utilizada. Se utilizan dispositivos de memoria ECC que detectan y corrigen errores; otros tipos de memoria que no son ECC pueden conllevar a una corrupción de los datos.

Para aumentar la confiabilidad la mayoría de los servidores utilizan memoria para detección y corrección de errores, discos redundantes, fuentes redundantes y más. Es común que estos componentes pueden ser sustituidos en caliente, permitiendo que los técnicos puedan cambiar piezas defectuosas en un servidor sin la necesidad de tener que apagarlo. Los servidores cuentan usualmente con mejores disipadores para prevenir un sobrecalentamiento. Como en la mayoría de los casos los servidores son administrados por administradores de sistema calificados, el sistema operativo con que cuentan está más enfocado en la estabilidad y el desempeño que en parecer acogedor y fácil de usar, siendo Linux el que mayor por ciento de uso toma.

Como la mayoría de los servidores son ruidosos y necesitan de estabilidad en el suministro eléctrico, buen acceso a Internet, y mayor seguridad, es común almacenarlos en centros de servidores. Como los servidores se agrupan siempre se busca reducir el consumo energético, pues la energía extra utilizada produce un aumento de la temperatura en la habitación lo que provocando que se excedan los límites de temperatura aceptables; por ello la mayoría de las habitaciones para servidores cuentan con equipos de aire acondicionado. La cubierta de la mayoría de los servidores tiende a ser plana y ancha (usualmente medida en "unidades rack"), adaptada para almacenar varios dispositivos juntos en un soporte para servidores. A diferencia de las computadoras ordinarias los servidores pueden ser configurados, encendidos, apagados o reiniciados remotamente usando administración remota, usualmente basada en IPMI.

Muchos servidores se demoran en arrancar el hardware e inicializar el sistema operativo. Es frecuente que los servidores realicen extensas pruebas de memoria antes de inicializar además la inicialización y verificación de servicios de administración remotos. Los controladores de discos duros inician los dispositivos secuencialmente, en vez de todos a la vez, para no sobrecargar la fuente de alimentación con la carga de arranque, y luego inician el chequeo del sistema RAID para probar que las operaciones redundantes funcionen de forma correcta. Es común que un servidor tome varios minutos para inicializarse pero puede que no sea necesario reiniciarlo en meses o años.

Los sistemas operativos orientados a servidores cuentan con ciertas cualidades que los hacen más adecuados para el entorno de un servidor, como

En muchos casos, los sistemas operativos orientados a servidores pueden interactuar con sensores de hardware para detectar estados como sobrecalentamiento, fallos de discos o del procesador, y en consecuencia alertar a su operador o tomar medidas de rectificación por sí mismo.

Como los servidores deben proveer un conjunto limitado de servicios a múltiples usuarios mientras que una computadora personal debe soportar una amplia variedad de funcionalidades requeridas por su usuario, los requerimientos de un sistema operativo para un servidor son diferentes de aquellos en una computadora de escritorio. Aunque es posible que un sistema operativo haga que una computadora provea servicios y responda rápidamente a los requerimientos de un usuario, es común el uso de diferentes sistemas operativos en servidores y computadoras de personal. Algunos sistemas operativos vienen en sus versiones personales (desktop) y servidores (server) con interfaces de usuario similares.

Los sistemas operativos para servidores de Windows y Mac OS X son usados en una minoría de los servidores, ya que también existen otros sistemas operativos de pagos para mainframes como z/OS. Los sistemas operativos predominantes en servidores son aquellos que siguen distribuciones de software open source de UNIX , como los basados en Linux y FreeBSD.
El ascenso de los servidores basados en microprocesadores se facilitó a partir del desarrollo de UNIX para ejecutarse sobre la arquitectura de microprocesador x86. La familia de sistemas operativos de Microsoft Windows también puede ejecutarse sobre el hardware x86 y desde Windows NT, está disponible para versiones adecuadas para uso en servidores.

Mientras que el rol de los sistemas operativos para servidores y para computadoras personales permanece diferente, las mejoras en la confiabilidad tanto del hardware como del sistema operativo han hecho borrosa la distinción entre estas dos clases. Hoy en día muchos sistemas operativos para computadoras personales y para servidores comparten las mismas bases en su código, difiriendo mayormente en su configuración. El cambio hacia las aplicaciones web y las plataformas middleware también han enseñado la demanda de servidores especializados para aplicaciones.

En la siguiente lista hay algunos tipos comunes de servidores:












Sin embargo, de acuerdo al rol que asumen dentro de una red se dividen en:


En 2010, los data centers (servidores, enfriamiento, y resto de infraestructura eléctrica), consumieron del 1.1 al 1.5% de la energía eléctrica en el mundo y del 1.7 al 2.2% en los Estados Unidos.

Concretamente, este consumo es menor que el de 6 mil millones de teléfonos móviles que hay en el mundo cuando van a recargar sus baterías. Incluso este consumo puede parecer despreciable, sobre la base de las tasas de consumo de la calefacción, el enfriamiento y el calentamiento de agua domésticos, que asciende a los dos dígitos. Finalmente, el informe Smart2020, estima que ICT (Information and Communications Technology) ahorra más de 5 veces su huella de carbono.t que el resto de la economía por aumento de la eficiencia.

Las clases de tamaño incluyen:



</doc>
<doc id="2621" url="https://es.wikipedia.org/wiki?curid=2621" title="Spot">
Spot

La palabra spot puede referirse a:








</doc>
<doc id="2623" url="https://es.wikipedia.org/wiki?curid=2623" title="Secure Shell">
Secure Shell

SSH (Secure SHell, en español: intérprete de órdenes seguro) es el nombre de un protocolo y del programa que lo implementa, y sirve para acceder servidores privados a través de una puerta trasera (también llamada "backdoor"). Permite manejar por completo el servidor mediante un intérprete de comandos, y también puede redirigir el tráfico de X (Sistema de Ventanas X) para poder ejecutar programas gráficos si tenemos ejecutando un Servidor X (en sistemas Unix y Windows). Se le asignó el puerto TCP 22. 

Además de la conexión a otros dispositivos, SSH nos permite copiar datos de forma segura (tanto archivos sueltos como simular sesiones FTP cifradas), gestionar claves RSA para no escribir claves al conectar a los dispositivos y pasar los datos de cualquier otra aplicación por un canal seguro tunelizado mediante SSH.

SSH trabaja de forma similar a como se hace con telnet. La diferencia principal es que SSH usa técnicas de cifrado que hacen que la información que viaja por el medio de comunicación vaya de manera no legible, evitando que terceras personas puedan descubrir el usuario y contraseña de la conexión ni lo que se escribe durante toda la sesión; aunque es posible atacar este tipo de sistemas por medio de ataques de REPLAY y manipular así la información entre destinos.

Al principio sólo existían los r-commands, que eran los basados en el programa rlogin, el cual funciona de una forma similar a telnet.

La primera versión del protocolo y el programa eran libres y los creó un finlandés llamado Tatu Ylönen, pero su licencia fue cambiando y terminó apareciendo la compañía SSH Communications Security, que lo ofrecía gratuitamente para uso doméstico y académico, pero exigía el pago a otras empresas. En el año 1997 (dos años después de que se creara la primera versión) se propuso como borrador en la IETF.

A principios de 1999 se empezó a escribir una versión que se convertiría en la implementación libre por excelencia, la de OpenBSD, llamada OpenSSH.

Existen 2 versiones de SSH, la versión 1 de SSH hace uso de muchos algoritmos de cifrado patentados (sin embargo, algunas de estas patentes han expirado) y es vulnerable a un agujero de seguridad que potencialmente permite a un intruso insertar datos en la corriente de comunicación. La suite OpenSSH bajo Red Hat Enterprise Linux utiliza por defecto la versión 2 de SSH, la cual tiene un algoritmo de intercambio de claves mejorado que no es vulnerable al agujero de seguridad en la versión 1. Sin embargo, la suite OpenSSH también soporta las conexiones de la versión 1.




</doc>
<doc id="2625" url="https://es.wikipedia.org/wiki?curid=2625" title="Sucrea">
Sucrea

Sucrea, es un género de plantas herbáceas perteneciente a la familia de las poáceas. Es originario de Brasil.
El nombre del género fue otorgado en honor de Dimitri Sucre.



</doc>
<doc id="2626" url="https://es.wikipedia.org/wiki?curid=2626" title="Streptostachys">
Streptostachys

Streptostachys, es un género de plantas herbáceas de la familia de las gramíneas o poáceas. Es originario del norte de Sudamérica. Comprende 8 especies descritas y de estas, solo 6 aceptadas.

El género fue descrito por Nicaise Augustin Desvaux y publicado en "Nouveau Bulletin des Sciences, publié par la Société Philomatique de Paris" 2: 190. 1810. La especie tipo es: "Streptostachys asperifolia"

A continuación se brinda un listado de las especies del género "Streptostachys" aceptadas hasta noviembre de 2014, ordenadas alfabéticamente. Para cada una se indica el nombre binomial seguido del autor, abreviado según las convenciones y usos.





</doc>
<doc id="2628" url="https://es.wikipedia.org/wiki?curid=2628" title="Streptochaeta">
Streptochaeta

Streptochaeta, es un género de plantas herbáceas perteneciente a la familia de las poáceas. Es originario de América desde México hasta Argentina. Es el único miembro de la tribu Streptochaeteae. Comprende 5 especies descritas y de estas, solo 4 aceptadas. 
Son hierbas erectas perennes emergiendo de coronas nodosas; tallos en su mayoría simples; plantas hermafroditas. Lígula ausente; pseudopecíolo anchamente sulcado, terminando en un corto pulvínulo; láminas aplanadas, ovadas, asimétricas, teseladas. Inflorescencia una espiga solitaria terminal de pseudoespiguillas dispuestas en espiral en el delgado raquis angular, desarticulándose de éste en grupo y por lo general pendiendo por algún tiempo de la punta del mismo, enredadas en las aristas ensortijadas y retorcidas; pseudoespiguillas teretes, sésiles, usualmente con 11 brácteas rígidas espiralmente imbricadas; brácteas 1–5 mucho más cortas que el resto, dispuestas en espiral, bráctea 6 la más larga, rematada en una arista alargada, retorcida, enrollada en espiral, brácteas 7 y 8 lado con lado, brácteas 9–11 verticiladas, formando un cono alrededor de la flor bisexual; lodículas ausentes; estambres 6, unidos en la base de los filamentos; ovario fusiforme; estilos 3. Fruto una cariopsis.
El género fue descrito por Heinrich Adolph Schrader y publicado en " Flora Brasiliensis seu Enumeratio Plantarum" 2(1): 536. 1829. La especie tipo es: "Streptochaeta spicata" Schrad. ex Nees
Streptochaeta: nombre genérico que deriva del [[griego antiguo|griego: "strepto" = "retorcido" y "chaeta" = "pelos largos".
El [[cromosoma|número cromosómico]] básico es x = 11, con números cromosómicos somáticos de 2n = 22. [[diploide]]
A continuación se brinda un listado de las [[especie]]s del género "Streptochaeta" aceptadas hasta noviembre de [[2013]], ordenadas alfabéticamente. Para cada una se indica el [[nombre binomial]] seguido del [[autor de nombre científico|autor]], abreviado según las convenciones y usos.

[[Categoría:Streptochaeta]]

</doc>
<doc id="2630" url="https://es.wikipedia.org/wiki?curid=2630" title="Stiporyzopsis">
Stiporyzopsis

Stiporyzopsis, es un género monotípico híbrido entre los géneros de la familia de las poáceas ("Oryzopsis × Stipa").





</doc>
<doc id="2631" url="https://es.wikipedia.org/wiki?curid=2631" title="Sistema digital">
Sistema digital

Un sistema digital es un conjunto de dispositivos destinados a la generación, transmisión, manejo, procesamiento o almacenamiento de señales digitales. También, y a diferencia de un sistema analógico, un sistema digital es una combinación de dispositivos diseñados para manipular cantidades físicas o información que estén representadas en forma digital; es decir, que solo puedan tomar valores discretos. 

Para el análisis y la síntesis de sistemas digitales binarios se utiliza como herramienta el álgebra de Boole.



Para la implementación de los circuitos digitales, se utilizan puertas lógicas (AND, OR y NOT), construidas generalmente a partir de transistores. Estas puertas siguen el comportamiento de algunas funciones booleanas.

Según el propósito de los sistemas digitales, se clasifican en:



Z = F(X)

Z= valor señales de las salidas;
X= valor señales de las entradas;
F= circuito transformador de señales (compuertas electrónicas)


Z = F(X,Q)

Z= valor señales de las salidas;
X= valor señales de las entradas;
Q= elementos de memoria (Flip Flops);
F= circuito transformador de señales (compuertas electrónicas)

Paso 1. Enunciado del problema

Paso 2. Análisis: Especificación de variables de entrada y de salida

Paso 3. Modelado: Definición de las funciones de Boole que especifican el comportamiento del sistema

Paso 4. Simplificación de las funciones de Boole (opcionalmente)

Paso 5. Diagrama lógico

Paso 6. Selección circuitos integrados

Paso 7. Ensamble del sistema digital (tablero de pruebas o circuito preimpreso)

Paso 8. Pruebas




</doc>
<doc id="2632" url="https://es.wikipedia.org/wiki?curid=2632" title="Spodiopogon">
Spodiopogon

Spodiopogon, es un género de plantas herbáceas de la familia de las gramíneas o poáceas. Es originario de Asia y Oriente Medio. 
El nombre del género deriva de las palabras griegas "spodios" (ceniza) y "pogon" (barba), tal vez refiriéndose al pelo de la inflorescencia. 
Tiene los números cromosómicos somáticos de 2n = 40 y 42. Cromosoma relativamente pequeños.



</doc>
<doc id="2633" url="https://es.wikipedia.org/wiki?curid=2633" title="Sorghastrum">
Sorghastrum

Sorghastrum, es un género de plantas herbáceas de la familia de las gramíneas o poáceas. Es originario de África tropical y Subtropical y América. 
El género fue descrito por George Valentine Nash y publicado en "Manual of the Flora of the northern States and Canada" 71. 1901. La especie tipo es: "Sorghastrum avenaceum" (Michx.) Nash. 
El nombre del género se compone de "Sorghum" (otro género de misma familia) y de la palabra latina "astrum" (una pobre imitación), refiriéndose a la semejanza entre los géneros. 
El número cromosómico básico es x = 10, con números cromosómicos somáticos de 2n = 20, 40 y 60. 2, 4, y 6 ploides. 




</doc>
<doc id="2634" url="https://es.wikipedia.org/wiki?curid=2634" title="Sitanion">
Sitanion

Sitanion, es un género de plantas herbáceas de la familia de las gramíneas o poáceas. Es originario de las regiones templadas de Norteamérica. 
El nombre del género deriva de la palabra griega "sitos" (grano). 
El número cromosómico básico es x = 7, con números cromosómicos somáticos de 2n = 28. 4 ploide. 



</doc>
<doc id="2637" url="https://es.wikipedia.org/wiki?curid=2637" title="Sclerochloa">
Sclerochloa

Sclerochloa es un género de plantas herbáceas perteneciente a la familia de las poáceas. Es originario del sur de Europa hasta el oeste de Asia. Comprende 31 especies descritas. 
El género fue descrito por Ambroise Marie François Joseph Palisot de Beauvois y publicado en "Essai d'une Nouvelle Agrostographie" 97, 177. 1812. La especie tipo es: "Sclerochloa dura" (L.) P.Beauv. 



</doc>
<doc id="2638" url="https://es.wikipedia.org/wiki?curid=2638" title="Schizostachyum">
Schizostachyum

Schizostachyum, es un género de plantas herbáceas perteneciente a la familia de las poáceas. Es originario del este de Asia. 

Las siguientes especies han sido excluidas de este género:




</doc>
<doc id="2639" url="https://es.wikipedia.org/wiki?curid=2639" title="Schizachyrium">
Schizachyrium

Schizachyrium, es un género de plantas herbáceas perteneciente a la familia de las poáceas. Es originario de las regiones tropicales del mundo.

El nombre del género deriva de las palabras griegas "schizein" (dividir) y "achuron" (paja), refiriéndose al lema superior. 

El número cromosómico básico es x = 5 y 10, con números cromosómicos somáticos de 2n = 20, 30, 40, y 50. 




</doc>
<doc id="2642" url="https://es.wikipedia.org/wiki?curid=2642" title="Saugetia">
Saugetia

Saugetia es un género de plantas herbáceas perteneciente a la familia de las poáceas. Es originario de las Indias Occidentales.

Algunos autores lo incluyen en el género "Enteropogon".



</doc>
<doc id="2643" url="https://es.wikipedia.org/wiki?curid=2643" title="Sasa">
Sasa

Sasa es un género botánico de bambúes. Las especies de este género tienen una rama por nudo. Pertenece a la familia de las poáceas. Es originario del este de Asia. Comprende 488 especies descritas y de estas, solo 61 aceptadas.
El género fue descrito por Makino & Shib. y publicado en "Botanical Magazine" 15(168): 18. 1901. 
Sasa: nombre genérico que proviene del nombre japonés para un pequeño bambú.


</doc>
<doc id="2644" url="https://es.wikipedia.org/wiki?curid=2644" title="Sacciolepis">
Sacciolepis

Sacciolepis es un género de plantas herbáceas perteneciente a la familia de las poáceas. Es originario de las regiones tropicales y subtropicales del mundo.
El nombre del género deriva de las palabras griegas "sakkion" (una pequeña bolsa) y "lepis" (escala), aludiendo a la gluma en forma de saco. 



</doc>
<doc id="2652" url="https://es.wikipedia.org/wiki?curid=2652" title="Siglo XVII">
Siglo XVII

El siglo XVII (17) d. C. (siglo decimoséptimo después de Cristo) o siglo XVII EC (siglo decimoséptimo de la era común) comenzó el 1 de enero de 1601 y terminó el 31 de diciembre de 1700. Es llamado el «Siglo de la Física» debido a que en este siglo las aportaciones de Galileo Galilei, René Descartes e Isaac Newton dieron origen a la física clásica y a un sistema de pensamiento mecanicista.













La población del centro de España, la más numerosa densa y pujante y con la mayor densidad de ciudades grandes y medias, empieza a declinar desde 1580 y tiene un descenso prolongado durante el siglo XVII, debido entre otras causas a la emigración americana, las numerosas epidemias, un índice de celibato de hasta el 10% de la población y a la expulsión de los moriscos. La cornisa cantábrica y Cataluña mantienen algún crecimiento.

El centro de España pierde un millón de habitantes, pero en la periferia se mantiene la población, por lo que en conjunto disminuye probablemente en un millón de habitantes en la centuria y cambia su distribución geográfica: en el futuro, el centro estará despoblado, excepto Madrid; y la periferia, densamente poblada.

Así, en la época de la expulsión de los moriscos (1609-1610) se estiman habitantes, de ellos en la Corona de Aragón, mientras que para 1717 se estiman habitantes, en la Corona de Aragón, es decir, los mismos o más que en 1610. La despoblación se había producido en el centro.

El siglo XVII es de un esplendor sin parangón, debido a que permite este tiempo desligarse de las ataduras provenientes de la Edad Media. 
El Renacimiento del siglo XVI es la puerta de entrada para que en los 100 años que corrieron de 1600 a 1700 la sociedad pudiese zafarse del viejo molde que implantaba métodos rígidos de comportamiento y actuación especialmente impuestos por la Iglesia.

Al romper estos viejos moldes se permitió salirse de la rigidez de las estructuras lineales e imprimir nuevas formas de movimiento especialmente en el campo de las artes, donde podrían ser la pintura, escultura y arquitectura. Este adelanto de imprimir movimiento, rescatar las formas celestiales por medio de la ornamentación, y el paso de lo estático a lo dinámico se contempla como el estilo barroco, que es un estilo moderno que deja atrás al manierismo del siglo precedente.

El barroco que se presenta en diferentes manifestaciones artísticas incluida la literatura en sus dos vertientes culteranismo y conceptismo, permite arraigar a la sociedad de entonces a un nuevo estilo de vida, en el que se adapta y acepta vivir bajo situaciones en constante cambio.








</doc>
<doc id="2654" url="https://es.wikipedia.org/wiki?curid=2654" title="Sedeniones">
Sedeniones

Los sedeniones forman un álgebra 16-dimensional sobre los números reales y se obtienen aplicando la Construcción de Cayley-Dickson sobre los octoniones.

Como en los octoniones, la multiplicación de sedeniones no es conmutativa, ni asociativa. Pero al contrario que los octoniones, los sedeniones no tienen ni siquiera la propiedad de ser un álgebra alternativa. Sin embargo, tienen la propiedad de ser potencia-asociativos.

Los sedeniones tienen el 1 como elemento neutro e inversas para la multiplicación, pero no son un álgebra de división, ya que tienen divisores del cero.

Todo sedenión es una combinación lineal de los sedeniones unitarios
1, "e", "e", "e", "e", "e", "e", "e", "e", "e", "e", "e", "e", "e", "e" y "e",
que forman la base del espacio vectorial de los sedeniones. La tabla de multiplicación de estos sedeniones unitarios es la siguiente.



</doc>
<doc id="2656" url="https://es.wikipedia.org/wiki?curid=2656" title="Sabiduría">
Sabiduría

La sabiduría es un carácter que se desarrolla con la aplicación de la inteligencia en la experiencia propia, obteniendo conclusiones que nos dan un mayor entendimiento, que a su vez nos capacitan para reflexionar, sacando conclusiones que nos dan discernimiento de la verdad, lo bueno y lo malo. La sabiduría y la moral se interrelacionan dando como resultado un individuo que actúa con buen juicio. Algunas veces se toma a la "sabiduría" como una forma especialmente bien desarrollada de sentido común. 

En ciencias de la información, la Sabiduría constituye el vértice de la pirámide constituida, de menor a mayor complejidad, por dato, información, conocimiento y sabiduría.

En la Sabiduría se destaca el juicio sano basado en conocimiento y entendimiento; la aptitud de valerse del conocimiento con éxito, y el entendimiento para resolver problemas, evitar o impedir peligros, alcanzar ciertas metas, o aconsejar a otros. Es lo opuesto a la tontedad, la estupidez y la locura, y a menudo se contrasta con estas. Tomás de Aquino define la sabiduría como "el conocimiento cierto de las causas más profundas de todo" ("In Metaphysica", I, 2). Por eso, para él, la sabiduría tiene como función propia ordenar y juzgar todos los conocimientos.

La sabiduría toma sus referencias de lo que se denomina memoria a largo plazo. En otras palabras, lo vivido ha de haberse experimentado con suficiente frecuencia o intensidad como para que no se borre de nuestro recuerdo, se inserte en los esquemas de lo que consideramos bueno o malo y se tome en cuenta como parte de los procesos de supervivencia del individuo.

La mayoría de los psicólogos consideran la "sabiduría" como distinta de las habilidades cognitivas medidas por los exámenes de inteligencia. La "sabiduría" es con frecuencia considerada como un rasgo que puede ser desarrollado por la experiencia, pero no enseñado. Cuando se aplica a asuntos prácticos, la palabra "sabiduría" es sinónimo de prudencia. Algunos consideran la sabiduría como una cualidad que incluso un niño, de otra forma inmaduro, puede poseer con independencia de la experiencia o el conocimiento completo. 
La Sabiduría según una definición muy explícita de la misma es: «La forma correcta de aplicar el conocimiento» y va mucho más allá que el mismo intelecto, mostrando así lo elemental de la Vida. 

La cultura contemporánea limita la importancia de la "sabiduría" y de la intuición.

El nivel de la "sabiduría" o la prudencia como una virtud es reconocida en fuentes culturales, filosóficas y religiosas. Algunos definen la "sabiduría" en un sentido utilitario, como una forma de prever las consecuencias y actuar para maximizar el bien común a largo plazo.

La sabiduría implica amplitud de conocimiento y profundidad de entendimiento, que son los que aportan la sensatez y claridad de juicio que la caracterizan. El hombre sabio ‘atesora conocimiento’ y así tiene un fondo al que recurrir. (Pr 10:14.) Aunque la “sabiduría es la cosa principal”, el consejo es: “Con todo lo que adquieres, adquiere entendimiento”. (Pr 4:5-7.) El entendimiento (término amplio que con frecuencia abarca el discernimiento) añade fuerza a la sabiduría, contribuyendo en gran manera a la discreción y la previsión, cualidades que también son características notables de la sabiduría. La discreción supone prudencia, y se puede expresar en forma de cautela, autodominio, moderación o comedimiento. El hombre “discreto [una forma de fró·ni·mos]” edifica su casa sobre la masa rocosa, previendo la posibilidad de una tormenta; el insensato la edifica sobre la arena y experimenta desastre. (Mt 7:24-27.)

El término hebreo jokj·máh (verbo, ja·kjám) y el griego so·fí·a, así como sus afines, son los vocablos básicos que comunican el concepto de “sabiduría”. También está la palabra hebrea tu·schi·yáh, que se puede traducir por “trabajo eficaz” o “sabiduría práctica”, y las palabras griegas fró·ni·mos y fró·nē·sis (de frēn, la “mente”), que se refieren a la “sensatez”, “discreción” o “sabiduría práctica”.


</doc>
<doc id="2661" url="https://es.wikipedia.org/wiki?curid=2661" title="Sinclair ZX Spectrum">
Sinclair ZX Spectrum

El Sinclair ZX Spectrum es un ordenador de 8 bits basado en el microprocesador Zilog Z80A, fabricado por la compañía británica Sinclair Research y lanzado al mercado el 23 de abril de 1982.

En Europa, el "Sinclair ZX Spectrum" fue uno de los microordenadores domésticos más populares de los años 1980.

Su optimizado y compacto diseño hizo las delicias de miles de aficionados a la informática y los videojuegos. Aún hoy perduran miles de fans del Spectrum que siguen jugando a sus juegos (con emuladores que cargan sus ficheros volcados de cintas). Además hay un mercado de coleccionismo tanto de cintas de juegos originales como de los propios Spectrum.

Las características del ZX Spectrum original incluían:






Posteriormente se desarrolló una nueva carcasa, que consistía en un teclado mejorado con teclas duras y 4 capas de membrana, para permitir la pulsación de dos teclas de función en una sola, y la carcasa más profesional, con bordes cuadrados en lugar de redondeados, que llevó el nombre de ZX Spectrum + (ZX Spectrum Plus). Este desarrollo también se vendió como actualización y solía incluirse junto con una ampliación de memoria para los Spectrum de 16 kB, que añadía un botón de "reset" y una mejor ventilación.

En definitiva, el diseño del ordenador estaba increíblemente optimizado y exprimía sus aparentemente pequeñas posibilidades al máximo. Todas estas características convertían al ZX Spectrum en un equipo muy asequible y versátil, consiguiendo acercar la microinformática a un elevado número de personas.

Una de las peculiaridades del ZX Spectrum es su sistema de vídeo, al ser capaz de mostrar una matriz de 256x192 pixeles, pero la resolución de color era únicamente de 32x24, por lo que grupos de 8x8 pixels compartían información de color.

Dicha información de color o atributos consistían en: Color de fondo o "paper", color de tinta o "ink", atributo de brillo, y un atributo flash.

El "color de fondo" se aplica a los "pixels 0", y el "color de tinta" que se aplica a los "pixels 1", pudiendo seleccionarse cada uno entre siete colores.

El atributo "brillo" aumentaba el brillo de los colores (excepto el negro, que no variaba), por lo que en pantalla podían mostrarse en total hasta 15 colores (siete por dos niveles de brillo, más el negro)

El atributo "flash" hacía que los dos atributos de color "fondo/tinta" se intercambiasen varias veces por segundo, dando un efecto de parpadeo.

Así, tenemos 256x192 = 49152 bits = 6144 bytes destinados al bitmap (2048 bytes para cada tercio de pantalla) y 32x24 = 768 bytes dedicados al color, brillo, y flash, totalizando un total de 6912 bytes.

El problema de tener distintas resoluciones para el bitmap y el color obligaba a los programadores de juegos, especialmente durante las últimas etapas de vigencia del ordenador, a adoptar soluciones ingeniosas para minimizar las colisiones entre colores, fenómeno conocido como "attribute clash" en el mundo anglosajón. Esto era debido a que el ZX-Spectrum no fue concebido en origen como una máquina de videojuegos. Si bien el "attribute clash" permitía reducir el tamaño necesario para vídeo a 6,75 kB, esto hacía que algunos de los gráficos mostrados tuvieran una apariencia de poca calidad si el diseño no era minucioso.

El hardware fue diseñado por Richard Altwasser y la carcasa y apariencia es un diseño de Rick Dickinson. El software (firmware de la ROM), así como el profuso manual de instrucciones fue obra de Steve Vickers. Todos ellos habían participado en el diseño de los modelos anteriores de Sinclair, el ZX80 y el ZX81.

En abril de 1982 aparecieron dos modelos: uno con 16 Kb a un precio de 125 libras (ampliable a 48 Kb por 60 libras) y otro con 48 Kb de fábrica por 175 libras. Con la salida de imagen en color y con un sonido muy aceptable, destacaba su pequeño tamaño y su teclado con teclas de goma dura que mantenía la tradicional forma de los modelos anteriores de presentar palabras completas con pulsaciones.

Con un precio tan ajustado, sobre todo comparado con los modelos de la competencia en el momento, los pedidos se dispararon y Sinclair y la empresa ensambladora de la máquina, Timex, no daban abasto. En julio de 1982 ya habían 30.000 pedidos pendientes de atender y a finales de agosto (debido a las vacaciones de verano de la plantilla que fueron escrupulosamente respetadas) ya eran 40.000 pedidos retrasados con la consiguiente molestia de muchos compradores. El propio Clive Sinclair hizo una disculpa pública en los medios de comunicación y se comprometió a tener los pedidos entregados en septiembre de ese mismo año, cosa que cumplió.

En marzo de 1983 ya se habían vendido más de 200.000 unidades del ZX-Spectrum, y el mercado de los videojuegos domésticos se había convertido en un rentable fenómeno a nivel mundial. Sinclair Research Ltd. se convirtió en apenas unos meses en una de las compañías del sector más sólidas y con más valor del momento. El precio de sus máquinas descendió hasta 99,95 libras para el ZX-Spectrum de 16 Kb, 129,95 libras para el ZX-Spectrum 48 Kb y 39,95 libras para el anterior modelo, el ZX81. 

En 1981, Altwasser y Vickers se desvincularon de Sinclair para formar su propia compañía, a la cual llamarían Jupiter Cantab (una abreviatura de Cantabridgian). Allí lanzaron al mercado una máquina de idéntica arquitectura a la empleada en la compañía de la que salían, el Jupiter Ace, que sin embargo no tuvo prácticamente repercusión (apenas se comercializaron unas 5.000 unidades). 

Con el paso de los años fueron apareciendo diversos periféricos, como por ejemplo dispositivos de almacenamiento propios (ZX Microdrive), interfaces de disco (OPUS Discovery, DISCiPLE, Beta Disk), lápices ópticos, ratones (AMX Mouse, Kempston Mouse, Star Mouse), impresoras (la ZX Printer apareció durante 1983 a un precio inicial de 39,95 libras), o mandos de juego (joysticks) que podían ser conectados directamente, por medio de la ZX Interface 2 o a través de otras interfaces que salieron posteriormente al mercado, como las de Kempston Micro Electronics.

El software del ZX Spectrum se compone actualmente de más de 20.000 títulos. A pesar de que el hardware del ZX Spectrum imponía unos límites y restricciones notables, su software era muy diverso, incluyendo implementaciones de muchos lenguajes de programación, entre ellos C, Pascal, Prolog (ej: "micro-PROLOG"), Modula-2, LISP, o Forth, diversos ensambladores/desensambladores de Z80 (ej: "OCP Editor/Assembler", "HiSoft Devpac", "ZEUS Assembler", "Artic Assembler)", compiladores de Sinclair BASIC (ej: "MCoder", "COLT, HiSoft BASIC"), extensiones del Sinclair BASIC (ej: "Beta BASIC", "Mega Basic"), programas de bases de datos (ej: "VU-File"), procesadores de texto (ej: "Tasword II"), hojas de cálculo (ej: "VU-Calc"), programas de diseño gráfico (ej: "OCP Art Studio", "The Artist", "Paintbox", "Melbourne Draw"), y de modelado 3D ("VU-3D"), aparte de, principalmente, videojuegos.

El Spectrum +128 fue fabricado en España por Investrónica, la filial de El Corte Inglés de distribución y fabricación de ordenadores, y distribuidora oficial de Sinclair Research. Investrónica también había distribuido bajo marca propia (InvesDisk) un interface de disco junto con el sistema TOS para el ZX Spectrum, desarrollado y comercializado por Timex Computer, la filial de Timex en Portugal. El desarrollo conjunto se realiza en la sede española y es por ello que en 1985 aparece primero en España.

El modelo de 128K podía funcionar en modo 48 KB o 128 KB. La mayoría de los programas comerciales se ejecutaban en el modo 48K, pero en los últimos tiempos aparecieron programas comerciales que eran compatibles con las dos versiones. Incorporaba un chip de sonido AY-3-8912 (el mismo que Timex incluiría en sus modelos de Spectrum unos años antes), un pequeño teclado numérico anexo pero independiente y un editor de textos integrado en el sistema operativo firmware; en la versión inglesa fueron eliminados el teclado numérico y el editor de texto, introduciendo en el manejo del sistema operativo un conjunto de menús que mantendrían posteriormente los modelos de Amstrad.

A la memoria RAM por encima de los 64 KB que puede direccionar directamente el microprocesador Z80 se podía acceder alegóricamente como ""Disco RAM"". Mantenía pese a los cambios bastante compatibilidad con el hardware periférico desarrollado para los modelos de 48 KB. Las versiones del +3 y +2A/B permitían mapear la memoria ram en los 64 kB direccionables, pudiendo sustituir la memoria ROM por RAM.

En los juegos, especialmente cuando se ejecutaban en el modo 128, se podía disponer de las mejoras del modelo, como la carga de varias fases en RAM. En el modo de 48 kB únicamente, el mapeado de memoria estaba inhabilitado, por lo que el chip AY-3-8912 se podía seguir utilizando, no así la memoria adicional.

El sistema de gráficos incluía un sistema de doble buffer que permitía seleccionar la memoria de representación entre 2 lugares de memoria fijos, lo que posibilitaba cambiar instantáneamente de pantalla permitiendo algunos trucos gráficos, como poder dibujar en la "pantalla shadow" y mostrarla justo al terminar de dibujarse, sin que se apreciara como se dibujaba progresivamente. Esta característica se podía utilizar para mostrar gráficos con mayor riqueza visual (aumentando la resolución vertical o emulando una mayor paleta gráfica). El empleo de esta característica requería el uso de 13,5 kB de memoria.

En 1986, la compañía Sinclair Research vendió a Amstrad, por 5 millones de libras, la marca comercial Sinclair y su línea de productos informáticos. A partir de ese momento Amstrad procedió a sacar al mercado los siguientes modelos:




En 1992 Amstrad decidió retirar los modelos de Spectrum del mercado, ante la popularización de las máquinas de 16 y 32 bits.

A mediados de 1982, Timex obtuvo la licencia para vender todos los productos Sinclair en Estados Unidos cediéndoles el derecho de usar su nombre por una comisión del 5% sobre las ventas y comenzando a vender productos bajo la marca Timex Sinclair que finalmente llegarían a comercializarse a través de varias filiales también en Argentina, Polonia y Portugal. La filial estadounidense lanzó el Timex Sinclair 1000 y el Timex Sinclair 1500 (ambos basados en el Sinclair ZX81) y el Timex Sinclair 2068 (basado en el ZX Spectrum). Por su parte la filial portuguesa lanzó el Timex Computer 2048 (basado también en el ZX Spectrum) y el Timex Computer 2068 (basado en el Timex Sinclair 2068). A partir de esta última máquina, y producto de un acuerdo entre Polish Unimor y Timex Computer fue desarrollado solo para el mercado polaco también el Unipolbrit Komputer 2086.

A modo de ejemplo de las diferencias entre los modelos Sinclair y Timex, el TS2068 fue lanzado a fines de 1983 (antes del cierre de la filial estadounidense, a mediados de 1984), está dotada de un puerto para poner cartuchos con software, dos entradas para joysticks, un chip de sonido AY-3-8912 (el mismo que posteriormente se adoptaría para los modelos de 128Kb de Sinclair y Amstrad), una extensión de ROM de 8 KB que incluía nuevos comandos para el Sinclair BASIC y dos modos de vídeo: un modo extendido en color de 32x192 (contra los 32x24 originales del Spectrum) y un modo monocromático de 512x192 pixels. Dentro de esta línea de mejoras al modelo original de Sinclair, se llegó en 1987 a desarrollar prototipos de una versión mejorada, el Timex Computer 3256, que finalmente no llegaría a ser lanzado al mercado.

La comercialización de los ordenadores de la serie Timex Sinclair se prologaría hasta 1989, cuando Timex Computer de Portugal decidió salir del mercado informático.

En Europa, Asia y también América se fabricaron diversos clones basados en la exitosa máquina de Sinclair Research, como la serie TK (el TK 90X fue su principal representante) fabricado por la empresa brasileña Microdigital o el Inves Spectrum de la española Investrónica. En Argentina lo comercializó la empresa Czerweny bajo el nombre de CZ Spectrum.

Algunos ordenadores lanzados por otras compañías y basados también en el microprocesador Z80 (como el Enterprise 128 o Tatung Einstein) dispusieron también de periféricos que permitían ejecutar el software del Spectrum en ellos.

Adicionalmente, Amstrad trabajó en el sucesor del ZX Spectrum +3, denominado proyecto "Loki". Se trataba de un ordenador de 8 bits llevado al límite, con soporte de modos de vídeo más avanzados y memoria comparable a la de un ordenador de 16 bits. Dicho proyecto nunca llegó a finalizarse, debido al declive de los ordenadores de 8 bits.
Paralelamente la compañía Miles Gordon Technology desarrolló su propia propuesta como sucesor, el SAM Coupé. Con un procesador Z80 a 6 MHz y 256 KB de RAM, incluía una disquetera de 3,5" como dispositivo de almacenamiento.

En los países occidentales, tras su retirada del mercado en 1992, la plataforma Spectrum persistirá principalmente a través de programas emuladores, aunque surgirán iniciativas menores, como interfaces para disco duro IDE, soporte para CompactFlash, sistemas operativos alternativos o expansiones gráficas.
A finales de 2014 se anunció la aparición del Sinclair ZX Spectrum Vega, una vídeo consola basada en el Spectrum.

En los países del este europeo se desarrollarán en la década de 1990 varios ordenadores clónicos del Spectrum, muchas veces con ampliaciones diversas (Scorpion ZS 256, Pentagon 1024-SL, Kay 1024, Sprinter, ATM Turbo, Dubna 48K, Hobbit, Didaktik M, etc).




</doc>
<doc id="2665" url="https://es.wikipedia.org/wiki?curid=2665" title="SCADA">
SCADA

SCADA, acrónimo de Supervisory Control And Data Acquisition (Supervisión, Control y Adquisición de Datos) es un concepto que se emplea para realizar un software para ordenadores que permite controlar y supervisar procesos industriales a distancia. Facilita retroalimentación en tiempo real con los dispositivos de campo (sensores y actuadores), y controla el proceso automáticamente. Provee de toda la información que se genera en el proceso productivo (supervisión, control calidad, control de producción, almacenamiento de datos, etc.) y permite su gestión e intervención.

La realimentación, también denominada retroalimentación o feedback es, en una organización, el proceso de compartir observaciones, preocupaciones y sugerencias, con la intención de recabar información, a nivel individual o colectivo, para mejorar o modificar diversos aspectos del funcionamiento de una organización. La realimentación tiene que ser bidireccional de modo que la mejora continua sea posible, en el escalafón jerárquico, de arriba para abajo y de abajo para arriba.

En la teoría de control, la realimentación es un proceso por el que una cierta proporción de la señal de salida de un sistema se redirige de nuevo a la entrada. Esto es de uso frecuente para controlar el comportamiento dinámico del sistema. Los ejemplos de la realimentación se pueden encontrar en la mayoría de los sistemas complejos, tales como ingeniería, arquitectura, economía, sociología y biología.

Existen dos tipos de sistemas principalmente: los de lazo abierto o no realimentados y los de lazo cerrado o realimentados. Los sistemas de lazo cerrado funcionan de tal manera que hacen que la salida vuelva al principio para que se analice la diferencia con un valor de referencia y en una segunda opción la salida se vaya ajustando, así hasta que el error sea 0. Cualquier sistema que tenga como objeto controlar una cantidad como por ejemplo temperatura, velocidad, presión, caudal, fuerza, posición, entre otras variables, son normalmente de lazo cerrado. Los sistemas de lazo abierto no se comparan a la variable controlada con una entrada de referencia. Cada ajuste de entrada determina una posición de funcionamiento fijo en los elementos de control (por ejemplo con temporizadores).

Es así que, la realimentación es un mecanismo o proceso cuya señal se mueve dentro de un sistema y vuelve al principio de éste como en un bucle, que se llama "bucle de realimentación". En un sistema de control (que tiene entradas y salidas), parte de la señal de salida vuelve de nuevo al sistema como parte de su entrada; a esto se le llama "realimentación" o retroalimentación.

La realimentación comprende todas aquellas soluciones de aplicación que hacen referencia a la captura de información de un proceso o planta, no necesariamente industrial, para que, con esta información, sea posible realizar una serie de análisis o estudios con los que se pueden obtener valiosos indicadores que permitan una retroalimentación sobre un operador o sobre el propio proceso, tales como:



Este esquema es un ejemplo de la aplicación del sistema SCADA en áreas industriales. Estas áreas pueden ser:


Supervisión: acto de observar el trabajo o tareas de otro (individuo o máquina) que puede no conocer el tema en profundidad, supervisar no significa el control sobre el otro, sino el guiarlo en un contexto de trabajo, profesional o personal, es decir con fines correctivos y/o de modificación.

Automática: ciencia tecnológica que busca la incorporación de elementos de ejecución autónoma que emulan el comportamiento humano o incluso superior. 

Principales familias: autómatas, robots, controles de movimiento, adquisición de datos, visión artificial, etc.

PLC: Programmable Logic Controller, Controlador Lógico Programable.

PAC: Programmable Automation Controller, Controlador de Automatización Programable. 

Un sistema SCADA incluye un hardware de señal de entrada y salida, controladores, interfaz hombre-máquina (HMI), redes, comunicaciones, base de datos y software.

El término SCADA usualmente se refiere a un sistema central que supervisa y controla un sitio completo o una parte de un sitio que nos interesa controlar (el control puede ser sobre máquinas en general, depósitos, bombas, etc.) o finalmente un sistema que se extiende sobre una gran distancia (kilómetros / millas). La mayor parte del control del sitio es en realidad realizada automáticamente por una Unidad Terminal Remota (UTR), por un Controlador Lógico Programable (PLC) y más actualmente por un Controlador de Automatización Programable (PAC). Las funciones de control del servidor están casi siempre restringidas a reajustes básicos del sitio o capacidades de nivel de supervisión. Por ejemplo un PLC puede controlar el flujo de agua fría a través de un proceso, pero un sistema SCADA puede permitirle a un operador cambiar el punto de consigna (set point) de control para el flujo, y permitirá grabar y mostrar cualquier condición de alarma como la pérdida de un flujo o una alta temperatura. La realimentación del lazo de control es cerrada a través del RTU o el PLC; el sistema SCADA supervisa el desempeño general de dicho lazo.
El sistema SCADA también puede mostrar gráficos históricos, tendencias, tablas con alarmas y eventos entre otras funciones. Puede y debe estar sujeto a permisos y accesos de los usuarios y desarrolladores de acuerdo a su nivel jerárquico en la organización y la función que cumple dentro de ésta.
Necesidades de la supervisión de procesos:

Una interfaz Hombre - Máquina o HMI ("Human Machine Interface") es el aparato que presenta los datos a un operador (humano) y a través del cual éste controla el proceso.

Los sistemas HMI podemos pensarlos como una "ventana de un proceso". Esta ventana puede estar en dispositivos especiales como paneles de operador o en un ordenador. Los sistemas HMI en ordenadores se los conoce también como software (o aplicación) HMI o de monitorización y control de supervisión. Las señales del proceso son conducidas al HMI por medio de dispositivos como tarjetas de entrada/salida en el ordenador, PLC's (Controladores lógicos programables), PACs (Controlador de automatización programable ), RTU (Unidades remotas de I/O) o DRIVER's (Variadores de velocidad de motores). Todos estos dispositivos deben tener una comunicación que entienda el HMI.

La industria de HMI nació esencialmente de la necesidad de estandarizar la manera de monitorizar y de controlar múltiples sistemas remotos, PLCs y otros mecanismos de control. Aunque un PLC realiza automáticamente un control pre-programado sobre un proceso, normalmente se distribuyen a lo largo de toda la planta, haciendo difícil recoger los datos de manera manual, los sistemas SCADA lo hacen de manera automática. Históricamente los PLC no tienen una manera estándar de presentar la información al operador. La obtención de los datos por el sistema SCADA parte desde el PLC o desde otros controladores y se realiza por medio de algún tipo de red, posteriormente esta información es combinada y formateada. Un HMI puede tener también vínculos con una base de datos para proporcionar las tendencias, los datos de diagnóstico y manejo de la información así como un cronograma de procedimientos de mantenimiento, información logística, esquemas detallados para un sensor o máquina en particular, incluso sistemas expertos con guía de resolución de problemas. Desde cerca de 1998, virtualmente todos los productores principales de PLC ofrecen integración con sistemas HMI/SCADA, muchos de ellos usan protocolos de comunicaciones abiertos y no propietarios. Numerosos paquetes de HMI/SCADA de terceros ofrecen compatibilidad incorporada con la mayoría de PLCs.

SCADA es popular debido a esta compatibilidad y seguridad. Ésta se usa desde aplicaciones a pequeñas escalas, como controladores de temperatura en un espacio, hasta aplicaciones muy grandes como el control de plantas nucleares.

La solución de SCADA a menudo tiene componentes de sistemas de control distribuido, DCS ("Distribuited Control System"). El uso de RTUs o PLCs o últimamente PACs sin involucrar computadoras maestras está aumentando, los cuales son autónomos ejecutando procesos de lógica simple. Frecuentemente se usa un lenguaje de programación funcional para crear programas que corran en estos RTUs y PLCs, siempre siguiendo los estándares de la norma IEC 61131-3. La complejidad y la naturaleza de este tipo de programación hace que los programadores necesiten cierta especialización y conocimiento sobre los actuadores que van a programar. Aunque la programación de estos elementos es ligeramente distinta a la programación tradicional, también se usan lenguajes que establecen procedimientos, como pueden ser FORTRAN, C o Ada95. Esto les permite a los ingenieros de sistemas SCADA implementar programas para ser ejecutados en RTUs o un PLCs.

Los tres componentes de un sistema SCADA son:


La RTU se conecta al equipo físicamente y lee los datos de estado como los estados abierto/cerrado desde una válvula o un interruptor, lee las medidas como presión, flujo, voltaje o corriente. Por el equipo el RTU puede enviar señales que pueden controlarlo: abrirlo, cerrarlo, intercambiar la válvula o configurar la velocidad de la bomba, ponerla en marcha, pararla.

La RTU puede leer el estado de los datos digitales o medidas de datos analógicos y envía comandos digitales de salida o puntos de ajuste analógicos.

Una de las partes más importantes de la implementación de SCADA son las alarmas. Una alarma es un punto de estado digital que tiene cada valor NORMAL o ALARMA. La alarma se puede crear en cada paso que los requerimientos lo necesiten. Un ejemplo de una alarma es la luz de "tanque de combustible vacío"del automóvil. El operador de SCADA pone atención a la parte del sistema que lo requiera, por la alarma. Pueden enviarse por correo electrónico o mensajes de texto con la activación de una alarma, alertando al administrador o incluso al operador de SCADA.

El término "Estación Maestra" se refiere a los servidores y al software responsable para comunicarse con el equipo del campo (RTUs, PLCs, etc) en estos se encuentra el software HMI corriendo para las estaciones de trabajo en el cuarto de control, o en cualquier otro lado. En un sistema SCADA pequeño, la estación maestra puede estar en un solo computador. A gran escala, en los sistemas SCADA la estación maestra puede incluir muchos servidores, aplicaciones de software distribuido, y sitios de recuperación de desastres.

El sistema SCADA usualmente presenta la información al personal operativo de manera gráfica, en forma de un diagrama de representación. Esto significa que el operador puede ver un esquema que representa la planta que está siendo controlada. Por ejemplo un dibujo de una bomba conectada a la tubería puede mostrar al operador cuanto fluido está siendo bombeado desde la bomba a través de la tubería en un momento dado o bien el nivel de líquido de un tanque o si la válvula está abierta o cerrada. Los diagramas de representación puede consistir en gráficos de líneas y símbolos esquemáticos para representar los elementos del proceso, o pueden consistir en fotografías digitales de los equipos sobre los cuales se animan las secuencias.

Los bloques software de un SCADA (módulos), permiten actividades de adquisición, supervisión y control.


El paquete HMI para el sistema SCADA típicamente incluye un programa de dibujo con el cual los operadores o el personal de mantenimiento del sistema pueden cambiar la apariencia de la interfaz. Estas representaciones pueden ser tan simples como unas luces de tráfico en pantalla, las cuales representan el estado actual de un campo en el tráfico actual, o tan complejas como una pantalla de multiproyector representando posiciones de todos los elevadores en un rascacielos o todos los trenes de una vía férrea. Plataformas abiertas como GNU/Linux que no eran ampliamente usadas inicialmente, se usan debido al ambiente de desarrollo altamente dinámico y porque un cliente que tiene la capacidad de acomodarse en el campo del hardware y mecanismos a ser controlados que usualmente se venden UNIX o con licencias OpenVMS. Hoy todos los grandes sistemas son usados en los servidores de la estación maestra así como en las estaciones de trabajo HMI.

En vez de confiar en la intervención del operador o en la automatización de la estación maestra los RTU pueden ahora ser requeridos para operar ellos mismos, realizando su propio control sobre todo por temas de seguridad. El software de la estación maestra requiere hacer más análisis de datos antes de ser presentados a los operadores, incluyendo análisis históricos y análisis asociados con los requerimientos de la industria particular. Los requerimientos de seguridad están siendo aplicados en los sistemas como un todo e incluso el software de la estación maestra debe implementar los estándares más fuertes de seguridad en ciertos mercados.

Para algunas instalaciones, los costos que pueden derivar de los fallos de un sistema de control es extremadamente alto, es posible incluso que haya riesgo de herir a personas. El hardware del sistema SCADA es generalmente lo suficientemente robusto para resistir condiciones de temperatura, humedad, vibración y voltajes extremos pero en estas instalaciones es común aumentar la fiabilidad mediante hardware redundante y varios canales de comunicación. Una parte que falla puede ser fácilmente identificada y su funcionalidad puede ser automáticamente desarrollada por un hardware de backup. Una parte que falle puede ser reemplazada sin interrumpir el proceso. La confianza en cada sistema puede ser calculado estadísticamente y este estado es el significado de tiempo medio entre fallos, el cual es una variable que acumula tiempos entre fallas. El resultado calculado significa que el tiempo medio entre fallos de sistemas de alta fiabilidad puede ser de decadas.

Los sistemas SCADA tienen tradicionalmente una combinación de radios y señales directas seriales o conexiones de módem para conocer los requerimientos de comunicaciones, incluso Ethernet e IP sobre SONET (fibra óptica) es también frecuentemente usada en sitios muy grandes como ferrocarriles y estaciones de energía eléctrica. Es más, los métodos de conexión entre sistemas puede incluso que sea a través de comunicación wireless (por ejemplo si queremos enviar la señal a una PDA, a un teléfono móvil) y así no tener que emplear cables.

Para que la instalación de un SCADA sea perfectamente aprovechada, debe de cumplir varios objetivos:


Para desarrollar un sistema SCADA es necesario un IDE en el cual diseñar, entre otras cosas:
También funciona con los controladores lógicos establecidos por National Instrument tales como LABVIEW y MULTISIM PROTEUS entre otros incluso se establece conexión con micro controladores tales como el ARDUINO, haciendo a SCADA una herramienta bastante útil para los sistemas de control automatizado.
Así pues, una de las soluciones en el control SCADA es utilizar la aplicación creada junto con un programa para monitorizar, controlar y automatizar señales analógicas y digitales, capturadas a través de tarjetas de adquisición de datos. Uno de los programas más utilizados para este fin es el LabView (National Instruments).


Un SCADA sirve para supervisar y su principal objetivo es medir con la finalidad de corregir.

Tenemos un proceso químico, que puede ser desde una fábrica de gelatina, a una de antibióticos, que queremos supervisar.
Lo que pondremos en la planta de producción serán PLCs, HMIs, etc, lo que se denomina Nivel I, ó nivel básico de automatización. Los datos obtenidos por estos hardwares industriales son transportados a través de un bus o varios buses a un servidor (server), que es el supervisor, el que controla, mediante el mencionado SCADA. Este envío de datos se puede hacer a través de ethernet, por ejemplo. 

El servidor, a su tiempo, manda los datos a una base de datos con la finalidad de almacenar la información (para trabajar con ella, crear históricos de errores o alarmas). Esta base de datos puede estar integrada dentro del disco duro del propio servidor.
También es posible que el servidor mande la información a otro PC, PDA, Telf, Internet, es decir, transmita la información a otros sistemas operativos, en los cuales los clientes, accionistas, jefes, supervisores, pueden acceder a la información.




</doc>
<doc id="2667" url="https://es.wikipedia.org/wiki?curid=2667" title="SUSE Linux">
SUSE Linux

SUSE Linux es una de las distribuciones Linux existentes a nivel mundial, se basó en sus orígenes en Slackware. Entre las principales virtudes de esta distribución se encuentra el que sea una de las más sencillas de instalar y administrar, ya que cuenta con varios asistentes gráficos para completar diversas tareas en especial por su gran herramienta de instalación y configuración YasT.

Su nombre "SuSE" es el acrónimo, en alemán ""Software und Systementwicklung"" ("Desarrollo de Sistemas y de Software"), el cual formaba parte del nombre original de la compañía y que se podría traducir como "desarrollo de software y sistemas". El nombre actual de la compañía es "SuSE LINUX", habiendo perdido el primer término su significado (al menos oficialmente).

El 4 de noviembre de 2003, la compañía multinacional estadounidense Novell anunció que iba a comprar "SuSE LINUX". La adquisición se llevó a cabo en enero de 2004. En el año 2005, en la LinuxWorld, Novell, siguiendo los pasos de RedHat Inc., anunció la liberación de la distribución "SuSE Linux" para que la comunidad fuera la encargada del desarrollo de esta distribución, que ahora se denomina openSUSE.

El 4 de agosto de 2005, el portavoz de Novell y director de relaciones públicas "Bruce Lowry" anunció que el desarrollo de la serie "SUSE Professional" se convertiría en más abierto y entraría en el intento del proyecto de la comunidad openSUSE de alcanzar a una audiencia mayor de usuarios y desarrolladores. El software, por la definición de código abierto, tenía ya su código fuente "abierto", pero ahora el proceso de desarrollo sería más "abierto" que antes, permitiendo que los desarrolladores y usuarios probaran el producto y ayudaran a desarrollarlo.

Anteriormente, todo el trabajo de desarrollo era realizado por "SUSE", y la versión 10.0 fue la primera versión con una beta pública. Como parte del cambio, el acceso en línea al servidor YaST de actualización sería complementario para los usuarios de "SUSE Linux", y siguiendo la línea de la mayoría de distribuciones de código abierto, existiría tanto la descarga gratuita disponible mediante web como la venta del sistema operativo "en caja". Este cambio en la filosofía condujo al lanzamiento de "SUSE Linux" 10.0 el 6 de octubre de 2005 en "OSS" (código completamente abierto), "eval" (tiene tanto código abierto como aplicaciones propietarias y es una versión realmente completa) y al por menor en centros especializados.

"SUSE" incluye un programa único de instalación y administración llamado YaST2 que permite realizar actualizaciones, configurar la red y el cortafuegos, administrar a los usuarios, y muchas más opciones todas ellas integradas en una sola interfaz amigable. Además incluye varios escritorios, entre ellos los más conocidos que son KDE y Gnome, siendo el primero el escritorio por omisión. La distribución incorpora las herramientas necesarias para redistribuir el espacio del disco duro permitiendo así la coexistencia con otros sistemas operativos existentes en el mismo.

Usa sistemas de paquetes RPM (RPM package manager) originalmente desarrollados por Red Hat aunque no guarda relación con esta distribución.

También es posible utilizar el sistema de instalación CNR (Click 'N Run) originalmente creado por la empresa que distribuía Lindows OS (que ahora se llama Linespire y Freespire en su versión gratuita). Este sistema sincroniza nuestra máquina al servidor CNR y al hacer clic en la página de navegación y alguno de los programas, este se instala de manera automática en el ordenador.
Antiguamente, "SUSE" primero lanzaba las versiones personales y profesionales en paquetes que incluían una extensa documentación impresa y esperaba algunos meses antes de lanzar las versiones en sus servidores.

Comenzando con la versión 9.2, una imagen ISO de 1 DVD de "SUSE Professional" fue lanzada, así como una versión de evaluación del LiveDVD arrancable. El servidor FTP continúa funcionando y tiene la ventaja de las instalaciones en línea: sólo se descargan los paquetes que el usuario cree que necesita. La ISO tiene ventajas en cuanto a facilidad de instalación de paquetes de forma sencilla y sin conexión a Internet. Las distribuciones de DVD "en caja" soportan instalaciones x86 y x86-64, pero los CD-ROM incluidos no disponen de soporte para x86-64.
Desde la versión 9, es posible descargar el archivo ISO correspondiente a la distribución, pero después de ser instalado empieza un período de evaluación que inicialmente fue de 30 días y en las versiones posteriores es de 60 días. Este período permite usar en forma libre y gratuita los servicios de actualización de software de Novell, luego de lo cual se debe pagar una suscripción para obtener actualizaciones.







</doc>
<doc id="2671" url="https://es.wikipedia.org/wiki?curid=2671" title="Sumeria">
Sumeria

Sumeria (del acadio "Šumeru"; en sumerio cuneiforme 𒆠𒂗𒂠 "ki-en-gi", aproximadamente 'tierra, país', 'señor', cañaveral') es una región histórica de Oriente Medio, parte sur de la antigua Mesopotamia, entre las planicies aluviales de los ríos Éufrates y Tigris. La civilización sumeria está considerada como la primera y más antigua civilización del mundo. Aunque la procedencia de sus habitantes, los sumerios, es incierta, existen numerosas hipótesis sobre sus orígenes, siendo la más aceptada actualmente la que argumenta que no habría ocurrido ninguna ruptura cultural con el período de Uruk, lo que descartaría factores externos, como podían ser invasiones o migraciones desde otros territorios lejanos.

El término "sumerio" también se aplica a todos los hablantes de la lengua sumeria. En dicha lengua, esta región era denominada "Kengi (ki)", equivalente al acadio "mat Sumeri", esto es, "tierra de Súmer".

El término "sumerio" es el nombre común dado a los antiguos habitantes de baja Mesopotamia por sus sucesores, los semitas acadios. Los sumerios se llamaban a sí mismos "sag-giga", que significa literalmente "el pueblo de cabezas negras". La palabra acadia "shumer" puede representar este nombre en el dialecto, pero se desconoce por qué los acadios llamaron "Shumeru" a las tierras del sur. Algunas palabras como la bíblica "Shinar", la egipcia "Sngr", o la Indoeuropea Hitita "Šanhar(a)" pueden haber sido variantes de "Šumer". De acuerdo al historiador babilonio Beroso, los sumerios fueron "extranjeros de cabezas negras".

En la Baja Mesopotamia:
Asumiendo que existían asentamientos humanos desde el Neolítico como demuestra la cultura de Jarmo, (6700 a. C.-6500 a. C.) y en el Calcolítico las de cultura Hassuna-Samarra (5500 a. C.-5000 a. C.), El Obeid (5000 a. C.-4000 a. C.), Uruk (4000 a. C.-3200 a. C.) y Yemdet Nasr (3200 a. C.-3000 a. C.).

No existen registros escritos de esa etapa para conocer el origen de este pueblo, y tampoco los cráneos hallados en los enterramientos aclaran el problema de su origen, debido a que están representadas tanto la dolicocefalia como la braquicefalia, con algunos testimonios del tipo armenoide. Se investigan las esculturas sumerias que muestran un alto índice de cráneos braquicéfalos en sus representaciones que quizá podían dilucidar la procedencia de este pueblo, junto con las coloraciones y las dimensiones de las esculturas, que son una mezcla entre caucásicos y miembros de raza negra. Con todo esto no es suficiente evidencia para solucionar el problema puesto que la plástica podría haberlas idealizado, como pasaba en las esculturas egipcias.

Se ha descartado la posibilidad de identificación basada en la evolución de los tipos craneales en el conjunto del Oriente Medio, pues éstos aparecen bastante mezclados. Sin embargo se pueden distinguir cuatro grandes grupos con rasgos pertenecientes a distintas épocas: antes de 4000 a. C. sólo se encuentran poblaciones dolicocéfalas del tipo "mediterráneo"; los "eurafricanos", que sólo son una variedad de este grupo, y que no tuvieron un papel apreciable hasta 3000 a. C.; el tipo "alpinos", braquicéfalos que se manifiestan moderadamente después de 2500 a. C., y los "armenoides", derivados tal vez de estos alpinos que aparecen en abundancia después de 500 a. C. Los pueblos descendientes de los cimerios tienden a tener en promedio las cabezas más "redondeadas" (braquicéfalas) que los demás pueblos de esa área y la palabra "sumerio" puede ser una transliteración de la palabra "cimerios" según algunos filólogos. Es por esto que varios investigadores creen que ambos pueblos son un mismo pueblo en diferentes épocas, pero no hay suficientes evidencias para sustentar esta hipótesis.

Parece posible que los sumerios fuesen una tribu proveniente de fuera, posiblemente de las estepas, pero su origen concreto es desconocido. Esto es lo que se ha venido denominando desde el siglo XX como el "problema sumerio."

En cualquier caso, es durante el período del Obeid cuando se producen avances que cristalizan en el período de Uruk, y que sirven para considerar este momento como el inicio de la civilización sumeria.

Algunos estudiosos también postulan que los sumerios establecidos en Mesopotamia, no tendrían un origen autóctono, sino que provendrían de la cultura que fundó la ciudad de Mohenjo-Daro (que existió entre el 2600 a. C. y el 1800 a. C.) en India.

Uruk, la "Erec" bíblica y la árabe "Warka", es el escenario de descubrimientos fundamentales para la historia de la humanidad: aparece la rueda en torno al 3500 a. C., y la escritura en el 3300 a. C., siendo ésta la datación más antigua de tablillas de arcilla con escritura cuneiforme encontrada hasta la fecha. Estos registros escritos confirman que los sumerios no eran un pueblo indoeuropeo, ni camita, ni semita, ni tampoco elamo-drávida (grupo, este último, al que pertenece el pueblo elamita, por ejemplo). Así lo demuestra su lengua de tipo aglutinante. No obstante, se especula, como se ha dicho, que los sumerios no fueron el primer pueblo en asentarse en la baja Mesopotamia, en el curso bajo del Creciente fértil, sino que llegaron en un determinado momento de la Edad del Cobre o Calcolítico, allá por el año 3500 antes de nuestra era, durante el período ahora denominado Uruk.

La difusión de los avances de la cultura de Uruk por el resto de Mesopotamia dio lugar al nacimiento de la cultura Sumeria. Estas técnicas permitieron la proliferación de las ciudades por nuevos territorios. Estas ciudades pronto se caracterizaron por la aparición de murallas, lo que parece indicar que las guerras entre ellas fueron frecuentes. También destaca la expansión de la escritura que saltó desde su papel administrativo y técnico hasta las primeras inscripciones dedicatorias en las estatuas consagradas de los templos.

Pese a la existencia de las listas reales sumerias la historia de este período es relativamente desconocida, ya que gran parte de los reinados expuestos en ellas tienen fechas imposibles. En realidad, estas listas se confeccionaron a partir del siglo XVII a. C., y su creación se debió probablemente al deseo de los monarcas de remontar su linaje hasta tiempos épicos. Algunos de los reyes son probablemente reales pero de muchos otros no hay constancia histórica y otros de los que se sabe su existencia no figuran en ellas.

Hacia 2350 a. C., Sargón, un usurpador de origen acadio, se hizo con el poder en la ciudad de Kiš. Fundó una nueva capital, Agadé y conquistó el resto de ciudades sumerias, venciendo a Lugalzagesi, el rey de Umma hasta entonces dominante. Este fue el primer gran Imperio de la historia y sería continuado por los sucesores de Sargón, que se tendrían que enfrentar a constantes revueltas. Entre ellos destacó el nieto del conquistador, Naram-Sin. Esta etapa marcó el inicio de la decadencia de la cultura e idioma sumerios en favor de los acadios.

El imperio se deshizo hacia el 2220 a. C., debido a las constantes revueltas y las invasiones de los nómadas amorreos y, principalmente, gutis. Tras su caída, toda la región cayó bajo el dominio de estas tribus, quienes se impusieron sobre las ciudades-estado de la región, especialmente en el entorno de la destruida Agadé. Las crónicas sumerias los describen constantemente de forma negativa, como "horda de bárbaros" o "dragones de montaña", pero es posible que la realidad no fuese tan negativa; en algunos centros se produjo un verdadero florecimiento de las artes. Es el caso de la ciudad de Lagaš, especialmente durante el gobierno del patesi Gudea. Además de la calidad artística, en las obras de Lagaš se utilizaron materiales provenientes de regiones lejanas: madera de cedro del Líbano o diorita, oro y cornalina del valle del Indo; lo que parece indicar que el comercio no se debió ver especialmente lastrado. Las ciudades meridionales, más alejadas del centro de poder guti, compraban su libertad a cambio de importantes tributos; Uruk y Ur prosperaron durante sus IV y II dinastías.

Según una tablilla conmemorativa fue Utu-hengal, rey de Uruk, quien en torno a 2100 a. C. derrotó y expulsó a los gobernantes gutis de las tierras sumerias. Su éxito no le sería de mucho provecho ya que poco después el rey de Ur, Ur-Nammu, consiguió la hegemonía en toda la región con la llamada III dinastía de Ur o Renacimiento sumerio. El imperio surgido a raíz de esta hegemonía sería tan extenso o más que el de Sargón, del que tomaría la idea de imperio unificador. Esta influencia se aprecia incluso en la denominación de los monarcas, que a imitación de los acadios se harán llamar "reyes de Sumer y Acad"

A Ur-Nammu le sucederá su hijo, Shulgi, quien combatió contra Elam y las tribus nómadas de los Zagros. A éste le sucedió su hijo Amar-Suen (Amar-Sin) y a éste primero un hermano suyo, Shu-Sin y después otro Ibbi-Sin. En el reinado de este último los ataques de los amorreos, provenientes de Arabia, se hicieron especialmente fuertes y en 2003 a. C. caería el último imperio predominantemente sumerio. En adelante será la cultura acadia la que predomine y, posteriormente, Babilonia heredará el papel de los grandes imperios sumerios.

La desaparición del Imperio Acadio permitió el renacimiento de Sumer y el regreso al régimen de las ciudades estado. Tienen gran relevancia las reformas de Gudea de la Dinastía de Lagaš en esta época neosumeria (2175 a. C.). Posteriormente en la III Dinastía de Ur, Ur-Nammu lleva a cabo un código bien estructurado con numerosos cambios. En esta época se empiezan a nombrar como Reyes de Sumer y Akkad (2111 a. C.). Shulgi en 2093 a. C. impulsará una evolución referente a los pesos y medidas existentes, a la vez que reforzará las fronteras por el acoso de los semitas-amoritas.

Pese a ello, finalmente sucumbió a los ataques de los amoritas (amorreos) los cuales llevaban tropas auxiliares elamitas-semitas, procedentes de la meseta de Irán, que prevalecieron y saquearon Ur(2003 a. C.). Se vuelve a un estado de fragmentación política y proliferan dinastías locales. Rimsin creará un pequeño imperio en 1792 a. C. donde se introducirá la propiedad privada, dándose una sociedad pre-capitalista. En cambio, en Babilonia se entronizará una dinastía amorita (1792 a. C.).

La sociedad de la III Dinastía de Ur se organiza de esta manera:


Con respecto a la organización social, la sociedad sumeria era jerárquica y estratificada, al igual que las de todas las civilizaciones. En la cúspide de la pirámide social se encontraba el rey, a quien seguía en importancia una elite de sacerdotes, jefes militares y funcionarios de alto nivel. A continuación se ubican los comerciantes, funcionarios menores, artesanos especializados y, luego, los campesinos y artesanos. En el nivel más bajo de la sociedad correspondía a los esclavos.

A fines del 4º milenio a. C. Sumeria se dividió en una docena de Ciudades estado independientes cuyos límites fueron definidos por medio de canales y mojones. Estas ciudades eran grandes centros mercantiles. Cada una estaba centrada en un templo dedicado al dios patrono particular de la ciudad y gobernado por un "patesi" ("Ennsi"), o en ocasiones por un rey ("lugal"). Los patesi eran sacerdotes supremos y jefes militares absolutos, auxiliados por una aristocracia constituida por burócratas y sacerdotes. El patesi controlaba la construcción de diques, canales de riego, templos y silos, imponiendo y administrando los tributos a los que toda la población estaba sujeta. Las ciudades estado sumerias tradicionalmente eran ciudades-templos, ya que los sumerios consideraban que los dioses fundaban las ciudades para que fuesen centros de culto. Más tarde, conforme a la religión, los dioses se limitaban a comunicar a los soberanos los planos de los santuarios. El vínculo de los patesis con los ritos religiosos de la ciudad era extremadamente íntimo.

Los templos (entre los cuales se destacaban los piramidales "ziqqurat") estaban ligados al poder estatal, y sus riquezas eran usufructuadas por los soberanos, considerados intermediarios entre los dioses y los hombres. Junto con los templos de las ciudades, homenajeando a su dios patrono, no era infrecuente que se erigiesen zigurats; pirámides de ladrillos macizos cocidos al sol que servían de santuarios y acceso a los dioses cuando éstos descendían hasta su pueblo.

Con el desarrollo de las ciudades, las tentativas de supremacía de unas sobre otras se tornó inevitable. Durante un milenio se sucedieron luchas por el control sobre los derechos de uso del agua, de las rutas de comercio y el cobro de tributos a tribus nómadas.
Las primeras cinco ciudades desde las que se ejerció el poder predinástico son —entre paréntesis aparece el nombre actual del paraje—:


Otras ciudades principales:


Otras ciudades menores, de sur a norte:


El idioma sumerio se considera una lengua aislada ya que no está emparentada con ninguna familia lingüística conocida, aunque se han hecho muchos intentos fallidos por relacionar el sumerio a otros grupos lingüísticos. El sumerio es claramente diferente del acadio, una lengua de claro origen semítico, con el coexistió en la región alternándose como lenguas dominantes. Ambas lenguas usaron la escritura cuneiforme, originalmente desarrollada por los sumerios y cuyo uso sobrepasó al de la propia lengua sumeria por más de un milenio.

El sumerio era un idioma aglutinante, es decir, los monemas (unidades de significado) se pegaban unos con otros para crear palabras enteras, en contraste con las lenguas flexivas como el acadio o las lenguas indoeuropeas. Por tanto tipológicamente el sumerio difiere notablemente de otras lenguas de la región ya que el sumerio prefiere utilizar afijos para expresar lo mismo. En cambio otras lenguas cercanas como el elamita, las lenguas hurrito-urartianas y algunas lenguas caucásicas muestran tipologías lingüísticas más similares al sumerio, aunque no parecen directamente relacionadas con él.

Los sumerios inventaron jeroglíficos pictóricos que más tarde dieron lugar a la escritura cuneiforme propiamente dicha, y su lengua junto con el del Antiguo Egipto compiten por el crédito de ser la lengua más tempranamente documentada. Ha sobrevivido un gran corpus formado por cientos de miles de textos en sumerio, la gran mayoría de estos textos en tablillas de arcilla. Los textos sumerios conocidos incluyen textos personales y cartas de negocios y transacciones, recibos, listas de léxico, leyes, himnos y plegarias, encantamientos mágicos e incluso textos científicos de matemáticas, astronomía y medicina. Las inscripciones monumentales y los textos escritos en diferentes objetos como estatuas o ladrillos también eran bastante comunes. Muchos textos sobrevivieron en múltiples copias, ya que fueron transcritos varias veces por escribas en formación. El sumerio siguió siendo la lengua litúrgica usada en oficios religiosos y la lengua de los textos legales en Mesopotamia mucho después de que los semitas se convirtieran en el grupo hegemónico en la región.

La comprensión de los textos en sumerio puede ser complicada hoy en día, incluso para los expertos, principalmente por el uso de caracteres jeroglíficos de difícil interpretación. Los más difíciles son los textos más antiguos, que en muchos casos no dan toda la estructura gramatical de la lengua que siempre cambiaba.

Tratar un asunto tal como la religión sumeria puede ser complicado, dado que las prácticas y creencias adoptadas por aquellos pueblos variaron mucho a través del tiempo y la distancia, cada ciudad poseía su propia visión mitológica y/o teológica. Los sumerios fueron posiblemente los primeros en escribir sobre sus creencias, que luego fueron la inspiración para gran parte de la mitología, religión y astrología mesopotámica, aunque ello no implica que su religión fuera la primera y que no hubieran tomado costumbres y ritos de otros pueblos.

Los sumerios veían los movimientos a su alrededor como la magia de los espíritus, magia que era la única explicación que tenían de cómo funcionaban las cosas. Esos espíritus eran sus dioses. Y con muchos espíritus alrededor, creían en varios dioses, que tenían emociones humanas. Creían que el sol, la luna y las estrellas eran dioses, al igual que los juncos que crecían a su alrededor y la cerveza que destilaban.

Creían que los dioses controlaban el pasado y el futuro, que les revelaban las habilidades que poseían, incluyendo la escritura, y que los dioses les proporcionaban todo lo que necesitaban saber. No tenían la visión de que su civilización se hubiera desarrollado por sus propios esfuerzos. Y tampoco tenían visión de progreso tecnológico o social.

Cada uno de los dioses sumerios (en su propia lengua, "dingir" y en plural, "dingir-dingir" o "dingira-ne-ne") era asociado a ciudades diferentes, y la importancia religiosa a ellos atribuida se intensificaba o declinaba dependiendo del poder político de la ciudad asociada. Según la tradición sumeria, los dioses crearon el ser humano a partir del barro con el propósito que fueran servidos por sus nuevas criaturas. Cuando estaban enojados o frustrados, los dioses expresaban sus sentimientos a través de terremotos o catástrofes naturales: la esencia primordial de la religión sumerio se basaba, por lo tanto, en la creencia de que toda la humanidad estaba a merced de los dioses. Nótese la similitud de la creación del hombre a partir del barro con el relato del Génesis.

Entre las principales figuras mitológicas adoradas por los sumerios, es posible citar:

Los sumerios probablemente hayan cavado en la tierra unos metros y encontrado agua. Los sumerios creían que la tierra era un gran disco flotando en el mar. Llamaron a ese mar Nammu y pensaban que había estado desde siempre en el tiempo. Creían que Nammu había creado los peces, los pájaros, cerdos salvajes y otras criaturas que aparecieron en las tierras pantanosas y húmedas.

Según ellos, Nammu había creado el cielo y la tierra. El cielo se había separado de la tierra, dando nacimiento al dios masculino An y la tierra, una diosa llamada Ki. Creían que Ki y An habían procreado un hijo llamado Enlil, que era la atmósfera, el viento y la tormenta. Creían que él separó el día de la noche y que había abierto una concha invisible dejando caer agua desde el cielo. Creían que junto con su madre y Ki, Enlil sentó las bases de la creación de las plantas, los humanos y otras criaturas, que hacía germinar las semillas y que había dado forma a la humanidad a partir de la arcilla, impregnándola.

El universo consistía en un disco plano cerrado por una cúpula de latón. La vida después de la muerte implicaba un descenso al vil submundo, donde se pasaba la eternidad en una existencia deplorable, en una especie de infierno.

Creían que los cultivos crecían porque un dios masculino se estaba apareando con su esposa diosa. Ellos veían los meses húmedos y calurosos del verano, cuando los campos y praderas se teñían de marrón, como el momento de la muerte de los dioses. Cuando los campos florecían de nuevo en primavera, creían que sus dioses resucitaban. Marcaron a éste, como el comienzo del año, que era celebrado en sus templos con música y cantos.

No creían en el cambio social, aunque los sacerdotes sumerios alteraban las historias que contaban, creando nuevos giros en los cuentos antiguos; sin reconocer esto como un cambio inducido por los humanos o preguntándose por qué habían fallado en hacerlo bien la primera vez. Las nuevas ideas eran simplemente revelaciones de sus dioses.

Había diferentes tipos de sacerdotes. Algunos de los más comunes eran:

Los templos sumerios consistían en una nave central con corredores en ambos lados, flanqueados por aposentos para los sacerdotes. En una de las puntas del corredor se encontraba un púlpito y una plataforma construida con ladrillos de barro, usada para sacrificios animales y vegetales.

Los graneros y depósitos generalmente se localizaban en la proximidad de los templos. Más tarde, los sumerios comenzaron a construir sus templos en la cima de las colinas artificiales, terraplenadas y multifacetadas: esos templos especiales se llamaban zigurats.

Los sumerios fueron precursores de muchas conceptos religiosos , sagas cosmogónicas y relatos que luego aparecieron recogidas por otros pueblos mesopotámicos y regiones vecinas . Entre ellas podemos citar: la creación del mundo, la separación de las aguas primordiales, la formación del hombre con arcilla o las ideas del paraíso y el Diluvio Universal (que aparece en la Epopeya de Gilgameš). Escritos de V. Scheil y S. N. Kramer, consideran la creación de Eva a partír de la costilla de Adán como un mito sumerio, ya que en sumerio, las palabras "hacer vivir" y "costilla" se escribían igual: "ti". También la idea de la resurrección de los muertos, atribuida a innumerables religiones, aparece en Sumeria por primera vez.

Los sumerios mantenían una producción de cebada, garbanzo, lentejas, mijo, trigo, nabo, dátiles, cebolla, ajo, lechuga, puerro y mostaza. También criaban ganado, cordero, cabra y puerco. Además de eso, usaban novillos como opción principal en el trabajo de carga y burros como animal de transporte. Los sumerios pescaban peces y cazaban aves gallináceas.

La agricultura sumeria dependía mucho del riego, efectuándose a través del uso de canales, estanques, diques y depósitos de agua. Las frecuentes y violentas inundaciones del Tigris, y en menor medida, del Éufrates, hacían que los canales necesitaran de reparación frecuente y de la continua extracción del limo, y el reemplazo continuo de los marcadores de inspección y mojones. El gobierno ordenaba a determinados ciudadanos la tarea de trabajar en los canales, aunque los ricos podían excluirse de esta tarea.

Después de la temporada de inundaciones y luego de la temporada de Equinoccio de Primavera y Akitu o Festival de Año Nuevo, con el uso de canales, los granjeros irrigaban sus campos y entonces drenaban el agua. Posteriormente dejaban que los novillos macerasen la tierra y matasen las hierbas dañinas. El paso siguiente era dragar los campos con picos. Después que se secara, araban, gradaban y rastrillaban el campo tres veces, revolviendo la tierra después con una azada antes de la siembra. Lamentablemente, la alta tasa de evaporación dio lugar a un aumento gradual de la salinidad de los campos. Por el período de Ur III, los agricultores pasaron del trigo a la cebada como principal cultivo, ya que ésta es más tolerante a la sal.

Los sumerios realizaban la cosecha durante la fase seca del otoño en equipos de tres personas que consistían en dos segadores y un enfardador. Los campesinos utilizaban un tipo de cosechadora arcaica para separar la cabeza de los cereales de sus respectivos tallos: una especie de carro de clasificación, que separaba los granos de los cereales. Después cribaban la mezcla de granos y barcia.

Las casi constantes guerras, durante 2000 años, entre las ciudades estado sumerias ayudaron a desarrollar la técnica y tecnología militar de Sumeria a un alto nivel. La primera guerra que se registra fue entre Lagaš y Umma en el año 2525 a. C. en una estela llamada la Estela de los Buitres. Este registro también muestra al rey de Lagaš liderando un ejército sumerio compuesto en su mayoría de infantería. Los soldados de infantería llevaban lanzas, cascos de cobre y escudos de cuero o mimbre. Los lanceros se muestran dispuestos en lo que parece ser una formación de falange, que requiere entrenamiento y disciplina. Esto implica que los sumerios hayan hecho uso de soldados profesionales.

La influencia clave en el ejército sumerio fue su paupérrima posición estratégica. Los obstáculos naturales para la defensa existían solamente en las fronteras del oeste (desierto) y del sur (golfo Pérsico). Cuando los enemigos más populosos y poderosos aparecían por el norte o el este, los sumerios se volvían susceptibles a los ataques. Los sumerios participaban en guerras con sitio entre sus ciudades, defendidas por murallas de ladrillos de barro que, obviamente, no podían detener los enemigos que ya conocían ese material.

Los sumerios inventaron el carruaje, al cual ataban onagros (burros salvajes). Esas carrozas antiguas no funcionaban tan bien en combate como los modelos construidos posteriormente. Algunos sugieren que las carrozas servían primariamente como medio de transporte, aunque en tiempo de guerra transportaban hachas de guerra y lanzas. La carroza o carruaje sumerio constituía de un dispositivo de cuatro ruedas manejado por un equipo de dos personas y atado a cuatro onagros. La carroza estaba compuesta por cestas entretejidas, y las ruedas poseían un sólido diseño de tres piezas.
Los sumerios usaban fundas y arcos simples, más tarde la humanidad inventaría el arco compuesto.

La planicie del Tigris-Éufrates carecía de minerales y árboles. Las edificaciones sumerias comprendían estructuras planoconvexas hechas de ladrillos de barro, desprovistas de argamasa o cemento. Debido a que los ladrillos planoconvexos eran de composición relativamente inestable, los albañiles sumerios añadían una mano extra de ladrillos, puestos perpendicularmente cada pocas hiladas. Entonces ahí, rellenaban los huecos con betume.

Las construcciones hechas con ladrillos de barro se acababan deteriorando, de forma que eran periódicamente destruidas, niveladas y reconstruidas en el mismo lugar. Esa constante reconstrucción elevó gradualmente el nivel de las ciudades, de modo que se erigieron por arriba de la planicie a su alrededor. Las construcciones resultantes se conocían con el nombre de tell y se encontraban en todo el antiguo Oriente Próximo.

El tipo más famoso e impresionante de entre las edificaciones sumerias, eran los Zigurats o torres escalonadas, una construcción de largas y amplias plataformas sobrepuestas en cuya cima había templos. Algunos académicos han teorizado que estas estructuras podrían haber sido la base de la torre de Babel bíblica, que se describe en el Génesis.

Los sellos cilíndricos sumerios también describen casas construidas con cañas, similares a aquellas construidas por los árabes de las tierras bajas de la parte sur de Irak, hasta una fecha tan reciente como el 400 a. C. Por otro lado, los templos sumerios y palacios hicieron uso de materiales y técnicas más avanzadas como refuerzos (soportes para los ladrillos), recesos (esquinas), pilastras y clavos de arcilla.

Los sumerios desarrollaron un complejo sistema de metrología alrededor del 4000 a. C. Esta metrología avanzada resultó en la creación de la aritmética, la geometría y el álgebra. Desde el 2600 a. C. en adelante, los sumerios escribieron tablas de multiplicación en tabletas de arcilla y trataron con ejercicios geométricos y problemas de división. Los primeros rastros de la numeración babilónica también se remontan a este periodo. El periodo que abarca desde el 2700 al 2300 a. C. vio la primera aparición del ábaco, y una tabla de columnas sucesivas que delimitaron el orden sucesivo de magnitud de su sistema de numeración sexagesimal. Los sumerios fueron los primeros en usar un sistema de numeración de notación posicional. Otros pueblos mesopotámicos quizás hayan usado algún tipo de regla de cálculo en cálculos astronómicos. 

Una tablilla encontrada en Nippur puede ser considerada el primer manual de medicina del mundo. En esa tablilla, donde había fórmulas químicas y mágicas (encantamientos), usaban términos tan especializados que para traducirse se precisó de la ayuda de químicos.

En la farmacia, se usaban sustancias vegetales, animales y minerales. Laxantes y diuréticos fueron la mayoría de los remedios de aquel pueblo. Determinadas cirugías también eran puestas en práctica. Los sumerios manufacturaban salitre, conseguido a partir de la orina, la cal, de cenizas o de la sal. Combinaban esos materiales con leche, piel de cobra, caparazón de tortuga, casia, mirto, timo, sauces, higo, pera, abeto y/o dátil. A partir de ahí, mezclaban esos agentes con vino, usando el resultado obtenido de dos formas: o pasando el producto como si fuera una crema, o luego se mezclaba junto con la cerveza, consumiendo el remedio por vía oral.

Los sumerios explicaban la enfermedad como una consecuencia del aprisionamiento, y la consecuente tentativa de escape, de un demonio dentro del cuerpo humano. El objetivo del remedio era persuadir al demonio a creer que continuar residiendo en aquel cuerpo sería una experiencia desagradable. Comúnmente los sumerios colocaban un cordero o una cabra cerca del enfermo. En el caso de no haber ovejas a disposición, probaban suerte con una estatua, que, si se conseguía transferir el demonio dentro de sí, sería cubierta de betún.

La literatura sumeria comprende tres grandes temas: mitos, himnos y lamentaciones. Los mitos se componen de breves historias que tratan de perfilar la personalidad de los dioses mesopotámicos: Enlil, principal dios y progenitor de las divinidades menores; Inanna, diosa del amor y de la guerra, o Enki, dios del agua potable frecuentemente enfrentado a Ninhursag, diosa de las montañas. Los himnos son textos de alabanza a los dioses, reyes, ciudades o templos. Las lamentaciones relatan temas catastróficos como la destrucción de ciudades o templos y el abandono de los dioses resultante.

Algunas de estas historias es posible que se apoyasen en hechos históricos como guerras, inundaciones o la actividad constructora de un rey importante, magnificados y distorsionados con el tiempo.

Una creación propia de la literatura sumeria fue un tipo de poemas dialogados basados en la oposición de conceptos contrarios. También los proverbios forman parte importante de los textos sumerios.

Los sumerios tal vez sean más recordados debido a sus muchas invenciones. Algunos especialistas les dan el crédito por la invención de la rueda y el torno alfarero. Su sistema de escritura cuneiforme fue el primer sistema de escritura del que se tenga evidencia, adelantándose a los jeroglíficos egipcios en, por lo menos, 75 años. Los sumerios estaban entre los primeros astrónomos, poseyendo la primera visión heliocéntrica de la que se tenga conocimiento (la próxima aparecería de vuelta en el 1500 a. C. por parte de los Vedas en la India). Afirmaban también que el sistema solar se constituía de cinco planetas (ya que únicamente sólo se podían ver cinco planetas a simple vista).

Desarrollaron también conceptos matemáticos usando sistemas numéricos basados en 6 y 10. A través de ese sistema, inventaron el reloj con 60 segundos, 60 minutos y 12 horas, además del calendario de 12 meses que usamos actualmente. También construyeron sistemas legales y administrativos con cortes judiciales, prisiones y las primeras ciudades estado. La invención de la escritura posibilitó a los sumerios el almacenamiento del conocimiento y la posibilidad de transferirlo a otros. Eso llevó a la creación de las escuelas, a la educación y oficialización de la matemática, religión, burocracia, división de trabajo y sistemas de clases sociales.

Los sumerios también inventaron la carroza y, posiblemente, las formaciones militares. Inventaron la cerveza. Lo más importante de todo, tal vez, sea el hecho que de acuerdo con muchos académicos, los sumerios fueron los primeros en tratar tanto plantas como animales. En el caso de lo primero, a través de plantaciones sistémicas y de la cosecha de una descendencia de grama mutante, conocida actualmente como einkorn, y de simientes de mijo y trigo. Con relación a lo segundo, los sumerios domesticaron a través del confinamiento y de la procreación de carneros ancestrales (similares a la cabra montés y al ganado salvaje (búfalos). Fue la primera vez que esas especies fueron domesticadas y criadas a gran escala.






</doc>
<doc id="2673" url="https://es.wikipedia.org/wiki?curid=2673" title="Santa Fe">
Santa Fe

Santa Fe puede hacer referencia a:























</doc>
<doc id="2676" url="https://es.wikipedia.org/wiki?curid=2676" title="Sinople">
Sinople

En heráldica, sinople, sínople e incluso sinoble (del francés "sinople" y este nombre del de la antigua ciudad de Sinope) es la denominación del color verde. De entre los esmaltes heráldicos, pertenece al grupo de los colores, junto con el gules (rojo), el azur (azul), el sable (negro) y el púrpura.

Una de las primeras menciones de un escudo verde se encuentra en el "Roman de Troie", de Benoît de Sainte-Maure, que data de alrededor del año 1155, donde este esmalte es descrito con el nombre que recibía en Francia en ese momento: "vert" (‘verde’).

En los inicios de la heráldica, entre los siglos XII y XIII, este era el esmalte menos utilizado: el historiador Michel Pastoureau halló que aparecía en menos del 5 % de las armerías europeas. Para explicar esta escasa presencia del verde en los escudos, se ha argüido que hasta el siglo XIII este color tuvo connotaciones negativas (el Diablo y sus criaturas, el islam, la inestabilidad); que hasta fines del siglo XIV fue difícil de fabricar y de fijar satisfactoriamente; o que no destacaba contra el verde de la hierba.

Sin embargo, durante el siglo XIV la Europa occidental experimentó una revalorización del color verde y, si bien este color nunca fue ampliamente usado en armerías, hacia comienzos del siglo XV ya estaba bien establecido dentro del canon heráldico.

Hasta principios del siglo XIV, el término "sinople" se empleaba en la literatura francesa como designación poética del color rojo. Este vocablo derivaba de "sinope", "sinopis", palabras latinas que en la antigüedad clásica se referían por lo general al rojo, en alusión a una clase de ocre rojo muy apreciado que se extraía en Capadocia y se exportaba desde el puerto de Sinope, en Anatolia. Aun después de su adopción por parte de la heráldica con el significado de «verde», "sinople" conservó su significado literario de «rojo» durante unos dos siglos más.

Se desconoce por qué motivo la palabra "sinople" experimentó ese cambio de significado en la jerga heráldica; Pastoureau ubica este cambio entre los años 1380 y 1400, o tal vez unas décadas antes. Se ha sugerido que los heraldos franceses cambiaron el nombre del esmalte "vert" por "sinople" debido a que "vert" era homófono con "vair" (‘veros’), un forro heráldico.

En los reinos españoles, recién a mediados del siglo XV el término heráldico francés "sinople" comenzó a ser traducido como esmalte heráldico verde, en lugar de rojo.

En cuanto al aparente despropósito de la adopción del nombre de un pigmento rojo para designar a un esmalte verde, el jesuita y heraldista Claude-François Menestrier (1631–1705), en su obra "L' Art du blason justifié", lo explica citando parte del texto de un folleto manuscrito que se remontaría a alrededor del año 1400, y que trataba acerca de colores para pintura e ilustración. En su libro, Menestrier copia un capítulo de este folleto, en donde se menciona un pigmento llamado "sinoplum", el cual —siempre según el manuscrito— provenía de la «ciudad de Sinopoli» y era a veces rojo y a veces verde. Esto sugiere que en algún momento se llamó "sinoplum" al renombrado ocre rojo de Anatolia, el actual rojo carmesí, y también a alguna clase de pigmento verde, igualmente importado.

El sinople no se encuentra definido con exactitud. En consecuencia, el tono y el matiz de verde a emplear para representarlo quedan a criterio del artista heráldico. Se recomienda, sin embargo, que el verde sea fuerte y fiel a su naturaleza; no debe inclinarse demasiado hacia el amarillo ni hacia el azul.

Cuando no se dispone de colores, el sinople puede representarse mediante un rayado muy fino de líneas oblicuas paralelas que van desde el ángulo superior izquierdo del dibujo hasta el inferior derecho, según el método atribuido al jesuita Silvestre Pietra Santa. Este es el método de representación que se ve comúnmente en grabados a una tinta.

Debajo se presentan dos ejemplos antiguos y notables del uso del esmalte sinople.


Amadeo VI, duque de Saboya (1334–1383) fue apodado el «Conde Verde» debido a que solía vestir de este color. Si bien los colores de las armas de los Saboya eran plata y gules, durante la vida del Conde Verde se sumó el sinople a la librea de la Casa de Saboya. Estos tres colores, bastante más tarde, darían origen a la actual bandera de Italia.

De entre las figuras heráldicas, suelen ser de sinople las y los montes, en representación del color de la hierba, aunque las reglas de la heráldica no impiden que se les asigne cualquier otro color.

Hacia el inicio del Renacimiento se desarrolló un sistema de correspondencias simbólicas para los colores heráldicos que hoy se encuentra en desuso.
Es de notar que hacia 1828 este sistema era considerado absurdo por el heraldista inglés William Berry, aunque el español Francisco Piferrer, en 1858, lo comenta como si todavía fuese válido.

Si bien Jean Courtois, Heraldo Sicilia del Reino de Aragón, menciona en su tratado "Le blason des couleurs" (1414) que cualquiera de estas asociaciones del sinople puede usarse para blasonar, en la práctica es posible que solamente se hayan usado el sistema planetario y el sistema de piedras preciosas. Para Alberto y Arturo García Caraffa (1919), el blasonado con gemas correspondía a los títulos y el de planetas a los soberanos.
Arthur Fox-Davies cita un ejemplo de blasonado con piedras preciosas que data de 1458.

Debajo se dan algunas de las antiguas correspondencias simbólicas del sinople, así como algunos de los nombres «griegos» que se le atribuyeron.

Además, de acuerdo con Courtois, el sinople sería considerado por algunos como el «menos noble» de los colores heráldicos.

Los metales heráldicos:

Los otros esmaltes heráldicos principales:
Y además:


</doc>
<doc id="2677" url="https://es.wikipedia.org/wiki?curid=2677" title="Sable">
Sable

El sable es un arma blanca curva y (generalmente) de un solo filo, pensada para cortar, habitualmente usada en caballería e infantería (oficiales) en el siglo XIX e incluso XX. Este carácter curvo de la hoja y su filo único, diferencia tradicionalmente al sable de la espada.

Esta arma blanca es de tajo y surgió por la necesidad de velocidad en combate. Esta se logra al cortar y no dejar incrustada la hoja del arma en el cuerpo del adversario (al contrario de la mayoría de las espadas de una mano, que son de estocada). 

La curvatura, que está ubicada generalmente desde la punta hasta la mitad del sable, genera un tajo profundo.

La curvatura del sable pretende conseguir, en teoría, que un hombre a caballo, al descargar el brazo con esta arma, dibuje un amplio círculo sobre el infante logrando que en el punto de corte el sable siempre sea tangencial. Por esta razón no se ensarta, sino que corta, con lo que aumenta la herida sin clavar el arma. Debido a ello los sables pensados para caballería tienen una gran curvatura, son casi circulares; los pensados para infantería poseen una curvatura menor, pues debe concederse importancia a la función defensiva: mantener alejado al enemigo y parar sus golpes.

El sable moderno es, junto con la espada y el florete, una de las tres armas de esgrima. Deriva del arma que usaban los soldados de caballería. Tiene un protector en forma de cuenco, que se curva bajo la mano, y una hoja en forma de T en sección transversal. La longitud del sable es de 90 cm y su peso máximo es de 500 g. Los tocados o puntos se pueden conseguir embistiendo con la punta o produciendo un corte con el filo de la hoja. El blanco válido es todo el cuerpo de cintura para arriba, incluyendo cabeza y brazos. Los asaltos de sable son los más rápidos y ágiles en esgrima, por lo que requieren una buena forma física.

La danza del sable ("raks al sayf") es originaria de la danza marcial tradicional de Egipto "El Ard", que es realizada por hombres que llevan los sables en forma vertical, listos para pelear, mientras bailan. "Raks al Sayf" implica balancear el objeto sobre la cabeza, cadera, estómago, hombros, etc. No existe mucha documentación que indique que la danza del sable bailada por mujeres sea común, salvo algunas pinturas.



</doc>
<doc id="2678" url="https://es.wikipedia.org/wiki?curid=2678" title="Sanguíneo">
Sanguíneo

En heráldica, sanguíneo ("sanguine" en inglés) es la denominación de un color rojo oscuro. Es muy poco utilizado, y su uso se limita a las armerías de las naciones angloparlantes.

En la heráldica occidental, los colores universalmente aceptados son siete: oro, plata, gules, azur, sable, sinople y púrpura. Los demás esmaltes y metales son de invención posterior y suelen restringirse a la heráldica de determinada nación o región; tal es el caso del sanguíneo.
En inglés, los colores heráldicos llevan nombres derivados de la heráldica francesa, excepto el sanguíneo ("sanguine") y el morado ("murrey"), cuyos nombres son denominaciones cromáticas que eran de uso común en el idioma inglés al momento del establecimiento del color heráldico sanguíneo. "Sanguine" (del francés antiguo "sanguin", fem. "sanguine", y este del latín "sanguineus", ‘relativo a la sangre’) era a comienzos del siglo XIV la denominación de una variedad de tela roja, aunque hacia fines del siglo XIV ya se registra el uso de "sanguine" como denominación de color con el significado de ‘rojo sangre’.

En la heráldica inglesa este color no se considera esmalte, metal ni forro, sino que se encuentra en una categoría aparte denominada «mancha» ("stain"), junto con otros dos colores: el leonado ("tenné") y el morado ("murrey"). Algunos heraldistas, históricamente, señalaron que estas «manchas» eran los colores indicados para agregar a los escudos brisuras denotativas de infamia, pero otros autores, al no haber encontrado ejemplos de lo antedicho, dudan de que alguna vez estas brisuras se hayan llevado a la práctica. Por otra parte, el sanguíneo se ha usado de la misma manera que los demás colores heráldicos, sin que parezca tener connotaciones infamantes.

La coloración del sanguíneo heráldico no se encuentra definida con exactitud, por lo que su tono y matiz quedan a criterio del artista heráldico. Se recomienda, sin embargo, que el color empleado sea intenso y fiel a su naturaleza, a riesgo de que pueda confundirse con otro color heráldico, como el gules o el morado.

Cuando no se dispone de colores, el color sanguíneo puede representarse mediante un entramado de líneas horizontales y diagonales que se cruzan, como se ve a la izquierda de estas líneas, aunque el patrón indicado no es el único que existe para este esmalte. Este es el método de representación que se ve comúnmente en grabados a una tinta.

En la heráldica alemana existe un esmalte similar llamado "Blut" (‘sangre’) o "Blutrot" (‘rojo sangre’).


</doc>
<doc id="2679" url="https://es.wikipedia.org/wiki?curid=2679" title="Síndrome de adaptación espacial">
Síndrome de adaptación espacial

El síndrome de adaptación espacial es la forma específica de cinetosis que sufren los astronautas durante un viaje por el espacio y su causa es la ausencia de gravedad. Reduce el rendimiento de los astronautas durante los primeros días de vuelo espacial, pero normalmente la adaptación se produce a los pocos días. Debe evitarse el movimiento excesivo que empeora los síntomas.



</doc>
<doc id="2680" url="https://es.wikipedia.org/wiki?curid=2680" title="Salud">
Salud

La salud (del latín "salus, -utis") es un estado de bienestar o de equilibrio que puede ser visto a nivel subjetivo (un ser humano asume como aceptable el estado general en el que se encuentra) o a nivel objetivo (se constata la ausencia de enfermedades o de factores dañinos en el sujeto en cuestión). El término salud se contrapone al de enfermedad, y es objeto de especial atención por parte de la medicina y de las ciencias de la salud.

La salud es un estado de completo bienestar físico, mental y también social, no solamente la ausencia de enfermedad o dolencia, según la definición presentada por la Organización Mundial de la Salud (OMS) en su constitución aprobada en 1948. Este concepto se amplía a: «La salud es un estado de completo bienestar físico, mental y social, y no solamente la ausencia de afecciones o enfermedades». En la salud, como en la enfermedad, existen diversos grados de afectación y no debería ser tratada como una variable dicotómica. Así, se reformularía de la siguiente manera: «La salud es un estado de bienestar físico, mental y social, con capacidad de funcionamiento, y no sólo la ausencia de afecciones o enfermedades». También puede definirse como el nivel de eficacia funcional o metabólica de un organismo tanto a nivel micro (celular) como a nivel macro (social).

Dentro del contexto de la promoción de la salud, la salud ha sido considerada no como un estado abstracto, sino como un medio para llegar a un fin, como un recurso que permite a las personas llevar una vida individual, social y económicamente productiva. La salud es un recurso para la vida diaria, no el objetivo de la vida. Se trata de un concepto positivo que acentúa los recursos sociales y personales, así como las aptitudes físicas.

La forma física es la capacidad que tiene el cuerpo para realizar cualquier tipo de ejercicio donde muestra que tiene resistencia, fuerza, agilidad, habilidad, coordinación y flexibilidad.

Existe también la salud mental, la cual se caracteriza por el equilibrado estado emocional de una persona y su autoaceptación (gracias al autoaprendizaje y al autoconocimiento); en términos clínicos, es la ausencia de cualquier tipo de enfermedad mental. 

Estas definiciones han sido cuestionadas ya que se la considera una definición ideal, puesto que no toda la población alcanzaría ese estado. Hoy asumimos que la salud es un proceso en el cual el individuo se desplaza sobre un eje salud-enfermedad acercándose a uno u otro extremo según se refuerce o rompa el equilibrio.

La salud se concibe como la posibilidad que tiene una persona de gozar de una armonía biopsicosocial, en interacción dinámica con el medio en el cual vive.

No obstante, el concepto de salud ("buena salud") es subjetivo. Muchas personas se han acostumbrado a vivir con un estado de mala salud crónica como si fuera normal, influenciadas por el entorno social o familiar, sus vivencias personales que le imposibilitan contrastar con una situación de buena salud y, en ocasiones, la falta de apoyo o soluciones por parte de los profesionales de la salud, entre otras razones. Este hecho está impidiendo el reconocimiento y diagnóstico de trastornos que, sin tratamiento, pueden provocar consecuencias graves sobre la salud. Como ejemplos destacados cabe citar la enfermedad celíaca o la malnutrición en personas de edad avanzada. Asimismo, esta aceptación de una mala salud crónica como algo esperable o normal y la falta de concienciación acerca de la importancia de la prevención, conducen a un bajo seguimiento de los tratamientos prescritos en enfermedades crónicas diagnosticadas, con las consiguientes respercusiones negativas sobre la salud.

La alimentación es el principal factor que influye sobre la salud. Una buena salud se consigue mediante una dieta equilibrada, con una gran variedad de alimentos, equilibrio entre calorías, ingerir las comidas diarias recomendadas, entre otros. 

Podemos mirar en la pirámide alimentaria los alimentos para una nutrición sana y equilibrada.
Sin una nutrición saludable, se pueden contraer enfermedades como lo son: obesidad, desnutrición, etc.; se deben consumir pocas grasas y lípidos, muchas frutas y verduras, los productos de origen animal se deben consumir de manera regular, los cereales se deben consumir de manera constante, antes de cada comida se deben lavar frutas y verduras.
En la nutrición, un dato muy importante es la higiene que es necesaria para evitar enfermedades estomacales.
No debemos olvidar el ejercicio que sirve para una buena digestión.
También es muy importante no ponernos a dieta sin instrucciones de un especialista, ya que no es seguro.
Lo mejor, es comer todos los alimentos que nos ofrece la pirámide alimentaria, lo importante, es consumirlas en porciones adecuadas.
La Dieta mediterránea está considerada como altamente saludable, ya que algunos de los compuestos bioactivos presentes en ella incluyen compuestos fenólicos, isoprenoides y alcaloides que contribuyen a efectos saludables comúnmente asociados a dicha dieta.

La práctica regular de actividad física en cualquier edad produce un bienestar y mejora tanto en el estado de ánimo como físicamente.

El ejercicio físico es cualquier movimiento corporal repetido con el propósito de conservar la salud o mejorarla. A menudo también es dirigido hacia el mejoramiento de la capacidad atlética y/o la habilidad. El ejercicio físico regular es un componente necesario en la prevención de algunas enfermedades como problemas cardíacos, enfermedades cardiovasculares, Diabetes mellitus tipo 2, sobrepeso, dolores de espalda, entre otros.

El ejercicio físico se debe practicar con mesura y de forma equilibrada, prestando atención a los cambios físicos internos para aprender a comprender la relación causa-efecto entre el movimiento físico concreto y su efecto directo con los cambios internos percibidos.

Recomendable porque puede llevar a un desgaste físico de ciertas partes del cuerpo. Por eso, cabe insistir en el equilibrio de fuerzas, tanto internas como externas, y a ello ayuda el autoconocimiento mediante un crítico autoanálisis (autoexámenes de conciencia mientras se desarrolla la actividad física).

El ejercicio físico es necesario para una salud equilibrada; además, debe complementarse con una dieta equilibrada y una adecuada calidad de vida. Sus beneficios pueden resumirse en los siguientes puntos:


La cantidad mínima para prevenir enfermedades es de 30 minutos diarios de actividad física moderada. Otros hábitos que deben combinarse con la realización de ejercicios son: la buena alimentación, el descanso adecuado, la higiene y evitar el consumo de sustancias perjudiciales para el organismo, como el tabaco, el alcohol y otros estimulantes.

El descanso es necesario para que se produzcan en nuestro cuerpo las diferentes adaptaciones que aporta la actividad física y para que se produzca una mejora del rendimiento corporal.

La higiene es el conjunto de conocimientos y técnicas que aplican los individuos para el control de los factores que ejercen o pueden ejercer efectos nocivos sobre su salud. La higiene personal es el concepto básico del aseo, de la limpieza y del cuidado del cuerpo humano.
La higiene es un elemento imprescindible para la salud, ya que mantiene la limpieza del cuerpo, los cabellos y los dientes, cosa que previene infecciones y enfermedades.

La salud mental es un concepto que se refiere al bienestar emocional y psicológico del individuo. "Merriam-Webster" define salud mental como: «el estado del bienestar emocional y psicológico en el cual un individuo pueda utilizar sus capacidades cognitivas y emocionales, funcionar en sociedad, y resolver las demandas ordinarias de la vida diaria».

Según la OMS, no hay una definición oficial de salud mental. Las diferencias culturales, evaluaciones subjetivas, y la competición de teorías profesionales, hacen difícil definir "la salud mental". En general, la mayor parte de expertos convienen en que la salud mental y las enfermedades mentales no son excluyentes. En otras palabras, la ausencia de un desorden mental reconocido, no es necesariamente un indicador de contar con salud mental (probablemente debido al desconocimiento de la gran variedad de estados mentales aún por definir, y la corta edad de la ciencia médica en general tal como la conocemos hoy en día, y en especial de la ciencia que intenta definir con más exactitud estos trastornos o complejos salud-enfermedad que proponen tanto la psicología como la psiquiatría).

En la antigua Grecia nada se sabía de virus y bacterias, pero ya reconocían que la personalidad y sus características, desempeñan un rol fundamental en los orígenes de la enfermedad.

Galeno, una figura gigantesca del mundo antiguo, ya observó la existencia de un vínculo muy estrecho entre la melancolía y el cáncer de mama. De este modo, en estos primeros enfoques médicos, encontramos tempranamente un criterio holístico en la consideración de la salud y la enfermedad.

Platón remarcaba que la buena educación es la que tendía con fuerza a mejorar la mente juntamente con el cuerpo. Reconocía, de alguna manera, que la salud corporal conduce a la higiene mental, pero, al mismo tiempo, que el buen estado mental predispone al buen estado corporal. Así, establecía, específicamente, que el alma "buena", por su propia excelencia, mejora al cuerpo en todo sentido.

En los tiempos actuales, desde el siglo XX, especialmente, pero también desde mucho antes –e incluso en la medicina oriental antigua–, se comienza a reconocer la necesidad de concepción holística de la salud.

La concepción psicosomática nos obliga a atender nuestra interioridad como causa posible de perturbaciones del cuerpo. Esto es reconocido unánimemente por la clínica occidental, que ve que en los consultorios un altísimo porcentaje de consultas responde a distorsiones de la mente o de la personalidad, en sentido amplio.

Este nuevo enfoque no es dualista a la manera cartesiana. Concibe al hombre como una unidad, en la que con mucha frecuencia anidan los poderes curativos, que estimulados, ayudan a resolver los problemas somáticos. La filosofía médica no materialista de este modo va incrementándose en el mundo en que pudo predominar la medicina convencional.

Son todas aquellas actividades que presencian consecuencias nocivas y peligrosas para nuestra salud. Las más relevantes son:

Según el reporte de LaLonde, del año 1974 realizado en Canadá, se sugiere que existen cuatro determinantes generales que influyen en la salud, a los cuales se les llamó: "biología humana", "ambiente", "forma de vida" y la "organización del cuidado de la salud". De esta manera, la salud es mantenida por la ciencia y la práctica de medicina, pero también por esfuerzo propio. "Fitness", una dieta saludable, manejar el estrés, el dejar de fumar y de abusar de otras sustancias nocivas, entre otras medidas, son pasos para mejorar la salud de alguien. Por otra parte, el estilo de vida es el conjunto de comportamientos o aptitudes que desarrollan las personas, es decir, pueden ser saludables o nocivas para la salud y además podemos encontrar que es la causa de las enfermedades dentro del factor huésped.

Tener una dieta equilibrada, que incluya todos los grupos de alimentos, y realizar actividad física moderada con regularidad (150 minutos de ejercicio a la semana) son factores clave en la mejora de salud; además de no fumar, tener un consumo moderado de alcohol, comer cinco piezas de frutas y verduras al día y tener un peso adecuando a la talla de la persona. Estos cambios en los hábitos de vida combatiría enfermedades cardiovasculares crónicas y diabetes.

Es el estudio de la vida del ser humano o la información genética que cada individuo trae en sus genes, puede proteger o favorecer la aparición de enfermedades.
Dentro del factor biológico podemos destacar las enfermedades adquiridas por el medio como el dengue o el mal de chagas.

Son todos aquellos factores que provienen del exterior y sobre los cuales el ser humano "no tiene control".

Un informe, publicado el 4 de marzo de 2008 por la Organización para la Cooperación y el Desarrollo Económico (OCDE), advierte que "la contaminación del aire va a tener efectos crecientes sobre la salud a nivel mundial"; y si no se hace nada para remediarlo —como ha venido sucediendo hasta ahora—, advierte, en 2030 "el número de fallecimientos prematuros relacionados con el ozono troposférico se multiplicará por cuatro".

Son todos aquellos factores que provienen del exterior y sobre los cuales el ser humano sí tiene control. Los productos químicos domésticos alteran gravemente el ambiente doméstico y pasan a las personas a través de los alimentos a los cuales contaminan fácilmente por estar almacenados en los mismos habitáculos durante periodos de tiempo.

Para completar una forma de vida saludable es necesario seguir ciertas pautas tanto alimentarias como de hábitos de ejercicio físico.

En primer lugar una dieta equilibrada requiere la ingesta controlada y equilibrada consistente en una alta ingesta de verduras, frutas, legumbres y cereales —que contienen antioxidantes y fibra— y pescado, rico en ácidos grasos y omega 3. También son recomendables, en menor cantidad, carnes blancas, carnes rojas, con mucho control sobre estas últimas al contener grasas saturadas.

Por su parte, los hábitos de ejercicio físico son imprescindibles para quemar el exceso de calorías ingeridas, y tonificar músculos y huesos con vistas a la vejez. Su práctica reduce las probabilidades de padecer enfermedades de corazón, enfermedades relacionadas con la presión arterial y el colesterol.

En la parte de los hábitos tóxicos, cabe destacar el alcohol y el tabaco como unas de las fuentes más perjudiciales para la salud en tanto en cuanto a la gran extensión entre la población de estos hábitos.

El proceso que permite fortalecer los conocimientos, aptitudes y actitudes de las personas para participar responsablemente en el cuidado de su salud y para optar por estilos de vida saludables, facilitando el logro y conservación de un adecuado estado de salud individual, familiar y colectivo mediante actividades de participación social, comunicativa y educativa para la salud.

La promoción también está relacionada con la prevención. Te da el control sobre riesgos a enfermedades y cambia el estilo de vida a uno más saludable. Mientras sea acompañado por una dieta balanceada, crear una rutina de ejercicios y evitar situaciones que causen estrés, todo esto con el objetivo de disminuir el riesgo a enfermedades.



</doc>
<doc id="2681" url="https://es.wikipedia.org/wiki?curid=2681" title="Sistema nervioso">
Sistema nervioso

El sistema nervioso es un conjunto organizado de células especializadas en la conducción de señales eléctricas. La célula básica del sistema nervioso de todos los animales es la neurona. Las neuronas tienen la función de coordinar las acciones de los animales por medio de señales químicas y eléctricas enviadas de un lugar a otro del organismo. La mayor parte de los animales pluricelulares tienen sistemas nerviosos con características básicas similares, aunque con grado de complejidad muy variable. Únicamente carecen de él los animales que no tienen tejidos y órganos bien diferenciados, como los poríferos (esponjas), placozoos y mesozoos. 

El sistema nervioso capta estímulos del entorno (estímulos externos) o señales del mismo organismo (estímulos internos), procesa la información y genera respuestas diferentes según la situación. A modo de ejemplo podemos considerar un animal que a través de las células sensibles a la luz de la retina capta la proximidad de otro ser vivo. Esta información es transmitida mediante el nervio óptico al cerebro que la procesa y emite una señal nerviosa que a través de los nervios motores provoca la contracción de ciertos músculos con el objetivo de desplazarse en dirección contraria al peligro potencial. 



Las neuronas son las células que constituyen la unidad fundamental básica del sistema nervioso, se encuentran conectadas entre sí de manera compleja y tienen la propiedad de generar, propagar, codificar y conducir señales por medio de gradientes electroquímicos (electrolitos) a nivel de membrana axonal y de neurotransmisores a nivel de sinapsis y receptores. Los tejidos de sostén o mantenimiento está formado por las células gliales (neuroglia) y un sistema vascular especializado. 

La neurona al igual que todas las células, dispone de un citoplasma en el que existe un núcleo y diversos orgánulos como las mitocondrias y el aparato de Golgi. Su particularidad está en que del cuerpo celular arrancan diversas prolongaciones ramificadas que se llaman dendritas y otra única que recibe el nombre de axón. Las dendritas reciben la señal nerviosa en dirección al cuerpo celular, mientras que el axón la emite desde el cuerpo celular a otra neurona o una célula muscular, el axón puede dividirse en miles de ramas, cada una de las cuales lleva a la información a una célula diferente. La estructura básica del sistema nervioso está formada por redes de neuronas interconectadas por sus dendritas y axones. La zona de conexión entre dos neuronas recibe el nombre de sinapsis. 

Con base en la división morfológica entre las distintas partes anatómicas de las neuronas y sus distintas formas de organización se clasifican en cuatro tipos:

Las neuronas se clasifican también en tres grupos generales según su función:



Las neuronas se pueden comunicar entre sí gracias a impulsos eléctricos que circulan a través de sus prolongaciones. El impulso se denomina potencial de acción y es unidireccional desde el cuerpo celular al axón. En estado de reposo existe una diferencia de potencial entre el interior y el exterior de la neurona ya que ambos espacios están separados por la membrana celular, a dicha diferencia de potencial se la denomina potencial de membrana en reposo. 

Cuando se genera un potencial de acción o impulso nervioso, se producen dos fenómenos consecutivos que afectan a la membrana celular, alteran su permeabilidad a los iones Na+ y K+ y modifican el potencial de membrana en reposo. En primer lugar se abren los canales que facilitan la entrada de Na+ a la célula (despolarización), posteriormente se abren los canales de la membrana que hacen posible la salida de K+ de la célula (repolarización). El potencial de acción así generado se transmite unidireccionalmente a través del axón hasta alcanzar la siguiente conexión (sinapsis).
Se llama sinapsis a la comunicación funcional que se establece entre dos neuronas o entre una neurona y una célula muscular, mediante la sinapsis el impulso nervioso puede circular a través de varias neuronas enlazadas. La neurona de la que parte el impulso se llama presináptica y la que lo recibe se denomina postsináptica. Entre ambas existe un espacio que recibe el nombre de espacio sináptico, el cual separa las membranas de las dos células aledañas. Pueden distinguirse dos tipos de sinapsis: 

Un neurotransmisor es una sustancia química producida por las neuronas que se libera al espacio sináptico de una sinapsis química por la acción de un impulso nervioso o potencial de acción. Interacciona con un receptor específico en la neurona postsináptica donde produce una determinada respuesta que puede ser excitatoria o inhibitoria. Los neurotransmisores son un aspecto fundamental en la transmisión del impulso nervioso y resultan de gran interés en farmacología, pues muchos de los medicamentos que tienen alguna acción sobre el sistema nervioso actúan sobre ellos. 

Existen diferentes sustancias que actúan como neurotransmisores, algunas de las más importantes son las siguientes:

Las células gliales (conocidas también genéricamente como glía o neuroglía) son células del sistema nervioso que desempeñan, de forma principal, la función de soporte y protección de las neuronas. En los humanos se clasifican según su localización o por su morfología y función. Las diversas células de la neuroglía constituyen más de la mitad del volumen del sistema nervioso de los vertebrados. Las neuronas no pueden funcionar en ausencia de las células gliales.

Según su ubicación dentro del sistema nervioso ya sea central o periférico, las células gliales se clasifican en dos grandes grupos. Las células que constituyen la glía central son los astrocitos, oligodendrocitos, células ependimarias y las células de la microglía, suelen encontrarse en el cerebro, cerebelo, tronco cerebral y médula espinal. Las células que constituyen la glía periférica son las células de Schwann, células capsulares y células de Müller. Normalmente se encuentran a lo largo de todo el sistema nervioso periférico.

Por su morfología o función, entre las células gliales se distinguen las células macrogliales (astrocitos, oligodendrocitos ), las células microgliales (entre el 10 y el 15% de la glía) y las células ependimarias.

Puede dividirse en dos partes bien diferenciadas para facilitar su estudio: El sistema nervioso central que está compuesto por el encéfalo y la médula espinal, y el sistema nervioso periférico que incluye todos los nervios periféricos, tanto los nervios motores como los nervios sensitivos.

Durante el desarrollo del embrión, el tubo neural primitivo da origen a la formación de tres vesículas encefálicas que se denominan prosencéfalo, mesencéfalo y rombencéfalo. Posteriormente el prosencéfalo se divide y da origen al telencéfalo y el diencéfalo, mientras que el rombencéfalo da origen al metencéfalo y el mielencéfalo. El mesencéfalo permanece sin dividirse. De esta forma se constituyen las cinco porciones de las que surgen todas las partes del encéfalo totalmente desarrollado. 



Una división menos anatómica pero más funcional, es la que divide al sistema nervioso de acuerdo al rol que cumplen las diferentes vías neurales, sin importar si éstas recorren parte del sistema nervioso central o el periférico:



El sistema nervioso puede sufrir numerosas enfermedades de diferente origen: infecciosas, hereditarias, degenerativas, cerebrovasculares (por afectación de los vasos sanguíneos), desmielinizantes o tumorales. 

El acto reflejo es la unidad básica de la actividad nerviosa integrada y podría considerarse como el circuito primordial del cual partieron el resto de las estructuras nerviosas. Este circuito pasó de estar constituido por una sola neurona multifuncional en los diblásticos a dos tipos de neuronas en el resto de los animales llamadas aferentes y eferentes. En la medida que se fueron agregando intermediarios entre estos dos grupos de neuronas con el paso del tiempo evolutivo, como interneuronas y circuitos de mayor plasticidad, el sistema nervioso fue mostrando un fenómeno de concentración en regiones estratégicas dando pie a la formación del sistema nervioso central, siendo la cefalización el rasgo más acabado de estos fenómenos.

Para optimizar la transmisión de señales existen medidas como la redundancia, que consiste en la creación de vías alternas que llevan parte de la misma información garantizando su llegada a pesar de daños que puedan ocurrir. La mielinización de los axones en la mayoría de los vertebrados y en algunos invertebrados como anélidos y crustáceos es otra medida de optimización. Este tipo de recubrimiento incrementa la rapidez de las señales y disminuye el calibre de los axones ahorrando espacio y energía.

Otra característica importante es la presencia de metamerización del sistema nervioso, es decir, aquella condición donde se observa una subdivisión de las estructuras corporales en unidades que se repiten con características determinadas. Los tres grupos que principalmente muestran esta cualidad son los artrópodos, anélidos y cordados.

El filo de los cnidarios incluyen entre otros organismos las hidras y medusas. Presentan la forma más simple y primitiva de sistema nervioso que recibe el nombre de red nerviosa. En una red nerviosa las neuronas están dispersas sin una organización estructural compleja y no existe encéfalo.

El grupo de los equinodermos incluye la estrella de mar y el erizo de mar. Estos animales poseen sistema nervioso pero no cuentan con un encéfalo que centralice la actividad. Disponen de tres anillos nerviosos situados en planos diferentes alrededor del tubo digestivo. 

El filo de los platelmintos incluye unas 20 000 especies, entre las que se incluyen algunas de vida parasitaria como la taenia solium o solitaria que vive en el intestino humano. Su sistema nervioso presenta inicios de cefalización y 2 cordones nerviosos longitudinales que pueden considerarse un sistema nervioso central primitivo. Por otra parte el tejido nervioso contiene ya numerosas interneuronas, es decir neuronas de conexión entre las sensitivas y las motoras que aumentan la complejidad de los circuitos. 

El grupo de los anélidos incluye numerosas especies, siendo una de las más características la lombriz de tierra. Estos animales cuentan con un sistema nervioso formado por un cordon nervioso ventral doble y dos ganglios situados en cada metámero. Poseen un cerebro que está formado por la unión de dos ganglios dorsales que se comunican mediante conectivos al cordón nervioso ventral.

Dentro del grupo de los moluscos se encuentran los cefalópodos (calamares y pulpos). Estos tienen un cerebro y sistema sensorial que ha alcanzado gran desarrollo. El cerebro es comparativamente de tamaño muy grande en relación al de otros invertebrados por lo que los cefalópodos alcanzan elevadas capacidades de memoria y aprendizaje.

El grupo de los bivalvos que incluye las almejas y mejillones tiene un sistema nervioso menos desarrollado que el de los cefalópodos, probablemente por su vida sedentaria. Carecen de encéfalo pero dispones de varios ganglios que controlan diversas funciones, entre ellos dos ganglios cerebro-pleurales a ambos lados del esófago que controlan los órganos sensoriales y la cavidad del manto (moluscos). 

Los artrópodos son los animales más abundantes y variados de la tierra, incluyen los insectos, arácnidos y crustáceos. Poseen un sistema nervioso bien desarrollado que les permite tener un comportamiento complejo y coordinado. Su sistema nervioso central es de tipo ganglionar y consiste en una cadena de ganglios segmentarios unidos mediante un cordón nervioso ventral, algunos ganglios se fusionan en la región cefálica y dan lugar a un cerebro.

El sistema nervioso de los vertebrados consta de un cerebro bien desarrollado y una médula espinal. El sistema nervioso periférico está formado por diferentes nervios que se conectan con el sistema nervioso central. Estos nervios son de tipo aferente (transportan información sensorial hacia el sistema nervioso central) o eferentes (transportan ordenes motoras desde el cerebro hasta los órganos). Existen asimismo ganglios periféricos que son agrupaciones de neuronas enlazadas a algunos de los nervios pero no deben confundirse con el sistema ganglionar de los artrópodos. 

Se cree que la primera neurona surgió hace 600 millones de años, durante el período Ediacárico, en animales diblásticos como los cnidarios. La evolución del sistema nervioso en los animales ha tenido lugar a través de dos procesos claves llamados centralización y cefalización.



En los animales triblásticos o bilaterales, un grupo monofilético, existen dos tipos de planes corporales llamados protóstomos y deuteróstomos que poseen a su vez tres tipos de disposiciones del sistema nervioso siendo éstos los cicloneuros, los hiponeuros y los epineuros.Una diferencia esencial es que en protostomados y deuterostomados el SNC se encuentra en posiciones invertidas. Durante muchos años se consideró que estas y otras diferencias indicaban planes corporales y SNC esencialmente distintos, (por la posición relativa del SNC, Sistema Digestivo y vaso circulatorio principal.

Los animales diblásticos o radiados, una agrupación parafilética que engloba tanto cnidarios como a ctenóforos, normalmente cuentan con una red de plexos subectodérmicos sin un centro nervioso aparente, pero algunas especies ya presentan condensados nerviosos en un fenómeno que se entiende como el primer intento evolutivo para conformar un sistema nervioso central. Algunas disposiciones de estos condensados, como los anillos nerviosos en las medusas, recuerdan tendencias posteriores vistas en los cicloneuros.

Los animales protóstomos, que son triblásticos, como los platelmintos, nemátodos, moluscos, anélidos y artrópodos cuentan con un sistema nervioso hiponeuro, es decir es un sistema formado por ganglios cerebrales y cordones nerviosos ventrales. Los ganglios que forman el cerebro se sitúan alrededor del esófago, con conectivos periesofágicos que los unen a las cadenas nerviosas que recorren ventralmente el cuerpo del animal, en posición inferior respecto al tubo digestivo. Tal modelo de plan corporal queda dispuesto de esa forma cuando en la gástrula acontece un proceso embriológico llamado gastrorrafia.

Los animales deuteróstomos, que son triblásticos, se dividen en dos grupos según su simetría, radial o bilateral, o la disposición de su sistema nervioso, cicloneuros o epineuros. Dentro de los cicloneuros se encuentran los equinodermos (de simetría radial) y los hemicordados. El centro nervioso es un anillo situado alrededor de la boca (subectodérmico o subepidérmico). Dentro del grupo de los epineuros se encuentran los urocordados, los cefalocordados y los vertebrados en la que presentan un cordón nervioso hueco y tubular, dorsal al tubo digestivo. A partir de este cordón, en animales más complejos, se desarrolla el encéfalo y la médula espinal. Tales modelos de planes corporales quedan dispuestos de esa forma cuando en la gástrula acontecen unos procesos embriológicos llamados isoquilia en los cicloneuros o nototenia en el caso de los epineuros.




</doc>
<doc id="2682" url="https://es.wikipedia.org/wiki?curid=2682" title="Sistema">
Sistema

Un sistema (del latín "systēma", y este del griego σύστημα "sýstēma" 'reunión, conjunto, agregado') es un conjunto de componentes que se relacionan con al menos algún otro componente; puede ser material o conceptual. Todos los sistemas tienen composición, estructura y entorno, pero solo los sistemas materiales tienen mecanismo, y solo algunos sistemas materiales tienen figura (forma).

Según el sistemismo, todos los objetos son sistemas o componentes de otro sistema. Por ejemplo, un núcleo atómico es un sistema material físico compuesto de protones y neutrones relacionados por la interacción nuclear fuerte; una molécula es un sistema material químico compuesto de átomos relacionados por enlaces químicos; una célula es un sistema material biológico compuesto de orgánulos relacionados por enlaces químicos no-covalentes y rutas metabólicas; una corteza cerebral es un sistema material biológico compuesto de neuronas relacionadas por potenciales de acción y neurotransmisores; un ejército es un sistema material social y parcialmente artificial compuesto de personas y artefactos relacionados por el mando, el abastecimiento, la comunicación y la guerra; el anillo de los números enteros es un sistema conceptual algebraico compuesto de números positivos, negativos y el cero relacionados por la suma y la multiplicación; y una teoría científica es un sistema conceptual lógico compuesto de hipótesis, definiciones y teoremas relacionados por la correferencia y la deducción.

Un sistema conceptual, sistema formal o sistema ideal es un constructo compuesto por conceptos de cuatro diferentes tipos:
Así, los conceptos no son sistemas conceptuales, sino solo componentes de sistemas conceptuales. Sí son sistemas conceptuales

Un sistema material, sistema concreto o sistema real es una cosa compuesta (por dos o más cosas relacionadas) que posee propiedades que no poseen sus componentes, llamadas propiedades emergentes; por ejemplo, la tensión superficial es una propiedad emergente que poseen los líquidos pero que no poseen sus moléculas componentes. Al ser cosas, los sistemas materiales poseen las propiedades de las cosas, como tener energía (e intercambiarla), tener historia, yuxtaponerse con otras cosas y ocupar una posición en el espacio tiempo.

El esfuerzo por encontrar leyes generales del comportamiento de los sistemas materiales es el que funda la teoría de sistemas y, más en general, el enfoque de la investigación científica a la que se alude como sistemismo, sistémica o pensamiento sistémico, en cuyo marco se encuentran disciplinas y teorías como la cibernética, la teoría de la información, la teoría del caos, la dinámica de sistemas y otras.

El análisis más sencillo del concepto de sistema material es el que incluye los conceptos de composición, entorno, estructura y mecanismo (CEEM, por sus siglas). La composición de un sistema es el conjunto de sus partes componentes. El entorno o ambiente de un sistema es el conjunto de las cosas que actúan sobre los componentes del sistema, o sobre las que los componentes del sistema actúan. La estructura interna o endoestructura de un sistema es el conjunto de relaciones entre los componentes del sistema. La estructura externa o exoestructura de un sistema es el conjunto de relaciones entre los componentes del sistema y los elementos de su entorno. La estructura total de un sistema es la unión de su exoestructura y su endoestructura. Las relaciones más importantes son los vínculos o enlaces, aquellas que afectan a los componentes relacionados; las relaciones espaciotemporales no son vínculos. El mecanismo de un sistema es el conjunto de procesos internos que lo hacen cambiar algunas propiedades, mientras que conserva otras.

Además, la frontera de un sistema es el conjunto de componentes que están directamente vinculados (sin nada interpuesto) con los elementos de su entorno. La frontera de un sistema físico puede ser rígida o móvil, permeable o impermeable, conductor térmico (adiabática) o no, conductor eléctrico o no, e incluso puede ser aislante de frecuencias de audio. Además, algunos sistemas tienen figura (forma); pero no todo sistema con frontera tiene necesariamente figura. Si hay algún intercambio de materia entre un sistema físico y su entorno a través de su frontera, entonces el sistema es abierto; de lo contrario, el sistema es cerrado. Si un sistema cerrado tampoco intercambia energía, entonces el sistema es aislado. En rigor, el único sistema aislado es el universo. Si un sistema posee la organización necesaria para controlar su propio desarrollo, asegurando la continuidad de su composición y estructura (homeostasis) y la de los flujos y transformaciones con que funciona (homeorresis) —mientras las perturbaciones producidas desde su entorno no superen cierto grado—, entonces el sistema es autopoyético.



</doc>
<doc id="2683" url="https://es.wikipedia.org/wiki?curid=2683" title="Sexualidad">
Sexualidad

La sexualidad es el conjunto de condiciones que caracterizan el sexo de cada persona o animal. Desde el punto de vista histórico cultural, es el conjunto de fenómenos emocionales, de conducta y de prácticas asociadas a la búsqueda del placer sexual, que marcan de manera decisiva al ser humano en todas y cada una de las fases determinantes de su desarrollo.

Durante siglos se consideró que la sexualidad en los animales, incluyendo al ser humano, era de tipo instintiva. En esta convicción se basaron las teorías para fijar las formas no naturales de la sexualidad, entre las que se incluían todas aquellas prácticas no dirigidas a la procreación.

Sin embargo, hoy se sabe que algunos mamíferos muy desarrollados, como los delfines o algunos pingüinos, presentan un comportamiento sexual diferenciado, que incluye, además de homosexualidad (observada en más de 1500 especies de animales), variantes de la masturbación.

La sexualidad no solo comparte las partes del cuerpo del hombre y mujer, también somete a distintos comportamientos, no solo los marcados por nuestra sociedad, como el decir de que el color rosa es para las niñas y el azul para los niños, o que las mujeres tienen el deber de permanecer en el hogar a cumplir con las labores domésticas y que los hombres son los benefactores de lo indispensable; sino que también compete a los cambios psicológicos que distinguen al hombre y a la mujer, aunque se puede decir que estos están estrechamente ligados a las diferencias cerebrales que existen ya que "siempre se ha sospechado que los cerebros de las mujeres y los de los hombres, son un poco diferentes. Ahora la ciencia está apoyando un dato del saber popular: un nuevo estudio descubrió que los hombres tienen más sinapsis conectando a las células en una región particular del cerebro".
Así nos damos cuenta de que las reacciones entre ambos ante una misma situación son muy distintas.

La sexualidad es un universo complejo que engloba cuatro aspectos principales: biológico, psicológico, social y ético. Todos estos están relacionados entre sí, cada uno de ellos, juega un papel importante en la forma de cómo percibimos todo lo relacionado a lo sexual.
La sexualidad humana, de acuerdo con la Organización Mundial de la Salud, se define como:
Un aspecto central del ser humano, a lo largo de su vida. Abarca al sexo, las identidades y los papeles de género, el erotismo, el placer, la intimidad, la reproducción y la orientación sexual. Se vive y se expresa a través de pensamientos, fantasías, deseos, creencias, actitudes, valores, conductas, prácticas, papeles y relaciones interpersonales. La sexualidad puede incluir todas estas dimensiones, no obstante, no todas ellas se viven o se expresan siempre. La sexualidad está influida por la interacción de factores biológicos, psicológicos, sociales, económicos, políticos, culturales, éticos, legales, históricos, religiosos y espirituales.

Se propone que la sexualidad es un sistema de la vida humana que se compone de cuatro características, que significan sistemas dentro de un sistema. Estas características interactúan entre sí y con otros sistemas en todos los niveles del conocimiento, en particular en los niveles biológico, psicológico y social.

Las cuatro características son: el erotismo, la vinculación afectiva, la reproductividad y el sexo genético (genotipo) y físico (fenotipo).




La característica del sexo desarrollado, comprende el grado en que se experimenta la pertenencia a una de las categorías dimórficas (femenino o masculino). Es de suma importancia en la construcción de la identidad, parte de la estructura sexual, basado en el sexo, incluye todas las construcciones mentales y conductuales de ser hombre o mujer. Hay que tener en cuenta que es muy importante que sepamos cuales son nuestras actitudes más personales e íntimas hacia la sexualidad.

Uno de los productos de la interacción de estos holones es la orientación sexual. En efecto, cuando interactúan el erotismo (la capacidad de sentir deseo, excitación, orgasmo y placer), la vinculación afectiva (la capacidad de sentir, amar o enamorarse) y el género (lo que nos hace hombres o mujeres, masculinos o femeninos) obtenemos alguna de las orientaciones sexuales a saber: la bisexualidad, la heterosexualidad y la homosexualidad.

La definición de trabajo propuesta por la OMS (2006) orienta también la necesidad de atender y educar la sexualidad humana. Para esto es de suma importancia, reconocer los derechos sexuales (WAS, OPS,2000):






En la medida que estos Derechos sean reconocidos, ejercidos o respetados, llegarán a existir sociedades más sanas en el sentido sexual.

Es importante notar que la sexualidad se desarrolla y se expresa de diferentes maneras a lo largo de la vida de forma que la sexualidad de un infante no será la misma que la de un adolescente o un adulto. Cada etapa de la vida necesita conocimientos y experiencias específicos para su óptimo desarrollo. En este sentido, para los niños es importante conocer su cuerpo, sus propias sensaciones y aprender a cuidarlo. Un niño o una niña que puede nombrar las partes de su cuerpo (incluyendo el pene, el escroto o la vulva) y que ha aceptado que es parte de él, es más capaz de cuidarlo y defenderlo. También es importante para ellos conocer las diferencias y aprender que tanto los niños como las niñas son valiosos y pueden realizar actividades similares. En esta etapa aprenden a amar a sus figuras importantes primero (los padres, los hermanos) y a las personas que los rodean, pueden tener sus primeros enamoramientos infantiles (que son diferentes de los enamoramientos de los adolescentes) y también viven las primeras separaciones o pérdidas, aprenden a manejar el dolor ante estas. En cuanto a la reproductividad, empiezan a aprender a cuidar de los más pequeños (pueden empezar con muñecos o mascotas) y van desarrollando su capacidad reproductiva. También tienen grandes dudas sobre su origen, generalmente las dudas que tienen con respecto a la relación sexual necesitan la aclaración del sentido amoroso y del deseo de tenerlo que tuvieron sus padres. Les resulta interesante el embarazo y el nacimiento en un sentido de conocer su propio origen. Sobre todo será importante indagar la pregunta y responderla al nivel de conocimiento de acuerdo a la edad del menor.

La sexualidad adulta contiene los cuatro elementos en una interacción constante. Por ejemplo, si una mujer se siente satisfecha y orgullosa de ser mujer, es probable que se sienta más libre de sentir placer y de buscarlo ella misma. Esto genera un ambiente de cercanía afectiva y sexual con la pareja y un clima de mayor confianza que a su vez repercute en las actividades personales o familiares que expresan la reproductividad. En realidad podríamos empezar por cualquiera de las características en estas repercusiones positivas o también negativas.

Cada una de las características presentará problemas muy específicos. Así, encontramos en el sexo, los problemas de homofobia, violencia contra la mujer, desigualdad sexual, etcétera. En la vinculación afectiva se encuentran las relaciones de amor/odio, la violencia en la pareja, los celos, el control de la pareja. El erotismo presentará problemas tales como disfunciones sexuales o las infecciones de transmisión sexual. En cuanto la reproductividad se observan trastornos en la fertilidad o, más tarde, violencia y maltrato infantil, abandono de los hijos, etc.

Al igual que muchos animales, los seres humanos utilizan la excitación sexual con fines reproductivos y para el mantenimiento de vínculos sociales, pero le agregan el goce y el placer propio y el del otro. El sexo también desarrolla facetas profundas de la afectividad y la conciencia de la personalidad. En relación a esto, muchas culturas dan un sentido religioso o espiritual al acto sexual (Véase Taoísmo, Tantra), así como ven en ello un método para mejorar (o perder) la salud.

La complejidad de los comportamientos sexuales de los humanos es producto de su cultura, su inteligencia y de sus complejas sociedades, y no están gobernados enteramente por los instintos, como ocurre en casi todos los animales. Sin embargo, el motor base de gran parte del comportamiento sexual humano siguen siendo los impulsos biológicos, aunque su forma y expresión dependen de la cultura y de elecciones personales; esto da lugar a una gama muy compleja de comportamientos sexuales. En muchas culturas, la mujer lleva el peso de la preservación de la especie.

Desde el punto de vista psicológico, la sexualidad es la manera de vivir la propia situación. Es un concepto amplio que abarca todo lo relacionado con la realidad sexual. Cada persona tiene su propio modo de vivir el hecho de ser mujer u hombre, su propia manera de situarse en el mundo, mostrándose tal y como es. La sexualidad incluye la identidad sexual y de género que constituyen la conciencia de ser una persona sexuada, con el significado que cada persona dé a este hecho.

La diversidad sexual nos indica que existen muchos modos de ser mujer u hombre, más allá de los rígidos estereotipos, siendo el resultado de la propia biografía, que se desarrolla en un contexto sociocultural. Hoy en día se utilizan las siglas GLTB (o LGBT) para designar al colectivo de gais, lesbianas, transexuales y bisexuales.

La sexualidad se manifiesta también a través del deseo erótico que genera la búsqueda de placer erótico a través de las relaciones sexuales, es decir, comportamientos sexuales tanto autoeróticos (masturbación), como heteroeróticos (dirigidos hacia otras personas, éstos a su vez pueden ser heterosexuales u homosexuales). El deseo erótico (o libido) que es una emoción compleja, es la fuente motivacional de los comportamientos sexuales. El concepto de sexualidad, por tanto, no se refiere exclusivamente a las “relaciones sexuales”, sino que éstas son tan sólo una parte de aquel objetivo.

Se desarrolla de forma lenta, y a una edad llegada justa, con técnicas generalmente nuevas.




</doc>
<doc id="2688" url="https://es.wikipedia.org/wiki?curid=2688" title="Santísima Trinidad (desambiguación)">
Santísima Trinidad (desambiguación)

Santísima Trinidad o Santa Trinidad puede referirse a:

Se define la Santísima Trinidad como la Comunión Perfecta de Amor indefectible de tres Personas: el Padre (Creador), el Hijo (Verbo de Dios Encarnado, Jesucristo), y el Espíritu Santo, en un solo Dios. Por eso decimos que el Dios de los cristianos es Uno (Dios) y Trino (comunión de tres personas), entendiéndose esta religión dentro del "Monoteísmo" o creencia en un solo Dios, que en este caso es un Dios Uno y Trino. Cabe señalar que ni la palabra "trinidad" ni el concepto tienen referente específico en las sagradas escrituras.







</doc>
<doc id="2689" url="https://es.wikipedia.org/wiki?curid=2689" title="Suiza">
Suiza

Suiza (en alemán: «Schweiz», en francés: «Suisse», en italiano: «Svizzera», en romanche: «Svizra»), oficialmente Confederación Suiza (en alemán: "Schweizerische Eidgenossenschaft", en francés: "Confédération suisse", en italiano: "Confederazione Svizzera", en romanche: "Confederaziun svizra" y en latín: "Confoederatio Helvetica" —de ahí que su código ISO sea CH—), es un país sin salida al mar ubicado en la Europa central y que cuenta con una población de 8 500 000 habitantes (2018). Suiza es una república confederada de 26 estados, llamados cantones. Berna es la sede de las autoridades federales, mientras que los centros financieros del país se encuentran en las ciudades de Zúrich, Basilea, Ginebra y Lugano. Suiza es el cuarto país más rico del mundo, según su PIB per cápita, con 83 718 dólares estadounidenses (2011).

Limita al norte con Alemania, al oeste con Francia, al sur con Italia y al este con Austria y Liechtenstein. Se caracteriza diplomáticamente por su política de relaciones exteriores neutral, sin haber participado activamente en ningún conflicto internacional desde 1815. Suiza es la sede de varias organizaciones internacionales, como la Organización Mundial del Movimiento Scout, la Cruz Roja, la Organización Mundial del Comercio, la Unión Postal Universal, así como una de las dos oficinas de la ONU en Europa, además de ser sede de la FIFA, máximo organismo del fútbol a escala mundial, y de la UEFA, mayor ente del fútbol europeo; también es sede del COI, máximo organismo encargado de la realización de los Juegos Olímpicos y de la FIDE, máximo organismo del ajedrez en el ámbito mundial.

Suiza es una confederación multilingüe y cuenta con cuatro idiomas oficiales: alemán, francés, italiano y romanche. La fecha de su creación como Estado se fijó el 1 de agosto de 1291 de acuerdo con la tradición. Debido a este motivo, cada año se celebra la fiesta nacional el 1 de agosto.

Actualmente, se percibe como uno de los países más desarrollados del mundo. Por su política de neutralidad, el país alberga gran cantidad de inmigrantes provenientes de naciones de varios continentes, por lo que es considerado como uno de los países europeos con mayor diversidad cultural. Finalmente, es reconocida internacionalmente por su turismo de montaña y por sus relojes, chocolates, navajas, bancos, ferrocarriles y quesos.

El nombre "Suiza" proviene de "Schwyz", nombre de uno de los cantones de Waldstätten que conformaron el núcleo de la Antigua Confederación Suiza. El topónimo del cantón data del año 972 y procede del antiguo alto alemán "Suittes", emparentado con el verbo "swedan" que significa «quemar, chamuscar» (cf. islandés "svíða", danés y sueco "svide" «chamuscar»), haciendo referencia a la tala y quema mediante el cual se quema una zona boscosa para construir algunas viviendas en la zona (artiga). El uso del nombre para esta área se extendió para denominar a todo el cantón, y después de la Guerra de Suabia en 1499 gradualmente se utilizó para nombrar a toda la confederación. El nombre en alemán de Suiza para el país, "Schwiiz", es homónimo al del cantón y su capital, por lo que para distinguirse se emplea un artículo determinado en "d'Schwiiz" para referirse al país y la forma simple "Schwiiz" para el cantón y la ciudad.

El antiguo nombre del país, Helvetia, deriva de la palabra "Helvetii", una tribu celta que habitó en la meseta suiza antes de la época de los romanos. La primera mención del nombre "Helvetti" data del año 300 a. C. Los nombres del neolatín "Confoederatio Helvetica" o "Helvetia" fueron introducidos cuando Suiza se convirtió en un Estado federal en 1848, remontándose a la República Helvética.

Los vestigios humanos más antiguos que existen datan de hace 150 000 años aproximadamente. Asimismo, las herramientas de agricultura más antiguas fueron halladas en Gächlingen y se estima que datan del 5300 a. C.

Las tribus más antiguas conocidas en el área pertenecen a las culturas Hallstatt y La Tène, llamada así debido al sitio arqueológico de La Tène, ubicado al norte del lago de Neuchâtel. La cultura de La Tène floreció a finales de la Edad de Hierro, alrededor del 450 a. C., posiblemente bajo influencia de las civilizaciones griega y etrusca. Uno de los más importantes grupos étnicos de la región fueron los helvecios. En el 58 a. C., las fuerzas de Julio César derrotaron a los helvecios en la batalla de Bibracte. En el año 15 a. C., Tiberio, quien más tarde sería emperador de Roma, y Druso el Mayor conquistaron los Alpes, integrándolos al creciente Imperio romano. La región ocupada por los helvecios, de donde proviene el nombre "Confoederatio Helvetica", pasó a formar parte de la provincia romana de Galia Bélgica y más tarde de la provincia Germania Superior, mientras la porción este de la Suiza moderna estuvo integrada a la provincia romana de Raetia.

En la Alta Edad Media, la parte occidental de la actual Suiza formó parte del territorio del Reino de Borgoña desde el siglo IV. Los alamanes se establecieron en la meseta suiza en el siglo V y en los valles de los Alpes en el siglo VIII, formando Alemannia, y quedando el actual territorio de Suiza dividido entre los reinos de Borgoña y de Alemannia. En el siglo VI, la región entera pasó a formar parte del Imperio franco tras la victoria de Clodoveo I sobre los alamanes en Tolbiac en el año 504. Posteriormente los francos también dominarían a los burgundios.

Entre los siglos VI y VIII Suiza continuó bajo la hegemonía franca (las dinastías merovingia y carolingia). En 843, tras alcanzar su máxima extensión bajo el reinado de Carlomagno, el imperio franco fue dividido en el Tratado de Verdún. El territorio de la actual Suiza quedó dividido entre Francia Oriental y Francia Media hasta que fue unificada por el Sacro Imperio Romano Germánico en el siglo XI.

Para el año 1200, la meseta suiza pertenecía a los dominios de las casas de Saboya, Zähringer, Habsburgo y Kyburg. Algunas regiones (Uri, Schwyz y Unterwalden, después conocidas en conjunto como "Waldstätten"), fueron anexadas como inmediaciones imperiales para garantizar el control del imperio sobre los puertos de montaña. Cuando la dinastía Kyburg cayó en 1264, los Habsburgo extendieron sus territorios al este de la meseta suiza durante el reinado de Rodolfo I, que fue emperador del Sacro Imperio en 1273.

La Antigua Confederación Suiza fue una alianza entre las comunidades de los valles centrales de los Alpes. La Confederación facilitó el desarrollo de varios intereses comunes (libre comercio) y aseguró la paz en las principales rutas mercantiles en las montañas. La Carta Federal de 1291, firmada por las comunidades rurales de Uri, Schwyz y Unterwalden, es considerada el documento que sentó las bases para la fundación de la confederación, aunque es probable que alianzas similares ya hubiesen existido desde décadas anteriores.
En 1353, los tres cantones originales se habían unido con los cantones de Glaris y Zug y con las ciudades-Estado de Lucerna, Zúrich y Berna para formar la Antigua Confederación de los ocho cantones que existió hasta finales del siglo XV. La expansión territorial ayudó a incrementar el poder y la riqueza de la confederación. En 1460, los confederados controlaban gran parte de los territorios al sur y oeste del río Rin hasta la cordillera de los Alpes. En 1499 la victoria de Suiza sobre la Liga de Suabia y la casa de Habsburgo en la guerra de Suabia dio como resultado una independencia "de facto" del Sacro Imperio.

La Antigua Confederación Suiza había adquirido una reputación de invencible durante estas guerras, pero la expansión de la Confederación sufrió un revés en 1515, con la derrota en la batalla de Marignano. Esto marcó el fin de la llamada época "heroica" de la historia de Suiza. El éxito de la Reforma de Ulrico Zuinglio en algunos cantones llevó a varias guerras internas en el país entre 1529 y 1531, las guerras de Kappel ("Kappeler Kriege"). Ya en 1648, más de un siglo después de estas contiendas, Johann Rudolf Wettstein, como enviado de la Confederación Suiza, consiguió mediante hábiles negociaciones que las potencias firmantes del Tratado de Westfalia reconocieran oficialmente la independencia de Suiza con respecto al Sacro Imperio Romano Germánico y su neutralidad en las guerras ("Ancien Régime").

Los siglos XVI y XVII estuvieron caracterizados por el creciente autoritarismo de las familias gobernantes. En 1653, esta situación, combinada con la crisis financiera traída por la Guerra de los Treinta Años, produjo el estallido de la Guerra de los campesinos suizos. Sumado a esto, permanecía el conflicto religioso entre los cantones católicos y los cantones protestantes, que entre 1656 y 1712 llevaron a violentos enfrentamientos, como la batalla de Villmergen.

En 1798, las fuerzas de la Revolución francesa conquistaron Suiza e impusieron una nueva constitución. Esta constitución centralizaba el gobierno y abolía los cantones, y tanto el territorio de Mulhouse como el valle de Valtellina fueron separados de Suiza. El nuevo régimen, conocido como la República Helvética, fue muy impopular. Había sido impuesto por un ejército invasor, destruyendo siglos de costumbres y tradiciones y convirtiendo a Suiza en un Estado satélite de Francia. La fuerte represión efectuada por Francia durante la rebelión de Nidwalden (septiembre de 1798) fue un ejemplo de la presencia opresiva del ejército francés y de la resistencia local a la ocupación.

Suiza tuvo que entrar en el bloqueo continental, lo que dañó y estimuló a su industria al mismo tiempo, y tuvo que suministrar tropas. Al principio 16 000 hombres, que se redujeron a 12 000 en 1811, pero a pesar de los incentivos ofrecidos y de las amenazas de reclutamiento forzoso, los efectivos nunca llegaron a las cifras exigidas.

Cuando estalló la guerra entre Francia y sus rivales, las fuerzas de Rusia y Austria invadieron Suiza. El pueblo suizo se negó a combatir al lado de los franceses en nombre de la República Helvética. En 1803, Napoleón organizó una reunión con líderes políticos suizos en París; el resultado de esta reunión fue el documento llamado Acta de Mediación, el cual restablecía en gran parte la autonomía de Suiza y la Confederación de 19 cantones. Desde entonces, gran parte de la política suiza se encaminaría a equilibrar la tradición de los cantones autónomos con la necesidad de un gobierno central.

En 1815, el Congreso de Viena restableció por completo la independencia de Suiza, y las potencias europeas accedieron a reconocer permanentemente la neutralidad del país. Tropas suizas sirvieron a varios gobiernos extranjeros hasta 1860, cuando pelearon en el sitio de Gaeta. El tratado también aumentó la extensión territorial de Suiza, con la integración de los cantones de Valais, Neuchâtel y Ginebra. Los límites de Suiza no han cambiado desde aquel entonces.

El cantón de Berna fue uno de los tres cantones que presidieron el Tagsatzung (antiguo consejo ejecutivo y legislativo) junto con Lucerna y Zúrich. La capital del cantón fue elegida en 1848 como sede de las autoridades federales, principalmente debido a su cercanía con el área francófona del país.

La restauración del poder fue solamente temporal. Después de un periodo de disturbios con repetidos enfrentamientos violentos, como el "Züriputsch" en 1839, estalló la guerra civil en 1847 cuando algunos de los cantones católicos trataron de establecer una alianza entre ellos ("Sonderbund"). La guerra duró menos de un mes, causando menos de cien víctimas, la mayoría de las cuales se debieron a fuego amigo. La guerra del Sonderbund parece muy pequeña comparada con otros conflictos que existieron en la Europa del siglo XIX y en la historia de su sociedad.

La guerra mostró a los habitantes la necesidad de unidad para fortalecerse ante sus vecinos europeos. Suizos de todos los estratos sociales, ya fuesen católicos, protestantes, liberales o conservadores, se percataron de que los cantones progresarían más si aunaran sus intereses económicos y religiosos.

Así, mientras el resto de Europa se encontraba en medio de revoluciones y guerras, los suizos promulgaron una constitución más moderna, la cual daba al gobierno un diseño federal, en gran parte inspirado en el modelo estadounidense. Esta constitución impuso una autoridad central, dejando a los cantones el derecho de autogobernarse y resolver cuestiones locales. Además la asamblea nacional se dividió en una cámara alta (el Consejo de los Estados de Suiza, con dos representantes por cada cantón) y una cámara baja (Consejo Nacional de Suiza, con representantes electos de todo el país). Para introducir cualquier cambio en la constitución se volvió obligatorio realizar un referéndum.

Asimismo se implantó un sistema único de pesas y medidas, y en 1850 el franco suizo se convirtió en la única moneda oficial del país. El artículo 11 de la constitución prohibió el envío de tropas al extranjero, pero hizo una excepción con los Estados Pontificios, al no considerar mercenarios a los miembros del ejército papal (Guardia Suiza). En tal sentido, en 1860 el ejército suizo fue obligado a participar al lado de Francisco II de las Dos Sicilias en el sitio de Gaeta.
Una de las cláusulas más importantes de la constitución era la que establecía que podía ser reescrita completamente si la ocasión lo demandaba, de esta forma la constitución evolucionaría totalmente en lugar de ser modificada año tras año. Esta característica de la constitución se volvió muy útil con la llegada de la Revolución industrial, cuando varios proclamaron que era hora de modificar la constitución. Un primer borrador fue rechazado por la población en 1872, pero dos años más tarde se aceptaron las modificaciones. Fue aquí cuando se introdujo un referéndum facultativo para la creación y modificación de leyes a nivel federal. También se establecieron normas que regulaban el ejército, el comercio y otras cuestiones legales. Finalmente, en 1891, la constitución fue revisada de nuevo y se implantó un inusual sistema de democracia directa, el cual sigue siendo único hasta el día de hoy.

Suiza no fue invadida en ninguna de las dos guerras mundiales. Durante la Primera Guerra Mundial, Suiza dio asilo a Vladimir Illych Ulyanov (Lenin) donde permaneció hasta 1917. En 1917 la neutralidad de Suiza fue seriamente cuestionada por el escándalo protagonizado por Robert Grimm y Arthur Hoffmann, cuando intentaron pactar una tregua entre Rusia y Alemania. No obstante, en 1920, Suiza entró en la Sociedad de Naciones, la cual tenía su sede en Ginebra, con la única condición de que quedaría libre de todo requerimiento militar.

Durante la Segunda Guerra Mundial, el ejército alemán realizó detallados planes de invasión (Operación Tannenbaum) pero nunca invadió Suiza. El país fue capaz de mantener su independencia gracias a una combinación de disuasiones militares, concesiones a Alemania y muy buena suerte en las operaciones militares que retrasaron la invasión alemana. También existieron intentos por parte del Partido Nazi suizo para anexar el país a Alemania, pero fallaron. La prensa suiza criticó duramente al Tercer Reich, insultando frecuentemente a su Führer. Suiza fue una importante base de espionaje para ambos bandos durante el conflicto, además de que a menudo actuó como mediadora en las comunicaciones entre los Aliados y las fuerzas del Eje. La Cruz Roja Internacional, con sede en Ginebra, jugó un papel muy importante durante este y otros conflictos.
El comercio con Suiza fue bloqueado por los Aliados y por los países del Eje. La cooperación económica y la ampliación del crédito para el Tercer Reich variaban según el riesgo de invasión y de la disponibilidad de otros socios comerciales. Las concesiones alcanzaron su punto máximo luego de que fuera cortada una línea ferroviaria que conectaba al país con la Francia de Vichy, dejando a Suiza completamente rodeada por el Eje. En el transcurso de la guerra, Suiza recibió más de 300 000 refugiados, de los cuales 104 000 soldados extranjeros, que fueron aceptados según los "Derechos y obligaciones de los países neutrales", documento firmado en las Conferencias de la Haya de 1899 y 1907; 60 000 de los refugiados eran civiles que habían escapado de la persecución de los nazis. De estos, alrededor de 27 000 eran judíos. Sin embargo, las estrictas políticas de inmigración y asilo, así como las relaciones financieras con la Alemania Nazi, generaron controversia. Durante la guerra, la Fuerza Aérea Suiza combatió aeronaves de ambos bandos. En mayo y junio de 1940, derribaron once aviones de la Luftwaffe que habían invadido el espacio aéreo suizo, obligando a otras naves intrusas a retirarse después de un cambio de la política en las relaciones con Alemania. Más de cien bombarderos Aliados y sus tripulaciones fueron albergados durante la guerra. En 1944, los Aliados bombardearon por error las ciudades de Schaffhausen (matando a cuarenta personas), Stein am Rhein, Vals y Rafz (con dieciocho muertos), así como Basilea y Zúrich el 4 de marzo de 1945.
En 1959, las mujeres recibieron el derecho a votar en algunos cantones, convirtiéndose este en ley federal en 1971. En 1963, Suiza se adhirió al Consejo de Europa. A finales de la década de 1970, una parte del cantón de Berna se separó y creó el nuevo cantón de Jura. En 1984, Elisabeth Kopp fue la primera mujer en el Consejo Federal Suizo y fue en 1999 cuando llegó a la presidencia la primera mujer, Ruth Dreifuss. El 18 de abril de ese mismo año, la población suiza votó a favor de una revisión completa de la constitución federal.
En 2002, Suiza se convirtió en miembro de pleno derecho de la ONU, dejando a la Ciudad del Vaticano como el único Estado reconocido que no pertenece a la ONU. Suiza fue uno de los fundadores de la EFTA, pero no es miembro del Espacio Económico Europeo (EEE). Una solicitud de adhesión fue enviada a la Unión Europea en mayo de 1992, pero no prosiguió cuando el acceso al EEE fue rechazado en referéndum en diciembre de ese año. Desde entonces se han realizado múltiples referendos y votaciones sobre la entrada de Suiza en la Unión Europea, pero debido a las diversas reacciones que ha tenido la población, el proceso de obtención de la adhesión se ha detenido. Sin embargo, la ley suiza ha ido cambiando gradualmente para ajustarse a lo que la Unión Europea y el gobierno suizo afirman, a través de la firma de acuerdos bilaterales. Suiza y Liechtenstein han estado rodeados totalmente por la Unión Europea desde el ingreso de Austria en 1995. El 5 de junio de 2005, el 55 % de los votantes suizos accedieron a unirse al Tratado de Schengen, un resultado que ha sido catalogado por la Unión Europea como una señal de apoyo por parte de Suiza, un país que es tradicionalmente percibido como independiente o aislacionista.

La constitución federal de 1848 es el fundamento legal del Estado federal moderno y la tercera constitución más antigua aún en vigencia en todo el mundo (después de la estadounidense y la noruega). Una nueva versión de la constitución fue adoptada en 1999, pero no introdujo cambios notables en la estructura federal. Esta delimita los derechos y obligaciones básicos de los ciudadanos, su participación activa en la política, divide el poder entre la confederación y los cantones y define las autoridades y jurisdicciones federales. Existen tres principales cuerpos de gobierno a nivel federal: el parlamento bicameral (poder legislativo), el Consejo Federal (poder ejecutivo) y la Corte Suprema Federal de Suiza o Tribunal Supremo Federal (poder judicial). La función de la Corte Suprema Federal es la de atender las apelaciones en contra de las cortes cantonales o federales. Los jueces o magistrados son elegidos por la Asamblea Federal para un periodo de seis años.

El Parlamento suizo se compone de dos cámaras: el Consejo de los Estados, que cuenta con 46 representantes (dos de cada cantón y uno de cada semicantón), los cuales son elegidos por cada cantón bajo su propio sistema; y el Consejo Nacional, el cual consta de 200 miembros elegidos mediante un sistema de representación proporcional, dependiendo de la población de cada cantón. Los miembros de las dos cámaras son elegidos cada cuatro años. Cuando ambas cámaras se encuentran en sesión conjunta, se les conoce como Asamblea Federal. A través de referendos los ciudadanos pueden rechazar o aceptar cualquier ley proveniente del parlamento, y por medio de iniciativas pueden introducir nuevos puntos a la constitución federal, haciendo de Suiza una democracia directa.

El Consejo Federal constituye el gobierno federal, dirige la Administración Federal y hace de jefe de Estado. Está integrado por siete miembros elegidos para un mandato de cuatro años por la Asamblea Federal, quien también vigila las acciones del consejo. El presidente de la Confederación es elegido por la asamblea de entre los siete miembros del consejo, tradicionalmente en rotación y solo por un periodo de un año; el presidente dirige el gobierno y asume sus funciones representativas. Sin embargo, el presidente es un "primus inter pares" sin poderes adicionales, y permanece a la cabeza de su departamento durante su administración.

Desde 1959, el gobierno federal suizo ha estado formado por una coalición de los cuatro principales partidos políticos, cada uno teniendo un número de asientos que difícilmente refleja su popularidad entre los votantes y el número de representantes en el parlamento. Desde 1959 hasta 2003, la clásica distribución de 2 CVP/PDC, 2 SPS/PSS, 2 FDP/PRD y 1 SVP/UDC fue conocida como la «fórmula mágica» ("Zauberformel"). Actualmente los siete asientos del Consejo Federal se encuentran distribuidos de la siguiente forma:

Los ciudadanos suizos son materia de tres jurisdicciones legales: la comuna, el cantón y la confederación. La constitución federal de 1848 define un sistema de democracia directa (a veces llamada "semidirecta" o democracia representativa directa debido a que tiene una mayor similitud con instituciones de una democracia parlamentaria). Los instrumentos de la democracia directa suiza a nivel federal, conocidos como derechos civiles ("Volksrechte" o "droits civiques"), incluyen el derecho a elaborar una "iniciativa constitucional" y a un "referéndum", los cuales pueden influir en las decisiones del parlamento.

Por medio de un "referéndum", un grupo de ciudadanos puede poner en disputa alguna ley que haya sido aprobada por el parlamento si puede conseguir —en un plazo de cien días— más de 50 000 firmas que estén en contra de la ley. Si lo logra, se lleva a cabo una votación nacional, donde se decide por mayoría simple si la ley es rechazada o no. Ocho cantones unidos también pueden realizar un referéndum para la aprobación de alguna ley federal.

De manera similar, la "iniciativa constitucional" permite a los ciudadanos solicitar que una enmienda constitucional sea puesta en votación si logran 100 000 firmas que apoyen la enmienda en un plazo de 18 meses. El parlamento puede complementar la enmienda propuesta con una contrapropuesta, donde los votantes tendrán que indicar su preferencia en las papeletas, en caso de que ambas propuestas sean aceptadas. Las enmiendas constitucionales, ya sean de iniciativa popular o parlamentaria, deben ser aceptadas por una mayoría doble del voto nacional y del voto cantonal.

La Confederación Suiza se compone de 26 cantones:

<nowiki>*</nowiki>

Su población varía entre los 15 000 habitantes del cantón de Appenzell Rodas Interiores y los 1,2 millones de habitantes del cantón de Zúrich, mientras que su superficie varía entre los 37 km² de Basilea-Ciudad y los 7100 km² de los Grisones. Los cantones comprenden un total de 2889 municipios. Dentro de Suiza existen dos enclaves: Büsingen, perteneciente a Alemania, y Campione d'Italia, perteneciente a Italia.

El 11 de mayo de 1919, en un referéndum organizado en el estado federado austriaco de Vorarlberg, más del 80 % de la población votó a favor de que se integrara a la Confederación Suiza. Sin embargo, la oposición del gobierno de Austria, los Aliados, los liberales suizos, los suizos-italianos y los romandos impidió la anexión de Vorarlberg.

Tradicionalmente, Suiza evita todas las alianzas que puedan implicar acción militar, política o económica y ha sido neutral desde su expansión en 1515. No fue hasta 2002 cuando Suiza se convirtió en miembro completo de la ONU, pero fue el primer Estado en adherirse a la organización después de un referéndum. Suiza mantiene relaciones diplomáticas con casi todas las naciones e históricamente ha actuado como intermediario de otros Estados. Suiza no es miembro de la Unión Europea; la población suiza ha rechazado la membresía desde principios de la década de 1990. Sin embargo, desde 2005 forma parte del espacio de Schengen.

Un número alto de instituciones internacionales tienen su sede en Suiza, en parte debido a su política de neutralidad. La Cruz Roja fue fundada en 1863, y tiene su centro de operaciones en el país. A pesar de que Suiza es uno de los países que más recientemente se integraron a la ONU, en Ginebra se encuentra la segunda sede más grande de la organización después de la ubicada en Nueva York. Ginebra también es sede de varias organizaciones dependientes de la Organización de las Naciones Unidas (ONU), como la Organización Mundial de la Salud (OMS), la Organización Mundial de Comercio (OMC), la Organización Internacional del Trabajo (OIT), la Unión Internacional de Telecomunicaciones (UIT), la Organización Mundial de la Propiedad Intelectual (OMPI), la Organización Meteorológica Mundial (OMM) de la Conferencia de las Naciones Unidas sobre Comercio y Desarrollo (CNUCYD), y de la Unión Interparlamentaria (UIP); además de otras 200 organizaciones internacionales.

Incluso muchas federaciones y organizaciones deportivas tienen su sede en el país; como el Comité Olímpico Internacional (COI), la Federación Internacional de Esgrima (FIE) y el Tribunal de Arbitraje Deportivo (TAS) en Lausana; la Federación Internacional de Luchas Asociadas (FILA) en Corsier-sur-Vevey; la Federación Internacional de Hockey sobre Hielo (IIHF) y la Federación Internacional de Fútbol Asociación (FIFA) en Zúrich; la Unión Europea de Asociaciones de Fútbol (UEFA) en Nyon; y la Federación Internacional de Baloncesto (FIBA) en Ginebra. Otras federaciones y organizaciones deportivas también tienen su sede en Suiza, tales como: la Federación Internacional de Hockey (FIH), la Federación Internacional de Voleibol (FIVB), la Federación Internacional de Balonmano (IHF), la Federación Internacional de Tenis de Mesa (ITTF), la Federación Internacional de Esquí (FIS), la Unión Internacional de Patinaje sobre Hielo (ISU), la Federación Internacional de Natación (FINA), la Federación Aeronáutica Internacional (FAI), la Federación Internacional de Béisbol (IBAF), la Federación Mundial de Bridge (WBF) y la Liga Europea de Bridge (EBL), la Unión Internacional de Asociaciones de Alpinismo (UIAA), la Federación Internacional de Gimnasia (FIG) y la Unión Ciclista Internacional (UCI). Debido a que muchas federaciones y organizaciones deportivas tienen su sede en Suiza, el país es conocido como la capital mundial del deporte.

En Suiza también se encuentra la sede de la Unión Internacional de Química Pura y Aplicada (IUPAC), del Banco de Pagos Internacionales (BPI), de la Organización Internacional de Normalización (ISO), del Foro Económico Mundial (WEF), de la Oficina del Alto Comisionado para los Derechos Humanos (OACDH), de la Comisión Económica de las Naciones Unidas para Europa (UNECE o ECE), de la Organización Internacional para las Migraciones (OIM), del Consejo Mundial de Iglesias (CMI), de la Unión Europea de Radiodifusión (UER), de la Comisión Internacional de Juristas (CIJ), de la Organización del Bachillerato Internacional (OBI), de la Organización Mundial del Movimiento Scout (OMMS), de la Asociación Mundial de Cardiología (AMC), de la Asociación Cristiana de Jóvenes (YMCA), y de la Asociación Europea de Libre Comercio (EFTA).

También se encuentra el Palacio de las Naciones que es un complejo de edificios que fueron construidos entre 1929 y 1937 en el seno del Parque Ariana en Ginebra. Sirvió de sede a la Sociedad de Naciones (SDN) hasta 1946. Más tarde fue ocupado por la Organización de las Naciones Unidas (ONU), y en 1966, el palacio se convierte en sede de la Oficina de la Organización de las Naciones Unidas en Ginebra (ONUG) y en la segunda más importante de la organización después de la sede de Nueva York.

También se encuentra la sede principal del Fondo Mundial para la Naturaleza (WWF), y del Fondo Mundial de lucha contra el sida, la tuberculosis y la malaria.

Muchos organismos internacionales que tienen su sede en Suiza, fueron fundados en otros países, y tuvieron sus sedes en su país de fundación y en otros países, hasta llegar finalmente a Suiza, donde su sus sedes radicarán permanentemente y nunca más se cambiarán a otro país, debido a su política de neutralidad; un claro ejemplo, son la FIFA y el COI que fueron fundados en París (Francia), pero decidieron cambiar su sede de origen a Suiza (la FIFA fundada en París se trasladó a Zúrich y el COI también fundado en París, a Lausana). Otros organismos internacionales como la Cruz Roja Internacional (CRI), el Comité Internacional de la Cruz Roja (CICR) y la Federación Internacional de Sociedades de la Cruz Roja y de la Media Luna Roja (IFRC) fueron fundados directamente en Suiza, y nunca cambiarán sus sedes a otro país, debido a su política de neutralidad.

Las Fuerzas Armadas Suizas se componen del ejército y la Fuerza Aérea Suiza. Como Suiza es un país sin otra salida al mar que a través de las aguas internacionales del río Rin, no cuenta con una marina de guerra, pero en los lagos limítrofes el ejército hace uso de botes armados. La peculiaridad del Ejército Suizo es el sistema de milicia. Los soldados profesionales constituyen solo el 5 % del personal militar. El resto son ciudadanos alistados de entre 20 y 34 años. Los ciudadanos suizos tienen prohibido servir en tropas extranjeras, con la excepción de la Guardia Suiza, que sirve al papa.

La estructura de la milicia suiza estipula que los soldados deben mantener en casa su propio equipo, incluyendo la famosa navaja del ejército suizo y sus armas personales. Algunas organizaciones y partidos políticos encuentran esta práctica como controvertida y peligrosa. A la edad de 19 años, el servicio militar es obligatorio para todos los ciudadanos varones; las mujeres pueden servir voluntariamente. Cerca de las dos terceras partes de los jóvenes suizos son declarados aptos para el servicio; mientras que los descartados deben pagar un impuesto especial en su lugar. Anualmente, cerca de 20 000 personas son entrenadas para el combate en un curso de 18 a 21 semanas. La reforma «Ejército XXI» fue adoptada por voto popular en 2009, y reemplazó al antiguo modelo "Ejército 95", reduciendo el número de efectivos de 400 000 a 200 000. De estos, 120 000 son soldados activos y 80 000 reservistas.
En total, solo se han declarado tres movilizaciones generales para asegurar la integridad y neutralidad de Suiza. La primera con motivo de la Guerra Franco-Prusiana entre 1870 y 1871. La segunda fue decidida en respuesta al estallido de la Primera Guerra Mundial en agosto de 1914. La tercera movilización tuvo lugar en septiembre de 1939 en respuesta a la invasión alemana a Polonia, y Henri Guisan fue elegido comandante en jefe.

Debido a su neutralidad, el ejército no puede tomar parte en conflictos armados en otros países, pero ha participado en varias misiones de paz alrededor del mundo. Desde 2000, el departamento de defensa también utiliza el sistema de inteligencia Onyx para monitorizar las comunicaciones por satélite. Después del fin de la Guerra Fría ha habido numerosos intentos para reducir la actividad militar e incluso disolver el ejército. Uno de los referendos más importantes sobre este tema tuvo lugar el 26 de noviembre de 1989 y, aunque no fue aprobado, mostró que un alto porcentaje de la población suiza está a favor de dichas iniciativas.

Extendiéndose sobre las laderas norte y sur de los Alpes, Suiza comprende una gran variedad de formas de relieve y climas en un área de 41 277 km². La población total es de algo más de 8 millones de habitantes, resultando en una densidad de población de unos 187 hab./km². La parte sur del país es montañosa y se encuentra menos densamente poblada que la parte norte, donde el terreno, en parte boscoso y en parte despejado, cuenta con la presencia de varios lagos.

Suiza se puede dividir en tres áreas topográficas básicas: los Alpes suizos en el sur, la meseta suiza en el centro, y las montañas de Jura en el norte. Los Alpes es una cordillera de montañas altas que corren a través del centro y sur del país, ocupando cerca del 60 % de la superficie total. Entre los picos más altos de los Alpes suizos, siendo el mayor la Punta Dufour ("Dufourspitze") con 4634 msnm, se encuentran múltiples valles, con cascadas y glaciares. Estos conforman la cabecera de algunos de los ríos más importantes de Europa, como el Rin, el Ródano, el Eno, el Aar y el Tesino. Otros ríos corren por el país y desembocan en los grandes lagos que hay en el territorio nacional como el lago Lemán, el lago de Zúrich, el lago de Neuchâtel o el lago de Constanza.

Una de las montañas más famosas del país es el Cervino (4478 msnm) en los Alpes Peninos, formando parte de la frontera con Italia. Otras de las montañas más altas del país se encuentran en esa zona: la Punta Dufour (4634 msnm), el Dom (4545 msnm) y el Weisshorn (4506 msnm). En la sección de los Alpes berneses, al norte de Lauterbrunnen, se encuentra un valle con 72 cascadas, también conocido por los montes Jungfrau (4158 msnm) y Eiger (3970 msnm), y otros de los valles más pintorescos de la región. En el sureste destaca el valle de Engadina, donde se encuentra la comuna de Sankt Moritz, y el pico más alto de la zona es el Piz Bernina (4049 msnm).

La parte norte del país es la más poblada, ocupando cerca del 30 % de la superficie del país, es también llamada meseta o Tierra Media ("Mittelland"). Cuenta con amplios valles con colinas, bosques y pastizales, usualmente utilizados para la agricultura y la ganadería. Es en esta zona donde se ubican las ciudades y los lagos más grandes de Suiza. El lago más grande del país es el lago Lemán, ubicado en la parte oeste y compartido con Francia.

El clima es por lo general templado, pero puede variar mucho de localidad a localidad, de las condiciones glaciares en la cima de las montañas a un clima casi mediterráneo en el sur del país. Los veranos suelen ser cálidos y húmedos con lluvias periódicas que ayudan al desarrollo de la agricultura en la región. Los inviernos en las montañas alternan días de sol y nieve, mientras las tierras más bajas tienden a tener días nublados y neblinosos. Un fenómeno climatológico llamado Efecto Föhn puede ocurrir en cualquier época del año, incluso en invierno, y se caracteriza por el paso del aire cálido del Mediterráneo por los Alpes desde Italia. Las zonas con menos precipitaciones son los valles del sur en el Valais, donde se cultiva el valioso azafrán y viñedos para la producción de vinos. Los Grisones también tienden a ser más secos y ligeramente más fríos, aunque a veces reciben numerosas nevadas en invierno. Las condiciones más húmedas del país persisten en las alturas de los Alpes y en el cantón del Tesino, donde las lluvias y nevadas son abundantes. La zona este tiende a ser más fría que la zona oeste del país, además de que las precipitaciones suelen ser escasas a lo largo del año, con variaciones menores entre el paso de las estaciones. El otoño suele ser la estación más seca del país, aunque los patrones del clima en Suiza pueden variar mucho de un año a otro, haciendo que sea muy difícil predecirlo.

Los ecosistemas de Suiza pueden ser particularmente vulnerables, lo cual se debe a que los múltiples valles delicados separados por las montañas a menudo forman ecosistemas únicos. Las regiones montañosas en sí son también vulnerables, con una amplia gama de plantas que no se encuentran a esas altitudes en otras partes del mundo, pero que se enfrentan el maltrato de los visitantes y de la ganadería.

La estructura geológica de Suiza es esencialmente el resultado de la colisión entre las placas de África y Europa que ha ocurrido durante los últimos millones de años. Este fenómeno es especialmente visible en la falla de cabalgamiento de Sardona, declarada por UNESCO Patrimonio de la Humanidad.

Desde el punto de vista geológico, Suiza se divide en cinco regiones principales. Los Alpes, que esencialmente se componen de granito y el macizo del Jura, que es una cordillera de pliegues más joven, de calizas. Entre la cadena montañosa de Jura y Los Alpes se encuentra la meseta suiza, que en parte es llana y en parte con lomajes. Se agregan además el Valle del Po en el extremo sur del Tesino, el valle del Mendrisiotto (Mendrisio), como asimismo la fosa tectónica del Alto Rin en las inmediaciones de Basilea, la que está ubicada en su mayor parte fuera de Suiza.

Las enormes masas de hielo — las que durante las distintas glaciaciones avanzaron adentrándose mucho hacia el territorio de la Meseta Suiza — marcaron de manera determinante y dieron forma a la topografía de la Suiza actual durante los últimos dos millones de años.

Suiza cuenta con una de las economías capitalistas más estables, poderosas y modernas del mundo, ubicada entre las diez mejores según el Índice de Libertad Económica de 2009. El PIB nominal per cápita de Suiza es más alto que el de la mayoría de las economías europeas, solo superado por el de Luxemburgo. La moneda oficial del país es el franco suizo (CHF).

El índice de paridad de poder adquisitivo (PPA) de Suiza se encuentra entre los quince mejores del mundo. El reporte de competitividad del Foro Económico Mundial coloca a la economía de Suiza como la segunda más competitiva en el mundo. En gran parte del siglo XX, Suiza fue el país más rico en Europa por un margen considerable.

Suiza es el hogar de algunas de las corporaciones multinacionales más grandes del mundo. Las compañías más grandes de Suiza son Glencore, Nestlé, Novartis, Hoffmann-La Roche, ABB, Sika AG y Adecco. También destacan los bancos mundiales UBS AG, Servicios Financieros Zúrich, Credit Suisse Group, Swiss Re y los grupos relojeros Swatch y Richemont. UBS AG es un banco privado y un banco de inversión que se ocupa de la gestión de riquezas ("wealth management") y de activos ("asset management") de clientes privados, corporativos e institucionales. Entre sus servicios, ofrecidos a nivel nacional y mundial, destaca también la banca de inversión ("investment banking"). Junto a Credit Suisse, UBS es el mayor y más antiguo banco presente en la Confederación Helvética. 
Las actividades económicas más importantes en Suiza se encuentran la industria química, la industria farmacéutica, la fabricación de instrumentos musicales y de medición, las inmobiliarias, los servicios financieros y el turismo. Las principales exportaciones del país son los productos químicos (34 % de los bienes exportados), la maquinaria electrónica (20,9 %) y los instrumentos de precisión y relojes (16,9 %). Los servicios exportados suman un tercio de los bienes exportados.

La población económicamente activa llega a los 3,8 millones de personas. Suiza cuenta con un mercado laboral más flexible que los países vecinos y el índice de desempleo se mantiene bajo. Sin embargo, el índice de desempleo aumentó de 1,7 % en junio de 2000 a 3,9 % en septiembre de 2004. En abril de 2009 el índice de desempleo había bajado hasta 3,4 %, en parte debido al alza de la economía que comenzó a mediados de 2003.
El sector privado en la economía suiza es inmenso, además de que el país cuenta con bajas tasas de impuestos para los estándares occidentales, siendo una de las de los países desarrollados. El lento crecimiento económico de Suiza en la década de 1990 y principios de 2000 trajo consigo una serie de reformas económicas para adaptarse al modelo de la Unión Europea. Según Credit Suisse, solo el 37 % de los habitantes del país es dueño de su propia casa, uno de los índices más bajos en toda Europa. El aumento de los precios de los alimentos y bienes raíces fueron del 145 y 171 % en 2007, mientras que en Alemania fueron del 104 y 113 %. El proteccionismo agrícola, una rara excepción a la política de libre comercio suiza, contribuye al alza de los precios de los alimentos. Según la OCDE, la liberalización de los mercados está retrasando algunas economías europeas como Suiza. Sin embargo, el PPA suizo es uno de los más altos en el mundo. Aparte de la agricultura, las barreras económicas y del comercio entre la Unión Europea y Suiza son mínimas y el país ha firmado múltiples acuerdos de libre comercio con otros países del mundo.

La electricidad generada en Suiza proviene en un 56 % de centrales hidroeléctricas, un 34 % de centrales nucleares y un 5 % de centrales térmicas y de otros combustibles convencionales como el carbón.

El 18 de mayo de 2003, fueron rechazadas dos iniciativas antinucleares: "Moratorium Plus", que pedía el cese de la construcción de nuevas plantas de energía nuclear (41,6 % a favor y 58,4 % en contra), y "Electricidad Sin Energía Nuclear" (33,7 % a favor y 66,3 % en contra). La antigua moratoria de diez años para la construcción de nuevas centrales de energía nuclear fue el resultado de una iniciativa ciudadana de 1990, en la cual el sí ganó con el 54,5 % de los votos, contra el no que obtuvo 45,5 %. La Oficina Federal de Energía Suiza (SFOE) es la responsable de responder y atender todas las quejas y dudas sobre el abastecimiento y utilización de la energía, junto con el Departamento Federal de Medio Ambiente, Transporte, Energía y Comunicaciones (DETEC). Estas agencias apoyan el concepto de la «Sociedad de 2000 vatios» para reducir en más de la mitad el consumo de energía del país para el año 2050.
La administración de las vías terrestres suizas es financiada a través de la viñeta suiza y con los impuestos sobre los vehículos. El sistema de autopistas suizo requiere la compra de una pegatina o viñeta, con un valor de 40 CHF, por un año, tanto para vehículos de pasajeros como de carga. La red de carreteras suizas tiene una longitud de 1638 km (2000) y un área aproximada de 41 290 km², lo que convierte a Suiza en uno de los países con mayor número de autopistas en proporción a su tamaño. El aeropuerto más grande del país es el Aeropuerto Internacional de Zúrich, por el cual pasaron más de 20,7 millones de pasajeros en 2007. A este le siguen el Aeropuerto Internacional de Ginebra con 10,8 millones de pasajeros y el Aeropuerto de Basilea-Mulhouse con 4,3 millones de pasajeros, ambos aeropuertos son compartidos con Francia.

La red ferroviaria cuenta con 5063 km, transportando a más de 350 millones de pasajeros anualmente. En 2007, cada ciudadano suizo había recorrido un promedio de 2103 km en tren. La red ferroviaria es administrada principalmente por la SBB-CFF-FFS, excepto en gran parte de los Grisones, donde los 366 km de vía estrecha son operados por el Ferrocarril Rético, que incluye algunas líneas que son Patrimonio de la Humanidad. La construcción de túneles a través de los Alpes ha reducido la duración de los viajes que se efectúan entre el norte y el sur.

Suiza es altamente activa en cuanto al reciclaje y las regulaciones anticontaminantes, siendo uno de los recicladores más grandes del mundo, con un aprovechamiento de los materiales reciclables que va del 66 % al 96 %. En muchos lugares de Suiza, la recolección de basura en los vecindarios no es gratuita. La basura (excepto materiales peligrosos, baterías, etc.) es recogida solo si está en bolsas con una calcomanía que demuestra el pago, o en bolsas oficiales entregadas al depositar el pago del servicio. Esto supone un incentivo económico para reciclar, ya que el reciclaje es gratuito. Oficiales de salubridad y la policía revisan los depósitos de basura para buscar aquellas bolsas donde no se verifique el pago del servicio así como antiguas cuentas y recibos que puedan dar pista de dónde provienen aquellas bolsas. Las multas por no pagar el sistema de recolección de basura van de los 200 a los 500 CHF.

La educación en Suiza es muy diversa debido a que la constitución del país delega la autoridad del sistema escolar a cada cantón. Existen escuelas públicas y privadas, incluyendo muchos colegios de renombre internacional. La edad mínima para ingresar en la escuela primaria es de seis años en todos los cantones. La escuela primaria consta de cuatro o seis grados, dependiendo de la escuela. Tradicionalmente, la primera lengua extranjera que se enseñaba en las primarias era alguno de los otros idiomas oficíales, aunque en el año 2000 en algunos cantones se empezaron a dar cursos de inglés. Al final de la escuela primaria (o al comienzo de la escuela secundaria), los alumnos están separados en varios grupos (a menudo tres) de acuerdo a sus capacidades intelectuales. Los que aprenden más rápido son inscritos en clases avanzadas para ser preparados para el examen matura o bachillerato y para estudios más específicos, mientras que los escolares que asimilan los conocimientos un poco más lentamente reciben una educación más adecuada a sus necesidades. En Suiza también se encuentra el Instituto Le Rosey, apodado «la escuela de los reyes» debido a los números monarcas que han estudiado en él. Es conocido como uno de los internados más caros y lujosos del mundo.

Existen 12 universidades en Suiza, diez de ellas son administradas a nivel cantonal y suelen ofrecer carreras no técnicas. La primera universidad del país fue fundada en 1460 en Basilea (con una facultad de Medicina), y tiene fama de ser uno de los mejores centros de investigación química y médica en Suiza. La mayor universidad del país es la Universidad de Zúrich con cerca de 25 000 estudiantes. Los dos institutos administrados por el gobierno federal, la ETH en Zúrich (fundada en 1855) y la EPFL en Lausana (fundada en 1969, anteriormente asociada a la Universidad de Lausana), gozan de una excelente reputación internacional. En 2008, la ETH Zúrich figuraba entre los mejores quince institutos del campo "Ciencias Naturales y Matemáticas" según una lista publicada por la Universidad de Shanghái Jiao Tong, mientras la EPFL se encontraba en el puesto 18.º de la categoría "Ingeniería/Tecnología y ciencias computacionales". Además, existen varias universidades de ciencias aplicadas. Suiza tiene el segundo mayor índice de estudiantes extranjeros en educación terciaria, solo por detrás de Australia.

Hay varios científicos suizos que han sido galardonados con el premio Nobel, por ejemplo, el famoso físico alemán nacionalizado suizo Albert Einstein, quien desarrolló la teoría de la relatividad mientras trabajaba en Berna. Más recientemente Vladimir Prelog, Heinrich Rohrer, Richard Ernst, Edmond Fischer, Rolf Zinkernagel y Kurt Wüthrich recibieron el premio Nobel de diversas ciencias. En total, hay 113 ganadores del premio Nobel que tienen alguna conexión con Suiza, y el Premio Nobel de la Paz ha sido entregado nueve veces a organizaciones con sede en el país.

En Ginebra se encuentra el laboratorio más grande del mundo, el CERN, dedicado a la investigación de la física de partículas. Otro importante centro de investigación es el Instituto Paul Scherrer. Entre las invenciones muy conocidas figuran el LSD, el microscopio de efecto túnel (premio Nobel) y el popular velcro. Algunas tecnologías ayudaron a la exploración de nuevos mundos, como el globo presurizado de Auguste Piccard y el batiscafo de Jacques Piccard, que le permitió llegar al punto más profundo del océano.

La Agencia Espacial Suiza, llamada Oficina Espacial Suiza, participó en el desarrollo de varios programas y tecnologías espaciales. En 1975 también fue uno de los diez fundadores de la Agencia Espacial Europea y es el séptimo contribuyente más importante para la AEE. En el sector privado, varias compañías están implicadas en la industria espacial, como Oerlikon Space y Maxon Motors.

En 2009, Suiza contaba con una población estimada en 7 725 200 habitantes. Los extranjeros que residen y trabajan temporalmente en el país conformaban en 2007 el 22,1 % de la población. La mayoría de ellos (60 %) provienen de países de la Unión Europea o de la EFTA. Los italianos son el grupo extranjero más grande del país, siendo el 17,3 % de la población extranjera total. Son seguidos por los alemanes (13,2 %), inmigrantes de Serbia y Montenegro (11,5 %) y Portugal (11,3 %). En los últimos años se produjo una fuerte inmigración albanesa, sobre todo procedente de Kosovo. Los inmigrantes de Sri Lanka, la mayoría de ellos refugiados tamiles, son el grupo asiático más grande del país. En la década de 2000, instituciones nacionales e internacionales han expresado su preocupación sobre lo que ellos creen es un incremento en la xenofobia, particularmente en algunas campañas políticas. Sin embargo, la alta proporción de ciudadanos extranjeros en el país, así como la integración de elementos extranjeros a la cultura suiza, subrayan la apertura de la sociedad suiza. Por su parte, Zúrich y Ginebra figuran en el segundo y tercer lugar de las ciudades con mejor calidad de vida en el mundo. El 99.0 % de la población está alfabetizada.

Suiza se encuentra en el cruce de algunas de las grandes culturas europeas, las cuales han influenciado fuertemente el idioma y la cultura del país. Suiza tiene tres idiomas oficiales (alemán, francés, italiano) y uno parcialmente oficial, el romanche. Todas las leyes y documentos oficiales deben estar en todas las lenguas oficiales. Los suizos suelen hablar el idioma de su región y consumir los medios de comunicación en su idioma. En el sistema educativo de Suiza, los estudiantes aprenden en el idioma nativo de su región, como segundo idioma otra lengua nacional (alemán, francés o italiano), y como tercera lengua pueden elegir entre otro idioma nacional o inglés.

En cada cantón solo un idioma nacional es oficial, normalmente el idioma local, aunque hay cantones donde hay bilingüismo e incluso trilingüismo.

El suizo-alemán(63,7 % de la población total lo habla, junto con extranjeros residentes en el país; 72,5 % de los residentes con la ciudadanía suiza en 2000) en el norte, este y centro del país

El suizo-alemán hablado en Suiza es predominantemente un grupo de dialectos del alemán conocidos como suizo-alemán, aunque en las escuelas y medios escritos se usa el alemán estándar. La mayoría de las transmisiones en radio y televisión se dan en suizo-alemán. De forma similar, existen dialectos del franco-provenzal que son hablados en algunas comunidades rurales de la parte francófona, conocida como Romandía, entre los que se encuentran el vaudois, el gruérien, el jurassien, el empro, el fribourgeois y el neuchatelois. Finalmente, en la parte italiana del país se habla el tesinés (un dialecto lombardo). Además, los tres idiomas oficiales cuentan con algunos términos que no son entendidos fuera de Suiza, por ejemplo, palabras extraídas de otro idioma (en alemán utilizan la palabra "billette" que proviene del francés), o de palabras parecidas en otro idioma (en italiano se usa el término "azione" no para "acción", sino como "descontar o rebajar", que proviene del alemán "Aktion"). Aprender otro de los idiomas nacionales es obligatorio para todos los escolares suizos, por lo que se supone que la mayoría de los suizos son bilingües.

El francés es hablado por un 20,4 %; siendo lengua materna (incluyendo extranjeros) para el 21,0 %. Se habla en el oeste.

El italiano es hablado por un 6,5 %; siendo lengua materna (incluyendo extranjeros) para el 4,3 %. Se habla en el sur.. 

El romanche, una lengua romance que es hablada localmente por una minoría (0,5 %; 0,6 %) en el sureste, en el cantón de Grisones, es designado por la constitución federal como un idioma nacional junto con el alemán, el francés y el italiano (artículo 4 de la constitución), y como un idioma oficial si las autoridades desean comunicarse con personas que hablan este idioma (artículo 70), pero las leyes federales y otros documentos oficiales no deben ser escritas obligatoriamente en este idioma. El gobierno federal debe de comunicarse en los idiomas oficiales, y en el parlamento federal se da una interpretación simultánea en alemán, francés e italiano.

En 2006 la esperanza de vida al nacer era de 79 años para los hombres y 84 años para las mujeres, una de las más altas en el mundo. Los ciudadanos suizos cuentan con un seguro médico que es obligatorio, permitiendo el acceso a una amplia variedad de servicios médicos modernos. Sin embargo, los gastos en los cuidados para la salud son particularmente altos, ya que desde 1990 se ha registrado un aumento en la cantidad de presupuesto que se utiliza para cubrir los gastos médicos, que para 2003 ocupaban el 11,5 % del PIB; esta situación se ha reflejado en los altos precios de los servicios dados. Con una población cada vez más anciana y nuevas tecnologías en el cuidado de la salud, se espera que estos gastos continúen en aumento.

Entre dos tercios y tres cuartas partes de la población vive en zonas urbanas. Suiza pasó de ser un país rural a uno urbanizado en solo setenta años. Desde 1935 el desarrollo urbano ocupó gran parte del paisaje suizo desocupado los últimos 2000 años. Esta dispersión urbana no solo afecta a la meseta suiza, sino también a las montañas de Jura y de los Alpes y continúan aumentando las concesiones para el uso de la tierra. Sin embargo, desde principios del siglo XXI, el crecimiento de la población es mayor en las zonas urbanas que en cualquier otra área.

Suiza cuenta con una densa red de ciudades, donde las poblaciones grandes, medianas y pequeñas se complementan. La meseta suiza está densamente poblada, con una población relativa de 450 hab./km² y el paisaje continuamente muestra signos de la presencia del hombre. El tamaño de las áreas metropolitanas más grandes: Zúrich, Ginebra-Lausana, Basilea y Berna, tiende a incrementarse. En una comparación internacional la importancia de estas áreas urbanas es mayor de lo que sugiere su número de habitantes. Además, las dos ciudades de Zúrich y Ginebra son reconocidas por la buena calidad de vida que ofrecen.

Suiza no tiene ninguna religión de estado oficial, aunque la mayoría de los cantones (excepto el de Ginebra y el de Neuchâtel) reconocen sus propias iglesias oficiales. En todos los casos incluyen la Iglesia católica y la Iglesia reformada de Suiza que son financiadas con el impuesto eclesiástico. Estas iglesias, y en algunos cantones la Iglesia católica antigua y las congregaciones judías, son financiadas por diezmos pagados por los creyentes.

El cristianismo es la religión predominante en Suiza, dividido entre la Iglesia católica (41,8 % de la población) y varias iglesias protestantes (40 %). La inmigración ha traído el islam (4,3 %, predominante entre los kosovares y los turcos) y a la Iglesia ortodoxa (1,8 %) como las minorías religiosas más importantes. La encuesta del Eurobarómetro de 2005 anunció que el 48 % de los suizos entrevistados era teísta, el 39 % expresó creer en "un espíritu o una fuerza de la vida", el 9 % era ateo y el 4 % agnóstico. El 30 de noviembre de 2009 el 57,5 % de los suizos votó a favor de la prohibición de los alminares en el país, lo que ocasionó que se llevaran a cabo varias protestas en varias partes del mundo por parte de musulmanes.

El país ha estado históricamente dividido entre los católicos y los protestantes, con una compleja mezcla de territorios con mayorías católicas y protestantes por todo el país. En 1597, el cantón de Appenzell fue oficialmente dividido en dos para los católicos y protestantes. Las ciudades más grandes (Berna, Zúrich y Basilea) son predominantemente protestantes. El centro del país, así como el Tesino, son tradicionalmente católicos. La constitución federal de 1848, bajo la reciente impresión de los enfrentamientos entre los cantones católicos y protestantes que culminaron en la "Sonderbundskrieg", define un Estado consociacional, permitiendo la coexistencia pacífica entre ambos grupos. En 1980 se votó una iniciativa para separar completamente la iglesia y el Estado pero fue rechazada, con solo el 21,1 % de la población a favor.

La cultura de Suiza está influida por los países vecinos, pero a través de los años se ha desarrollado una cultura distinta e independiente con algunas diferencias regionales. En particular, las regiones francófonas se orientaron más hacia la cultura francesa. En general, los suizos son conocidos por su larga tradición humanitaria, ya que Suiza fue el lugar de nacimiento del movimiento de la Cruz Roja y alberga al Consejo de Derechos Humanos de las Naciones Unidas. De forma similar, en la Suiza alemana están más orientados hacia la cultura alemana, aunque los hablantes del suizo alemán se identifican estrictamente como suizos debido a la diferencia entre el alto alemán y los dialectos del alemán suizo. En la Suiza italiana se percibe mayormente la cultura italiana. En resumen, una región tiene una conexión cultural más estrecha con el país vecino que comparte su idioma. La lingüísticamente aislada cultura romanche en las montañas del este de Suiza se esfuerza por mantener viva sus tradiciones no solo lingüísticas.

Muchas zonas montañosas están altamente conectadas con las culturas deportivas del esquí en invierno y del senderismo en verano. A lo largo del año, algunas zonas tienen una cultura de ocio para atraer el turismo, incluso en primavera y verano, las estaciones más tranquilas, cuando hay menos visitantes y mayor presencia suiza. Una tradicional cultura de granjas y cultivos también predomina en algunas zonas y las pequeñas granjas continúan omnipresentes en las afueras de las ciudades.

En el cine, las producciones estadounidenses conforman la gran mayoría de las carteleras, aunque varias películas suizas han tenido éxito comercial. El arte folclórico se mantiene vivo gracias a varias organizaciones ubicadas a lo largo del territorio nacional, donde se fomenta la música, la danza, la poesía, la talla de madera y el bordado. La trompa de los Alpes, una trompa hecha de madera, junto con el yodel y el acordeón, se han convertido en el símbolo internacional de la música suiza tradicional.

Como la confederación, desde su fundación en 1291, estuvo compuesta casi exclusivamente por regiones de habla alemana, las primeras obras literarias están en alemán. En el siglo XVIII, el francés se convirtió en el idioma de moda en Berna y otras regiones, mientras la influencia de los aliados francófonos y otros territorios se iba marcando más que antes.

Entre los autores clásicos de la literatura suiza en alemán se encuentran Jeremias Gotthelf (1797-1854), Gottfried Keller (1819-1890) y Conrad Ferdinand Meyer (1825-1898). Los cuatro máximos representantes de la literatura suiza del siglo XX son Carl Spitteler (1845-1924) (Premio Nobel de Literatura, 1919), Robert Walser (1878-1956), Max Frisch (1911-1991) y Friedrich Dürrenmatt (1921-1990), autor de "Die Physiker" ("Los físicos") y "Das Versprechen" ("La promesa").

Los escritores suizos francófonos más prominentes son Jean-Jacques Rousseau (1712-1778), Germaine de Stael (1766-1817) y Benjamin Constant (1767-1830). Autores más recientes incluyen a Blaise Cendrars (nacido Frédéric Sauser, 1887-1961), a Charles Ferdinand Ramuz (1878-1947), cuyas novelas describen la vida de los campesinos que habitaban las zonas montañosas, en una época decadente, a Gustave Roud (1897-1976) y a Philippe Jaccottet (n. 1925). Autores de habla italiana y romanche también han contribuido a la literatura suiza, pero de una forma más modesta.

Ferdinand de Saussure, lingüista suizo, padre del estructuralismo.

Probablemente, la creación más famosa de la literatura suiza sea "Heidi", la historia de una niña huérfana que vive con su abuelo en los Alpes, uno de los libros para niños más populares en el mundo que se ha convertido en un símbolo de Suiza. Su creadora, Johanna Spyri (1827-1901), escribió otras obras con temas similares.

El concepto de protección del patrimonio apareció en el país a finales del siglo XIX. Además, siete sitios culturales son parte del patrimonio de la humanidad: la ciudad vieja de Berna, la abadía de San Galo, el convento benedictino de Saint-Jean-des-Sœurs, los tres castillos de Bellinzona, los viñedos Lavaux, el ferrocarril rético en el paisaje del Albula y del Bernina y el urbanismo relojero de las villas de La Chaux-de-Fonds y del Locle.

Muchos castillos y fortificaciones fueron construidos en la Edad Media por las familias dinásticas, que les sirvieron tanto como residencias como de lugares defensivos, destacando el castillo de Chillon, Lenzburgo, Mesocco, Burgdorf, Kyburgo o los tres castillos de Bellinzona. Las villas medievales estaban fortificadas y algunas, como Murten/Morat, conservan y mantienen sus murallas, aunque en la mayoría de los casos solo quedan vestigios en el corazón de ciudades como de Zug, la "puerta de Spalen" en Basilea o la de Berna.

Los edificios religiosos aparecieron a partir del siglo VI y se construyeron conventos, monasterios, iglesias y catedrales, entre los que sobresalen la abadía de San Galo, la abadía de Einsiedeln, la abadía de San Mauricio de Agaune, la catedral de Basilea, la abadía de Romainmôtier y la catedral de Lausana.

Hay edificios públicos, algunos que datan de la época romana, como el anfiteatro de Avenches, y también ayuntamientos, siendo el más antiguo el de Berna (1406). El ayuntamiento de Basilea (1504-1514), con sus fachadas de color rojo, es muy característico. La torre cuadrada en el patio del ayuntamiento de Ginebra (1555) es una edificación típica de la tradición renacentista francesa en piedra tallada. En el siglo XIX, se elevan nuevos edificios públicos para oficinas de correos, estaciones de ferrocarril, museos, teatros, iglesias y escuelas, como el palacio Federal, la estación central de Zúrich, el Museo Nacional Suizo, el Gran Teatro de Ginebra y la Universidad de Zúrich.

Suiza tiene algunos conjuntos urbanos notable: el casco antiguo de Berna, con sus soportales, plazas y fuentes, es representativo de la ciudad medieval en Europa. Al final del siglo XIX, nacen nuevos distritos en el lugar que ocupaban las antiguas fortificaciones de las grandes ciudades, como la Bahnhofstrasse de Zúrich o el Cinturón fazyste de Ginebra. El crecimiento es objeto de planificación urbana: en 1834, La Chaux-de-Fond, que fue destruida por el fuego, se reconstruyó de acuerdo a una nueva estructura urbana (véase Ensemble urbain du XIXe siècle de La Chaux-de-Fonds). A principios del siglo XX se crean viviendas para los trabajadores basándose en el modelo de la Werkbund, como la parcelación de Freidorf (1919-1921) en Muttenz, síntesis entre el ideal de la ciudad jardín y del movimiento cooperativo. En el periodo comprendido entre 1945 y 1975 se construyeron en los suburbios de las grandes ciudades nuevas ciudades satélites, como Le Lignon en las afueras de Ginebra.

Desde el siglo XV, aparecen las casas civiles de estilo gótico en piedra, por ejemplo, el Grimmenturm de la Spiegelgasse en Zúrich, la casa Tavel en Ginebra, el "Haus zum Rüden" en Zúrich, el "Haus zum Ritter" en Schaffhausen, el "hotel de Ratzé" (1583-1586) en Friburgo y la "casa Serodine" (1620) en Ascona. Durante el Renacimiento, se abrieron arcadas en el Ticino y en el patio del castillo de Muralto, el antiguo "Palazzo Rusca", en Lugano, y el "Colleggio Papio", en Ascona. En la Suiza alemana, el primer edificio renacentista fue el palacio Ritter (1556), en Lucerna.

Las casas particulares barrocas estaban ricamente decoradas con ménsulas en uno o varios pisos, como en Schaffhausen, y tenían miradores de madera o piedra, como en San Galo. Por ejemplo, el "Herrenstube" y el "Frontwagenturm" en Schaffhausen. En Zúrich, se erigieron dos casas de corporaciones en piedra tallada y presentan un aspecto severo: "Zimmerleuten" (1708) y "Saffran" (1719-1723). La región occidental está más influenciada por la arquitectura barroca francesa; este estilo se impuso en la Suiza romanda a finales del siglo XVII. Se trata de verdaderas mansiones en la rue des Granges, en Ginebra, con patio de honor. También hay ejemplos de estilo rococó.

Desde 1800, se diseñaron grandes villas clasicistas como el palacio Eynard (1817-1821) en Ginebra. Más tarde, en el siglo XX aparecen algunas realizaciones arquitectónicas del movimiento Moderno: la villa Le Lac (1923) y el inmueble Clarté (1931) en Ginebra de Le Corbusier, o la Cité Halen (1957-1961) del Atelier 5, cerca de Berna, un ejemplo de casas individuales contiguas en terraza para la clase media. 

La gran diversidad de espacios naturales en Suiza se refleja en la gran variedad de casas rústicas, que se construyen en diversas variedades alpinas: las "Gotthardhaus" ('casas del Gotardo'), de madera, que se encuentran en aislados valles de montaña del Ticino, del Valais y en los Grisones; casa "valaisanne", del Valais, de madera, típica de la región del Valais y del Val d'Hérens; la casa tisinesa, del Ticino, en moellons; la casa engadinesa decorada con pinturas murales y Sgraffitems; las casas del Oberland bernés y Simmental, de madera maciza trabajada con sierra, "Strickbau" o en maderos cuadrados, cortados con un hacha.

En la meseta suiza, la casa bernesa, cubiertas con un enormes tejados en caballete con carpinterías decoradas con motivos esculpidos; las chaumières (cabañas) argovianas las casas à colombages de madera en la meseta oriental y en Zúrich; las granjas de usos múltiples ("Dreisässenhäuser") del noroeste y de la meseta romanda, construidas en piedra.

En el Jura, las granjas jurasinas tienen grandes fachadas piñón, totalmente en piedra recibidas con cal.

Las infraestructuras, como puentes y túneles, son numerosas. El puente del Diablo en el corazón de los Alpes en el camino hacia el paso de San Gotardo o el Mittlere Brücke, sobre el Rin en Basilea, son ejemplos históricos. Muchos puentes medievales son de madera como el Kapellbrücke, en Lucerna. En el siglo XIX, se construyeron algunas puentes suspendidos de cables de acero en Ginebra (puente de San Antonio) y Friburgo (Gran puente), que en 1834, en el momento de su construcción, era el más largo de su género. Muchos puentes y túneles para el ferrocarril, como el viaducto de Landwasser, los túneles de del Gotardo y del Simplon se construyeron en el cambio del siglo XX. El puente de Salginatobel o el viaducto de Chillon son obras carreteras del siglo XX.

La libertad de prensa y el derecho de libre expresión están reconocidos por la constitución de Suiza. La Agencia de Noticias Suiza (SNA) transmite durante todo el día información sobre política, sociedad, economía y cultura en los tres idiomas oficiales. La SNA es la que aporta casi todas las noticias sobre Suiza, y varios servicios de noticias extranjeros colaboran con ella.

Históricamente, Suiza ha tenido el mayor número de periódicos publicados en proporción a su población y tamaño. Los periódicos más influyentes son el "Tages-Anzeiger", el "Neue Zürcher Zeitung" (ambos en alemán) y "Le Temps" (en francés), pero casi cada ciudad cuenta con su periódico local. La diversidad cultural del país contribuye a la publicación de múltiples periódicos.

En contraste a los medios impresos, las radiodifusoras siempre han estado en gran parte bajo el control del gobierno. La Radiodifusora Suiza, cuyo nombre recientemente se cambió a SRG SSR, es la encargada de producir y transmitir varios programas nacionales de radio y televisión. Los estudios de la SRG SSR están distribuidos a través de las diferentes regiones lingüísticas. Los programas de radio son producidos en seis estudios centrales y cuatro estudios locales, mientras que los programas de televisión se realizan en Zúrich (SF), Ginebra (TSR), Lugano (RTSI) y Coira (RTR). Una gran compañía de transmisión por cable también permite el acceso de la población suiza a los programas de países vecinos.

La gastronomía de Suiza es multifacética. Mientras algunos platos como la "fondue", la "raclette" o el "rösti" están presentes en todas las cocinas del país, cada región desarrolló su propia gastronomía, coincidiendo cada zona gastronómica con las distintas zonas lingüísticas. La cocina tradicional suiza usa ingredientes parecidos a los de otros países europeos, entre otros productos lácteos y quesos como el gruyer o el emmental, producido en valles de Gruyère y de Emmental, de donde toman sus nombres.

El chocolate se ha fabricado en Suiza desde el siglo XVIII, pero ganó su reputación a finales del siglo XIX con la invención de técnicas más modernas, como el conchado y el templado, que ayudaron a mejorar la calidad de los productos. Además, otro de los grandes adelantos suizos en esta industria fue la invención del chocolate con leche en 1875 por Daniel Peter. por ello una de las tartas típicas suizas es la "Tre Choklad" (Tarta de tres chocolates).

El vino, principalmente blanco, se produce sobre todo en Valais, Vaud, Ginebra y Tesino. Los viñedos han existido en la zona desde la época de los romanos, e incluso se hallaron vestigios que podrían datar de fechas anteriores. Las variedades más producidas son el Chasselas (llamado "Fendant" en Valais) y el Pinot Noir. El Merlot es la principal variedad producida en Tesino.

En algunas zonas rurales como St. Gallen o Appenzell aún mantienen la tradición de consumir carne de perro.

Gran parte de los deportes más populares en Suiza son deportes de invierno. El esquí y el montañismo son muy practicados en el país tanto por suizos como por extranjeros, ya que sus cumbres nevadas atraen a alpinistas de todo el mundo. El país ha organizado múltiples campeonatos y torneos mundiales de deportes invernales, incluyendo dos ediciones de los juegos olímpicos de invierno en 1928 y 1948, ambos en Sankt Moritz. Además, en Engelberg, se celebra anualmente una de las pruebas de la Copa de Mundo de saltos de esquí.

Muchos suizos también son seguidores del hockey sobre hielo y apoyan a uno de los 12 clubes en la Liga A. En abril de 2009 Suiza fue la sede del Campeonato Mundial de Hockey sobre Hielo, por décima ocasión.

Como otros europeos, muchos suizos son aficionados del fútbol. El país cuenta con su propia selección nacional, organizada por la Asociación Suiza de Fútbol. Ha disputado ocho Copas del Mundo, siendo los cuartos de final su mejor resultado. Además ha participado en cuatro Eurocopas, donde solo en una, la Eurocopa 2016, ha pasado de la primera fase. Suiza organizó la Copa Mundial de Fútbol de 1954, así como la Eurocopa 2008 junto a Austria. La principal competición de fútbol del país es la Super Liga Suiza.

El automovilismo, el motociclismo y otros deportes similares fueron prohibidos en Suiza después del desastre de Le Mans en 1955 con la excepción de eventos como la carrera de montaña. Esta prohibición fue retirada en junio de 2007. Durante este periodo, siguieron surgiendo en varias regiones del país varios corredores exitosos como Clay Regazzoni, Jo Siffert y Alain Menu, y Peter Sauber fundó la escudería que lleva su nombre y que lleva comptiendo en la Fórmula 1 desde el año 1992.

El ciclismo es otro deporte que también cuenta con una amplia promoción y participación. En Suiza, se celebran gran variedad de pruebas ciclistas como la Vuelta a Suiza y el Tour de Romandía, además de que el país ha sido sede de campeonatos internacionales como el Campeonato Mundial de Ciclismo en Ruta. Entre los ciclistas suizos más destacados se encuentran Fabian Cancellara, Alex Zülle y Tony Rominger.

El tenis ha cobrado popularidad en Suiza, con jugadores de la talla de Roger Federer, Stanislas Wawrinka y Martina Hingis.

En balonmano, la Selección de balonmano de Suiza ha logrado la medalla de bronce en los Juegos Olímpicos de 1936, además de los cuartos puestos logrados en el Campeonato Mundial de Balonmano Masculino de 1954 y en el Campeonato Mundial de Balonmano Masculino de 1993.

Además existen otros deportes donde varios deportistas suizos han sido exitosos, como la esgrima (Marcel Fischer), el piragüismo (Ronnie Dürrenmatt), la vela (Alinghi), el kayakismo (Mathias Röthenmund), el voleibol (Sascha Heyer, Markus Egger, Paul y Martin Laciga), snowboard (Martina Weber), entre otros.

Suiza es junto a Australia, Francia y el Reino Unido, uno de los cuatro únicos países que se han hecho presentes en todas las ediciones de los Juegos Olímpicos.
Los deportes tradicionales suizos incluyen la lucha llamada "Schwingen", una antigua tradición de los cantones rurales del centro del país. El "steinstossen" es la variante suiza del lanzamiento de peso, una competición donde se arroja lo más lejos posible una pesada piedra. Practicado entre la población alpina desde la época prehistórica, se popularizó en Basilea alrededor del siglo XIII. El hornussen es otro deporte autóctono de Suiza, el cual es una mezcla entre el béisbol y el golf y es practicado principalmente en la zona norte del país.

En Suiza se ubican las sedes de numerosos organismos deportivos, entre ellos el Comité Olímpico Internacional, el Tribunal de Arbitraje Deportivo (TAS), la FIFA, la UEFA, y las federaciones internacionales de baloncesto (FIBA), hockey sobre césped (FIH), hockey sobre hielo (IIHF), voleibol (FIVB), balonmano (IHF), tenis de mesa (ITTF), esquí (FIS), patinaje sobre hielo (ISU) y natación (FINA).











</doc>
<doc id="2691" url="https://es.wikipedia.org/wiki?curid=2691" title="Siglo X a. C.">
Siglo X a. C.

El siglo X a. C. comenzó el 1 de enero de 1000 a. C. y terminó el 31 de diciembre de 901 a. C.


</doc>
<doc id="2692" url="https://es.wikipedia.org/wiki?curid=2692" title="Saskatchewan">
Saskatchewan

Saskatchewan () es una provincia del oeste de Canadá, es la provincia central de las Praderas canadienses. Su capital es Regina y su ciudad más poblada es Saskatoon.

La mayor parte de su población se concentra en la parte sur de la provincia. La agricultura es una parte fundamental en la economía de Saskatchewan, sobre todo el trigo, del que se cosecha el 45% de todo Canadá. Otra fuente fundamental de la economía de la provincia es la minería. Saskatchewan es la mayor productora de uranio del mundo.

El nombre de la provincia proviene del río Saskatchewan, cuyo nombre deriva del cree: "kisiskāciwani-sīpiy", que significa "río de curso veloz".

A grandes rasgos, Saskatchewan tiene forma de trapecio, con un área de 588.276,09 km². Sin embargo, debido a su tamaño, los límites septentrional y meridional, que son segmentos de los paralelos 49° norte y 60° norte, respectivamente, presentan una curvatura apreciable. Además, el límite oriental de la provincia está parcialmente torcido en lugar de seguir un meridiano, cuando las líneas de corrección fueron ideadas por topógrafos antes del programa (1880-1928). Saskatchewan limita al oeste con Alberta, al norte con los Territorios del Noroeste, al este con Manitoba, y al sur con los estados estadounidenses de Montana y Dakota del Norte. Saskatchewan es la única provincia canadiense en la que ninguna de sus fronteras se corresponden con rasgos geográficos físicos. Es también una de las dos únicas provincias sin salida al mar, junto con Alberta.

Saskatchewan está formada por dos regiones naturales principales: el Escudo Canadiense en el norte y las Llanuras Interiores en el sur. El norte de Saskatchewan está cubierto principalmente por el bosque boreal excepto las Dunas de Arena del Lago Athabasca, las dunas de arena activas más grandes del mundo al norte del paralelo 58°, adyacentes a la orilla sur del lago Athabasca. El sur de Saskatchewan contiene otra área con dunas de arena conocidas como "las Grandes Colinas de Arena" que cubren 300 km². Las Colinas del Ciprés (Cypress Hills), localizadas en el borde sudoeste de Saskatchewan y "Killdeer Badlands" (Grasslands National Park) son áreas de la provincia que permanecieron sin congelarse durante el último período de glaciación.

El punto más alto de la provincia, a 1.468 metros de altura, está localizado en las Colinas del Ciprés. El punto más bajo, con 213 metros, está en la orilla de lago Athabasca en el extremo norte. La provincia tiene nueve cuencas hidrográficas distintas formadas por varios ríos que desembocan en aguas del océano Ártico, bahía de Hudson y golfo de México.

Saskatchewan está lejos de cualquier masa significativa de agua. Esto, combinado con su latitud norte produce un verano frío tipo clima continental húmedo (en la clasificación climática de Köppen, Dfb) en la mitad este, y clima de estepa de seco a semiárido (en la clasificación de Köppen, Bsk) en la parte occidental de la provincia. Los veranos pueden ser muy calientes, con temperaturas por encima de 32 °C durante el día. Hay rachas de vientos cálidos del sur procedentes de los Estados Unidos durante la mayor parte de julio y agosto. Si bien los inviernos pueden ser penetrantes, con temperaturas máximas por debajo de -17 °C durante semanas, los vientos "chinook" (vientos cálidos y húmedos) a menudo soplan del sur, trayendo períodos suaves. La precipitación anual media va de 30 a 45 cm a lo largo de la provincia, con el grueso de la lluvia en junio, julio y agosto.

Los diez municipios más grandes según su población
La siguiente lista no incluye a Lloydminster, que tiene una población total de 23.632 habitantes y se extiende a ambos lados de la frontera con Alberta. De acuerdo con el censo de 2001, sólo 7.840 personas vivían en el sector perteneciente a Saskatchewan, lo que posicionaría a esta ciudad en el undécimo lugar respecto a los municipios más grandes de la provincia. Todas las comunidades relacionadas son consideradas ciudades por la provincia, con la excepción de Corman Park, que es una municipalidad rural. Los municipios de la provincia con una población de 5.000 o más habitantes reciben oficialmente el estatus de ciudad.

Antes de la llegada de los europeos, Saskatchewan se encontraba habitada por las tribus athabaskan, algonquiana, y sioux. El primer europeo en instalarse en Saskatchewan fue Henry Kelsey en 1690, que navegó a lo largo del río Saskatchewan en un intento por comerciar con piel, comprándosela a los indígenas de la zona. El primer establecimiento de origen europeo fue la Compañía de la Bahía de Hudson, situada en Cumberland House y fundada por Samuel Hearne en 1774.

Tras la venta de la Luisiana en 1803 por Francia a Estados Unidos, parte de las provincias actuales de Alberta y Saskatchewan, en el actual Canadá, pasaron a Estados Unidos, que cedería esa parte al Reino Unido en 1818.

A mediados del siglo XIX, las expediciones científicas encabezadas por John Palliser y Henry Youle Hind exploraron la región de la pradera provincial.

En la década de 1870, el Gobierno de Canadá formó los Territorios del Noroeste para administrar al vasto territorio comprendido entre la Columbia Británica y Manitoba. El gobierno accedió también a la firma de una serie de tratados con los nativoamericanos de su entorno, lo que fomentó la relación entre las "Primeras Naciones" (en inglés, "First Nations"), como se las conoce hoy en día, y la Corona. Poco después, las Primeras Naciones se verían empujadas a ciertas reservas.

La colonización de la provincia cogió vuelo cuando la Canadian Pacific Railway (Vía canadiense del Pacífico) fue construida a comienzos de los años 1880, y el gobierno federal dividió a la tierra conforme a la "Medición del dominio terrestre", otorgando fanegas libres a colonos voluntariosos.

La policía montada del noroeste edificó unos cuantos puestos y fortificaciones a lo largo de Saskatchewan, entre los que destacan: Fort Walsh en los Montes Cipreses, y Wood Mountain, puesto en el centro meridional de Saskatchewan, cerca de la frontera con Estados Unidos.

En 1876, siguiendo a la Batalla del Pequeño Cuerno Grande el cacique de los Lakota, Toro Sentado, guio a su gente hacia Wood Mountain, cuya reserva se fundó en 1914.

Muchos integrantes de los Métis, que no habían sido signatarios de tratado alguno, se trasladaron al distrito de Saskatchewan Rivers, al norte de la actual Saskatoon, tras la Rebelión del Red River que tuvo lugar en Manitoba en 1870. A comienzos de la década de 1880, el gobierno de Canadá rehusó oír las quejas de los Métis, que partían de temas vinculados a la ocupación territorial. Finalmente, en 1885, los Métis, mandados por Louis Riel, provocaron la Rebelión del Noroeste en reclamo de un gobierno provisional. Fueron vencidos por la milicia canadiense asentada en las praderas de la "Canadian Pacific Railway". Riel se rindió y fue declarado culpable de traición por un tribunal de Regina. Finalmente fue ejecutado el 16 de noviembre de 1885.

Con la llegada de más colonos a la región, la población fue creciendo, y Saskatchewan pasó a ser considerada una provincia el 1 de septiembre de 1905; el día de inauguración fue el 4 de ese mes.

El "Homestead Act" permitía a los colonos adquirir millas cuadradas de tierra para el cercamiento de haciendas, y ofrecía un cuarto adicional tras realizar el cometido inicial de dicha concesión. La inmigración llegó a la cima en 1910 y, pese a las dificultades de la vida fronteriza y de su alejamiento respecto a la ciudad y sus ventajas, se consiguió establecer una sociedad agraria próspera y estable.

En 1913 la Asociación de Criadores de Ganado de Saskatchewan logró establecerse como la primera organización agrícola de la provincia. La convención fundacional de 1913 había fijado, para entonces, tres objetivos principales que servirían de orientación: vigilar la legislación; seguir los intereses de la CG en la forma más honorable y legítima posible; y sugerir al parlamento el cambio de condiciones y requisitos cuando lo estime conveniente. En 1970, la primera reunión anual en Canadá tuvo lugar en Regina.

La economía de Saskatchewan es tradicionalmente agrícola; sin embargo, la emergente diversificación ha propiciado que ahora esta actividad, junto a la forestación, la pesca y la caza constituyan tan sólo el 6,8% del PIB de la provincia. El trigo es el cultivo más común, y quizás el único representativo de Saskatchewan, pero también están presentes otros como colza, lino, centeno, avena, arvejas, lentejas, mijo, y cebada. Asimismo, la minería es de vital importancia para la provincia. Saskatchewan es el primer exportador mundial de potasa. En la zona septentrional la actividad forestal recobra cierta relevancia.

Saskatchewan es también el mayor proveedor de uranio del mundo, y abastece a la mayor parte del hemisferio occidental. La industria de este mineral es seguida de cerca por el gobierno provincial que avala su cotización en el mercado internacional.

El PIB de Saskatchewan era en 2003 de 32 mil millones de dólares canadienses, con sectores económicos que se dividían de la siguiente manera:

Las compañías importantes que se encuentran en Sasckatchewan son la Hill family's Harvard Developments, Viterra (previamente Saskatchewan Wheat Pool), la Concentra Financial Services, la metalúrgica Ipsco (aunque su base operacional se asiente en Lisle, un barrio de Chicago), la productora de maquinaria agrícola Brandt Industries, PotashCorp y Cameco.

La Crown corporation incluye a las entidades más destacadas de la provincia: SaskTel, SaskEnergy (proveedora de gas natural), y SaskPower. El Bombardier opera en el NATO Flying Training Centre (Centro de entrenamiento aéreo de la OTAN) en 15 Wing, próximo a Moose Jaw. Bombardier obtuvo un contrato a largo plazo a fines de los años 1990, por un monto de 2,8 mil millones del gobierno federal, para la compra de material aeronáutico militar y la gestión de adiestramiento.

Saskatchewan posee la misma forma de gobierno que otras provincias canadienses, con su Teniente-Gobernador (que representa a la monarquía), el "Premier" (o Primer Ministro), y una legislatura unicameral.

Durante muchos años, Saskatchewan ha sido una de las provincias de Canadá más de izquierdas, reflejando la voluntad de muchos de sus ciudadanos en cuestiones de alienación por los intereses del gran capital. En 1944 Tommy Douglas se convirtió en "premier" y estableció el primer gobierno socialista regional de Norteamérica. La mayor parte de sus MAL (Miembros de la Asamblea Legislativa) representaban a pequeños pueblos y predios rurales. Bajo su Cooperative Commonwealth Federation (CCF), el gobierno de Saskatchewan haría de ésta la primera provincia en contar con servicio de atención médica general. En 1961, Douglas abdicó de su cargo para convertirse en la primera figura política federal del Nuevo Partido Democrático.

A lo largo del período de posguerra, la CCF y sus sucesores, los Nuevos Demócratas de Saskatchewan, dominaron el campo político de la mano de Douglas Allan Blakeney, y Roy Romanow, todos sirviendo como "premiers" durante un tiempo, y transformándose en figuras nacionales. La urbanización desde la Segunda Guerra Mundial había alterado la economía provincial al despojarla de su base agrícola, lo que ocasionó una ligera emigración de los campos a la ciudad. Como resultado, hubo un correspondiente cambio en la ideología del NDS, que pasó a preocuparse más de los asuntos urbanos que de los rurales.

El Partido Liberal de Saskatchewan fue el principal en el poder durante gran parte de los primeros años de vida de la provincia, gobernando de 1905 a 1929 y de 1934 a 1944. Emergió nuevamente en 1964, pero se volvió insignificante tras la derrota del gobierno liberal de Ross Thatcher en 1971. El Partido Progresivo Conservador de Saskatchewan encabezado por Grant Devine, reemplazó gradualmente a los liberales como el nuevo rival del NDS, consiguiendo una apabullante victoria en la "Matanza del lunes por la noche" (Monday Night Massacre) de 1982. No obstante, la popularidad de los conservadores cayó en picado a causa de los grandes déficits, aliándose con el gobierno federal de Mulroney en 1991. Muchos miembros de la Asamblea Legislativa, incluyendo a algunos ministros de gabinete, fueron declarados culpables de apropiación de fondos públicos, por lo que el Partido Conservador fue suspendido, aunque recientemente ha anunciado su intención de prsentarse a la próxima elección provincial.

Actualmente, la oposición oficial en la provincia la representa el Partido de Saskatchewan, una nueva facción política fundada en 1997 y que comprende a los antiguos simpatizantes de los "Tories", a los primeros liberales e incluso a algunos neodemócratas frustrados por la incapacidad de evolución del NDS en materia de economía y población. El actual "premier" de Saskatchewan es el neodemócrata Lorne Calvert, cuyo gobierno fue reelecto en la elección general de la provincia en 2003, por la mínima mayoría posible: el NDS obtuvo 30 de los 58 escaños de la Asamblea Legislativa y el PS los 28 restantes. Los primeros representan a ciudades y pueblos, y los segundos se centran mayoritariamente en la defensa del ámbito rural. Las "Primeras Naciones" y los Métis se hallan involucrados en la política y otras instituciones pero su representación es muy escasa. Un largo debate entre los círculos académicos canadienses gira en torno a si la extensión del sufragio a las "Primeras Naciones" inadvertidamente "regulariza" su papel de miembro de las naciones que han firmado tratados internacionales con la Corona en momentos en los que la etnia local era diferente.

Además de los tres largos períodos del NDS como gobierno provincial, Sakatchewan se inclina más hacia el derecho de política federal. De las 14 dependencias federales de la provincia, 12 son comúnmente ocupadas por miembros conservadores del Parlamento. Mientras que Sakatchewan dispone de una mayoría gubernamental del NDS, el NDS federal ha sido desplazado de la provincia durante dos elecciones consecutivas. Los únicos liberales son el Ministro de Finanzas Ralph Goodale, y Gary Merasty, primer Jefe Supremo del Consejo Superior de Prince Albert, cuya elección trajo a flote alegaciones de un posible fraude.

En 2005, Saskatchewan celebró su centenario. Para tal efecto, la Royal Canadian Mint (Real Ceca del Canadá) puso en circulación una moneda de 5 dólares canadienses conmemorativa que representaba los campos de trigo de la provincia. Además, se acuñó otra moneda similar de 25 centavos. La reina Isabel II de Inglaterra asistió a la ceremonia, y la cantante canadiense Joni Mitchell publicó un álbum en homenaje a Saskatchewan.

Aunque los habitantes de ascendencia europea componen la mayoría de la población, los aborígenes constituyen una minoría bastante considerable. Las etnias que no pertenecen a ninguno de los susodichos grupos son, en proporción, insignificantes.

"Fuente: Statistics Canada"

Origen étnico
"Nota: En el cuestionario del censo de 2001 un individuo podía hacer constar más de un origen étnico, por lo que la suma de las cifras que siguen sobrepasa el 100%."


La primera educación en las praderas fue impartida dentro del grupo familiar de la Primera Nación o las primeras familias de comerciantes de pieles. Había sólo unos pocos misioneros o escuelas de puestos de comercio establecidas en la Tierra de Rupert, más tarde conocida como los Territorios del Noroeste.

En 1886 se forman los 76 primeros distritos escolares de los Territorios del Noroeste y la primera reunión del Consejo de Educación. El gran incremento de la inmigración condujo a la formación de bloques étnicos. Las comunidades buscaban para sus hijos una educación similar a la de las escuelas de sus lugares de procedencia. Se construyen cabañas de troncos y residencias para las asambleas de la comunidad, escuelas, iglesias, bailes y reuniones.

Los prósperos años veinte y los agricultores que se establecieron con éxito en sus haciendas proporcionaron la financiación para regularizar la educación. Los libros de texto, las escuelas normales para profesores educados formalmente, los planes de estudio escolares y los programas arquitectónicos estatales de escuelas de arte proporcionaron continuidad cultural a toda la provincia. El inglés como lengua escolar ayudó a proporcionar la estabilidad económica al poder comerciar unos con otros. El número de escuelas de distrito individuales a lo largo de Saskatchewan alcanzó aproximadamente el número de 5.000, a la altura de las escuelas de distrito individuales del sistema educativo a finales de los años 1940.

Tras la Segunda Guerra Mundial, la transición de muchas escuelas individuales a un menor número pero mayores y tecnológicamente modernas escuelas de ciudad se produjo como un medio de asegurar la educación técnica. Los autobuses escolares, las carreteras, y los vehículos familiares permitieron estudiar en escuelas alejadas del lugar de residencia. Los tractores y demás maquinaria agrícola indujeron un cambio de granjas familiares y cosechas de subsistencia a grandes cultivos. Ya no había necesidad de comunidades cada 10 ó 16 kilómetros de separación o dentro del alcance de un carro y un caballo. Esta evolución todavía sigue y según el análisis de la primavera de 2007, otras 50 escuelas consolidadas rurales afrontan ahora el cierre inminente.

Los vales escolares (certificado por el cual se ofrece a los padres la posibilidad de pagar por la educación de sus hijos en una escuela de su opción, en lugar la escuela pública a la que fueron asignados) han sido propuestos recientemente como un medio de permitir la competencia entre escuelas rurales y hacer practicable la operación de escuelas cooperativas en áreas rurales.


11. La novela de Canadá de Richard Ford.



</doc>
<doc id="2695" url="https://es.wikipedia.org/wiki?curid=2695" title="Semántica lingüística">
Semántica lingüística

La semántica lingüística es un subcampo de la semántica general y de la lingüística que estudia la codificación del significado dentro de las expresiones lingüísticas. Etimológicamente, el término viene del griego "sēmantikós", que quería decir 'significado relevante', derivada de "sêma", lo que significaba 'signo'.

Una lengua es un sistema convencional para la comunicación verbal, es decir, un sistema para transmitir mensajes convencionalmente codificados, que transmitan información o permitan interaccionar con otros individuos.

La transmisión de información requiere algún tipo de codificación del contenido semántico en forma de expresiones lingüísticas. La sintaxis codifica explícitamente algunas de las relaciones sintácticas de la situación o estado de hechos descrito por el mensaje. Así, los nombres representan las entidades físicas que intervienen en un estado de hechos, mientras que el verbo describe estados de algunas de estas entidades o los procesos que realizan unas entidades sobre las otras. Los diferentes tipos de entidades materiales pueden ser clasificados de acuerdo con el tipo de función que desempeñan en cada estado de hechos en diferentes papeles temáticos. 
Así, una descripción gramatical de una lengua debe contener ciertos principios que describan cómo se codifican los papeles temáticos de las entidades que intervienen en una oración. Por ello, la información semántica es una parte integral de la gramática.

Sin embargo, la semántica lingüística no se agota en el estudio de los papeles temáticos y su codificación. Por ejemplo, la semántica léxica trata de la codificación de significados, tanto en la dimensión paradigmática, y también de los significados obtenidos mediante derivación mediante diversos procedimiento morfológicos.

Existen dos tipos de homonimia:

La semántica léxica, rama de la Lingüística que estudia el significado de las palabras, se puede enfocar desde una perspectiva onomasiológica, en la que se parte del significado para llegar a la forma, o desde una perspectiva semasiológica, que parte de la forma (significante) para llegar al estudio del significado.

Una adecuada descripción de las lenguas naturales debe contener datos de significado, referencia lingüística y condiciones de verdad. Pero los análisis semánticos también se aplican a aquellas expresiones construidas de palabras: las frases y las oraciones. Tradicionalmente las frases y las oraciones han recibido más atención que las palabras que las componen.


Sin embargo el concepto de referente conlleva ciertos problemas. Por un lado, no funciona siempre, ya que no todos los verbos denotan acción, ni todos los adjetivos, cualidades... Tampoco funciona cuando el nombre se refiere a una entidad que no existe, algo imaginario. Por último, varias expresiones pueden compartir el mismo referente pero significar cosas muy distintas. Por todo ello, cuando se estudia la palabra tenemos en cuenta lo siguiente:


Por lo tanto es una rama de la gramática lingüística muy importante en la elaboración de textos.


De este modo, mientras que "perro" y "chucho" denotan el mismo significado, sus connotaciones son muy diferentes. La connotación varía según a quien se le sugiera. De tal manera, la palabra "pacifista" tiene distintas connotaciones en la jerga militar y en un grupo de ""hippies"".

Cabe mencionar que los sinónimos no existen, dado que se pierde la ley de la lengua, "la que dice que una lengua busca la eficiencia, el menor esfuerzo que hay en una lengua".

Así mismo los semas constituyen una parte fundamental en cuanto a los constituyentes del significado, siendo en este contexto la unidad básica funcional.



</doc>
<doc id="2696" url="https://es.wikipedia.org/wiki?curid=2696" title="Senegal">
Senegal

El Senegal, cuyo nombre oficial es República del Senegal (en francés: "République du Sénégal"), es un estado soberano de África Occidental cuya forma de gobierno es la república semipresidencialista. Su territorio está organizado en catorce regiones.

Debe su nombre al río Senegal, que marca la frontera este y norte del país. Senegal limita con el océano Atlántico al oeste, con Mauritania al norte, con Malí al este, y con Guinea y Guinea-Bisáu al sur. Gambia forma un enclave virtual dentro de Senegal, siguiendo el río Gambia durante más de 300 km tierra adentro. Las islas de Cabo Verde se encuentran 560 km mar adentro, frente a la costa senegalesa. La población del país se estima en aproximadamente 13 millones de personas. El clima es tropical con dos estaciones, una seca y otra lluviosa.

Dakar, la capital de Senegal, se ubica en el punto más occidental del país, en la Península de Cabo Verde. Durante los siglos XVII y XVIII, numerosos puestos comerciales pertenecientes a diferentes potencias coloniales se establecieron en la costa. La ciudad de St. Louis se convirtió en esta época en la capital del África Occidental Francesa antes de que se mudara a Dakar en 1902. Dakar se convirtió posteriormente en su capital en 1960 en el momento de la independencia de Francia.

El país recibe su nombre a partir del río Senegal, cuya etimología es discutida. Una teoría popular, propuesta por David Boilat en 1853, afirma que deriva de la expresión wólof "sunu gaal", que significa "nuestra canoa", resultado de la dificultad para comunicarse entre los marineros portugueses del siglo XV y los pescadores wólof. Los historiadores modernos creen que el nombre hace referencia, probablemente, a la etnia bereber de los zenaga, quienes vivieron en la orilla norte del río. Una teoría que compite con las anteriores afirma que el nombre deriva de la ciudad medieval de "Sanghana" (también conocida como Isenghan, Asengan o Singhanah), descrita por el geógrafo árabe al-Bakri en 1068 como localizada en la boca del río. A pesar de todo lo anterior, la teoría de "nuestra canoa" ha sido popularmente abrazada en el Senegal moderno por su aire caluroso y su utilidad en los llamamientos a la solidaridad nacional (por ejemplo, "estamos todos en la misma canoa") hace que se oiga con frecuencia en los medios de comunicación.

Algunos serer del sur creen que el nombre del río deriva, originalmente, de la unión de los términos serer "Sene" (de Roge Sene, Deidad Suprema en la religión serer) y "O Gal" (que significa "cuerpo de agua").

Hallazgos arqueológicos por toda el área indican que Senegal estuvo habitado en tiempos prehistóricos. El islam se estableció en el valle del río Senegal en el siglo XI; el 95 % de los senegaleses de hoy en día son musulmanes. En los siglos XII y XIV, el área estuvo bajo la influencia de los imperios mandingas del Este; el imperio Jolof de Senegal también fue fundado durante este tiempo. En el siglo XVI, el imperio Jolof se dividió en cuatro reinos competidores: los Jolof, Waalo, Cayor y Baol.

Varias potencias europeas (Portugal, los Países Bajos, e Inglaterra) compitieron por el comercio en esa área desde el siglo XV, hasta que en 1677, Francia terminó con la posesión de lo que se había convertido en un importante punto de partida del comercio de esclavos (la isla de Gorea, cercana a Dakar). Sólo a partir de los años 1850 los franceses, bajo el gobernador Louis Faidherbe, comenzaron a expandirse por el propio territorio senegalés.

En enero de 1959, Senegal y el Sudán francés se unieron para formar la Federación de Malí, que se convirtió en una nación totalmente independiente el 20 de junio de 1960, como resultado de la independencia y el acuerdo de transferencia de poder firmado con Francia el 4 de abril de 1960. Debido a dificultades políticas internas, la Federación se disolvió el 20 de agosto de 1960. Senegal y Soudan (renombrado como la República de Malí) proclamaron su independencia. Léopold Senghor, un conocido poeta internacional de la negritud, político y estadista, fue elegido como primer presidente de Senegal en agosto de 1960.

Después de la disolución de la Federación de Malí, el Presidente Senghor y el Primer Ministro Mamadou Dia gobernaron juntos bajo un sistema parlamentario. En diciembre de 1962, su rivalidad política propició un intento de golpe de Estado por parte del Primer Ministro. El golpe fue reducido sin derramamiento de sangre, y Dia fue arrestado y encarcelado. Senegal adoptó una nueva constitución que consolidó el poder del Presidente. En 1980, el Presidente Senghor se retiró de la política y transfirió el cargo a su sucesor elegido a dedo, Abdou Diouf, en 1981.

Senegal se unió con Gambia para formar la Confederación de Senegambia el 1 de febrero de 1982. Sin embargo, la integración imaginada de los dos países nunca se llevó a cabo, y la unión fue disuelta en 1989. A pesar de diálogos de paz, un grupo separatista del sur, en la región de Casamance, se ha enfrentado esporádicamente con las fuerzas gubernamentales desde 1982. Senegal tiene una larga historia de participación en el mantenimiento de la paz internacional.

Abdou Diouf fue el presidente entre 1981 y 2000. Fomentó una más que amplia participación política, redujo la intervención del gobierno en la economía, y amplió los compromisos diplomáticos de Senegal, particularmente con otras naciones en desarrollo. La política interna a veces se desbordó en violencia callejera, tensiones en las fronteras, y un movimiento separatista violento en la región del sur de Casamance. No obstante, el compromiso de Senegal con la democracia y los derechos humanos se ha consolidado con el tiempo. Diouf sirvió cuatro mandatos como Presidente. En la elección presidencial de 2000, fue derrotado en libre y justa competencia por el líder de la oposición, Abdoulaye Wade. Senegal experimentó su segunda transición pacífica al poder, y la primera de un partido político a otro.

El 30 de diciembre de 2004, el Presidente Abdoulaye Wade anunció que firmaría un tratado de paz con dos facciones separatistas del "Mouvement des Forces Démocratiques de la Casamance" (MFDC), en la región de Casamance.

Senegal es una república presidencial. El presidente es elegido cada cinco años desde 2001, año en que Mame Madior Boye fue la primera mujer en acceder al cargo de primera ministra de Senegal, previamente siendo elegido cada siete, por voto adulto. El actual presidente es Macky Sall, elegido en marzo de 2012.

Senegal tiene más de 80 partidos políticos. El Parlamento bicameral está formado por la Asamblea Nacional, que cuenta con 120 asientos, y el Senado, que dispone de 100 asientos y fue reinstaurado en 2007. En Senegal existe también un sistema judicial independiente. Las altas instancias de justicia nacionales son el Consejo Constitucional y la Corte de Justicia, siendo sus miembros nombrados por el Presidente.

Senegal funciona democráticamente, siendo reconocido como uno de los países con una cultura democrática más exitosa y arraigada de África. Los administradores locales son nombrados por el Presidente y son responsables ante él. Los morabitos, líderes religiosos de las diferentes cofradías musulmanas de Senegal, también tienen una cierta influencia política en el país, especialmente durante la presidencia de Wade. En 2009, no obstante, Freedom House rebajó el estatus político de Senegal desde 'Libre' hasta 'Parcialmente Libre', como consecuencia del aumento de la concentración del poder en el ejecutivo.

En 2008, Senegal terminó en la 12ª posición del "Ibrahim Index of African Governance". El "Ibrahim Index" es un indicador comprensivo de la gobernanza en África, basado en un número de diferentes variables que reflejan el éxito con que cada gobierno provee de bienes políticos básicos a sus ciudadanos. En 2010, Senegal estaba en el puesto 15º del índice.

En 2012, Senegal organizó elecciones presidenciales, unos comicios controvertidos como consecuencia de la candidatura de dudosa legitimidad del Presidente Abdoulaye Wade. Finalmente, de las elecciones resultó victorioso Macky Sall, y Wade aceptó su derrota. Este resultado pacífico y democrático fue saludado por numerosos observadores internacionales —como la UE— como una muestra de "madurez". En 2013, Aminata Touré fue la primera ministra de Senegal hasta 2014.

Senegal se subdivide en 14 regiones, cada una de ellas administrada por un "Conseil Régional" (Consejo Regional) elegido según el peso de la población al nivel de cada "Arrondissement". El país está además subdividido en 45 "Départements", 103 "Arrondissements" (ninguno de los cuales dispone de funciones administrativas) y por "Collectivités Locales", cada una de las cuales elige oficiales administrativos.
Las capitales regionales tienen el mismo nombre que sus regiones:


Senegal está situado en la parte Oeste del continente africano, entre 12º8' y 16º41' de latitud norte, y 11º21' y 17º32' de longitud Oeste. Su punto oeste, el Cabo Verde (y particularmente el emplazamiento del Club Med de Dakar), constituye la parte más occidental del continente africano.

El paisaje senegalés consiste principalmente en planos ondulados por la arena del oeste de Sahel que crecen hasta faldas de montaña en el sudeste. Allí se encuentra también el punto más alto de Senegal, un accidente geográfico sin nombre cerca de Nepen Diakha, con 581 m de altura. El límite norteño está formado por el río Senegal. Otros ríos destacables son el río Gambia y el río Casamance. La capital Dakar yace sobre la península Cabo Verde, el punto más occidental del África continental.

El país se extiende sobre 196 722 km². Comparado con los países vecinos (Malí y Mauritania), Senegal es un país minúsculo.

El clima es de tipo saheliano con:

En el litoral, la mar suaviza las temperaturas, las cuales son del orden de 16 °C a 30 °C; pero en el centro y en el este de Senegal, pueden llegar a los 41 °C.

Durante el invierno de Europa, Senegal se convierte en un destino apreciado para actividades turísticas.

Según WWF, el territorio de Senegal se reparte entre cuatro ecorregiones:

Los parques y reservas naturales representan el 8% del territorio nacional. Tienen un papel importante en la preservación del medio ambiente y contribuyen de manera significativa al desarrollo turístico. En estos espacios protegidos se ha identificado un total de 169 especies de mamíferos, y 540 especies de aves.

Senegal cuenta con seis parques nacionales: el Parque nacional de Niokolo-Koba, al este del país; el Parque nacional de las aves de Djoudj; el Parque nacional de la Lengua de Berbería, en la región de Saint-Louis; el Parque nacional de las Islas de la Magdalena a lo largo de Dakar; el Parque nacional del Delta del Salum en el sur, y el Parque nacional de la Baja Casamance, cerrado desde hace unos años debido a disturbios en la región.

El país incluye también una treintena de reservas naturales de menor tamaño, como la Reserva de Guembeul, la Reserva de Bandia, la Reserva Natural de Popenguine, o el espacio marino protegido de Bamboung.

Después de que su economía se contrajera un 2,1% en 1993, Senegal puso en marcha un importante programa de reformas económicas con el apoyo de la comunidad internacional de donantes. Este paquete de reformas comenzó con una devaluación de la moneda del país, el Franco CFA, del 50%. Se desmantelaron asimismo los controles de precios y los subsidios del Gobierno. Como resultado de lo anterior, la inflación senegalesa se contuvo y disminuyó, aumentó la inversión y el PIB creció alrededor de un 5% al año entre 1995 y 2001.

Los principales sectores económicos en Senegal son el procesamiento de alimentos, la minería, el cemento, los fertilizantes artificiales, la industria química, la refinería de productos petrolíferos importados y el turismo. Las principales exportaciones del país son la pesca, los químicos, el algodón, la tela, los cacahuetes y el fosfato de calcio, y los principales mercados exteriores son Francia, Estados Unidos, China, Italia, India y Reino Unido.

Como miembro de la Unión Económica y Monetaria de África Occidental (UEMOA), Senegal está dando pasos hacia una mayor integración regional y unos aranceles externos unificados. Senegal también forma parte de la Organización para la Armonización del Derecho Mercantil en África.

Senegal logró la plena conectividad a Internet en 1996, creando entonces un "mini boom" de los servicios de las tecnologías de la información. Desde el punto de vista negativo, Senegal tiene profundos problemas socio económicos derivados del elevado desempleo crónico y de la desigualdad de ingresos. Senegal es un gran receptor de ayuda internacional. Entre los donantes están Estados Unidos, Japón, Francia, China y España.

Senegal tiene una población superior a los 13 millones, de la cual aproximadamente un 42% vive en zonas rurales. La densidad humana varía desde 77/km en la región oeste y central hasta 2/km en las áridas regiones orientales. Esta población crece muy rápidamente, con un índice de fecundidad superior a los 5 hijos por mujer. Se observa una gran diversidad étnica, siendo los principales grupos étnicos los siguientes: wólofs (43,3%), peuls (33,8%), sererers (14,7%), diolas (3,7%), malinkés (3,0%), soninkés (1,1%), y algunas etnias menos numerosas y más locales, sin contar los 50.000 europeos (franceses en su mayoría) y libaneses presentes en el medio urbano. Existen numerosas comunidades senegalesas en el exterior, siendo una de las principales minorías étnicas instaladas en Francia, existiendo ya incluso segundas o terceras generaciones de emigrantes.

De acuerdo con la "World Refugee Survey 2008", publicada por el Comité de Refugiados e Inmigrantes de EEUU, Senegal tiene una población de refugiados y buscadores de asilo que ascendía en 2007 a 23.800. La mayoría de esta población (20.200) provenía de Mauritania.

El idioma oficial es el francés, mientras que el wólof está considerado como el idioma nacional, aunque se hablan más idiomas locales. Al contrario que en otros países del África Negra en donde es idioma oficial, el uso del francés está muy extendido, y prácticamente toda la población lo habla, bien como primer idioma o bien como segundo.

La población se encuentra en el upside down senegalesa es mayoritariamente musulmana, alcanzando su número aproximadamente un 84% del total de la población, aunque el número de musulmanes practicantes se reduce bastante. Los cristianos, sobre todo los católicos, están presentes en un 6%. Senegal es reconocido por su tolerancia religiosa. No es raro encontrar miembros de una misma familia pertenecientes a religiones distintas. Los matrimonios interreligiosos son numerosos. Las fiestas cristianas son igualmente celebradas y respetadas por las diferentes cofradías musulmanas y demás comunidades.

El Islam, religión mayoritaria en Senegal, se caracteriza por la presencia de diferentes cofradías a las cuales los ciudadanos suelen adherirse de manera formal o informal.

Los murides o "mourides" constituyen la cofradía más numerosa. Se reconoce fácilmente a sus discípulos. Su centro religioso se encuentra en Touba, donde está una de las mayores mezquitas de África. El fundador es el morabito Ahmadou Bamba (1853-1927). Los Tidjanes son otra gran cofradía. Tienen por ciudad santa a Tivaouane. Kaolack es también una ciudad importante desde el punto de vista religioso, ya que allí se asienta el morabito Baye Niass, quien difunde un adoctrinamiento pacífico. Finalmente, los Layènes sigue los preceptos de Seydina Limammou Laye, y su centro neurálgico es la zona de Yoff, en Dakar.

Los seguidores del cristianismo se encuentran principalmente en Casamance, en país Sérère, y en las principales ciudades como Dakar y Saint-Louis. Los cristianos senegaleses realizan un peregrinaje a Popenguine. La catedral de Dakar fue construida a comienzos del siglo XX por el padre Daniel Brottier, fundador de los orfelinatos. Senegal está representado ante la Santa Sede por un cardenal, un arzobispado y un obispado.

Un cierto número de senegaleses son animistas y mantienen estas creencias ancestrales. Los senegaleses animistas practican más o menos estas antiguas creencias, en pequeños agradecimientos o demandas de protección, vertiendo agua o leche a los pies de un árbol, generalmente un baobab (la casa de los espíritus).

La capital de Senegal es Dakar, de lejos la mayor ciudad de Senegal con más de dos millones de residentes. La segunda ciudad más poblada es Touba, una "communauté rurale" (comuna rural) "de jure", con medio millón.

Senegal es conocido en toda África por su influencia y herencia musical, gracias a la popularidad del mbalax, que tiene su origen en la tradición percusiva serer. Esta música fue popularizada por Youssou N'Dour, entre otros, logrando gran éxito internacional. La percusión sabar es especialmente popular. El sabar se utiliza fundamentalmente en las celebraciones especiales, como bodas. Otro instrumento es el tama. Otros músicos populares de renombre internacional son Ismael Lô, Cheikh Lô, Orchestra Baobab, Baaba Maal, Akon Thione Seck, Viviane, y Pape Diouf.

Senegal es conocido por la tradición, típica de África Occidental, de la narración de historias, realizada por los "griots", quienes han mantenido la historia de la región viva durante miles de años a través de sus palabras y música. La profesión del "griot" se pasa de generación en generación, y requiere de años de entrenamiento y aprendizaje en genealogía, historia y música. Los "griots" dan voz a generaciones y generaciones de la sociedad africana.

La lucha senegalesa es el deporte nacional del país. Tradicionalmente era practicada como una distracción y para defender el orgullo de la aldea, pero desde hace unos años la retransmisión de los combates por televisión y la aparición de patrocinadores privados han profesionalizado el deporte (antiguamente los luchadores vencedores de los combates recibían piezas de ganado y ahora se celebran veladas en las que se reparten importantes cantidades de dinero) y los luchadores son personajes muy populares en todo el país. La fama que ha adquirido el deporte en los últimos años ha trascendido fronteras, y la variante sin golpes se ha expandido a otros países, sobre todo Francia, en donde reside la mayor comunidad senegalesa en el exterior. 




</doc>
<doc id="2705" url="https://es.wikipedia.org/wiki?curid=2705" title="Sesleriella">
Sesleriella

Sesleriella es un género de plantas herbáceas perteneciente a la familia de las poáceas. Es originario del centro y sur de Europa.

Algunos autores lo incluyen en el género "Sesleria".



</doc>
<doc id="2710" url="https://es.wikipedia.org/wiki?curid=2710" title="Solanaceae">
Solanaceae

Las solanáceas (Solanaceae ) son una familia de plantas herbáceas o leñosas con las hojas alternas, simples y sin estípulas pertenecientes al orden Solanales, de las dicotiledóneas. Comprende aproximadamente 98 géneros y unas 2700 especies, con una gran diversidad de hábito, morfología y ecología. La familia es cosmopolita, distribuyéndose por todo el globo con la excepción de la Antártida. La mayor diversidad de especies se halla en América del Sur y América Central. En esta familia se incluyen especies alimenticias tan importantes como la papa o patata "(Solanum tuberosum)," el tomate "(Solanum lycopersicum"), la berenjena "(Solanum melongena") y los chiles, ajíes o pimientos "(Capsicum"). Muchas plantas ornamentales muy populares pertenecen a las solanáceas, como "Petunia", "Schizanthus," "Salpiglossis" y "Datura". Ciertas especies son mundialmente conocidas por sus usos medicinales, sus efectos psicotrópicos o por ser ponzoñosas. Finalmente, pero no por ello menos importante, las solanáceas incluyen muchos organismos modelos para investigar cuestiones biológicas fundamentales a nivel celular, molecular y genético, tales como el tabaco y la petunia. 

Las solanáceas son plantas herbáceas, subarbustos, arbustos, árboles o lianas. Pueden ser anuales, bienales o perennes, erguidas o decumbentes. Pueden estar provistas de tubérculos subterráneos. No presentan laticíferos, ni látex, ni jugos coloreados. 
Pueden presentar una agregación basal o terminal de hojas o pueden no tener ninguno de ambos tipos. Las hojas son generalmente alternas o alternadas a opuestas (o sea, alternas en la base de la planta y opuestas hacia la inflorescencia). La consistencia de las hojas puede ser herbácea, coriácea, o pueden estar transformadas en espinas. En general las hojas son pecioladas o subsésiles, raramente sésiles. Frecuentemente son inodoras pero, en ocasiones, son aromáticas o fétidas. La lámina foliar puede ser simple o compuesta, en este último caso, pueden ser ternadas o pinnatífidas. La nerviación de las hojas es reticulada y no presentan un meristema basal. Con respecto a la anatomía foliar, las láminas son en general dorsiventrales, sin cavidades secretoras. Los estomas se hallan en general confinados a una de las caras de las hojas, raramente se los halla en ambas caras. 

Las flores son en general hermafroditas, si bien hay especies monoicas, andromonoicas o dioicas (como por ejemplo, algunos "Solanum" o "Symonanthus"). La polinización es entomófila. Las flores pueden ser solitarias o estar agregadas en inflorescencias cimosas, terminales o axilares. Las flores son de tamaño intermedio, fragantes (como en "Nicotiana"), fétidas ("Anthocercis") o inodoras. Las flores son actinomorfas, levemente cigomorfas o marcadamente cigomorfas (como por ejemplo, las flores con corola bilabiada en "Schizanthus"). Las irregularidades en la simetría pueden deberse al androceo, al perianto o a ambos a la vez. Las flores en la gran mayoría de los casos presentan un perianto diferenciado en cáliz y corola (con 5 sépalos y 5 pétalos, respectivamente), un androceo con 5 estambres y dos carpelos unidos formando un gineceo con ovario súpero (se dice, entonces, que son pentámeras y tetracíclicas). Usualmente presentan un disco hipógino. El cáliz es gamosépalo (ya que los sépalos están unidos entre sí formando un tubo), con los 5 (a veces 4 o 6) segmentos iguales entre sí, es pentalobulado, con los lóbulos más cortos que el tubo, es persistente y puede muy a menudo ser acrescente. La corola usualmente presenta 5 pétalos que también se hallan unidos entre sí formando un tubo. La corola puede ser campanulada, rotada, infundibuliforme o tubular. 

El androceo presenta 5 estambres (raramente 2, 4 o 6), libres entre sí, opositisépalos (es decir, alternan con los pétalos), son usualmente fértiles o, en algunos casos (ejemplo, en "Salpiglossideae") con estaminoideos. En este último caso, pueden presentar un solo estaminoideo ("Salpiglossis") o 3 ("Schizanthus"). Las anteras pueden ser conniventes, tocándose en su extremo superior formando un anillo, o totalmente libres, dorsifijas o basifijas, bitecas, con dehiscencia poricida o a través de pequeñas fisuras longitudinales. El filamento de los estambres puede ser filiforme o aplanado. Los estambres pueden estar insertos dentro del tubo corolino o exertos. La microsporogénesis es simultánea, la tétrada de microsporas es tetrahédrica o isobilateral. Los granos de polen son bicelulares al momento de la dehiscencia, usualmente aperturados y colpados. 

El gineceo es bi-carpelar (raramente 3- o 5-locular), de ovario súpero y presenta dos lóculos. Los lóculos pueden estar secundariamente divididos por falsos septos, como en el caso de "Nicandreae" y "Datureae". El gineceo está situado en posición oblicua respecto al plano mediano de la flor. Presentan un único estilo y un solo estigma, este último simple o bilobado. Cada lóculo lleva de 1 a 50 óvulos anátropos o hemianátropos de placentación axilar. El desarrollo del saco embrionario puede ser del tipo "Polygonum" o del tipo "Allium". Los núcleos polares del saco embrionario se fusionan con antelación a la fertilización. Presentan 3 antípodas, usualmente efímeras, o persistentes como en el caso de "Atropa". El fruto en las solanáceas puede ser una baya (como en el caso de "Solanum"), una drupa, o una cápsula. Las cápsulas son normalmente septicidas o, raramente, loculicidas o valvares. Las semillas son usualmente endospermadas, oleosas (raramente almidonosas), sin pelos conspicuos. El embrión, que puede ser recto a curvo, presenta dos cotiledones. Los números cromosómicos básicos van desde x=7 a x=12. Muchas especies son poliploides.

A pesar de la descripción previa, las solanáceas exhiben una gran variabilidad morfológica, aún en los caracteres reproductivos. Ejemplos de la acentuada diversidad de la familia son:
En general todas las solanáceas presentan un gineceo formado por dos carpelos. No obstante, hay géneros con gineceo monocarpelar ("Melananthus" ), o con 3 a 4 carpelos como "Capsicum", o con 3 a 5 carpelos, como el caso de "Nicandra", algunas especies de "Jaborosa" y de "Trianaea". Finalmente, existe al menos un caso registrado de una especie ("Iochroma umbellatum") que posee gineceos con 4 carpelos.

Usualmente el número de lóculos en el ovario es igual que el número de carpelos. No obstante, hay especies en que tales números no son idénticos debido a la existencia de falsos septos (paredes internas que subdividen cada lóculo), como por ejemplo "Datura" y algunos miembros de la tribu Lycieae (los géneros "Grabowskia" y "Vassobia").

En las solanáceas los óvulos son, en general, anátropos. Sin embargo, hay géneros con óvulos anacampilótropos (ejemplo en "Phrodus", "Grabowskia" o "Vassobia"), hemítropos ("Cestrum") o hemicampilótropos ("Capsicum", "Schizanthus" y "Lycium"). Con respecto al número de óvulos por lóculo, en general éstos son varios, algunas veces son pocos (dos pares en cada lóculo en "Grabowskia", un par en cada lóculo en "Lycium") y, excepcionalmente, se encuentra un sólo óvulo en cada lóculo, como por ejemplo, en "Melananthus".

Los frutos en las solanáceas son en su gran mayoría bayas o cápsulas (incluyendo pixidios) y, con menor frecuencia, drupas.
Las bayas son típicas en las subfamilias Cestroideae, Solanoideae (con la excepción de "Datura", "Oryctus", "Grabowskia" y la tribu Hyoscyameae) y la tribu Juanulloideae (con la excepción de "Markea"). 
Las cápsulas son características de las subfamilias Cestroideae (con la excepción de "Cestrum") y Schizanthoideae, las tribus Salpiglossoideae, Anthocercidoideae y el género "Datura". La tribu Hyoscyameae presenta pixidios. 
Las drupas son típicas en la tribu Lycieae y en Iochrominae.

Los alcaloides son bases nitrogenadas, producidas por las plantas como metabolitos secundarios, que presentan una acción fisiológica intensa sobre los animales aun a bajas dosis. 
Entre los alcaloides más famosos se encuentran los presentes en las solanáceas, denominados tropanos. Las plantas que contienen estas sustancias han sido utilizadas durante siglos como venenos. No obstante, pese a su reconocido efecto ponzoñoso, muchas de estas sustancias presentan valiosas propiedades farmacéuticas. Las solanáceas se caracterizan por contar con muchas especies que contienen diversos tipos de alcaloides más o menos activos o venenosos, tales como la escopolamina, la atropina, la hiosciamina y la nicotina. Estos se encuentran en plantas como el beleño ("Hyoscyamus albus"), la belladona ("Atropa belladonna"), el chamico o estramonio ("Datura stramonium"), la mandrágora ("Mandragora autumnalis"), el tabaco y otras. Algunos de los principales tipos de alcaloides de las solanáceas son:




A pesar de que las solanáceas se hallan en todos los continentes, la mayor riqueza de especies se halla en América Central y América del Sur. Otros dos centros de diversidad incluyen Australia y África. Las solanáceas pueden ocupar una gran variedad de ecosistemas, desde los desiertos hasta los bosques tropicales y, frecuentemente se las halla también en la vegetación secundaria que coloniza áreas perturbadas.

A continuación se provee una sinopsis taxonómica completa de las solanáceas, incluyendo subfamilias, tribus y géneros, la cual está basada en los estudios más recientes sobre la sistemática molecular de la familia. 

 Es una subfamilia caracterizada por la presencia de fibras pericíclicas, androceo con 4 o 5 estambres, frecuentemente didínamos. Los números cromosómicos básicos son muy variables, desde x=7 hasta x=13. La subfamilia comprende 8 géneros (divididos en 3 tribus) y aproximadamente 195 especies distribuidas en América. El género "Cestrum" es el más importante con respecto al número de especies ya que incluye 175 de las 195 especies de la subfamilia. La tribu "Cestreae" tiene la particularidad de incluir taxones con cromosomas largos (de 7,21 a 11,51 µm de longitud), cuando el resto de la familia, en general, posee cromosomas cortos (como por ejemplo 1,5 a 3,52 µm de longitud en Nicotianoideae).

Subfamilia caracterizada por presentar fruto en drupa y semillas con embriones curvos con cotiledones grandes y carnosos. El número cromosómico básico es x=13. Incluye 4 géneros y 5 especies que se distribuyen por las Antillas Mayores. Basados en datos moleculares, algunos autores incluyen dentro de esta subfamilia a los géneros monotípicos "Tsoala" , endémico de Madagascar, y "Metternichia" del sudeste de Brasil. Goetzeaceae se considera un sinónimo de esta subfamilia.

La sistemática molecular indica que Petunioideae es el clado hermano de las subfamilias con número cromosómico x=12 (Solanoideae y Nicotianoideae). Presentan calisteginas, un alcaloide del tipo de los tropanos. El androceo está formado por 4 estambres (raramente 5), usualmente de dos longitudes diferentes. Los números cromosómicos básicos en esta subfamilia pueden ser x=7, 8, 9 y 11. Comprende 13 géneros y unas 160 especies, distribuidas por América Central y América del Sur. Basados en datos moleculares, algunos autores consideran que los géneros oriundos de la Patagonia "Benthamiella", "Combera" y "Pantacantha" forman un clado con categoría de tribu (Benthamielleae) que se debe disponer en la subfamilia Goetzeoideae. 

Comprende hierbas anuales o bianuales, con alcaloides tropanos, sin fibras pericíclicas, con pelos y granos de polen característicos. Las flores son zigomorfas. El androceo presenta 2 estambres y 3 estaminoideos, la dehiscencia de las anteras es explosiva. El embrión es curvo. El número cromosómico básico es x=10. "Schizanthus" es un género bastante atípico dentro de las solanáceas, por sus flores fuertemente zigomorfas y por su número cromosómico básico. Los datos morfológicos y moleculares indican que "Schizanthus" es un género hermano de las restantes solanáceas y que divergió tempranamente del resto de la familia, probablemente en el Cretáceo tardío o en el Terciario temprano, hace unos 50 millones de años. La gran diversificación en los tipos de flores en "Schizanthus" ha sido el producto de la adaptación de las especies de este género a los diferentes grupos de polinizadores existentes en los ecosistemas mediterráneo, alpino de altura y desértico de Chile y áreas adyacentes de Argentina.

Incluye hierbas anuales, con fibras pericíclicas, las flores son zigomorfas, el androceo con 4 estambres, didínamos o con 3 estaminoideos, el embrión es recto y corto. El número cromosómico básico es x=12. Incluye 4 géneros y unas 30 especies distribuidas en Sudamérica. 



 



Los siguientes géneros todavía no se hallan ubicados en ninguna de las subfamilias reconocidas de solanáceas. 

Las solanáceas comprenden 98 géneros y unas 2700 especies. No obstante, esa inmensa riqueza de especies no está uniformemente distribuida entre todos los géneros. Así, los 8 géneros más importantes de la familia concentran más del 60 % de las especies, como se muestra en la tabla de abajo. De hecho, únicamente "Solanum" —el género que tipifica a la familia— incluye casi el 50 % de la totalidad de especies de solanáceas. 

Entre las solanáceas se cuentan especies alimenticias tan importantes para el ser humano como la patata o papa ("Solanum tuberosum"), el tomate o jitomate ("Solanum lycopersicum"), el chile, ají o pimiento ("Capsicum annuum") o la berenjena ("Solanum melongena"). "Nicotiana tabacum," originaria de América, se cultiva en todo el mundo para producir tabaco.
Muchas solanáceas son malezas importantes en varias partes del mundo. Su importancia radica en que pueden ser hospedantes de plagas o enfermedades de los cultivos y, por ende, su presencia incrementa las pérdidas de rendimiento o calidad del producto cosechado debidas a tales factores. Ejemplo de esto son "Acnistus arborescens" y "Browalia americana" como hospedantes de tisanópteros que dañan luego al cultivo asociado, y ciertas especies de "Datura" como hospedantes de varios tipos de virus que se transmiten luego a las solanáceas cultivadas. Algunas especies de malezas, como por ejemplo "Solanum mauritianum" en Sudáfrica, representan problemas ecológicos y económicos tan graves que se están realizando estudios tendientes a realizar el control biológico de las mismas mediante el uso de insectos. 

Varias especies arbóreas o arbustivas de solanáceas se cultivan como ornamentales. Algunos ejemplos son "Brugmansia x candida" ("trompeta del ángel"), cultivada por sus grandes flores péndulas con forma de trompeta, "Brunfelsia latifolia", cuyas flores muy fragantes cambian de color desde el violeta al blanco en un período de 3 días. Otras especies arbustivas cultivadas por sus atractivas flores son "Lycianthes rantonnetii" (jazmín del Paraguay) de flores azul-violeta, "Nicotiana glauca" ("tabaco silvestre") de flores amarillas. Otras especies y géneros ornamentales de solanáceas son la petunia "(Petunia × hybrida)", "Lycium, Solanum, Cestrum," "Calibrachoa × hybrida" y "Solandra." Inclusive se ha obtenido un híbrido entre "Petunia" y "Calibrachoa" (el cual constituye un nuevo notogénero denominado "× Petchoa" G. Boker & J. Shaw) que se comercializa como ornamental. Muchas otras especies, en particular las que producen alcaloides, se utilizan en farmacología y medicina "(Nicotiana, Hyoscyamus", y "Datura").
Muchas especies de la familia, entre ellas el tabaco y el tomate, sirven como organismos modelo para tratar de dilucidar cuestiones biológicas básicas. Uno de tales aspectos, la genómica de las solanáceas, es un proyecto internacional que intenta responder a la interrogante de cómo puede un mismo conjunto común de genes o proteínas dar origen a organismos morfológica y ecológicamente tan diferenciados entre sí como son las Solanáceas. Un primer gran objetivo de este proyecto fue secuenciar el genoma del tomate. Para ello, cada uno de los 12 cromosomas del genoma haploide del tomate se asignó a distintos centros de secuenciación en diferentes países. Así, los cromosomas 1 y 10 le correspondieron a Estados Unidos, el 3 y el 11 a China, el 2 a Corea, el 4 al Reino Unido, el 5 a India, el 7 a Francia, el 8 a Japón, el 9 a España y el 12 a Italia. La secuenciación del genoma mitocondrial fue responsabilidad de Argentina y el genoma del cloroplasto lo secuenció la Unión Europea.




</doc>
<doc id="2713" url="https://es.wikipedia.org/wiki?curid=2713" title="Simone de Beauvoir">
Simone de Beauvoir

Simone de Beauvoir (París, 9 de enero de 1908 - París, 14 de abril de 1986) fue una escritora, profesora y filósofa francesa defensora de los derechos humanos y feminista. Escribió novelas, ensayos, biografías y monográficos sobre temas políticos, sociales y filosóficos. Su pensamiento se enmarca en la corriente filosófica del existencialismo y su obra "El segundo sexo", se considera fundamental en la historia del feminismo. Fue pareja del también filósofo Jean Paul Sartre.

Nació en el piso familiar, situado en el bulevar Raspail de París en el marco de una familia burguesa con moral cristiana muy estricta. Era hija de Georges Bertrand de Beauvoir, que trabajó un tiempo como abogado y era un actor aficionado, y de Françoise Brasseur, una mujer profundamente religiosa. Ella y su hermana pequeña Hélène de Beauvoir, con quien mantuvo siempre una estrecha relación, fueron educadas en colegios católicos. Fue escolarizada desde sus cinco años en el Cours Désir, donde solía enviarse a las hijas de familias burguesas. Su hermana menor Hélène de Beauvoir (conocida con el apodo de Poupette) la siguió dos años más tarde.

Desde su niñez, de Beauvoir destacó por sus habilidades intelectuales, que hicieron que acabase cada año primera de su clase. Compartía brillantez escolar con Elizabeth Lacoin (llamada Zaza en la autobiografía que escribe de Beauvoir), que se convirtió rápidamente en su mejor amiga.

Desde adolescente, por otro lado, se rebelaría contra la fe familiar declarándose atea y considerando que la religión era una manera de subyugar al ser humano.

Después de la Primera Guerra Mundial, su abuelo materno, Gustave Brasseur, entonces presidente del Banco de la Meuse, presentó la quiebra, lo que precipitó a toda la familia en el deshonor y la vergüenza. Como consecuencia de esta ruina familiar, los padres de Simone se vieron obligados a abandonar la residencia señorial del bulevar Raspail y a trasladarse a un apartamento oscuro, situado en un quinto piso sin ascensor en la calle de Rennes. Georges de Beauvoir, que había planeado vivir con el dinero de su esposa y de su familia, vio sus planes defraudados. La culpa que sintió entonces Françoise no la abandonó nunca a lo largo de su vida y la dote desaparecida se convirtió en una vergüenza familiar.

La pequeña Simone sufrió la situación, y vio cómo las relaciones entre sus padres se deterioraban poco a poco. Hecho importante en el nacimiento de las ideas políticas feministas de Simone, toda su infancia será marcada por el hecho de haber nacido mujer: su padre no le escondió el hecho de que hubiese deseado un hijo, con el sueño de que hubiese cursado estudios en la prestigiosa Escuela Politécnica de París. Muchas veces le comentó a Simone: «Tienes un cerebro de hombre» Apasionado por el teatro, que practicaba como aficionado, compartía este gusto con su esposa y sus hijas, junto con su amor por la literatura. Georges de Beauvoir le indicó a menudo a Simone que para él «el oficio más bonito es el de escritor». Con su esposa, compartía la convicción de que, dada la mediocre condición económica en la que se hallaba la familia, la única esperanza de mejora social para sus dos hijas eran los estudios.

Los de Beauvoir veranearon a menudo en Saint-Ybard, en la propiedad de Mayrignac situada en Correze. El parque, fundado alrededor de 1880 por su abuelo, Ernest Bertrand de Beauvoir, fue adquirido a principios de siglo XIX por el bisabuelo, Narcisse Bertrand de Beauvoir. De Beauvoir narró estos tiempos felices en sus "Memorias de una joven formal". El contacto con la naturaleza y los largos paseos solitarios por el campo hicieron surgir en el espíritu de la joven Simone la ambición de un destino fuera de lo común.

Con solamente quince años ya estaba decidida sobre la forma de este destino: quería ser escritora. Tras haber aprobado el bachillerato en 1925, de Beauvoir empezó sus estudios superiores en el Instituto Católico de París, institución religiosa privada a la que solían asistir las muchachas de buena familia. Allí completó su formación matemática, mientras que ampliaba su formación literaria en el Instituto Sainte-Marie de Neuilly. Tras su primer año universitario en París, logró obtener certificados de matemáticas generales, literatura y latín. En 1926, se dedicó a estudiar filosofía y obtuvo en junio de 1927 su certificado de filosofía general. Tras estos reconocimientos acabó licenciándose en letras, con especialización en filosofía, en la primavera de 1928, tras haber aprobado también unas certificaciones de ética y de psicología. Sus estudios universitarios concluyeron en 1929 con la redacción de una tesina sobre Leibniz, culminación de sus estudios superiores.

Tras haber sido profesora agregada de filosofía en 1929, de Beauvoir, o Castor, apodo que le dio su amigo René Maheu y que Sartre siguió usando, en un juego de palabras entre «Beauvoir» y "beaver", en inglés, se preparó para ser profesora titular. Su primer destino fue Marsella. Sartre obtuvo a su vez un puesto en Le Havre en marzo de 1931 y la perspectiva de separarse de él destrozó a de Beauvoir. Para que pudiesen ser nombrados en el mismo instituto, Sartre le propuso que se casasen a lo que ella se negó. En "La fuerza de las Cosas", explicó el porqué:
Este grupo de amigos, que se llamaban entre ellos «la pequeña familia», permaneció unido hasta la muerte de sus miembros, pese a las tensiones ligeras o a los conflictos más serios que atravesaron.
Poco antes de la Segunda Guerra Mundial, la pareja Sartre-de Beauvoir fue destinada a París. De 1936 a 1938, de Beauvoir enseñó en el liceo Molière, del que fue despedida tras haber entablado una relación amorosa con Bianca Bienenfeld, una de sus alumnas.

Las editoriales Gallimard y Grasset rechazaron su primera novela, "Primaldad de lo espiritual", escrita entre 1935 y 1937, que se publicó tardíamente en 1979 con el título "Cuando predomina lo espiritual". "La Invitada" se publicó en 1943; en esta novela, la escritora describía, mediante personajes ficticios, la relación entre Sartre, Olga y ella misma, a la vez que elaboraba una reflexión filosófica sobre la lucha entre las consciencias y las posibilidades de la reciprocidad. Fue un éxito editorial inmediato que la llevó a ser suspendida en junio de 1943 de la Educación Nacional, tras la presentación de una denuncia por incitación a la perversión de personas menores en diciembre de 1941 por la madre de Nathalie Sorokine, una de sus alumnas. Se la reintegró como profesora tras la Liberación; durante la Ocupación trabajó para la radio libre francesa («Radio Vichy»), donde organizó programas dedicados a la música.

Con Sartre, Raymond Aron, Michel Leiris, Maurice Merleau-Ponty, Boris Vian y otros intelectuales franceses de izquierda, fue la fundadora de una revista, "Les Temps Modernes", que pretendía difundir la corriente existencialista a través de la literatura contemporánea. De forma paralela, continuó sus producciones personales: tras la publicación de varios ensayos y novelas donde hablaba de su compromiso con el comunismo, el ateísmo y el existencialismo. Consiguió independizarse económicamente y se dedicó plenamente a ser escritora. Viajó por numerosos países (EE. UU., China, Rusia, Cuba...) donde conoció a otras personalidades comunistas como Fidel Castro, Che Guevara, Mao Zedong o Richard Wright. En los Estados Unidos, entabló una relación pasional con el escritor americano Nelson Algren con quien mantuvo una intensa relación epistolar, llegando a intercambiar unas trescientas cartas.

Su consagración literaria tuvo lugar el año 1949: la publicación de "El segundo sexo", del que se vendieron más de veintidós mil ejemplares en la primera semana, causó escándalo y fue objeto de animados debates literarios y filosóficos. La Santa Sede, por ejemplo, se mostró contraria al ensayo. François Mauriac, que siempre tuvo animosidad hacia la pareja, publicó en "Les Temps Modernes" un editorial que creó polémica al afirmar: «ahora, lo sé todo sobre la vagina de vuestra jefa». "El segundo sexo" se tradujo a varios idiomas: en los Estados Unidos se vendieron un millón de ejemplares y se convirtió en el marco teórico esencial para las reflexiones de las fundadoras del movimiento de liberación la mujer. De Beauvoir se convirtió en precursora del movimiento feminista al describir a una sociedad en la que se relega a la mujer a una situación de inferioridad. Su análisis de la condición femenina, en ruptura con las creencias existencialistas, se apoya en los mitos, las civilizaciones, las religiones, la anatomía y las tradiciones. Este análisis desató un escándalo, en particular el capítulo dedicado a la maternidad y al aborto, entonces equiparado al homicidio. Describía el matrimonio como una institución burguesa repugnante, similar a la prostitución en la que la mujer depende económicamente de su marido y no tiene posibilidad de independizarse.

"Los Mandarines", publicado el 1954, marcó el reconocimiento de su talento literario por la comunidad intelectual: se le otorgó por esta novela el prestigioso Premio Goncourt. De Beauvoir era por entonces una de las escritoras con más lectores a nivel mundial. En esta novela, que trata de la posguerra, expuso su relación con Nelson Algren aunque siempre a través de personajes ficticios. Algren, celoso, ya no aguantaba más la relación que unía a de Beauvoir y Sartre: la ruptura entre ella y Algren demostró la fuerza del lazo que unía a los dos filósofos y la de su pacto. Posteriormente, de julio de 1952 a 1959, de Beauvoir vivió con Claude Lanzmann.

A partir de 1958, emprendió la escritura de su autobiografía, en la que describe el mundo burgués en el que creció, sus prejuicios, sus tradiciones degradantes y los esfuerzos que llevó a cabo para deshacerse de ellos pese a su condición de mujer. También relata su relación con Sartre, que calificó de éxito total. Pese a todo y a la fuerza del lazo pasional que aún los unía, ya no eran una pareja en el sentido sexual, aunque de Beauvoir se lo hiciese creer a sus lectores.

En 1964, publicó "Una muerte muy dulce", que relata la muerte de su madre: Sartre consideró siempre que éste fue el mejor escrito de de Beauvoir. La eutanasia o el luto forman el núcleo de este relato cargado de emoción. A lo largo de su luto, a la escritora le acompaña una muchacha que conoció entonces: Sylvie Le Bon, estudiante en filosofía. La relación que unió a las dos mujeres era ambigua: madre-hija, de amistad o de amor. En su cuarto escrito autobiográfico, "Final de cuentas", de Beauvoir declaraba que compartió con Sylvie el mismo tipo de relación que la unió, cincuenta años antes, a su mejor amiga Zaza. Sylvie Le Bon fue adoptada oficialmente como hija por la escritora quien la nombró heredera de su obra literaria y de sus bienes.

Tras la muerte de Sartre en 1980 publicó en 1981 "La ceremonia del adiós" donde relató los diez últimos años de vida de su compañero sentimental, aunque los detalles médicos e íntimos de la vida del filósofo fueron mal recibidos por muchos de sus seguidores. Este texto se completó con la publicación de sus conversaciones con Sartre grabadas en Roma entre agosto y septiembre de 1974. En estos diálogos Sartre reflexionaba sobre su vida y expresaba algunas dudas sobre su producción intelectual. Al publicar estas conversaciones íntimas, de Beauvoir pretendió demostrar cómo su difunta pareja había sido manipulada por el filósofo y escritor francés Benny Lévy. Lévy hizo que Sartre reconociera una cierta «inclinación religiosa» en el existencialismo pese a que Sartre y los demás existencialistas hubiesen proclamado siempre que el ateísmo era uno de sus pilares. Para de Beauvoir, Sartre ya no disponía de la plenitud de sus capacidades intelectuales cuando había sostenido este debate con Lévy y no estaba en situación de enfrentarse a éste filosóficamente. En estos textos que desvelan la vida de Sartre también dejó ver lo mala que fue su relación con la hija adoptiva de Sartre, Arlette Elkaïm-Sartre. Concluye "La Ceremonia del adiós" con la frase siguiente: «Su muerte nos separa. Mi muerte no nos reunirá. Así es; ya es demasiado bello que nuestras vidas hayan podido juntarse durante tanto tiempo».

De 1955 a 1986, residió en el número 11 bis de la calle Victor-Schœlcher de París, donde murió acompañada de su hija adoptiva y de Claude Lanzmann. Se la enterró en el cementerio de Montparnasse de la capital francesa, en la división 20, al lado de Sartre. Simone de Beauvoir fue enterrada llevando en su mano el anillo de plata que le regaló su amante Nelson Algren al despertar de su primera noche de amor.

A lo largo de su período universitario en París Simone de Beauvoir conoció a otros jóvenes intelectuales, entre ellos Jean-Paul Sartre que calificó con admiración de genio. Una relación mítica nació entre los dos filósofos, que sólo acabó con la muerte de Sartre. Simone será su «amor necesario», en oposición a los «amores contingentes» que los dos conocerán de forma paralela: un pacto de polifidelidad, que renovaban cada dos años, se estableció entre ellos a partir de 1929, más o menos un año tras su encuentro. Ambos cumplieron este pacto filosófico: él tuvo muchos amores contingentes, ella no tantos.
El clímax de la carrera universitaria de la pareja sucedió en 1929, cuando Sartre y de Beauvoir se presentaron al concurso de la agregación de filosofía, que ganó él mientras ella quedaba en segundo puesto.

Pese a este éxito, la muerte repentina de su amiga Zaza el mismo año causó un gran sufrimiento a la filósofa. De Beauvoir, criada por una madre religiosa, perdió su fe cristiana con catorce años, tal como relató en sus "Memorias de una joven formal": años antes de sus estudios filosóficos, la joven se había emancipado de su familia y de sus "valores burgueses".

El encuentro con Sartre supone para de Beauvoir el comienzo de una vida de permanente diálogo intelectual con un interlocutor privilegiado de un nivel que ella definía como mayor al suyo, al menos al inicio de la relación. Sartre y de Beauvoir no se separaron desde que se conocieron, ni durante la separación de ésta de su familia. Su relación perduró hasta la muerte de Sartre. Sin embargo, nunca se casaron ni vivieron bajo el mismo techo. Ambos vivieron en completa libertad, practicando el poliamor y sintiéndose felices con el lazo que habían creado entre ellos. Este esquema relacional novedoso se cimentaba en el rechazo profundo y visceral del modo de vida burgués.

Simone se creía única, pero ante Sartre tuvo que reconocer: «Era la primera vez en mi vida que yo me sentía intelectualmente dominada por alguno». Decidieron unir sus vidas, pero en un amor libre porque ni de Beauvoir ni Sartre aceptaban el matrimonio:

De todos modos ella lo amó y lo aceptó tal como era. Sartre propuso la fórmula de su relación: «Entre nosotros se trata de un amor necesario, pero conviene que también conozcamos amores contingentes». En La Habana, Cuba, cuando visitan a Fidel Castro y se reúnen con Che Guevara, este último les manifiesta a ambos que su amor es un amor revolucionario.

Durante la Segunda Guerra Mundial y la ocupación alemana de París, vivió en la ciudad tomada escribiendo su primera novela, "La invitada" (1943), donde exploró los dilemas existencialistas de la libertad, la acción y la responsabilidad individual, temas que abordó igualmente en novelas posteriores como "La sangre de los otros" (1944) y "Los mandarines" (1954), novela por la que recibió el Premio Goncourt.

En 1945 junto a Jean Paul Sartre y otros eruditos del momento fundaron la revista Tiempos Modernos.

Las tesis existencialistas, según las cuales cada uno es responsable de sí mismo, se introducen también en una serie de obras autobiográficas, cuatro en total, entre las que destacan "Memorias de una joven de buena familia" (también conocida como "Memorias de una joven formal") (1958) y "Final de cuentas" (1972). Sus obras ofrecen una visión sumamente reveladora de su vida y su tiempo.

Entre sus ensayos destaca "El segundo sexo" (1949), un análisis sobre el papel de las mujeres en la sociedad y la construcción del rol y la figura de la mujer; "La vejez" (1970), centrada en la situación de la ancianidad en el imaginario occidental y en donde criticó su marginación y ocultamiento, y "La ceremonia del adiós" (1981), polémica obra que evoca la figura de su compañero de vida, Jean Paul Sartre.

Además de sus aportaciones en el feminismo cabe destacar sus reflexiones sobre la creación literaria, sobre el desarrollo de la izquierda antes y después de la Segunda Guerra Mundial, sobre el dolor y la percepción del yo, sobre los linderos del psicoanálisis y sobre las premisas profundas del existencialismo.

Simone de Beauvoir definió el feminismo en 1963 como "una manera de vivir individualmente y una manera de luchar colectivamente", explica la doctora en filosofía, Teresa López Pardina, una de las principales especialistas en la figura de la escritora y filósofa francesa.

De Beauvoir sostiene que "la mujer" o lo que entendemos por mujer es un producto cultural que se ha construido socialmente. Denuncia que la mujer se ha definido a lo largo de la historia siempre respecto a algo (como madre, esposa, hija, hermana) y reivindica que la principal tarea de la mujer es reconquistar su propia identidad específica y desde sus propios criterios. Las características que se identifica en las mujeres no les vienen dadas de su genética, sino por cómo han sido educadas y socializadas. Como resumen de este pensamiento escribió una de sus frases más célebres: ""No se nace mujer, se llega a serlo""

En 1949 cuando publicó "El Segundo Sexo" era una voz solitaria en la sociedad occidental en la que tras el movimiento sufragista y la obtención del derecho al voto femenino se había vuelto a recluir a las mujeres en el hogar. El libro que en su momento fue un escándalo y que con el tiempo se está considerado un "clásico" que permite hacer balance del recorrido hacia la igualdad de los sexos señala la filósofa Alicia Puleo. Las teóricas de las distintas y contrapuestas corrientes del feminismo (liberal, radical y socialista) que resurgiría en los años sesenta después de un largo paréntesis de silencio -señala Puleo- reconocer ser ""hijas de Beauvoir"".

El ser humano considera de Beauvoir no es una "esencia" fija sino una "existencia": "proyecto", "trascendencia", "autonomía", "libertad" que no puede escamotearse a un individuo por el hecho de pertenecer al "segundo sexo". La idea fundamental de "El Segundo Sexo" —destaca Puleo— es hoy asumida por millones de personas que no han leído esta obra ni han oído hablar de ella y sus principios han sido incorporados a las políticas de igualdad europeas y han dado lugar a los estudios feministas y de género de centros universitarios de vanguardia"."

De Beauvoir expresó en los términos de la filosofía existencialista todo un ciclo de reivindicaciones de igualdad de las mujeres que comienza con la Ilustración y lleva a la obtención del voto y al acceso a la enseñanza superior en primer tercio del siglo XX destaca la filósofa Celia Amorós.

De Beauvoir tuvo también un papel determinante en la legalización del aborto en Francia. Con Halimi fundó el movimiento "Choisir" y fue una de las redactoras del Manifiesto de las 343 -firmado por mujeres de la política, la cultura y distintas áreas de la sociedad francesa como la escritora Marguerite Duras, la abogada Gisèle Halimi o las cineastas Françoise Sagan, Jeanne Moreau y Agnes Vardà reconociendo haber abortado- publicado el 5 de abril de 1971 por la revista "Le Nouvel Observateur."

Sobre el aborto señaló:""El aborto es parte integral de la evolución en la naturaleza y la historia humana. Esto no es un argumento ni a favor o en contra, sino un hecho innegable. No hay pueblo, ni época donde el aborto no fuera practicado legal o ilegalmente. El aborto está completamente ligado a la existencia humana…"."La actividad de Simone de Beauvoir fue, junto con la Gisèle Halimi y Elisabeth Badinter, clave para lograr el reconocimiento de los malos tratos sufridos por las mujeres durante la Guerra de Argelia.


La relación entre la escritora y Jean-Paul Sartre y su posición en relación al poliamor han generado numerosas controversias sobre el tipo de relación que mantenían. También la firma en 1977 de Simone de Beauvoir junto a Jean Paul Sartre y otros intelectuales de izquierda contemporáneos de una petición solicitando la liberación de dos hombres arrestados por haber mantenido relaciones sexuales con menores de edad publicada en "Le Monde" en 1977.











</doc>
<doc id="2717" url="https://es.wikipedia.org/wiki?curid=2717" title="Daphne gnidium">
Daphne gnidium

El torvisco o torrisco (Daphne gnidium) es un arbusto de la familia timeleáceas.

Es una planta distribuida por la región mediterránea, en casi toda la península ibérica, archipiélagos canario y balear y norte de África, donde crece en matorrales, pinares y terrenos no cultivados desde el nivel del mar hasta a 1000 metros de altitud.

En la comarca granadina de la Alpujarra, en plena Sierra de la Contraviesa, se sitúa el municipio de Torvizcón, topónimo que significa 'tierra del torvisco'.

Es un arbusto con muchas hojas con forma de punta de espada, todas dirigidas hacia arriba: tal vez, podría ser confundida con una Euphorbia, pero cortando una hoja rápidamente vemos que no sale látex blanco. Desarrolla las flores blancas al final del verano y el otoño. Hay que tener cuidado con esta planta porque es irritante. 

Su fruto es de color rojo, en baya. Tiene hojas lanceoladas, estrechas. Dado el potente efecto purgante de la corteza y de las hojas del torvisco es considerado venenoso, ya que puede producir ampollas en la piel tras un prolongado contacto.

El Torvisco es un arbusto con propiedades sorprendentes. Ha sido usado como amuleto y repelente de malos espíritus desde la Prehistoria. Es la mejor especie vegetal de la Península Ibérica para hacer ligaduras, su corteza es una cuerda natural por su flexibilidad y resistencia, pudiéndose hacer nudos muy firmes. En algunas zonas de Zamora se acostumbra a atarle una correa de "Daphne gnidium" a la cola de los corderos como remedio para frenar la descomposición estomacal; también es conocido su valor como insecticida en el gallinero, manteniendo a las gallinas a salvo del piojillo. Esta especie ha sido empleada, desde tiempos inmemoriales, como medio de pesca en lagunas y arroyos: su resina tóxica ataca al oxígeno del agua, por lo que echando ramas de esta planta se envenenaba a los peces que, después de dos o tres horas, sólo había que recoger. Esta forma de pesca se llama Entorviscar y además de ser peligrosa por indiscriminada está penada por la ley en España.

"Daphne gnidium" fue descrita por Carlos Linneo y publicado en "Species Plantarum" 1: 357-358. 1753.

Daphne; nombre genérico que lo encontramos mencionado por primera vez en los escritos del médico, farmacéutico y botánico griego que practicaba en la antigua Roma; Dioscórides (Anazarbus, Cilicia, en Asia Menor, c. 40 - c. 90). Probablemente en la denominación de algunas plantas de este género se recuerda la leyenda de Apolo y Dafne. El nombre de Daphne en griego significa "laurel", ya que las hojas de estas plantas son muy similares a las del laurel. 

gnidium: epíteto geográfico que alude a su localización en Gnido.





</doc>
<doc id="2720" url="https://es.wikipedia.org/wiki?curid=2720" title="Tom Bombadil">
Tom Bombadil

Tom Bombadil ("Iarwain ben-adar" en sindarin, que significa «el más viejo y el que no tiene padre») es un enigmático personaje literario creado por J. R. R. Tolkien en el poema «Las aventuras de Tom Bombadil», publicado en el poemario del mismo nombre, donde se le describe así:

Aparece posteriormente en el Libro I de "El Señor de los Anillos", en donde asiste a Frodo, a Sam, a Merry y a Pippin en su periplo fuera de la Comarca. Aquí se nos da una descripción de su aspecto físico:

Si bien el origen y la naturaleza de Tom Bombadil continúan siendo un misterio, una cosa está clara: Tom Bombadil mantiene una relación especial y muy cercana con la Naturaleza. En apariencia, Tom Bombadil no es más que un hombre más bien bajo de estatura que lleva un abrigo azul, unas grandes botas amarillas y un sombrero adornado con una pluma azul. Su cara rojiza está enmarcada por una larga barba marrón y ojos de color azul brillante. Tom vivía junto con su esposa, Baya de Oro, en una casa junto a la colina entre el Bosque Viejo y la Quebradas de los Túmulos. A esta zona Tom la llamaba "mi país" y no solía cruzar sus fronteras aunque sabemos que salió de ahí para visitar al Granjero Maggot en La Comarca y para ponerse al corriente con Cebadilla Mantecona en la posada «El Póney Pisador» de Bree. 

El origen de Tom es incierto, pero según él mismo ya vivía en la Tierra Media antes de que Melkor llegara a ella y que Beleriand desapareciera bajo los mares. Esto significa que ya se encontraba en ella antes de la llegada de los Valar, de entre los cuales Melkor fue el primero en llegar a Arda.

No se sabe muy bien qué rol y qué importancia tuvo en las batallas de la Primera y Segunda Edad, pero fue testigo, o al menos contemporáneo, de éstas. También fue testigo de la destrucción de la mayoría de los bosques que antiguamente cubrían la mayoría de la Tierra Media. Tampoco se sabe muy bien el nivel de interacción con los pueblos de la Tierra Media, pero en algún momento se convirtió en un icono folclórico de los elfos, hombres y enanos.

Como cada año al final del verano, Tom recogía lirios para su mujer, Baya de Oro, y fue en una de estas salidas en septiembre del año 3018 de la Tercera Edad cuando se encontró con Frodo y Sam y les ayudó a rescatar a Merry y Pippin del Viejo Hombre-Sauce. Tras conseguir con una canción que el Hombre Sauce soltara a los Hobbits, Tom los invita a su casa, donde pasan un par de noches. Normalmente, siempre cantaba o hablaba en forma de versos, lo que le hacía parecer un ser absurdo, pero dentro del Bosque Viejo su poder era absoluto y no había mal lo bastante poderoso para alcanzarlo. Uno de esos días, en su casa, Tom se puso el Anillo Único y, ante el asombro de todos, no se volvió invisible, devolviéndolo con una sonrisa a su portador.

Tom Bombadil les aconsejó antes de partir que debían dirigirse al Gran Camino del Este, pero los Hobbits acabaron perdidos al desorientarse en la niebla. Tras ser capturados por los Tumularios, Frodo cantó la canción que le enseñó Tom Bombadil y éste acudió en su ayuda. Tras liberar a los Hobbits, Tom les proporcionó armas de los Tumularios y escogió para Baya de Oro un broche con una gran piedra azul.

Cuando los ponis de Merry desaparecieron de los establos de «El Póney Pisador», acabaron reuniéndose con Gordo Terronillo, el poni de Tom y se quedan al cuidado de Bombadil hasta que éste los envió de vuelta con Cebadilla Mantecona. Muchos elfos de Rivendel, como Elrond o Erestor, u otros seres de la Tierra Media, como Gandalf, quisieron que Tom asistiese al Concilio de Elrond, pero no le invitaron debido a que supusieron que no se desplazaría más allá de sus fronteras.

Durante el Concilio de Elrond una opción sugerida fue entregarle el anillo a Tom, pero esta idea fue rechazada, ya que aunque el anillo no tenía poder sobre él, Tom podía no tener el suficiente poder para protegerlo de Sauron o para destruirlo, y en cualquier caso no parecía que fuera a ser un guardián muy atento. Tras la Guerra del Anillo, Gandalf fue a visitar a Tom Bombadil y disfrutó con él de largos paseos. Nadie sabe con certeza qué ocurrió en estos paseos y de qué charlaron.

De origen desconocido, más viejo que la misma tierra, ama la naturaleza sobre todas las cosas y puede controlarla gracias a la poesía. Su naturaleza es motivo de discusión entre los seguidores de la serie de libros:

Probablemente el personaje sea, tal y como llegó a sugerir el mismo Tolkien, un enigma intencional, inclasificable dentro del mundo imaginario de la Tierra Media. Por ello, queda claro que Tom es una presencia única en la Tierra Media.

Dentro de la familia de J. R. R. Tolkien, Tom Bombadil era originalmente un muñeco holandés perteneciente a la infancia de Tolkien. Tolkien escribió después un poema acerca de él llamado «Las aventuras de Tom Bombadil», publicado en la Revista Oxford en 1934, mucho antes de que comenzara a escribir "El Señor de los Anillos". En la carta 19, del 16 de diciembre de 1937 Tolkien le dice al editor de "El hobbit": «¿Piensa que Tom Bombadil, el espíritu de la campiña (en proceso de desvanecimiento) de Oxford y Berkshire, podría convertirse en el héroe de una historia? ¿O está, como lo sospecho, por entero atesorado en los versos que le adjunto? (“Las aventuras de Tom Bombadil”). Aun así, podría ampliar el retrato». Y claro que lo hizo.

Cuando Tolkien decidió introducir a Tom en la trilogía, necesitó ser cambiado un poco respecto a él o su poema, excepto por la pluma en su sombrero - cambió de una de pavo real a una de cisne, debido a que los pavos reales no vivían en la Tierra Media. En el tomo uno de la Historia del Señor de los Anillos, gracias a Christopher Tolkien, podemos leer como Tolkien lo fue configurando. En una carta escrita al original corrector de la Trilogía en 1954, Tolkien revela un poco acerca del papel literario de Tom o lo que podría ser su función. Al principio de la carta, escribe que "aún en una Edad mitológica deben haber algunos enigmas, como siempre los hay. Tom Bombadil es uno de ellos (intencionadamente)".

En un primer borrador Tom ni siquiera aparece, salva a los hobbits cantando de lejos. Luego los vuelve a salvar en los Túmulos y ahí los lleva a su casa. En anotaciones más tardías, la historia se parece más a la de ahora, pero la naturaleza de Tom no era segura, sólo ideas. Incluso quería que el granjero Maggot fuese pariente de Tom, dice refiriéndose al granjero: “que no sea un hobbit, sino otra criatura; no un enano, sino alguien emparentado con Tom Bombadil”. En este esquema incluso, los hobbits sólo se quedaban una noche en la casa de Tom y se iban, o sea que Tom no tenía mucha importancia hasta ese momento. Luego, con el tiempo, las relecturas y las correcciones, le iría dando más importancia al personaje, convirtiéndolo cada vez en un ser más enigmático y neutral.

Gandalf denomina a Tom Bombadil el más viejo de la existencia; esto es reafirmado por su nombre en sindarin que es «Iarwain Ben-adar», que significa ‘el más viejo y sin padre’. Los enanos le llaman «Forn», que en escandinavo quiere decir ‘antiguo’ y ‘que pertenece al pasado’. Los hombres le llaman «Orald», en alemán se puede traducir como ‘muy antiguo’. Todos tienen en común que aparentemente significan ‘viejo’. Todo ello se puede simplificar en que seguramente Tom Bombadil es el ser más viejo de toda la Tierra Media y moraba allí antes que todos los demás seres. 

Tom Bombadil no aparece en ninguna de la tres películas dirigidas por Peter Jackson (alegó que no era necesario para la historia), como tampoco lo hacen ninguno de los hechos narrados en la novela sobre el Bosque Viejo en la . Sin embargo, en la versión extendida en DVD de la segunda de las películas de la trilogía (""), se incluyó una escena muy similar a la del Viejo Hombre-Sauce, pero trasladada a Fangorn y protagonizada por un ucorno. En esta escena el que «canta» para liberar a Merry y Pippin no es Tom Bombadil sino Bárbol.

Sin embargo, sí aparece en el videojuego de "", y en "". La representación del personaje en los dos videojuegos es diferente: en "La batalla por la Tierra Media II" se le presenta como un alegre anciano de cabellera y barba largas y blancas, ataviado con la ropa que se describe en el libro, y con el poder de usar su voz como un arma, combatiendo a sus enemigos cantando y bailando; mientras tanto, en el primer videojuego mencionado, Bombadil es más fiel a la descripción de la novela, con pelo castaño.
También aparece en el videojuego "Lord of the Rings Online" y se lo puede encontrar en su casa entre el bosque y la quebrada de los túmulos. También aparece en el videojuego "Lego El Señor de los Anillos", como personaje disponible para "comprar" y conserva su aspecto característico: chaqueta azul, botas amarillas, barba y cabello castaños y el sombrero. Aparece en los límites de Hobbiton y sólo se puede comprar de día.



</doc>
<doc id="2721" url="https://es.wikipedia.org/wiki?curid=2721" title="J. R. R. Tolkien">
J. R. R. Tolkien

John Ronald Reuel Tolkien, CBE (AFI: ) (Bloemfontein, hoy Sudáfrica; 3 de enero de 1892 - Bournemouth, Dorset, 2 de septiembre de 1973), a menudo citado como J. R. R. Tolkien o JRRT, fue un escritor, poeta, servidor militar, filólogo, lingüista y profesor universitario británico nacido en el desaparecido Orange al sur de África, conocido principalmente por ser el autor de las novelas clásicas de fantasía heroica "El hobbit" y "El Señor de los Anillos".

De 1925 a 1945, Tolkien ocupó la cátedra Rawlinson y Bosworth en la Universidad de Oxford, enseñando anglosajón y, de 1945 a 1959, fue profesor de Lengua y Literatura inglesa en Merton. Era amigo cercano del también escritor C. S. Lewis y ambos eran miembros de un informal grupo de debate literario conocido como los Inklings. Tolkien fue nombrado Comendador de la Orden del Imperio Británico por la reina Isabel II el 28 de marzo de 1972.

Después de su muerte, el tercer hijo de Tolkien, Christopher, publicó una serie de obras basadas en las amplias notas y manuscritos inéditos de su padre, entre ellas "El Silmarillion" y "Los hijos de Húrin". Estos libros, junto con "El hobbit" y "El Señor de los Anillos", forman un cuerpo conectado de cuentos, poemas, historias de ficción, idiomas inventados y ensayos literarios sobre un mundo imaginado llamado Arda, y más extensamente sobre uno de sus continentes, conocido como la Tierra Media. Entre 1951 y 1955, Tolkien aplicó la palabra "legendarium" a la mayor parte de estos escritos.

Si bien escritores como William Morris, Robert E. Howard y E. R. Eddison precedieron a Tolkien en el género literario de fantasía con obras tan famosas e influyentes como las de "Conan el Bárbaro", el gran éxito de "El hobbit" y "El Señor de los Anillos" cuando se publicaron en Estados Unidos condujo directamente al resurgimiento popular del género. Esto ha causado que Tolkien sea identificado popularmente como «el padre» de la literatura moderna de fantasía, o más concretamente, de la alta fantasía. Los trabajos de Tolkien han inspirado muchas otras obras de fantasía y han tenido un efecto duradero en todo el campo. En 2008, el periódico "The Times" le clasificó sexto en una lista de «Los 50 escritores británicos más grandes desde 1945».

Por los datos que se conocen, la mayoría de los antepasados paternos de Tolkien fueron artesanos. La familia Tolkien tenía sus raíces en el estado alemán de Baja Sajonia, aunque se había afincado en Inglaterra en el siglo XVIII, y adaptado intensamente a su cultura. El apellido «Tolkien» es la forma anglicanizada del alemán «Tollkiehn», cuyo origen radica en "tollkühn" (‘temerario’). En cambio, la familia Suffield, antepasados maternos de Tolkien, tenían fuerte raigambre en la ciudad de Birmingham, donde se dedicaron al comercio al menos desde principios del siglo XIX, tras desplazarse allí desde Evesham (Worcestershire).

John Ronald Reuel nació en Bloemfontein, capital del Estado Libre de Orange, la noche del domingo 3 de enero de 1892. Sus padres fueron Arthur Tolkien y Mabel Suffield, ambos del Reino Unido. Recibió el mismo nombre que su abuelo paterno, John, pues en su familia era costumbre llamar así al primogénito del hijo mayor. Su medio tío John, el mayor de los hijos de John Benjamin Tolkien, solo tuvo hijas, por lo que Arthur decidió llamar a su hijo según la costumbre. Su segundo nombre, Ronald, fue puesto por deseo de Mabel, ya que ella creía que el bebé iba a ser una niña y tenía pensado llamarla Rosalind, y finalmente eligió Ronald como sustituto. Reuel, que proviene del antiguo hebreo y que significa ‘próximo a Dios’, era el segundo nombre de su padre. Ronald sería el nombre que utilizarían sus padres, sus parientes y su esposa, a pesar de no sentirse totalmente identificado con él; sus allegados le llamaban John Ronald, «Tollers» o simplemente Tolkien.

El niño fue bautizado el 31 de enero en la catedral de Bloemfontein. Tiempo después, cuando Ronald comenzaba a andar, una tarántula lo picó en el jardín de su casa, un evento que algunos aseguran tiene paralelos en sus historias, a pesar de que Tolkien admitió no tener ningún recuerdo del accidente ni miedo a las arañas de adulto. El 17 de febrero de 1894 nació su hermano menor, Hilary Arthur.
A pesar de que Arthur quiso permanecer en África, el clima del lugar perjudicaba la salud de Ronald por lo que, en 1895, cuando contaba tres años, se trasladó con su madre Mabel y su hermano Hilary a Inglaterra, en lo que debía ser una prolongada visita familiar, mientras su padre permanecía en Orange, a cargo de la venta de diamantes y otras piedras preciosas para el Banco de Inglaterra. La intención de Arthur Tolkien era la de reunirse con su familia en Inglaterra, pero murió el 15 de febrero de 1896 de una fiebre reumática. Su sorpresiva muerte dejó a la familia sin ingresos, por lo que Mabel debió llevar a sus hijos a vivir con su propia familia en Birmingham.
Ese mismo año volvieron a mudarse a Sarehole (en la actualidad, en Hall Green), por entonces una pequeña villa de Worcestershire, más tarde absorbida por Birmingham. A Ronald le encantaba explorar el cercano bosque de la turbera de Moseley y la aceña de Sarehole, así como las colinas de Clent y de Lickey, lugares que más adelante inspirarían algunos pasajes en sus obras, junto con otros parajes de Worcestershire como Bromsgrove y Alvechurch, Alcester (Warwickshire) y la granja de su tía, Bag End («Bolsón Cerrado»), nombre que utilizaría en sus relatos.

Mabel se encargó de la educación de sus dos hijos. Ronald era un alumno muy aplicado. Su gran interés por la botánica procedía de las enseñanzas de Mabel, que despertó en su hijo el placer de mirar y sentir las plantas. Ronald disfrutaba dibujando paisajes y árboles, pero sus lecciones favoritas eran aquellas relacionadas con los idiomas, puesto que su madre comenzó a enseñarle las bases del latín a tan temprana edad. De esta forma, ya podía leer a los cuatro años y escribir de forma fluida poco después.

Tolkien asistió a la King Edward's School de Birmingham. Mientras estudiaba allí participó en el desfile de coronación de Jorge V, siendo ubicado justo al exterior de las puertas del palacio de Buckingham. Más tarde fue inscrito en la escuela de San Felipe del oratorio de Birmingham.

En 1900, Mabel se convirtió junto con sus dos hijos al catolicismo a pesar de la fuerte oposición de su familia, de confesión baptista, que como consecuencia retiró toda la ayuda económica que le había estado prestando desde que se quedó viuda. En 1904, cuando Ronald tenía doce años, Mabel falleció debido a complicaciones de diabetes —una enfermedad muy peligrosa antes de la aparición de la insulina— en Fern Cottage (Rednal), donde vivía con sus hijos en una casa alquilada. Por mantenerse en el catolicismo frente a la retirada de la ayuda económica familiar, durante toda su vida Ronald vivió convencido de que su madre había sido una verdadera mártir de su fe, lo que le produjo una profunda impresión en sus propias creencias católicas.

Durante su orfandad, Ronald y Hilary fueron educados por el padre Francis Xavier Morgan, un sacerdote católico del oratorio de Birmingham, situado en la zona de Edgbaston. Morgan, andaluz aunque de padre galés, había apoyado moral y económicamente a Mabel Tolkien tras su conversión y había enseñado al joven Ronald las bases del idioma español que más tarde emplearía en la creación de su «naffarin». El Oratorio estaba casi bajo la sombra de las torres de Perrott's Folly y Edgbaston Waterworks, que inspirarían las imágenes de las torres oscuras de Orthanc y Minas Morgul de "El Señor de los Anillos".

Otra influencia notable que recibió en esta etapa fueron las pinturas románticas medievalistas de Edward Burne-Jones y la hermandad prerrafaelita, muchas de cuyas obras pertenecen hoy al Museo y Galería de Arte de Birmingham (Birmingham Museum and Art Gallery), que las expuso al público a partir de 1908.

En 1908, a los dieciséis años, Tolkien conoció en el orfanato a Edith Mary Bratt, de quien se enamoró pese a ser tres años menor. El padre Morgan le prohibió encontrarse, hablar e incluso mantener correspondencia con ella hasta que él cumpliese los veintiún años, lo que el joven obedeció al pie de la letra.

En 1911, mientras estaba en el colegio King Edward de Birmingham, Tolkien formó junto con tres amigos (Rob Gilson, Geoffrey Smith y Cristopher Wiseman) una sociedad semisecreta conocida como la "T.C., B.S.", las iniciales del Tea Club and Barrovian Society («Club de Té y Sociedad Barroviana»), en alusión a su afición de tomar el té en Barrow's Stores, cerca de la escuela, así como en la biblioteca de la propia escuela (de forma ilegal). Después de dejar la escuela, los miembros mantuvieron el contacto. De hecho, celebraron en diciembre de 1914 un "concilio" en Londres, en casa de Wiseman. Para Tolkien, el resultado de este encuentro supuso un fuerte impulso para escribir poesía. 

Más allá de las uniones íntimas de literatura, estudios y juegos, fluía un propósito mayor. Según John Garth, escritor, editor e investigador, galardonado por su obra "Tolkien y la Gran Guerra. El origen de la Tierra Media" ("Tolkien and the Great War)," Smith “declaró que a través del arte, los cuatro tendrían que dejar el mundo mejor de lo que lo encontraron” . Además, Smith creía que él y sus colegas poseían la responsabilidad “de restablecer sensatez, higiene, y el amor de real y verdadera belleza en los pechos de todos”. Tolkien declaró un parecer que “ellos tenían un “poder que estremecía al mundo”". 

En el verano de 1911, Tolkien viajó de vacaciones a Suiza, un viaje que rememoró en una carta en 1968 de forma aún muy vívida, donde señalaba que el viaje de Bilbo a través de las Montañas Nubladas (incluyendo el «deslizamiento por las piedras resbaladizas hasta el bosque de pinos») está directamente basado en sus aventuras con su grupo de doce compañeros de excursión desde Interlaken hasta Lauterbrunnen, y en su acampada en las morrenas más allá de Mürren. Cincuenta y siete años más tarde, Tolkien recordaba su profunda pena al abandonar las vistas de las nieves perpetuas de Jungfrau y Silberhorn, «la "Silvertine" (Celebdil) de mis sueños».
Después de muchas trabas e impedimentos del padre Francis (que deseaba que Tolkien se centrase en acabar sus estudios de Filología Inglesa en Oxford con honores), la misma tarde del día de su vigésimo primer cumpleaños Tolkien escribió una carta a Edith para declararle su amor y preguntarle si deseaba casarse con él. Ella le respondió que ya estaba comprometida, ya que creía que Tolkien la había olvidado. Se reunieron bajo un viaducto de ferrocarril, donde renovaron su amor, tras lo cual Edith devolvió su anillo de compromiso y decidió casarse con él. Tras comprometerse en Birmingham en enero de 1913, Edith se convirtió al catolicismo ante la insistencia de Tolkien, y se casaron el 22 de marzo de 1916 en Warwick.

Antes de su matrimonio, sus viajes le llevaron a Cornualles donde, debido al amor que sentía por los paisajes desde la época de su infancia, quedó impresionado por la visión de la singular costa córnica y el mar. Se licenció en 1915 en el Exeter College, donde Joseph Wright, catedrático de Lingüística histórica, había ejercido una gran influencia en el interés de Tolkien por distintas lenguas, con matrícula de honor en Lengua inglesa, en la modalidad «Lingüística inglesa y literatura hasta Chaucer».

Después de su graduación, Tolkien se unió al Ejército Británico que luchaba por entonces en la Primera Guerra Mundial. Se enroló con el rango de teniente segundo, especializado en lenguaje de signos, en el 11º Batallón de Servicio de los Fusileros de Lancashire, que fue enviado a Francia en 1916 con la Fuerza Expedicionaria Británica. Tolkien sirvió como oficial de comunicaciones en la batalla del Somme hasta que enfermó debido a la denominada «fiebre de las trincheras» el 27 de octubre, por lo que fue trasladado a Inglaterra el 8 de noviembre.

Durante su convalecencia en una cabaña en Great Haywood (Staffordshire), comenzó a trabajar en lo que llamó "El libro de los cuentos perdidos" con «La caída de Gondolin». Entre 1917 y 1918, sufrió recaídas de su enfermedad, aunque se había restablecido lo suficiente como para hacer tareas de mantenimiento en varios campamentos, tras lo que fue ascendido al rango de teniente. Cuando fue destinado a Kingston upon Hull, fue un día a caminar con su esposa por los bosques de la cercana Roos, y Edith comenzó a bailar para él en una densa arboleda de cicutas, rodeada de flores blancas. Esta escena inspiró el pasaje del encuentro de Beren y Lúthien, y Tolkien solía referirse a Edith como «su Lúthien». Tolkien y Edith tuvieron cuatro hijos: el sacerdote John Francis Reuel (1917-2003), el maestro de escuela Michael Hilary Reuel (1920-1984), el escritor Christopher John Reuel (n. 1924) y la trabajadora social Priscilla Anne Reuel (n. 1929).

El primer trabajo civil de Tolkien tras la guerra fue como lexicógrafo asistente en la redacción para la primera edición del "Oxford English Dictionary", donde trabajó durante dos años principalmente en la historia y etimología de las palabras de origen germánico que comenzaban por la letra W, rastreando su origen en el alto alemán, alemán medio e incluso nórdico antiguo. En 1920. ocupó el puesto de profesor no titular de Lengua inglesa en la Universidad de Leeds, donde alcanzó el cargo de profesor, reformando con su magisterio la enseñanza de esta disciplina. En Leeds conoció a E. V. Gordon, con quien publicó la que es considerada la mejor edición hasta la fecha de la obra anónima de la "Alliterative Revival", "Sir Gawain y el Caballero Verde", escrita en inglés medio a finales del siglo XIV.

En 1924 nació su tercer hijo, Christopher, quien se encargaría de publicar póstumamente todos los manuscritos que su padre había dejado desparramados por el estudio en su casa de Northmoor Road. En 1925, regresó a Oxford como profesor de Anglosajón en el Pembroke College. Fue durante su estancia en Pembroke que Tolkien escribió "El hobbit" y los dos primeros volúmenes de "El Señor de los Anillos".

Aunque Tolkien nunca esperó que sus historias ficcionales se volvieran tan populares, en 1937 C. S. Lewis lo persuadió para que publicara "El hobbit", originalmente escrito para sus hijos. Sin embargo, el libro a su vez atrajo a lectores adultos, y se volvió lo suficientemente popular como para la editorial, George Allen & Unwin, por lo que le pidieron a Tolkien que escribiera una secuela a la obra. En 1929 nació su hija Priscilla.

En 1928 Tolkien ayudó a "sir" Mortimer Wheeler en la excavación de un "asclepeion" romano en Lydney Park (Gloucestershire). Respecto a las publicaciones académicas, su conferencia de 1936 titulada «» tuvo una decisiva influencia en los estudios acerca del mito de Beowulf.
En Oxford, Tolkien trabó amistad con el profesor y escritor C. S. Lewis, (futuro autor de "Las crónicas de Narnia"), con quien disentía al principio a causa de sus convicciones religiosas (Lewis era agnóstico, y posteriormente se hizo protestante), pero que acabó siendo uno de sus principales correctores, junto con los otros miembros del club literario que formaron, los "Inklings". Sus miembros se reunían los viernes antes de comer en el "pub" Eagle and Child, y la noche de los jueves en las habitaciones de Lewis en el Magdalen College para recitar las obras que cada uno componía, así como romances y extractos de las grandes obras épicas del Norte de Europa.
Desde su adolescencia, Tolkien había empezado a escribir una serie de mitos y leyendas sobre la Tierra Media. Echaba en falta en su país una mitología del carácter de la griega, por ejemplo, y se proponía inventar «una mitología para Inglaterra», que más tarde daría lugar a "El Silmarillion", originalmente denominado "El libro de los cuentos perdidos". Dichos relatos están supuestamente inspirados en un cuento publicado en 1927 por Edward Wyke-Smith titulado "El maravilloso país de los snergs" (también el "Kalevala" finlandés, las sagas escandinavas y, en general, un poco de toda la mitología europea de cualquier origen).

En 1957, Tolkien viajaba a Estados Unidos para recibir títulos honoríficos de las principales universidades, como Marquette (donde hoy en día se conservan los manuscritos originales de sus obras) y Harvard. El viaje tuvo que suspenderse, pues Edith cayó enferma. Tolkien se retiró dos años después de su cargo en Oxford. En 1961, C. S. Lewis lo propuso como candidato para el Premio Nobel de Literatura, pero el jurado desestimó la propuesta por su «pobre prosa». En 1965, se publicó la primera edición de "El Señor de los Anillos" en Estados Unidos. En 1968, la familia Tolkien se trasladó a Poole, cerca de Bournemouth.

En esta época fue nombrado doctor "honoris causa" por varias universidades, vicepresidente de la Philological Society y miembro de la Royal Society of Literature. En 1969, la reina Isabel II le nombró Comendador de la Orden del Imperio Británico. En su honor se fundaron, en primer lugar, la Mythopoeic Society norteamericana y la Tolkien Society británica, y decenas de sociedades similares en diversos países.

Edith murió el 29 de noviembre de 1971, a la edad de 82 años. Tolkien volvió a Oxford, donde falleció 21 meses después, el 2 de septiembre de 1973, con 81 años, y fue enterrado en la misma tumba que su mujer. Esta tumba, situada en el cementerio de Wolvercote, en Oxford, presenta los nombres de «Beren» y «Lúthien» para Ronald y Edith, respectivamente, extraídos de la leyenda incluida en "El Silmarillion" acerca del amor entre estos dos seres de diferente naturaleza (la doncella elfa Lúthien y el mortal Beren) y del robo de uno de los Silmarils.

Tolkien fue un devoto católico, y así se sintió el instrumento de la conversión de C. S. Lewis del ateísmo al cristianismo. Sin embargo, se decepcionó cuando este se volvió anglicano (iglesia a la que Tolkien se refería como «una patética y oscurecedora mezcolanza de tradiciones medio recordadas y creencias mutiladas»), en lugar de católico.

Tolkien educó intensamente a sus hijos en su religión. En una carta fechada el 8 de enero de 1944, y dirigida a su hijo Christopher con la intención de darle ánimos, le insta, tras explicarle un poco de doctrina católica, a recurrir a las alabanzas: «Yo las utilizo mucho (en latín): el "Gloria Patri"; el "Gloria in excelsis"; el "Laudate Dominum"; el "Laudate pueri Dominum" (que me gusta en especial), uno de los salmos dominicales y el "Magnificat"» y la carta continúa señalando varias otras formas religiosas de buscar tranquilidad e inspiración.

En sus últimos años, Tolkien quedó profundamente decepcionado por las reformas y cambios llevados a cabo tras el Concilio Vaticano Segundo, tal como recuerda su nieto Simon Tolkien:
Es un comentario habitual, que existen paralelismos entre la saga de la Tierra Media y ciertos hechos de la vida de Tolkien. Suele argumentarse que "El Señor de los Anillos" representa a Inglaterra durante e inmediatamente después de la Segunda Guerra Mundial. Tolkien repudió ardientemente esta opinión en el prefacio a la segunda edición de su novela, declarando que prefería la aplicabilidad a la alegoría. Trató este tema con mayor extensión en su ensayo «Sobre los cuentos de hadas», en el que argumenta que los cuentos de hadas son válidos porque son consistentes consigo mismos y con algunas verdades sobre la realidad. Concluyó que el cristianismo en sí mismo sigue este patrón de consistencia interna y verdad externa. Su creencia en las verdades fundamentales del cristianismo y su lugar en la mitología lleva a los comentaristas a encontrar temas cristianos en "El Señor de los Anillos", a pesar de su notable falta de referencias abiertamente religiosas, ceremonias religiosas o apelaciones a Dios. Tolkien se opuso vehementemente al uso de referencias religiosas por parte de C. S. Lewis en sus historias, que muchas veces eran abiertamente alegóricas. Sin embargo, Tolkien escribió que la escena del Monte del Destino ejemplifica líneas del Padre nuestro.
Con todo, no puede obviarse que en su carta de respuesta (Cartas nº 142), Tolkien reconoció que: «El Señor de los Anillos es, por supuesto, una obra fundamentalmente religiosa y católica».
Su amor por los mitos y su fe devota se unieron en su creencia en que la mitología «es el eco divino de la Verdad». Expresó este punto de vista en su poema «Mitopoeia», y su idea de que los mitos contienen ciertas «verdades fundamentales» se convirtió en un tema central de los "Inklings" en su conjunto.

Las ideas políticas de Tolkien estaban guiadas por su estricto catolicismo, por lo que sus puntos de vista eran predominantemente conservadores, en el sentido de favorecer las convenciones y ortodoxias establecidas por encima de la innovación y la modernización. Apoyó el bando de Franco durante la Guerra Civil Española, tras tener noticias de que milicianos «rojos» estaban destruyendo iglesias y matando a sacerdotes y monjas en la zona republicana. Tras reunirse con él en 1944, Tolkien expresó su admiración por el poeta sudafricano católico Roy Campbell, a quien consideraba defensor de la fe católica por sus acciones con el bando franquista.

Siguiendo la opinión predominante en la Gran Bretaña de la época, se mostraba de acuerdo con la política de apaciguamiento defendida por el gobierno de Chamberlain. Considerando que Hitler y el nazismo eran menos peligrosos que los soviéticos, escribió una carta durante la Crisis de Múnich en la que manifestaba la creencia de que estos últimos eran responsables de los problemas de Europa y que estaban tratando de volver a los británicos y franceses en contra de Hitler. Sin embargo, Tolkien siempre condenó la doctrina racial del Partido Nazi y su antisemitismo como algo «totalmente pernicioso y acientífico». Cuando, en febrero de 1938, sus editores en Alemania le pidieron confirmación sobre si era de ascendencia aria, remitió dos borradores de respuesta distintos a sus editores ingleses. En el que se conserva (es decir, el que no se envió a Alemania), después de ridiculizar la mitificación del origen ario (hindú o persa) de los pueblos germánicos, replica:
En 1967 protestó contra una descripción de la Tierra Media como «nórdica», un término que le desagradaba por su asociación con la teoría racial de nombre similar. Tolkien no sentía por Hitler más que desprecio y le acusaba: «Arruina, pervierte, aplica erradamente y vuelve por siempre maldecible ese noble espíritu nórdico, suprema contribución a Europa, que siempre amé e intenté presentar en su verdadera luz». Tiempo después hablaría de Hitler como de uno de los «idiotas militares», «un pillo vulgar e ignorante, además de tener otros defectos (o la fuente de ellos)». Del otro bando, el suyo, tampoco le gustaba la propaganda antialemana demagógica y maniquea empleada durante la Segunda Guerra Mundial para reforzar el esfuerzo de guerra británico.

Sorprendentemente, en 1943 escribió: «Mis opiniones políticas se inclinan más y más hacia el anarquismo (entendido filosóficamente, lo cual significa la abolición del control, no hombres barbados armados de bombas) o hacia la monarquía “inconstitucional”. Arrestaría a cualquiera que empleara la palabra Estado (en cualquier otro sentido que no fuera el reino inanimado de Inglaterra y sus habitantes, algo que carece de poder, derechos o mente) [...]». Estas palabras le han llevado a ser calificado de «anarquista monárquico».

La cuestión del racismo o racialismo en la obra de Tolkien ha sido objeto de un cierto debate académico. Christine Chism clasifica las acusaciones en tres categorías distintas: racismo intencional, un prejuicio eurocentrista inconsciente, y una evolución de un racismo latente en sus primeras obras, a un repudio consciente de las tendencias racistas en sus últimos trabajos. John Yatt ha escrito: «Los “blancos” son buenos, los “oscuros” son malos, los orcos son los peores de todos». Sin embargo, otros críticos como Tom Shippey o Michael D. C. Drout no están de acuerdo con una generalización tan radical a partir de los hombres «blancos» y «oscuros» de Tolkien en «buenos» y «malos». La obra de Tolkien también ha sido defendida en este sentido por racistas declarados como el Partido Nacional Británico.

Ya se ha comentado anteriormente su postura sobre la política racial en Alemania; sobre las condiciones de vida de la gente de color en Sudáfrica, antes del "apartheid", escribió a su hijo Christopher:

Tolkien perdió a la mayoría de sus amigos en las trincheras durante la Primera Guerra Mundial, lo que le ponía indefectiblemente en contra de la guerra en general. Cerca del final de la Segunda, declaró que los Aliados no eran mejores que los nazis y que se comportaban como orcos en sus llamadas a una completa destrucción de Alemania. En algunos fragmentos de las Cartas a su hijo Christopher, deja ver la amargura e inutilidad humana que le provoca la guerra, y compara hechos reales con los de sus libros: «...estamos intentando conquistar a Sauron con el Anillo. Y (según parece) lo lograremos. Pero el precio es, como lo sabrás, criar nuevos Saurons y lentamente ir convirtiendo a hombres y elfos en orcos». Horrorizado por los bombardeos atómicos sobre Hiroshima y Nagasaki, se refirió a los científicos del Proyecto Manhattan como «físicos lunáticos» y «constructores de Babel». También escribió: «[...] no conozco nada sobre el imperialismo británico o americano en el Lejano Oriente que no me llene de dolor y repugnancia [...]».

Tolkien, gran amante y defensor de los árboles y los bosques, demostró en sus escritos un gran respeto por la naturaleza. También manifestó su rechazo a los efectos colaterales de la industrialización, que consideraba devoradora del paisaje rural inglés. Esta actitud conservacionista puede ser percibida en su trabajo, siendo el caso más palpable su retrato de la «industrialización forzada» de la Comarca al final de "El retorno del Rey".

Durante la mayor parte de su vida se mostró hostil incluso a los automóviles y prefería conducir su bicicleta. Caricaturizó este aspecto de su personalidad en "El señor Bliss", cuento infantil profusamente ilustrado por él mismo y editado de forma póstuma por Baillie Tolkien.

El primer poema que Tolkien consiguió publicar fue «La batalla del Campo del Este», en 1911, cuando tenía diecinueve años.

Desde hacía tiempo, Tolkien estaba interesado en el inglés antiguo o anglosajón y se había dedicado a leer varias obras en esta lengua, entre ellas, el poema anónimo «Christ I»; dos líneas de este le impresionaron especialmente:

En 1914, inspirado por estas líneas, escribió el poema «El viaje de Eärendel, la estrella vespertina», que narraba el viaje por el cielo del marinero Eärendel, más tarde convertido en Eärendil. Este poema sería imprescindible en el desarrollo de su futuro "legendarium"

Tolkien continuó escribiendo numerosos poemas, algunos de ellos relacionados con su "legendarium" y que más tarde serían incluidos por su hijo Christopher en los volúmenes de "La historia de la Tierra Media". En 1917, cuando estaba hospitalizado debido a una enfermedad contraída durante la Primera Guerra Mundial, comenzó a trabajar en otros poemas que se convertirían en la base de las historias principales de "El Silmarillion": «El cuento de Tinúviel», «Turambar y el Foalókê», y «La caída de Gondolin»; con el paso de los años, estos poemas se convirtieron en textos en prosa que evolucionaron hasta las historias de Beren y Lúthien, "Los hijos de Húrin" y "La caída de Gondolin", respectivamente.

En 1953, publicó con bastante éxito el poema «El regreso de Beorhtnoth, hijo de Beorhthelm», aunque ya estaba terminado desde 1945. Escrito en verso aliterativo, es una continuación del inacabado poema anglosajón «La batalla de Maldon».

En 1961, una tía le pidió que escribiera un libro dedicado a Tom Bombadil, personaje que aparece en "El Señor de los Anillos". Aunque solo los dos primeros poemas están dedicados a dicho personaje, Tolkien tituló el poemario como "Las aventuras de Tom Bombadil y otros poemas de El Libro Rojo", e incluyó en él otros poemas datados de la década de 1920.

J. R. R. Tolkien acostumbraba desde siempre a narrar historias a sus propios hijos, por los motivos más diversos. Así, concibió el relato de "Roverandom" en 1925, como un cuento para sus hijos John (ocho años) y Michael (cinco) durante unas vacaciones. Michael estaba muy encariñado aquel verano de uno de sus juguetes: un perrito en miniatura, de plomo pintado de blanco y negro. Desafortunadamente, un día paseando por la playa con su padre, lo dejó en el suelo para jugar y lo perdió. Aunque Tolkien y sus dos hijos mayores pasaron horas buscándolo, no fue posible recuperarlo, por lo que el autor imaginó la historia que hoy conocemos como "Roverandom" para consolar al pequeño Michael.

Se trata de un cuento infantil que narra la historia de un perrito llamado Rover que muerde a un brujo, por lo que este lo castiga convirtiéndolo en juguete. Un niño compra ese juguete, pero lo pierde en la playa. Entonces, el hechicero de la arena le hace vivir aventuras desde la Luna hasta el fondo del mar.

Este cuento no fue publicado hasta 1998, de manera póstuma.

 Tolkien escribió un breve esquema de su mitología del que los cuentos de Beren y Lúthien y el de Túrin formaban parte, y ese esquema fue evolucionando hasta convertirse en el «Quenta Silmarillion», una historia épica que Tolkien comenzó tres veces pero nunca publicó. Tolkien confiaba en publicarla al abrigo del éxito de "El Señor de los Anillos", pero a las editoriales (tanto a Allen & Unwin como a Collins) no las convenció; puesto que, además, los costes de impresión eran muy altos en la posguerra. La historia de esta continua reescritura se cuenta en la serie póstuma de "La historia de la Tierra Media", editada por el hijo de Tolkien, Christopher. Desde 1936, aproximadamente, Tolkien empezó a extender su marco de trabajo para abarcar la narración de la caída de Númenor («Akallabêth»), inspirada en la leyenda de la Atlántida. No fue hasta 1977, de manera póstuma, que los escritos que componen "El Silmarillion" vieron la luz, recopilados y editados por Christopher Tolkien. A los relatos mencionados («Quenta Silmarillion» y «Akallabêth»), se añadieron para la publicación otros más breves, de los primeros y los últimos tiempos de la Tierra Media: «Ainulindalë», «Valaquenta» y «De los Anillos de Poder y la Tercera Edad».

Tolkien escribía las historias de su "legendarium" para su propio deleite, el de su familia y el de su círculo literario, sin intención de alcanzar con ellas al gran público. Sin embargo, por casualidad, otro libro que había escrito en 1932 para sus propios hijos y al que había titulado "El hobbit" pasó de mano en mano sin intervención del autor hasta llegar a Susan Dagnall, una empleada de la editorial londinense George Allen & Unwin. Ésta le enseñó el libro al presidente de la empresa, Stanley Unwin, quien se lo dio a su hijo pequeño, Rayner, para que lo leyera; la historia le gustó tanto que decidieron publicarlo.

En este libro se narran las aventuras del hobbit Bilbo Bolsón que, junto con el mago Gandalf y una compañía de enanos, se verá envuelto en un viaje para recuperar el reino de Erebor, arrebatado a los enanos por el dragón Smaug.

Si bien se trata de una historia infantil, el libro atrajo también la atención de lectores adultos y se hizo lo suficientemente popular como para que Stanley Unwin le pidiera a Tolkien que trabajara en una secuela, más tarde conocida como "El Señor de los Anillos".

Aunque no se encontraba inspirado para tratar el tema, la petición de Stanley Unwin de una secuela para "El hobbit" impulsó a Tolkien a comenzar la que sería su obra más famosa, "El Señor de los Anillos", una novela de fantasía épica subdividida en tres volúmenes y publicada entre 1954 y 1955. Tolkien invirtió más de diez años en la creación de la historia y los apéndices de la novela, tiempo durante el cual recibió el apoyo constante de los Inklings, en particular de su amigo más cercano, C. S. Lewis, al que prestaba o leía los borradores que iba escribiendo para que los juzgara. Tanto los acontecimientos de "El hobbit" como los de "El Señor de los Anillos" están enmarcados en el contexto de "El Silmarillion", pero en una época bastante posterior.

La intención original de Tolkien al empezar a escribir "El Señor de los Anillos" era que este fuera un cuento para niños al estilo de "El hobbit", pero poco después recordó el anillo encontrado por Bilbo Bolsón y decidió centrar la historia en torno a él y su devenir, convirtiéndose en un escrito más oscuro y serio; por ello, a pesar de ser una continuación directa de "El hobbit", fue dirigido a un público más maduro. Por otro lado, Tolkien aprovechó más en esta novela la inmensa historia de Beleriand, que había ido construyendo en años anteriores y que finalmente fue publicada de forma póstuma en el "El Silmarillion" y otros volúmenes.

"El Señor de los Anillos" se volvió tremendamente popular en la década de 1960 y se ha mantenido así desde entonces, situándose como una de las obras de ficción más populares del siglo XX a juzgar por sus ventas y las encuestas de lectores, como la realizada por las librerías Waterstone's de Reino Unido y la cadena de televisión Channel 4, que eligió a "El Señor de los Anillos" como el mejor libro del siglo.

Se trata de una versión en prosa del ciclo de Kullervo del poema épico finlandés "Kalevala". Escrito por J. R. R. Tolkien cuando era un estudiante en el Exeter College, Oxford, de 1914 a 1915, fue una época inestable para el autor y esta es la sensación que se refleja en la oscura temática de la historia.

Tolkien aprendió latín, francés y alemán de su madre y, mientras estaba en el colegio, aprendió inglés medio, inglés antiguo, finlandés, gótico, griego, italiano, noruego antiguo, español, galés y galés medieval. También estuvo familiarizado con el esperanto, danés, neerlandés, lombardo, noruego, ruso, serbio, sueco y antiguas formas del alemán moderno y eslovaco, lo que revela su profundo conocimiento lingüístico sobre todas las lenguas germánicas.

Su pasión por los idiomas comenzó a los ocho o nueve años de edad, cuando se deleitaba con el sonido del latín pronunciado por su madre o se entretenía con su prima Mary inventando sus propias lenguas, como el «animálico» o el «nevbosh» (‘nuevo disparate’). Algo más tarde creó el «naffarin» (basado en el español que aprendía con la ayuda del padre Morgan). Después descubrió el gótico, el galés y el finlandés, base de sus grandes creaciones: el sindarin, la lengua de los sindar y, sobre todo, el quenya, la lengua de los noldor; alentado por sus profesores Kenneth Sisam, catedrático de instituto en Literatura Comparada y con quien competiría por la cátedra de anglosajón en el Merton College de la Universidad de Oxford, y Robert Gilson, quienes descubrieron en él a un gran filólogo.

Su carrera académica y su producción literaria son inseparables de su amor por el lenguaje y la filología. Se especializó en la filología del griego durante la universidad y en 1915 se graduó con nórdico antiguo como materia especial. De 1919 a 1920, tras licenciarse del ejército una vez finalizada la Primera Guerra Mundial, Tolkien trabajó como ayudante del redactor jefe de la primera edición del "Oxford English Dictionary", y se encargó de redactar los borradores para tres adiciones que aparecieron por primera vez en la edición publicada en octubre de 1921. En 1920, fue a la Universidad de Leeds como profesor de inglés, donde reclamó crédito por aumentar el número de estudiantes en lingüística de cinco a veinte. Dio cursos sobre el verso heroico en inglés antiguo, historia del inglés, varios textos en inglés antiguo y medio, filología del inglés antiguo y medio, filología introductoria a las lenguas germánicas, gótico, nórdico antiguo y galés medieval. Cuando en 1925, con treinta y tres años, Tolkien solicitó la cátedra Rawlinson y Bosworth, presumió de que sus estudiantes de filología germana en Leeds habían formado un «Club Vikingo».

Privadamente, a Tolkien lo atraían las «cosas de significación racial y lingüística», y contempló nociones de un heredado gusto por el lenguaje, donde calificó a la «lengua nativa» como opuesta a la «lengua materna» en su conferencia «El inglés y el galés», que es crucial para entender su concepto de la raza y el lenguaje. Consideraba el inglés medio de los Midlands occidentales su «lengua nativa» y, como le escribió a W. H. Auden en 1955, «Soy de los Midlands Occidentales por sangre (y tomé el inglés medio de estos como una lengua conocida tan pronto como posé mis ojos sobre ellos)».

Paralelamente a su trabajo profesional como filólogo, y algunas veces eclipsándolo hasta el extremo de que su producción académica permaneciera bastante escasa, estaba su afecto por la construcción de lenguas artificiales. Las de mayor desarrollo eran el quenya y el sindarin. El lenguaje y la gramática para Tolkien fueron una cuestión de estética y eufonía, y el quenya en particular fue diseñado por consideraciones «fonoestéticas»; fue previsto como un «elfolatín», y estaba basado fonológicamente en el latín, con ingredientes del finés y el griego. Una notable adición vino a fines de 1954 con el adunaico de Númenor, una lengua de «un sabor ligeramente semítico», conectada con el mito tolkieniano de la Atlántida que, por medio de «Los papeles del Notion Club», se liga directamente con sus ideas sobre la heredabilidad del lenguaje, y a través de la Segunda Edad del Sol el mito de Eärendil fue asentado en el "legendarium", de este modo proveyendo un enlace al «mundo real y primordial» del siglo XX de Tolkien con el pasado mitológico de la Tierra Media.

Tolkien consideraba los lenguajes inseparables de la mitología asociada con ellos y, consecuentemente, tomó tenue vista de las lenguas auxiliares: en 1930 un congreso de esperantistas escucharon esto de él, en su conferencia «Un vicio secreto», «La construcción de su lenguaje engendrará una mitología», pero en 1956 concluyó que el «volapük, esperanto, ido, novial, etc., están muertos, más que otras lenguas ancestrales no utilizadas, debido a que sus autores nunca inventaron ninguna leyenda en esperanto».

La popularidad de los libros de Tolkien ha tenido un pequeño pero duradero efecto en el uso del lenguaje en la literatura fantástica en particular, e incluso en importantes diccionarios, que hoy en día comúnmente aceptan el restablecimiento tolkiano de las palabras "dwarves" ("enanos") y "elvish" ("élfico") (en contraposición a "dwarfs" y "elfish"), que no habían estado en uso desde mitad aproximadamente el siglo XIX. Otros términos que ha acuñado, tales como "legendarium" y "eucatástrofe" son mayormente usados en conexión con su trabajo.

Varias obras de Tolkien han sido adaptadas al cine, empezando por la adaptación animada por Rankin/Bass de la novela "El hobbit" en 1977. Al año siguiente Ralph Bakshi dirigió "El Señor de los Anillos", una adaptación incompleta también para el cine de animación de la novela en tres volúmenes. El equipo de la Rankin/Bass creó también, en 1980, un especial animado de televisión titulado "El retorno del Rey", que incluía una recapitulación muy breve de los dos primeros tomos de "El Señor de los Anillos", y que se presentó como una continuación de la película de 1977.

Más de veinte años después, New Line Cinema y el director neozelandés Peter Jackson crearon la adaptación más exitosa de "El Señor de los Anillos", en "una trilogía de películas" protagonizadas por Elijah Wood, Viggo Mortensen, Sean Astin, Christopher Lee, Andy Serkis, Liv Tyler, Orlando Bloom e Ian McKellen, estrenadas en los años 2001: "El Señor de los Anillos: La Comunidad del Anillo", 2002 "El Señor de los Anillos: Las dos torres" y 2003 "El Señor de los Anillos: El retorno del rey".

Posteriormente, los mismos New Line Cinema y Peter Jackson (aunque durante un tiempo se barajó la dirección del mexicano Guillermo del Toro) abordaron también la adaptación de "El hobbit" en una trilogía de películas , con Martin Freeman como Bilbo Bolsón, Richard Armitage como Thorin, Ian McKellen como Gandalf, Orlando Bloom nuevamente como el alto elfo Legolas, Christopher Lee, que volvio a interpretar a Saruman, Benedict Cumberbatch como Smaug además de Andy Serkis. Estrenadas en 2012: "El hobbit: Un viaje inesperado", 2013 "El hobbit: La desolación de Smaug" y 2014 "El hobbit: La batalla de los cinco ejércitos".





</doc>
<doc id="2723" url="https://es.wikipedia.org/wiki?curid=2723" title="Tecnología">
Tecnología

La tecnología es la ciencia aplicada a la resolución de problemas concretos. Constituye un conjunto de conocimientos científicamente ordenados, que permiten diseñar y crear bienes o servicios que facilitan la adaptación al medio ambiente y la satisfacción de las necesidades esenciales y los deseos de la humanidad. Es una palabra de origen griego, τεχνολογία, formada por "téchnē" (τέχνη, "arte, técnica u oficio", que puede ser traducido como "destreza") y "logía" (λογία, el estudio de algo).

Aunque hay muchas tecnologías muy diferentes entre sí, es frecuente usar el término "tecnología" en singular para referirse al conjunto de todas, o también a una de ellas. La palabra "tecnología" también se puede referir a la disciplina teórica que estudia los saberes comunes a todas las tecnologías, y en algunos contextos, a la educación tecnológica, la disciplina escolar abocada a la familiarización con las tecnologías más importantes.

La actividad tecnológica influye en el progreso social y económico, pero si su aplicación es meramente comercial, puede orientarse a satisfacer los deseos de los más prósperos (consumismo) y no a resolver las necesidades esenciales de los más necesitados. Este enfoque puede incentivar un uso no sostenible del medio ambiente. Ciertas tecnologías humanas, por su uso intensivo, directo o indirecto, de la biosfera, son causa principal del creciente agotamiento y degradación de los recursos naturales del planeta.

Sin embargo, la tecnología también puede ser usada para proteger el medio ambiente, buscando soluciones innovadoras y eficientes para resolver de forma sostenible las crecientes necesidades de la sociedad, sin provocar un agotamiento o degradación de los recursos materiales y energéticos del planeta o aumentar las desigualdades sociales. Ciertas tecnologías humanas han llevado a un avance descomunal en los estándares y calidad de vida de billones de personas en el planeta, logrando simultáneamente una mejor conservación del medio ambiente.

La tecnología engloba a todo conjunto de acciones sistemáticas cuyo destino es la transformación de las cosas, es decir, su finalidad es saber hacer y saber por qué se hace.

Actualmente hay una era tecnológica, etapa histórica dominada por la producción de bienes y por su comercialización, en la que el factor energía tiene un papel primordial. Toda la actividad científico-técnica gravita permanentemente sobre el bienestar humano, sobre el progreso social y económico de los pueblos y sobre el medio ambiente donde se manifiesta la actividad industrial.

En la prehistoria, las tecnologías han sido usadas para satisfacer necesidades esenciales (alimentación, vestimenta, vivienda, protección personal, relación social, comprensión del mundo natural y social), y en la historia también para obtener placeres corporales y estéticos (deportes, música, hedonismo en todas sus formas) y como medios para satisfacer deseos (simbolización de estatus, fabricación de armas y toda la gama de medios artificiales usados para persuadir y dominar a las personas).

La tecnología aporta grandes beneficios a la humanidad, su papel principal es crear mejores herramientas útiles para simplificar el ahorro de tiempo y esfuerzo de trabajo. La tecnología juega un papel principal en nuestro entorno social ya que gracias a ella podemos comunicarnos de forma inmediata gracias a la telefonía celular.

Después de un tiempo, las características novedosas de los productos tecnológicos son copiadas por otras marcas y dejan de ser un buen argumento de venta. Toman entonces gran importancia las creencias del consumidor sobre otras características independientes de su función principal, como las estéticas y simbólicas.

Más allá de la indispensable adecuación entre forma y función técnica, se busca la belleza a través de las formas, colores y texturas. Entre dos productos de iguales prestaciones técnicas y precios, cualquier usuario elegirá seguramente al que encuentre más bello. A veces, caso de las prendas de vestir, la belleza puede primar sobre las consideraciones prácticas. Frecuentemente compramos ropa "bonita" aunque sepamos que sus ocultos detalles de confección no son óptimos, o que su duración será breve debido a los materiales usados. Las ropas son el rubro tecnológico de máxima venta en el planeta porque son la "cara" que mostramos a las demás personas y condicionan la manera en que nos relacionamos con ellas.

Cuando la función principal de los objetos tecnológicos es la simbólica, no satisfacen las necesidades básicas de las personas y se convierten en medios para establecer estatus social y relaciones de poder.

Las joyas hechas de metales y piedras preciosas no impactan tanto por su belleza (muchas veces comparable al de una imitación barata) como por ser claros indicadores de la riqueza de sus dueños. Las ropas costosas de "primera marca" han sido tradicionalmente indicadores del estatus social de sus portadores. En la América colonial, por ejemplo, se castigaba con azotes al esclavo o liberto africano que usaba ropas españolas por "pretender ser lo que no es".

El caso más destacado y frecuente de objetos tecnológicos fabricados por su función simbólica es el de los grandes edificios: catedrales, palacios, rascacielos gigantes. Están diseñados para empequeñecer a los que están en su interior (caso de los amplios atrios y altísimos techos de las catedrales), deslumbrar con exhibiciones de lujo (caso de los palacios), infundir asombro y humildad (caso de los grandes rascacielos). No es casual que los terroristas del 11 de septiembre de 2001 eligieran como blanco principal de sus ataques a las Torres Gemelas de Nueva York, sede de la Organización Mundial del Comercio y símbolo del principal centro del poderío económico estadounidense.

El Programa Apolo fue lanzado por el Presidente John F. Kennedy en el clímax de la Guerra Fría, cuando Estados Unidos estaba aparentemente perdiendo la carrera espacial frente a los rusos, para demostrar al mundo la inteligencia, riqueza, poderío y capacidad tecnológica de los Estados Unidos. Con las pirámides de Egipto, es el más costoso ejemplo del uso simbólico de las tecnologías.

Las tecnologías usan, en general, métodos diferentes del científico, aunque la experimentación es también usado por las ciencias. Los métodos difieren según se trate de tecnologías de producción artesanal o industrial de artefactos, de prestación de servicios, de realización u organización de tareas de cualquier tipo.

Un método común a todas las tecnologías de fabricación es el uso de herramientas e instrumentos para la construcción de artefactos. Las tecnologías de prestación de servicios, como el sistema de suministro eléctrico hacen uso de instalaciones complejas a cargo de personal especializado.

Los principales medios para la fabricación de artefactos son la energía y la información. La energía permite dar a los materiales la forma, ubicación y composición que están descritas por la información. Las primeras herramientas, como los martillos de piedra y las agujas de hueso, sólo facilitaban y dirigían la aplicación de la fuerza, por parte de las personas, usando los principios de las máquinas simples. El uso del fuego, que modifica la composición de los alimentos haciéndolos más fácilmente digeribles, proporciona iluminación haciendo posible la sociabilidad más allá de los horarios diurnos, brinda calefacción y mantiene a raya a alimañas y animales feroces, modificó tanto la apariencia como los hábitos humanos.

Las herramientas más elaboradas incorporan información en su funcionamiento, como las pinzas pelacables que permiten cortar la vaina a la profundidad apropiada para arrancarla con facilidad sin dañar el alma metálica. El término «instrumento», en cambio, está más directamente asociado a las tareas de precisión, como en instrumental quirúrgico, y de recolección de información, como en instrumentación electrónica y en instrumentos de medición, de navegación náutica y de navegación aérea.

Las máquinas herramientas son combinaciones complejas de varias herramientas gobernadas (actualmente, muchas mediante computadoras) por información obtenida desde instrumentos, también incorporados en ellas.

Aunque con grandes variantes de detalle según el objeto, su principio de funcionamiento y los materiales usados en su construcción, las siguientes son las etapas comunes en la invención de un artefacto novedoso:

Según el divulgador científico Asimov: 
Guilford, destacado estudioso de la psicología de la inteligencia, identifica como las principales destrezas de un inventor las incluidas en lo que denomina "aptitudes de producción divergente". La creatividad, facultad intelectual asociada a todas las producciones originales, ha sido discutida por de Bono, quien la denomina "pensamiento lateral". Aunque más orientado a las producciones intelectuales, el más profundo estudio sobre la resolución de problemas cognitivos es hecho por Newell y Simon, en el celebérrimo libro "Human problem solving".

Muchas veces la palabra tecnología se aplica a la informática, la micro-eléctrica, el láser o a las actividades especiales, que son duras. Sin embargo, la mayoría de las definiciones que hemos visto también permiten e incluyen a otras, a las que se suele denominar blandas.

Las tecnologías blandas –en las que su producto no es un objeto tangible– pretenden mejorar el funcionamiento de las instituciones u organizaciones para el cumplimiento de sus objetivos. Dichas organizaciones pueden ser empresas industriales, comerciales o de servicio institucional, como o sin fines de lucro, etc. Entre las ramas de la tecnología llamadas blandas se destacan la educación (en lo que respecta al proceso de enseñanza), la organización, la administración, la contabilidad y las operaciones, la logística de producción, el "marketing" y la estadística, la psicología de las relaciones humanas y del trabajo, y el desarrollo de "software".

Se suele llamar duras aquellas tecnologías que se basan en conocimiento de las ciencias duras, como la física o la química. Mientras que las otras se fundamentan en ciencias blandas, como la sociología, la economía, o la administración.

Se considera que una tecnología es apropiada cuando tiene efectos beneficiosos sobre las personas y el medio ambiente. Aunque el tema es hoy (y probablemente seguirá siéndolo por mucho tiempo) objeto de intenso debate, hay acuerdo bastante amplio sobre las principales características que una tecnología debe tener para ser social y ambientalmente apropiada:

Los conceptos tecnologías apropiadas y tecnologías de punta son completamente diferentes. Las tecnologías de punta, término publicitario que enfatiza la innovación, son usualmente tecnologías complejas que hacen uso de muchas otras tecnologías más simples. Las tecnologías apropiadas frecuentemente, aunque no siempre, usan saberes propios de la cultura (generalmente artesanales) y materias primas fácilmente obtenibles en el ambiente natural donde se aplican. Algunos autores acuñaron el término tecnologías intermedias para designar a las tecnologías que comparten características de las apropiadas y de las industriales.


Las nuevas tecnologías son nuevas porque, en lo sustancial, han aparecido –y, sobre todo, se han perfeccionado, difundido y asimilado– después de la Segunda Guerra Mundial. Desde entonces su desarrollo se ha caracterizado por una fuerte aceleración; sus consecuencias son de una magnitud y trascendencia que no tenían antecedentes.

Si recorremos listas de nuevas tecnologías (NT) preparadas en Singapur, México, Tokio, Boston o Buenos Aires, podemos sorprendernos de que algunas no tengan más de tres líneas, mientras que otras cubren varias páginas. Pero, si estudiamos estos listados, veremos que –más allá del detalle o de sus diferentes objetivos– la mayoría coincide en destacar tres NT: las biotecnologías (BT), las de los nuevos materiales (NM) y las tecnologías de la información (TI).

Esta síntesis deja de lado otras NT –como algunas ambientales, las energéticas o las espaciales– pero agrupa a las de mayor difusión y en las que se manifiestan con mayor claridad los efectos que más nos importan.

Las NT se alimenta de producción científica más avanzada, a la que se suele definir como la que constituye la frontera del conocimiento. Por eso también se habla de tecnologías de punta o, en inglés, "hot technologies" (tecnologías calientes).

En algunos países se destaca la importancia estratégica de estas tecnologías: se sostiene que si no se las domina será imposible, en el medio y largo plazo, dominar las manufacturas de producto que se aseguren una posición relevante en la competencia económica y comercial internacional. Por eso, se las suele denominar tecnologías estratégicas.

Las tecnologías, aunque no son objetos específicos de estudio de la Economía, han sido a lo largo de toda la historia, y lo son aún actualmente, parte imprescindible de los procesos económicos, es decir, de la producción e intercambio de cualquier tipo de bienes y servicios.

Desde el punto de vista de los productores de bienes y de los prestadores de servicios, las tecnologías son un medio indispensable para obtener renta.

Desde el punto de vista de los consumidores, las tecnologías les permiten obtener mejores bienes y servicios, usualmente (pero no siempre) más baratos que los equivalentes del pasado.
Desde el punto de vista de los trabajadores, las tecnologías han disminuido los puestos de trabajo al reemplazar crecientemente a los operarios por máquinas.

La mayoría de las teorías económicas da por sentada la disponibilidad de las tecnologías. Schumpeter es uno de los pocos economistas que asignó a las tecnologías un rol central en los fenómenos económicos. En sus obras señala que los modelos clásicos de la economía no pueden explicar los ciclos periódicos de expansión y depresión, como los de Kondrátiev, que son la regla más que la excepción. El origen de estos ciclos, según Schumpeter, es la aparición de innovaciones tecnológicas significativas (como la introducción de la iluminación eléctrica domiciliaria por Edison o la del automóvil económico por Ford) que generan una fase de expansión económica. La posterior saturación del mercado y la aparición de empresarios competidores cuando desaparece el monopolio temporario que da la innovación, conducen a la siguiente fase de depresión. El término "empresario schumpeteriano" es hoy corrientemente usado para designar a los empresarios innovadores que hacen crecer su industria gracias a su creatividad, capacidad organizativa y mejoras en la eficiencia.

La producción de bienes requiere la recolección, fabricación o generación de todos sus insumos. La obtención de la materia prima inorgánica requiere las tecnologías mineras. La materia prima orgánica (alimentos, fibras textiles...) requiere de tecnologías agrícolas y ganaderas. Para obtener los productos finales, la materia prima debe ser procesada en instalaciones industriales de muy variado tamaño y tipo, donde se ponen en juego toda clase de tecnologías, incluida la imprescindible generación de energía.

Hasta los servicios personales requieren de las tecnologías para su buena prestación. Las ropas de trabajo, los útiles, los edificios donde se trabaja, los medios de comunicación y registro de información son productos tecnológicos. Servicios esenciales como la provisión de agua potable, tecnologías sanitarias, electricidad, eliminación de residuos, barrido y limpieza de calles, mantenimiento de carreteras, teléfonos, gas natural, radio, televisión, etc. no podrían brindarse sin el uso intensivo y extensivo de múltiples tecnologías.

Las tecnologías de las telecomunicaciones, en particular, han experimentado enormes progresos a partir del desarrollo y puesta en órbita de los primeros satélites de comunicaciones; del aumento de velocidad y memoria, y la disminución de tamaño y coste de las computadoras; de la miniaturización de circuitos electrónicos (circuito integrados); de la invención de los teléfonos celulares; etc. Todo ello permite comunicaciones casi instantáneas entre dos puntos cualesquiera del planeta, aunque la mayor parte de la población todavía no tiene acceso a ellas.

El comercio moderno, medio principal de intercambio de mercancías (productos tecnológicos), no podría llevarse a cabo sin las tecnologías del transporte fluvial, marítimo, terrestre y aéreo. Estas tecnologías incluyen tanto los medios de transporte (barcos, automotores, aviones, trenes, etc.), como también las vías de transporte y todas las instalaciones y servicios necesarios para su eficaz realización y eficiente uso: puertos, grúas de carga y descarga, carreteras, puentes, aeródromos, radares, combustibles, etc. El valor de los fletes, consecuencia directa de la eficiencia de las tecnologías de transporte usadas, ha sido desde tiempos remotos y sigue siendo hoy uno de los principales condicionantes del comercio.

Un país con grandes recursos naturales será pobre si no tiene las tecnologías necesarias para su ventajosa explotación, lo que requiere una enorme gama de tecnologías de infraestructura y servicios esenciales. Asimismo, un país con grandes recursos naturales bien explotados tendrá una población pobre si la distribución de ingresos no permite a ésta un acceso adecuado a las tecnologías imprescindibles para la satisfacción de sus necesidades básicas. En la actual economía capitalista, el único bien de cambio que tiene la mayoría de las personas para la adquisición de los productos y servicios necesarios para su supervivencia es su trabajo. La disponibilidad de trabajo, condicionada por las tecnologías, es hoy una necesidad humana esencial.

Si bien las técnicas y tecnologías también son parte esencial del trabajo artesanal, el trabajo fabril introdujo variantes tanto desde el punto de vista del tipo y propiedad de los medios de producción, como de la organización y realización del trabajo de producción. El alto costo de las máquinas usadas en los procesos de fabricación masiva, origen del capitalismo, tuvo como consecuencia que el trabajador perdiera la propiedad, y por ende el control, de los medios de producción de los productos que fabricaba. Perdió también el control de su modo de trabajar, de lo que es máximo exponente el taylorismo.

Según Frederick W. Taylor, la organización del trabajo fabril debía eliminar tanto los movimientos inútiles de los trabajadores —por ser consumo innecesario de energía y de tiempo— como los tiempos muertos —aquellos en que el obrero estaba ocioso. Esta "organización científica del trabajo", como se la llamó en su época, disminuía la incidencia de la mano de obra en el costo de las manufacturas industriales, aumentando su productividad. Aunque la idea parecía razonable, no tenía en cuenta las necesidades de los obreros y fue llevada a límites extremos por los empresarios industriales. La reducción de las tareas a movimientos lo más sencillos posibles se usó para disminuir las destrezas necesarias para el trabajo, transferidas a máquinas, reduciendo en consecuencia los salarios y aumentando la inversión de capital y lo que Karl Marx denominó la plusvalía. Este exceso de especialización hizo que el obrero perdiera la satisfacción de su trabajo, ya que la mayoría de ellos nunca veía el producto terminado. Asimismo, llevada al extremo, la repetición monótona de movimientos generaba distracción, accidentes, mayor ausentismo laboral y pérdida de calidad del trabajo. Las tendencias contemporáneas, una de cuyas expresiones es el toyotismo, son de favorecer la iniciativa personal y la participación en etapas variadas del proceso productivo (flexibilización laboral), con el consiguiente aumento de satisfacción, rendimiento y compromiso personal en la tarea.

Henry Ford, el primer fabricante de automóviles que puso sus precios al alcance de un obrero calificado, logró reducir sus costos de producción gracias a una rigurosa organización del trabajo industrial. Su herramienta principal fue la cadena de montaje que reemplazó el desplazamiento del obrero en busca de las piezas al desplazamiento de éstas hasta el puesto fijo del obrero. La disminución del costo del producto se hizo a costa de la transformación del trabajo industrial en una sencilla tarea repetitiva, que resultaba agotadora por su ritmo indeclinable y su monotonía. La metodología fue satirizada por el actor y director inglés Charles Chaplin en su clásico film Tiempos modernos y hoy estas tareas son realizadas por robots industriales.

La técnica de producción en serie de grandes cantidades de productos idénticos para disminuir su precio, está perdiendo gradualmente validez a medida que las maquinarias industriales son crecientemente controladas por computadoras, ellas permiten variar con bajo costo las características de los productos en la cadena de producción. Éste es, por ejemplo, el caso del corte de prendas de vestir, aunque siguen siendo mayoritariamente cosidas por costureras con la ayuda de máquinas de coser individuales, en puestos fijos de trabajo.

El toyotismo, cuyo nombre proviene de la fábrica de automóviles Toyota, su creadora, modifica las características negativas del fordismo. Se basa en la flexibilidad laboral, el fomento del trabajo en equipo y la participación del obrero en las decisiones productivas. Desde el punto de vista de los insumos, disminuye el costo de mantenimiento de inventarios ociosos mediante el sistema "just in time", donde los componentes son provistos en el momento en que se necesitan para la fabricación. Aunque mantiene la producción en cadena, reemplaza las tareas repetitivas más agobiantes, como la soldadura de chasis, con robots industriales.

Uno de los instrumentos de que dispone la Economía para la detección de los puestos de trabajos eliminados o generados por las innovaciones tecnológicas es la matriz insumo-producto (en inglés, "input-output matrix") desarrollada por el economista Wassily Leontief, cuyo uso por los gobiernos recién empieza a difundirse. La tendencia histórica es la disminución de los puestos de trabajo en los sectores económicos primarios ( agricultura, ganadería, pesca, silvicultura) y secundarios (minería, industria, sector energético y construcción) y su aumento en los terciarios (transporte, comunicaciones, servicios, comercio, turismo, educación, finanzas, administración, sanidad). Esto plantea la necesidad de medidas rápidas de los gobiernos en reubicación de mano de obra, con la previa e indispensable capacitación laboral.

La mayoría de los productos tecnológicos se hacen con fines de lucro y su publicidad es crucial para su exitosa comercialización. La publicidad —que usa recursos tecnológicos como la imprenta, la radio y la televisión— es el principal medio por el que los fabricantes de bienes y los proveedores de servicios dan a conocer sus productos a los consumidores potenciales.

Idealmente la función técnica de la publicidad es la descripción de las propiedades del producto, para que los interesados puedan conocer cuan bien satisfará sus necesidades prácticas y si su costo está o no a su alcance. Esta función práctica se pone claramente de manifiesto sólo en la publicidad de productos innovadores cuyas características es imprescindible dar a conocer para poder venderlos. Sin embargo, usualmente no se informa al usuario de la duración estimada de los artefactos o el tiempo de mantenimiento y los costos secundarios del uso de los servicios, factores cruciales para una elección racional entre alternativas similares. No cumplen su función técnica, en particular, las publicidades de sustancias que proporcionan alguna forma de placer, como los cigarrillos y el vino cuyo consumo prolongado o excesivo acarrea riesgos variados. En varios países, como Estados Unidos y Uruguay, el alto costo que causan en tecnologías médicas hizo que se obligara a advertir en sus envases los riesgos que acarrea el consumo del producto. Sin embargo, aunque lleven la advertencia en letra chica, estos productos nunca mencionan su función técnica de cambiar la percepción de la realidad, centrando sus mensajes en asociar el consumo sólo con el placer, el éxito y el prestigio.

La elección, desarrollo y uso de tecnologías puede tener impactos muy variados en todos los órdenes del quehacer humano y sobre la naturaleza. Uno de los primeros investigadores del tema fue McLuhan, quien planteó las siguientes cuatro preguntas a contestar sobre cada tecnología particular:
Este cuestionario puede ampliarse para ayudar a identificar mejor los impactos, positivos o negativos, de cada actividad tecnológica tanto sobre las personas como sobre su cultura, su sociedad y el medio ambiente:

Cada cultura distribuye de modo diferente la realización de las funciones y el usufructo de sus beneficios. Como la introducción de nuevas tecnologías modifica y reemplaza funciones humanas, cuando los cambios son suficientemente generalizados puede modificar también las relaciones humanas, generando un nuevo orden social. Las tecnologías no son independientes de la cultura, integran con ella un sistema socio-técnico inseparable. Las tecnologías disponibles en una cultura condicionan su forma de organización, así como la cosmovisión de una cultura condiciona las tecnologías que está dispuesta a usar.

En su libro "Los orígenes de la civilización" el historiado Vere Gordon Childe ha desarrollado detalladamente la estrecha vinculación entre la evolución tecnológica y la social de las culturas occidentales, desde sus orígenes prehistóricos. Marshall McLuhan ha hecho lo propio para la época contemporánea en el campo más restringido de las tecnologías de las telecomunicaciones.

Desde tiempos prehistóricos, el hombre ha utilizado sus conocimientos para fabricar herramientas y máquinas para servir a sus propósitos, desde la rueda al ordenador. Algunos ahora alaban la tecnología como el fundamento de toda prosperidad, y creen que debieran imponerse pocas restricciones a su desarrollo. Otros la condenan como la causa de masivo daño al medio ambiente, y hacen un llamado a la imposición de controles estrictos. Pero la verdad es que es ambas cosas, y ninguna de las dos. La tecnología ha ayudado a traer riqueza a gran parte del mundo, mas también ha sido el instrumento de mucho del daño ocasionado al planeta y a la vida sobre él. Pero en sí misma es neutral: por bien o por mal, sus efectos dependen del uso que nosotros hacemos de ella.

Además del creciente reemplazo de los ambientes naturales (cuya preservación en casos particularmente deseables ha obligado a la creación de parques y reservas naturales), la extracción de ellos de materiales o su contaminación por el uso humano, está generando problemas de difícil reversión. Cuando esta extracción o contaminación excede la capacidad natural de reposición o regeneración, las consecuencias pueden ser muy graves. Son ejemplos:

Se pueden mitigar los efectos que las tecnologías producen sobre el medio ambiente estudiando los impactos ambientales que tendrá una obra antes de su ejecución, sea ésta la construcción de un caminito en la ladera de una montaña o la instalación de una gran fábrica de papel a la vera de un río. En muchos países estos estudios son obligatorios y deben tomarse recaudos para minimizar los impactos negativos (rara vez pueden eliminarse por completo) sobre el ambiente natural y maximizar (si existen) los impactos positivos (caso de obras para la prevención de aludes o inundaciones).

Para eliminar completamente los impactos ambientales negativos no debe tomarse de la naturaleza o incorporar a ella más de los que es capaz de reponer, o eliminar por sí misma. Por ejemplo, si se tala un árbol se debe plantar al menos uno; si se arrojan residuos orgánicos a un río, la cantidad no debe exceder su capacidad natural de degradación. Esto implica un costo adicional que debe ser provisto por la sociedad, transformando los que actualmente son costos externos de las actividades humanas (es decir, costos que no paga el causante, por ejemplo los industriales, sino otras personas) en costos internos de las actividades responsables del impacto negativo. De lo contrario se generan problemas que deberán ser resueltos por nuestros descendientes, con el grave riesgo de que en el transcurso del tiempo se transformen en problemas insolubles.

El concepto de desarrollo sustentable o sostenible tiene metas más modestas que el probablemente inalcanzable impacto ambiental nulo. Su expectativa es permitir satisfacer las necesidades básicas, no suntuarias, de las generaciones presentes sin afectar de manera irreversible la capacidad de las generaciones futuras de hacer lo propio. Además del uso moderado y racional de los recursos naturales, esto requiere el uso de tecnologías específicamente diseñadas para la conservación y protección del medio ambiente.

A pesar de lo que afirmaban los luditas, y como el propio Marx señalara refiriéndose específicamente a las maquinarias industriales, las tecnologías no son ni buenas ni malas. Los juicios éticos no son aplicables a la tecnología, sino al uso que se hace de ella: la tecnología puede utilizarse para fabricar un cohete y bombardear un país, o para enviar comida a una zona marcada por la hambruna. Cuando la tecnología está bajo el dominio del lucro, se utiliza principalmente para el beneficio monetario, lo cual puede generar prejuicios subjetivos hacia la tecnología en sí misma y su función.

Cuando el lucro es la finalidad principal de las actividades tecnológicas, caso ampliamente mayoritario, el resultado inevitable es considerar a las personas como mercancía e impedir que la prioridad sea el beneficio humano y medioambiental, dando lugar a una alta ineficiencia y negligencia medioambiental.

Cuando hay seres vivos involucrados (animales de laboratorio y personas), caso de las tecnologías médicas, la experimentación tecnológica tiene restricciones éticas inexistentes para la materia inanimada.

Las consideraciones morales rara vez entran en juego para las tecnologías militares, y aunque existen acuerdos internacionales limitadores de las acciones admisibles para la guerra, como la Convención de Ginebra, estos acuerdos son frecuentemente violados por los países con argumentos de supervivencia y hasta de mera seguridad.

Los artefactos han inundado todos los ámbitos de la vida: el acceso a la información, las comunicaciones, el comercio, la banca, las relaciones con las administraciones públicas, la educación, etc. Pero no todos los individuos tienen acceso en igualdad de condiciones a estas prestaciones, por lo que, si se hiciera un estudio de caso aplicando el modelo SCOT (acrónimo en inglés de Construcción Social de la Tecnología), se debería definir dentro de los grupos sociales de relevancia (GSR) al conjunto de posibles usuarios de artefactos que posean alguna discapacidad visual (ceguera o discapacidad visual grave según se establece legalmente en la escala de Wecker).

El estudio y análisis del impacto que las tecnologías tienen sobre este GSR se conoce con el nombre de tiflotecnología (del griego "tiflos" = ciego). Los resultados obtenidos de este estudio se aplican a los artefactos para que estos puedan ser utilizados por personas pertenecientes a este colectivo. Con ello, se consigue que la accesibilidad y la usabilidad sean universales.

La necesidad de la universalización del acceso a la información se basa en la premisa de que la sociedad de la información y del conocimiento tiende a excluir a aquellos grupos o individuos que no utilizan habitualmente dichas tecnologías, por lo que pueden ser considerados como analfabetos digitales, creándose, de esta manera, una nueva brecha digital.

Salvar esta brecha digital pasa por aceptar la existencia de una tecnología general y otra específica y que ambas circulen paralelamente de tal manera que, a la hora de diseñar un nuevo producto, este contenga un conjunto de estándares que permitan la accesibilidad universal y la usabilidad del artefacto.

En el campo de la discapacidad visual, sobre todo en el ámbito de la informática, se han alcanzado algunas metas que parecían inalcanzables. Así, no nos ha de sorprender que una persona ciega pueda acceder a las páginas Web de la prensa, artículos académicos, blogs, etc., a través de un ordenador de sobre mesa, un teléfono inteligente o una "tablet"; asimismo, no ha de extrañar que un usuario ciego pueda retirar un libro de cualquier biblioteca para leerlo en su casa gracias al "software" de reconocimiento de texto que permite transformar lo escrito en voz.

Todos los avances en materia de accesibilidad universal y usabilidad general han generado una serie de productos tiflotécnicos tales como el "software" magnificador de textos (ZoomText, "software" para el reconocimiento de pantalla ( JAWS for Windows), "software" de lectura ( Open Book), sistemas de grabación y reproducción de texto accesible ( DAISY), sistemas de audio descripción para programas de televisión, cine y documentales ( AUDESC), lupas televisión, etc., que permiten al usuario ciego o deficiente visual grave integrarse social y laboralmente.

Desde diferentes posiciones ideológicas, se han realizado críticas a la tecnología de forma global o parcial. Estas críticas consideran que o bien ciertas tecnologías suponen una amenaza, un riesgo o un mal de algún tipo, independientemente del uso que se las dé, o bien el conjunto de las tecnologías actuales suponen de manera inherente un mal. Entre las primeras, destacan aquellas críticas que se oponen a la tecnología nuclear, aquellas que se oponen a la posesión de armas de fuego y la argumentación que Francis Fukuyama realiza en su libro "El fin del hombre. Consecuencias de la revolución biotecnológica", la cual se centra en los aspectos negativos de la biotecnología para el ser humano. Entre las segundas, destacan las obras de Jacques Ellul dedicadas al estudio de la ""Technique"", en especial "La edad de la técnica", el manifiesto "La sociedad industrial y su futuro" y el libro de Jerry Mander "En ausencia de lo sagrado. El fracaso de la tecnología y la supervivencia de las naciones indias". Este último autor expone que "en el actual clima de culto tecnológico está mal visto hablar contra la tecnología. A la menor crítica te expones a que te llamen 'ludita', con lo que se pretende equiparar oposición a la tecnología y estupidez". 
La idea de la neutralidad de la tecnología también es discutida por muchos de estos críticos. Así, Nicolás Martín Sosa defendía que "la tecnología, digámoslo una vez más, no es neutra; en toda sociedad organizada induce un conjunto de conceptos, de modelos de relaciones y de poderes que moldean nuestra forma de vivir y de pensar". Mander sostenía que "la idea de que la tecnología es neutral no es neutral en sí misma, puesto que nos impide ver hacia dónde nos dirigimos y favorece directamente a los promotores de la vía tecnológica centralizada".

Los estudios de CTS (Ciéncia, Tecnología y Sociedad) tienen como claro objetivo analizar la relación entre el desarrollo de la ciencia y la tecnología con los problemas de nuestra sociedad. La investigación en CTS concluye que el desarrollo de la ciencia y la tecnología no se puede entender al margen de condicionantes de tipo político, social, económico o cultural.

En este sentido, cabe destacar que el valor de la ciencia y la tecnología para la educación de los ciudadanos es algo que hoy no se discute. Tanto es así, que en la actualidad la educación en valores no es menos importante para el desarrollo del individuo que la adquisición de saberes y destrezas. Ciencia, tecnología y valores son, por tanto, elementos básicos de la propia definición de educación en nuestros tiempos.

En una nota publicada en el diario "Clarín", Daniel Filmus afirma: «una educación que forme ciudadanos participativos y solidarios, que utilicen críticamente las nuevas tecnologías, ayudará a la construcción de una sociedad más justa, humana y sin exclusiones».

La tecnología es conocimiento aplicado socialmente y los valores y las creencias de esa sociedad son los que influyen en los efectos de esa tecnología (Westby & Atencio, 2002).

De acuerdo a Shanker (1998), la ciencia y la tecnología son la base del poder, la clave de la prosperidad, simultáneamente son un instrumento culturalmente poderoso que disuelve no solo la resistencia física sino las actitudes de vida. La sociedad se transforma y se adapta a los cambios en la tecnología.

Y este componente social de la ciencia i tecnología es el que desarrollaron i con el modelo SCOT (). El modelo SCOT representa la aproximación constructivista social en los actuales estudios sociales de la tecnología.

Un punto esencial en el planteamiento del modelo SCOT es la noción de que los diferentes grupos sociales relevantes (GSR) asociados con el desarrollo de un artefacto tecnológico, compartían un significado unánime del artefacto técnico y pretendían hacer prevalecer su concepción. El otro punto esencial es el de la flexibilidad interpretativa, el proceso de cierre mediante el cual desaparece la flexibilidad de un artefacto.

En este sentido, cabe la posibilidad que también exista una visión influida por el género, como se darían en casos estudiados como el de la bicicleta o el de la lavadora.

En cuanto al caso de la lavadora, aunque «la concepción y el desarrollo de la tecnología aparecen teóricamente de forma asexuada o al margen de las relaciones sociales de sexo», su concepción tenía un claro destinatario, y eran las mujeres. Cabe decir, sin embargo, que la lavadora, lejos de ser un artefacto de emancipación y liberador se convirtió en una subordinación para ellas, muy lejos de la liberación que representaba la bicicleta para y .

En este estudio se destaca la total ausencia de mujeres en el proceso de diseño y en los puestos de responsabilidad técnica. Sin embargo, las investigadoras concluyeron que las operarias debían ser mujeres porque las usuarias potenciales de estos aparatos eran mujeres, en tanto que eran las amas de casa. El problema radica en que la mujer no dispone de los conocimientos técnicos adecuados, por lo que los hombres siguen manteniendo el control técnico del objeto.

La relación entre la altura de la mujer y el tamaño de los mandos de los aparatos es algo a tener en cuenta, ya que deja entrever que éstos han sido concebidos para hombres.

Otro hecho destacable es la forma de carga de la lavadora. La mayor parte de las máquinas en España son de apertura frontal ya que las de carga superior suponen un montaje más costoso. Además, Alemán relaciona la carga frontal con el hecho de que la mujer ya está acostumbrada a una posición curvada dada su condición de ama de casa.

Destacar también la utilización eficaz y eficiente de la lavadora por parte de la mujer, que ligada a su cultura doméstica, hace que la mujer siga siendo la responsable de organizar las coladas a la unidad familiar. En este sentido «el nuevo electrodoméstico aparece, por tanto, como un elemento de conservadurismo social y no como un factor de emancipación o de transformación progresiva de las relaciones sociales de sexo».

Finalmente, destacar que la concepción de la lavadora, y sobre todo, su uso, «confirma a la mujer como principal actora en este tipo de funciones». Por este motivo no es raro que las mujeres «sienten un cierto malestar hacia la tecnología, o se desentienden de ella, ya que en lugar de ser innovaciones liberadoras para las mujeres, confirman muy frecuentemente su subordinación».

La relación entre género y tecnología se creó como respuesta a la larga marginalización de las mujeres respecto a profesiones y trabajos de orientación técnica.

La ciencia y la tecnología son fundamentales en el desarrollo económico de los países. Esta importancia creciente junto con las persistentes desigualdades entre mujeres y hombres en el ámbito tecnológico, hace que se planteen cuestiones urgentes e inevitables desde una perspectiva de género, la única finalidad es su total desaparición.

Aunque las barreras formales que impedían la participación de la mujer en la actividad tecnológica van desapareciendo con el paso del tiempo, siguen existiendo dificultades de acceso a puestos de responsabilidad y poder ligados a la escasa presencia profesional en esta área. Los motivos pueden ser de equilibrio entre el trabajo y la vida personal, los patrones y los enfoques de productividad específicos del género, los criterios de medición del rendimiento y de promoción, de motivación, de exclusión social e institucional, e incluso de identificación de lo científico y tecnológico con 'lo masculino'.

Y si la ciencia y la tecnología no están libres de la política ni por encima de ella, entonces en una sociedad caracterizada por jerarquías de género, los artefactos deben estar marcados también por el género. Dicho de otro modo, hemos llegado a ver la tecnología como algo a lo que se le ha dado forma socialmente, pero esta forma ha sido realizada por los hombres a favor de la exclusión de las mujeres. En general, la tecnología ha sido retratada como fuerza negativa, reproduciendo en lugar de transformando la división sexual del trabajo y el poder en el hogar y el trabajo.




</doc>
<doc id="2727" url="https://es.wikipedia.org/wiki?curid=2727" title="Cabecera IP">
Cabecera IP

Puede variar entre (0100) o (0110) dependiendo si se utiliza IP versión 4 o IP versión 6. Este campo describe el formato de la cabecera utilizada. En la tabla se describe la versión 4 (IPv4).

Longitud de la cabecera, en palabras de 32 bits. Su valor mínimo es de 5 palabras (5x32 = 160 bits, 20 bytes) para una cabecera correcta, y el máximo de 15 palabras (15x32 = 480 bits, 60 bytes).

Indica una serie de parámetros sobre la calidad de servicio deseada durante el tránsito por una red. Algunas redes ofrecen prioridades de servicios, considerando determinado tipo de paquetes "más importantes" que otros (en particular estas redes solo admiten los paquetes con prioridad alta en momentos de sobrecarga). Estos 8 bits se agrupan de la siguiente manera:

· Los 3 primeros bits están relacionados con la precedencia de los mensajes, un indicador adjunto que indica el nivel de urgencia basado en el sistema militar de precedencia (véase ) de la CCEB, un organización de comunicaciones electrónicas militares formada por 5 naciones. La urgencia que estos estados representan aumenta a medida que el número formado por estos 3 bits lo hace, y responden a los siguientes nombres.

· Los 5 bits de menor peso son independientes e indican características del servicio.

Es el tamaño total, en octetos, del datagrama, incluyendo el tamaño de la cabecera y el de los datos. El tamaño mínimo de los datagramas usados normalmente es de 576 octetos (64 de cabeceras y 512 de datos). Una máquina no debería enviar datagramas menores o mayores de ese tamaño a no ser que tenga la certeza de que van a ser aceptados por la máquina destino.

En caso de fragmentación este campo contendrá el tamaño del fragmento, no el del datagrama original.

Identificador único del datagrama. Se utilizará, en caso de que el datagrama deba ser fragmentado, para poder distinguir los fragmentos de un datagrama de los de otro. El originador del datagrama debe asegurar un valor único para la pareja origen-destino y el tipo de protocolo durante el tiempo que el datagrama pueda estar activo en la red. El valor asignado en este campo debe ir en formato de red.

Actualmente utilizado sólo para especificar valores relativos a la fragmentación de paquetes. Los 3 bits (por orden de mayor a menor peso) son:

En paquetes fragmentados indica la posición, en unidades de 64 bits, que ocupa el paquete actual dentro del datagrama original. El primer paquete de una serie de fragmentos contendrá en este campo el valor 0.

Indica el máximo número de enrutadores que un paquete puede atravesar. Cada vez que algún nodo procesa este paquete disminuye su valor en, como mínimo, una unidad. Cuando llegue a ser 0, el paquete será descartado. Típicamente toma el valor 64 o 128 en los datagramas.

Indica el protocolo de las capas superiores al que debe entregarse el paquete Vea Números de protocolo IP para comprender como interpretar este campo.

Aunque no es obligatoria la utilización de este campo, cualquier nodo debe ser capaz de interpretarlo. Puede contener un número indeterminado de opciones, que tendrán dos posibles formatos:

Se determina con un sólo octeto indicando el Tipo de opción, el cual está dividido en 3 campos.

Un octeto para el Tipo de opción, otro para el Tamaño de opción, y uno o más octetos conformando los Datos de opción.

El Tamaño de opción incluye el octeto de Tipo de opción, el de Tamaño de opción y la suma de los octetos de datos.

La siguiente tabla muestra las opciones actualmente definidas:

Utilizado para asegurar que el tamaño, en bits, de la cabecera es un múltiplo de 32. El valor usado es el 0.



</doc>
<doc id="2730" url="https://es.wikipedia.org/wiki?curid=2730" title="Tierra">
Tierra

La Tierra (del latín "Terra", deidad romana equivalente a Gea, diosa griega de la feminidad y la fecundidad) es un planeta del sistema solar que gira alrededor de su estrella —el Sol— en la tercera órbita más interna. Es el más denso y el quinto mayor de los ocho planetas del sistema solar. También es el mayor de los cuatro terrestres o rocosos.

La Tierra se formó hace aproximadamente 4550 millones de años y la vida surgió unos mil millones de años después. Es el hogar de millones de especies, incluyendo los seres humanos y actualmente el único cuerpo astronómico donde se conoce la existencia de vida. La atmósfera y otras condiciones abióticas han sido alteradas significativamente por la biosfera del planeta, favoreciendo la proliferación de organismos aerobios, así como la formación de una capa de ozono que junto con el campo magnético terrestre bloquean la radiación solar dañina, permitiendo así la vida en la Tierra. Las propiedades físicas de la Tierra, la historia geológica y su órbita han permitido que la vida siga existiendo. Se estima que el planeta seguirá siendo capaz de sustentar vida durante otros 500 millones de años, ya que según las previsiones actuales, pasado ese tiempo la creciente luminosidad del Sol terminará causando la extinción de la biosfera.

La superficie terrestre o corteza está dividida en varias placas tectónicas que se deslizan sobre el magma durante periodos de varios millones de años. La superficie está cubierta por continentes e islas; estos poseen varios lagos, ríos y otras fuentes de agua, que junto con los océanos de agua salada que representan cerca del 71 % de la superficie constituyen la hidrósfera. No se conoce ningún otro planeta con este equilibrio de agua líquida, que es indispensable para cualquier tipo de vida conocida. Los polos de la Tierra están cubiertos en su mayoría de hielo sólido (indlandsis de la Antártida) o de banquisas (casquete polar ártico). El interior del planeta es geológicamente activo, con una gruesa capa de manto relativamente sólido, un núcleo externo líquido que genera un campo magnético, y un sólido núcleo interior compuesto por aproximadamente un 88 % de hierro.

La Tierra interactúa gravitatoriamente con otros objetos en el espacio, especialmente el Sol y la Luna. En la actualidad, la Tierra completa una órbita alrededor del Sol cada vez que realiza 365,26 giros sobre su eje, lo cual es equivalente a 365,26 días solares o un año sideral. El eje de rotación de la Tierra se encuentra inclinado 23,4° con respecto a la perpendicular a su plano orbital, lo que produce las variaciones estacionales en la superficie del planeta con un período de un año tropical (365,24 días solares). La Tierra posee un único satélite natural, la Luna, que comenzó a orbitar la Tierra hace 4530 millones de años; esta produce las mareas, estabiliza la inclinación del eje terrestre y reduce gradualmente la velocidad de rotación del planeta. Hace aproximadamente , durante el llamado bombardeo intenso tardío, numerosos asteroides impactaron en la Tierra, causando significativos cambios en la mayor parte de su superficie.

Tanto los minerales del planeta como los productos de la biosfera aportan recursos que se utilizan para sostener a la población humana mundial. Sus habitantes están agrupados en unos 200 estados soberanos independientes, que interactúan a través de la diplomacia, los viajes, el comercio y la acción militar. Las culturas humanas han desarrollado muchas ideas sobre el planeta, incluida la personificación de una deidad, la creencia en una Tierra plana o en la Tierra como centro del universo, y una perspectiva moderna del mundo como un entorno integrado que requiere administración.

Los científicos han podido reconstruir información detallada sobre el pasado de la Tierra. Según estos estudios el material más antiguo del sistema solar se formó hace millones de años, y en torno a unos 4550 millones de años atrás (con una incertidumbre del 1 %) se habían formado ya la Tierra y los otros planetas del sistema solar a partir de la nebulosa solar, una masa en forma de disco compuesta del polvo y gas remanente de la formación del Sol. Este proceso de formación de la Tierra a través de la acreción tuvo lugar mayoritariamente en un plazo de . La capa exterior del planeta, inicialmente fundida, se enfrió hasta formar una corteza sólida cuando el agua comenzó a acumularse en la atmósfera. La Luna se formó poco antes, hace unos 4530 millones de años.
El actual modelo consensuado sobre la formación de la Luna es la teoría del gran impacto, que postula que la Luna se creó cuando un objeto del tamaño de Marte, con cerca del 10 % de la masa de la Tierra, impactó tangencialmente contra ésta. En este modelo, parte de la masa de este cuerpo podría haberse fusionado con la Tierra, mientras otra parte habría sido expulsada al espacio, proporcionando suficiente material en órbita como para desencadenar nuevamente un proceso de aglutinamiento por fuerzas gravitatorias, y formando así la Luna.

La desgasificación de la corteza y la actividad volcánica produjeron la atmósfera primordial de la Tierra. La condensación de vapor de agua, junto con el hielo y el agua líquida aportada por los asteroides y por protoplanetas, cometas y objetos transneptunianos, produjeron los océanos. El recién formado Sol solo tenía el 70 % de su luminosidad actual: sin embargo, existen evidencias que muestran que los primitivos océanos se mantuvieron en estado líquido; una contradicción denominada la «paradoja del joven Sol débil», ya que aparentemente el agua no debería ser capaz de permanecer en ese estado líquido, sino en el sólido, debido a la poca energía solar recibida. Sin embargo, una combinación de gases de efecto invernadero y mayores niveles de actividad solar contribuyeron a elevar la temperatura de la superficie terrestre, impidiendo así que los océanos se congelaran. Hace 3500 millones de años se formó el campo magnético de la Tierra, lo que ayudó a evitar que la atmósfera fuese arrastrada por el viento solar.

Se han propuesto dos modelos para el crecimiento de los continentes: el modelo de crecimiento constante, y el modelo de crecimiento rápido en una fase temprana de la historia de la Tierra. Las investigaciones actuales sugieren que la segunda opción es más probable, con un rápido crecimiento inicial de la corteza continental, seguido de un largo período de estabilidad. En escalas de tiempo de cientos de millones de años de duración, la superficie terrestre ha estado en constante remodelación, formando y fragmentando continentes. Estos continentes se han desplazado por la superficie, combinándose en ocasiones para formar un supercontinente. Hace aproximadamente 750 millones de años (Ma), uno de los primeros supercontinentes conocidos, Rodinia, comenzó a resquebrajarse. Los continentes más tarde se recombinaron nuevamente para formar Pannotia, entre , y finalmente Pangea, que se fragmentó hace 180 Ma hasta llegar a la configuración continental actual.

La Tierra proporciona el único ejemplo conocido de un entorno que ha dado lugar a la evolución de la vida. Se presume que procesos químicos altamente energéticos produjeron una molécula auto-replicante hace alrededor de 4000 millones de años, y hace entre 3500 y 3800 millones de años existió el último antepasado común universal. El desarrollo de la fotosíntesis permitió que los seres vivos recogiesen de forma directa la energía del Sol; el oxígeno resultante acumulado en la atmósfera formó una capa de ozono (una forma de oxígeno molecular [O]) en la atmósfera superior. La incorporación de células más pequeñas dentro de las más grandes dio como resultado el desarrollo de las células complejas llamadas eucariotas. Los verdaderos organismos multicelulares se formaron cuando las células dentro de colonias se hicieron cada vez más especializadas. La vida colonizó la superficie de la Tierra en parte gracias a la absorción de la radiación ultravioleta por parte de la capa de ozono.

En la década de 1960 surgió una hipótesis que afirmaba que durante el período Neoproterozoico, desde 750 hasta los 580 Ma, se produjo una intensa glaciación en la que gran parte del planeta fue cubierto por una capa de hielo. Esta hipótesis ha sido denominada la "Glaciación global", y es de particular interés, ya que este suceso precedió a la llamada explosión del Cámbrico, en la que las formas de vida multicelulares comenzaron a proliferar.

Tras la explosión del Cámbrico, hace unos se han producido cinco extinciones en masa. De ellas, el evento más reciente ocurrió hace , cuando el impacto de un asteroide provocó la extinción de los dinosaurios no aviarios, así como de otros grandes reptiles, salvándose algunos pequeños animales como los mamíferos, que por aquel entonces eran similares a las actuales musarañas. Durante los últimos los mamíferos se diversificaron, hasta que hace varios millones de años, un animal africano con aspecto de simio conocido como el orrorin tugenensis adquirió la capacidad de mantenerse en pie. Esto le permitió utilizar herramientas y favoreció su capacidad de comunicación, proporcionando la nutrición y la estimulación necesarias para desarrollar un cerebro más grande, y permitiendo así la evolución de la raza humana. El desarrollo de la agricultura y de la civilización permitió a los humanos alterar la Tierra en un corto espacio de tiempo como no lo había hecho ninguna otra especie, afectando tanto a la naturaleza como a la diversidad y cantidad de formas de vida.

El presente patrón de edades de hielo comenzó hace alrededor de y luego se intensificó durante el Pleistoceno, hace alrededor de . Desde entonces las regiones en latitudes altas han sido objeto de repetidos ciclos de glaciación y deshielo, en ciclos de 40-100 mil años. La última glaciación continental terminó hace 10 000 años.

El futuro del planeta está estrechamente ligado al del Sol. Como resultado de la acumulación constante de helio en el núcleo del Sol, la luminosidad total de la estrella irá poco a poco en aumento. La luminosidad del Sol crecerá en un 10 % en los próximos 1,1 Ga y en un 40 % en los próximos 3,5 Ga. Los modelos climáticos indican que el aumento de la radiación podría tener consecuencias nefastas en la Tierra, incluyendo la pérdida de los océanos del planeta.

Se espera que la Tierra sea habitable por alrededor de otros a partir de este momento, aunque este periodo podría extenderse hasta si se elimina el nitrógeno de la atmósfera. El aumento de temperatura en la superficie terrestre acelerará el ciclo del CO inorgánico, lo que reducirá su concentración hasta niveles letalmente bajos para las plantas (10 ppm para la fotosíntesis C) dentro de aproximadamente 500 a 900 millones de años. La falta de vegetación resultará en la pérdida de oxígeno en la atmósfera, lo que provocará la extinción de la vida animal a lo largo de varios millones de años más. Después de otros mil millones de años, todas las aguas superficiales habrán desaparecido y la temperatura media global alcanzará los 70 °C. Incluso si el Sol fuese eterno y estable, el continuo enfriamiento interior de la Tierra se traduciría en una gran pérdida de CO debido a la reducción de la actividad volcánica, y el 35 % del agua de los océanos podría descender hasta el manto debido a la disminución del vapor de ventilación en las dorsales oceánicas.

El Sol, siguiendo su evolución natural, se convertirá en una gigante roja en unos 5 Ga. Los modelos predicen que el Sol se expandirá hasta unas 250 veces su tamaño actual, alcanzando un radio cercano a 1 UA (unos 150 millones de km). El destino que sufrirá la Tierra entonces no está claro. Siendo una gigante roja, el Sol perderá aproximadamente el 30 % de su masa, por lo que sin los efectos de las mareas, la Tierra se moverá a una órbita de 1,7 UA (unos 250 millones de km) del Sol cuando la estrella alcance su radio máximo. Por lo tanto se espera que el planeta escape inicialmente de ser envuelto por la tenue atmósfera exterior expandida del Sol. Aun así, cualquier forma de vida restante sería destruida por el aumento de la luminosidad del Sol (alcanzando un máximo de cerca de 5000 veces su nivel actual). Sin embargo, una simulación realizada en 2008 indica que la órbita de la Tierra decaerá debido a los efectos de marea y arrastre, ocasionando que el planeta penetre en la atmósfera estelar y se vaporice.

La Tierra es un planeta terrestre, lo que significa que es un cuerpo rocoso y no un gigante gaseoso como Júpiter. Es el más grande de los cuatro planetas terrestres del sistema solar en tamaño y masa, y también es el que tiene la mayor densidad, la mayor gravedad superficial, el campo magnético más fuerte y la rotación más rápida de los cuatro. También es el único planeta terrestre con placas tectónicas activas. El movimiento de estas placas produce que la superficie terrestre esté en constante cambio, siendo responsables de la formación de montañas, de la sismicidad y del vulcanismo.
El ciclo de estas placas también juega un papel preponderante en la regulación de la temperatura terrestre, contribuyendo al reciclaje de gases con efecto invernadero como el dióxido de carbono, por medio de la renovación permanente de los fondos oceánicos.

La forma de la Tierra es muy parecida a la de un esferoide oblato, una esfera achatada por los polos, resultando en un abultamiento alrededor del ecuador. Este abultamiento está causado por la rotación de la Tierra, y ocasiona que el diámetro en el ecuador sea 43 km más largo que el diámetro de un polo a otro. Hace aproximadamente 22 000 años la Tierra tenía una forma más esférica, la mayor parte del hemisferio norte se encontraba cubierto por hielo, y a medida de que el hielo se derretía causaba una menor presión en la superficie terrestre en la que se sostenía, causando esto un tipo de «rebote». Este fenómeno siguió ocurriendo hasta mediados de los años noventa, cuando los científicos se percataron de que este proceso se había invertido, es decir, el abultamiento aumentaba. Las observaciones del satélite GRACE muestran que, al menos desde 2002, la pérdida de hielo de Groenlandia y de la Antártida ha sido la principal responsable de esta tendencia.

La topografía local se desvía de este esferoide idealizado, aunque las diferencias a escala global son muy pequeñas: la Tierra tiene una desviación de aproximadamente una parte entre 584, o el 0,17 %, desde el esferoide de referencia, que es menor que la tolerancia del 0,22 % permitida en las bolas de billar. Las mayores desviaciones locales en la superficie rocosa de la Tierra son el monte Everest (8 848 m sobre el nivel local del mar) y el abismo Challenger, al sur de la fosa de las Marianas (10 911 m bajo el nivel local del mar). Debido a la protuberancia ecuatorial, el punto terrestre más alejado del centro de la tierra es el volcán Chimborazo en Ecuador.

La circunferencia en el ecuador es de . El diámetro en el ecuador es de y en los polos de 

El diámetro medio de referencia para el esferoide es de unos , que es aproximadamente , ya que el metro se definió originalmente como la diezmillonésima parte de la distancia desde el ecuador hasta el Polo Norte por París, Francia.

La primera medición del tamaño de la Tierra fue hecha por Eratóstenes, el 240 a. C.. En esa época se aceptaba que la Tierra era esférica. Eratóstenes calculó el tamaño de la Tierra midiendo el ángulo con que alumbraba el Sol en el solsticio, tanto en Alejandría como en Siena, distante 750km. El tamaño que obtuvo fue de un diámetro de y una circunferencia de , es decir, con un error de sólo el 6% respecto a los datos actuales.

Posteriormente Posidonio de Apamea repitió las mediciones en el año 100 a. C., obteniendo el dato de para la circunferencia, considerablemente más impreciso respecto a los datos actuales. Este último valor fue el que aceptó Ptolomeo, por lo que prevaleció ese valor en los próximos siglos.

Cuando Magallanes dio la vuelta a todo el planeta en 1521, se restableció el dato calculado por Eratóstenes.

La masa de la Tierra es aproximadamente de 5,98 kg. Se compone principalmente de hierro (32,1 %), oxígeno (30,1 %), silicio (15,1 %), magnesio (13,9 %), azufre (2,9 %), níquel (1,8 %), calcio (1,5 %) y aluminio (1,4 %), con el 1,2 % restante formado por pequeñas cantidades de otros elementos. Debido a la segregación de masa, se cree que la zona del núcleo está compuesta principalmente de hierro (88,8 %), con pequeñas cantidades de níquel (5,8 %), azufre (4,5 %), y menos del 1 % formado por trazas de otros elementos.

El geoquímico F. W. Clarke (1847-1931), llamado «el padre de la geoquímica por haber determinado la composición de la corteza de la Tierra», calculó que un poco más del 47 % de la corteza terrestre se compone de oxígeno. Los componentes de las rocas más comunes de la corteza de la Tierra son casi todos los óxidos. Cloro, azufre y flúor son las únicas excepciones significativas, y su presencia total en cualquier roca es generalmente mucho menor del 1 %. Los principales óxidos son los de sílice, alúmina, hierro, cal, magnesia, potasa y sosa. La sílice actúa principalmente como un ácido, formando silicatos, y los minerales más comunes de las rocas ígneas son de esta naturaleza. A partir de un cálculo sobre la base de 1672 análisis de todo tipo de rocas, Clarke dedujo que un 99,22 % de las rocas están compuestas por 11 óxidos (véase el cuadro a la derecha). Todos los demás compuestos aparecen solo en cantidades muy pequeñas.

Clarke enseña la química y la física en la Universidad de Howard en Washington desde 1873 hasta 1874 y la Universidad de Cincinnati desde 1873 hasta 1883. Desarrolló el primer informe de gobierno en la enseñanza de la ciencia en los Estados Unidos, que fue publicado en la revista "Science" en octubre de 1881.

En 1908 publicó la primera edición de su obra, los datos de Geoquímica, que fue publicado por el United States Geological Survey, mientras que él era químico en jefe. La quinta edición fue publicada en 1924, cuando se retiró de la actividad. Falleció en su ciudad natal, el 23 de mayo de 1931.

El interior de la Tierra, al igual que el de los otros planetas terrestres, está dividido en capas según su composición química o sus propiedades físicas (reológicas), pero, a diferencia de los otros planetas terrestres, tiene un núcleo interno y externo distintos. Su capa externa es una corteza de silicato sólido, químicamente diferenciado, bajo la cual se encuentra un manto sólido de alta viscosidad. La corteza está separada del manto por la discontinuidad de Mohorovičić, variando el espesor de la misma desde un promedio de 6 km en los océanos a entre 30 y 50 km en los continentes. La corteza y la parte superior fría y rígida del manto superior se conocen comúnmente como la litosfera, y es de la litosfera de lo que están compuestas las placas tectónicas. Debajo de la litosfera se encuentra la astenosfera, una capa de relativamente baja viscosidad sobre la que flota la litosfera. Dentro del manto, entre los 410 y 660 km bajo la superficie, se producen importantes cambios en la estructura cristalina. Estos cambios generan una zona de transición que separa la parte superior e inferior del manto. Bajo el manto se encuentra un núcleo externo líquido de viscosidad extremadamente baja, descansando sobre un núcleo interno sólido. El núcleo interno puede girar con una velocidad angular ligeramente superior que el resto del planeta, avanzando de 0,1 a 0,5° por año.

El calor interno de la Tierra proviene de una combinación del calor residual de la acreción planetaria (20 %) y el calor producido por la desintegración radiactiva (80 %). Los isótopos con mayor producción de calor en la Tierra son el potasio-40, el uranio-238, el uranio-235 y el torio-232. En el centro del planeta, la temperatura puede llegar hasta los 7000 °K y la presión puede alcanzar los 360 GPa. Debido a que gran parte del calor es proporcionado por la desintegración radiactiva, los científicos creen que en la historia temprana de la Tierra, antes de que los isótopos de reducida vida media se agotaran, la producción de calor de la Tierra fue mucho mayor. Esta producción de calor extra, que hace aproximadamente 3000 millones de años era el doble que la producción actual, pudo haber incrementado los gradientes de temperatura dentro de la Tierra, incrementando la convección del manto y la tectónica de placas, permitiendo la producción de rocas ígneas como las komatitas que no se forman en la actualidad.

El promedio de pérdida de calor de la Tierra es de , que supone una pérdida global de . Una parte de la energía térmica del núcleo es transportada hacia la corteza por plumas del manto, una forma de convección que consiste en afloramientos de roca a altas temperaturas. Estas plumas pueden producir puntos calientes y coladas de basalto. La mayor parte del calor que pierde la Tierra se filtra entre las placas tectónicas, en las surgencias del manto asociadas a las dorsales oceánicas. Casi todas las pérdidas restantes se producen por conducción a través de la litosfera, principalmente en los océanos, ya que allí la corteza es mucho más delgada que en los continentes.

La mecánicamente rígida capa externa de la Tierra, la litosfera, está fragmentada en piezas llamadas placas tectónicas. Estas placas son elementos rígidos que se mueven en relación uno con otro siguiendo uno de estos tres patrones: bordes convergentes, en los que dos placas se aproximan; bordes divergentes, en los que dos placas se separan, y bordes transformantes, en los que dos placas se deslizan lateralmente entre sí. A lo largo de estos bordes de placa se producen los terremotos, la actividad volcánica, la formación de montañas y la formación de fosas oceánicas. Las placas tectónicas se deslizan sobre la parte superior de la astenosfera, la sólida pero menos viscosa sección superior del manto, que puede fluir y moverse junto con las placas, y cuyo movimiento está fuertemente asociado a los patrones de convección dentro del manto terrestre.

A medida que las placas tectónicas migran a través del planeta, el fondo oceánico se subduce bajo los bordes de las placas en los límites convergentes. Al mismo tiempo, el afloramiento de material del manto en los límites divergentes crea las dorsales oceánicas. La combinación de estos procesos recicla continuamente la corteza oceánica nuevamente en el manto. Debido a este proceso de reciclaje, la mayor parte del suelo marino tiene menos de de años de edad. La corteza oceánica más antigua se encuentra en el Pacífico Occidental, y tiene una edad estimada de unos de años. En comparación, la corteza continental más antigua registrada tiene de años de edad.

Las siete placas más grandes son la Pacífica, Norteamericana, Euroasiática, Africana Antártica, Indoaustraliana y Sudamericana. Otras placas notables son la placa Índica, la placa Arábiga, la placa del Caribe, la placa de Nazca en la costa occidental de América del Sur y la placa Escocesa en el sur del océano Atlántico. La placa de Australia se fusionó con la placa de la India hace entre 50 y 55 millones de años. Las placas con movimiento más rápido son las placas oceánicas, con la placa de Cocos avanzando a una velocidad de 75 mm/año y la placa del Pacífico moviéndose 52-69 mm/año. En el otro extremo, la placa con movimiento más lento es la placa eurasiática, que avanza a una velocidad típica de aproximadamente 21 mm/año.

El relieve de la Tierra varía enormemente de un lugar a otro. Cerca del 70,8 % de la superficie está cubierta por agua, con gran parte de la plataforma continental por debajo del nivel del mar. La superficie sumergida tiene características montañosas, incluyendo un sistema de dorsales oceánicas, así como volcanes submarinos, fosas oceánicas, cañones submarinos, mesetas y llanuras abisales. El restante 29,2 % no cubierto por el agua se compone de montañas, desiertos, llanuras, mesetas y otras geomorfologías.

La superficie del planeta se moldea a lo largo de períodos de tiempo geológicos, debido a la erosión tectónica. Las características de esta superficie formada o deformada mediante la tectónica de placas están sujetas a una constante erosión a causa de las precipitaciones, los ciclos térmicos y los efectos químicos. La glaciación, la erosión costera, la acumulación de los arrecifes de coral y los grandes impactos de meteoritos también actúan para remodelar el paisaje.
La corteza continental se compone de material de menor densidad, como las rocas ígneas, el granito y la andesita. Menos común es el basalto, una densa roca volcánica que es el componente principal de los fondos oceánicos. Las rocas sedimentarias se forman por la acumulación de sedimentos compactados. Casi el 75 % de la superficie continental está cubierta por rocas sedimentarias, a pesar de que estas solo forman un 5 % de la corteza. El tercer material rocoso más abundante en la Tierra son las rocas metamórficas, creadas a partir de la transformación de tipos de roca ya existentes mediante altas presiones, altas temperaturas, o ambas. Los minerales de silicato más abundantes en la superficie de la Tierra incluyen el cuarzo, los feldespatos, el anfíbol, la mica, el piroxeno y el olivino. Los minerales de carbonato más comunes son la calcita (que se encuentra en piedra caliza) y la dolomita.

La pedosfera es la capa más externa de la Tierra. Está compuesta de tierra y está sujeta a los procesos de formación del suelo. Existe en el encuentro entre la litosfera, la atmósfera, la hidrosfera y la biosfera. Actualmente el 13,31 % del total de la superficie terrestre es tierra cultivable, y solo el 4,71 % soporta cultivos permanentes. Cerca del 40 % de la superficie emergida se utiliza actualmente como tierras de cultivo y pastizales, estimándose un total de 1,3 km² para tierras de cultivo y 3,4 km² para tierras de pastoreo.

La elevación de la superficie terrestre varía entre el punto más bajo de –418 m en el mar Muerto a una altitud máxima, estimada en 2005, de 8848 m en la cima del monte Everest. La altura media de la tierra sobre el nivel del mar es de 840 m.

El satélite ambiental Envisat de la ESA desarrolló un retrato detallado de la superficie de la Tierra. A través del proyecto GLOBCOVER se desarrolló la creación de un mapa global de la cobertura terrestre con una resolución tres veces superior a la de cualquier otro mapa por satélite hasta aquel momento. Utilizó reflectores radar con antenas de ancho sintéticas, capturando con sus sensores la radiación reflejada.

La NASA completó un nuevo mapa tridimensional, que es la topografía más precisa del planeta, elaborada durante cuatro años con los datos transmitidos por el transbordador espacial "Endeavour". Los datos analizados corresponden al 80 % de la masa terrestre. Cubre los territorios de Australia y Nueva Zelanda con detalles sin precedentes. También incluye más de mil islas de la Polinesia y la Melanesia en el Pacífico sur, así como islas del Índico y el Atlántico. Muchas de esas islas apenas se levantan unos metros sobre el nivel del mar y son muy vulnerables a los efectos de las marejadas y tormentas, por lo que su conocimiento ayudará a evitar catástrofes; los datos proporcionados por la misión del Endeavour tendrán una amplia variedad de usos, como la exploración virtual del planeta.

La abundancia de agua en la superficie de la Tierra es una característica única que distingue al "Planeta Azul" de otros en el Sistema Solar. La hidrosfera de la Tierra está compuesta fundamentalmente por océanos, pero técnicamente incluye todas las superficies de agua en el mundo, incluidos los mares interiores, lagos, ríos y aguas subterráneas hasta una profundidad de 2000 m. El lugar más profundo bajo el agua es el abismo Challenger de la fosa de las Marianas, en el océano Pacífico, con una profundidad de –10 911,4 m.

La masa de los océanos es de aproximadamente 1,35 toneladas métricas, o aproximadamente 1/4400 de la masa total de la Tierra. Los océanos cubren un área de 361,84 km² con una profundidad media de 3682,2 m, lo que resulta en un volumen estimado de 1,3324 km³. Si se nivelase toda la superficie terrestre, el agua cubriría la superficie del planeta hasta una altura de más de 2,7 km. El área total de la Tierra es de 5,1 km². Para la primera aproximación, la profundidad media sería la relación entre los dos, o de 2,7 km. Aproximadamente el 97,5 % del agua es salada, mientras que el restante 2,5 % es agua dulce. La mayor parte del agua dulce, aproximadamente el 68,7 %, se encuentra actualmente en estado de hielo.

La salinidad media de los océanos es de unos 35 gramos de sal por kilogramo de agua (35 ‰). La mayor parte de esta sal fue liberada por la actividad volcánica, o extraída de las rocas ígneas ya enfriadas. Los océanos son también un reservorio de gases atmosféricos disueltos, siendo estos esenciales para la supervivencia de muchas formas de vida acuática. El agua de los océanos tiene una influencia importante sobre el clima del planeta, actuando como un foco calórico de gran tamaño. Los cambios en la distribución de la temperatura oceánica pueden causar alteraciones climáticas, tales como la Oscilación del Sur, El Niño.

La presión atmosférica media al nivel del mar se sitúa en torno a los 101,325 kPa, con una escala de altura de aproximadamente 8,5 km. Está compuesta principalmente de un 78 % de nitrógeno y un 21 % de oxígeno, con trazas de vapor de agua, dióxido de carbono y otras moléculas gaseosas. La altura de la troposfera varía con la latitud, entre 8 km en los polos y 17 km en el ecuador, con algunas variaciones debido a la climatología y los factores estacionales.

La biosfera de la Tierra ha alterado significativamente la atmósfera. La fotosíntesis oxigénica evolucionó hace 2700 millones de años, formando principalmente la atmósfera actual de nitrógeno-oxígeno. Este cambio permitió la proliferación de los organismos aeróbicos, así como la formación de la capa de ozono que bloquea la radiación ultravioleta proveniente del Sol, permitiendo la vida fuera del agua. Otras funciones importantes de la atmósfera para la vida en la Tierra incluyen el transporte de vapor de agua, proporcionar gases útiles, quemar los meteoritos pequeños antes de que alcancen la superficie, y moderar la temperatura. Este último fenómeno se conoce como el efecto invernadero: trazas de moléculas presentes en la atmósfera capturan la energía térmica emitida desde el suelo, aumentando así la temperatura media. El dióxido de carbono, el vapor de agua, el metano y el ozono son los principales gases de efecto invernadero de la atmósfera de la Tierra. Sin este efecto de retención del calor, la temperatura superficial media sería de –18 °C y la vida probablemente no existiría.

La atmósfera terrestre no tiene unos límites definidos, haciéndose poco a poco más delgada hasta desvanecerse en el espacio exterior. Tres cuartas partes de la masa atmosférica están contenidas dentro de los primeros 11 km de la superficie del planeta. Esta capa inferior se llama troposfera. La energía del Sol calienta esta capa y la superficie bajo ésta, causando la expansión del aire. El aire caliente se eleva debido a su menor densidad, siendo sustituido por aire de mayor densidad, es decir, aire más frío. Esto da como resultado la circulación atmosférica que genera el tiempo y el clima a través de la redistribución de la energía térmica.

Las líneas principales de circulación atmosférica las constituyen los vientos alisios en la región ecuatorial por debajo de los 30° de latitud, y los vientos del oeste en latitudes medias entre los 30° y 60°. Las corrientes oceánicas también son factores importantes para determinar el clima, especialmente la circulación termohalina que distribuye la energía térmica de los océanos ecuatoriales a las regiones polares.

El vapor de agua generado a través de la evaporación superficial es transportado según los patrones de circulación de la atmósfera. Cuando las condiciones atmosféricas permiten la elevación del aire caliente y húmedo, el agua se condensa y se deposita en la superficie en forma de precipitaciones. La mayor parte del agua es transportada a altitudes más bajas mediante los sistemas fluviales y por lo general regresa a los océanos o es depositada en los lagos. Este ciclo del agua es un mecanismo vital para sustentar la vida en la tierra y es un factor primario de la erosión que modela la superficie terrestre a lo largo de períodos geológicos. Los patrones de precipitación varían enormemente, desde varios metros de agua por año a menos de un milímetro. La circulación atmosférica, las características topológicas y las diferencias de temperatura determinan las precipitaciones medias de cada región.

La cantidad de energía solar que llega a la Tierra disminuye al aumentar la latitud. En las latitudes más altas la luz solar incide en la superficie en un ángulo menor, teniendo que atravesar gruesas columnas de atmósfera. Como resultado, la temperatura media anual del aire a nivel del mar se reduce en aproximadamente 0,4 °C por cada grado de latitud alejándose del ecuador. La Tierra puede ser subdividida en franjas latitudinales más o menos homogéneas con un clima específico. Desde el ecuador hasta las regiones polares, se encuentran la zona intertropical (o ecuatorial), el clima subtropical, el clima templado y los climas polares. El clima también puede ser clasificado en función de la temperatura y las precipitaciones, en regiones climáticas caracterizadas por masas de aire bastante uniformes. La metodología de clasificación más usada es la clasificación climática de Köppen (modificada por el estudiante de Wladimir Peter Köppen, Rudolph Geiger), que cuenta con cinco grandes grupos (zonas tropicales húmedas, zonas áridas, zonas húmedas con latitud media, clima continental y frío polar), que se dividen en subtipos más específicos.

Por encima de la troposfera, la atmósfera suele dividir en estratosfera, mesosfera y termosfera. Cada capa tiene un gradiente adiabático diferente, que define la tasa de cambio de la temperatura con respecto a la altura. Más allá de éstas se encuentra la exosfera, que se atenúa hasta penetrar en la magnetosfera, donde los campos magnéticos de la Tierra interactúan con el viento solar. Dentro de la estratosfera se encuentra la capa de ozono; un componente que protege parcialmente la superficie terrestre de la luz ultravioleta, siendo un elemento importante para la vida en la Tierra. La línea de Kármán, definida en los 100 km sobre la superficie de la Tierra, es una definición práctica usada para establecer el límite entre la atmósfera y el espacio.

La energía térmica hace que algunas de las moléculas en el borde exterior de la atmósfera de la Tierra incrementen su velocidad hasta el punto de poder escapar de la gravedad del planeta. Esto da lugar a una pérdida lenta pero constante de la atmósfera hacia el espacio. Debido a que el hidrógeno no fijado tiene un bajo peso molecular puede alcanzar la velocidad de escape más fácilmente, escapando así al espacio exterior a un ritmo mayor que otros gases. La pérdida de hidrógeno hacia el espacio contribuye a la transformación de la Tierra desde su inicial estado reductor a su actual estado oxidante. La fotosíntesis proporcionó una fuente de oxígeno libre, pero se cree que la pérdida de agentes reductores como el hidrógeno fue una condición previa necesaria para la acumulación generalizada de oxígeno en la atmósfera. Por tanto, la capacidad del hidrógeno para escapar de la atmósfera de la Tierra puede haber influido en la naturaleza de la vida desarrollada en el planeta. En la atmósfera actual, rica en oxígeno, la mayor parte del hidrógeno se convierte en agua antes de tener la oportunidad de escapar. En cambio, la mayor parte de la pérdida de hidrógeno actual proviene de la destrucción del metano en la atmósfera superior.

El campo magnético de la Tierra tiene una forma similar a un dipolo magnético, con los polos actualmente localizados cerca de los polos geográficos del planeta. En el ecuador del campo magnético (ecuador magnético), la fuerza del campo magnético en la superficie es , con un momento magnético dipolar global de . Según la teoría del dínamo, el campo se genera en el núcleo externo fundido, región donde el calor crea movimientos de convección en materiales conductores, generando corrientes eléctricas. Estas corrientes inducen a su vez el campo magnético de la Tierra. Los movimientos de convección en el núcleo son caóticos; los polos magnéticos se mueven y periódicamente cambian de orientación. Esto da lugar a reversiones geomagnéticas a intervalos de tiempo irregulares, unas pocas veces cada millón de años. La inversión más reciente tuvo lugar hace aproximadamente 700 000 años.

El campo magnético forma la magnetosfera, que desvía las partículas de viento solar. En dirección al Sol, el arco de choque entre el viento solar y la magnetosfera se encuentra a unas 13 veces el radio de la Tierra. La colisión entre el campo magnético y el viento solar forma los cinturones de radiación de Van Allen; un par de regiones concéntricas, con forma tórica, formadas por partículas cargadas muy energéticas. Cuando el plasma entra en la atmósfera de la Tierra por los polos magnéticos se crean las auroras polares.

El período de rotación de la Tierra con respecto al Sol, es decir, un día solar, es de alrededor de 86 400 segundos de tiempo solar (86 400,0025 segundos SIU). El día solar de la Tierra es ahora un poco más largo de lo que era durante el siglo XIX debido a la aceleración de marea, los días duran entre 0 y 2ms SIU más.
El período de rotación de la Tierra en relación a las estrellas fijas, llamado día estelar por el Servicio Internacional de Rotación de la Tierra y Sistemas de Referencia (IERS por sus siglas en inglés), es de del tiempo solar medio (UT1), o de El período de rotación de la Tierra en relación con el equinoccio vernal, mal llamado el "día sidéreo", es de del tiempo solar medio (UT1) . Por tanto, el día sidéreo es más corto que el día estelar en torno a 8,4 ms. La longitud del día solar medio en segundos SIU está disponible en el IERS para los períodos 1623-2005 y 1962-2005.

Aparte de los meteoros en la atmósfera y de los satélites en órbita baja, el movimiento aparente de los cuerpos celestes vistos desde la Tierra se realiza hacia al oeste, a una velocidad de 15°/h = 15'/min. Para las masas cercanas al ecuador celeste, esto es equivalente a un diámetro aparente del Sol o de la Luna cada dos minutos (desde la superficie del planeta, los tamaños aparentes del Sol y de la Luna son aproximadamente iguales).

La Tierra orbita alrededor del Sol a una distancia media de unos 150 millones de kilómetros, completando una órbita cada 365,2564 días solares, o un año sideral. Desde la Tierra, esto genera un movimiento aparente del Sol hacia el este, desplazándose con respecto a las estrellas a un ritmo de alrededor de 1°/día, o un diámetro del Sol o de la Luna cada 12 horas. Debido a este movimiento, en promedio la Tierra tarda 24 horas (un día solar) en completar una rotación sobre su eje hasta que el sol regresa al meridiano. La velocidad orbital de la Tierra es de aproximadamente 29,8 km/s (107 000 km/h), que es lo suficientemente rápida como para recorrer el diámetro del planeta (12 742 km) en siete minutos, o la distancia entre la Tierra y la Luna (384 000 km) en cuatro horas.

La Luna gira con la Tierra en torno a un baricentro común, debido a que este se encuentra dentro de la Tierra, a 4541 km de su centro, el sistema Tierra-Luna no es un planeta doble, la Luna completa un giro cada 27,32 días con respecto a las estrellas de fondo. Cuando se combina con la revolución común del sistema Tierra-Luna alrededor del Sol, el período del mes sinódico, desde una luna nueva a la siguiente, es de 29,53 días. Visto desde el polo norte celeste, el movimiento de la Tierra, la Luna y sus rotaciones axiales son todas contrarias a la dirección de las manecillas del reloj (sentido anti-horario). Visto desde un punto de vista situado sobre los polos norte del Sol y la Tierra, la Tierra parecería girar en sentido anti-horario alrededor del Sol. Los planos orbitales y axiales no están alineados: El eje de la Tierra está inclinado unos 23,4 grados con respecto a la perpendicular al plano Tierra-Sol, y el plano entre la Tierra y la Luna está inclinado unos 5 grados con respecto al plano Tierra-Sol. Sin esta inclinación, habría un eclipse cada dos semanas, alternando entre los eclipses lunares y eclipses solares.

La esfera de Hill, o la esfera de influencia gravitatoria, de la Tierra tiene aproximadamente 1,5 Gm (o 1 500 000 kilómetros) de radio. Esta es la distancia máxima en la que la influencia gravitatoria de la Tierra es más fuerte que la de los más distantes Sol y resto de planetas. Los objetos deben orbitar la Tierra dentro de este radio, o terminarán atrapados por la perturbación gravitatoria del Sol.

Desde el año de 1772, se estableció que cuerpos pequeños pueden orbitar de manera estable la misma órbita que un planeta, si esta permanece cerca de un punto triangular de Lagrange (también conocido como «punto troyano») los cuales están situados 60° delante y 60° detrás del planeta en su órbita. La Tierra es el cuarto planeta con un asteroide troyano (2010 TK7) después de Júpiter, Marte y Neptuno de acuerdo a la fecha de su descubrimiento Este fue difícil de localizar debido al posicionamiento geométrico de la observación, este fue descubierto en el 2010 gracias al telescopio WISE (Wide-Field Infrared Survey Explorer) de la NASA, pero fue en abril de 2011 con el telescopio «Canadá-Francia-Hawái» cuando se confirmó su naturaleza troyana, y se estima que su órbita permanezca estable dentro de los próximos 10 000 años.

La Tierra, junto con el Sistema Solar, está situada en la galaxia Vía Láctea, orbitando a alrededor de 28 000 años luz del centro de la galaxia. En la actualidad se encuentra unos 20 años luz por encima del plano ecuatorial de la galaxia, en el brazo espiral de Orión.

Debido a la inclinación del eje de la Tierra, la cantidad de luz solar que llega a un punto cualquiera en la superficie varía a lo largo del año. Esto ocasiona los cambios estacionales en el clima, siendo verano en el hemisferio norte ocurre cuando el Polo Norte está apuntando hacia el Sol, e invierno cuando apunta en dirección opuesta. Durante el verano, el día tiene una duración más larga y la luz solar incide más perpendicularmente en la superficie. Durante el invierno, el clima se vuelve más frío y los días más cortos. En la zona del Círculo Polar Ártico se da el caso extremo de no recibir luz solar durante una parte del año; fenómeno conocido como la noche polar. En el hemisferio sur se da la misma situación pero de manera inversa, con la orientación del Polo Sur opuesta a la dirección del Polo Norte.
Por convenio astronómico, las cuatro estaciones están determinadas por solsticios (puntos de la órbita en los que el eje de rotación terrestre alcanza la máxima inclinación hacia el Sol —solsticio de verano— o hacia el lado opuesto —solsticio de invierno—) y por equinoccios, cuando la inclinación del eje terrestre es perpendicular al Sol. En el hemisferio norte, el solsticio de invierno se produce alrededor del 21 de diciembre, el solsticio de verano el 21 de junio, el equinoccio de primavera el 20 de marzo y el equinoccio de otoño el 23 de septiembre. En el hemisferio sur la situación se invierte, con el verano y los solsticios de invierno en fechas contrarias a la del hemisferio norte. De igual manera sucede con el equinoccio de primavera y de otoño.

El ángulo de inclinación de la Tierra es relativamente estable durante largos períodos de tiempo. Sin embargo, la inclinación se somete a nutaciones; un ligero movimiento irregular, con un período de 18,6 años. La orientación (en lugar del ángulo) del eje de la Tierra también cambia con el tiempo, precesando un círculo completo en cada ciclo de 25 800 años. Esta precesión es la razón de la diferencia entre el año sidéreo y el año tropical. Ambos movimientos son causados por la atracción variante del Sol y la Luna sobre el abultamiento ecuatorial de la Tierra. Desde la perspectiva de la Tierra, los polos también migran unos pocos metros sobre la superficie. Este movimiento polar tiene varios componentes cíclicos, que en conjunto reciben el nombre de movimientos cuasiperiódicos. Además del componente anual de este movimiento, existe otro movimiento con ciclos de 14 meses llamado el bamboleo de Chandler. La velocidad de rotación de la Tierra también varía en un fenómeno conocido como variación de duración del día.

En tiempos modernos, el perihelio de la Tierra se produce alrededor del 3 de enero y el afelio alrededor del 4 de julio. Sin embargo, estas fechas cambian con el tiempo debido a la precesión orbital y otros factores, que siguen patrones cíclicos conocidos como ciclos de Milankovitch. La variación de la distancia entre la Tierra y el Sol resulta en un aumento de alrededor del 6,9 % de la energía solar que llega a la Tierra en el perihelio en relación con el afelio. Puesto que el hemisferio sur está inclinado hacia el Sol en el momento en que la Tierra alcanza la máxima aproximación al Sol, a lo largo del año el hemisferio sur recibe algo más de energía del Sol que el hemisferio norte. Sin embargo, este efecto es mucho menos importante que el cambio total de energía debido a la inclinación del eje, y la mayor parte de este exceso de energía es absorbido por la superficie oceánica, que se extiende en mayor proporción en el hemisferio sur.

La Luna es el satélite natural de la Tierra. Es un cuerpo del tipo terrestre relativamente grande: con un diámetro de alrededor de la cuarta parte del de la Tierra, es el segundo satélite más grande del Sistema Solar en relación al tamaño de su planeta, después del satélite Caronte de su planeta enano Plutón. Los satélites naturales que orbitan los demás planetas se denominan "lunas" en referencia a la Luna de la Tierra.

La atracción gravitatoria entre la Tierra y la Luna causa las mareas en la Tierra. El mismo efecto en la Luna ha dado lugar a su acoplamiento de marea, lo que significa que su período de rotación es idéntico a su periodo de traslación alrededor de la Tierra. Como resultado, la luna siempre presenta la misma cara hacia nuestro planeta. A medida que la Luna orbita la Tierra, diferentes partes de su cara son iluminadas por el Sol, dando lugar a las fases lunares. La parte oscura de la cara está separada de la parte iluminada del terminador solar.

Debido a la interacción de las mareas, la Luna se aleja de la Tierra a una velocidad de aproximadamente 38 mm al año. Acumuladas durante millones de años, estas pequeñas modificaciones, así como el alargamiento del día terrestre en alrededor de 23 µs, han producido cambios significativos. Durante el período devónico, por ejemplo, (hace aproximadamente ) un año tenía 400 días, cada uno con una duración de 21,8 horas.

La Luna pudo haber afectado dramáticamente el desarrollo de la vida, moderando el clima del planeta. Evidencias paleontológicas y simulaciones computarizadas muestran que la inclinación del eje terrestre está estabilizada por las interacciones de marea con la Luna. Algunos teóricos creen que sin esta estabilización frente al momento ejercido por el Sol y los planetas sobre la protuberancia ecuatorial de la Tierra, el eje de rotación podría ser caóticamente inestable, mostrando cambios caóticos durante millones de años, como parece ser el caso de Marte.

Vista desde la Tierra, la Luna está justo a una distancia que la hace que el tamaño aparente de su disco sea casi idéntico al del Sol. El diámetro angular (o ángulo sólido) de estos dos cuerpos coincide porque aunque el diámetro del Sol es unas 400 veces más grande que el de la Luna, también está 400 veces más distante. Esto permite que en la Tierra se produzcan los eclipses solares totales y anulares.

La teoría más ampliamente aceptada sobre el origen de la Luna, la teoría del gran impacto, afirma que ésta se formó por la colisión de un protoplaneta del tamaño de Marte, llamado Tea, con la Tierra primitiva. Esta hipótesis explica (entre otras cosas) la relativa escasez de hierro y elementos volátiles en la Luna, y el hecho de que su composición sea casi idéntica a la de la corteza terrestre.

A fecha de 2016, el planeta Tierra tiene nueve cuasisatélites naturales o asteroides coorbitales conocidos: el (3753) Cruithne, 
el 2002 AA, 2003 YN, 
2004 GU, 2006 FV, 2010 SO 2013 LX, 2014 OL y 2016 H.

A fecha de 2011, existen 931 satélites operativos creados por el hombre orbitando la Tierra.

Un planeta que pueda sostener vida se denomina habitable, incluso aunque en él no se originara vida. La Tierra proporciona las (actualmente entendidas como) condiciones necesarias, tales como el agua líquida, un ambiente que permite el ensamblaje de moléculas orgánicas complejas, y la energía suficiente para mantener un metabolismo. Hay otras características que se cree que también contribuyen a la capacidad del planeta para originar y mantener la vida: la distancia entre la Tierra y el Sol, así como su excentricidad orbital, la velocidad de rotación, la inclinación axial, la historia geológica, la permanencia de la atmósfera, y la protección ofrecida por el campo magnético.

Se denomina "biosfera" al conjunto de los diferentes tipos de vida del planeta junto con su entorno físico, modificado por la presencia de los primeros. Generalmente se entiende que la biosfera empezó a evolucionar hace 3500 millones de años. La Tierra es el único lugar donde se sabe que existe vida. La biosfera se divide en una serie de biomas, habitados por plantas y animales esencialmente similares. En tierra, los biomas se separan principalmente por las diferencias en latitud, la altura sobre el nivel del mar y la humedad. Los biomas terrestres situados en los círculos ártico o antártico, en gran altura o en zonas extremadamente áridas son relativamente estériles de vida vegetal y animal; la diversidad de especies alcanza su máximo en tierras bajas y húmedas, en latitudes ecuatoriales.

La Tierra proporciona recursos que son explotados por los seres humanos con diversos fines. Algunos de estos son recursos no renovables, tales como los combustibles fósiles, que son difícilmente renovables a corto plazo.

De la corteza terrestre se obtienen grandes depósitos de combustibles fósiles, consistentes en carbón, petróleo, gas natural y clatratos de metano. Estos depósitos son utilizados por los seres humanos para la producción de energía, y también como materia prima para la producción de sustancias químicas. Los cuerpos minerales también se han formado en la corteza terrestre a través de distintos procesos de mineralogénesis, como consecuencia de la erosión y de los procesos implicados en la tectónica de placas. Estos cuerpos albergan fuentes concentradas de varios metales y otros elementos útiles.

La biosfera de la Tierra produce muchos productos biológicos útiles para los seres humanos, incluyendo (entre muchos otros) alimentos, madera, fármacos, oxígeno, y el reciclaje de muchos residuos orgánicos. El ecosistema terrestre depende de la capa superior del suelo y del agua dulce, y el ecosistema oceánico depende del aporte de nutrientes disueltos desde tierra firme. Los seres humanos también habitan la tierra usando materiales de construcción para construir refugios. Para 1993, el aprovechamiento de la tierra por los humanos era de aproximadamente:

La cantidad de tierras de regadío en 1993 se estimaban en 2481250 km².

Grandes áreas de la superficie de la Tierra están sujetas a condiciones climáticas extremas, tales como ciclones tropicales, huracanes, o tifones que dominan la vida en esas zonas. Muchos lugares están sujetos a terremotos, deslizamientos, tsunamis, erupciones volcánicas, tornados, dolinas, ventiscas, inundaciones, sequías y otros desastres naturales.

Muchas áreas concretas están sujetas a la contaminación causada por el hombre del aire y del agua, a la lluvia ácida, a sustancias tóxicas, a la pérdida de vegetación (sobrepastoreo, deforestación, desertificación), a la pérdida de vida salvaje, la extinción de especies, la degradación del suelo y su agotamiento, a la erosión y a la introducción de especies invasoras.

Según las Naciones Unidas, existe un consenso científico que vincula las actividades humanas con el calentamiento global, debido a las emisiones industriales de dióxido de carbono y el calor residual antropogénico. Se prevé que esto produzca cambios tales como el derretimiento de los glaciares y superficies heladas, temperaturas más extremas, cambios significativos en el clima y un aumento global del nivel del mar.

La cartografía —el estudio y práctica de la elaboración de mapas—, y subsidiariamente la geografía, han sido históricamente las disciplinas dedicadas a describir la Tierra. La topografía o determinación de lugares y distancias, y en menor medida la navegación, o determinación de la posición y de la dirección, se han desarrollado junto con la cartografía y la geografía, suministrando y cuantificando la información necesaria.

La Tierra tiene aproximadamente 7000000000 de habitantes al mes de octubre de 2011. Las proyecciones indicaban que la población humana mundial llegaría a siete mil millones a principios de 2012, pero esta cifra fue superada a mediados de octubre de 2011 y se espera llegar a 9200 millones en 2050. Se piensa que la mayor parte de este crecimiento tendrá lugar en los países en vías de desarrollo. La densidad de población varía mucho en las distintas partes del mundo, pero la mayoría de la población vive en Asia. Está previsto que para el año 2020 el 60 % de la población mundial se concentre en áreas urbanas, frente al 40 % en áreas rurales.

Se estima que solo una octava parte de la superficie de la Tierra es apta para su ocupación por los seres humanos; tres cuartas partes está cubierta por océanos, y la mitad de la superficie terrestre es: desierto (14 %), alta montaña (27 %), u otros terrenos menos adecuados. El asentamiento permanente más septentrional del mundo es Alert, en la Isla de Ellesmere en Nunavut, Canadá. (82°28'N). El más meridional es la Base Amundsen-Scott, en la Antártida, casi exactamente en el Polo Sur. (90°S)

Las naciones soberanas independientes reclaman la totalidad de la superficie de tierra del planeta, a excepción de algunas partes de la Antártida y la zona no reclamada de Bir Tawil entre Egipto y Sudán. En el año 2011 existen , incluyendo los 192 . Hay también 59 territorios dependientes, y una serie de , y otras entidades. Históricamente, la Tierra nunca ha tenido un gobierno soberano con autoridad sobre el mundo entero, a pesar de que una serie de estados-nación han intentado dominar el mundo, sin éxito.

Las Naciones Unidas es una organización mundial intergubernamental que se creó con el objetivo de intervenir en las disputas entre las naciones, a fin de evitar los conflictos armados. Sin embargo, no es un gobierno mundial. La ONU sirve principalmente como un foro para la diplomacia y el derecho internacional. Cuando el consenso de sus miembros lo permite, proporciona un mecanismo para la intervención armada.
El primer humano en orbitar la Tierra fue Yuri Gagarin el 12 de abril de 1961. Hasta el 2004, alrededor de 400 personas visitaron el espacio exterior y alcanzado la órbita de la Tierra. De estos, doce han caminado sobre la Luna. En circunstancias normales, los únicos seres humanos en el espacio son los de la Estación Espacial Internacional. La tripulación de la estación, compuesta en la actualidad por seis personas, suele ser reemplazada cada seis meses. Los seres humanos que más se han alejado de la Tierra se distanciaron 400 171 kilómetros, alcanzados en la década de 1970 durante la misión Apolo 13.

La palabra Tierra proviene del latín "Tellus" o "Terra" que era equivalente en griego a "Gea", nombre asignado a una deidad, al igual que los nombres de los demás planetas del Sistema Solar. El símbolo astronómico estándar de la Tierra consiste en una cruz circunscrita por un círculo.

A diferencia de lo sucedido con el resto de los planetas del Sistema Solar, la humanidad no comenzó a ver la Tierra como un objeto en movimiento, en órbita alrededor del Sol, hasta alcanzado el siglo XVI. La Tierra a menudo se ha personificado como una deidad, en particular, una diosa. En muchas culturas la diosa madre también es retratada como una diosa de la fertilidad. En muchas religiones los mitos sobre la creación recuerdan una historia en la que la Tierra es creada por una deidad o deidades sobrenaturales. Varios grupos religiosos, a menudo asociados a las ramas fundamentalistas del protestantismo o el islam, afirman que sus interpretaciones sobre estos mitos de creación, relatados en sus respectivos textos sagrados son la verdad literal, y que deberían ser consideradas junto a los argumentos científicos convencionales de la formación de la Tierra y el desarrollo y origen de la vida, o incluso reemplazarlos. Tales afirmaciones son rechazadas por la comunidad científica y otros grupos religiosos. Un ejemplo destacado es la controversia entre el creacionismo y la teoría de la evolución.

En el pasado hubo varias creencias en una Tierra plana, pero esta creencia fue desplazada por el concepto de una Tierra esférica, debido a la observación y a la circunnavegación. La perspectiva humana acerca de la Tierra ha cambiado tras el comienzo de los vuelos espaciales, y actualmente la biosfera se interpreta desde una perspectiva global integrada. Esto se refleja en el creciente movimiento ecologista, que se preocupa por los efectos que causa la humanidad sobre el planeta.

En muchos países se celebra el 22 de abril el Día de la Tierra, con el objetivo de hacer conciencia de las condiciones ambientales del planeta.



</doc>
<doc id="2733" url="https://es.wikipedia.org/wiki?curid=2733" title="Tragedia">
Tragedia

La tragedia es una forma literaria teatral o dramática del lenguaje solemne cuyos personajes protagónicos son ilustres y se ven enfrentados de manera misteriosa, invencible e inevitable, a causa de un error fatal o condición de carácter (la llamada hamartia) contra un destino fatal ("fatum", hado o sino) o los dioses, generando un conflicto cuyo final es irremediablemente triste: la destrucción del héroe protagonista, quien muere o enloquece.

El término procede de la voz griega "tragoedia" o “canto del macho cabrío” ("τραγῳδία", palabra compuesta de τράγος “carnero” y "ᾠδή" “canción”) y alude a la canción de los griegos atenienses que era entonada procesionalmente en honor del dios Dioniso en sus fiestas Dionisias.

El género se define como una obra dramática de asunto terrible y desenlace funesto en la que intervienen personajes ilustres o heroicos, y emplea un estilo de lenguaje sublime o solemne. Aristóteles, en su "Poética", dejó la primera definición del término:

Las tragedias acaban generalmente en la muerte, el exilio o en la destrucción física, moral y económica del personaje principal, quien se enfrenta a un conflicto insoluble que le obliga a cometer un error fatal o hamartia al intentar "hacer lo correcto" en una situación en la que lo correcto simplemente no puede hacerse. El héroe trágico es sacrificado así a esa fuerza que se le impone, y contra la cual se rebela con orgullo insolente o "hybris".

También existe un tipo de tragedia de sublimación, en las que el personaje principal es mostrado como un héroe que desafía las adversidades con la fuerza de sus virtudes, ganándose de esta manera la admiración del espectador, como es el caso de "Antígona" de Sófocles.

La tragedia nació como tal en Grecia con las obras de Tespis y Frínico, y se consolidó con la tríada de los grandes trágicos del clasicismo griego: Esquilo, Sófocles y Eurípides. Las tragedias clásicas se caracterizan, según Aristóteles, por generar una catarsis en el espectador.

La tradición atribuye a Tespis la primera composición trágica, pero apenas se conservan restos de sus obras. Después, entre otros autores, destacaron e hicieron evolucionar la tragedia, por orden cronológico, Esquilo, Sófocles y Eurípides.

Aristóteles en su "Poética" señala sobre las partes de la tragedia se dividen en prólogo, episodio, éxodo, y la parte del coro que se divide a la vez en párodo y estásimo. El prólogo precede al párodo del coro. Después vienen siete episodios entrelazados por cada estásimo para concluir con el éxodo, intervención del coro que no es cantado. En cuanto al estásimo, es un canto de coro sin anapesto ni troqueo.


La primera tragedia latina la compuso Livio Andrónico y se representó en la vieja Roma en el año 514 de su fundación (240 a. C.) en tiempo del consulado de Cayo Claudio Centón y M. Sempronio, unos ciento sesenta años después de la muerte de Sófocles y Eurípides y doscientos veinte años antes de la de Virgilio. Trasidas de Ennio, Pacuvio y Accio, Séneca compuso ya en la Edad de plata de la literatura latina once que se han conservado e influyeron poderosamente el teatro en lengua vulgar del Renacimiento y el Barroco; destaca en especial la inspirada en la tragedia homónima de Sófocles, "Fedra".

La tragedia reapareció hasta la época del Renacimiento y aún por traducciones o imitaciones de la antigüedad. Cierto que se encuentran algunos ensayos en lengua vulgar, sobre todo, en Grecia, desde los siglo XIII al XVI pero es indudable que la primera tragedia regular es "Sofonisba", compuesta por Gian Giorgio Trissino y representada en Roma en 1515. En 1552, el poeta Jodelle, el primero en Francia, hizo representar la tragedia de su invención "Cleopatra cautiva". Robert Garnier (1544-1590), Alexandre Hardy y Jean Mairet siguieron su ejemplo hasta que en 1635 apareció Corneille, con su primera tragedia, "Medea", siguiéndole después Racine que elevó a la perfección el restaurado género. Entre los autores modernos que más se han distinguido en la tragedia hay que citar:




</doc>
<doc id="2735" url="https://es.wikipedia.org/wiki?curid=2735" title="Trigonometría">
Trigonometría

La trigonometría es una rama de la matemática, cuyo significado etimológico es 'la medición de los triángulos'. Deriva de los términos griegos τριγωνοϛ "trigōnos" 'triángulo' y μετρον "metron" 'medida'.

En términos generales, la trigonometría es el estudio de las razones trigonométricas: seno, coseno, tangente, cotangente, secante y cosecante. Interviene directa o indirectamente en las demás ramas de la matemática y se aplica en todos aquellos ámbitos donde se requieren medidas de precisión. La trigonometría se aplica a otras ramas de la geometría, como es el caso del estudio de las esferas en la geometría del espacio.

Posee numerosas aplicaciones, entre las que se encuentran: las técnicas de triangulación, por ejemplo, son usadas en astronomía para medir distancias a estrellas próximas, en la medición de distancias entre puntos geográficos, y en sistemas globales de navegación por satélites.

Los antiguos egipcios y los babilonios conocían ya los teoremas sobre las proporciones de los lados de los triángulos semejantes. Pero las sociedades prehelénicas carecían de la noción de una medida del ángulo y por lo tanto, los lados de los triángulos se estudiaron en su medida, un campo que se podría llamar trilaterometría.

Los astrónomos babilonios llevaron registros detallados sobre la salida y puesta de las estrellas, el movimiento de los planetas y los eclipses solares y lunares, todo lo cual requiere la familiaridad con la distancia angular medida sobre la esfera celeste. Sobre la base de la interpretación de una tablilla cuneiforme Plimpton 322 (c. 1900 a. C.), algunos incluso han afirmado que los antiguos babilonios tenían una tabla de secantes. Hoy, sin embargo, hay un gran debate acerca de si se trata de una tabla de ternas pitagóricas, una tabla de soluciones de ecuaciones de segundo grado, o una tabla trigonométrica.
Los egipcios, en el segundo milenio antes de Cristo, utilizaban una forma primitiva de la trigonometría, para la construcción de las pirámides. El "Papiro de Ahmes", escrito por el escriba egipcio Ahmes (c. 1680-1620 a. C.), contiene el siguiente problema relacionado con la trigonometría:

La solución al problema es la relación entre la mitad del lado de la base de la pirámide y su altura. En otras palabras, la medida que se encuentra para la "seked" es la cotangente del ángulo que forman la base de la pirámide y su respectiva cara.

En la medición de ángulos y, por tanto, en trigonometría, se emplean tres unidades, si bien la más utilizada en la vida cotidiana es el grado sexagesimal, en matemáticas es el radián la más utilizada, y se define como la unidad natural para medir ángulos, el grado centesimal se desarrolló como la unidad más próxima al sistema decimal, se usa en topografía, arquitectura o en construcción.

La trigonometría es una rama importante de las matemáticas dedicada al estudio de la relación entre los lados y ángulos de un triángulo rectángulo y una circunferencia. Con este propósito se definieron una serie de funciones, las que han sobrepasado su fin original para convertirse en elementos matemáticos estudiados en sí mismos y con aplicaciones en los campos más diversos.

El triángulo ABC es un triángulo rectángulo en C; lo usaremos para definir las razones seno, coseno y tangente, del ángulo formula_1, correspondiente al vértice A, situado en el centro de la circunferencia.




En el esquema su representación geométrica es:


En el esquema su representación geométrica es:


En el esquema su representación geométrica es:

Normalmente se emplean las relaciones trigonométricas seno, coseno y tangente, y salvo que haya un interés específico en hablar de ellos o las expresiones matemáticas se simplifiquen mucho, los términos cosecante, secante y cotangente no suelen utilizarse

Además de las funciones anteriores, existen otras funciones trigonométricas. Matemáticamente se pueden definir empleando las ya vistas. Su uso no es muy corriente, pero sí se emplean, dado su sentido geométrico. Veamos:

El seno cardinal o función sinc (x) definida:

El verseno, es la distancia que hay entre la cuerda y el arco en una circunferencia, también se denomina sagita o flecha, se define:

El semiverseno, se utiliza en navegación al intervenir en el cálculo esférico:

El coverseno, 

El semicoverseno

La exsecante:

En trigonometría, cuando el ángulo se expresa en radianes (dado que un radián es el arco de circunferencia de longitud igual al radio), suele denominarse arco a cualquier cantidad expresada en radianes; por eso las funciones recíproca se denominan con el prefijo arco, cada razón trigonométrica posee su propia función recíproca:

y es igual al seno de x, la función recíproca:

x es el arco cuyo seno vale y, o también x es el arcoseno de y.

si:
y es igual al coseno de x, la función recíproca:

x es el arco cuyo coseno vale y, que se dice: x es el arcocoseno de y.

si:
y es igual al tangente de x, la función recíproca:

x es el arco cuya tangente vale y, o x es igual al arcotangente de y.

NOTA:
Es común, que las funciones recíprocas sean escritas de esta manera:

pero se debe tener cuidado de no confundirlas con:

Si aplicamos el criterio para obtener las funciones recíprocas en el sentido estricto, definiendo el arcoseno como la recíproca del seno, el arcocoseno como la recíproca del coseno y el arco tangente como la recíproca de la tangente, lo obtenido es la gráfica de la derecha. Es fácil percatarse que estas representaciones no cumplen la unicidad de la imagen, que forma parte de la definición de función, eso es para un valor de x dado existen un número infinito de valores que son su función, por ejemplo: el arcoseno de 0 es 0, pero también lo son cualquier múltiplo entero de formula_24.

Para cualquier n número entero.

Dado que la recíproca de una función no tiene que cumplir necesariamente la unicidad de imagen, solo las funciones inyectivas y biyectivas dan funciones recíprocas con esta propiedad, esta situación se repite para el resto de las funciones recíprocas trigonométricas.

A fin de garantizar el cumplimiento de la definición de función, en cuanto a la unicidad de imagen, y que por tanto las funciones trigonométricas recíprocas cumplan los criterios de la definición de función, se suele restringir tanto el dominio como el codominio, esta corrección permite un análisis correcto de la función, a pesar de que no coincida exactamente con la reciproca de la función trigonométrica original. Así tenemos que:

La función arcoseno se define:

La función arcocoseno se define:

La función arcotangente se define:

Esta restricción garantiza el cumplimiento de la definición de función, en cuanto a la existencia y unicidad de la imagen, si bien tiene inconvenientes como el no poder comparar el arcoseno y el arcocoseno al estar definidos en codominios diferentes, o el de presentar discontinuidades inexistentes, tanto si se emplean las funciones trigonométricas reciprocas en su forma directa como corregida se ha de ser consciente de ello, y comprender las ventajas e inconvenientes que esto supone.

Del mismo modo que las funciones trigonométricas directas recíprocas, cuando el ángulo se expresa en radianes, se denomina arco a ese ángulo, y se emplea el prefijo arco para la función trigonométrica recíproca, así tenemos que:

y es igual a la cosecante de x, la función recíproca:

x es el arco cuya cosecante vale y, o también x es la arcocosecante de y.

si:
y es igual al secante de x, la función recíproca:

x es el arco cuya secante vale y, que se dice: x es el arcosecante de y.

si:
y es igual al cotangente de x, la función recíproca:

x es el arco cuya cotangente vale y, o x es igual al arcocotangente de y.

Al igual que en las funciones directas, si aplicamos el criterio para obtener las funciones recíprocas, dado que las funciones trigonométricas inversas no son inyectivas, lo obtenido es la gráfica de la derecha, que no cumplen la unicidad de la imagen, que forma parte de la definición de función.

Para que se cumpla la definición de función, definimos un dominio y un codominio restringidos. Así tenemos que:

La función arcocosecante se define:

La función arcosecante se define:

La función arcocotangente se define:

Esta restricción garantiza el cumplimiento de la definición de función.

A continuación algunos valores de las funciones que es conveniente recordar:

Para el cálculo del valor de las funciones trigonométricas se confeccionaron . La primera de estas tablas fue desarrollada por Johann Müller Regiomontano en 1467, que nos permiten, conocido un ángulo, calcular los valores de sus funciones trigonométricas. En la actualidad dado el desarrollo de la informática, en prácticamente todos los lenguajes de programación existen bibliotecas de funciones que realizan estos cálculos, incorporadas incluso en calculadoras electrónicas de bolsillo, por lo que el empleo actual de las tablas resulta obsoleto.

Dados los ejes de coordenadas cartesianas xy, de centro O, y una circunferencia goniométrica (circunferencia de radio la unidad) con centro en O; el punto de corte de la circunferencia con el lado positivo de las x, lo señalamos como punto E.

Nótese que el punto A es el vértice del triángulo, y O es el centro de coordenada del sistema de referencia:

a todos los efectos.

La recta r, que pasa por O y forma un ángulo formula_1 sobre el eje de las x, corta a la circunferencia en el punto B, la vertical que pasa por B, corta al eje x en C, la vertical que pasa por E corta a la recta r en el punto D.

Por semejanza de triángulos:

Los puntos E y B están en la circunferencia de centro O, por eso la distancia formula_41 y formula_42 son el radio de la circunferencia, en este caso al ser una circunferencia de radio = 1, y dadas las definiciones de las funciones trigonométricas:

tenemos:

La tangente es la relación del seno entre el coseno, según la definición ya expuesta.

Para ver la evolución de las funciones trigonométricas según aumenta el ángulo, daremos una vuelta completa a la circunferencia, viéndolo por cuadrantes.Como consecuencia de esta consideración, los segmentos correspondientes a cada función trigonométrica variarán de longitud, siendo esta variación función del ángulo, partiendo en el primer cuadrante de un ángulo cero.

Partiendo de esta representación geométrica de las funciones trigonométricas, podemos ver las variaciones de las funciones a medida que aumenta el ángulo formula_45.

Para formula_46, tenemos que B, D, y C coinciden en E, por tanto:

Si aumentamos progresivamente el valor de formula_1, las distancias formula_49 y formula_50 aumentarán progresivamente, mientras que formula_51 disminuirá.

Vale recordar que el punto B pertenece a la circunferencia y cuando el ángulo aumenta se desplaza sobre ella.

El punto E es la intersección de la circunferencia con el eje x y no varia de posición.

Los segmentos: formula_51 y formula_49 están limitados por la circunferencia y por tanto su máximo valor absoluto será 1, pero formula_50 no está limitado, dado que D es el punto de corte de la recta r que pasa por O, y la vertical que pasa por E, en el momento en el que el ángulo formula_55 rad, la recta r será la vertical que pasa por O. Dos rectas verticales no se cortan, o lo que es lo mismo la distancia formula_50 será infinita.

El punto C coincide con A y el coseno vale cero. El punto B esta en el eje y en el punto más alto de la circunferencia y el seno toma su mayor valor: uno.

Para un ángulo recto las funciones toman los valores:

Cuando el ángulo formula_1 supera el ángulo recto, el valor del seno empieza a disminuir según el segmento formula_49, el coseno aumenta según el segmento formula_51, pero en el sentido negativo de las x, el valor del coseno toma sentido negativo, si bien su valor absoluto aumenta cuando el ángulo sigue creciendo.

La tangente para un ángulo formula_1 inferior a formula_62 rad se hace infinita en el sentido positivo de las y, para el ángulo recto la recta vertical r que pasa por O y la vertical que pasa por E no se cortan, por lo tanto la tangente no toma ningún valor real, cuando el ángulo supera los formula_62 rad y pasa al segundo cuadrante la prolongación de r corta a la vertical que pasa por E en el punto D real, en el lado negativo de las y, la tangente formula_50 por tanto toma valor negativo en el sentido de las y, y su valor absoluto disminuye a medida que el ángulo formula_1 aumenta progresivamente hasta los formula_66 rad.

Resumiendo: en el segundo cuadrante el seno de formula_1, formula_49, disminuye progresivamente su valor desde 1, que toma para formula_69 rad, hasta que valga 0, para formula_70 rad, el coseno,formula_51, toma valor negativo y su valor varia desde 0 para formula_69 rad, hasta –1, para formula_73 rad.

La tangente conserva la relación:

incluyendo el signo de estos valores.

Para un ángulo llano tenemos que el punto D esta en E, y B y C coinciden en el eje de las x en el lado opuesto de E, con lo que tenemos:

En el tercer cuadrante, comprendido entre los valores del ángulo formula_70 rad a formula_77 rad, se produce un cambio de los valores del seno, el coseno y la tangente, desde los que toman para formula_78 rad:

Cuando el ángulo formula_1 aumenta progresivamente, el seno aumenta en valor absoluto en el sentido negativo de las y, el coseno disminuye en valor absoluto en el lado negativo de las x, y la tangente aumenta del mismo modo que lo hacía en el primer cuadrante.

A medida que el ángulo crece el punto C se acerca a O, y el segmento formula_51, el coseno, se hace más pequeño en el lado negativo de las x.

El punto B, intersección de la circunferencia y la vertical que pasa por C, se aleja del eje de las x, en el sentido negativo de las y, el seno, formula_49.

Y el punto D, intersección de la prolongación de la recta r y la vertical que pasa por E, se aleja del eje las x en el sentido positivo de las y, con lo que la tangente, formula_50, aumenta igual que en el primer cuadrante

Cuando el ángulo formula_1 alcance formula_85 rad, el punto C coincide con O y el coseno valdrá cero, el segmento formula_49 será igual al radio de la circunferencia, en el lado negativo de las y, y el seno valdrá –1, la recta r del ángulo y la vertical que pasa por E serán paralelas y la tangente tomara valor infinito por el lado positivo de las y.

El seno el coseno y la tangente siguen conservando la misma relación:

que se cumple tanto en valor como en signo, nótese que a medida que el coseno se acerca a valores cercanos a cero, la tangente tiende a infinito.

En el cuarto cuadrante, que comprende los valores del ángulo formula_1 entre formula_85 rad y formula_89 rad, las variables trigonométricas varían desde los valores que toman para formula_85 rad:

hasta los que toman para formula_92 rad pasando al primer cuadrante, completando una rotación:

como puede verse a medida que el ángulo formula_1 aumenta, aumenta el coseno formula_51 en el lado positivo de las x, el seno formula_49 disminuye en el lado negativo de las y, y la tangente formula_50 también disminuye en el lado negativo de las y.

Cuando formula_1, vale formula_92 ó formula_100 al completar una rotación completa los puntos B, C y D, coinciden en E, haciendo que el seno y la tangente valga cero, y el coseno uno, del mismo modo que al comenzarse el primer cuadrante.

Dado el carácter rotativo de las funciones trigonométricas, se puede afirmar en todos los casos:

Que cualquier función trigonométrica toma el mismo valor si se incrementa el ángulo un número n entero de rotaciones completas.

Partiendo de una circunferencia de radio uno, dividida en cuatro cuadrantes, por dos rectas perpendiculares, que se cortan en el centro de la circunferencia O, estas rectas cortan a la circunferencia en los puntos A, B, C y D, la recta horizonte AC también la podemos llamar eje x y la recta vertical BD eje y. Dada una recta r, que pasa por el centro de la circunferencia y forma un ángulo α con OA, eje x, y corta a la circunferencia en F, tenemos que la vertical que pasa por F corta al eje x en E, la vertical que pasa por A corta a la recta r en G. Con todo esto definimos, como ya se vio anteriormente, las funciones trigonométricas:

para el seno:

dado que:

Para el coseno:

dado que:

Para la tangente:

dado que:

partiendo de estas definiciones, podemos ver algunos caso importantes:

Si a partir del eje vertical OB trazamos la recta r a un ángulo α en el sentido horario, la recta r forma con el eje x un ángulo 90-α. Así, el valor de las funciones trigonométricas de este ángulo,conocidas las de α,serán:

El triángulo OEF,rectángulo en E, siendo el ángulo en F α, por lo tanto:

en el mismo triángulo OEF, tenemos que:

viendo el triángulo OAG, rectángulo en A, siendo el ángulo en G igual a α, podemos ver:

Si a partir de eje vertical OB trazamos la recta r a un ángulo α, medido en sentido trigonométrico, el ángulo formado por el eje horizontal OA y la recta r será 90+α. La prolongación de la recta r corta a la circunferencia en F y a la vertical que pasa por A en G.

El triángulo OEF es rectángulo en E y su ángulo en F es α, por lo tanto tenemos que:

En el mismo triángulo OEF podemos ver:

En el triángulos OAG rectángulo A y siendo α el ángulo en G, tenemos:

Si sobre el eje horizontal OC, trazamos la recta r a un ángulo α, el ángulo entre el eje OA y la recta r es de 180-α, dado el triángulo OEF rectángulo en E y cuyo ángulo en O es α, tenemos:

en el mismo triángulo OEF:

En el triángulo OAG, rectángulo en A y con ángulo en O igual a α, tenemos:

Sobre la circunferencia de radio uno, a partir del eje OC con un ángulo α trazados la recta r, el ángulo del eje OA y la recta r es de 180+α, como se ve en la figura. En el triángulo OEF rectángulo en E se puede deducir:

en el mismo triángulo OEF tenemos:

en el triángulo OAG, rectángulo en A, vemos que:

Sobre el eje OD y con un ángulo α medido en sentido horario trazamos la recta r. El ángulo entre el eje OA y la recta r es de 270-α. En el triángulo OEF, rectángulo en E, tenemos:

por otra parte en el mismo triángulo OEF, tenemos:

en el triángulo OAG rectángulo en A, y siendo α el ángulo en G, tenemos;

Sobre el eje OD y con un ángulo α medido en sentido trigonométrico, trazamos la recta r. El ángulo entre el eje OA y la recta r es de 270+α. En el triángulo OEF, rectángulo en E, tenemos:

por otra parte en el mismo triángulo OEF, tenemos:

en el triángulo OAG rectángulo en A, y siendo α el ángulo en G, tenemos;

Sobre la circunferencia de radio uno, a partir del eje OA con un ángulo α medido en sentido horario trazados la recta r, el ángulo del eje OA y la recta r es de -α, o lo que es lo mismo 360-α como se ve en la figura. En el triángulo OEF rectángulo en E se puede deducir:

en el mismo triángulo OEF tenemos:

en el triángulo OAG, rectángulo en A, vemos que:

Una identidad es una igualdad en que se cumple para todos los valores permisibles de la variable. En trigonometría existen seis identidades fundamentales:

Como en el triángulo rectángulo cumple la función que:

de la figura anterior se tiene que:

por tanto:

entonces para todo ángulo α, se cumple la identidad Pitagórica: 

que también puede expresarse:

El seno y coseno se definen en matemática compleja, gracias a la fórmula de Euler como:
Por lo tanto, la tangente quedará definida como:
Siendo formula_139.





</doc>
<doc id="2736" url="https://es.wikipedia.org/wiki?curid=2736" title="Terry Pratchett">
Terry Pratchett

Terence David John "Terry" Pratchett, OBE (Beaconsfield, 28 de abril de 1948 – Broad Chalke, 12 de marzo de 2015) fue un escritor británico de fantasía y ciencia ficción. Sus obras más conocidas corresponden a la serie del "Mundodisco" ("Discworld"); además ha escrito novelas juveniles, relatos cortos y ha colaborado en la redacción de guiones para las adaptaciones televisivas de sus novelas.

Con más de 70 millones de libros vendidos en 37 idiomas, es el segundo autor británico de ficción con más ventas después de J.K. Rowling.

Nació en Beaconsfield en Buckinghamshire en 1948. A los 11 años, entró a estudiar en la escuela técnica de High Wycombe, en cuya revista estudiantil publicó su primer relato a los 13 años, titulado "The Hades Business". Dos años más tarde sería publicada para la venta comercial. Orientó sus estudios al periodismo, dejó la escuela en 1965 para trabajar en "Bucks Free Press" y aprobó el curso del "National Council" para la Formación de Periodistas. En ese mismo año escribiría su segundo relato corto, "The Night Dweller".

En 1971 se publica su primer libro llamado "The carpet people" que, aunque poco numerosas, recibió unas buenas críticas. Algunas calificaron la calidad de la obra como extraordinaria. Posteriormente le seguirían "The dark side of the sun" en 1976 y "Strata" en 1981. Dejó el "Bucks Free Press" por el "Western Daily Press" el 28 de septiembre de 1970, y volvió al anterior en 1972 ya como subdirector. Un año más tarde volvería a dejarlo, esta vez por el "Bath Chronicle". En esta época realizó una serie de tiras cómicas por las que no pasará a la historia.

En 1981 pasó a ocupar el cargo de responsable de relaciones públicas en una central nuclear (la empresa para la que trabajaba, actualmente "PowerGen", tenía responsabilidad sobre tres centrales nucleares), justo antes del desastre de Three Mile Island. Precisamente este era su trabajo cuando escribió su primera novela sobre el Mundodisco: "El color de la magia" (1983). El filón del Mundodisco "(Discworld)", una saga de fantasía ambientada en un mundo hilarante que parodia el nuestro es el conjunto de novelas que le ha dado fama internacional.

En 1986 se publicó "La luz fantástica", continuación de la anterior. Con la popularidad del hombre del sombrero aumentando por momentos, en 1987 decidió dedicarse únicamente a escribir. "Ritos Iguales" (1987), "Mort" (1987) y "Rechicero" (1988) serían las siguientes novelas en aparecer. Ese mismo año firmó un nuevo contrato para seis novelas más (no necesariamente de "Mundodisco") y a partir de ese momento no dejó de escribir.

Autor prolífico, Pratchett escribió las siguientes novelas sobre el Mundodisco en tiempo récord: "Brujerías" (1988), "Pirómides" (1989), "¡Guardias! ¿Guardias?" (1989), "Eric" (1990), "Imágenes en acción" (1990), "El segador" (1991), "Brujas de viaje" (1991), "Dioses Menores" (1992), "Lores y damas" (1992), "Hombres de armas" (1993), "Soul Music" (1994), "Tiempos interesantes" (1994), "Mascarada" (1995), "Pies de barro" (1996), "Papá Puerco" (1996), "¡Voto a bríos!" (1997), "El país del fin del mundo" (1998), "Carpe jugulum" (1998), "El quinto elefante" (1999), "La verdad" (2000), "Ladrón del tiempo" (2001), "El último héroe" (2001), "El asombroso Mauricio y sus roedores sabios" (2001), "Ronda de noche" (2002), "Los pequeños hombres libres (The wee free men)" (2003), "Regimiento monstruoso" (2003), "Un sombrero de cielo" (2004), "Cartas en el asunto" (2004), "¡Zas!" (2005), "Wintersmith" (2006), "Dinero a mansalva" (2007), "El Atlético invisible" (2009), "I Shall Wear Midnight" (2010) y "Snuff" (2011). También ha publicado tres volúmenes de "The Science of Discworld" (La ciencia del Mundodisco) en colaboración con Ian Stewart, matemático y popular autor de libros de divulgación, y Jack Cohen, biólogo y colaborador de escritores en la creación de alienígenas plausibles.

Mención especial merece la trilogía del Éxodo de los Gnomos, no relacionada con el Mundodisco, cuyos títulos son "Camioneros" (1989), "Cavadores" (1989) y "La nave" (1990).

En 1998 fue nombrado Oficial de la Orden del Imperio Británico en el cumpleaños de la Reina como reconocimiento a los servicios prestados a la literatura (aunque en principio pensó que se trataba de una broma), al año siguiente recibió el título de Doctor Honoris Causa en Literatura por la Universidad de Warwick y en 2001 el mismo título honorario por la Universidad de Portsmouth. Como curiosidad se puede comentar que en la ceremonia de investidura de la Universidad de Warwick él mismo replicó haciendo Doctores de la Universidad Invisible a Ian Stewart y Jack Cohen, quienes escribieron conjuntamente la novela "The Science of Discworld". En los fastos de Año Nuevo de 2009 fue asimismo armado Caballero.

Terry Pratchett anunció el 11 de diciembre de 2007 que padecía un mal de Alzheimer prematuro. Sin embargo, anunció que plantaría cara a la enfermedad porque pensaba que "aún hay tiempo para escribir al menos unos libros más".

El escritor incidió en su anuncio en que no era simplemente un "no estoy muerto". Aunque utilizó su característica ironía al decir que "por supuesto, estaré muerto en un futuro, como todo el mundo. Para mí, quizá más tarde de lo que piensas". Según su comunicado, este caso de Alzheimer era una versión "muy rara" de la enfermedad. El autor mencionaba un "golpe fantasma" recibido en el pasado que había salido a la superficie. Pratchett dijo tomarse este problema con "filosofía" y un "suave optimismo".

Terry Pratchett falleció el 12 de marzo de 2015 en su casa cerca de Stonehenge, en Broad Chalke (Wiltshire), donde vivía con su mujer Lyn y su hija Rhianna Pratchett, también escritora.


Ordenadas en orden de aparición y lectura (las que no tienen título en castellano no se han traducido todavía al castellano).






Películas
Actores: David Jason, Marc Warren y Michelle Dockery

DURACIÓN189 min.
DIRECTOR : Vadim Jean
GUIÓN : Vadim Jean 
MÚSICA : Paul E. Francis, David A. Hughes
FOTOGRAFÍA : Gavin Finney
REPARTO David Jason, Sean Astin, Tim Curry, Jeremy Irons, Brian Cox, James Cosmo, Christopher Lee, Terry Pratchett

DURACIÓN : 185 min.
DIRECTOR : Jon Jones
GUIÓN : Bev Doyle, Richard Kurti
MÚSICA: John Lunn
FOTOGRAFÍA : Gavin Finney
REPARTO Richard Coyle, David Suchet, Claire Foy, Andrew Sachs, Charles Dance, Timothy West, Terry Pratchett

Animación

Videojuegos

Teatro

Series de TV

Pratchett fue uno de los primeros escritores en comunicarse con sus fans vía internet y es uno de los contribuidores de los grupos de noticias Usenet alt.fan.pratchett y alt.books.pratchett durante más de una década.

El interés de Terry Pratchett por los orangutanes no está sólo reducido al Bibliotecario, un popular personaje de Mundodisco. Ha estado trabajando para la "Orang-ute foundation" visitando Borneo con el canal 4 para rodar un capítulo de "Jungle Crew" en su hábitat. Siguiendo sus pasos, los fans en las convenciones de mundodisco tomaron la "Orang-Ute" como beneficencia. A partir de ese momento, en cada convención a la que acude el autor se hace una subasta donde los fans puedan pujar para que su nombre aparezca en el siguiente libro, todos los fondos recaudados se destinan a la "Orang-Ute".




</doc>
<doc id="2737" url="https://es.wikipedia.org/wiki?curid=2737" title="Taxón">
Taxón

En biología, un taxón o taxon (del griego "τάξις", transliterado como "táxis", «ordenamiento») es un grupo de organismos emparentados, que en una clasificación dada han sido agrupados, asignándole al grupo un nombre en latín, una descripción si es una especie, y un tipo. Cada descripción formal de un taxón es asociada al nombre del autor o autores que la realizan, los cuales se hacen figurar detrás del nombre. En latín el plural de taxón es "taxa", y es como suele usarse en inglés, pero en español el plural adecuado es «taxones» o «táxones». La disciplina que define a los taxones se llama taxonomía.

La finalidad de clasificar los organismos en taxones formalmente definidos en lugar de grupos informales, es la de proveer grupos cuya circunscripción (esto es, de qué organismos están compuestos) sea estricta y cuya denominación tenga valor universal, independientemente de la lengua utilizada para la comunicación. Nótese que los taxones existen dentro de una clasificación dada, sujeta a cambios y sobre la que pueden presentarse discrepancias; lo que obliga, respecto a ciertas denominaciones problemáticas, a especificar en el sentido de qué autor se está usando el nombre.

La definición aquí dada corresponde a la definición de taxón de la escuela cladista y la evolucionista, al que se le asignó un "nombre" según las reglas escritas en los Códigos Internacionales de Nomenclatura. Cada escuela de clasificación agrupa a los organismos de forma diferente, en la escuela fenética los taxones no son grupos de parentenesco sino de organismos con rasgos similares. En la escuela cladista sólo son taxones los grupos monofiléticos (que agrupan un ancestro más todos sus descendientes), en la escuela evolucionista también pueden ser taxones los grupos parafiléticos. Se puede decir que en general, un taxón es un grupo de organismos asociado a un conjunto de atributos que determinan la pertenencia de esos organismos a ese grupo. En la escuela cladista, el conjunto de atributos son los caracteres heredados de su antecesor común.

En el sistema linneano de clasificación, cada taxón tiene asociada además una categoría taxonómica, que lo ubicaría jerárquicamente en un sistema de clasificación.

Los Códigos Internacionales de Nomenclatura proveen reglas con el objetivo de que los taxones tengan un y sólo un "nombre correcto", el nombre que debería ser utilizado para referirse al taxón. Para ello el autor debe publicar el nombre del taxón en una revista científica, que debe estar en latín, asociado a una categoría taxonómica, un "tipo", y una descripción si es una especie. El primer nombre publicado en regla es el "nombre correcto" del taxón. El nombre se aplica a un taxón que tenga en su circunscripción al tipo. En los Códigos, el taxón se define por su circunscripción, su posición (sus relaciones con otros taxones en un sistema de clasificación), y su categoría taxonómica.

Para clasificar los organismos, la taxonomía utiliza desde Carlos Linneo un sistema jerárquico. En este esquema organizativo, cada grupo de organismos en particular es un taxón, y el nivel jerárquico en el que se lo sitúa es su categoría. Análogamente, en geografía política: "país", "provincia" y "municipio" serían categorías, mientras que "Canadá", "Ontario" y "Toronto" serían los taxones. Del mismo modo: "familia", "género" y "especie" son categorías taxonómicas, mientras que Rosaceae, "Rosa" y "Rosa canina" son ejemplo de taxones de esas categorías.

En relación al taxón "especie", Ernst Mayr (1996) hace distinción entre "el taxón" y "la categoría taxonómica": 

La siguiente es una lista general (ordenada de lo general a particular) de categorías taxonómicas a las que se asocian los diversos taxones: 

Hay que notar, que la asociación de un taxón a un rango determinado (categoría), es algo relativo y restringido al esquema particular usado (sistema). Tanto es así, que es probable que un taxón ocupe categorías diferentes según los sistemas de clasificación (organizados por diferentes autores, criterios, etc.); generalmente, ello ocurre en el ámbito de las categorías más abarcativas (familia, orden, clase, etc.).

El estatus ontológico de los taxones es uno de los asuntos más discutidos en filosofía de la biología. Principalmente existen dos corrientes:


Existen dos tipos de taxones:


Se llama taxón monotípico al que sólo contiene un miembro de la categoría inmediatamente subordinada. Por ejemplo, una familia que sólo contiene un género, sin que importe cuántas especies contenga éste.

La nomenclatura establece una terminología consensuada que permite saber, a partir del sufijo de un taxón cualquiera, cuál es su categoría taxonómica y dar cuenta de su posición en la jerarquía sistemática. La siguiente tabla muestra esa nomenclatura:

Por debajo de la categoría de género, todos los nombres de taxones son llamados «combinaciones». La mayoría reciben también una terminación latina más o menos codificada en función de la disciplina. Se distinguen varias categorías de combinaciones:





</doc>
<doc id="2739" url="https://es.wikipedia.org/wiki?curid=2739" title="Theales">
Theales

Las Theales son un Orden en el Sistema Cronquist, más o menos primitivo, de las Dilénidas (subclase Dilleniidae), a veces perianto y gineceo helicoidal; sistemática compleja, con un número de familias variable (las pasan a Violales). Gineceos poco evolucionados, de carpelos más o menos soldados en general cerrados, a gineceo plurilocular con placentación axial. Semillas con embrión grande y poco endosperma. Leñosas y con hojas simples.

En el sistema APG II (usado ampliamente aquí) los taxones involucrados se asignan a otros muchos órdenes diferentes: Ericales, Malvales, Malpighiales.



</doc>
<doc id="2740" url="https://es.wikipedia.org/wiki?curid=2740" title="Theaceae">
Theaceae

Theaceae, las Teáceas, son una familia de plantas con flores perteneciente al orden Ericales. Es originaria de países subtropicales e intertropicales de América y Asia.
Son arbustos o árboles, generalmente siempreverdes; plantas hermafroditas, dioicas o ginodioicas. Hojas simples, coriáceas, alternas y perennes (hoja lauroide). Flores solitarias, actinomorfas, con dos envueltas (a veces helicoidales); cáliz con 5 - 7 sépalos libres; corola con al menos 5 pétalos carnosos (con tendencia a la soldadura); androceo con numerosos estambres helicoidalmente dispuestos (en "Camellia", androceo espirulado), a veces soldados a la corola; ginx con 2 - 5 carpelos cerrados; fruto general en cápsula (a veces drupáceo). 
Se divide en las siguientes tribus:


</doc>
<doc id="2741" url="https://es.wikipedia.org/wiki?curid=2741" title="Tamaricaceae">
Tamaricaceae

Tamaricaceae es una pequeña familia de plantas leñosas, arbustos o pequeños árbolillos del orden Caryophyllales. Consta de cinco géneros con alrededor de 100 - 120 especies, la mayoría de las regiones templadas y cálidas del viejo mundo; en terrenos algo húmedos y salinos.

Hojas simples y enteras, muy reducidas, imbricadas, escuamiformes o aciculares, alternas. Flores hermafroditas, actinomorfas, tetrámeras (Tamarix boveana) o pentámeras, hipoginas; reunidas en inflorescencias en espigas o en racimos, a veces aisladas. La corola tiene de 4 a 5 (6) pétalos libres, alternando con sépalos. Los estambres, a veces numerosos, se insertan en un disco carnoso necatarífero. El ovario es súpero. Los frutos son cápsulas; con dispersión anemócora. Semillas con numerosos pelos largos (vilano). 

 5 + 5


</doc>
<doc id="2742" url="https://es.wikipedia.org/wiki?curid=2742" title="Tilioideae">
Tilioideae

Tilioideae es una subfamilia de fanerógamas perteneciente a la familia Malvaceae.

Las tiliáceas son árboles y arbustos, raramente hierbas. Hojas alternas, simples, siempre cordadas, ordinariamente con estípulas caducas. Flores inconspicuas; hermafroditas; actinomorfas; tetrámeras o pentámeras; dialipétalas; numerosos estambres, a menudo fasciculados (poliadelfos); gineceo sincárpico, ovario súpero, con 2 - 10 carpelos cerrados. Inflorescencias en dicasio de dicasios, con metatopía, en algunos casos provistas de una gran bráctea membranosa. Frutos capsulares o nuciformes. Unas 400 especies, algunas de zonas templadas, pero la mayoría tropicales.


</doc>
<doc id="2746" url="https://es.wikipedia.org/wiki?curid=2746" title="Teligonáceas">
Teligonáceas

En el sistema de Cronquist de clasificación científica de los vegetales, las teligonáceas eran una familia monotípica de hierbas, conteniendo el género "Theligonum". Se caracterizaban por poseer hojas simples, opuestas o alternas. Flores unisexuales, de disposición monoica, monoclamídeas; las masculinas con 7-20 estambres y las femeninas con ovario ínfero. Fruto drupáceo. En los bordes de lagunas.

En la clasificación filogenética más reciente, se les considera parte de las rubiáceas.


</doc>
<doc id="2747" url="https://es.wikipedia.org/wiki?curid=2747" title="Trapa">
Trapa

Trapa es el único género de la familia monotípica de las trapáceas, en el orden de las mirtales, que comprende cinco especies de hierbas acuáticas. Poseen hojas sumergidas lineares y flotantes arrosetadas, de peciolo hinchado, rómbicas y dentadas. Flores poco vistosas, flotantes, hermafrodita, regulares, tetrámeras y de ovario semiínfero. Frutos nuciformes, coriaceos, con 4 cuernos. Son nativas de las regiones templadas de Asia y África.
"Trapa bicornis"
"Trapa korshinskyi"
"Trapa litwinowii"
"Trapa maximowiczii"
"Trapa natans"


</doc>
<doc id="2749" url="https://es.wikipedia.org/wiki?curid=2749" title="Thymelaeaceae">
Thymelaeaceae

Las timeleáceas (Thymelaeaceae) son una cosmopolita familia del orden Malvales compuesta por 50 géneros y 898 especies.
Las especies incluyen principalmente árboles y arbustos, con algunas trepadoras y herbáceas.

Se caracterizan por hojas simples, enteras, alternas o raramente opuestas, sin estípulas. Flores generalmente hermafroditas, regulares, a menudo tetrámeras, con perianto doble, o a veces con los pétalos abortados; corola (junto con el hipanto) tubular o urceolado, levemente corolino, sin solución de continuidad con el cáliz; corola inexistente o reducida; androceo diplostémono con 4-8 estambres sobre los sétalos (y alternos); gineceo súpero, con 1-2 carpelos (que dan lugar a 1-2 lóculos); suelen estar agrupadas en inflorescencias pequeñas. Frutos nuciformes o drupáceos.

La familia está más diversificada en el hemisferio sur que en el norte con las mayores concentraciones de especies en África y Australia.Los géneros son predominantemente africanos.

Varios de los géneros tienen importancia económica. "Gonystylus" se cotiza por su dura y blanca madera. La corteza de "Edgeworthia" y "Wikstroemia" se utiliza como componente del papel.

"Daphne" se cultiva por el dulce aroma de sus flores. Las especies de "Wikstroemia", "Daphne", "Phaleria", "Dais", "Pimelea" y otros géneros se cultivan como ornamentales.

Muchas de las especies son venenosas si se comen.



</doc>
<doc id="2751" url="https://es.wikipedia.org/wiki?curid=2751" title="TVR">
TVR

TVR se puede referir a:


</doc>
<doc id="2752" url="https://es.wikipedia.org/wiki?curid=2752" title="Transporte">
Transporte

El transporte es una actividad del sector terciario, entendida como el desplazamiento de objetos, animales o personas de un lugar (punto de origen) a otro (punto de destino) en un vehículo (medio o sistema de transporte) que utiliza una determinada infraestructura (red de transporte). Esta ha sido una de las actividades terciarias que mayor expansión ha experimentado a lo largo de los últimos dos siglos, debido a la industrialización; al aumento del comercio y de los desplazamientos humanos tanto a escala nacional como internacional; y los avances técnicos que se han producido y que han repercutido en una mayor rapidez, capacidad, seguridad y menor coste de los transportes.

Para lograr llevar a cabo la acción de transporte se requieren varios elementos, que interactuando entre sí, permiten que se lleve a cabo:


El sistema de transporte requiere de varios elementos, que interactúan entre sí, para la práctica del transporte y sus beneficios:


Los ingenieros de transporte utilizan estos conceptos a la hora de concebir, planificar, diseñar y operar un sistema de transporte al tener que pasa para un sistema eficiente, es deseable que la demanda utilice al máximo la infraestructura existente. La demanda deberá solo en muy pocas ocasiones superar la oferta.

Uno de los ejemplos más ilustrativos es el de las vías. La oferta para este caso son las vías y los vehículos las demandan. Cuando pocos vehículos demandan la vía, se dice que la infraestructura está prestando un buen servicio, pero es ineficiente. Cuando muchos vehículos utilizan la vía de forma funcional, operarán de forma eficiente la infraestructura, pero el servicio que presta a los usuarios ya no es tan bueno. Cuando demasiados vehículos demandan las vías se forma congestión y esto se considera inaceptable.

El transporte puede ser clasificado de varias maneras de forma simultánea. Por ejemplo, referente al tipo de viaje, al tipo de elemento transportado o al acceso. Por ejemplo, el transporte de pasajeros generalmente se clasifica en transporte público y el transporte privado.

El transporte de carga es la disciplina que estudia la mejor forma de llevar de un lugar a otro bienes. Asociado al transporte de carga se tiene la Logística que consiste en colocar los productos de importancia en el momento preciso y en el destino deseado.
La diferencia más grande del transporte de pasajeros es que para éste se cuentan el tiempo de viaje y el confort.

Esta clasificación es muy importante por las diferencias que implican los dos tipos de viajes. Mientras los viajes urbanos son cortos, muy frecuentes y recurrentes, los viajes interurbanos son largos, menos frecuentes y recurrentes.

Se denomina transporte público a aquel en el que los viajeros comparten el medio de transporte y que está disponible para el público en general. Incluye diversos medios como autobuses, trolebuses, tranvías, trenes, ferrocarriles suburbanos o ferris. En el transporte interregional también coexiste el transporte aéreo y el tren de alta velocidad.

El transporte público se diferencia del transporte privado básicamente en que:

El más representativo de los modos de transporte privado es el automóvil. Sin embargo, la caminata y la bicicleta también están dentro de esta clasificación. El taxi, pese a ser un servicio de acceso abierto al público, es clasificado como transporte privado.

El transporte escolar o transporte de estudiantes lleva a cabo viajes de niños y adolescentes desde los lugares de residencia hasta los colegios y vice versa. Pese a que muchos de estos viajes se llevan a cabo en medios de transporte privado, es también muy frecuente que se lleven a cabo de forma colectiva en buses y caravanas especiales para este propósito.

En Estados Unidos y otros países es habitual que se dediquen autobuses para llevar a los escolares de su lugar de residencia a la escuela. La normativa de Estados Unidos obliga a que un cuidador adulto, aparte del conductor, vaya en el autobús y que los autobuses no tengan más de 16 años.

En otros países como Alemania o Finlandia, los alumnos van solos en los vehículos del transporte público de la ciudad. Normalmente, los estudiantes reciben una tarjeta que les permite hacer uso de estos servicios por todo el semestre de forma ilimitada, a un costo muy bajo (subsidiado).

En el caso de un país Sudamericano, Chile posee una tarifa diferida para estudiantes a la que se puede optar presentando un Pase escolar al momento de cancelar el pasaje. Este a su vez permite optar a diferentes tarifas, siendo Educación primaria pasaje sin costo y Educación secundaria además de Educación superior un porcentaje del pasaje adulto (50 % en regiones y 33 % en Santiago). En Davis, Estados Unidos, más del 40 % de los niños y niñas van al colegio en bicicleta.

Los modos de transporte son combinaciones de redes, vehículos y operaciones. Incluyen el caminar, la bicicleta, el coche, la red de carreteras, los ferrocarriles, el transporte fluvial y marítimo (barcos, canales y puertos), el transporte aéreo (aeroplanos, aeropuertos y control del tráfico aéreo), incluso la unión de varios o los tres tipos de transporte. Se habla de reparto modal para describir, en un ámbito dado, cómo se distribuyen los viajes entre los distintos modos.

Según los modos de transporte utilizados, el transporte se clasifica o categoriza en:

Asimismo, puede distinguirse entre transporte público y transporte privado dependiendo de la propiedad de los medios de transporte utilizados.

También puede ser interesante la distinción entre el transporte de mercancías y el transporte de pasajeros.

La modelización de transporte o modelación de transporte permite planificar situaciones futuras y actuales del transporte urbano. El concepto de “modelo” debe ser entendido como una representación, necesariamente simplificada, de cualquier fenómeno, proceso, institución y, en general, de cualquier “sistema”. Es una herramienta de gran importancia para el planificador, pues permite simular escenarios de actuación y temporales diversos que ayudan a evaluar alternativas y realizar el diagnóstico de futuro.

El esquema clásico de modelación es el de cuatro etapas o cuatro pasos.

A veces, según los datos disponibles y el tipo de análisis que se desea se puede prescindir del modelo de generación, quedando en tres etapas y obteniéndose únicamente el modelo de distribución. En corredores de carreteras sin transporte público realmente competitivo, es frecuente suponer que no hay trasvase modal y sólo se use el de distribución (o un modelo de crecimientos) y el de asignación únicamente.

También se pueden mencionar otros tipos de modelos como los de usos del suelo que permiten análisis interrelacionados y complejos entre actividad en el territorio y transportes.

Las redes se diseñan considerando tres aspectos: la geometría, la resistencia y la capacidad. En la práctica, el diseño de transporte centra sus miras en tomar los diseños geométricos y definir su ancho, número de carriles, vías o diámetro. Su producto es tomado por el especialista en pavimentos, rieles, puentes o ductos y convertido en espesores de calzada, balasto, vigas o paredes de tubería. El ingeniero de transporte es también responsable de definir el funcionamiento del sistema considerando el tiempo.

Los principales métodos para el diseño de redes incluyen el método de las cuatro etapas, el uso de la teoría de colas, la simulación y los métodos que podrían llamarse de coeficientes empíricos.

En este método de modelización de transporte se calcula separadamente la "generación de viajes", o número de personas o cantidad de carga que produce un área; la "distribución de viajes" de viajes, que permite estimar el número de viajes o cantidad de carga entre cada zona de origen y destino; la "partición modal", es decir, el cálculo del número de viajes o cantidad de carga que usarán los diferentes modos de transporte y su conversión en número de vehículos; y, finalmente, la "asignación", o la definición de qué segmentos de la red o rutas utilizará cada uno de los vehículos.

Este proceso se realiza utilizando la densidad y la localización de población o de carga actual para verificar que los volúmenes previstos por el método estén de acuerdo con la realidad. Finalmente, se usan las estimaciones de población futura para recalcular el número de vehículos en cada arco de la red que se usará para el diseño. Se utiliza principalmente para diseñar el transporte y es exigido por ley en muchas zonas urbanas.

Utiliza la estadística y ciertas asunciones sobre el proceso del servicio. Permite estimar, a partir de las tasas de llegada de los clientes (ya sean vehículos o personas) y de la velocidad de atención de cada canal de servicio, la longitud de cola y el tiempo promedio de atención. La tasa de llegada de los clientes debe analizarse para conocer, no solamente su intensidad en número de clientes por hora, sino su distribución en el tiempo. Se ha hallado, experimentalmente, que la distribución de Poisson y las distribuciones geométricas reflejan bien la llegada aleatoria de clientes y la llegada de clientes agrupados, respectivamente. Se utiliza principalmente para la estimación de número de casetas de peaje, surtidores en estaciones de combustible, puestos de atención en puertos y aeropuertos y número de cajeros o líneas de atención al cliente requeridas en un establecimiento. La teoría de colas se basa en procesos estocásticos...

Existen dos tipos principales de simulaciones en computador utilizadas en la ingeniería de transporte: macrosimulaciones y microsimulaciones.

Las macrosimulaciones utilizan ecuaciones que reflejan parámetros generales de la corriente vehicular, como velocidad, densidad y caudal. Muchas de las ideas detrás de estas ecuaciones están tomadas del análisis de flujo de líquidos o gases o de relaciones halladas empíricamente entre estas cantidades y sus derivadas.

Las segundas simulan cada vehículo o persona individualmente y hacen uso de ecuaciones que describen el comportamiento de estos vehículos o personas cuando siguen a otro (ecuaciones de seguimiento vehicular) o cuando circulan sin impedimentos.

Utilizan ecuaciones de tipo teórico pero, en general, parten de mediciones que indican la capacidad de una red en condiciones ideales. Esta capacidad, normalmente, va disminuyendo a medida que la red o circunstancias se alejan de ese ideal.

Los métodos proporcionan coeficientes menores que la unidad, por los que se debe multiplicar la capacidad "ideal" de la red para encontrar la capacidad en las condiciones dadas. y eso más cosas

El transporte y la comunicación son tanto sustitutos como complementos. Aunque el avance de las comunicaciones es importante y permite trasmitir información por telégrafo, teléfono, fax o correo electrónico, el contacto personal tiene características propias que no se pueden sustituir.

El crecimiento del transporte sería imposible sin la comunicación, vital para sistemas de transporte avanzados (control de trenes, control del tráfico aéreo, control del estado del tránsito en carretera, etc.). No existe, sin embargo, relación probada entre el crecimiento de estos dos sistemas. El mejor previsor del crecimiento de un sistema de transporte es el crecimiento del producto interno bruto (PIB) de un área. Resulta, además, relativamente fácil encontrar predicciones del PIB. La utilización de series históricas para predecir el crecimiento futuro del sistema de transporte puede llevar a serios errores (problema de la "suboptimización" o de análisis fragmentario de un sistema).

El transporte y el uso de la tierra están relacionados de manera directa. Dependiendo del uso de la tierra se generan actividades específicas que no necesariamente coinciden con el lugar de residencia de quienes las desarrollan, en cuyo caso se deben trasladar. Una jornada puede ser dividida entre el tiempo gastado en actividades y el tiempo gastado viajando desde y hacia el lugar en el cual se desarrollan tales actividades. Se dice que el transporte es "una demanda indirecta", dado que carece de fin en sí mismo, pero es necesario para desarrollar las actividades en el sitio de destino.

La agrupación de una variedad de actividades dentro de la misma zona terrestre minimiza la necesidad del transporte. Por el contrario, la organización por zonas de actividades exclusivas la aumenta. Sin embargo, hay economías de escala al agrupar actividades, lo que impide una organización de actividades por zonas completamente heterogéneas.

También el transporte y el uso de tierra actúan recíprocamente de otro modo, dado que los servicios de transporte consumen tierra, al igual que las ciudades. Un sistema de transporte eficiente puede minimizar el uso de la tierra. Sin embargo, este ahorro debe ser comparado con el coste; un sistema de transporte eficiente en una ciudad grande puede tener un coste sumamente elevado.

El transporte es un consumidor importante de energía, la puede obtener mediante la quema de combustibles, hasta no hace mucho mayoritariamente fósiles en motores de combustión. En el proceso de combustión se generan emisiones gaseosas contaminantes (CO, CO, NO, SO y otros, como partículas) cuya nocividad depende de la fuente de energía usada.

Suele sostenerse que los vehículos eléctricos impulsados son "limpios", al igual que aquellos que usan celdas de hidrógeno. Pero, en realidad, depende de la fuente de la que provenga la electricidad. Si usan electricidad producida en centrales alimentadas por combustibles fósiles, la contaminación es más localizada que con los coches de combustión, ya que pueden aplicarse técnicas de captura y almacenamiento de carbono. Si se utilizan fuentes renovables (electricidad renovable) no existe este problema de emisiones.

Dado que se prevé el agotamiento de combustibles fósiles hacia el 2050, el transporte mundial enfrenta el reto de modificar completamente sus sistemas en algo menos de cinco décadas. Se prevé que los vehículos de hidrógeno serán los más económicos, si se extrapolan las tecnologías actuales, con lo cual deberemos aprender a producirlo por otros métodos distintos del altamente contaminante que se usa hoy en día (tratamiento de gas natural con vapor), que genera inmensas cantidades de dióxido de carbono, si queremos que su uso no contribuya aún más al calentamiento global.

Durante los últimos años los vehículos han estado haciéndose más limpios, como consecuencia de regulaciones ambientales más estrictas e incorporación de mejores tecnologías, (convertidores catalíticos, etc.), y, sobre todo, por un mejor aprovechamiento del combustible. Sin embargo, esta situación ha sido más que compensada por la subida tanto del número de vehículos como del uso creciente anual de cada vehículo, lo cual determina que ciudades con más de 1.000.000 de habitantes presenten problemas de índices de contaminación atmosférica excesivos, afectando la salud de la población.

En el año 2009 la NASA promovió el denominado Desafío Vuelo Verde, un concurso por el que se premia con medio millón de dólares al proyecto más original y eficiente. Más de media docena de proyectos se han presentado hasta el momento. La iniciativa se desarrolla en cooperación con las siguientes instituciones norteamericanas: Departamento de Agricultura y Energía, la Agencia de Protección Ambiental, la Fundación Nacional de Ciencia y el Instituto Nacional de Comercio de Estándares y Tecnología, junto con la Oficina de Patentes y Marcas. Algunos de estos proyectos son:

Cri-Cri
Un aeroplano acrobático de fabricación francesa y completamente eléctrico. Funciona con cuatro motores y tiene una autonomía de vuelo de 30 minutos a una velocidad de crucero de 100 km/h.

SugarVolt
Modelo híbrido diseñado por Boeing. Funciona con una combinación de turbinas (de hélice) de queroseno, que son las que lo hacen despegar. Pero, una vez en el aire, al necesitar menos energía, vuela con un motor eléctrico.

Skyhawk 172
La firma aeronáutica Cessna prevé tener lista para finales de 2011. El avión de pequeña envergadura dispondrá solamente de dos asientos, pero aun así se estima que sea uno de los más usados debido a su parecido con un modelo anterior, al que se le han incluido mejoras.

El Superboeing
No todos los aviones ecológicos serán eléctricos. Este es un prototipo supersónico de Boeing que funciona con combustible de alto rendimiento, y que está siendo diseñado en colaboración con la NASA. Además en cuanto al medio aéreo se plantea otro hándicap a tener en cuenta. En la actualidad las innovaciones que se han hecho en los aviones acotan en demasía los vuelos comerciales. Es decir el mercado se cerca. En su gran mayoría son jets privados de pocas plazas que no permiten el transporte masivo de pasajeros, por lo tanto, la problemática inicial de hacer factible una opción poco contaminante al gran público, en este caso, es imposible, por el momento.

El transporte y la distribución de la energía por medios de transporte han ocasionado múltiples accidentes que han afectado gravemente a personas, instalaciones y medio ambiente. El transporte de la energía varía dependiendo del tipo de energía a transportar.

El medio también causa impactos importantes sobre el sistema energético; cabe destacar el efecto de los terremotos, huracanes, tormentas, variaciones bruscas de temperatura, etc.



http://Wikipedia.com



</doc>
<doc id="2754" url="https://es.wikipedia.org/wiki?curid=2754" title="Tropaeolaceae">
Tropaeolaceae

La familia de las tropeoláceas (Tropaeolaceae) comprende tres géneros de plantas dentro del orden de las brasicales.
Son hierbas anuales o perennes, muchas veces subsuculentas, escandentes o raramente procumbentes, a veces con rizomas tuberosos; plantas hermafroditas. Hojas alternas; láminas enteras, lobadas o palmadamente divididas, peltadas o subpeltadas, palmatinervias; pecíolos largos, normalmente del mismo largo de la lámina o más largos; estípulas presentes o ausentes. Flores comúnmente solitarias, axilares, vistosas, marcadamente irregulares o a veces subactinomorfas ("Trophaeastrum"), con pedúnculos largos y péndulos o erectos ("Trophaeastrum"); sépalos 5, libres, imbricados, uno de ellos en general largamente espolonado; pétalos 5, libres, imbricados, unguiculados, los 2 superiores usualmente más pequeños que los 3 inferiores, enteros, serrados o lobados, ciliados o no; estambres 8, filamentos libres, anteras pequeñas, basifijas, con dehiscencia longitudinal; pistilo simple, estilo delgado, estigma seco, 3-lobado, ovario 3-locular, con 1 óvulo por lóculo. Fruto un esquizocarpio con 3 mericarpos o sámaras ("Magallana"); semillas con un embrión grande y recto y 2 cotiledones gruesos, endosperma ausente.




</doc>
<doc id="2757" url="https://es.wikipedia.org/wiki?curid=2757" title="Taxaceae">
Taxaceae

Las taxáceas (nombre científico Taxaceae) son una familia de coníferas del orden Cupressales. Hojas lineales, aplanadas, enteras, agudas en el ápice, son de óvulos solitarios sin conos, y semillas con una cubierta externa dura asociada a un arilo carnoso usualmenste brillantemente colorido. Son mayormente del Hemisferio Norte.

Árboles de tamaño pequeño a moderado o arbustos, usualmente no resinosos o ligeramente resinosos, con fragancias o no. Madera sin canales de resina. Hojas simples, persistentes por muchos años, que se desprenden de a una, dispuestas en espiral (opuesta en una especie), muchas veces torcidas de forma de parecer dísticas, linealesa, aplanadas, enteras, agudas en el ápice, con 1 o ningún canal de resina. Dioicos (raramente monoicos). Estróbilos de microsporangios con 6-14 microsporofilos, microsporangios 2-9 por microsporofilo, arreglados radialmente alrededor del microsporofilo o limitados a su superficie abaxial, polen sin "sacca". Óvulos solitarios y sin conos, semillas con una cubierta exterior dura, asociada a una arilo carnoso usualmente brillantemente coloreado, cotiledones 2 (ocasionalmente 1 o 3).

Mayormente del Hemisferio Norte, extendiéndose desde el sur de Guatemala hasta Java, con un género endémico de Nueva Caledonia. 

Tienden a crecer en sitios húmedos del fondo de los valles donde se acumula la hojarasca.

Taxaceae es único entre las coníferas en que su semilla solitaria no está asociada a escama del cono. El arilo es un crecimiento del eje bajo la semilla. Algunos sistemáticos han removido a esta familia de las coníferas porque no tiene cono, pero la embriología, la anatomía de la madera, la química, y la morfología de la hoja y del polen atan a esta familia incuestionablemente a las demás coníferas. El cono se piensa que se ha perdido, y la semilla solitaria con el arilo es por lo tanto un carácter derivado.

Las secuencias de ADN, la morfología, la anatomía, y la química de los alcaloides dividen la familia en dos clados, uno incluyendo a "Taxus", "Austrotaxus" y "Pseudotaxus", y el otro comprendiendo a "Torreya" y a "Amentotaxus". La familia está aparentemente más cercanamente emparentada a (y puede ser parafilética sin) Cephalotaxaceae, una familia monogenérica del este de Asia que tiene óvulos pareados asociados a un pequeño crecimiento considerado una escama del cono reducida y que crece a lo largo del eje del cono. Usualmente sólo una de las dos semillas madura, y desarrollan una cubierta externa jugosa que se asemeja, pero no es homóloga a, el arilo de las taxáceas. Las semillas solitarias y como drupas de muchas Podocarpaceae también se parecen a las semillas ariladas de las taxáceas, pero los datos de secuencias de ADN indican que la carnosidad apareció más de una vez (Stefanovic et al. 1998).

La clasificación, según Christenhusz et al. 2011, que también provee una secuencia lineal de las gimnospermas hasta género:


"Taxus" es ampliamente cultivado como ornamental y para madera fina en Norteamérica y Europa. Es una de las maderas de coníferas más finas, ahora utilizada en amoblamientos de alta calidad. "Torreya" es menos importante como ornamental, pero su madera, su semilla comestible, y el aceite de su semilla son valorados en Asia. 

"Taxus" contiene taxol, uno de los muchos altamente venenosos alcaloides de las hojas, tallos, y semillas. La potente actividad antimitótica del taxol hace que tenga uso en la quimioterapia del cáncer.





</doc>
<doc id="2758" url="https://es.wikipedia.org/wiki?curid=2758" title="Tauromaquia">
Tauromaquia

La tauromaquia (del idioma griego ταῦρος, "taūros" 'toro', y μάχομαι, "máchomai" 'luchar') se define como «el arte de lidiar toros», tanto a pie como a caballo, y se remonta a la Edad de Bronce. Su expresión más moderna y elaborada es la corrida de toros, una fiesta que nació en España en el siglo XII y que se practica también en Portugal, sur de Francia y en diversos países de Hispanoamérica como México, Colombia, Perú, Venezuela, Ecuador y Costa Rica. Es también espectáculo de exhibición en China, Filipinas y Estados Unidos. Las corridas de toros han despertado diversas polémicas desde sus comienzos entre partidarios y detractores.

En sentido amplio, la tauromaquia incluye todo el desarrollo previo al espectáculo como tal, desde la cría del toro a la confección de la vestimenta de los participantes, además del diseño y publicación de carteles y otras manifestaciones artísticas o de carácter publicitario, que varían de acuerdo a los países y regiones donde la tauromaquia es parte de la cultura nacional.

Esta actividad tiene antecedentes que se remontan a la Edad de Bronce, y se ha desarrollado a lo largo de siglos como una forma de demostración de valentía, al estilo de algunas tribus que aún practican ritos de paso de la niñez a la edad adulta.

En la antigua Roma se presentaban espectáculos con uros (raza bovina extinta) que eran arrojados a la arena del circo para su captura y muerte por parte de algunos representantes de familias nobles, quienes mostraban así sus dotes de cazadores. También se arrojaban en manadas a los cristianos durante las ejecuciones públicas efectuadas en la época de la persecución; y además, se utilizaba a estos animales durante los enfrentamientos de gladiadores como entretenimiento adicional.

Es en la Edad Media cuando comienza la práctica taurina del lanceo de toros a caballo, a la que se sabe eran aficionados Carlomagno, Alfonso X el Sabio y los califas almohades, entre otros. Aunque de acuerdo a la "Carta histórica sobre el origen y progresos de las fiestas de toros en España" (1777) escrita por Nicolás Fernández de Moratín atribuía a Rodrigo Díaz de Vivar ser el primero en lancear toros a caballo.
Según crónica de la época, en 1128 «...en que casó Alfonso VII en Saldaña con Doña Berenguela la chica, hija del Conde de Barcelona, entre otras funciones, hubo también fiestas de toros.»

En 1387, durante el reinado de Juan I, tuvo lugar la primera corrida de toros en Barcelona en la plaza del Rey, según se recoge de forma oficial en el Archivo General de la Corona de Aragón, que se encuentra en Barcelona.

Estos espectáculos se presentaban en plazas públicas y lugares abiertos como parte de celebraciones de victorias bélicas, patronímicos y fiestas, con el consecuente riesgo que esto suponía para los espectadores. En 1215 según las pautas marcadas en el IV concilio de Letran se prohibía la asistencia y participación del clero en estos eventos.

Durante el siglo XVI evoluciona la tauromaquia hacia los encierros de varas (predecesora de las actuales corridas de rejones), en los que participaba la realeza; incluso Carlos I de Inglaterra y su lugarteniente Lord Buckingham participaron en este evento durante su estancia en España, repitiendo luego la experiencia en su país, invitando a los embajadores de los reinos de Francia y España. 

En la plaza mayor de Madrid se celebraban dos tipos de corridas de toros: las usuales, en las que asistía el hombre de a pie, y las reales, reservadas a selectos personajes de la Corte. Las primeras se organizaban por el Concejo de la Villa, las segundas por los encargados del protocolo y fiestas de la Corte: Mayordomía Real, y por regla general eran más lujosas. Se solían celebrar las corridas populares sin fecha fija en torno a las fechas de San Juan (junio), en Santa Ana (agosto) y posteriormente las de San Isidro (mayo), las de San Pedro y San Pablo.

Carlos I de España (no nacido en este país) lanceó un toro en la celebración del nacimiento de su hijo Felipe II en 1527 y en 1677 el rey Carlos II celebraba su cumpleaños con una "fiesta de Toros" en la Plaza Mayor de Madrid donde habitualmente se realizaban festejos taurinos.

Durante esta época la nobleza comienza a utilizar a sus peones y escuderos para distraer al toro mientras cambiaban algún caballo cansado o herido, o para rescatarlos de una caída. Con la aparición de los picadores en sustitución de las lanzas, para dar a los nobles, a lomo de caballo, el privilegio de matar al toro, estos peones y auxiliares adquieren la responsabilidad de llevar al toro al picador, con lo que evoluciona la faena de capote y adquiere valor estético. En muchas ocasiones, si el de a caballo no podía matar al toro, se delegaba la responsabilidad en los de a pie.

La tauromaquia es la evolución de los trabajos ganaderos de conducción, encierro y sacrificio en los macelos o mataderos urbanos que comenzaron a construirse en España durante el siglo XVI. Estos profesionales de la conducción del ganado vacuno, entonces toro bravo, y los matarifes aportaron creatividad y virtuosismo a las tareas más arriesgadas, que inmediatamente fueron de interés para los más diversos espectadores. Las primeras noticias sobre estas suertes prodigiosas son del Matadero de Sevilla, en el cual además está documentada la presidencia encarnada por un representante de la autoridad municipal, situado en una torre mirador o palco proyectado por el arquitecto Asensio de Maeda y conocido por una importante cantidad de óleos que recogen la actividad taurina en ese momento. En el matadero sevillano también se proyectaron las primeras tribunas para espectadores en la segunda mitad del siglo XVI
A partir del siglo XVII comienzan a surgir nombres entre los toreros de a pie, por su estilo y valor, además de la simpatía que a estos se les tenía por ser parte del mismo pueblo y no de la nobleza, siendo solicitados por el público para presentarse como evento principal.

Paulatinamente, el gusto del público se inclina por los toreros de a pie, y, si bien con extrañas variaciones, a lo largo de los siglos y en concreto durante el siglo XVIII se van estableciendo todos los elementos de las corridas modernas. La tradición de arrastrar Tras la muerte del toro que hasta entonces se retiraba en carreta, data del 10 de agosto de 1623 en las que el corregidor Juan de Castro y Castilla decide por primera vez introducir esta modificación en la plaza Mayor de Madrid. Se utilizaban tres mulas en las corridas ordinarias y seis en las Reales..

Del siglo XVIII algunas de las primeras figuras conocidas del toreo, como "Costillares", Pepe-Hillo y Pedro Romero.

Durante el siglo XVIII se construyeron las primeras plazas de toros en España, en 1707 consta que existía una plaza de toros cuadrada en el Arenal de Sevilla, en 1764 se inaugura la plaza de toros de Zaragoza y en 1792 la Plaza de toros vieja de Tarazona.

Ya en el siglo XIX, toreros como "Paquiro", "Cúchares", "Lagartijo" y "Frascuelo", fueron quienes dieron a la corrida la estructura definitiva que tiene hasta la actualidad.

En la década de 1910 a 1920 se desarrolla la llamada Época Dorada de la tauromaquia, protagonizada por la rivalidad profesional entre Juan Belmonte y José Gómez (conocido como "Gallito" o "Joselito"), que inauguraron el camino hacia el toreo moderno.

Posteriormente a la Guerra Civil Española se produce un auge en el mundo taurino, especialmente gracias al surgimiento de la figura de Manolete, para muchos el más vertical de los toreros en la historia; a este auge siguen figuras como Luis Miguel Dominguín, el mexicano Carlos Arruza, Pepe Luis Vázquez, Antonio Bienvenida, Pepín Martín Vázquez, Silverio Pérez, Miguel Báez «El Litri», Julio Aparicio y Agustín Parra «Parrita». Si bien esta época se cierra con el fallecimiento de Manolete en la llamada Tragedia de Linares, surge entonces otra famosa rivalidad que apasiona al mundo taurino, la de Dominguín y Antonio Ordóñez.

Ya en los años 1950 se alza la figura de particular elegancia del venezolano César Girón, quien lidera en dos ocasiones (1954 y 1956), el escalafón taurino en España, hazaña que repetiría su hermano Curro en 1959 y 1961. Destacan en los años 1960, además del mencionado Curro Girón, toreros como Curro Romero, Paco Camino, El Viti, Diego Puerta, y Manolo Martínez, además de la sensación que causó el surgimiento del poco ortodoxo y revolucionario, pero muy triunfador, Manuel Benítez, "el Cordobés". Los años 1970 y 1980 son los de mayor expansión comercial del mundo de los toros, llegando a haber corridas incluso en el Astrodome de Houston, con la participación de Manuel Benítez «el Cordobés». Las grandes figuras de esta época son: José Mari Manzanares, Pedro Gutiérrez Moya El Niño de la Capea, Dámaso González, Morenito de Maracay, Francisco Rivera «Paquirri», El Yiyo, Nimeño II, Antoñete y Juan Antonio Ruiz «Espartaco», líder de la estadística en forma consecutiva desde 1985 hasta 1991.

De acuerdo a los datos facilitados por la Asociación Nacional de Organizadores de Espectáculos Taurinos (ANOET) en España se vienen celebrando anualmente entre 13.965 y 16.218 "Festejos" taurinos entre el 2007 al 2014.

Las nuevas figuras del toreo presentan gran diversidad en su estilo y proyección; personalidades tan particulares como Enrique Ponce, y Joselito —de toreo clásico—; Julián López, "el Juli", José Tomás, Manuel Jesús Cid "el Cid", Miguel Ángel Perera, Pepín Liria, Morante de la Puebla, José María Manzanares, Luis Bolívar, Antonio Puerta, y el francés Sebastián Castella, son algunos de los toreros más célebres del siglo XXI.

Además de la corrida en sí, la tauromaquia incluye la crianza y conocimiento de los toros, llamados de lidia (denominación de los mismos de acuerdo a su pelaje, cornamenta, comportamiento, porte, etcétera). Incluye además lo concerniente a la confección de la ropa del matador y demás participantes dentro del espectáculo, así como las manifestaciones artísticas relacionadas con la actividad (confección de carteles, entre otras).
Existen diversas asociaciones que promocionan la tauromaquia en los diferentes países, por ejemplo en España la Asociación Nacional de Espectáculos Taurinos (ANOET).

Actualmente, la actividad más conocida de la tauromaquia es la corrida de toros. En consecuencia, con la consideración de cómo se lleve la responsabilidad de la lidia y muerte del toro (si el torero va a pie o a caballo), existen dos tipos de corridas de toros; de toreros a pie y de toreros a caballo (de rejones o rejoneadores).

Normalmente, una corrida se desarrolla en tres partes, llamadas tercios, en las cuales el toro es lidiado respectivamente por los picadores, «que, montando un caballo protegido por un peto, utilizan una vara con una puya para preparar al toro para el tercio de muleta»; los banderilleros, «quienes se encargan del auxilio al matador, bregan al toro y adornan al toro colocando pares de banderillas (generalmente son tres pares)»; y el último tercio, y el más importante, el de muerte, en el que el torero lidia al toro manejando la muleta y el «ayudado» (espada de madera o de aluminio), que sostiene con la mano derecha. El torero principalmente empieza a medir la distancia del toro, lo que se llama «terreno», para empezar a cuajar su faena, hasta empezar a meterle la cabeza en cada suerte o engaño; después coloca al burel con los cuartos delanteros parejos, para que se abra y no pinche en hueso; eso es para asegurar la estocada, y, si es correcta, a petición del presidente y el respetable, se cortan los trofeos.

El presidente es quien recompensa la actuación del torero. Al término de la lidia, el presidente enseña un pañuelo de color blanco, si el premio de la faena es para una oreja, y dos pañuelos para dos trofeos. Al principio de ella también puede enseñar un pañuelo verde si el toro no es apto para torear (cojo, cuerno mal, etc.), o uno naranja para indultarlo si el toro es de gran calidad. La opinión del público es posiblemente de más peso para los participantes: ha habido corridas en donde el público saca en hombros al torero sin que el juez haya concedido siquiera la oreja, o por el contrario: premios del presidente a pesar del descontento de los asistentes.


El toro de fuego es un armazón metálico, que imita la forma de un toro, sobre cuyo espinazo se coloca un bastidor con elementos pirotécnicos. Muy utilizado en festejos de pueblos de España.

Es transportado por una persona, que tras encender una mecha, corre persiguiendo a la gente asustándoles con las chispas que van soltando sus diferentes elementos.

Este mismo juego popular es utilizado también en Paraguay y es conocido como el «toro candil». Es típico de las Fiestas de San Juan.

En los pueblos de los andes peruanos, el armazón utilizado es de madera, y el juego es conocido como «vaca loca», y es típico en las vísperas al día central de las festividades patronales.

En Ecuador, el juego de la vaca loca es popular y se realiza en la noche, especialmente en las vísperas de algún santo. La vaca loca es un triángulo abierto de madera para que pueda colocarse en los hombros una o dos personas. Está adornado con papel de varios colores, y lleva a los costados fuegos artificiales, voladores, que son carrizos con pólvora que al ser encendidos salen disparados. En la parte superior tiene dos cuernos, que en varias ocasiones son de verdad; es decir, de toros o vacas. Al son de la banda de pueblo sale la vaca loca y persigue a las personas que se encuentran concentradas en el centro de una plaza tratando de sorprender a los distraídos. La gente corre y se divierte por la emoción y bulla que se produce. 

De igual manera se realiza en El Salvador pero con el nombre de torito pinto, especialmente el día en que corresponde la quema de pólvora de una fiesta patronal.


El preámbulo de las fiestas va aparejado con el desarrollo de los Ciclos Culturales Taurinos de San Marcos en Beas de Segura, que se vienen celebrando desde el año 1995.
Se celebra el día antes del corpus, donde los mozos corren agarrados a una larga maroma sujeta a las astas del toro

El toro embolado en un festejo tradicional de España, en el que se colocan a un toro en sus astas dos bolas de fuego. No se conocen realmente sus orígenes, pero junto a otros festejos taurinos en los que no se le da muerte al animal pueden tener su origen en la civilización minoica como se recogen en muchos frescos y cerámicas.
La zona de mayor actividad se encuentra en la Comunidad Valenciana. Actualmente se está intentado regular dichas actividades, acogiéndose con el calificativo de "Bous al carrer". También en otras regiones existen festejos similares como el toro de ronda en Aragón.

"A Festa do Boi de Allariz" es un festejo que se celebra el día de Corpus Christi y consiste en soltar un buey que recorre las calles de esta población de Galicia. Fue recuperado en 1983 basándose en antiguas leyendas populares de origen judío.

El encierro consiste en correr delante de una manada no muy numerosa de toros, vaquillas o novillos, entre los que puede haber también cabestros que dirijan a la manada. Por lo general, los mejores corredores intentan correr lo más cerca posible de los toros, pero sin llegar a tocarlos.

Festejos taurinos populares que suelen celebrarse en muchos pueblos de España. Consiste en la suelta de reses de lidia para recreo de la afición. El recinto donde tienen lugar es una plaza de toros fija o portátil y puede ser también una plaza del pueblo cerrada con carros u otras barreras provisionales. Por regla general las reses que se lidian son erales o vaquillas, pero en algunas ocasiones se han lidiado también cuatreños. Las reses se deben sacrificar después del festejo y nunca en presencia del público. El motivo del sacrificio es que estas reses, tras la capea, "aprenden" y en posteriores capeas pueden ir a por la gente, más que al capote.

El toreo cómico o charlotadas, aunque denostado por los puristas, fue un género taurino muy popular. Consistía en la introducción de números circenses de payasos mientras se lidiaban novillos o vaquillas. Generalmente participaban payasos toreros que realizaban recortes de forma cómica y parodiaban la lidia o simulaban estorbar a un torero serio.

Los concursos de recortes están formados por jóvenes que se enfrentan a cuerpo limpio y por turnos a animales en puntas, con el fin de arrimarse al máximo al asta del mismo para alzarse con el primer puesto.

Las corridas camarguesas o corridas a la "cocarde" (en francés, "course camarguaise" o "course libre") son festejos que tienen lugar en las plazas de toros de los pueblos de Languedoc-Rosellón. En este tipo de corridas no se mata al toro.

La corrida landesa ("course landaise") es un espectáculo basado en saltos y recortes en el cual las vaquillas salen emboladas. Era el divertimento tradicional de los gascones.

Suerte típica del toreo portugués, llevada a cabo por los pegadores o mozos de forcado, quienes trabajan en cuadrillas de 8 elementos y la corrida termina cuando toman e inmovilizan al toro sin más implemento que las manos.

El toreo de la vincha es la única fiesta taurina que existe en la República Argentina; en ella no se lastima al toro, sólo hay que quitarle una vincha; el ritual dice que quien logre quitarle la vincha al toro debe ofrendársela a la Virgen.
Es herencia de la época del Virreinato del Río de La Plata que se lleva a cabo en la localidad de Casabindo, departamento Cochinoca, provincia de Jujuy. La ceremonia se lleva a cabo los 15 de agosto, para festejar de este modo la Asunción de la virgen María. La fiesta comienza el día anterior, a la tarde llegan hasta el lugar bandas de sicuris de localidades vecinas y por la noche se agrupan en una gran peña donde comparten bebidas típicas como la chicha y comidas tradicionales al sonido del erke (instrumento precolombino). Al día siguiente se realiza una misa a cargo del obispo de Humahuaca, la fiesta continua con una procesión que atraviesa la plaza de toros con una imagen de la virgen llevada por los fieles a paso lento por toda la localidad mientras una banda de músicos imita melodías litúrgicas de la semana santa andaluza, luego de esto comienza el toreo.

La corraleja es una fiesta popular de la Costa Caribe de Colombia, donde en una plaza se torean varios novillos sin sacrificarlos. Entre las corralejas más populares se encuentran las de Sincelejo, Cotorra, Planeta Rica y Cereté.

En Colombia también se lleva a cabo una temporada taurina que comenzó hacía el año 1917, sin embargo desde los años de la independencia de la corona española se celebraban algunos festejos taurinos en el Circo de Toros de San Diego en Bogotá y el Circo Taurino El Palo en Medellín.
Hacía las primeras décadas del siglo XX, la tauromaquia colombiana comenzó su vida adulta, con sendos festejos taurinos que principalmente se realizaban en Bogotá y en Medellín, hasta la creación de la Feria de Manizales (1955), que abrió definitivamente a Colombia como gran centro taurino y poco después se comenzó a organizar la Feria de Cali (1957). Estos dos acontecimientos hicieron de la temporada colombiana la primera de Sudamérica. Actualmente la temporada comienza hacia finales del mes de diciembre con la Feria de Cali en la plaza Cañaveralejo, siguiendo con la Feria de Manizales en la Monumental de Manizales, de la cual se dice que es una feria en América con «duende» andaluz; y las temporadas taurinas de Medellín (Plaza de Toros La Macarena) y Bogotá (Plaza de toros de Santamaría). También hay temporada Taurina en las otras dos plazas de primera categoría en Cartagena de Indias y Bucaramanga.

En Costa Rica las corridas de toros más populares se realizan a fin de año, entre las fechas del 24 de diciembre y 2 de enero. Generalmente se realizan en la plaza de toros de Zapote (distrito del cantón de San José) y son organizadas por la Comisión de Festejos Populares. También existen otras corridas importantes como lo son las de las Fiestas de Palmares (Costa Rica) y las de Zapote (dos de las fiestas populares más importantes del país). Las corridas se realizan siempre sin matar al toro ni causarle daño físico. Existen distintas modalidades de toreo en el país, incluyendo el toreo tradicional (sin la muerte del toro), el toreo bufón y el toreo improvisado, consistente en un grupo numeroso de toreros aficionados esquivando en conjunto la arremetida de un toro. Esta última es la modalidad más popular, conocida como «toreo a la tica».

Cabe destacar que en Costa Rica la ley prohíbe el sacrificio de toros y otros animales en espectáculos públicos o privados. (Ver Reglamento Actividades Taurinas, N° 19183-G-S). La «estocada» final la realizan los toreros quitándole un adorno floral que el toro lleva en el lomo. Tampoco se le pica ni hiere con banderillas.

La Feria de Jesús del Gran Poder ―en Quito (Ecuador)― se festeja del 28 de noviembre al 6 de diciembre con motivo de la fundación de San Francisco de Quito. Los nueve días de corridas se celebran en «La Monumental» (nombre dado a la plaza de toros de Quito). Según algunos críticos de la tauromaquia, es una de las más importantes ferias taurinas de América y año tras año atrae a nacionales y extranjeros. Así mismo la feria tiende a tomar toda la iniciativa española, debido a que sus corridas son efectuadas de igual forma, con los tercios, hasta el despacho del bovino. Debido a su gran influencia en la cultura ecuatoriana, el toreo y el rejoneo se han sumado a las muchas aficiones del público.

La Feria de Latacunga San Isidro Labrador en Ecuador se festeja en la última semana de noviembre como programa de cierre de las fiestas de Indepencia de la ciudad. La feria de Latacunga se desarrolla en la Plaza San Isidro Labrador, se viene realizando dos o tres días de feria, el viernes, sábado y domingo de la última semana de noviembre. La feria se ejecuta al estilo español, en ella han participado los mejores toreros del escalafón mundial como El Juli, El Pirata Juan José Padilla, El Fandi, Morante de la Puebla y varios toreros nacionales de renombre.

La Feria de Riobamba Señor del Buen Suceso en Ecuador se festeja en las primeras semanas del mes de abril como inicio de las fiestas de Independencia de la ciudad, mejor conocida como «La Sultana de los Andes». Esta feria se realiza en la Plaza de toros de Riobamba, «Raúl Dávalos». De igual manera que las otras ferias realizadas en el Ecuador, la feria del Señor del Buen Suceso se ejecuta al estilo español, en ella han participado los toreros mejor ubicados en el escalafón mundial como «El Ciclón de Jerez» Juan José Padilla, El Fandi, Finito de Córdoba, Miguel Abellan, Uceda Leal, Víctor Puerto y varios toreros ecuatorianos.

La Feria de Ambato Nuestra Señora de la Merced se realiza todos los años en el mes de febrero en honor a las fiesta de la ciudad. Esta feria se realiza en la conocida Plaza de Toros Monumental de Ambato. La Feria Nuestra Señora de la Merced es la que abre la temporada taurina en Ecuador, seguida de la Feria de Riobamba Señor del Buen Suceso, La Feria de Latacunga San Isidro Labrador y terminando con La Feria Jesús del Gran Poder en Quito (Ecuador). Toreros de gran nivel como Sebastián Castella, Iván Fandiño, Diego Silveti, Joselito Adame, Manuel Escribano, Diego Urdiales, Miguel Abella, entre otros han sido testigos de esta importante Feria.

La Feria de Valencia: Una de las tradiciones que mantiene la ciudad de Valencia desde el inicio de su cantonización son las tradicionales ferias taurinas consideradas como unas de las mejores en Ecuador, que se la realiza anualmente para celebrar sus fiestas patronales de San Francisco de Asís, faena que se desarrolla los primeros días del mes de octubre de cada año.

Las comunidades portuguesas asentadas en el estado de California en Estados Unidos han conservado las corridas de toros al estilo portugués aunque haciéndole algunas modificaciones. En las corridas californianas no hay derrame de sangre, por lo que son llamadas "Bloodless bullfight". Por ende, no se pica, no se le clavan banderillas al lomo del animal ni se le da muerte en el ruedo. El toro lleva un velcro sobre su lomo por lo que las banderillas van adheridas allí, por lo demás son iguales a una corrida portuguesa, incluso grandes toreros europeos o americanos (mexicanos particularmente) torean anualmente en las plazas californianas.

Se realiza en Aguascalientes, México y se conforma comúnmente por catorce grandes festejos taurinos, de los cuales son: una novillada y trece corridas de toros. Toda esta fiesta brava dentro de los festejos de la Feria Nacional de San Marcos se realiza a mediados del mes de abril y parte de mayo de cada año. La fiesta brava en Aguascalientes, sobre todo en su feria de San Marcos, es uno de los principales atractivos de la feria, ya que se presentan grandes figuras de toreo. Todos los festejos taurinos se realizan en la Plaza Monumental Aguascalientes, en las instalaciones de la feria.

Es llevado a cabo en Jalostotitlán, Jalisco, México durante diez días de festejo con tres fechas con corridas de toros en la monumental plaza de Toros Fermín Espinosa «Armillita» del municipio Jalostotitlán, en Jalisco, México. Se realiza durante el mes de febrero la mayoría de las veces, pero su fecha varía dependiendo del miércoles de ceniza, concluyendo el martes de Carnaval.

Se festeja en Autlán de la Grana, Jalisco, México durante diez días de festejos que incluyen cuatro fechas con corridas de toros en la Plaza de Toros Alberto Balderas del municipio de Autlán de Navarro, en Jalisco, México. Se realiza durante el mes de febrero, ajustando la fecha para concluir precisamente el martes de Carnaval; exactamente el día anterior del miércoles de ceniza.

Esta fiesta taurina es una costumbre importada por los españoles, pero modificada por los lugareños de la región de Ayacucho, de la provincia de Lucanas, de la ciudad de Puquio, en los Andes. El festejo, conocido como Yáwar Fiesta («Fiesta de Sangre» en quechua), tiene lugar en el mes de mayo (coincide con la celebración del Señor de la Asención), y se suele interpretar como una celebración de la expulsión de los conquistadores españoles por los quechuas, aun cuando no existe un consenso académico definitivo sobre su simbología y función. Para ello colocan a un Cóndor salvaje en el lomo del toro de lidia o toro "pucllay", previamente inmovilizado el toro con una cuerda. Una vez bien asegurado, cortan la cuerda y el toro sale furioso, dando saltos, al sentir los fuertes picotazos del Cóndor. Cuando el toro queda agotado, liberan al ave, y, tras homenajearlo, lo devuelven a las montañas. Si el cóndor no sobrevive al festejo (que dura una semana), se considera de mal augurio. Esta celebración ha sido retratada por el escritor peruano José María Arguedas cuando vivió en Puquio, en una novela titulada «Yawar Fiesta».

En Venezuela las corridas de toros tienen importancia en el occidente del país, sobre todo en los estados centrales y andinos, según registros las ferias más antiguas datan de 1843 en un poblado merideño de la antigua parroquia de Regla de los Bailadores, hoy llamada Tovar. En la actualidad las corridas de toros han sido prohibidas en algunas localidades y estados quedando resumidas a los estados Zulia, Carabobo, Aragua, Trujillo, Guárico, Yaracuy, Barinas, Táchira y Mérida, siendo estos dos últimos los más taurinos del país.

Venezuela cuenta con importantes Plazas de Toros, unas vigentes para tal fin, otras con un nuevo uso en virtud de la prohibición de las corridas en ellas, siendo la Monumental de Valencia la más grande de Venezuela y la segunda en el mundo, la Maestranza César Girón de Maracay la de mayor prestigio taurino en el país, las Monumentales de Maracaibo, Pueblo Nuevo (San Cristóbal) y Román Eduardo Sandía (Mérida) las más regulares junto al Coliseo El Llano de Tovar, esta última, única con techo en esa nación, mientras que la Monumental de Oriente (Barcelona), el Coliseo Perla del Torbes (Táriba) y el Nuevo Circo (Caracas) ya no cuentan con eventos taurinos regularmente, especialmente el último, que ya no alberga corridas.

Los eventos taurinos en Venezuela se dividen en dos renglones: La Temporada Grande y la Temporada menor o extendida. La temporada grande llamada así por su regularidad anual, por la existencia de una localidad y recinto, la tradición, la formalidad, el prestigio, la existencia de afición así como de autoridad municipal en el área, compuesta por seis ferias llamadas "Las Grandes", Tovar, Valencia, Maracaibo, San Cristóbal, Mérida y Maracay en ese orden, lo cual suma un aproximado de 25 a 30 espectáculos. La temporada menor o extendida alberga las seis ferias ya mencionadas más otros festejos, algunos regulares realizados en plazas de segunda, y otros más entre regulares e irregulares realizadas en plazas portátiles.

Venezuela ha tenido importantes figuras del toreo como: César Girón, Antonio Bienvenida, Leonardo Benítez, José Nelo "Morenito de Maracay" y Bernardo Valencia, de gran renombre en Venezuela, México, Colombia y España; otros grandes en ascenso: Rafael Orellana, Hassan Rodríguez "El Califa de Aragua", Fabio Castañeda, César Vanegas, César Valencia, Marcos Peña "El Pino", Manolo Vanegas y Jesús Enrique Colombo, los dos últimos hasta ahora novilleros. Venezuela también posee exponentes del rejoneo, siendo Javier Rodríguez el de connotación histórica en el país, además de José Luis Rodríguez, Rafa Rodríguez y Francisco Javier Rodríguez.

Las ganaderías de toros de lidia son empresas que dependen de los espectáculos taurinos, dado que esta variedad bovina no tiene ningún otro propósito comercial, debido a su bajo rendimiento, tanto de leche como de carne. Grandes criadores de toros de lidia han alcanzado renombre por las características particulares de trapío de sus astados, y sus nombres son de perdurable reconocimiento en el mundo taurino. A continuación se enumeran algunas de las más importantes de Europa y América.

Nota: La lista no es exhaustiva, se listan solo las ganaderías con reconocido prestigio y antigüedad.

Las plazas de toros, conocidas también como "cosos taurinos" y anteriormente como "circos taurinos", son estructuras arquitectónicas cerradas, con estilos arquitectónicos diversos, de acuerdo a su antigüedad. En general, se trata de un recinto cerrado de forma circular, con tendidos y servicios que rodean un espacio central, llamado ruedo o arena, en donde se realiza el espectáculo taurino. El ruedo es un terreno de tierra batida, rodeado por una valla o barrera, y con varios burladeros, en donde se preparan y refugian los matadores y subalternos. El callejón está separado del ruedo por una estructura o pared, generalmente de madera y de aproximadamente 140 centímetros de altura, que posee estribos hacia el ruedo y en ocasiones también hacia el callejón para facilitar el acceso de los alternantes en caso de emergencia. Dispone de puertas de acceso batientes para la entrada y salida de los participantes (puerta de cuadrilla) y los toros (puerta de toriles), aunque la cantidad y disposición de estos accesos varía de un recinto a otro. La plaza de toros más grande del mundo se encuentra en México, con una capacidad aproximada de 41 000 personas sentadas, seguida por la Plaza de toros Monumental de Valencia, en Venezuela.





Temporada Grande:


Temporada menor:

Temporada Grande: llamada así por su regularidad y su prestigio:


Temporada menor: llamada así por su irregularidad, tipo de eventos y carencia de Plaza de Toros en algunos Casos



La situación legal de los toros ha ido cambiando a lo largo de la historia y de los diferentes territorios, sufriendo tanto prohibiciones como reconocimientos a nivel cultural. Una de las primeras prohibiciones que tuvieron lugar fue en Noviembre de 1567 cuando el Papa Pío V dictó una bula papal "De Salute Gregis" y prohibía las corridas de toros y otras bestias que ponían en peligro la vida de los participantes, al ser calificados como «cosa del Demonio, ajena a lo cristiano, debido a la gran cantidad de muertos, heridos y lisiados que provocan», la bula fue abolida en 1575 ocho años más tarde por su sucesor el Papa Gregorio XIII en la bula Nuper
Siquidem, a petición de Felipe II de España.

En España se recoge en la legislación la dimensión cultural de las corridas de toros desde 1991, y en en las leyes para regulación de la Tauromaquia de 2013 y para la salvaguardia del Patrimonio Cultural Inmaterial de 2015), aunque su estatus legal varía entre comunidades autónomas.

Las corridas de toros fueron "de facto" prohibidas en Canarias por el Parlamento de Canarias el 30 de abril de 1991 a partir de una Iniciativa Legislativa Popular impulsada por el diputado regional Miguel Cabrera Pérez-Camacho, entonces perteneciente a la Agrupación Tinerfeña de Independientes y posteriormente en el Partido Popular. La Ley 8/1991, de 30 de abril, de protección de los animales, en su artículo 5, establece:

En diciembre de 2009 el Parlamento de Cataluña empezó a debatir una ley para prohibir las corridas de toros, propuesta a través de una Iniciativa Legislativa Popular (ILP) generada con 180.000 firmas de ciudadanos. La primera votación el 18 de diciembre acabó a favor de los defensores de la abolición, con lo que la ILP pasó a las siguientes fases de su proceso, que acabará transformándola en ley si se vota otra vez a favor a mediados del 2010.

El 28 de julio de 2010, el Parlamento de Cataluña aprobó con 68 votos a favor, 55 en contra y 9 abstenciones abolir las corridas de toros en Cataluña a partir del 1 de enero de 2012 posteriormente, el 20 de octubre de 2016, el Tribunal Constitucional, declaraba inconstitucional la prohibición taurina en Cataluña.

En abril de 2016 el parlamento Balear aprobó una ley para prohibir a partir de junio de dicho año las corridas de toros., aunque en Noviembre de 2017 el consejo de ministro aprobó recurrir la ley ante el tribunal constitucional.

Legalmente solo se pueden matar toros en aquellos lugares donde se demuestre que son una tradición arraigada ininterrumpidamente, las plazas del sudeste y del sudoeste fundamentalmente. En 2011, Francia (Ministère de la Culture) declaró la tauromaquia elemento del "Patrimonio Cultural Inmaterial nacional".

En 1836 en Portugal, durante el reinado de María II de Portugal, fue decretada la prohibición de la muerte de los toros en el ruedo, y para complemento de la lidia de los "cavaleiros" (rejoneadores), se comenzó a pegar el toro, lo que dio lugar a que en el siglo XIX tuviera lugar formalmente el origen de los forcados como la conocemos el día de hoy.

Algunas manifestaciones de la tauromaquia, y particularmente las corridas de toros en las que el animal muere, son objeto de controversia y debate desde tiempos antiguos. Desde el punto de vista de los derechos de los animales, diferentes organizaciones consideran que las corridas y otras manifestaciones son una práctica de crueldad hacia los toros (al ser parte central del espectáculo en algunas de sus formas la práctica de clavar adornos al toro en su lomo —las banderillas— y su posterior muerte por medio de una estocada del torero), dando lugar al activismo antitaurino que ha abogado tradicionalmente por posiciones favorables a la restricción de la tauromaquia o bien a su prohibición total. Organizaciones no gubernamentales como PETA, y partidos políticos como el Partido Animalista Contra el Maltrato Animal de España, niegan que las corridas puedan ser equiparadas a una manifestación cultural, artística o deportiva.

La fiesta taurina está íntimamente ligada a su aspecto ancestral, tradicional y popular. La cultura que ha cubierto siempre el discurrir histórico de la fiesta, da idea de su relevancia: la tauromaquia ha sido plasmada por artistas tales como Goya, Picasso, Manet, Enrique Simonet, Alberto Gironella, Lucas Villaamil y Paco Puertas, así como en pasodobles del famoso compositor mexicano Agustín Lara. La tauromaquia es ejercicio de múltiple comprensión, y puede ser admirada o criticada, pero sus componentes, ya citados, le permiten perdurar en el tiempo y generar amplio debate a su alrededor. Por ejemplo, el gobierno de España, a través del Ministerio del Interior, hace referencia al aspecto cultural de las corridas de toros en su reglamentación de las escuelas taurinas: «Para fomento de la fiesta de toros, en atención a la tradición y vigencia cultural de la misma, podrán crearse escuelas taurinas para la formación de nuevos profesionales taurinos y el apoyo y promoción de su actividad.»

El historiador y crítico de la estética y de la literatura española, Menéndez Pelayo, enseñó en su "Historia de las ideas estéticas en España" que la tauromaquia pertenece a las artes secundarias:
El filósofo José Ortega y Gasset explicaba que era impensable estudiar la historia de España sin considerar las corridas de los toros.
Si muchos de los escritores y filósofos de la Generación del 98, no gustaban de las corridas de toros, era porque la culpaban del atraso de la sociedad española. Así, Unamuno explicaba que no le gustaban las corridas, no porque fuese un espectáculo cruento, sino porque se perdía mucho tiempo hablando de ella y esto explicaba la formación cultural de sus espectadores. Ortega y Gasset, en su obra "La caza y los toros", se extrañaba de que el toreo, siendo un ejercicio callado diese tanto que hablar. Posteriormente, la Generación del 27 en su mayoría fue amante de la fiesta, sobre la cual escribieron, pintaron y esculpieron. Vale citar las palabras con las que Federico García Lorca manifestaba su abierto apoyo y gusto por la tauromaquia: «El toreo es probablemente la riqueza poética y vital de España, increíblemente desaprovechada por los escritores y artistas, debido principalmente a una falsa educación pedagógica que nos han dado y que hemos sido los hombres de mi generación los primeros en rechazar. Creo que los toros es la fiesta más culta que hay en el mundo».
Antonio Machado deja clara su postura en su obra "Juan de Mairena": «Con el toro no se juega, puesto que se le mata, sin utilidad aparte, como si dijéramos de un modo religioso, en holocausto a un dios desconocido.»

Ortega y Gasset, al igual que otros autores como el académico José María de Cossío, realizaba un paralelismo entre las corridas de toros y la historia de España:

Otros intelectuales contemporáneos, como Enrique Tierno Galván, subrayaron, en abierta contradicción con los del 98, el carácter socialmente pedagógico de la tauromaquia: «Los toros son el acontecimiento que más ha educado social, e incluso políticamente, al pueblo español». Y abundaba en el refinamiento del gusto artístico que supone para sus aficionados:

Una larga lista de escritores de varios países ha escrito exaltando el toreo como una parte importante del alma de sus pueblos. Entre los artistas vivos que defienden el toreo se encuentra el peruano Mario Vargas Llosa, el escultor y pintor colombiano Fernando Botero y el escultor y pintor mexicano Humberto Peraza.

Entre los partidarios de la tauromaquia se encuentran el pintor Francisco de Goya (si bien tuvo, al parecer, una postura ambivalente con respecto a los espectáculos taurinos) y los escritores Nicolás Fernández de Moratín y Valle-Inclán. Filósofos como Fernando Savater o Enrique Tierno Galván, y artistas como Joaquín Sabina o Joan Manuel Serrat, aducen que estas críticas de los antitaurinos obedecen a la ignorancia, ya que el toro de lidia vive en libertad en su hábitat natural y, sin las corridas, no solo se extinguiría el toro bravo, sino el propio ecosistema en que se desenvuelve (las dehesas), sin embargo hay alegatos que refieren a que estas pueden ser protegidas por ley sin la necesidad de criar toros. Otros defensores del toreo, como el catedrático Andrés Amorós, argumenta que nadie ama más al toro que un buen aficionado a las corridas: «nadie admira más su belleza, nadie exige con más vehemencia su integridad y se indigna con mayor furia ante cualquier maltrato, desprecio o manipulación fraudulenta.»

Organizaciones a favor de los animales han criticado la financiación de la tauromaquia con dinero público. Según un estudio de la Fundación Altarriba, organización a favor de los animales, las subvenciones a las corridas de toros en 2007 ascendieron a la cantidad de 564 millones de euros, unos 12 euros por cada español. Parte de este dinero procede de los fondos europeos destinados a la ganadería, destinándose 220 euros por cabeza de ganado a los criadores de toros de lidia.

Desde el sector a favor de las corridas de toros, periodistas como Paco Aguado han defendido la solvencia de las corridas de toros y contestado a las críticas hacia las subvenciones que reciben criticando los impuestos con las que se gravan, que consideran excesivos.





</doc>
<doc id="2761" url="https://es.wikipedia.org/wiki?curid=2761" title="Los tres mosqueteros (juego)">
Los tres mosqueteros (juego)

Tres Mosqueteros, juego abstracto entre dos jugadores. Requiere un tablero de 5x5. Tres fichas de un color, que serán los mosqueteros, y 22 de otro color, que serán la Guardia del Cardenal.

El jugador que tiene los Tres Mosqueteros mueve primero, y luego ambos jugadores se alternan. Por turno cada jugador mueve una de sus fichas, de este modo:



Ambos jugadores tienen objetivos diferentes:


Si se van a jugar varias partidas, conviene que ambos jugadores se alternen en los roles de Mosqueteros y Guardia del Cardenal. Para que haya más emoción en el torneo, se puede usar este sistema de puntuación:

Tras un número par de partidas, gana el torneo quien suma más puntos.



</doc>
<doc id="2762" url="https://es.wikipedia.org/wiki?curid=2762" title="Tiempo">
Tiempo

El tiempo es una magnitud física con la que medimos la duración o separación de acontecimientos.
El tiempo permite ordenar los sucesos en secuencias, estableciendo un pasado, un futuro y un tercer conjunto de eventos ni pasados ni futuros respecto a otro. En mecánica clásica esta tercera clase se llama "presente" y está formada por eventos simultáneos a uno dado.

En mecánica relativista el concepto de tiempo es más complejo: los hechos simultáneos ("presente") son relativos al observador, salvo que se produzcan en el mismo lugar del espacio; por ejemplo, un choque entre dos partículas.

Su unidad básica en el Sistema Internacional es el segundo, cuyo símbolo es formula_1 (debido a que es un símbolo y no una abreviatura, no se debe escribir con mayúscula, ni como "seg", ni agregando un punto posterior).

Dados dos eventos puntuales "E" y "E", que ocurren respectivamente en instantes de tiempo "t" y "t", y en puntos del espacio diferentes "P" y "P", todas las teorías físicas admiten que estos pueden cumplir una y solo una de las siguientes tres condiciones:


Dado un evento cualquiera, el conjunto de eventos puede dividirse según esas tres categorías anteriores. Es decir, todas las teorías físicas permiten, fijado un evento, clasificar a los eventos en: (1) pasado, (2) futuro y (3) resto de eventos (ni pasados ni futuros). La clasificación de un tiempo presente es debatible por la poca durabilidad de este intervalo que no se puede medir como un estado actual sino como un dato que se obtiene en una continua sucesión de eventos. En mecánica clásica esta última categoría está formada por los sucesos llamados simultáneos, y en mecánica relativista, por los eventos no relacionados causalmente con el primer evento. Sin embargo, la mecánica clásica y la mecánica relativista difieren en el modo concreto en que puede hacerse esa división entre pasado, futuro y otros eventos y en el hecho de que dicho carácter pueda ser absoluto o relativo respecto al contenido de los conjuntos.
En mecánica clásica, el tiempo se concibe como una magnitud absoluta, es decir, es un escalar cuya medida es idéntica para todos los observadores (una magnitud relativa es aquella cuyo valor depende del observador concreto). Esta concepción del tiempo recibe el nombre de tiempo absoluto. Esa concepción está de acuerdo con la concepción filosófica de Kant, que establece el espacio y el tiempo como necesarios para cualquier experiencia humana. Kant asimismo concluyó que el espacio y el tiempo eran conceptos subjetivos. Mas, no por ello, Kant establecerá que tiempo y espacio sean dimensiones absolutas, ni en sí mismas, sí apoyadas, en cambio, por Newton y Leibniz respectivamente. Para Kant no son dimensiones sino formas puras de la intuición suministrada por la experiencia, de manera que, al no tratarse de magnitudes, no hay posible choque entre ellas.
Fijado un evento, cada observador clasificará el resto de eventos según una división tripartita clasificándolos en: (1) eventos pasados, (2) eventos futuros y (3) eventos ni pasados y ni futuros. La mecánica clásica y la física prerrelativista asumen:
Aunque dentro de la teoría especial de la relatividad y dentro de la teoría general de la relatividad, la división tripartita de eventos sigue siendo válida, no se verifican las últimas dos propiedades:

En mecánica relativista la medida del transcurso del tiempo depende del sistema de referencia donde esté situado el observador y de su estado de movimiento, es decir, diferentes observadores miden diferentes tiempos transcurridos entre dos eventos causalmente conectados. Por tanto, la duración de un proceso depende del sistema de referencia donde se encuentre el observador.

De acuerdo con la teoría de la relatividad, fijados dos observadores situados en diferentes marcos de referencia, dos sucesos A y B dentro de la categoría (3) (eventos ni pasados ni futuros), pueden ser percibidos por los dos observadores como simultáneos, o puede que A ocurra "antes" que B para el primer observador mientras que B ocurre "antes" de A para el segundo observador. En esas circunstancias no existe, por tanto, ninguna posibilidad de establecer una noción absoluta de simultaneidad independiente del observador. Según la relatividad general el conjunto de los sucesos dentro de la categoría (3) es un subconjunto tetradimensional topológicamente abierto del espacio-tiempo. Cabe aclarar que esta teoría solo parece funcionar con la rígida condición de dos marcos de referencia solamente. Cuando se agrega un marco de referencia adicional, la teoría de la Relatividad queda invalidada: el observador A en la Tierra percibirá que el observador B viaja a mayor velocidad dentro de una nave espacial girando alrededor de la Tierra a 7000 kilómetros por segundo. El observador B notará que el dato de tiempo al reloj. se ha desacelerado y concluye que el tiempo se ha dilatado por causa de la velocidad de la nave. Un observador C localizado fuera del sistema solar, notará que tanto el hombre en tierra como el astronauta girando alrededor de la Tierra, están viajando simultáneamente —la nave espacial y el planeta Tierra— a 28 kilómetros por segundo alrededor del Sol. La más certera conclusión acerca del comportamiento del reloj en la nave espacial, es que ese reloj está funcionando mal, porque no fue calibrado ni probado para esos nuevos cambios en su ambiente. Esta conclusión está respaldada por el hecho que no existe prueba alguna que muestre que el tiempo es objetivo.

Solo si dos sucesos están atados causalmente todos los observadores ven el suceso "causal" antes que el suceso "efecto", es decir, las categorías (1) de eventos pasados y (2) de eventos futuros causalmente ligados sí son absolutos. Fijado un evento "E" el conjunto de eventos de la categoría (3) que no son eventos ni futuros ni pasados respecto a "E" puede dividirse en tres subconjuntos:

Las curiosas relaciones causales de la teoría de la relatividad, conllevan a que no existe un tiempo único y absoluto para los observadores, de hecho cualquier observador percibe el espacio-tiempo o espacio tetradimensional según su estado de movimiento, la dirección paralela a su cuadrivelocidad coincidirá con la dirección temporal, y los eventos que acontecen en las hipersuperficies espaciales perpendiculares en cada punto a la dirección temporal, forman el conjunto de acontecimientos simultáneos para ese observador.

Lamentablemente, dichos conjuntos de acontecimientos percibidos como simultáneos difieren de un observador a otro.

Si el tiempo propio es la duración de un suceso medido en reposo respecto a ese sistema, la duración de ese suceso medida desde un sistema de referencia que se mueve con velocidad constante con respecto al suceso viene dada por:

En mecánica cuántica debe distinguirse entre la mecánica cuántica convencional, en la que puede trabajarse bajo el supuesto clásico de un tiempo absoluto, y la mecánica cuántica relativista, dentro de la cual, al igual que sucede en la teoría de la relatividad, el supuesto de un tiempo absoluto es inaceptable e inapropiado.

Se ha señalado que la dirección del tiempo está relacionada con el aumento de entropía, aunque eso parece deberse a las peculiares condiciones que se dieron durante el Big Bang. Aunque algunos científicos como Penrose han argumentado que dichas condiciones no serían tan peculiares si consideramos que existe un principio o teoría física más completa que explique por qué nuestro universo, y tal vez otros, nacen con condiciones iniciales aparentemente improbables, que se reflejan en una bajísima entropía inicial.

La cronología (histórica, geológica, etc.) permite datar los momentos en los que ocurren determinados hechos (lapsos relativamente breves) o procesos (lapsos de duración mayor). En una línea de tiempo se puede representar gráficamente los momentos históricos en puntos y los procesos en segmentos.

Las formas e instrumentos para medir el tiempo son de uso muy antiguo, y todas ellas se basan en la medición del movimiento, del cambio material de un objeto a través del tiempo, que es lo que puede medirse. En un principio, se comenzaron a medir los movimientos de los astros, especialmente el movimiento aparente del Sol, dando lugar al tiempo solar aparente. El desarrollo de la astronomía hizo que, de manera paulatina, se fueron creando diversos instrumentos, tales como los relojes de sol, las clepsidras o los relojes de arena y los cronómetros. Posteriormente, la determinación de la medida del tiempo se fue perfeccionando hasta llegar al reloj atómico. Todos los relojes modernos desde la invención del reloj mecánico, han sido construidos con el mismo principio del "tic tic tic". El reloj atómico está calibrado para contar 9 192 631 770 vibraciones del átomo de Cesio para luego hacer un "tic".





</doc>
<doc id="2764" url="https://es.wikipedia.org/wiki?curid=2764" title="Temperatura">
Temperatura

La temperatura es una magnitud referida a las nociones comunes de calor medible mediante un termómetro. En física, se define como una magnitud escalar relacionada con la energía interna de un sistema termodinámico, definida por el principio cero de la termodinámica. Más específicamente, está relacionada directamente con la parte de la energía interna conocida como «energía cinética», que es la energía asociada a los movimientos de las partículas del sistema, sea en un sentido traslacional, rotacional, o en forma de vibraciones. A medida que sea mayor la energía cinética de un sistema, se observa que este se encuentra más «caliente»; es decir, que su temperatura es mayor.

En el caso de un sólido, los movimientos en cuestión resultan ser las vibraciones de las partículas en sus sitios dentro del sólido. En el caso de un gas ideal monoatómico se trata de los movimientos traslacionales de sus partículas (para los gases multiatómicos los movimientos rotacional y vibracional deben tomarse en cuenta también).

El desarrollo de técnicas para la medición de la temperatura ha pasado por un largo proceso histórico, ya que es necesario darle un valor numérico a una idea intuitiva como es lo frío o lo caliente.

Multitud de propiedades fisicoquímicas de los materiales o las sustancias varían en función de la temperatura a la que se encuentren, como por ejemplo su estado (sólido, líquido, gaseoso, plasma), su volumen, la solubilidad, la presión de vapor, su color o la conductividad eléctrica. Así mismo es uno de los factores que influyen en la velocidad a la que tienen lugar las reacciones químicas.

La temperatura se mide con termómetros, los cuales pueden ser calibrados de acuerdo a una multitud de escalas que dan lugar a unidades de medición de la temperatura. En el Sistema Internacional de Unidades, la unidad de temperatura es el kelvin (K), y la escala correspondiente es la escala Kelvin o escala absoluta, que asocia el valor «cero kelvin» (0 K) al «cero absoluto», y se gradúa con un tamaño de grado igual al del grado Celsius. Sin embargo, fuera del ámbito científico el uso de otras escalas de temperatura es común. La escala más extendida es la escala Celsius, llamada «centígrada»; y, en mucha menor medida, y prácticamente solo en los Estados Unidos, la escala Fahrenheit.

La temperatura es una propiedad física que se refiere a las nociones comunes de calor o ausencia de calor, sin embargo su significado formal en termodinámica es más complejo.
Termodinámicamente se habla de la velocidad promedio o la energía cinética (movimiento) de las partículas de las moléculas, siendo de esta manera, a temperaturas altas, la velocidad de las partículas es alta, en el cero absoluto las partículas no tienen movimiento.
A menudo el calor o el frío percibido por las personas tiene más que ver con la sensación térmica (ver más abajo), que con la temperatura real. Fundamentalmente, la temperatura es una propiedad que poseen los sistemas físicos a nivel macroscópico, la cual tiene una causa a nivel microscópico, que es la energía promedio por la partícula. Y actualmente, al contrario de otras cantidades termodinámicas como el calor o la entropía, cuyas definiciones microscópicas son válidas muy lejos del equilibrio térmico, la temperatura solo puede ser medida en el equilibrio, precisamente porque se define como un promedio.

La temperatura está íntimamente relacionada con la energía interna y con la entalpía de algún sistema: a mayor temperatura mayores serán la energía interna y la entalpía del sistema.

La temperatura es una propiedad intensiva, es decir, que no depende del tamaño del sistema, sino que es una propiedad que le es inherente y no depende ni de la cantidad de sustancia ni del material del que este compuesto.

Antes de dar una definición formal de temperatura, es necesario entender el concepto de equilibrio térmico. Si dos partes de un sistema entran en contacto térmico es probable que ocurran cambios en las propiedades de ambas. Estos cambios se deben a la transferencia de calor entre las partes. Para que un sistema esté en equilibrio térmico debe llegar al punto en que ya no hay intercambio neto de calor entre sus partes, además ninguna de las propiedades que dependen de la temperatura debe variar.

Una definición de temperatura se puede obtener de la Ley cero de la termodinámica, que establece que si dos sistemas A y B están en equilibrio térmico, con un tercer sistema C, entonces los sistemas A y B estarán en equilibrio térmico entre sí. Este es un hecho empírico más que un resultado teórico. Ya que tanto los sistemas A, B, y C están todos en equilibrio térmico, es razonable decir que comparten un valor común de alguna propiedad física. Llamamos a esta propiedad "temperatura".

Sin embargo, para que esta definición sea útil es necesario desarrollar un instrumento capaz de dar un significado cuantitativo a la noción cualitativa de esa propiedad que presuponemos comparten los sistemas A y B. A lo largo de la historia se han hecho numerosos intentos, sin embargo en la actualidad predominan el sistema inventado por Anders Celsius en 1742 y el inventado por William Thomson (más conocido como lord Kelvin) en 1848.

También es posible definir la temperatura en términos de la segunda ley de la termodinámica, la cual dice que la entropía de todos los sistemas, o bien permanece igual o bien aumenta con el tiempo, esto se aplica al Universo entero como sistema termodinámico. La entropía es una medida del desorden que hay en un sistema. 

Este concepto puede ser entendido en términos estadísticos, considere una serie de tiros de monedas. Un sistema perfectamente ordenado para la serie, sería aquel en que solo cae cara o solo cae cruz. Sin embargo, existen múltiples combinaciones por las cuales el resultado es un desorden en el sistema, es decir que haya una fracción de caras y otra de cruces. Un sistema desordenado podría ser aquel en el que hay 90 % de caras y 10 % de cruces, o 60 % de caras y 40 % de cruces. Sin embargo es claro que a medida que se hacen más tiros, el número de combinaciones posibles por las cuales el sistema se desordena es mayor; en otras palabras el sistema evoluciona naturalmente hacia un estado de desorden máximo es decir 50 % caras 50 % cruces de tal manera que cualquier variación fuera de ese estado es altamente improbable.

Para dar la definición de temperatura con base en la segunda ley, habrá que introducir el concepto de máquina térmica la cual es cualquier dispositivo capaz de transformar calor en trabajo mecánico. En particular interesa conocer el planteamiento teórico de la máquina de Carnot, que es una máquina térmica de construcción teórica, que establece los límites teóricos para la eficiencia de cualquier máquina térmica real.

En una máquina térmica cualquiera, el trabajo que esta realiza corresponde a la diferencia entre el calor que se le suministra y el calor que sale de ella. Por lo tanto, la eficiencia es el trabajo que realiza la máquina dividido entre el calor que se le suministra:

Donde "W" es el trabajo hecho por la máquina en cada ciclo. Se ve que la eficiencia depende solo de "Q" y de "Q". Ya que "Q" y "Q" corresponden al calor transferido a las temperaturas "T" y "T", es razonable asumir que ambas son funciones de la temperatura:

Sin embargo, es posible utilizar a conveniencia, una escala de temperatura tal que

Sustituyendo la ecuación (3) en la (1) relaciona la eficiencia de la máquina con la temperatura:

Hay que notar que para "T" = 0 K la eficiencia se hace del 100 %, temperaturas inferiores producen una eficiencia aún mayor que 100 %. Ya que la primera ley de la termodinámica prohíbe que la eficiencia sea mayor que el 100 %, esto implica que la mínima temperatura que se puede obtener en un sistema microscópico es de 0 K. Reordenando la ecuación (4) se obtiene:

Aquí el signo negativo indica la salida de calor del sistema. Esta relación sugiere la existencia de una función de estado "S" definida por:

Donde el subíndice indica un proceso reversible. El cambio de esta función de estado en cualquier ciclo es cero, tal como es necesario para cualquier función de estado. Esta función corresponde a la entropía del sistema, que fue descrita anteriormente. Reordenando la ecuación siguiente para obtener una definición de temperatura en términos de la entropía y el calor:

Para un sistema en que la entropía sea una función de su energía interna "E", su temperatura está dada por:

Esto es, el recíproco de la temperatura del sistema es la razón de cambio de su entropía con respecto a su energía.

Las escalas de medición de la temperatura se dividen fundamentalmente en dos tipos, las relativas y las absolutas. Los valores que puede adoptar la temperatura en cualquier escala de medición, no tienen un nivel máximo, sino un nivel mínimo: el cero absoluto. Mientras que las escalas absolutas se basan en el cero absoluto, las relativas tienen otras formas de definirse.








Las escalas que asignan los valores de la temperatura en dos puntos diferentes se conocen como "escalas a dos puntos". Sin embargo en el estudio de la termodinámica es necesario tener una escala de medición que no dependa de las propiedades de las sustancias. Las escalas de este tipo se conocen como escalas absolutas o escalas de temperatura termodinámicas.

Con base en el esquema de notación introducido en 1967, en la Conferencia General de Pesos y Medidas (CGPM), el símbolo de grado se eliminó en forma oficial de la unidad de temperatura absoluta.

"Aclaraciones: No se le antepone la palabra" grado "ni el símbolo º. Cuando se escribe la palabra completa, «kelvin», se hace con minúscula, salvo que sea principio de párrafo."


Las siguientes fórmulas asocian con precisión las diferentes escalas de temperatura:

De manera simplificada entre las cuatro escalas que se encuentran vigentes, se puede emplear la siguiente equivalencia:

Para un gas ideal, la teoría cinética de gases utiliza mecánica estadística para relacionar la temperatura con el promedio de la energía total de los átomos en el sistema. Este promedio de la energía es independiente de la masa de las partículas, lo cual podría parecer contraintuitivo para muchos. El promedio de la energía está relacionado exclusivamente con la temperatura del sistema, sin embargo, cada partícula tiene su propia energía la cual puede o no corresponder con el promedio; la distribución de la energía, (y por lo tanto de las velocidades de las partículas) está dada por la distribución de Maxwell-Boltzmann.
La energía de los gases ideales monoatómicos se relaciona con su temperatura por medio de la siguiente expresión:

donde "n", número de moles, "R", constante de los gases ideales. En un gas diatómico, la relación es:

El cálculo de la energía cinética de objetos más complicados como las moléculas, es más difícIl. Se involucran grados de libertad adicionales los cuales deben ser considerados. La segunda ley de la termodinámica establece sin embargo, que dos sistemas al interactuar el uno con el otro adquirirán la misma energía promedio por partícula, y por lo tanto la misma temperatura.

En una mezcla de partículas de varias masas distintas, las partículas más masivas se moverán más lentamente que las otras, pero aun así tendrán la misma energía promedio. Un átomo de Neón se mueve relativamente más lento que una molécula de hidrógeno que tenga la misma energía cinética. Una manera análoga de entender esto es notar que por ejemplo, las partículas de polvo suspendidas en un flujo de agua se mueven más lentamente que las partículas de agua. Para ver una ilustración visual de este hecho vea este enlace. La ley que regula la diferencia en las distribuciones de velocidad de las partículas con respecto a su masa es la ley de los gases ideales.

En el caso particular de la atmósfera, los meteorólogos han definido la temperatura atmosférica (tanto la temperatura virtual como la potencial) para facilitar algunos cálculos.

Es importante destacar que la sensación térmica es algo distinto de la temperatura tal como se define en termodinámica. La sensación térmica es el resultado de la forma en que la piel percibe la temperatura de los objetos y/o de su entorno, la cual no refleja fielmente la temperatura real de dichos objetos y/o entorno. La sensación térmica es un poco compleja de medir por distintos motivos:

Por todo ello, la sensación de comodidad depende de la incidencia combinada de los factores que determinan estos cuatro tipos de intercambio: temperatura seca, temperatura radiante, temperatura húmeda (que señala la capacidad del aire para admitir o no la evaporación del sudor) y la velocidad del aire (que incide sobre la convección y la evaporación del sudor). La incidencia en las pérdidas de la transmisión es pequeña, salvo que la piel, o parte, esté en contacto con objetos fríos (pies descalzos, asiento frío con poca ropa de abrigo...).

Se llama temperatura seca del aire de un entorno (o más sencillamente: "temperatura seca") a la temperatura del aire, prescindiendo de la radiación calorífica de los objetos que rodean ese ambiente concreto, y de los efectos de la humedad relativa y de los movimientos de aire. Se puede obtener con el termómetro de mercurio, respecto a cuyo bulbo, reflectante y de color blanco brillante, se puede suponer razonablemente que no absorbe radiación.

La temperatura radiante tiene en cuenta el calor emitido por radiación de los elementos del entorno.

Se toma con un termómetro de globo, que tiene el depósito de mercurio o bulbo, encerrado en una esfera o "globo" metálico de color negro, para asemejarlo lo más posible a un cuerpo negro y así absorber la máxima radiación. 

Las medidas se pueden tomar bajo el sol o bajo la sombra. En el primer caso se tendrá en cuenta la radiación solar, y se dará una temperatura bastante más elevada.

También sirve para dar una idea de la sensación térmica.

La temperatura de bulbo negro hace una función parecida, dando la combinación de la temperatura radiante y la ambiental.

Temperatura de bulbo húmedo o "temperatura húmeda", es la temperatura que da un termómetro bajo sombra, con el bulbo envuelto en una mecha de algodón húmedo bajo una corriente de aire. La corriente de aire se produce mediante un pequeño ventilador o poniendo el termómetro en un molinete y haciéndolo girar. Al evaporarse el agua, absorbe calor rebajando la temperatura, efecto que reflejará el termómetro. Cuanto menor sea la humedad relativa del ambiente, más rápidamente se evaporará el agua que empapa el paño. Este tipo de medición se utiliza para dar una idea de la sensación térmica, o en los psicrómetros para calcular la humedad relativa y la temperatura del punto de rocío.




</doc>
<doc id="2765" url="https://es.wikipedia.org/wiki?curid=2765" title="Talauma">
Talauma

Talauma es un género de plantas de la familia de las Magnoliáceas, orden "Magnoliales", subclase "Magnoliidae", clase "Magnoliopsida", división "Magnoliophyta".




</doc>
<doc id="2774" url="https://es.wikipedia.org/wiki?curid=2774" title="Tragus">
Tragus

Tragus, es un género de plantas herbáceas perteneciente a la familia de las poáceas. Es originario de África. Comprende 36 especies descritas y de estas, solo 8 aceptadas.
Son plantas anuales o perennes, cespitosas. La lígula es una membrana ciliada; con láminas anchamente lineares, aplanadas. Inflorescencia una panícula cilíndrica espiculífera con espiguillas dispuestas en fascículos de 2-5 sobre pedúnculos cortos, los fascículos desarticulándose como una unidad. Espiguillas similares o las superiores reducidas y estériles, con 1 flósculo bisexual, sin una extensión de la raquilla; glumas marcadamente desiguales, la inferior diminuta, hialina, la superior tan larga como la espiguilla, marcadamente 5-7-nervia con grandes aguijones en las nervaduras; lema 3-nervia, membranácea; pálea casi tan larga como la lema, membranácea, 2-carinada, convexa en el dorso; lodículas 2; estambres 3; estilos 2. Fruto una cariopsis; embrión 2/5-1/2 la longitud de la cariopsis; hilo punteado.
El género fue descrito por Albrecht von Haller y publicado en "Historia Stirpium Indigenarum Helvetiae Inchoata" 2: 203. 1768. La especie tipo es: "Tragus racemosus"
Tragus: nombre genérico posiblemente del griego: "tragos", una parte de la oreja, literalmente, "cabra", o de Hieronymus Tragus, el nombre griego para Jerome Bock (1498-1554), médico, investigador, y uno de los tres padres de la botánica alemana. 
Número de la base del cromosoma, x = 10. 2n = 20 y 40. 2 y 4 ploide. 
A continuación se brinda un listado de las especies del género "Tragus" aceptadas hasta junio de 2015, ordenadas alfabéticamente. Para cada una se indica el nombre binomial seguido del autor, abreviado según las convenciones y usos.



</doc>
<doc id="2775" url="https://es.wikipedia.org/wiki?curid=2775" title="Triplachne nitens">
Triplachne nitens

Triplachne, es un género monotípico de plantas herbáceas perteneciente a la familia de las poáceas. Su única especie: Triplachne nitens (Guss.) Link, es originaria de la región del Mediterráneo.
Es una planta anual con tallos de hasta 25 cm de altura, geniculado-ascendentes o decumbentes, glabros. Hojas glabras, glaucas, con lígula de 1,5-3 mm y limbo de hasta 5 cm x 4 mm. Panícula de hasta 5 x 1 cm, elipsoidea. Espiguillas de c. 4 mm, cuneiformes. Gluma inferior de 3,8-4,1 mm, la superior de 3,1-3,3 mm. Lema de c. 2 mm, con los 2 nervios laterales prolongados en arístulas apicales de 0,8 mm, más o menos hirsuta en la mitad inferior; arista subbasal de c. 3,5 mm, generalmente incluida en las glumas, parda en la parte inferior e hialina en la superior. Anteras de 0,6 mm. Cariopsis de 1 x 0,3 mm. Florece de abril a mayo.
Se encuentra en arenales costeros, en Algeciras (La Línea). Península Ibérica, Baleares, Sicilia, islas del E del Mediterráneo, Norte de África, SW de Asia.
"Triplachne nitens" fue descrita por (Guss.) Link y publicado en "Hortus Regius Botanicus Berolinensis" 2: 241. 1833.



</doc>
<doc id="2777" url="https://es.wikipedia.org/wiki?curid=2777" title="Trisetum">
Trisetum

Trisetum es un género de plantas herbáceas perteneciente a la familia de las poáceas. Es originario del centro y sudoeste de Asia.
Son plantas perennes, cespitosas, a veces con rizomas cortos; cañas de 5-300 cm de alto, erectas o geniculadas en la base, glabras o pilosas bajo la panícula; lígulas membranáceas, truncadas a ovales; láminas planas a conduplicadas, a veces filiformes, glabras o pilosas. Inflorescencia una panícula espiciforme o laxa, contraída o abierta, oval o piramidal; raquis glabro, escabroso o piloso. Espiguillas (1-)2-6-floras, cortamente pediceladas; comprimidas lateralmente; raquilla pilosa, prolongada más allá del antecio superior; desarticulación de la raquilla sobre las glumas y entre los antecios; glumas heteromorfas a subisomorfas, lanceoladas a oval-lanceoladas, iguales o desiguales, menores o mayores que el conjunto de los antecios; gluma inferior 1(-3)-nervia, usualmente más angosta y más corta que la superior; gluma superior 3(-5)-nervia; lemmas lanceoladas, (3-)5(-7)-nervias, con el ápice y márgenes hialinos, glabras o pilosas en el dorso, levemente aquilladas y comprimidas, ápice con 2(-4) arístulas o sétulas cortas, 2-dentado o más raramente entero o subentero; arista inserta en el tercio superior, medio o subapical, exserta, usualmente tan larga como la lemma, geniculada o sólo divaricada, retorcida o no, raramente recta; callo corto, obtuso, con pelos cortos; páleas biaquilladas, hialinas, usualmente más cortas que las lemmas; estambres 3, anteras de 0,3-4,5 mm de largo; lodículas 2, hialinas, con 2 o 3 lóbulos en el ápice; ovario glabro o con pelos cortos y brillantes en el ápice; cariopsis comprimida, blanda; hilo corto, punctiforme; endosperma líquido, semilíquido, raramente sólido, seco. Número básico de cromosomas x = 7.
El género fue descrito por Christiaan Hendrik Persoon y publicado en "Synopsis Plantarum" 1: 97. 1805. La especie tipo es: "Trisetum flavescens" (L.) P. Beauv.
El nombre del género deriva del latín "tri" = (tres) y "setum" = (cerdas), aludiendo a los lemas con tres aristas. 



</doc>
<doc id="2780" url="https://es.wikipedia.org/wiki?curid=2780" title="Teatro">
Teatro

El teatro (del griego: "θέατρον", "theátron" o «lugar para contemplar» derivado de "θεάομαι", "theáomai" o «mirar») es la rama de las artes escénicas relacionada con la actuación, que representa historias actuadas frente a los espectadores o frente a una cámara usando una combinación de discurso, gestos, escenografía, música, sonido o espectáculo.

También se entiende por «teatro» al género literario que comprende las obras de teatro representadas ante un público o para ser grabadas y reproducidas en cine ante un público, así como a la edificación donde se presentan tradicionalmente dichas obras o grabaciones. En adición a la narrativa común, el estilo de diálogo, el teatro también toma otras formas como la ópera, el ballet, el cine, la ópera china o la pantomima.

El Día Mundial del Teatro se celebra desde 1961, cada 27 de marzo.

La mayoría de los estudios consideran que los orígenes del teatro deben buscarse en la evolución de los rituales mágicos relacionados con la caza, al igual que las pinturas rupestres, o la recolección agrícola que, tras la introducción de la música y la danza, se embocaron en auténticas ceremonias dramáticas donde se rendía culto a los dioses y se expresaban los principios espirituales de la comunidad. Este carácter de manifestación sagrada resulta un factor común a la aparición del teatro en todas las civilizaciones.

Las manifestaciones dramáticas en Asia se remontan a épocas antiquísimas. En China se practicaban ya, bajo la forma de poemas escenificados, a fines del segundo milenio antes de la era cristiana. En la India su aparición también es antigua. Allí, en el Mahabhárata, poema épico que adquirió su forma definitiva hacia el siglo IV a. C., se menciona específicamente este arte revela la existencia de remotas formas teatrales relacionadas con las creencias védicas. El carácter marcadamente ritual y simbólico del teatro oriental que determinó un protagonismo de la música y la danza muy superior a la occidental, marcó asimismo en Japón, el desarrollo del teatro 能, en español llamado "Nō" surgido en el siglo XV, del que dos centurias más tarde brotó el 歌舞伎 o "kabuki", el más popular, y basado sobre todo en la capacidad de los intérpretes, y en la apelación a los sentidos antes que al intelecto.

En Antiguo Egipto, a mediados del segundo milenio antes de la edad cristiana, se representaban ya dramas acerca de la muerte y resurrección de Osiris. Se comienza el teatro por medio de máscaras y dramatizaciones con ellas.

Las raíces del teatro de la antigua Grecia están basadas en los ritos órficos y en los festivales celebrados para Dioniso, donde se llevaban a cabo las escenificaciones de la vida de los dioses acompañadas de danzas y cantos (Ditirambos). Más tarde comenzaron las primeras representaciones ya propiamente dramáticas, ejecutadas en las plazas de los pueblos por compañías que incluían solo un actor y un coro. A fines del Siglo VI a.C alcanzó extraordinaria celebridad el legendario poeta e intérprete Tespis, en cuyo honor la frase "el carro de Tespis" alude, aún hoy, al conjunto del mundo del teatro.

El teatro griego surge tras la evolución de las artes y ceremonias griegas como la fiesta de la vendimia (ofrecida a Dionisios) donde los jóvenes iban danzando y cantando hacia el templo del dios, a ofrecerle las mejores vides. Luego un joven que resaltó entre el grupo de jóvenes se transformó en el Corifeo o maestro del coro, quien dirigió al grupo. Con el tiempo aparecieron el bardo y el rapsoda, que eran recitadores.

En el curso del siglo V a. C., durante la edad clásica de Grecia, se establecieron los modelos tradicionales de la tragedia y la comedia, y los dramaturgos Esquilo y Sófocles añadieron respectivamente un segundo y tercer actor a la acción, lo que dio a ésta una complejidad que hacía necesaria la creación de mayores escenarios. Para ello se erigieron grandes teatros de piedra, entre los que cabe citar el aún conservado de Epidauro en el siglo V a. C., capaz de albergar unas 12.000 personas, y el de Dioniso, en Atenas, en el siglo IV A.C.

Su construcción se realizaba mediante el aprovechamiento de las faldas de una colina, donde se disponían en forma semicircular las gradas que rodeaban la "orquestra", espacio circular en el que se efectuaba la mayor parte de la representación. Tras la "orquestra" se levantaba una edificación llamada "skené", escena, destinada a que los actores cambiaran su vestimenta. Delante de ella se levantaba una pared columnada, el proscenio, que podía sostener superficies pintadas que evocaban el lugar de la acción. Estos decorados, junto con las túnicas y máscaras empleadas por los actores y máquinas, constituían todo el aparato escénico.

Las representaciones del teatro griego se hacían al aire libre, contaba con coro (dirigido por el Corifeo o maestro del coro) que cantaba el coro y danzaba en torno a un altar. En el teatro griego se representaban dos tipos de obras: la tragedia, obra dramática de final desgraciado que trataba de temas de leyendas heroicas y utilizaba, oportunamente, a los dioses para su final, y la comedia satírica, que criticaba humorísticamente a políticos y a las obras e incurrían en una mímica iniciada por un coro de sátiros, y comedias que tenían por tema asuntos de la vida cotidiana; todas estaban escritas en verso y utilizaban máscaras.

Los teatros romanos heredaron los rasgos fundamentales de los griegos, si bien introdujeron ciertos elementos distintivos. Construidos inicialmente en madera, sólo en el año 52 a. C. Pompeyo, erigió en Roma el primero en piedra. A diferencia de sus modelos helénicos, se levantaban sobre el suelo plano y poseían varias plantas erigidas en mampostería. Con objeto de mejorar la acústica, los arquitectos romanos redujeron la orquesta a un semicírculo, y los espectáculos se presentaban sobre una plataforma, el "pulpitum", levantada delante de la antigua "skene" que constituye el origen de los modernos escenarios. La "frons scaenae" era una fachada monumental de varios pisos, que servía de fondo de escenario. El graderío ("cávea") se divide en 3 partes: "Ima", "media" y "suma ", ubicándose la primera en la zona inferior donde se sentaban los senadores y la clase dirigente; quedando asentados en la superior las mujeres y los esclavos y en la media el pueblo llano. El conjunto podía cubrirse con un "velum". Roma optó también por la comedia, ya que éstos tomaron el teatro como una manera de divertirse o entretenerse.

En las culturas americanas prehispánicas el teatro llegó a adquirir un notable desarrollo, particularmente entre los mayas. Una de las obras más representativas del teatro maya es el drama quiche Rabinal Achí. El teatro maya se hallaba parcialmente vinculado a los ciclos agrícolas y a la épica de sus eventos históricos, y entre los aztecas e Incas, sociedades que en correspondencia con su estructura teocrática dieron a sus actividades teatrales un matiz eminentemente guerrero y religioso.

Tras siglos de misterioso olvido, acaso por las prohibiciones durante la inquisición, la recuperación del teatro en Occidente tuvo principal apoyo en el clero, que lo empleó con fines religiosos. Así, desde el siglo XI, fue habitual la representación en las iglesias de "misterios" y "moralidades", cuyo objeto era presentar de forma sencilla la doctrina cristiana a los fieles. A fin de facilitar la comprensión, el latín cedió paso paulatinamente a las lenguas vernáculas, y en los siglos XIII y XIV, tanto las piezas religiosas como las florecientes farsas profanas comenzaron a representarse.

La eclosión del Renacimiento en Italia tuvo consecuencias decisivas sobre la evolución del teatro, pues, al surgir una producción dramática de carácter culto, inspirada en los modelos clásicos y destinada a las clases aristocráticas, se generalizó en el transcurso del siglo XVI la construcción de salas cubiertas y dotadas de mayores comodidades.

Como primero de los teatros modernos suele citarse el Olímpico de Vicenza, diseñado por Andrea Palladio y finalizado en 1585, que constituía una versión de los modelos romanos y presentaba, al fondo del escenario, una perspectiva tridimensional con vistas urbanas. El modelo clásico del teatro italiano, vigente en muchos aspectos, fue no obstante el teatro Farnese de Parma, erigido en 1618, cuya estructura incluía el escenario, enmarcado por un arco proscenio y separado del público por un telón, y una platea en forma de herradura rodeada por varios pisos de galerías. Durante este tiempo se desarrolló también en Italia una forma de teatro popular, la comedia del arte, que con su énfasis en la libertad de improvisación del actor dio un gran avance a la técnica interpretativa.

Muy diferentes fueron los teatros erigidos en Inglaterra durante el reinado de Isabel I de Inglaterra, época de excepcional esplendor del género dramático, entre los que se destacó el londinense The Globe donde presentaba sus obras William Shakespeare. Carentes de techo y construidos de madera, su rasgo más característico era el escenario elevado rectangular, en torno al cual el público rodeaba a los actores por tres lados, mientras las galerías se reservaban para la nobleza.

En España, y en la misma época que el teatro Isabelino en Inglaterra (siglos XVI y XVII) se crean instalaciones fijas para el teatro al aire libre denominadas Corrales de Comedias, con las que guardan similitudes constructivas. A diferencia del caso inglés, en España si han pervivido algunos ejemplos de estas edificaciones. Exponentes de esta época son los autores Lope de Vega, Tirso de Molina y Calderón de la Barca, claros exponentes del importante Siglo de Oro español.

El transcurso de los siglos XVII y XVIII dio lugar a un gran enriquecimiento de la escenografía. La recuperación por parte del drama clásico francés de la regla de las tres unidades —acción, tiempo y lugar— hizo innecesaria la simultaneidad de decorados, con lo que se empleó sólo uno en cada acto, y pronto se generalizó la costumbre de cambiarlos en los entreactos. Posteriormente, la creciente popularidad de la ópera, que requería varios montajes, favoreció el desarrollo de máquinas perfeccionadas que dieran mayor apariencia de veracidad a efectos tales como: la desaparición de actores y la simulación de vuelos —las llamadas "glorias", por ejemplo hacían posible el descenso de las alturas del escenario de una nube que portaba a los cantantes. El teatro de la Scala de Milán, finalizado en 1778, constituye un ejemplo de las grandes dimensiones que eran precisas para albergar tanto al público como a la tramoya y al aparato escénico.

Durante la mayor parte del siglo XIX las ideas arquitectónicas y escenográficas se mantuvieron en esencia inalterables, si bien las exigencias de libertad creativa iniciadas por los autores románticos condujeron a fines de la centuria a un replanteamiento general del arte dramático en sus diversos aspectos.

Fundamental en este sentido fue la construcción del monumental Festspielhaus de Bayreuth, Alemania, erigido en 1876 de acuerdo con las instrucciones del compositor Richard Wagner, que constituyó la primera ruptura respecto a los modelos italianos. Su diseño en abanico, con la platea escalonada, el oscurecimiento del auditorio durante su representación y la ubicación de la orquesta en un pequeño foso, eran elementos concebidos para centrar la atención de los espectadores sobre la acción y abolir en lo posible la separación entre escenario y público.

Esta exigencia de integración entre el marco arquitectónico, la escenografía y la representación fue acentuada en los últimos decenios del siglo XIX y primeros del XX por la creciente importancia concedida a la figura del director gracias a personalidades como el alemán Max Reinhardt, autor de espectaculares montajes, el francés André Antoine, adalid del naturalismo, el ruso Konstantín Stanislavski, director y actor cuyo método de interpretación ejercería gran influencia sobre el teatro moderno, o el escenógrafo británico Edward Gordon Craig, que en su defensa de un teatro poético y estilizado abogó por la creación de escenarios más sencillos y dúctiles.

La aparición del teatro moderno, pues, se caracterizó por su absoluta libertad de planteamiento mediante el diálogo con formas tradicionales y las nuevas posibilidades técnicas darían lugar a una singular transformación del arte teatral. En el campo del diseño arquitectónico y escenográfico las mayores innovaciones se debieron al desarrollo de nueva maquinaria y al auge adquirido por el arte de la iluminación, circunstancias que permitieron la creación de escenarios dotados de mayor plasticidad (circulares, móviles, transformables, etc.) y liberaron al teatro de la apariencia pictórica proporcionada por la estructura clásica del arco del proscenio.

El teatro africano, entre tradición e historia, se está encauzando actualmente por nuevas vías. Todo predispone en África al teatro. El sentido del ritmo y de la mímica, la afición por la palabra y la verborrea son cualidades que todos los africanos comparten en mayor o menor medida y que hacen de ellos actores natos. La vida cotidiana de los africanos transcurre al ritmo de variadas ceremonias, rituales o religiosas, concebidas y vividas generalmente como verdaderos espectáculos. No obstante, aunque África ha conocido desde siempre este tipo de ceremonias, cabe preguntarse si se trataba realmente de teatro; a los ojos de muchos, estos espectáculos están demasiado cargados de significado religioso para que puedan considerarse como tal. Otros estiman que los tipos de teatro africanos guardan cierto parecido, como en otros tiempos la tragedia griega, como un preteatro que nunca llegara totalmente a ser teatro si no se desacraliza. La fuerza y las posibilidades de supervivencia del teatro negro residirán, por lo tanto, en su capacidad para conservar su especificidad. En el África independiente está tomando forma un nuevo teatro.


Un drama es un modo específico de ficción representado en una obra de teatro. El término proviene de una palabra griega que significa "acción", la cual deriva del verbo δράω, "dráō", "hacer" o "actuar". La puesta en escena de un drama en el teatro, es realizada por actores en un escenario frente a una audiencia, presupone la adopción de modos colaborativos de producción y una forma colectiva de recepción. A diferencia de otras formas de literatura, la estructura dramática de los textos, se encuentra directamente influenciada por esta producción colaborativa y recepción colectiva. La tragedia de comienzos de la edad moderna "Hamlet" (1601) de Shakespeare y la tragedia clásica ateniense "Oedipus Rex" (c. 429 AdC) de Sófocles son algunas de las mejores obras de arte dramático. Un ejemplo moderno sería "Largo viaje hacia la noche" de Eugene O'Neill (1956).

El modo dramático ha sido considerado un género de la poesía, y se lo ha contrastado con los modos épico y lírico comenzando con la "Poética" de Aristóteles (c. 335 AdC), la obra más antigua sobre teoría dramática. El uso de la palabra "drama" en un sentido estricto se utiliza para hacer referencia un tipo específico de obra de teatro del siglo XIX. En este sentido drama se refiere a una obra que no es ni una comedia ni una tragedia, por ejemplo, la obra "Thérèse Raquin" (1873) de Émile Zola o la obra "Ivanov" (1887) de Chekhov . Sin embargo en la antigua Grecia, la palabra "drama" abarcaba todos los tipos de obras de teatro, tragedias, comedias, y otras formas intermedias.

A menudo el drama es combinado con elementos de música y danza: por lo general en la ópera la totalidad del texto del drama es cantado; los musicales por su parte por lo general contienen tanto diálogo hablado como canciones; y algunas formas de drama incluyen música incidental o un acompañamiento musical que acompaña y refuerza el diálogo (por ejemplo el melodrama y el Nō japonés). En ciertos períodos históricos (la Antigua Roma y el Romanticismo moderno) algunos dramas fueron escritos para ser leídos en vez de para ser puestos en escena. En la improvisación, el drama no existe previo al momento de la obra; los actores crean y desarrollan un argumento dramático de manera espontánea ante la audiencia.

La frase de Aristóteles "los diversos tipos asociados con las distintas partes de la obra" es una referencia a los orígenes estructurales del drama. En el mismo las diversas partes con diálogo eran escritas en el dialecto del Ática mientras que las partes corales (recitados o cantados) se realizaban en dialecto dórico, estas discrepancias reflejaban los distintos orígenes religiosos y las métricas poéticas de las partes que eran fusionadas en una nueva entidad, el "drama" teatral.

La tragedia se encuentra entroncada con una tradición específica de drama que ha desempeñado un rol único y muy importante en la definición histórica de la civilización occidental. La tradición ha tenido múltiples expresiones discontinuas, el término a menudo ha sido utilizado para hacer referencia a un poderoso efecto de identidad cultural y continuidad histórica—"los Griegos y los Isabelinos, como un formato cultural; los Helenos y Cristianos, en una actividad cotidiana," tal como lo plantea Raymond Williams. Desde sus orígenes oscuros en los teatros de Atenas hace 2,500 años, de donde ha sobrevivido solo una fracción de las obras de Esquilo, Sófocles y Eurípides, mediante las elaboraciones particulares a través de las obras de Shakespeare, Lope de Vega, Racine, y Schiller, hasta la más recientes tragedias naturalistas de Strindberg, las meditaciones modernistas de Beckett sobre la muerte, la pérdida, y el sufrimiento, y los retrabajos postmodernistas de Müller del canon trágico, la tragedia ha continuado siendo un ámbito importante de experimentación cultural, negociación, lucha y cambio. En la senda de la "Poética" (335 AdC) de Aristóteles, la tragedia ha sido utilizada para marcar distinciones de género, ya sea con la poesía en general (donde lo trágico se contrapone con lo épico y lírico ) o con el drama (en el cual la tragedia se enfrenta a la comedia).

El teatro como se ha podido observar, constituye un todo orgánico del que sus diferentes elementos forman una parte indisoluble. Esos elementos, no obstante, poseen cada uno características y leyes propias y, en función de la época, de la personalidad del director o de otras circunstancias, es habitual que se conceda a unos u otros mayor relevancia dentro del conjunto. Estos elementos son:

Las obras dramáticas se escriben en diálogos y en primera persona, en el que existe las acciones que van entre paréntesis, (llamado lenguaje de acotaciones).

En la tradición occidental, el texto, la obra dramática, se ha considerado siempre la pieza esencial del teatro, llamado "el arte de la palabra". Dado que, de forma más matizada, esta orientación predomina también en las culturas orientales, cabe cuando menos admitir como justificada tal primacía. A este respecto deben hacerse, no obstante, dos consideraciones: en primer lugar, el texto no agota el hecho teatral, pues una obra dramática no es teatro hasta que se representa, lo que implica como mínimo el elemento de la actuación; en segundo lugar, son numerosas las formas dramáticas arcaicas y los espectáculos modernos que prescinden por completo de la palabra o la subordinan a elementos cual la mímica, la expresión corporal, la danza, la música, el despliegue escénico.

El hecho de que la obra sólo adquiera plena vigencia en la representación determina además el carácter distintivo de la escritura dramática respecto a otros géneros literarios. La mayoría de los grandes dramaturgos de todos los tiempos, desde los clásicos griegos al inglés William Shakespeare, el francés Molière, el español Pedro Calderón de la Barca o el alemán Bertolt Brecht, basaron sus creaciones en un conocimiento directo y profundo de los recursos escénicos e interpretativos y en una sabia utilización de sus posibilidades.

La personalidad del director como artista creativo se consolidó a fines del siglo XIX, aunque su figura ya existía como coordinador de los elementos teatrales, desde la escenografía a la interpretación. A él corresponde convertir el texto, si existe, en teatro, con los procedimientos y objetivos que se precisen. Poderosos ejemplos de dicha tarea fueron los alemanes Bertolt Brecht y Erwin Piscator, dedicando su energía a conseguir del espectador su máxima capacidad de reflexión, o el ascetismo del polaco Jerzy Grotowski.

Las técnicas de actuación han variado enormemente a lo largo de la historia y no siempre de manera uniforme. En el teatro occidental clásico, por ejemplo los grandes actores, los "monstruos sagrados", tendían a enfatizar las emociones con objeto de destacar el contenido de la obra, en "la comedia del arte" el intérprete dejaba rienda suelta a su instinto; los actores japoneses del "Nō" y "kabuki", hacen patentes determinados estados de ánimo por medio de gestos simbólicos, bien de gran sutileza o deliberadamente exagerados.

En el teatro moderno se ha impuesto por lo general la orientación naturalista, en que el actor por medio de adquisición de técnicas corporales y psicológicas y del estudio de sí mismo y del personaje, procura recrear en escena la personalidad de éste. Tal opción, evolucionada en sus rasgos fundamentales a partir de las enseñanzas del ruso Konstantín Stanislavski y muy extendida en el ámbito cinematográfico, no es desde luego la única y en último extremo la elección de un estilo interpretativo depende de características del espectáculo y de las indicaciones del director.

Sin embargo, actualmente, a inicios del siglo XXI, la actuación teatral con tendencia naturalista está siendo replanteada seriamente. La teatralidad contemporánea requiere una crítica del naturalismo como simple reproducción del comportamiento humano, pero sin lazos con su entorno. Actualmente ha habido grandes transformaciones del trabajo de Stanislavski siendo las más importantes Antonin Artaud, Jerzy Grotowsky Étienne Decroux y Eugenio Barba. Estas técnicas, llamadas actualmente extra cotidianas implican una compleja síntesis de los signos escénicos.

De forma estricta, se entiende por decorado al ambiente en que se desarrolla una representación dramática, y por escenografía, al arte de crear los decorados. Hoy en día, tiende a introducirse en el concepto de "aparato escenográfico" a todos los elementos que permiten la creación de ese ambiente, entre los que cabría destacar fundamentalmente a la maquinaria o tramoya y la iluminación.

A lo largo del tiempo y en diferentes momentos de la historia del teatro, la escenografía ha sufrido importantes transformaciones. Antes de que el teatro existiera como lo conocemos ahora las representaciones se realizaban con un sentido ritual y en ellas ya se utilizaban los decorados para dar más realce, misterio, ambientación e imagen escénica y espectacularidad a los actos rituales.

En el teatro griego se utilizaban los "periactos" que eran unos apuntadores de base triangular, tenían estos unas mamparas o paneles prismáticos, en cuyos planos o caras se dibujaban, distintos decorados, de acuerdo a los requerimientos de la escena que se estaba representando.

En la antigüedad, la escenografía se hallaba condicionada a limitaciones técnicas y arquitectónicas, circunstancia que se mantuvo durante toda la Edad Media. Fue ya a fines del Renacimiento y, sobre todo, durante los siglos XVII y XVIII, cuando la escenografía comenzó a adquirir realce, gracias al perfeccionamiento de la perspectiva pictórica, que permitió dotar de mayor apariencia de profundidad al decorado, y posteriormente al desarrollo de la maquinaria teatral. En el siglo XIX, con la introducción del drama realista, el decorado se convirtió en el elemento básico de la representación. El descubrimiento de la luz eléctrica, en fin, dio pie al auge de la iluminación. Las candilejas, que en principio eran un elemento accesorio, se consideran poéticamente un símbolo del arte teatral.

Estrechamente vinculado con la concepción escénica, se ha hallado siempre el vestuario. En el teatro griego, la tosquedad de los decorados se compensaba por medio de máscaras —trágicas o cómicas— y las túnicas estilizadas de los actores, cuyo objeto era de resaltar el carácter arquetípico de los personajes. Durante el Barroco y el Neoclasicismo adquirieron importancia el maquillaje y el vestuario, si bien éste se empleó a menudo de forma anacrónica —se representaba por ejemplo una obra ambientada en Roma con ropajes franceses del siglo XVII hasta la aparición del realismo. En la actualidad, la elección del vestuario no es sino un elemento más dentro de la concepción general del montaje.

El narrador de parábolas hace bien en mostrar abiertamente al espectador todo lo que necesita para su parábola, esos elementos cuya ayuda pretende mostrar el curso ineluctable de su acción. El constructor escénico de la parábola muestra pues abiertamente los focos, los instrumentos de música, las máscaras, las paredes y las puertas, las escaleras, sillas y mesas, con cuya ayuda ha de construirse la parábola.

En la disposición tradicional a la italiana, la sala frente al escenario suele tener una forma de herradura, en los teatros más antiguos. La parte baja, la más amplía, es la platea o patio de butacas, donde los sillones o butacas se reparten en filas separadas por un pasillo central y enmarcadas por dos pasillos laterales. En los teatros más antiguos, el piso del patio de butacas es plano y ligeramente inclinado para preservar un mínimo de visibilidad. En los teatros contemporáneos, detrás del patio de butacas se encuentran los palcos y un anfiteatro en gradas que permite una buena visibilidad del escenario desde las filas más alejadas.

Para mayor aprovechamiento del espacio disponible en altura, la sala se estructura en varias plantas. Sobre el patio de butacas pueden existir una o dos amplías plantas voladas y retranqueadas. Los paramentos centrales y laterales se dedican a los palcos o a galerías abalconadas que se reparten en varias plantas. Tradicionalmente, la parte más alta del teatro se denomina gallinero; es la de menor visibilidad y la más económica.

Así, de mayor a menor precio de la entrada, el teatro se estructura en platea (planta baja), palcos (situados en la entreplanta) y anfiteatro (situados en las plantas superiores).

Son muchas las supersticiones que se han conservado en el medio teatral de la cultura de Occidente, creencias y costumbres que han ido perdiendo fuerza en tiempos más recientes pero que aún determinan el «modus operandi» en diferentes aspectos del espectáculo. De la larga lista de supersticiones se pueden mencionar:






</doc>
<doc id="2781" url="https://es.wikipedia.org/wiki?curid=2781" title="Texto">
Texto

Un texto es una composición de signos codificados en un sistema de escritura que forma una unidad de sentido. 

También es una composición de caracteres imprimibles (con grafema) generados por un algoritmo de cifrado que, aunque no tienen sentido para cualquier persona, sí puede ser descifrado por su destinatario original. En otras palabras, un texto es un entramado de signos con una intención comunicativa que adquiere sentido en determinado contexto.

Las ideas esenciales que comunica un texto están contenidas en lo que se suele denominar «macroproposiciones», unidades estructurales de nivel superior o global, que otorgan coherencia al texto constituyendo su hilo central, el esqueleto estructural que cohesiona elementos lingüísticos formales de alto nivel, como los títulos y subtítulos, la secuencia de párrafos, etc. En contraste, las «microproposiciones» son los elementos coayudantes de la cohesión de un texto, pero a nivel más particular o local. Esta distinción fue realizada por Teun van Dijk en 1980.

El nivel microestructural o local está asociado con el concepto de cohesión. Se refiere a uno de los fenómenos propios de la coherencia, el de las relaciones particulares y locales que se dan entre elementos lingüísticos, tanto los que remiten unos a otros como los que tienen la función de conectar y organizar. 
También es un conjunto de oraciones agrupadas en párrafos que habla de un tema determinado.

De acuerdo a Greimas, es un enunciado ya sea gráfico o fónico que nos permite visualizar las palabras que escuchamos que es utilizado para manifestar el proceso lingüístico. Mientras Hjelmslev usa ese término para designar el todo de una cadena lingüística ilimitada (§1).

En lingüística, no todo conjunto de signos constituye un texto.

Se le llama texto a la configuración de lengua o habla y se utilizan signos específicos (signo de la lengua o habla) y está organizada según reglas del habla o idioma.

Otra noción importante es que los textos (y discursos) no son sólo "monologales".
En lingüística, el término texto sirve tanto para producciones en que sólo hay un emisor (situaciones monogestionadas o monocontroladas) como en las que varios intercambian sus papeles (situaciones poligestionadas o policontroladas) como las conversaciones. El texto contiene conectores y signos, etc.

Ejemplos :

Este texto o conjunto de signos extraídos de un discurso debe reunir condiciones de textualidad. Las principales son:


Según los lingüistas Beaugrande y Dressler, todo texto bien elaborado ha de presentar siete características:


Así pues, un texto ha de ser coherente, cohesionado, comprensible para su lector ideal, intencionado, enmarcado en una situación comunicativa e inmerso en otros textos o géneros para alcanzar sentido; igualmente ha de poseer información en grado suficiente para resultar novedoso e interesante.

A fin de agrupar y clasificar la enorme diversidad de textos, se han propuesto . Estas se basan en distintos criterios como la función que cumple el texto en relación con los interlocutores o la estructura global interna que presenta.

La clasificación más simple de los textos, en función de las características que predominan en cada uno (se considera que no hay texto puro, es decir, no hay texto que tenga rasgos correspondientes únicamente a cada categoría, todo texto es híbrido), es como sigue:

expositivos


</doc>
<doc id="2791" url="https://es.wikipedia.org/wiki?curid=2791" title="Troyano">
Troyano

Troyano hace referencia a varios artículos:



</doc>
<doc id="2792" url="https://es.wikipedia.org/wiki?curid=2792" title="Tetris">
Tetris

Tetris es un videojuego de puzzle originalmente diseñado y programado por Alekséi Pázhitnov en la Unión Soviética. Fue lanzado el 6 de junio de 1984, mientras trabajaba para el Centro de Computación Dorodnitsyn de la Academia de Ciencias de la Unión Soviética en Moscú, RSFS de Rusia. Su nombre deriva del prefijo numérico griego "tetra"- (todas las piezas del juego, conocidas como Tetrominós que contienen cuatro segmentos) y del tenis, el deporte favorito de Pázhitnov.

En el "Tetris" se juega con los tetrominós, el caso especial de cuatro elementos de poliominós. Los poliominós se han utilizado en los rompecabezas populares por lo menos desde 1907, y el nombre fue dado por el matemático Solomon W. Golomb en 1953. Sin embargo, incluso la enumeración de los pentominós data de la antigüedad.

El juego (o una de sus muchas variantes) está disponible para casi cada consola de videojuegos y sistemas operativos de PC, así como en dispositivos tales como las calculadoras gráficas, teléfonos móviles, reproductores de multimedia portátiles, PDAs, reproductores de música en red e incluso como huevo de pascua en productos no mediáticos como los osciloscopios. También ha inspirado servicios de mesa y ha sido jugado en los costados de varios edificios, manteniendo el récord de ser el juego completamente funcional más grande del mundo gracias al esfuerzo de estudiantes holandeses en 1995 que iluminaron 15 pisos del Departamento de Ingeniería Eléctrica en la Universidad Técnica de Delft.

Aunque diferentes versiones de Tetris se habían vendido para una amplia gama de plataformas de ordenadores domésticos y arcades durante los años 1980, fue la inmensamente exitosa versión portátil para la Game Boy lanzada en 1989 la que lo convirtió en uno de los juegos más populares de todos los tiempos. La edición número 100 del "Electronic Gaming Monthly" otorgó a Tetris el número 1 en el ránking de "Mejores juegos de todos los tiempos". En 2007, Tetris ocupó el segundo lugar en los "100 mejores videojuegos de todos los tiempos" para IGN. Ha vendido más de 170 millones de copias para el año 2016. En enero de 2010, se anunció que el Tetris había vendido más de 100 millones de unidades para teléfonos móviles sólo desde el año 2005.

Alekséi Pázhitnov (Sokolov) se había inspirado en un juego de pentaminós que había comprado anteriormente. El nombre "tetris" deriva del étimo griego "tetra", que significa "cuatro", y hace referencia a la cantidad de cuadros que componen las piezas. Alekséi Pázhitnov programó una versión de su juego en un Electrónika 60, según la leyenda en una sola tarde. Hay que tener en cuenta que lo realmente complejo fue llegar a la idea original del juego y no la programación en sí misma. Hoy por hoy la mecánica del juego es muy conocida y es sencillo emularla.

"Tetris" comienza a ganar popularidad cuando Vadim Gerasimov, un joven de 16 años que trabajaba en la Academia, portó el juego a IBM PC. Desde ahí se distribuye gratuitamente a Hungría, donde es programado para Apple II y Commodore 64 por programadores húngaros. Estas versiones llaman la atención de Robert Stein, que intenta adquirir los derechos del juego. Antes de conseguir estos derechos, vende el concepto robado a la empresa inglesa Mirrorsoft y a su filial estadounidense: Spectrum Holobyte, que editan una versión para Atari ST y Sinclair ZX Spectrum. "Tetris" se comercializa en Europa y Estados Unidos en 1987 con la mención: "«Fabricado en Estados Unidos, creado en el extranjero»".

Tetris ha sido históricamente uno de los videojuegos más versionados y es, junto a las Torres de Hanói, el predilecto por los programadores noveles de juegos. Lucharon por robarse la idea y licenciarla Atari y Nintendo, lográndolo finalmente este último gracias a Henk Rogers. Fue el juego que acompañaría a su novedosa consola portátil Game Boy en su debut, lo que popularizó tanto "Tetris" como la consola por todo el mundo.

En 1991, Alekséi Pázhitnov emigra a Estados Unidos y, cinco años después, en 1996 funda su propia compañía, "Tetris Company", junto con Henk Rogers y se apropia de los derechos de autor.

Después del éxito de este juego, muchos otros trataron de imitarlo. Juegos como Columns o Collapse son ejemplos que han intentado seguir la estela que dejó Tetris, un juego que inauguró un género dentro del panorama arcade. Atari, por su parte, como "contraataque" por perder la licencia de este videojuego, sacó al mercado Klax, un juego de habilidad e inteligencia con una temática similar al Columns o Puyo Puyo.

Existe una versión gratuita muy popular en Internet denominada "TetriNET", que proporciona una versión multijugador en arquitectura cliente-servidor, en el que se pueden enfrentar a través de la red de 2 a 6 personas con la posibilidad de crear equipos. Fue creada en 1997 por St0rmCat y se encuentran en la actualidad clientes para los sistemas operativos Windows (el propio TetriNET y "Blocktrix", entre otros), GNU/Linux ("GTetrinet") y Mac OS X ("Tetrinet Aqua"). La singularidad de esta versión es que añade unos bonus especiales llamados en inglés "cookies" que permiten alterar el juego de los adversarios.

Distintos tetriminos, figuras geométricas compuestas por cuatro bloques cuadrados unidos de forma ortogonal, las cuales se generan de una zona que ocupa 5x5 bloques en el área superior de la pantalla. No hay consenso en cuanto a las dimensiones para el área del juego, variando en cada versión. Sin embargo, dos filas de más arriba están ocultas al jugador.
El jugador no puede impedir esta caída, pero puede decidir la rotación de la pieza (0°, 90°, 180°, 270°) y en qué lugar debe caer. Cuando una línea horizontal se completa, esa línea desaparece y todas las piezas que están por encima descienden una posición, liberando espacio de juego y por tanto facilitando la tarea de situar nuevas piezas. La caída de las piezas se acelera progresivamente. El juego acaba cuando las piezas se amontonan hasta llegar a lo más alto (3x5 bloques en el área visible), interfiriendo la creación de más piezas y finalizando el juego.

Existen distintas versiones del juego. La original tiene siete piezas diferentes. Licencias posteriores añadieron formas suplementarias y existen incluso ciertas licencias para formas tridimensionales.

La versión original de Pajitnov para la computadora Electronika 60 utilizaba soportes verdes para representar bloques.
Algunas personas se refieren a las piezas mediante el color del que están pintadas en una versión particular del juego “Tetris”, pero antes de la estandarización por parte de The Tetris Company en el año 2000, dichos colores han variado versión tras versión, con lo cual llamar a las piezas por su color carece de sentido. Por ejemplo, la pieza T es diferente en todas las versiones del juego.

Es una variante del Tetris, en la que el objetivo no es formar las clásicas líneas, sino más bien juntar grupos de tres o más bloques del mismo color. Los "quirks" son bloques de cierto color emparejados en una pieza. Estas piezas caen en principio como en el Tetris clásico, siendo el objetivo el planteado líneas arriba. Tiene varias modalidades, entre la que se destaca el modo "Duelo" contra la computadora y es representada por rivales virtuales con formas de animales. Al derrotar a cada rival, la dificultad y la velocidad del juego aumentan.

Cabe destacar que, cada vez que los jugadores juntan un número específico de bloques del mismo color, caen más "quirks" en la zona del contrincante.

El objetivo en esta variación es también formar líneas horizontales con los bloques, aunque aquí hay una serie de nuevas reglas, las cuales son las siguientes:

El fundamento del juego son las poliformas conocidas como poliominós, más concretamente las combinaciones de tetraminós. Si, por ejemplo, se produce una larga secuencia de piezas en forma de Z, en algún momento el jugador estará obligado a dejar un hueco en la esquina derecha, sin poder rellenar el hueco anterior. Ahora se produce una nueva secuencia de piezas en forma de S, y así hasta que las piezas se amontonan y acaba el juego.

Como la distribución de las piezas es aleatoria, esta secuencia terminará ocurriendo. En la práctica, esto no sucede porque el generador de números pseudoaleatorios utilizado en la mayor parte de las implementaciones es un generador lineal de congruencias que no devuelve una secuencia así.

Incluso con un generador teóricamente perfecto de números aleatorios y gravedad ingenua, un buen jugador podrá resistir la caída de 150 piezas, todas en forma de S o Z. La probabilidad de que, en un momento dado, las próximas 150 piezas sean todas así es de 1 / (7/2) (aproximadamente 1 / (4 × 10)). Este número tiene el mismo orden de magnitud que el número de átomos que hay en el universo conocido.

Se ha demostrado que varios de los subproblemas de Tetris son NP-completo.

De acuerdo con el Dr. Richard Haier, jugar al Tetris de forma prolongada puede llevar a una actividad cerebral más eficiente durante el juego. Cuando se juega a Tetris por primera vez, aumentan la función y actividad cerebral, incrementándose también el consumo de energía cerebral, medido por la tasa metabólica de la glucosa. A medida que el jugador de Tetris se vuelve más hábil, el cerebro reduce su consumo de energía y glucosa, indicando una actividad cerebral más eficiente para el juego. Jugar al Tetris de forma moderada (media hora al día por un período de tres meses) incrementa las funciones cognitivas tales como "pensamiento crítico, razonamiento, procesamiento del lenguaje" y aumenta también el espesor de la corteza cerebral.

En enero de 2009, un grupo de investigación de la Universidad de Oxford dirigido por la Dra. Emily Holmes informó en PLoS ONE que en voluntarios sanos, jugar Tetris poco después de ver material traumático en el laboratorio redujo el número de flashbacks que esas escenas provocaban a la semana siguiente. Creen que el juego puede interrumpir los recuerdos que se conservan de las visiones y los sonidos que se presenciaron, y que luego se vuelven a experimentar a través de flashbacks involuntarios y angustiantes. El grupo espera desarrollar más este enfoque como una posible intervención para reducir los flashbacks experimentados en el trastorno de estrés postraumático, pero enfatizó que estos son solo resultados preliminares.

Otro efecto notable es que, de acuerdo con un estudio canadiense de abril de 2013, se ha encontrado que jugar Tetris ayuda a los adolescentes con ambliopía (ojo vago), lo cual es mejor que tapar con un parche el ojo sano de la víctima para entrenar su ojo más débil. El Dr. Robert Hess, del equipo de investigación, dijo: "Es mucho mejor que aplicar parches, es mucho más divertido, es más rápido y parece funcionar mejor". Cuando se probó en el Reino Unido también pareció ayudar a los niños con ese problema.

El juego puede provocar que se imaginen combinaciones de Tetris de forma involuntaria aun cuando no se esté jugando (el llamado Efecto Tetris), aunque esto puede ocurrir con cualquier videojuego o situación real que proyecte las imágenes o escenarios de forma repetida, tales como rompecabezas.





</doc>
<doc id="2793" url="https://es.wikipedia.org/wiki?curid=2793" title="Terminal">
Terminal

Terminal hace referencia a varios artículos:

En cualquier transporte público, al sitio de llegada o salida de la línea:











</doc>
<doc id="2795" url="https://es.wikipedia.org/wiki?curid=2795" title="Tales de Mileto">
Tales de Mileto

Tales de Mileto (en griego antiguo: Θαλῆς ὁ Μιλήσιος Thalḗs o Milḗsios; Mileto, c. 624 a. C.-"ibidem", c. 546 a. C.) fue un filósofo, matemático, geómetra, físico y legislador griego. 

Vivió y murió en Mileto, polis griega de la costa jonia (hoy en Turquía). Fue el iniciador de la Escuela de Mileto a la que pertenecieron también Anaximandro (su discípulo) y Anaxímenes (discípulo del anterior). En la antigüedad se le consideraba uno de los Siete Sabios de Grecia. No se conserva ningún texto suyo y es probable que no dejara ningún escrito a su muerte. Desde el siglo V a. C. se le atribuyen importantes aportaciones en el terreno de la filosofía, la matemática, la astronomía, la física, etc., así como un activo papel como legislador en su ciudad natal.

A menudo Tales es considerado el iniciador de la especulación científica y filosófica griega y occidental, aunque su figura y aportaciones están rodeadas de grandes incertidumbres.

Se suele aceptar que Tales comenzó a usar el pensamiento deductivo aplicado a la geometría, y se le atribuye la enunciación de dos teoremas geométricos que llevan su nombre.

Los datos biográficos de Tales de Mileto son una mezcla de opiniones, hechos atribuidos a su persona, y citas con mayor o menor grado de verosimilitud, recogidas de diversos autores de épocas bastante posteriores, reinterpretados y expuestos a la luz de la mentalidad del narrador.

Tales de Mileto nació en la ciudad de Mileto (griego Μίλητος, turco: Milet) c. 624 a. C., una antigua ciudad en la costa occidental de Asia Menor (en lo que actualmente es la provincia de Aydın en Turquía), cerca de la desembocadura del río Menderes. La mayoría de los historiadores lo presentan como genuino milesio (aunque, según Diógenes Laercio, doxógrafo griego, fue admitido en la ciudad jonia de Mileto, a orillas del mar Egeo, después de ser expulsado de Fenicia junto con Nileo). Nacido o no en Mileto, es incuestionable que residió en aquella ciudad, y que fue allí donde desarrolló su filosofía, sus investigaciones científicas y sus intervenciones políticas.

Era hijo de Euxamias (o Examio) y de Cleobulinas (o Cleóbula), ambos oriundos de Fenicia y descendientes de Cadmo y Agenor. Puesto que los jonios comerciaban frecuentemente con Egipto y Babilonia, es probable que Tales visitara Egipto en alguna etapa de su vida, y allí podría, por un lado, haber recibido enseñanzas de los sacerdotes, quienes registraban con mucho celo todo evento astronómico o meteorológico excepcional por motivos religiosos y que poseían, por consiguiente, copiosa información al respecto; y, por el otro, haber adquirido conocimientos matemáticos, que los egipcios habían desarrollado a un nivel práctico con el fin de medir y delimitar las parcelas de tierra cuyos límites solían borrarse con las continuas crecidas del río Nilo.

Podrían haber sido condiscípulos suyos Solón y Ferécides de Siros, y una fuente lo vincula con Pitágoras, a quien habría recomendado viajar a Egipto y educarse con los sacerdotes de Menfis y Dióspolis, pero estos datos en absoluto son confiables, puesto que provienen de fuentes muy alejadas de la época de Tales. De los babilonios pudo también haber obtenido conocimientos científicos. Sí es más seguro que el filósofo Anaximandro haya sido su discípulo, así como Anaxímenes el de este.

Tanto Heródoto (I, 170) como Diógenes Laercio (I, 25) lo señalan como un sabio consejero político de jonios y lidios.

Entre las anécdotas que de Tales se cuentan, refiere Heródoto (I, 75) que logró desviar el río Halys para que fuera cruzado por el ejército de Creso (Heródoto mismo descree de esto, pero modernos especialistas no descartan por completo su veracidad). Aristóteles, por su parte, cuenta en su "Política" (I, 11, 1259a) cómo una vez que, habiéndosele reprochado su pobreza y su falta de preocupación por los asuntos materiales, y luego de haber previsto, gracias a sus conocimientos astronómicos, que habría una próspera cosecha de aceitunas la siguiente temporada, compró durante el invierno todas las prensas de aceite de Mileto y Quíos y las alquiló al llegar la época de la recolección, acumulando una gran fortuna y mostrando así que los filósofos pueden ser ricos si lo desean, pero que su ambición es bien distinta. Quizás la anécdota más conocida de Tales es aquella que nos refiere Heródoto: que predijo a los jonios el año en que sucedería un eclipse solar (lo que desde 2005 se sabe que fue por el conocimiento de un ciclo de eclipses babilónico), hacia el año 585 a. C. El eclipse ocurrió, en efecto, en medio de una batalla, lo que llevó a los contendientes a detenerse y a avanzar un acuerdo de paz, por temor de que el evento fuera una advertencia divina.

También es muy conocido lo que cuenta Platón en su diálogo Teeteto (174 A): que, al caer Tales en un pozo por ir mirando el movimiento de las estrellas, una campesina tracia se reía mientras el filósofo se excusaba diciendo «que tenía ansias de conocer las cosas del cielo pero que lo que estaba... justo a sus pies se le escapaba»

Apolodoro, en sus "Crónicas", afirma que murió a la edad de setenta y ocho años; Sosícrates, que murió en la olimpiada LVIII, a la edad de noventa años. Otra fecha en la que se afirma que murió se da en el año 585 a. C., aunque actualmente se acepta que murió cerca del año 546 a. C.

Simplicio de Cilicia escribió: «Se dice de Tales que no dejó nada escrito, excepto la llamada "Astrología náutica" "(Ναυτιχῆς αστρολογίας)"».

En cambio Diógenes Laercio escribe: «Según algunos, nada dejó escrito, pues dicen que la "Astrología náutica" que se le atribuye es de Foco Samio [...] Pero, según otros, escribió dos obras: "Sobre el solsticio" y "Sobre el equinoccio"».

Así, son tres las líneas de opinión: que solo escribió la "Astrología", que solo escribió "Sobre el solsticio" y "Sobre el equinoccio" y que no escribió nada. De cualquier manera, lo cierto es que, de haber escrito algo, sus escritos se perdieron pronto, y, respecto de las pocas fuentes que citan presuntos dichos de Tales, no puede determinarse con certeza si tales fuentes tenían en sus manos o bien escritos de Tales o bien fuentes secundarias o si solo repetían tradiciones orales.

Se atribuyen a Tales varios descubrimientos matemáticos registrados en los "Elementos" de Euclides: la definición I. 17 y las proposiciones I. 5, I. 15, I. 26 y III. 31.
Asimismo es muy conocida la leyenda acerca de un método de comparación de sombras que Tales habría utilizado para medir la altura de las pirámides egipcias: el milesio se percató de que se podría saber la altura exacta de las pirámides midiendo la sombra de estas en el momento del día en que su sombra era más o menos de igual tamaño que su cuerpo. Este método fue aplicado luego a otros fines prácticos de la navegación.
Se supone además que Tales conocía ya muchas de las bases de la geometría, como el hecho de que cualquier diámetro de un círculo lo dividiría en partes idénticas, que un triángulo isósceles tiene por fuerza dos ángulos iguales en su base o las propiedades relacionales entre los ángulos que se forman al cortar dos paralelas por una línea recta perpendicular.

Los egipcios habían aplicado algunos de estos conocimientos para la división y parcelación de sus terrenos. Esta necesidad surgió a raíz de que el Nilo, con sus constantes crecidas, borraba las líneas divisorias de los campos de cultivo, por lo que era necesaria una manera de medir de nuevo el terreno. Mas, según los pocos datos con los que se cuenta, Tales se habría dedicado en Grecia mucho menos al espacio (a las superficies) y mucho más a las líneas y a las curvas, alcanzando así su geometría un mayor grado de complejidad y abstracción.

Se considera a Tales de Mileto como el primer filósofo de Occidente por haber sido quien intentó la primera explicación racional a distintos fenómenos del mundo de la que se tiene constancia en la historia de la cultura occidental. En su tiempo predominaban aún las concepciones míticas, pero Tales buscaba una explicación racional, lo que se conoce como «el paso del mito al logos», donde la palabra griega "logos" alude en este contexto a «razón», uno de sus significados en castellano.

La filosofía de Tales de Mileto no se conoce de primera mano, pues no ha sobrevivido ningún escrito de Tales (de hecho, ni siquiera es seguro que haya escrito algo). Las afirmaciones registradas que se le atribuyen probablemente hayan llegado a los transmisores por segunda mano o incluso por tradición oral; entre las ideas que se le atribuyen, no es posible establecer a ciencia cierta cuánto es realmente de lo que Tales dijo como tampoco si Tales se expresó en los mismos términos en que sus ideas se han transmitido. En cuanto a su filosofía, contamos con el importante aporte de Aristóteles, el cual, en su descripción, diferencia los dichos atribuibles con alguna certeza al mismo Tales («Tales dijo que...») de los hechos dudosos («dicen que Tales dijo que...») y de sus propias opiniones («quizá Tales quiso decir que...»). Aristóteles lo considera, en su relato de las ideas metafísicas ("Metafísica", libro A) como el primero que se dedicó a investigar las primeras causas y los primeros principios, señalándolo así como el primer filósofo y fundador de la filosofía natural.

Cabe destacar que en su época, estos primeros filósofos (los presocráticos) no trataban acerca de ética, política o moral, de hecho se les considera físicos porque teorizaban racionalmente sobre el origen del universo, se dedicaban al estudio de la naturaleza y empezaron a estudiar el campo de las matemáticas, geometría y aritmética.

La explicación universal y racional que sostuvo Tales fue que el agua es origen de todas las cosas que existen, el elemento primero:

En cuanto al alma, la considera como dadora de vida, movimiento y divina. Como en la época en la que vive, todavía no se diferenciaba entre seres vivientes y no vivientes. Tales atribuye vida al agua, porque como el agua se mueve sola (véanse los mares o los ríos), esta debe tener alma, puesto que el alma es lo que hace moverse las cosas. Y también es divina (está llena de dioses) porque el alma es divina para él. «Así por lo tanto, el agua para Tales es, el origen de todo, está llena de dioses y tiene vida propia». Y de manera parecida que con el agua, razona para con las piedras imán. Como estas se mueven solas, piensa que están vivas, o que «hay algo vivo en ellas».

Y por último, de nuevo Aristóteles en "Acerca del cielo" y Séneca en "Cuestiones naturales" afirman que Tales sostenía que la tierra sobre la que pisamos es una especie de isla que «flota» sobre el agua de forma parecida a un leño y por ello la tierra a veces tiembla. Al no estar sostenida sobre unas bases fijas si no que como está flotando sobre el agua, esta la hace tambalearse.

Con todo esto, se puede entender claramente por qué se considera a Tales de Mileto como el primer filósofo de Occidente, y es que, como ya hemos dicho, fue el primer hombre occidental (del que se sabe) que trató de conocer la verdad del mundo mediante explicaciones racionales y no fantásticas o místicas, como hasta entonces se hacía en la Antigua Grecia por medio de los mitos. Y por lo tanto, Tales es verdaderamente importante para la Historia de la filosofía occidental. Fue el iniciador de la misma y con ello, creó un legado de búsqueda y amor a la sabiduría, que continuará inmediatamente con Anaximandro y Anaxímenes, y que llegará a su esplendor, en la Antigua Grecia; más de un siglo después con Sócrates, Platón y Aristóteles: tres filósofos que se han convertido en los pilares del pensamiento que hoy conocemos bajo el nombre de filosofía occidental.

Es muy probable que haya sido uno de los primeros hombres que llevaron la geometría al mundo griego, y Aristóteles lo consideraba el primero de los φυσικόι o ‘filósofos de la naturaleza’. Muchas de estas ideas parecen provenir de su educación egipcia. Igualmente, su idea de que la tierra flota sobre el agua puede haberse desprendido de ciertas ideas cosmogónicas del Oriente próximo.






</doc>
<doc id="2796" url="https://es.wikipedia.org/wiki?curid=2796" title="Telnet">
Telnet

Telnet ("Telecommunication Network") es el nombre de un protocolo de red que nos permite acceder a otra máquina para manejarla remotamente como si estuviéramos sentados delante de ella. También es el nombre del programa informático que implementa el cliente. Para que la conexión funcione, como en todos los servicios de Internet, la máquina a la que se acceda debe tener un programa especial que reciba y gestione las conexiones. El puerto que se utiliza generalmente es el 23.

Telnet sólo sirve para acceder en modo terminal, es decir, sin gráficos, pero es una herramienta muy útil para arreglar fallos a distancia, sin necesidad de estar físicamente en el mismo sitio que la máquina que los tenga. También se usaba para consultar datos a distancia, como datos personales en máquinas accesibles por red, información bibliográfica, etc. 

Aparte de estos usos, en general telnet se ha utilizado (y aún hoy se puede utilizar en su variante SSH) para abrir una sesión con una máquina UNIX, de modo que múltiples usuarios con cuenta en la máquina, se conectan, abren sesión y pueden trabajar utilizando esa máquina. Es una forma muy usual de trabajar con sistemas 
UNIX.

Su mayor problema es de seguridad, ya que todos los nombres de usuario y contraseñas necesarias para entrar en las máquinas viajan por la red como "texto plano" (cadenas de texto sin cifrar). Esto facilita que cualquiera que espíe el tráfico de la red pueda obtener los nombres de usuario y contraseñas, y así acceder él también a todas esas máquinas. Por esta razón dejó de usarse, casi totalmente, hace unos años, cuando apareció y se popularizó el SSH, que puede describirse como una versión cifrada de telnet -actualmente se puede cifrar toda la comunicación del protocolo durante el establecimiento de sesión (RFC correspondiente, en inglés- si cliente y servidor lo permiten, aunque no se tienen ciertas funcionalidad extra disponibles en SSH).

Hoy en día este protocolo también se usa para acceder a los BBS, que inicialmente eran accesibles únicamente con un módem a través de la línea telefónica. Para acceder a un BBS mediante telnet es necesario un cliente que dé soporte a gráficos ANSI y protocolos de transferencia de ficheros. Los gráficos ANSI son muy usados entre los BBS. Con los protocolos de transferencia de ficheros (el más común y el que mejor funciona es el ZModem) podrás enviar y recibir ficheros del BBS, ya sean programas o juegos o ya sea el correo del BBS (correo local, de FidoNet u otras redes).

Algunos clientes de telnet (que soportan gráficos ANSI y protocolos de transferencias de ficheros como Zmodem y otros) son mTelnet!, NetRunner, Putty, Zoc, etc..

Para iniciar una sesión con un intérprete de comandos de otro ordenador, puede emplear el comando "telnet" seguido del nombre o la dirección IP de la máquina en la que desea trabajar, por ejemplo si desea conectarse a la máquina "purpura.micolegio.edu.com" deberá teclear codice_1, y para conectarse con la dirección IP 1.2.3.4 deberá utilizar codice_2.

Una vez conectado, podrá ingresar el nombre de usuario y contraseña remoto para iniciar una sesión en modo texto a modo de consola virtual (ver Lectura Sistema de usuarios y manejo de clave). La información que transmita (incluyendo su clave) no será protegida o cifrada y podría ser vista en otros computadores por los que se transite la información (la captura de estos datos se realiza con un packet sniffer).

Una alternativa más segura para telnet, pero que requiere más recursos del computador, es SSH. Este cifra la información antes de transmitirla, auténtica la máquina a la cual se conecta y puede emplear mecanismos de autenticación de usuarios más seguros.

Actualmente hay sitios para hackers en los que se entra por telnet y se van sacando las password para ir pasando de nivel, ese uso de telnet aún es vigente.

Hay 3 razones principales por las que el telnet no se recomienda para los sistemas modernos desde el punto de vista de la seguridad:


En ambientes donde es importante la seguridad, por ejemplo en el Internet público, telnet no debe ser utilizado. Las sesiones de telnet no son cifradas. Esto significa que cualquiera que tiene acceso a cualquier router, switch, o gateway localizado en la red entre los dos anfitriones donde se está utilizando telnet puede interceptar los paquetes de telnet que pasan cerca y obtener fácilmente la información de la conexión y de la contraseña (y cualquier otra cosa que se mecanografía) con cualesquiera de varias utilidades comunes como "tcpdump" y "Wireshark".

Estos defectos han causado el abandono y despreciación del protocolo telnet rápidamente, a favor de un protocolo más seguro y más funcional llamado SSH, lanzado en 1995. SSH provee de toda la funcionalidad presente en telnet, la adición del "cifrado fuerte" para evitar que los datos sensibles tales como contraseñas sean interceptados, y de la autentificación mediante llave pública, para asegurarse de que el "computador remoto" es realmente quién dice ser.

Los expertos en seguridad computacional, tal como el instituto de SANS, y los miembros del newsgroup de "comp.os.linux.security" recomiendan que el uso del telnet para las conexiones remotas debería ser descontinuado bajo cualquier circunstancia normal.

Cuando el telnet fue desarrollado inicialmente en 1969, la mayoría de los usuarios de computadoras en red estaban en los servicios informáticos de instituciones académicas, o en grandes instalaciones de investigación privadas y del gobierno. En este ambiente, la seguridad no era una preocupación y solo se convirtió en una preocupación después de la explosión del ancho de banda de los años 90. Con la subida exponencial del número de gente con el acceso al Internet, y por la extensión, el número de gente que procura crackear los servidores de otra gente, telnet podría no ser recomendado para ser utilizado en redes con conectividad a Internet.




</doc>
<doc id="2797" url="https://es.wikipedia.org/wiki?curid=2797" title="Tabla periódica de los elementos">
Tabla periódica de los elementos

La tabla periódica de los elementos es una disposición de los elementos químicos en forma de tabla, ordenados por su número atómico (número de protones), por su configuración de electrones y sus propiedades químicas. Este ordenamiento muestra "tendencias periódicas", como elementos con comportamiento similar en la misma columna.

En palabras de Theodor Benfey, la tabla y la ley periódica «son el corazón de la química —comparables a la teoría de la evolución en biología (que sucedió al concepto de la Gran Cadena del Ser), y a las leyes de la termodinámica en la física clásica».

Las filas de la tabla se denominan períodos y las columnas grupos. Algunos grupos tienen nombres. Así por ejemplo el grupo 17 es el de los halógenos y el grupo 18 el de los gases nobles. La tabla también se divide en cuatro bloques con algunas propiedades químicas similares. Debido a que las posiciones están ordenadas, se puede utilizar la tabla para obtener relaciones entre las propiedades de los elementos, o pronosticar propiedades de elementos nuevos todavía no descubiertos o sintetizados. La tabla periódica proporciona un marco útil para analizar el comportamiento químico y es ampliamente utilizada en química y otras ciencias.

Dmitri Mendeléyev publicó en 1869 la primera versión de tabla periódica que fue ampliamente reconocida. La desarrolló para ilustrar tendencias periódicas en las propiedades de los elementos entonces conocidos, al ordenar los elementos basándose en sus propiedades químicas, si bien Julius Lothar Meyer, trabajando por separado, llevó a cabo un ordenamiento a partir de las propiedades físicas de los átomos. Mendeléyev también pronosticó algunas propiedades de elementos entonces desconocidos que anticipó que ocuparían los lugares vacíos en su tabla. Posteriormente se demostró que la mayoría de sus predicciones eran correctas cuando se descubrieron los elementos en cuestión. 

La tabla periódica de Mendeléyev ha sido desde entonces ampliada y mejorada con el descubrimiento o síntesis de elementos nuevos y el desarrollo de modelos teóricos nuevos para explicar el comportamiento químico. La estructura actual fue diseñada por Alfred Werner a partir de la versión de Mendeléyev. Existen además otros arreglos periódicos de acuerdo a diferentes propiedades y según el uso que se le quiera dar (en didáctica, geología, etc).

Se han descubierto o sintetizado todos los elementos de número atómico del 1 (hidrógeno) al 118 (oganesón); la IUPAC confirmó los elementos 113, 115, 117 y 118 el 30 de diciembre de 2015, y sus nombres y símbolos oficiales se hicieron públicos el 28 de noviembre de 2016. Los primeros 94 existen naturalmente, aunque algunos solo se han encontrado en cantidades pequeñas y fueron sintetizados en laboratorio antes de ser encontrados en la naturaleza. Los elementos con números atómicos del 95 al 118 solo han sido sintetizados en laboratorios. Allí también se produjeron numerosos radioisótopos sintéticos de elementos presentes en la naturaleza. Los elementos del 95 a 100 existieron en la naturaleza en tiempos pasados pero actualmente no. La investigación para encontrar por síntesis nuevos elementos de números atómicos más altos continúa.

La historia de la tabla periódica está íntimamente relacionada con varios aspectos del desarrollo de la química y la física: 

Aunque algunos elementos como el oro (Au), plata (Ag), cobre (Cu), plomo (Pb) y mercurio (Hg) ya eran conocidos desde la antigüedad, el primer descubrimiento científico de un elemento ocurrió en el , cuando el alquimista Henning Brand descubrió el fósforo (P). En el se conocieron numerosos nuevos elementos, los más importantes de los cuales fueron los gases, con el desarrollo de la química neumática: oxígeno (O), hidrógeno (H) y nitrógeno (N). También se consolidó en esos años la nueva concepción de elemento, que condujo a Antoine Lavoisier a escribir su famosa lista de sustancias simples, donde aparecían 33 elementos. A principios del , la aplicación de la pila eléctrica al estudio de fenómenos químicos condujo al descubrimiento de nuevos elementos, como los metales alcalinos y alcalino-térreos, sobre todo gracias a los trabajos de Humphry Davy. En 1830 ya se conocían 55 elementos. Posteriormente, a mediados del , con la invención del espectroscopio, se descubrieron nuevos elementos, muchos de ellos nombrados por el color de sus líneas espectrales características: cesio (Cs, del latín "caesĭus", azul), talio (Tl, de tallo, por su color verde), rubidio (Rb, rojo), etc. Durante el , la investigación en los procesos radioactivos llevó al descubrimiento en cascada de una serie de elementos pesados (casi siempre sustancias artificiales sintetizadas en laboratorio, con periodos de vida estable muy cortos), hasta alcanzar la cifra de 118 elementos con denominación oficialmente aceptados por la IUPAC en noviembre de 2016.

Lógicamente, un requisito previo necesario a la construcción de la tabla periódica era el descubrimiento de un número suficiente de elementos individuales, que hiciera posible encontrar alguna pauta en comportamiento químico y sus propiedades. Durante los siguientes dos siglos se fue adquiriendo un mayor conocimiento sobre estas propiedades, así como descubriendo muchos elementos nuevos.

La palabra «elemento» procede de la ciencia griega, pero su noción moderna apareció a lo largo del , aunque no existe un consenso claro respecto al proceso que condujo a su consolidación y uso generalizado. Algunos autores citan como precedente la frase de Robert Boyle en su famosa obra "El químico escéptico", donde denomina elementos «ciertos cuerpos primitivos y simples que no están formados por otros cuerpos, ni unos de otros, y que son los ingredientes de que se componen inmediatamente y en que se resuelven en último término todos los cuerpos perfectamente mixtos». En realidad, esa frase aparece en el contexto de la crítica de Robert Boyle a los cuatro elementos aristotélicos. 

A lo largo del , las tablas de afinidad recogieron un nuevo modo de entender la composición química, que aparece claramente expuesto por Lavoisier en su obra "Tratado elemental de química". Todo ello condujo a diferenciar en primer lugar qué sustancias de las conocidas hasta ese momento eran elementos químicos, cuáles eran sus propiedades y cómo aislarlas.

El descubrimiento de gran cantidad de elementos nuevos, así como el estudio de sus propiedades, pusieron de manifiesto algunas semejanzas entre ellos, lo que aumentó el interés de los químicos por buscar algún tipo de clasificación.

A principios del , John Dalton (1766-1844) desarrolló una concepción nueva del atomismo, a la que llegó gracias a sus estudios meteorológicos y de los gases de la atmósfera. Su principal aportación consistió en la formulación de un «atomismo químico» que permitía integrar la nueva definición de elemento realizada por Antoine Lavoisier (1743-1794) y las leyes ponderales de la química (proporciones definidas, proporciones múltiples, proporciones recíprocas). 

Dalton empleó los conocimientos sobre proporciones en las que reaccionaban las sustancias de su época y realizó algunas suposiciones sobre el modo como se combinaban los átomos de las mismas. Estableció como unidad de referencia la masa de un átomo de hidrógeno (aunque se sugirieron otros en esos años) y refirió el resto de los valores a esta unidad, por lo que pudo construir un sistema de masas atómicas relativas. Por ejemplo, en el caso del oxígeno, Dalton partió de la suposición de que el agua era un compuesto binario, formado por un átomo de hidrógeno y otro de oxígeno. No tenía ningún modo de comprobar este punto, por lo que tuvo que aceptar esta posibilidad como una hipótesis a priori. 

Dalton sabía que una parte de hidrógeno se combinaba con siete partes (ocho, afirmaríamos en la actualidad) de oxígeno para producir agua. Por lo tanto, si la combinación se producía átomo a átomo, es decir, un átomo de hidrógeno se combinaba con un átomo de oxígeno, la relación entre las masas de estos átomos debía ser 1:7 (o 1:8 se calcularía en la actualidad). El resultado fue la primera tabla de masas atómicas relativas (o pesos atómicos, como los llamaba Dalton), que fue posteriormente modificada y desarrollada en los años posteriores. Las inexactitudes antes mencionadas dieron lugar a toda una serie de polémicas y disparidades respecto a las fórmulas y los pesos atómicos, que solo comenzarían a superarse, aunque no totalmente, en el congreso de Karlsruhe en 1860.

En 1789 Antoine Lavoisier publicó una lista de 33 elementos químicos, agrupándolos en gases, metales, no metales y tierras. 

Los químicos pasaron el siglo siguiente buscando un esquema de clasificación más preciso. Uno de los primeros intentos para agrupar los elementos de propiedades análogas y relacionarlos con los pesos atómicos se debe al químico alemán Johann Wolfgang Döbereiner (1780-1849) quien en 1817 puso de manifiesto el notable parecido que existía entre las propiedades de ciertos grupos de tres elementos, con una variación gradual del primero al último. Posteriormente (1827) señaló la existencia de otros grupos en los que se daba la misma relación —cloro, bromo y yodo; azufre, selenio y telurio; litio, sodio y potasio—.
A estos grupos de tres elementos se los denominó tríadas. Al clasificarlas, Döbereiner explicaba que el peso atómico promedio de los pesos de los elementos extremos, es parecido al del elemento en medio.
Esto se conoció como la ley de Tríadas. Por ejemplo, para la tríada cloro-bromo-yodo, los pesos atómicos son respectivamente 36, 80 y 127; el promedio es 81, que es aproximadamente 80; el elemento con el peso atómico aproximado a 80 es el bromo, lo cual hace que concuerde con el aparente ordenamiento de tríadas. 

El químico alemán Leopold Gmelin trabajó con este sistema, y en 1843 había identificado diez tríadas, tres grupos de cuatro, y un grupo de cinco. Jean-Baptiste Dumas publicó el trabajo en 1857 que describe las relaciones entre los diversos grupos de metales. Aunque los diversos químicos fueron capaces de identificar las relaciones entre pequeños grupos de elementos, aún tenían que construir un esquema que los abarcara a todos.

En 1857 el químico alemán August Kekulé observó que el carbono está a menudo unido a otros cuatro átomos. El metano, por ejemplo, tiene un átomo de carbono y cuatro átomos de hidrógeno. Este concepto eventualmente se conocería como «valencia».

En 1862 de Chancourtois, geólogo francés, publicó una primera forma de tabla periódica que llamó la «hélice telúrica» o «tornillo». Fue la primera persona en notar la periodicidad de los elementos. Al disponerlos en espiral sobre un cilindro por orden creciente de peso atómico, de Chancourtois mostró que los elementos con propiedades similares parecían ocurrir a intervalos regulares. Su tabla incluye además algunos iones y compuestos. También utiliza términos geológicos en lugar de químicos y no incluye un diagrama; como resultado, recibió poca atención hasta el trabajo de Dmitri Mendeléyev.

En 1864 Julius Lothar Meyer, un químico alemán, publicó una tabla con 44 elementos dispuestos por valencia. La misma mostró que los elementos con propiedades similares a menudo compartían la misma valencia. Al mismo tiempo, William Odling —un químico inglés— publicó un arreglo de 57 elementos ordenados en función de sus pesos atómicos. Con algunas irregularidades y vacíos, se dio cuenta de lo que parecía ser una periodicidad de pesos atómicos entre los elementos y que esto estaba de acuerdo con «las agrupaciones que generalmente recibían». Odling alude a la idea de una ley periódica, pero no siguió la misma. En 1870 propuso una clasificación basada en la valencia de los elementos.

El químico inglés John Newlands produjo una serie de documentos de 1863 a 1866 y señaló que cuando los elementos se enumeran en orden de aumentar el peso atómico, las propiedades físicas y químicas similares se repiten a intervalos de ocho.
Comparó esta periodicidad con las octavas de la música. Esta llamada «ley de las octavas» fue ridiculizada por los contemporáneos de Newlands y la Chemical Society se negó a publicar su obra, porque dejaba de cumplirse a partir del calcio. Newlands fue sin embargo capaz de elaborar una tabla de los elementos y la utilizó para predecir la existencia de elementos faltantes, como el germanio. La Chemical Society solamente reconoció la importancia de sus descubrimientos cinco años después de que se le acreditaran a Mendeléyev, y posteriormente fue reconocido por la Royal Society, 

En 1867 Gustavus Hinrichs, un químico danés, publicó un sistema periódico en espiral sobre la base de los espectros, los pesos atómicos y otras similitudes químicas. Su trabajo fue considerado como idiosincrásico, ostentoso y laberíntico y esto puede haber llevado a que se desaconsejase su reconocimiento y aceptación.

En 1869, el profesor de química ruso Dmitri Ivánovich Mendeléyev publicó su primera Tabla Periódica en Alemania. Un año después Julius Lothar Meyer publicó una versión ampliada de la tabla que había creado en 1864, basadas en la periodicidad de los volúmenes atómicos en función de la masa atómica de los elementos.

Por esta fecha ya eran conocidos 63 elementos de los 90 que existen en la naturaleza. Ambos químicos colocaron los elementos por orden creciente de sus masas atómicas, los agruparon en filas o periodos de distinta longitud y situaron en el mismo grupo elementos que tenían propiedades químicas similares, como la valencia. Construyeron sus tablas haciendo una lista de los elementos en filas o columnas en función de su peso atómico y comenzando una nueva fila o columna cuando las características de los elementos comenzaron a repetirse.

El reconocimiento y la aceptación otorgada a la tabla de Mendeléyev vino a partir de dos decisiones que tomó. La primera fue dejar huecos cuando parecía que el elemento correspondiente todavía no había sido descubierto. No fue el primero en hacerlo, pero sí en ser reconocido en el uso de las tendencias en su tabla periódica para predecir las propiedades de esos elementos faltantes. Incluso pronosticó las propiedades de algunos de ellos: el galio (Ga), al que llamó eka-aluminio por estar situado debajo del aluminio; el germanio (Ge), al que llamó eka-silicio; el escandio (Sc); y el tecnecio (Tc), que, aislado químicamente a partir de restos de un sincrotrón en 1937, se convirtió en el primer elemento producido de forma predominantemente artificial.

La segunda decisión fue ignorar el orden sugerido por los pesos atómicos y cambiar los elementos adyacentes, tales como telurio y yodo, para clasificarlos mejor en familias químicas. En 1913, Henry Moseley determinó los valores experimentales de la carga nuclear o número atómico de cada elemento, y demostró que el orden de Mendeléyev corresponde efectivamente al que se obtiene de aumentar el número atómico.

El significado de estos números en la organización de la tabla periódica no fue apreciado hasta que se entendió la existencia y las propiedades de los protones y los neutrones. Las tablas periódicas de Mendeléyev utilizan el peso atómico en lugar del número atómico para organizar los elementos, información determinable con precisión en ese tiempo. El peso atómico funcionó bastante bien para la mayoría de los casos permitiendo predecir las propiedades de los elementos que faltan con mayor precisión que cualquier otro método conocido entonces. Moseley predijo que los únicos elementos que faltaban entre aluminio (Z = 13) y oro (Z = 79) eran Z = 43, 61, 72 y 75, que fueron descubiertos más tarde. La secuencia de números atómicos todavía se utiliza hoy en día incluso aunque se han descubierto y sintetizado nuevos elementos.

En 1871, Mendeléyev publicó su tabla periódica en una nueva forma, con grupos de elementos similares dispuestos en columnas en lugar de filas, numeradas I a VIII en correlación con el estado de oxidación del elemento. También hizo predicciones detalladas de las propiedades de los elementos que ya había señalado que faltaban, pero deberían existir. Estas lagunas se llenaron posteriormente cuando los químicos descubrieron elementos naturales adicionales. 

En su nueva tabla consigna el criterio de ordenación de las columnas se basan en los hidruros y óxidos que puede formar esos elementos y por tanto, implícitamente, las valencias de esos elementos. Aún seguía dando resultados contradictorios (Plata y Oro aparecen duplicados, y no hay separación entre Berilio y Magnesio con Boro y Aluminio), pero significó un gran avance. Esta tabla fue completada con un grupo más, constituido por los gases nobles descubiertos en vida de Mendeléyev, pero que, por sus características, no tenían cabida en la tabla, por lo que hubo de esperar casi treinta años, hasta 1904, con el grupo o valencia cero, quedando la tabla más completa.

A menudo se afirma que el último elemento natural en ser descubierto fue el francio —designado por Mendeléyev como eka-cesio— en 1939. Sin embargo, el plutonio, producido sintéticamente en 1940, fue identificado en cantidades ínfimas como un elemento primordial de origen natural en 1971.

La disposición de la tabla periódica estándar es atribuible a Horace Groves Deming, un químico americano que en 1923 publicó una tabla periódica de 18 columnas. En 1928 Merck and Company preparó un folleto con esta tabla, que fue ampliamente difundida en las escuelas estadounidenses. Por la década de 1930 estaba apareciendo en manuales y enciclopedias de química. También se distribuyó durante muchos años por la empresa Sargent-Welch Scientific Company.

La tabla periódica de Mendeléyev presentaba ciertas irregularidades y problemas. En las décadas posteriores tuvo que integrar los descubrimientos de los gases nobles, las «tierras raras» y los elementos radioactivos. Otro problema adicional eran las irregularidades que existían para compaginar el criterio de ordenación por peso atómico creciente y la agrupación por familias con propiedades químicas comunes. Ejemplos de esta dificultad se encuentran en las parejas telurio-yodo, argón-potasio y cobalto-níquel, en las que se hace necesario alterar el criterio de pesos atómicos crecientes en favor de la agrupación en familias con propiedades químicas semejantes. 

Durante algún tiempo, esta cuestión no pudo resolverse satisfactoriamente hasta que Henry Moseley (1867-1919) realizó un estudio sobre los espectros de rayos X en 1913. Moseley comprobó que al representar la raíz cuadrada de la frecuencia de la radiación en función del número de orden en el sistema periódico se obtenía una recta, lo cual permitía pensar que este orden no era casual sino reflejo de alguna propiedad de la estructura atómica. Hoy sabemos que esa propiedad es el número atómico (Z) o número de cargas positivas del núcleo.

La explicación que se acepta actualmente de la ley periódica surgió tras los desarrollos teóricos producidos en el primer tercio del siglo XX, cuando se construyó la teoría de la mecánica cuántica. Gracias a estas investigaciones y a desarrollos posteriores, se acepta que la ordenación de los elementos en el sistema periódico está relacionada con la estructura electrónica de los átomos de los diversos elementos, a partir de la cual se pueden predecir sus diferentes propiedades químicas.

En 1945 Glenn Seaborg, un científico estadounidense, sugirió que los actínidos, como los lantánidos, estaban llenando un subnivel f en vez de una cuarta fila en el bloque d, como se pensaba hasta el momento. Los colegas de Seaborg le aconsejaron no publicar una teoría tan radical, ya que lo más probable era arruinar su carrera. Como consideraba que entonces no tenía una carrera que pudiera caer en descrédito, la publicó de todos modos. Posteriormente se encontró que estaba en lo cierto y en 1951 ganó el Premio Nobel de Química por su trabajo en la síntesis de los actínidos.

En 1952, el científico costarricense Gil Chaverri presentó una nueva versión basada en la estructura electrónica de los elementos, la cual permite ubicar las series de lantánidos y actínidos en una secuencia lógica de acuerdo con su número atómico.

Aunque se producen de forma natural pequeñas cantidades de algunos elementos transuránicos, todos ellos fueron descubiertos por primera vez en laboratorios, el primero de los cuales fue el neptunio, sintetizado en 1939. La producción de estos elementos ha expandido significativamente la tabla periódica. Debido a que muchos son altamente inestables y decaen rápidamente, son difíciles de detectar y caracterizar cuando se producen. Han existido controversias relativas a la aceptación de las pretensiones y derechos de descubrimiento de algunos elementos, lo que requiere una revisión independiente para determinar cuál de las partes tiene prioridad, y por lo tanto los derechos del nombre. Flerovio (elemento 114) y livermorio (elemento 116) fueron nombrados el 31 de mayo de 2012. En 2010, una colaboración conjunta entre Rusia y Estados Unidos en Dubná, región de Moscú, Rusia, afirmó haber sintetizado seis átomos de teneso (elemento 117).

El 30 de diciembre de 2015 la IUPAC reconoció oficialmente los elementos 113, 115, 117, y 118, completando la séptima fila de la tabla periódica. El 28 de noviembre de 2016 se anunciaron los nombres oficiales y los símbolos de los últimos cuatro nuevos elementos aprobados hasta la fecha por la IUPAC (Nh, nihonio; Mc, moscovio; Ts, teneso; y Og, oganesón), que sustituyen a las designaciones temporales.

La tabla periódica actual es un sistema donde se clasifican los elementos conocidos hasta la fecha. Se colocan de izquierda a derecha y de arriba abajo en orden creciente de sus números atómicos. Los elementos están ordenados en siete hileras horizontales llamadas periodos, y en 18 columnas verticales llamadas grupos o familias.

Hacia abajo y a la izquierda aumenta el radio atómico y el radio iónico.

Hacia arriba y a la derecha aumenta la energía de ionización, la afinidad electrónica y la electronegatividad.

A las columnas verticales de la tabla periódica se las conoce como grupos o familias. Hay 18 grupos en la tabla periódica estándar. En virtud de un convenio internacional de denominación, los grupos están numerados de 1 a 18 desde la columna más a la izquierda —los metales alcalinos— hasta la columna más a la derecha —los gases nobles—.

Anteriormente se utilizaban números romanos según la última cifra del convenio de denominación de hoy en día —por ejemplo, los elementos del grupo 4 estaban en el IVB y los del grupo 14 en el IVA—. En Estados Unidos, los números romanos fueron seguidos por una letra «A» si el grupo estaba en el bloque s o p, o una «B» si pertenecía al d. En Europa, se utilizaban letras en forma similar, excepto que «A» se usaba si era un grupo precedente al 10, y «B» para el 10 o posteriores. Además, solía tratarse a los grupos 8, 9 y 10 como un único grupo triple, conocido colectivamente en ambas notaciones como grupo VIII. En 1988 se puso en uso el nuevo sistema de nomenclatura IUPAC y se desecharon los nombres de grupo previos. 

Algunos de estos grupos tienen nombres triviales —no sistemáticos—, como se ve en la tabla de abajo, aunque no siempre se utilizan. Los grupos del 3 al 10 no tienen nombres comunes y se denominan simplemente mediante sus números de grupo o por el nombre de su primer miembro —por ejemplo, «el grupo de escandio» para el 3—, ya que presentan un menor número de similitudes y/o tendencias verticales.

La explicación moderna del ordenamiento en la tabla periódica es que los elementos de un grupo poseen configuraciones electrónicas similares y la misma valencia, entendida como el número de electrones en la última capa. Dado que las propiedades químicas dependen profundamente de las interacciones de los electrones que están ubicados en los niveles más externos, los elementos de un mismo grupo tienen propiedades químicas similares y muestran una tendencia clara en sus propiedades al aumentar el número atómico. 


Por ejemplo, los elementos en el grupo 1 tienen una configuración electrónica "ns" y una valencia de 1 —un electrón externo— y todos tienden a perder ese electrón al enlazarse como iones positivos de +1. Los elementos en el último grupo de la derecha son los gases nobles, los cuales tienen lleno su último nivel de energía —regla del octeto— y, por ello, son excepcionalmente no reactivos y son también llamados «gases inertes». 

Los elementos de un mismo grupo tienden a mostrar patrones en el radio atómico, energía de ionización y electronegatividad. De arriba abajo en un grupo, aumentan los radios atómicos de los elementos. Puesto que hay niveles de energía más llenos, los electrones de valencia se encuentran más alejados del núcleo. Desde la parte superior, cada elemento sucesivo tiene una energía de ionización más baja, ya que es más fácil quitar un electrón en los átomos que están menos fuertemente unidos. Del mismo modo, un grupo tiene una disminución de electronegatividad desde la parte superior a la inferior debido a una distancia cada vez mayor entre los electrones de valencia y el núcleo. 

Hay excepciones a estas tendencias, como por ejemplo lo que ocurre en el grupo 11, donde la electronegatividad aumenta más abajo en el grupo. Además, en algunas partes de la tabla periódica como los bloques d y f, las similitudes horizontales pueden ser tan o más pronunciadas que las verticales.

Las filas horizontales de la tabla periódica son llamadas períodos. El número de niveles energéticos de un átomo determina el periodo al que pertenece. Cada nivel está dividido en distintos subniveles, que conforme aumenta su número atómico se van llenando en este orden:

Siguiendo esa norma, cada elemento se coloca según su configuración electrónica y da forma a la tabla periódica. 

Los elementos en el mismo período muestran tendencias similares en radio atómico, energía de ionización, afinidad electrónica y electronegatividad. En un período el radio atómico normalmente decrece si nos desplazamos hacia la derecha debido a que cada elemento sucesivo añadió protones y electrones, lo que provoca que este último sea arrastrado más cerca del núcleo. Esta disminución del radio atómico también causa que la energía de ionización y la electronegatividad aumenten de izquierda a derecha en un período, debido a la atracción que ejerce el núcleo sobre los electrones. La afinidad electrónica también muestra una leve tendencia a lo largo de un período. Los metales —a la izquierda— generalmente tienen una afinidad menor que los no metales —a la derecha del período—, excepto para los gases nobles.

La tabla periódica consta de 7 períodos:

La tabla periódica se puede también dividir en bloques de acuerdo a la secuencia en la que se llenan las capas de electrones de los elementos. Cada bloque se denomina según el orbital en el que el en teoría reside el último electrón: "s", "p", "d" y "f". El bloque s comprende los dos primeros grupos (metales alcalinos y alcalinotérreos), así como el hidrógeno y el helio. El bloque p comprende los últimos seis grupos —que son grupos del 13 al 18 en la IUPAC (3A a 8A en América)— y contiene, entre otros elementos, todos los metaloides. El bloque d comprende los grupos 3 a 12 —o 3B a 2B en la numeración americana de grupo— y contiene todos los metales de transición. El bloque f, a menudo colocado por debajo del resto de la tabla periódica, no tiene números de grupo y se compone de lantánidos y actínidos. Podría haber más elementos que llenarían otros orbitales, pero no se han sintetizado o descubierto; en este caso se continúa con el orden alfabético para nombrarlos. Así surge el bloque g, que es un bloque hipotético.

De acuerdo con las propiedades físicas y químicas que comparten, los elementos se pueden clasificar en tres grandes categorías: metales, metaloides y no metales. Los metales son sólidos generalmente brillantes, altamente conductores que forman aleaciones de unos con otros y compuestos iónicos similares a sales con compuestos no metálicos —siempre que no sean los gases nobles—. La mayoría de los no metales son gases incoloros o de colores; pueden formar enlaces covalentes con otros elementos no metálicos. Entre metales y no metales están los metaloides, que tienen propiedades intermedias o mixtas.

Metales y no metales pueden clasificarse en sub_categorías que muestran una gradación desde lo metálico a las propiedades no metálicas, de izquierda a derecha, en las filas: metales alcalinos —altamente reactivos—, metales alcalinotérreos —menos reactivos—, lantánidos y actínidos, metales de transición y metales post-transición. Los no metales se subdividen simplemente en no metales poliatómicos —que, por estar más cercanos a los metaloides, muestran cierto carácter metálico incipiente—, no metales diatómicos —que son esencialmente no metálicos— y los gases nobles, que son monoatómicos no metálicos y casi completamente inertes. Ocasionalmente también se señalan subgrupos dentro de los metales de transición, tales como metales refractarios y metales nobles.

La colocación de los elementos en categorías y subcategorías en función de las propiedades compartidas es imperfecta. Hay un espectro de propiedades dentro de cada categoría y no es difícil encontrar coincidencias en los límites, como es el caso con la mayoría de los sistemas de clasificación. El berilio, por ejemplo, se clasifica como un metal alcalinotérreo, aunque su composición química anfótera y su tendencia a formar compuestos covalentes son dos atributos de un metal de transición químicamente débil o posterior. El radón se clasifica como un no metal y un gas noble aunque tiene algunas características químicas catiónicas más características de un metal. También es posible clasificar con base en la división de los elementos en categorías de sucesos, mineralógicos o estructuras cristalinas. La categorización de los elementos de esta forma se remonta a por lo menos 1869, cuando Hinrichs escribió que se pueden extraer líneas sencillas de límites para mostrar los elementos que tienen propiedades similares, tales como metales y no metales, o los elementos gaseosos.

Hay tres variantes principales de la tabla periódica, cada una diferente en cuanto a la constitución del grupo 3. Escandio e itrio se muestran de manera uniforme ya que son los dos primeros miembros de este grupo; las diferencias dependen de la identidad de los miembros restantes.

El grupo 3 está formado por Sc, Y, y La, Ac. Lantano (La) y actinio (Ac) ocupan los dos puestos por debajo del itrio (Y). Esta variante es la más común. Hace hincapié en las similitudes de las tendencias periódicas bajando los grupos 1, 2 y 3, a expensas de las discontinuidades en las tendencias periódicas entre los grupos 3 y 4 y la fragmentación de los lantánidos y actínidos.

El grupo 3 está formado por Sc, Y, y Lu, Lr. Lutecio (Lu) y lawrencio (Lr) ocupan los dos puestos por debajo del itrio. Esta variante conserva un bloque f de 14 columnas de ancho, a la vez que desfragmenta a lantánidos y actínidos. Enfatiza las similitudes de tendencias periódicas entre el grupo 3 y los siguientes grupos a expensas de discontinuidades en las tendencias periódicas entre los grupos 2 y 3. 

El grupo 3 está formado por Sc, Y, y 15 lantánidos y 15 actínidos. Las dos posiciones por debajo de itrio contienen los lantánidos y los actínidos (posiblemente por notas al pie). Esta variante enfatiza las similitudes en la química de los 15 elementos lantánidos (La-Lu), a expensas de la ambigüedad en cuanto a los elementos que ocupan las dos posiciones por debajo de itrio del grupo 3, y aparentemente de un bloque f amplio de 15 columnas —solo puede haber 14 elementos en cualquier fila del bloque f—.

Las tres variantes se originan de las dificultades históricas en la colocación de los lantánidos de la tabla periódica, y los argumentos en cuanto a dónde empiezan y terminan los elementos del bloque f. Se ha afirmado que tales argumentos son la prueba de que «es un error de romper el sistema [periódico] en bloques fuertemente delimitados». Del mismo modo, algunas versiones de la tabla dos marcadores han sido criticados por lo que implica que los 15 lantánidos ocupan la caja única o lugar por debajo de itrio, en violación del principio básico de «un lugar, un elemento».

La tabla periódica moderna a veces se expande a su forma larga o de 32 columnas restableciendo los elementos del bloque f a su posición natural entre los bloques s y d. A diferencia de la forma de 18 columnas, esta disposición da como resultado «el aumento sin interrupciones a la secuencia de los números atómicos». También se hace más fácil ver la relación del bloque f con los otros bloques de la tabla periódica. Jensen aboga por una forma de tabla con 32 columnas con base en que los lantánidos y actínidos son relegados en la mente de los estudiantes como elementos opacos y poco importantes que pueden ser puestos en cuarentena e ignorados. A pesar de estas ventajas, los editores generalmente evitan la formulación de 32 columnas porque su relación rectangular no se adapta adecuadamente a la proporción de una página de libro.

Los científicos discuten la eficiencia de cada modelo de tabla periódica. Muchos cuestionan incluso que la distribución bidimensional sea la mejor. Argumentan que se basa en una convención y en conveniencia, principalmente por la necesidad de ajustarlas a la página de un libro y otras presentaciones en el plano. El propio Mendeléyev no estaba conforme y consideró la distribución en espiral, sin suerte. Algunos argumentos en favor de nuevos modelos consisten en, por ejemplo, la ubicación del grupo de los lantánidos y de los actínidos fuera del cuerpo de la tabla, e incluso que el helio debería estar ubicado en el grupo 2 de los alcaniotérreos pues comparte con ellos dos electrones en su capa externa. Por ello con los años se han desarrollado otras tablas periódicas ordenadas en forma distinta, como por ejemplo en triángulo, pirámide, tablas en escalones, torre y en espiral. A este último tipo corresponde la galaxia química, la espiral de Theodor Benfey y la forma en espiral-fractal de Melinda E Green. Se estima que se han publicado más de 700 versiones de la tabla periódica.

Según Phillip Stewart, si Mendeléyev hubiera seguido desarrollando el modelo en espiral, hubiera podido predecir las propiedades de los halógenos. Utilizando esta idea, el propio Stewart creó una tabla periódica en espiral a la que dio en llamar «Galaxia química», en la que acomoda la longitud creciente de los períodos en los brazos de una galaxia en espiral. 

En palabras de Theodor Benfey, la tabla y la ley periódica 
Su preocupación, pues, era estrictamente pedagógica. Por ese motivo diseñó una tabla periódica oval similar a un campo de fútbol que no mostraba saltos ni elementos flotantes. Ordena los elementos en una espiral continua, con el hidrógeno en el centro y los metales de transición, los lantánidos y los actínidos ocupando las penínsulas. No obstante, no se sintió satisfecho con el resultado, ya que no tenía espacio suficiente para los lantánidos. Por ello en un rediseño posterior creó una protusión para hacerles sitio y lo publicó en 1964 en la revista de la que era redactor jefe, "Chemistry (química)", de la American Chemical Society. La tabla fue modificada para dejar abierta la posibilidad de acomodar nuevos elementos transuránicos que todavía no se habían detectado, cuya existencia había sido sugerida por Glenn Seaborg, así como otros cambios menores. La espiral de Benfey fue publicada en calendarios, libros de texto y utilizada por la industria química, por lo cual se volvió popular.
La tabla fractal se basa en la continuidad de las características del elemento al final de una fila con el que se encuentra al inicio de la siguiente, lo que sugiere que la distribución podría representarse mejor con un cilindro en lugar de fraccionar la tabla en columnas. Además, en algunos casos había muchas diferencias entre algunos elementos con números atómicos bajos. Por otra parte, la tabla incorpora la familia de los actínidos y los lantánidos al diseño general, ubicándolos en el lugar que les correspondería por número atómico, en lugar de mantenerlos separados en dos grupos flotantes al final como sucede en la tabla estándar. El resultado es que las familias, en lugar de seguir columnas, siguen arcos radiales. Esta tabla evidencia la periodicidad introduciendo horquillas en el inicio de los períodos de longitud 8, 18 y 32.

La mayoría de las tablas periódicas son de dos dimensiones; sin embargo, se conocen tablas en tres dimensiones al menos desde 1862 (pre-data tabla bidimensional de Mendeléyev de 1869). Como ejemplos más recientes se puede citar la Clasificación Periódica de Courtines (1925), el Sistema de Lámina de Wrigley (1949), la hélice periódica de Giguère (1965) y el árbol periódico de Dufour (1996). Se ha descrito que la Tabla Periódica de Stowe (1989) tiene cuatro dimensiones —tres espaciales y una de color—. 

Las diversas formas de tablas periódicas pueden ser consideradas como un continuo en la química-física. Hacia el final del continuo químico se puede encontrar, por ejemplo, la Tabla Periódica Inorgánica de Rayner-Canham (2002), que hace hincapié en las tendencias, patrones, relaciones y propiedades químicas inusuales. Cerca del final del continuo físico está la tabla periódica ampliada escalonada por la izquierda de Janet (1928). Tiene una estructura que muestra una relación más estrecha con el orden de llenado de electrones por capa y, por asociación, la mecánica cuántica. En algún lugar en medio del continuo se ubica la ubica tabla periódica estándar; se considera que expresa las mejores tendencias empíricas en el estado físico, la conductividad eléctrica y térmica, los números de oxidación, y otras propiedades fácilmente inferidas de las técnicas tradicionales del laboratorio químico.

Los elementos 108 (hasio), 112 (copernicio) y 114 (flerovio) no tienen propiedades químicas conocidas. Otros elementos superpesados pueden comportarse de forma diferente a lo que se predice por extrapolación, debido a los efectos relativistas; por ejemplo, se predijo que el flerovio exhibiría posiblemente algunas propiedades similares a las de los gases nobles, aunque actualmente (2016) se coloca en el grupo del carbono. Sin embargo, experimentos posteriores sugieren que se comporta químicamente como plomo, como se espera a partir de su posición de la tabla periódica.

No está claro si los nuevos elementos encontrados continuarán el patrón de la tabla periódica estándar como parte del período 8 o se necesitará nuevos ajustes o adaptaciones. Seaborg espera que este periodo siga el patrón previamente establecido exactamente, de modo que incluiría un bloque s para los elementos 119 y 120, un nuevo bloque g para los próximos 18 elementos, y 30 elementos adicionales continuarían los bloques actuales f, d, y p. Los físicos tales como Pekka Pyykkö han teorizado que estos elementos adicionales no seguirían la regla de Madelung, que predice cómo se llenan de capas de electrones, y por lo tanto afectarán la apariencia de la tabla periódica estándar.

El número de posibles elementos no se conoce. En 1911 Elliot Adams, con base en la disposición de los elementos en cada fila de la tabla periódica horizontal, predijo que no existirían los elementos de peso atómico superior a 256 —lo que estaría entre los elementos 99 y 100 en términos de hoy en día—. La estimación reciente más alta es que la tabla periódica puede terminar poco después de la isla de estabilidad, que según se considere un modelo relativista o no se centrará alrededor de Z = 120 y N = 172 o Z = 124-126 y N = 184, ya que la extensión de la tabla periódica está restringida por las líneas de goteo de protones y de neutrones. Otras predicciones del fin de la tabla periódica incluyen al elemento 128 de John Emsley, al elemento 137 de Richard Feynman, y al elemento 155 de Albert Khazan.
El modelo de Bohr, no relativista, exhibe dificultad para los átomos con número atómico superior a 137, ya que estos requerirían que los electrones 1s viajen más rápido que c, la velocidad de la luz, lo que lo vuelve inexacto y no se puede aplicar a estos elementos.

La ecuación relativista de Dirac tiene problemas para elementos con más de 137 protones. Para ellos, la función de onda del estado fundamental de Dirac es oscilatoria, y no hay diferencia entre los espectros de energía positivo y negativo, como en la paradoja de Klein. Si se realizan cálculos más precisos, teniendo en cuenta los efectos del tamaño finito del núcleo, se encuentra que la energía de enlace excede el límite para los elementos con más de 173 protones. Para los elementos más pesados, si el orbital más interno (1s) no está lleno, el campo eléctrico del núcleo tira de un electrón del vacío, lo que resulta en la emisión espontánea de un positrón; sin embargo, esto no sucede si el orbital más interno está lleno, de modo que el elemento 173 no es necesariamente el final de la tabla periódica.

Solamente siguiendo las configuraciones electrónicas, el hidrógeno (configuración electrónica 1s) y el helio (1s) se colocan en los grupos 1 y 2, por encima de litio ([He]2s) y berilio ([He]2s). Sin embargo, esta colocación se utiliza rara vez fuera del contexto de las configuraciones electrónicas: cuando los gases nobles —entonces llamados «gases inertes»— fueron descubiertos por primera vez alrededor de 1900, se los identificaba como «el grupo 0», lo que reflejaba que no se les conocía ninguna reactividad química en ese momento, y el helio se colocó en la parte superior de ese grupo, porque compartía esta situación extrema. Aunque el grupo cambió su número formal, muchos autores siguieron colocando al helio directamente por encima del neón, en el grupo 18; uno de los ejemplos de tal colocación es la tabla IUPAC actual.
Las propiedades químicas del hidrógeno no son muy cercanas a los de los metales alcalinos, que ocupan el grupo 1, y por eso el hidrógeno a veces se coloca en otra parte: una de las alternativas más comunes es en el grupo 17. Una de las razones para ello es la estrictamente univalente química predominantemente no metálica del hidrógeno, la del flúor —el elemento colocado en la parte superior del grupo 17— es estrictamente univalente y no metálica. A veces, para mostrar cómo el hidrógeno tiene tanto propiedades correspondientes a las de los metales alcalinos y a los halógenos, puede aparecer en dos columnas al mismo tiempo. También puede aparecer por encima del carbono en el grupo 14: así ubicado, se adapta bien al aumento de las tendencias de los valores de potencial de ionización y los valores de afinidad de electrones, y no se aleja demasiado de la tendencia de electronegatividad. Por último, el hidrógeno a veces se coloca por separado de cualquier grupo porque sus propiedades en general difieren de las de cualquier grupo: a diferencia del hidrógeno, los otros elementos del grupo 1 muestran un comportamiento extremadamente metálico; los elementos del grupo 17 comúnmente forman sales —de ahí el término "halógeno"—; los elementos de cualquier otro grupo muestran una química multivalente. El otro elemento del periodo 1, el helio, a veces se coloca separado de cualquier grupo también. La propiedad que distingue al helio del resto de los gases nobles —a pesar de que su extraordinario carácter inerte está muy cerca del neón y el argón— es que, en su capa cerrada de electrones, el helio tiene solo dos electrones en el orbital más externo, mientras que el resto de los gases nobles tienen ocho.

Según IUPAC un metal de transición es «un elemento cuyo átomo tiene una subcapa d incompleta o que puede dar lugar a cationes». De acuerdo con esta definición, todos los elementos en los grupos del 3 al 11 son metales de transición y se excluye al grupo 12, que comprende zinc, cadmio y mercurio.

Algunos químicos consideran que los «elementos del bloque d» y los «metales de transición» son categorías intercambiables, incluyendo por tanto al grupo 12 como un caso especial de metal de transición en el que los electrones d no participan normalmente en el enlace químico. El descubrimiento de que el mercurio puede utilizar sus electrones d en la formación de fluoruro de mercurio (IV) (HgF) llevó a algunos científicos a sugerir que el mercurio puede ser considerado un metal de transición. Otros, como Jensen, argumentan que la formación de un compuesto como HgF4 puede ocurrir solo bajo condiciones muy anormales. Como tal, el mercurio no puede ser considerado como un metal de transición por ninguna interpretación razonable en el sentido normal del término.

En otros casos hay quienes no incluyen al grupo 3, argumentando que estos no forman iones con una capa d parcialmente ocupada y por lo tanto no presentan las propiedades características de la química de los metales de transición.

Aunque el escandio y el itrio son siempre los dos primeros elementos del grupo 3, la identidad de los próximos dos elementos no se resuelve. O bien son lantano y actinio, o lutecio y lawrencio. Existen argumentos físicos y químicos para apoyar esta última disposición, pero no todos los autores están convencidos.

Tradicionalmente se representa al lantano y al actinio como los restantes miembros del grupo 3. Se ha sugerido que este diseño se originó en la década de 1940, con la aparición de las tablas periódicas que dependen de las configuraciones electrónicas de los elementos y la noción de la diferenciación de electrones. 

Las configuraciones de cesio, bario y lantano son [Xe]6s, [Xe]6s y [Xe]5d6s. Por lo tanto el lantano tiene un electrón diferenciador 5d y esto lo establece «en el grupo 3 como el primer miembro del bloque d para el periodo 6». 

En el grupo 3 se ve un conjunto consistente de configuraciones electrónicas: escandio [Ar]3d4s, itrio [Kr]4d5s y lantano. Aún en el período 6, se le asignó al iterbio una configuración electrónica de [Xe]4f5d6s y [Xe]4f5d6s para el lutecio, lo que resulta «en un electrón diferenciante 4f para el lutecio y lo establece firmemente como el último miembro del bloque f para el período 6.» Matthias describe la colocación del lantano en virtud del itrio como «un error en el sistema periódico — por desgracia propagado mayoritariamente por la compañía Welch [Sargent-Welch] ... y ... todo el mundo la copió». Lavelle lo refutó aportando una serie de libros de referencia conocidos en los que se presentaban tablas periódicas con tal disposición.

Las primeras técnicas para separar químicamente escandio, itrio y lutecio se basaron en que estos elementos se produjeron juntos en el llamado «grupo de itrio», mientras que La y Ac se produjeron juntos en el «grupo del cerio». Por consiguiente, en los años 1920 y 30 algunos químicos colocaron el lutecio en el grupo 3 en lugar del lantano. 

Posteriores trabajos espectroscópicos encontraron que la configuración electrónica de iterbio era de hecho [Xe]4f6s. Esto significaba que iterbio y lutecio tenían 14 electrones f, «resultando en un electrón diferenciante d en lugar de f» para el último, lo que lo hacía un «candidato igualmente válido» para la siguiente posición de la tabla periódica en el grupo 3 debajo del itrio. Varios físicos en los años 1950 y 60 optaron por lutecio, a la luz de una comparación de varias de sus propiedades físicas con las del lantano. Esta disposición, en la que el lantano es el primer miembro del bloque f, es cuestionada por algunos autores ya que este elemento carece de electrones f. Sin embargo, se ha argumentado que esta no es una preocupación válida dado que existen otras anomalías en la tabla periódica, como por ejemplo el torio, que no tiene electrones f pero forma parte de ese bloque. En cuanto al lawrencio, su configuración electrónica se confirmó en 2015 como [Rn]5f7s7p, lo que representa otra anomalía de la tabla periódica, independientemente de si se coloca en el bloque d o f, pues la potencialmente aplicable posición de bloque p se ha reservado para el nihonio al que se le prevé una configuración electrónica de [Rn]5f6d7s7p.

Las muchas formas diferentes de la tabla periódica han llevado a preguntarse si existe una forma óptima o definitiva. Se cree que la respuesta a esta pregunta depende de si la periodicidad química tiene una verdad subyacente, o es en cambio el producto de la interpretación humana subjetiva, dependiente de la circunstancias, las creencias y las predilecciones de los observadores humanos. Se podría establecer una base objetiva para la periodicidad química determinando la ubicación del hidrógeno y el helio, y la composición del grupo 3. En ausencia de una verdad objetiva, las diferentes formas de la tabla periódica pueden ser consideradas variaciones de la periodicidad química, cada una de las cuales explora y hace hincapié en diferentes aspectos, propiedades, perspectivas y relaciones de y entre los elementos. Se cree que la ubicuidad de la tabla periódica estándar es una consecuencia de su diseño, que tiene un buen equilibrio de características en términos de facilidad de construcción y tamaño, y su descripción de orden atómico y tendencias periódicas.

Estado de los elementos en condiciones normales de presión y temperatura (0 °C y 1 atm).




</doc>
<doc id="2798" url="https://es.wikipedia.org/wiki?curid=2798" title="Teoría de la información">
Teoría de la información

La teoría de la información, también conocida como teoría matemática de la comunicación (Inglés: "mathematical theory of communication") o teoría matemática de la información, es una propuesta teórica presentada por Claude E. Shannon y Warren Weaver a finales de la década de los años 1940. Esta teoría está relacionada con las leyes matemáticas que rigen la transmisión y el procesamiento de la información y se ocupa de la medición de la información y de la representación de la misma, así como también de la capacidad de los sistemas de comunicación para transmitir y procesar información. La teoría de la información es una rama de la teoría matemática y de las ciencias de la computación que estudia la información y todo lo relacionado con ella: canales, compresión de datos y criptografía, entre otros.

La teoría de la información surgió a finales de la Segunda Guerra Mundial, en los años cuarenta. Fue iniciada por Claude E. Shannon a través de un artículo publicado en el "Bell System Technical Journal" en 1948, titulado "Una teoría matemática de la comunicación" (texto completo en inglés). En esta época se buscaba utilizar de manera más eficiente los canales de comunicación, enviando una cantidad de información por un determinado canal y midiendo su capacidad; se buscaba la transmisión óptima de los mensajes. 
Esta teoría es el resultado de trabajos comenzados en la década 1910 por Andrei A. Markovi, a quien le siguió Ralp V. L. Hartley en 1927, quien fue el precursor del lenguaje binario. A su vez, Alan Turing en 1936, realizó el esquema de una máquina capaz de tratar información con emisión de símbolos, y finalmente Claude Elwood Shannon, matemático, ingeniero electrónico y criptógrafo estadounidense, conocido como "el padre de la teoría de la información”, junto a Warren Weaver, contribuyó en la culminación y el asentamiento de la Teoría Matemática de la Comunicación de 1949 –que hoy es mundialmente conocida por todos como la Teoría de la Información-. Weaver consiguió darle un alcance superior al planteamiento inicial, creando un modelo simple y lineal: "Fuente/codificador/mensaje canal/decodificador/destino".
La necesidad de una base teórica para la tecnología de la comunicación surgió del aumento de la complejidad y de la masificación de las vías de comunicación, tales como el teléfono, las redes de teletipo y los sistemas de comunicación por radio. La teoría de la información también abarca todas las restantes formas de transmisión y almacenamiento de información, incluyendo la televisión y los impulsos eléctricos que se transmiten en las computadoras y en la grabación óptica de datos e imágenes.
La idea es garantizar que el transporte masivo de datos no sea en modo alguno una merma de la calidad, incluso si los datos se comprimen de alguna manera. Idealmente, los datos se pueden restaurar a su forma original al llegar a su destino. En algunos casos, sin embargo, el objetivo es permitir que los datos de alguna forma se conviertan para la transmisión en masa, se reciban en el punto de destino y sean convertidos fácilmente a su formato original, sin perder ninguna de la información transmitida.

El modelo propuesto por Shannon es un sistema general de la comunicación que parte de una fuente de información desde la cual, a través de un transmisor, se emite una señal, la cual viaja por un canal, pero a lo largo de su viaje puede ser interferida por algún ruido. La señal sale del canal, llega a un receptor que decodifica la información convirtiéndola posteriormente en mensaje que pasa a un destinatario. Con el modelo de la teoría de la información se trata de llegar a determinar la forma más económica, rápida y segura de codificar un mensaje, sin que la presencia de algún ruido complique su transmisión. Para esto, el destinatario debe comprender la señal correctamente; el problema es que aunque exista un mismo código de por medio, esto no significa que el destinatario va a captar el significado que el emisor le quiso dar al mensaje. La codificación puede referirse tanto a la transformación de voz o imagen en señales eléctricas o electromagnéticas, como al cifrado de mensajes para asegurar su privacidad. Un concepto fundamental en la teoría de la información es que la cantidad de información contenida en un mensaje es un valor matemático bien definido y medible. El término cantidad no se refiere a la cuantía de datos, sino a la probabilidad de que un mensaje, dentro de un conjunto de mensajes posibles, sea recibido. En lo que se refiere a la cantidad de información, el valor más alto se le asigna al mensaje que menos probabilidades tiene de ser recibido. Si se sabe con certeza que un mensaje va a ser recibido, su cantidad de información es cero.

Otro aspecto importante dentro de esta teoría es la resistencia a la distorsión que provoca el ruido, la facilidad de codificación y descodificación, así como la velocidad de transmisión. Es por esto que se dice que el mensaje tiene muchos sentidos, y el destinatario extrae el sentido que debe atribuirle al mensaje, siempre y cuando haya un mismo código en común. La teoría de la información tiene ciertas limitaciones, como lo es la acepción del concepto del código. El significado que se quiere transmitir no cuenta tanto como el número de alternativas necesario para definir el hecho sin ambigüedad. Si la selección del mensaje se plantea únicamente entre dos alternativas diferentes, la teoría de Shannon postula arbitrariamente que el valor de la información es uno. Esta unidad de información recibe el nombre de bit. Para que el valor de la información sea un bit, todas las alternativas deben ser igual de probables y estar disponibles. Es importante saber si la fuente de información tiene el mismo grado de libertad para elegir cualquier posibilidad o si se halla bajo alguna influencia que la induce a una cierta elección. La cantidad de información crece cuando todas las alternativas son igual de probables o cuanto mayor sea el número de alternativas. Pero en la práctica comunicativa real no todas las alternativas son igualmente probables, lo cual constituye un tipo de proceso estocástico denominado Markoff. El subtipo de Markoff dice que la cadena de símbolos está configurada de manera que cualquier secuencia de esa cadena es representativa de toda la cadena completa.

La Teoría de la Información se encuentra aún hoy en día en relación con una de las tecnologías en boga, Internet. Desde el punto de vista social, Internet representa unos significativos beneficios potenciales, ya que ofrece oportunidades sin precedentes para dar poder a los individuos y conectarlos con fuentes cada vez más ricas de información digital. Internet fue creado a partir de un proyecto del departamento de defensa de los Estados Unidos llamado ARPANET "(Advanced Research Projects Agency Network)" iniciado en 1969 y cuyo propósito principal era la investigación y desarrollo de protocolos de comunicación para redes de área amplia para ligar redes de transmisión de paquetes de diferentes tipos capaces de resistir las condiciones de operación más difíciles, y continuar funcionando aún con la pérdida de una parte de la red (por ejemplo en caso de guerra). 
Estas investigaciones dieron como resultado el protocolo TCP/IP "(Transmission Control Protocol/Internet Protocol)", un sistema de comunicaciones muy sólido y robusto bajo el cual se integran todas las redes que conforman lo que se conoce actualmente como Internet. 
El enorme crecimiento de Internet se debe en parte a que es una red basada en fondos gubernamentales de cada país que forma parte de Internet, lo que proporciona un servicio prácticamente gratuito. A principios de 1994 comenzó a darse un crecimiento explosivo de las compañías con propósitos comerciales en Internet, dando así origen a una nueva etapa en el desarrollo de la red. 
Descrito a grandes rasgos, TCP/IP mete en paquetes la información que se quiere enviar y la saca de los paquetes para utilizarla cuando se recibe. Estos paquetes pueden compararse con sobres de correo; TCP/IP guarda la información, cierra el sobre y en la parte exterior pone la dirección a la cual va dirigida y la dirección de quien la envía. Mediante este sistema, los paquetes viajan a través de la red hasta que llegan al destino deseado; una vez ahí, la computadora de destino quita el sobre y procesa la información; en caso de ser necesario envía una respuesta a la computadora de origen usando el mismo procedimiento.
Cada máquina que está conectada a Internet tiene una dirección única; esto hace que la información que se envía no equivoque el destino. Existen dos formas de dar direcciones, con letras o con números. Realmente, las computadoras utilizan las direcciones numéricas para mandar paquetes de información, pero las direcciones con letras fueron implementadas para facilitar su manejo a los seres humanos. Una dirección numérica está compuesta por cuatro partes. Cada una de estas partes está dividida por puntos. 

Una de las aplicaciones de la teoría de la información son los archivos ZIP, documentos que se comprimen para su transmisión a través de correo electrónico o como parte de los procedimientos de almacenamiento de datos. La compresión de los datos hace posible completar la transmisión en menos tiempo. En el extremo receptor, un software se utiliza para la liberación o descompresión del archivo, restaurando los documentos contenidos en el archivo ZIP a su formato original. La teoría de la información también entra en uso con otros tipos de archivo; por ejemplo, los archivos de audio y vídeo que se reproducen en un reproductor de MP3 / MP4 se comprimen para una fácil descarga y almacenamiento en el dispositivo. Cuando se accede a los archivos se descomprimen para que estén inmediatamente disponibles para su uso.

Una fuente es todo aquello que emite mensajes. Por ejemplo, una fuente puede ser una computadora y mensajes sus archivos; una fuente puede ser un dispositivo de transmisión de datos y mensajes los datos enviados, etc. Una fuente es en sí misma un conjunto finito de mensajes: todos los posibles mensajes que puede emitir dicha fuente. En compresión de datos se tomará como fuente el archivo a comprimir y como mensajes los caracteres que conforman dicho archivo.

Por la naturaleza generativa de sus mensajes, una fuente puede ser aleatoria o determinista. Por la relación entre los mensajes emitidos, una fuente puede ser estructurada o no estructurada (o caótica).

Existen varios tipos de fuente. Para la teoría de la información interesan las fuentes aleatorias y estructuradas. Una fuente es aleatoria cuando no es posible predecir cuál es el próximo mensaje a emitir por la misma. Una fuente es estructurada cuando posee un cierto nivel de redundancia; una fuente no estructurada o de información pura es aquella en que todos los mensajes son absolutamente aleatorios sin relación alguna ni sentido aparente. Este tipo de fuente emite mensajes que no se pueden comprimir; un mensaje, para poder ser comprimido, debe poseer un cierto grado de redundancia; la información pura no puede ser comprimida sin que haya una pérdida de conocimiento sobre el mensaje.

Un mensaje es un conjunto de ceros y unos. Un archivo, un paquete de datos que viaja por una red y cualquier cosa que tenga una representación binaria puede considerarse un mensaje. El concepto de mensaje se aplica también a alfabetos de más de dos símbolos, pero debido a que tratamos con información digital nos referiremos casi siempre a mensajes binarios.

Un código es un conjunto de unos y ceros que se usan para representar un cierto mensaje de acuerdo a reglas o convenciones preestablecidas. Por ejemplo, al mensaje 0010 lo podemos representar con el código 1101 usado para codificar la función (NOT). La forma en la cual codificamos es arbitraria. Un mensaje puede, en algunos casos, representarse con un código de menor longitud que el mensaje original. Supongamos que a cualquier mensaje "S" lo codificamos usando un cierto algoritmo de forma tal que cada "S" es codificado en "L(S)" bits; definimos entonces la información contenida en el mensaje "S" como la cantidad mínima de bits necesarios para codificar un mensaje.

La información contenida en un mensaje es proporcional a la cantidad de bits que se requieren como mínimo para representar al mensaje. El concepto de información puede entenderse más fácilmente si consideramos un ejemplo. Supongamos que estamos leyendo un mensaje y hemos leído "cadena de c"; la probabilidad de que el mensaje continúe con "caracteres" es muy alta. Así, cuando efectivamente recibimos a continuación "caracteres" la cantidad de información que nos llegó es muy baja pues estábamos en condiciones de predecir qué era lo que iba a ocurrir. La ocurrencia de mensajes de alta probabilidad de aparición aporta menos información que la ocurrencia de mensajes menos probables. Si luego de "cadena de c" leemos "himichurri" la cantidad de información que estamos recibiendo es mucho mayor.

La información es tratada como magnitud física, caracterizando la información de una secuencia de símbolos utilizando la entropía. Es parte de la idea de que los canales no son ideales, aunque muchas veces se idealicen las no linealidades, para estudiar diversos métodos de envío de información o la cantidad de información útil que se pueda enviar a través de un canal.

La información necesaria para especificar un sistema físico tiene que ver con su entropía. En concreto, en ciertas áreas de la física, extraer información del estado actual de un sistema requiere reducir su entropía, de tal manera que la entropía del sistema (formula_1) y la cantidad de información (formula_2) extraíble están relacionadas por:

De acuerdo a la teoría de la información, el nivel de información de una fuente se puede medir según la entropía de la misma. Los estudios sobre la entropía son de suma importancia en la teoría de la información y se deben principalmente a C. E. Shannon. Existe, a su vez, un gran número de propiedades respecto de la entropía de variables aleatorias debidas a A. Kolmogorov. Dada una fuente "F" que emite mensajes, resulta frecuente observar que los mensajes emitidos no resulten equiprobables sino que tienen una cierta probabilidad de ocurrencia dependiendo del mensaje. Para codificar los mensajes de una fuente intentaremos pues utilizar menor cantidad de bits para los mensajes más probables y mayor cantidad de bits para los mensajes menos probables, de forma tal que el promedio de bits utilizados para codificar los mensajes sea menor a la cantidad de bits promedio de los mensajes originales. Esta es la base de la compresión de datos. A este tipo de fuente se la denomina fuente de orden-0, pues la probabilidad de ocurrencia de un mensaje no depende de los mensajes anteriores. A las fuentes de orden superior se las puede representar mediante una fuente de orden-0 utilizando técnicas de modelización apropiadas.
Definimos la probabilidad de ocurrencia de un mensaje en una fuente como la cantidad de apariciones de dicho mensaje dividido entre el total de mensajes. Supongamos que "P" es la probabilidad de ocurrencia del mensaje-i de una fuente, y supongamos que "L "es la longitud del código utilizado para representar a dicho mensaje. La longitud promedio de todos los mensajes codificados de la fuente se puede obtener como:


A partir de aquí y tras intrincados procedimientos matemáticos que fueron demostrados por Shannon oportunamente se llega a que "H" es mínimo cuando "f(P) = log (1/P)". Entonces:

La longitud mínima con la cual puede codificarse un mensaje puede calcularse como "L=log(1/P) = -log(P)". Esto da una idea de la longitud a emplear en los códigos a usar para los caracteres de un archivo en función de su probabilidad de ocurrencia. Reemplazando "L "podemos escribir "H" como:

De aquí se deduce que la entropía de la fuente depende únicamente de la probabilidad de ocurrencia de cada mensaje de la misma, por ello la importancia de los compresores estadísticos (aquellos que se basan en la probabilidad de ocurrencia de cada carácter). Shannon demostró, oportunamente que no es posible comprimir una fuente estadísticamente más allá del nivel indicado por su entropía.







</doc>
<doc id="2800" url="https://es.wikipedia.org/wiki?curid=2800" title="Triscenia ovina">
Triscenia ovina

Triscenia, es un género monotípico de plantas herbáceas de la familia de las gramíneas o poáceas. Su única especie: Triscenia ovina Griseb., es originaria de Cuba.
Es una planta perenne; cespitosa. Con culmos de 20-50 cm de alto; herbácea; no ramificado arriba. Entrenudos de los culmos huecos. Hojas en su mayoría basales. La lámina es estrecha; setacea (o filiforme); acicular (reducido a la nervadura central); sin venación. Son plantas bisexuales, con espiguillas bisexuales; con los floretes hermafroditas. Las espiguillas todas por igual en la sexualidad. La inflorescencia de las ramas principales espigadas.

"Triscenia ovina" fue descrita por August Heinrich Rudolf Grisebach y publicado en "Plantae Wrightianae" 2: 534. 1863.





</doc>
<doc id="2802" url="https://es.wikipedia.org/wiki?curid=2802" title="Triodia">
Triodia

Triodia es un género de plantas herbáceas de la familia de las gramíneas o poáceas. Es endémico de Australia y tiene 112 especies descritas y de estas, solo 68 aceptadas.
Es una planta herbácea perenne que crece en regiones áridas y tiene las hojas punzantes y teñidas. Son conocidas popularmente como spinifex, aunque no pertenece al género costero "Spinifex". 

"Triodia" ha sido muy utilizado por los aborígenes australianos. Sus semillas se han recolectado para hacer pasteles, con la resina de la planta hacen pegamento para sus lanzas y con sus ramas hacían señales de humo para comunicarse a distancia con sus familias, ya que producen un intenso humo negro. 
El género fue descrito por Robert Brown y publicado en "Prodromus Florae Novae Hollandiae" 182. 1810. 
Triodia: nombre genérico que deriva de las palabras griegas: "treis" = (tres) y "odous" = (dientes), en referencia a 3 dientes o lemas tri-lobulado.


Numerosas especies fueron incluidas en "Triodia" pero posteriormente han sido clasificadas en otros géneros: "Austrofestuca Chascolytrum Danthonia Dasyochloa Deschampsia Diplachne Disakisperma Erioneuron Gouinia Graphephorum Leptocarydion Notochloe Plinthanthesis Poa Puccinellia Rytidosperma Scolochloa Spartina Torreyochloa Trichoneura Tridens Triplasis Tripogon Vaseyochloa"



</doc>
<doc id="2805" url="https://es.wikipedia.org/wiki?curid=2805" title="Thrasya">
Thrasya

Thrasya, es un género de plantas herbáceas de la familia de las gramíneas o poáceas. Es originario de Sudamérica tropical. Comprende 33 especies descritas y de estas, solo 22 aceptadas.

El género fue descrito por Carl Sigismund Kunth y publicado en "Nova Genera et Species Plantarum (quarto ed.)" 1: 120–121. 1815[1816]. La especie tipo es: "Thrasya paspaloides" 
A continuación se brinda un listado de las especies del género "Thrasya" aceptadas hasta noviembre de 2014, ordenadas alfabéticamente. Para cada una se indica el nombre binomial seguido del autor, abreviado según las convenciones y usos. 



</doc>
<doc id="2806" url="https://es.wikipedia.org/wiki?curid=2806" title="Themeda">
Themeda

Themeda, es un género de plantas herbáceas de la familia de las gramíneas o poáceas. Es originario de África, Asia y Australia.
El número cromosómico básico es x = 5 y 10, con números cromosómicos somáticos de 2n = 20, 40, 60 y 80 (y aneuploides). 



</doc>
<doc id="2807" url="https://es.wikipedia.org/wiki?curid=2807" title="Thamnocalamus">
Thamnocalamus

Thamnocalamus, es un género de plantas herbáceas perteneciente a la familia de las poáceas. Es originario del este de Asia y Sudáfrica. Comprende 28 especies descritas y de estas, solo 2 aceptadas.
El género fue descrito por William Munro y publicado en "Transactions of the Linnean Society of London" 26(1): 33, 157. 1868. La especie tipo es: "Thamnocalamus spathiflorus" (Trin.) Munro 
A continuación se brinda un listado de las especies del género "Thamnocalamus" aceptadas hasta noviembre de 2013, ordenadas alfabéticamente. Para cada una se indica el nombre binomial seguido del autor, abreviado según las convenciones y usos. 


</doc>
<doc id="2808" url="https://es.wikipedia.org/wiki?curid=2808" title="Tristachya">
Tristachya

Triscenia, es un género de plantas herbáceas de la familia de las gramíneas o poáceas. Es originario de América, África tropical y Madagascar.



</doc>
<doc id="2810" url="https://es.wikipedia.org/wiki?curid=2810" title="Tabla de verdad">
Tabla de verdad

Una tabla de verdad, o tabla de valores de verdad, es una tabla que muestra el valor de verdad de una proposición compuesta, para cada combinación de verdad que se pueda asignar.

Fue desarrollada por Charles Sanders Peirce por los años 1880, pero el formato más popular es el que introdujo Ludwig Wittgenstein en su "Tractatus logico-philosophicus", publicado en 1921.

Para establecer un Sistema formal se establecen las definiciones de los operadores. Las definiciones se harán en función del fin que se pretenda al construir el sistema que haga posible la formalización de argumentos:

El valor verdadero se representa con la letra V; si se emplea notación numérica se expresa con un uno: 1; en un circuito eléctrico, el circuito está cerrado.

El valor falso se representa con la letra F; si se emplea notación numérica se expresa con un cero: 0; en un circuito eléctrico, el circuito está abierto.
Para una variable lógica A, B, C, ... pueden ser verdaderas V, o falsas F, los operadores fundamentales se definen así:

La negación operador que se ejecuta, sobre un único valor de verdad, devolviendo el valor contradictorio de la proposición considerada. 

La conjunción es un operador, que actúa sobre dos valores de verdad, típicamente los valores de verdad de dos proposiciones, devolviendo el valor de verdad "verdadero" cuando ambas proposiciones son verdaderas, y "falso" en cualquier otro caso. Es decir, es verdadera cuando ambas son verdaderas

La tabla de verdad de la conjunción es la siguiente:

Que se corresponde con la columna 8 del algoritmo fundamental.

en simbología "^" hace referencia a el conector "y"

La disyunción es un operador lógico que actúa sobre dos valores de verdad, típicamente los valores de verdad de dos proposiciones, devolviendo el valor de verdad "verdadero" cuando una de las proposiciones es verdadera, o cuando ambas lo son, y "falso" cuando ambas son falsas.

La tabla de verdad de la disyunción es la siguiente:

Que se corresponde con la columna 2 del algoritmo fundamental.

El condicional material es un operador que actúa sobre dos valores de verdad, típicamente los valores de verdad de dos proposiciones, devolviendo el valor de "falso" sólo cuando la primera proposición es verdadera y la segunda falsa, y "verdadero" en cualquier otro caso.

La tabla de verdad del condicional material es la siguiente:

Que se corresponde con la columna 5 del algoritmo fundamental.

El bicondicional o doble implicación es un operador que funciona dando el valor de verdad cuando ambos valores son iguales y dando el valor de falsedad cuando ambos valores son diferentes.

La tabla de verdad del bicondicional es la siguiente:

Que se corresponde con la columna 7 del algoritmo fundamental.

Partiendo de un número n de variables, cada una de las cuales puede tomar el valor verdadero: V, o falso: F, por Combinatoria, podemos saber que el número total de combinaciones: Nc, que se pueden presentar es:

el número de combinaciones que se pueden dar con n variable, cada una de las cuales puede tomar uno entre dos valores lógicos es de dos elevado a n, esto es, el número de combinaciones: Nc, tiene crecimiento exponencial respecto al número de variable n:

Si consideramos que un sistema combinacional de n variables binarias, puede presentar un resultado verdadero: V, o falso: F, para cada una de las posibles combinaciones de entrada tenemos que se pueden construir Cp circuitos posibles con n variables de entrada, donde:

Que da como resultado la siguiente tabla:

Para componer una tabla de verdad, pondremos las n variables en una línea horizontal, debajo de estas variables desarrollamos las distintas combinaciones que se pueden formar con V y F, dando lugar a las distintas Nc, número de combinaciones. Normalmente solo se representa la función para la que se confecciona la tabla de verdad, y en todo caso funciones parciales que ayuden en su cálculo, en la figura, se pueden ver todas las combinaciones posibles Cp, que pueden darse para el número de variables dado.

Así podemos ver que para dos variables binarias: A y B, n= 2 , que pueden tomar los valores V y F, se pueden desarrollar cuatro combinaciones: Nc= 4, con estos valores se pueden definir dieciséis resultados distintos, Cp= 16, cada una de las cuales seria una función de dos variables binarias. Para otro número de variables se obtendrán los resultados correspondientes, dado el crecimiento exponencial de Nc, cuando n toma valores mayores de cuatro o cinco, la representación en un cuadro resulta compleja, y si se quiere representar las combinaciones posibles Cp, resulta ya complejo para n= 3.

Un circuito sin variables, puede presentar una combinación posible: Nc=1, con dos circuitos posibles: Cp=2. Que serían el circuito cerrado permanentemente, y el circuito abierto permanentemente.

En este caso se puede ver dos funciones con cero variables, caso 1 y 2, que no interviene ninguna variable.

Cada uno de estos circuitos admite una única posición y hay dos circuitos posibles.

El caso de una variable binaria, que puede presentar dos combinaciones posibles: Nc=2, con 4 circuitos posibles: Cp=4.

Se pueden ver las cuatro funciones, de una variable, del caso 1 al 4, siendo A la variable. Puede verse que:

Considérese dos variables proposicionales "A" y "B". Cada una puede tomar uno de dos valores de verdad: o V (verdadero), o F (falso). Por lo tanto, los valores de verdad de "A" y de "B" pueden combinarse de cuatro maneras distintas: o ambas son verdaderas; o "A" es verdadera y "B" falsa, o "A" es falsa y "B" verdadera, o ambas son falsas. Esto puede expresarse con una tabla simple:

Considérese además a "·" como una operación o función lógica que realiza una función de verdad al tomar los valores de verdad de "A" y de "B", y devolver un único valor de verdad. Entonces, existen 16 funciones distintas posibles, y es fácil construir una tabla que muestre qué devuelve cada función frente a las distintas combinaciones de valores de verdad de "A" y de "B".

Las dos primeras columnas de la tabla muestran las cuatro combinaciones posibles de valores de verdad de A y de B. Hay por lo tanto 4 líneas, y las 16 columnas despliegan todos los posibles valores que puede devolver una función.

De esta forma podemos conocer mecánicamente, mediante algoritmo, los posibles valores de verdad de cualquier conexión lógica interpretada como función, siempre y cuando definamos los valores que devuelva la función.

Se hace necesario, pues, definir las funciones que se utilizan en la confección de un sistema lógico.

De especial relevancia se consideran las definiciones para el Cálculo de deducción natural y las puertas lógicas en los circuitos electrónicos. Puede verse que:

Y también que:

Y que:

Las tablas nos manifiestan los posibles valores de verdad de cualquier proposición molecular, así como el análisis de la misma en función de las proposicíones que la integran, encontrándonos con los siguientes casos:

Se entiende por verdad contingente, o verdad de hecho, aquella proposición que puede ser verdadera o falsa, según los valores de las proposiciones que la integran. Sea el caso: formula_20.

Su tabla de verdad se construye de la siguiente manera:

Ocho filas que responden a los casos posibles que pueden darse según el valor V o F de cada una de las proposiciones A, B, C. 

Una columna en la que se establecen los valores de formula_21 aplicando la definición del disyuntor a los valores de B y de C en cada una de las filas.

Una columna en la que se establecen los valores resultantes de aplicar la definición de la conjunción entre los valores de A y valores de la columna formula_21, que representarán los valores de la proposición completa formula_20, cuyo valor de verdad es V o F según la fila de los valores de A, B, y C que consideremos. 

Donde podemos comprobar cuándo y por qué la proposición formula_20 es V y cuándo es F.

Se entiende por proposición contradictoria, o contradicción, aquella proposición que en todos los casos posibles de su tabla de verdad su valor siempre es F. Dicho de otra forma, su valor F no depende de los valores de verdad de las proposiciones que la forman, sino de la forma en que están establecidas las relaciones sintácticas de unas con otras. Sea el caso:

Procederemos de manera similar al caso anterior. Partiendo de la variable A y su contradicción, la conjunción de ambos siempre es falso, dado que si A es verdad su contradicción es falsa, y si A es falsa su contradicción es verdad, la conjunción de ambas da falso en todos los casos.

Se entiende por proposición tautológica, o tautología, aquella proposición que en todos los casos posibles de su tabla de verdad su valor siempre es V. Dicho de otra forma, su valor V no depende de los valores de verdad de las proposiciones que la forman, sino de la forma en que están establecidas las relaciones sintácticas de unas con otras. Sea el caso:

Siguiendo la mecánica algorítmica de la tabla anterior construiremos su tabla de verdad, tenemos la variable A en disyunción con su contradicción, si A es verdad, su negación es falsa y si A es falsa su negación es verdad, en cualquier caso una de las dos alternativas es cierta, y su disyunción es cierta en todos los casos.

En realidad toda la lógica está contenida en las tablas de verdad, en ellas se nos manifesta todo lo que implican las relaciones sintácticas entre las diversas proposiciones.

No obstante la sencillez del algoritmo, aparecen dos dificultades.


Esta dificultad ha sido magníficamente superada por la rapidez de los ordenadores, y no presenta dificultad alguna.


Por ello se construye un cálculo mediante cadenas deductivas:

Las proposiciones que constituyen el "antecedente" del esquema de inferencia, se toman como premisas de un argumento.

Se establecen como reglas de cálculo algunas tautologías como tales leyes lógicas, (pues garantizan, por su carácter tautológico, el valor V).

Se permite la aplicación de dichas reglas como reglas de sustitución de fórmulas bien formadas en las relaciones que puedan establecerse entre dichas premisas.

Deduciendo mediante su aplicación, como teoremas, todas las conclusiones posibles que haya contenidas en las premisas.

Cuando en un cálculo se establecen algunas leyes como principios o axiomas, el cálculo se dice que es axiomático.

El cálculo lógico así puede utilizarse como demostración argumentativa.

La aplicación fundamental se hace cuando se construye un sistema lógico que modeliza el lenguaje natural sometiéndolo a unas reglas de formalización del lenguaje. Su aplicación puede verse en el cálculo lógico.

Una aplicación importante de las tablas de verdad procede del hecho de que, interpretando los valores lógicos de verdad como 1 y 0 (lógica positiva) en el sentido que


Los valores de entrada o no entrada de corriente a través de un diodo pueden producir una salida 0 ó 1 según las condiciones definidas como función según las tablas mostradas anteriormente. 

Así se establecen las algunas funciones básicas: AND, NAND, OR, NOR, XOR, XNOR (o NXOR), que se corresponden con las funciones definidas en las columnas 8, 9, 2, 15, 10 y 7 respectivamente, y la función NOT.

En lugar de variables proposicionales, considerando las posibles entradas como EA y EB, podemos armar una tabla análoga de 16 funciones como la presentada arriba, con sus equivalentes en lógica de circuitos.

Esta aplicación hace posible la construcción de aparatos capaces de realizar estas computaciones a alta velocidad, y la construcción de circuitos que utilizan este tipo de análisis se hace por medio de puertas lógicas.

La Tabla de la verdad es una herramienta imprescindible en la recuperación de datos en las bases de datos como Internet con los motores de búsqueda o en una biblioteca con sus ficheros informatizados. Así mismo se utilizan para programar simulaciones lógicas de inteligencia artificial con lenguajes propios. También en modelos matemáticos predictores: meteorología, marketing y otros muchos.

La definición de la tabla de verdad corresponde a funciones concretas, en cada caso, así como a implementaciones en cada una de las tecnologías que pueden representar funciones lógicas en binario, como las puertas lógicas o los circuitos de conmutación. Se entenderá como verdad la conexión que da paso a la corriente; en caso contrario se entenderá como falso. Veamos la presentación de los dieciséis casos que se presentan con dos variables binarias A y B:


El primer caso en una función lógica que para todas las posibles combinaciones de A y B, el resultado siempre es verdadero, es un caso de tautología, su implementación en un circuito es una conexión fija.

En este segundo caso el resultado solo es falso si A y B son falsos, si una de las dos variables es verdad el resultado es verdad.

La función seria:
En el tercer caso es verdad si A es verdad y cuando A y B son falsos el resultado también es verdad.

Su función seria:
En el cuarto caso la función es cierta si A es cierta, los posibles valores de B no influyen en el resultado.

La función solo depende de A:

En el quinto caso si A es falso el resultado es verdadero, y si A y B son verdaderos el resultado también es verdadero, puede verse que este caso es idéntico al tercero permutando A por B.

Y si función es:
En el sexto caso la función es cierta si B es cierta, los valores de A no influyen en el resultado.

La función solo depende de B:

El séptimo caso corresponde a la relación bicondicional entre A y B, el resultado solo es verdad si A y B son ambos verdad o si A y B son ambos falsos.


En el octavo caso el resultado es verdad si A y B son verdad, en el resto de los valores de A y B el resultado es falso, corresponde a la conjunción de A y B, equivalente a un circuito en serie.

En el noveno caso el resultado solo es falso si A y B son verdad, en el resto de los valores de A y B el resultado es verdadero, corresponde a la disyunción de la negación A y de B, equivalente a un circuito en paralelo de conexiones inversas.


Podemos ver que el décimo caso es lo opuesto a la bicondicional, solo es verdad si A y B discrepan, si A y B son diferentes el valor es verdad, si A y B son iguales el resultado es falso.


En este caso podemos ver que cuando B es verdad el resultado es falso y que cuando B es falso el resultado es verdadero, independientemente del valor de A, luego la función solo depende de B, en sentido inverso.

En el caso doce, vemos que solo hay un combinación de A y B con resultado verdadero, que es A y la negación de B.


En el caso decimotercero podemos ver que el resultado es el opuesto de A, independientemente del valor de B:

Caso decimocuarto, el resultado de la función solo es verdad si A es falso y B verdadero, luego es equivalente a un circuito en serie de A en conexión inversa y de B en conexión directa.

En el caso decimoquinto, el resultado solo es verdad si A y B son falsos, Luego es necesario que tanto A como B sean falsos para que el resultado sea verdadero.


Por último en el caso decimosexto, tenemos que el resultado siempre es falso independientemente de los valores de A o de B.



</doc>
<doc id="2811" url="https://es.wikipedia.org/wiki?curid=2811" title="Tripsacum">
Tripsacum

Tripsacum, es un género de plantas herbáceas de la familia de las gramíneas o poáceas. Es originario de América.
El nombre del género deriva de las palabras griegas "tri" (tres) y "psakas" (pequeños), refiriéndose a la ruptura de los picos en (al menos) tres piezas. 
El número cromosómico básico es x = 9, con números cromosómicos somáticos de 2n = 36, 72, 90 y 108. 4, 8, 10 y 12 ploides. Nucléolos persistentes. 



</doc>
<doc id="2812" url="https://es.wikipedia.org/wiki?curid=2812" title="Triplasis">
Triplasis

Triplasis, es un género de plantas herbáceas perteneciente a la familia de las poáceas. Es originaria del sudoeste de los Estados Unidos. Comprende 23 especies descritas y de estas, solo 2 aceptadas.
Son plantas anuales o perennes, generalmente cespitosas o raramente rizomatosas. Hojas caulinares; la lígula es una hilera de tricomas; láminas lineares, aplanadas. Inflorescencias panículas pequeñas, terminales y axilares; cleistogenes también ocultos dentro de vainas foliares inferiores ligeramente infladas. Espiguillas lineares, comprimidas lateralmente, pediceladas, con varios flósculos bisexuales; desarticulación arriba de las glumas y entre los flósculos; glumas subiguales, 1-nervias, 2-fidas; entrenudos de la raquilla largos; lemas 2-lobadas, carinadas, 3-nervias, la nervadura central proyectándose como una arista, serícea, las nervaduras laterales seríceas, paralelas a la nervadura central; pálea arqueada hacia afuera, las quillas fuertemente viloso-ciliadas en el 1/2 superior; lodículas 2; estambres 3; estilos 2. Fruto una cariopsis; embrión de 1/2 la longitud de la cariopsis; hilo elíptico, de 1/4 la longitud de la cariopsis.
El género fue descrito por Ambroise Marie François Joseph Palisot de Beauvois y publicado en "Essai d'une Nouvelle Agrostographie" 81. 1812. La especie tipo es: "Triplasis americana" 
El nombre del género proviene del griego "triplasios" (triple), en alusión a las lemas con una arista y dos lóbulos. 
A continuación se brinda un listado de las especies del género "Triplasis" aceptadas hasta junio de 2015, ordenadas alfabéticamente. Para cada una se indica el nombre binomial seguido del autor, abreviado según las convenciones y usos. 




</doc>
<doc id="2813" url="https://es.wikipedia.org/wiki?curid=2813" title="Triniochloa">
Triniochloa

Triniochloa, es un género de plantas herbáceas de la familia de las gramíneas o poáceas. Es originario de México a Ecuador y Perú.
Son plantas perennes cespitosas. Hojas principalmente caulinares; vainas con los márgenes unidos; lígula una membrana; láminas lineares, aplanadas. Inflorescencia una panícula terminal. Espiguillas comprimidas dorsalmente, con 1 flósculo bisexual; desarticulación arriba de las glumas; glumas generalmente más cortas que el flósculo o a veces tan largas como él, membranáceas, 1-nervias, iguales, acuminadas; lema subcoriácea, subcilíndrica, 7-9-nervia, 2-dentada, aristada; callo oblicuo, barbado; arista geniculada, torcida por debajo del primer ángulo, insertada por detrás de la lema; palea sulcada, 2-lobada, casi tan larga como la lema; raquilla no extendida; lodículas unidas; estambres 3; ovario glabro; estilos 2. El fruto una cariopsis fusiforme, pardo oscuro; hilo linear.
El género fue descrito por Albert Spear Hitchcock y publicado en "Contributions from the United States National Herbarium" 17: 303. 1913.

Reference article Morales, J. F. 2003. Poaceae. 93(3): 598–821. In B. E. Hammel, M. H. Grayum, C. Herrera & N. Zamora Villalobos (eds.) Man. Pl. Costa Rica. Missouri Botanical Garden, St. Louis.


</doc>
<doc id="2815" url="https://es.wikipedia.org/wiki?curid=2815" title="Trachypogon">
Trachypogon

Trachypogon, es un género de plantas herbáceas de la familia de las gramíneas o poáceas. Es originario de América tropical, África y Madagascar.
El nombre del género deriva de las palabras griegas "trachus" (en bruto) y "pogon" (barba), refiriéndose a las aristas de las espiguillas femeninas fértiles. 
El número cromosómico básico es x = 5 y 10, con números cromosómicos somáticos de 
2n = 20 y 40. tetraploides. 



</doc>
<doc id="2822" url="https://es.wikipedia.org/wiki?curid=2822" title="Tipografía">
Tipografía

La tipografía (del griego τύπος "[típos]", ‘golpe’ o ‘huella’, y γράφω "[gráfο]", ‘escribir’) se dice que es el arte y la técnica en el manejo y selección de tipos para crear trabajos de impresión.

El tipógrafo Stanley Morison la definió como:

Al método de impresión que hace uso de tipos, también se le denomina "tipografía o impresión tipográfica" ("letterpress") en contraposición a otros métodos existentes, tales como impresión ófset, impresión digital, etc.

Se denomina tipografía a la tarea u oficio e industria que se ocupa de la elección y el uso de tipos (letras diseñadas con unidad de estilo) para desarrollar una labor de impresión. La cual hace referencia a los elementos letras, números y símbolos pertenecientes a un contenido impreso, ya sea en soporte físico o digital.

Es importante tener presente qué se quiere comunicar, porque ello definirá qué tipo es el más representativo para la intención buscada.






La imprenta en Europa se desarrolló en el auge del Renacimiento; sin embargo, los primeros impresos de Johannes Gutenberg como la Biblia de 42 líneas utilizaron un estilo de letra del período gótico la cual se le denominó: textura.

Los primeros tipos móviles, inventados por Johann Gutenberg, y el tipo de letra redonda o romana que le siguió en Italia, imitaban el estilo manuscrito de esos países en boga en aquellos momentos.

Aunque se sabe ahora que los chinos ya habían experimentado con tipos móviles de cerámica en el siglo XI, Gutenberg es reconocido como el padre del tipo móvil. Vivió en Maguncia (Alemania) y era orfebre de oficio, pero adquirió los conocimientos técnicos sobre el arte de la impresión. Ya se habían hecho impresiones a partir de bloques de madera tallados a mano muchos años antes.

En 1440 comenzó una serie de experimentos que, diez años después, darían como resultado la invención de la imprenta a partir de tipos móviles. Utilizó sus conocimientos sobre la tecnología y los materiales existentes —la prensa de tornillo, las tintas a base de aceite y el papel—, pero fue la manufactura de los tipos a la que le dedicó gran parte de sus esfuerzos.

Como orfebre conocía muy bien el modelado, mezcla y fundición de metales, lo que le permitió desarrollar un método para fabricar los tipos. Se trataba de grabar cada carácter en relieve de forma inversa sobre un troquel de acero que se incrustaba con un mazo en la terraja (una barra de cobre). La terraja se colocaba en la matriz, un molde maestro para fundir cada letra, según un proceso llamado justificación. Después, la matriz se colocaba en un molde manual ajustable sobre el que se vertía una aleación de plomo y antimonio, y de ese modo modelaba cada uno de los tipos.
Los frutos visibles de sus trabajos son la "Biblia de 42 líneas", en 1445, el libro más antiguo impreso en el mundo occidental, aunque imprimió Indulgencia de Maguncia el año anterior, para el cual utilizó un estilo cursivo de la letra gótica, llamada bastarda.

Los primeros tipos de letra redonda que aparecieron en Italia entre los años 1460 y 1470 estaban basados en la escritura manual humanista. Un renovado interés por la minúscula carolingia, había provocado un refinamiento en su diseño, el resultado fue el proyecto final para el primer tipo romano.

Después de 1460, el liderazgo en el desarrollo de los tipos móviles, pasó de Alemania a Italia, centro artístico del renacimiento. En 1465, en Subiaco, cerca de Roma, Conrad Sweynheym y Arnold Pennartz, dos alemanes que se habían desplazado a Italia, influenciados por el trabajo de Gutenberg, crearon un tipo híbrido, mezcla de características góticas y romanas. En 1467 se trasladaron a Roma y en 1470 habían creado un nuevo conjunto de letras, basados en la escritura humanista.

Mientras tanto en Venecia, en 1469, los hermanos da Spira, crearon otra tipo romano, superior al anterior. Pese a ello, en 1470 Nicholas Jenson creó un tipo de letra que superaba a todas las diseñadas en la época en Italia y que siguió perfeccionando, creando uno nuevo seis años después y conocido como "romana de letra blanca", utilizado para la impresión de Nonius Peripatetica. Desde entonces, las proporciones de Jenson han servido de inspiración para los diseños de tipos.

A pesar de que el estilo predominante en Italia era el romano, no era el único. Incluso Jenson continuó produciendo libros en letra gótica, al igual que muchos otros. En 1483, como cosa inusual, el alemán Erhard Ratdolt, imprimió Eusebius usando la letra gótica y la romana de forma conjunta.

Durante la Edad Media, la cultura del libro giraba en torno a los monasterios cristianos, de los cuales podría decirse que hacían de casas editoriales en el sentido moderno del término. Los libros no eran impresos, sino escritos por monjes especializados en esta tarea que eran llamados copistas; ellos desarrollaban su trabajo en un lugar que había en la mayoría de los monasterios llamado scriptorium que contaba con una biblioteca y un salón con una especie de escritorios similares a los atriles de las iglesias de la actualidad. En este lugar, los Monjes transcribían los libros de la biblioteca, ya fuera por encargo de un señor feudal o de otro monasterio.

Durante el estilo Gótico, Europa retornó paulatinamente a un sistema económico dependiente de las ciudades —y no del campo como lo fue tradicionalmente durante casi toda la Edad Media—, lo que determinó el nacimiento de los gremios, los cuales dieron paso a una mayor producción de libros. Los libros, generalmente religiosos, eran encargados por patrones que se les otorgaba a un gremio de artistas de libros, los cuales tenían especialistas capacitados en letreros, mayúsculas decorativas, decoración de letras, corrección de galeras y encuadernación; al ser este un proceso totalmente artesanal, un libro de 200 páginas podía llegar a demorarse de 5 a 6 meses, y se requerían aproximadamente 25 pieles de carnero para hacer la vitela donde se escribía e ilustraba con témpera de huevo, guache y una primitiva forma de óleo.

Las ciudades que más se fortalecieron durante el periodo gótico, fueron las de Europa del norte, como lo son París, Londres y un gran número de ciudades alemanas, las cuales fueron las primeras que adoptaron el sistema gremial; además de esto, la ciudad determinó el nacimiento de las universidades, lo cual hizo aumentar la demanda de manuscritos y planteó la necesidad de encontrar un nuevo modo de producción de libros, masivo y mucho más económico.

El papel llegó a Occidente, siguiendo las rutas de las caravanas que venían del lejano oriente en Asia hacia el mar Mediterráneo, hasta que alcanzó el mundo árabe, y estos, a su vez llevaron el invento a Europa durante las invasiones árabes que llegaron hasta España.

En poco tiempo, aproximadamente hacia mediados del siglo XIV, las primeras fábricas de papel se extendieron desde España a Francia, Italia, Gran Bretaña y Alemania. El mismo camino que tomó el papel, también lo hizo la xilografía, otro invento chino. Las primeras manifestaciones de este sistema de impresión, se pudieron ver en los juegos de naipes y en imágenes religiosas. Por ser estos los primeros diseños que se introdujeron en una cultura iletrada, representaron la primera manifestación de la democratización del arte de la imprenta en Europa. Estas imágenes iban cargadas de signos y símbolos, los cuales obligaban a una deducción lógica. La xilografía permitió que los libros estuvieran al alcance del común de la gente, la cual, en su mayoría era analfabeta y por tal razón, el libro de bloque traía muy poco texto y muchas ilustraciones, las cuales eran entendidas por cualquier persona, a diferencia del texto que necesitaba de la alfabetización de la población.

Este sistema, sin embargo, seguía siendo bastante caro, pues tomaba mucho tiempo grabar en la madera cada letra e ilustración, lo cual determinó que fueran libros de muy poca extensión, aproximadamente de 30 a 50 folios.

Los primeros libros de bloque se imprimieron con un sello de mano y tinta color sepia o gris, que luego sería reemplazada por la tinta negra. Después de imprimir el texto y las ilustraciones, estas se coloreaban a mano con la misma técnica que se aplicaba en los manuscritos góticos.

Algunos grabadores que hacían libros de bloque, al tratar de simplificar su trabajo, trataron de grabar cada letra independientemente para utilizarla varias veces en diferentes libros, pero al ser la madera un material muy maleable, las letras se deformaban al cabo de pocas impresiones. A mediados del siglo XV, surgió un nuevo invento, el cual recibió diferentes denominaciones, entre las que figuran «sistema de impresión por tipos móviles», «tipografía» e «imprenta».

El primero en realizar un proceso de impresión por tipos móviles de metal en Occidente fue el alemán Johannes Gutenberg, que produjo sus primeros impresos entre los años de 1448 y 1450. Cabe destacar que aunque el desarrollo de este proceso de impresión es principalmente europeo, se produjo gracias a ciertos cambios ocurridos en la Europa medieval:

Es así como Gutenberg adaptó una prensa, y fundió miles de tipos móviles en metal, los cuales se podían adaptar en la prensa por medio de una caja llamada tipográfica. En la impresión medieval de bloque, se usaba tinta de agua ligera extraída de las agallas del encino, la cual era muy bien absorbida por la madera, pero en el tipo de metal se corría o emborronaba. Para producir una tinta espesa y pegajosa, Gutenberg empleó aceite de linaza hervido, que después coloreado con pigmento de humo. Lo único que se hacía a mano en el impreso tipográfico, era el diseño de la letra capital, y la aplicación de su color.

En los manuscritos iluminados, los libros tenían una generosa cantidad de imágenes que fueron suprimidas paulatinamente de los libros tipográficos por la imposibilidad tecnológica de la época de fundir en metal toda una imagen; debido a que la producción de un manuscrito iluminado era sumamente costosa, la impresión de bloque y tipográfica, permitió abaratar estos costos, logrando así que la escritura, al igual que la información se difundiera y produjera cambios de pensamiento en Europa, los cuales traerían reformas, contrarreformas y revoluciones.

Hacia 1500, el invento de Gutenberg había tenido tan amplia difusión, que en Europa ya existían aproximadamente 1100 imprentas funcionando. En los países germanos el estilo de letra más usado era la fraktur (aunque el tipo utilizado en la primera Biblia de Gutenberg fue «textura»). A diferencia de Alemania, en el sur de Europa la costumbre en la Edad Media era utilizar la minúscula carolingia junto a las mayúsculas cuadradas romanas adaptadas de las inscripciones que se encontraban en las ruinas del Imperio romano, como la Columna de Trajano; por tal razón, este estilo de escritura, sirvió de modelo a los primeros impresores italianos, para crear las familias tipográficas clásicas o con serifas (también llamados "gavilanes" o "remates"). El primer tipo de letra con serifas apareció en el año de 1465, más tarde, tipógrafos e impresores de la talla de Nicolas Jenson y Aldo Manucio perfeccionaron estas primeras fundiciones, volviéndolas más estilizadas y refinadas además de incluir un nuevo estilo de letra que se llamó «bastardilla», el cual fue tomado de la caligrafía cancilleresca de la época; actualmente a este estilo de letra se le llama «itálica» (por el país de procedencia) o «cursiva», y es utilizado para resaltar en un texto palabras escogidas por el editor, extranjerismos y citas.

A estos primeros tipos romanos, clásicos o con serifa, se les dio el nombre de "estilo veneciano", pues las principales imprentas italianas que los producían se habían establecido en la ciudad de Venecia.

En Francia, cabe destacar al tipógrafo e impresor Claude Garamond, que creó entre las décadas de 1530 y 1550 una familia tipográfica francesa basada en el estilo veneciano, que con el tiempo se convirtió en el "estándar" de su época y otras posteriores.

Durante la industrialización se intenta automatizar la impresión, con dos vertientes diferenciadas. En la monotipia cada letra del alfabeto se funde en relieve por separado, y en la linotipia se funde cada línea entera por separado (de ahí su nombre), y al acabar la impresión cada línea se vuelve a fundir para crear nuevas líneas.

Los movimientos artísticos están íntimamente relacionados con la tipografía y su diseño son: futurismo, dadaísmo, constructivismo ruso, movimiento "De Stijl" y suprematismo.
PostScript es un lenguaje que codifica la información descriptiva, independientemente de la resolución o el sistema.


Partes que componen un tipo:

Los tipos de letra se clasifican a través de estilos por su forma y también por el momento en el que fueron diseñadas.

Los primeros tipos móviles creados por Johannes Gutenberg, imitaban la escritura manuscrita de la Edad media. Por esta razón no es de extrañar, que los primeros tipos que comenzaron a fundirse fueran la letra gótica o "fraktur" en Alemania y la humanística o romana (también llamada Veneciana) en Italia. La evolución del diseño tipográfico ha permitido establecer una clasificación de los tipos de letra por estilos generalmente vinculados con las épocas en las que fueron creadas las familias tipográficas.







Una forma de clasificar las letras es según tengan o no «serifas». Se entiende por "serifas", o "remates", las pequeñas líneas que se encuentran en las terminaciones de las letras, principalmente en los trazos verticales o diagonales. La utilidad de las serifas es facilitar la lectura, ya que estas crean en el ojo la ilusión de una línea horizontal por la que se desplaza la vista al leer.

Las letras sin serifas o de palo seco, son aquellas que no llevan ningún tipo de terminación; por lo general son consideradas inadecuadas para un texto largo ya que la lectura resulta incómoda pues existe una tendencia visual a identificar este tipo de letras como una sucesión de palos verticales consecutivos.

Por esta razón, las letras con serifas (llamadas también romanas) se utilizan en los periódicos, revistas y libros, así como en publicaciones que contienen textos extensos.
Las letras sin serifas o palo seco son usadas en titulares, rótulos, anuncios y publicaciones con textos cortos de estilo "valiante". Ante la aparición de los medios electrónicos, las letras de palo seco se han convertido también en el estándar para la edición en la web y los formatos electrónicos ya que por la baja resolución de los monitores las serifas terminan distorsionando el tipo. Esto se debe a que las curvas pequeñas son muy difíciles de reproducir en los píxeles de la pantalla.

También podemos clasificarlos dependiendo de su uso de una manera más coloquial como lo usan actualmente los tipografos en máquina de tipografía con Heidelberg de Aspas o Minerva

Son las letras o tipos, orlas, signos, números ... en definitiva todos los caracteres que se pueden imprimir.

Son piezas de altura inferior a los tipos y se utilizan para crear interlineados, entre palabros, espacios, lingotes o imposiciones.

La mayoría de las escrituras comparte la noción de una línea de base: una línea horizontal imaginaria sobre la cual se apoyan los caracteres. En algunas escrituras, hay partes de los glifos que van por debajo de la línea de base pendiente que atraviesa la distancia entre la línea de fondo y el glifo descendiente más bajo de un tipo de letra, y la parte de un glifo que desciende debajo de la línea de fondo tiene el conocido descendiente. Inversamente, subida atraviesa la distancia entre la línea de fondo y la tapa del glifo que alcanza lo más lejos posible de la línea de fondo. La subida y la pendiente pueden o no pueden incluir la distancia agregada por acentos o marcas diacríticas.

En las escrituras latina, griega y cirílica (designadas a veces colectivamente como LGC) se pueden referir a la distancia de la parte inferior a la superior de los glifos, de las minúsculas (línea mala) como x-altura, y la parte de un glifo que se levanta sobre la x-altura como ascendiente. La distancia de la línea de fondo a la tapa de la subida o los glifos mayúsculos regulares (línea del casquillo) también se conoce como la altura del casquillo. La altura del ascender puede tener un efecto dramático en la legibilidad y el aspecto de una fundición. El cociente entre la x-altura y la altura de la subida o del casquillo sirve a menudo para su caracterización.
Justificar o alinear un texto es la manera de acomodar las líneas en la caja. Es decir, es la manera en que se alinean entre sí, apoyándose en un lado, al centro o consiguiendo una forma caprichosa. Tomando en cuenta que la palabra "caja" apela al antiguo método de acomodar tipos (letras) en un recipiente de madera para conformar columnas, podemos imaginar claramente las líneas apoyadas a la izquierda en una columna, por ejemplo.

Los nombres que se dan a las formas de justificar un texto varían ocasionalmente entre los diferentes países, pero podemos decir que los más usuales son:

En la actualidad, las columnas de texto se aplican también en formas caprichosas ya sea siguiendo el contorno de una figura o creando una figura con ellas mismas. La creatividad ha desarrollado retratos formados con el texto de la biografía del personaje y un sinfín de aplicaciones se ven comúnmente en deformaciones legibles o prácticamente ilegibles, buscando atraer la atención del observador. Justificar es entonces, simplemente dar un formato cualquiera al texto en cuestión.

El espaciado o "tracking" se refiere al espacio que existe entre cada par de palabras en un texto en relación con el cuadratín o ancho y alto del cuerpo usado.

Una segunda forma de clasificar las letras es según el ancho o grueso, es decir, el espacio que ocupa horizontalmente cada letra. Desde los comienzos de la escritura y la caligrafía y por supuesto de la tipografía, los primeros maestros notaron que no todas las letras eran iguales en su ancho y por tal razón, el espacio entre cada una de ellas debería variar para que la lectura fuese fluida y equilibrada. Al contrario de este razonamiento, las letras de las máquinas de escribir ocupaban cada una el mismo espacio, de manera que en el texto se veían espacios distintos entre ellas. Teniendo en cuenta que no todas las letras tienen el mismo ancho: Una «m» ocupaba todo el espacio, mientras que una «i» ocupaba mucho menos. Si en el texto aparecían seguidas una «i» y una «l», el espacio entre ambas era muy grande, mientras que si aparecían seguidas una «m» y una «o» el espacio era muy reducido. De todo ello resultaba una considerable incomodidad de la lectura y, por ejemplo, en el caso de titulares o rótulos.

A continuación se muestra un ejemplo de cada uno de los dos tipos de letras:

Como lo dice la palabra, el interlineado es la separación existente que hay entre líneas. Se mide en puntos.

Los procesadores de textos de los ordenadores actuales disponen de una amplia gama de tipos —también llamados incorrectamente, por influencia del inglés, fuente—, tanto de un tipo como de otro.

La letra "Times New Roman" fue diseñada originalmente para el periódico inglés "The Times". Mediante este tipo de letra se conseguía una gran legibilidad y un excelente aprovechamiento del espacio, por lo que en seguida se generalizó su uso en los medios impresos y, sobre todo, en la prensa. La gran popularidad de la "Times New Roman" es un punto a su favor para su utilización incluso en medios electrónicos, pero para textos largos en formato electrónico puede producir fatiga, precisamente porque la forma en la que el ojo percibe los bordes en este formato es justo lo contrario que en el papel ya que por la poca resolución de los monitores, las serifas terminan distorsionando el contorno del glifo. Esto se debe a que las curvas pequeñas son muy difíciles de reproducir en los píxeles de la pantalla. Obviamente, la separación entre líneas también influye en la legibilidad de un texto electrónico. Para cartas y correos electrónicos ambos tipos de letras son apropiados, mientras que para informes y contratos (por lo general, largos) son más indicadas las letras con serifa.

Es posible afirmar que todos los tipos cuyo diseño es igual o similar a los tipos clásicos latinos (romanos) son los que ofrecen la mejor legibilidad. Hasta el momento el tipo que ofrece la máxima legibilidad en documentos impresos es la Times New Roman diseñado por Stanley Morison en 1932 para ser usado especialmente para el periódico londinense "The Times".

Sin embargo para la red hay quienes consideran que una de las mejores familias tipográficas es la Verdana, porque no cuenta con serifas que se distorsionen, por lo cual es una de las legibles incluso a tamaños ínfimos en los monitores.

Los principales formatos usados para tipos de letra en informática son: PS, True type y OpenType.






</doc>
<doc id="2823" url="https://es.wikipedia.org/wiki?curid=2823" title="Tejido (biología)">
Tejido (biología)

En biología, los tejidos son aquellos materiales biológicos naturales constituidos por un conjunto complejo y organizado de células, de uno o de varios tipos, distribuidas regularmente con un comportamiento fisiológico coordinado y un origen embrionario común. Se llama histología a la ciencia que estudia los tejidos orgánicos.

Muchas palabras del lenguaje común, como "pulpa", "carne" o "ternilla", designan materiales biológicos en los que un tejido determinado es el constituyente único o predominante; los ejemplos anteriores se corresponden, respectivamente, con "parénquima", "tejido muscular" o "tejido cartilaginoso".

Solo algunos reinos han logrado desarrollar la pluricelularidad en el curso de la evolución, y de estos únicamente en dos se reconoce la existencia de tejidos, a saber: en las plantas vasculares y en los animales (o metazoos). En general, se admite también que hay verdaderos tejidos en las algas pardas. Dentro de cada uno de estos grupos, los tejidos son esencialmente homólogos, pero son diferentes de un grupo a otro, y su estudio y descripción son independientes, por lo que se distinguen una histología vegetal y una histología animal.

En los animales, estos componentes celulares están inmersos en una matriz extracelular más o menos extensa, de características particulares para cada tejido.

Generalmente, esta matriz es generada por las propias células que componen el tejido, por lo que se dice que los tejidos están constituidos por un componente celular y, en algunos casos, por un componente extracelular. Es uno de los niveles de organización biológica, situado entre el nivel celular, en el escalón inferior, y el nivel orgánico, en el escalón superior.

La disciplina de la biología encargada del estudio de los tejidos orgánicos es la histología. Si se profundiza en los detalles, puede afirmarse que existen más de una centena de tejidos diferentes en los animales y algunas decenas en los vegetales, pero la inmensa mayoría son tan solo variedades de unos pocos tipos fundamentales. La estructura íntima de los tejidos escapa a simple vista, por lo cual se usa el microscopio para visualizarla.

Un tejido puede estar constituido por células de una sola clase, todas iguales, o por varios tipos de células dispuestas ordenadamente. El grado de especialización de los tejidos varía notablemente, tanto en lo funcional como en lo estructural.
Según su origen embriológico, pueden clasificarse en dos grandes grupos: tejidos especializados y tejidos no especializados.

Las células que forman parte de un tejido se especializan mediante procesos complejos. La diferenciación celular, como otros procesos celulares, está controlada por mecanismos de regulación de la expresión génica tales como el control genómico, el control transcripcional, el control postranscripcional, el control traduccional y el control postraduccional.

Existen cuatro tipos de tejidos fundamentales, en los animales:


Estos tejidos fundamentales, según su origen embriológico, se pueden clasificar en dos grandes grupos:



Los principales tejidos de los organismos eucariontes son los siguientes:


En el , mediante la utilización de impresoras 3D, se crearon tejidos artificiales. Se partió de la impresión de estructuras cartilaginosas, óseas y musculares, que se implantaron en roedores, con lo que se logró que los tejidos artificiales se convirtieran en tejidos funcionales y desarrollaran los vasos sanguíneos tisulares y, por consiguiente, que el material artificial respondiera como si se tratara de un tejido vivo natural.






</doc>
<doc id="2832" url="https://es.wikipedia.org/wiki?curid=2832" title="Técnicas de pintura">
Técnicas de pintura

Las técnicas de pintura se dividen de acuerdo a cómo se diluyen y fijan los pigmentos sobre el soporte a pintar. En general, si los pigmentos no son solubles en el aglutinante permanecen dispersos en él. Si el artista piensa en la perdurabilidad de su obra debe conocer las técnicas a emplear para que salga bien su trabajo y se sienta orgulloso

Cuando el vehículo empleado para fijar el pigmento es, en la mayoría de los casos, goma arábiga y el solvente es el agua. Las acuarelas son pigmentos muy finamente molidos y aglutinados en goma arábiga, que se obtiene de las acacias. La goma se disuelve fácilmente en agua y se adhiere muy bien al papel (soporte por excelencia para la acuarela). La goma además actúa como barniz, claro y delgado, dando mayor brillo y luminosidad al color. En un principio la goma arábiga se usaba sola, pero más tarde se añadieron otros componentes para retrasar el secado y añadir transparencia. La acuarela requiere del artista seguridad en los trazos y espontaneidad en la ejecución, ya que su mayor mérito consiste en la frescura y transparencia de los colores. Son pinturas a base de pigmentos muy diminutos los cuales en la mayoría de los casos la acuarela es disuelta en agua y los colores son claros.

Al "gouache" o "aguada" se le llama también "el color con cuerpo". Es una pintura al agua, opaca, hecha con pigmento molido menos fino que el de las acuarelas, y por ello es menos transparente. Al igual que la acuarela, su medio —o aglutinante— es la goma arábiga, aunque muchos gouaches modernos contienen plástico. El medio está ampliado con pigmento blanco, que es lo que lo hace más opaco, menos luminoso y menos transparente que la acuarela, pero a cambio los colores producidos son más sólidos.

En esta técnica se usan pinturas acrílicas en aerosol o sprays, además de esmaltes, ya que con este método la pintura se vuelve mucho más desgastada. Cuando a la pintura no se le incorporan estos difusores y se deja caer por gravedad esta técnica se denomina "dripping".

Los pasteles son pigmentos en polvo mezclados con la suficiente goma o resina para aglutinarlos formando una pasta seca y compacta. La palabra pastel deriva de la pasta con la que se elaboran estas pinturas. Esta pasta se moldea en la forma de una barrita del tamaño aproximado de un dedo, que se usa directamente sobre la superficie al trabajar (generalmente papel o madera). Son colores fuertes y opacos cuya mayor dificultad es la adhesión del pigmento a la superficie al pintar, por ello suelen usarse al finalizar el dibujo fijadores atomizados (spray) especiales. El pastel generalmente se usa como el "crayón" o el "grafito" (lápiz), y su recurso expresivo más afín es la línea con la cual se pueden hacer tramas. También suele usarse el polvo que tiende a soltar el pastel (semejante al de la "tiza") para aplicar color.

Cuando el aglutinante es una emulsión, generalmente de yema de huevo, agua y aceite. Tradicionalmente se mezcla la yema con el agua y el aceite, pero también se puede formar una emulsión con harina e incluso con yeso. Grandes obras maestras como por ejemplo "El nacimiento de Venus" de Sandro Botticelli se han realizado utilizando la técnica del temple al huevo.

A menudo el término fresco se usa incorrectamente para describir muchas formas de pintura mural. El verdadero fresco es a las técnicas pictóricas modernas lo que el latín es a los idiomas modernos. La técnica del fresco se basa en un cambio químico: los pigmentos de tierra molidos y mezclados con agua pura, se aplican sobre una argamasa reciente de cal y arena, mientras la cal está aún en forma de hidróxido de calcio. Debido al dióxido de carbono de la atmósfera, la cal se transforma en carbonato cálcico, de manera que el pigmento cristaliza en el seno de la pared. Los procedimientos para pintar al fresco son sencillos pero laboriosos, y consumen muchísimo tiempo. En la preparación de la cual se tardaba dos años.

La presentación de la tinta, también llamada "tinta china", es generalmente líquida aunque también puede ser una barra muy sólida que debe ser molida y diluida previamente. Se usa sobre papel, y los colores de tinta más empleados son el negro y el "sepia", aunque actualmente se usen muchos otros más. La tinta se aplica de diversas maneras, por ejemplo con plumas o plumillas que son más adecuadas para dibujo o caligrafía, y no para pinturas. Las diferentes puntas de plumillas se utilizan cargadas de tinta para hacer líneas y con ellas dibujar o escribir. Otro recurso para aplicar la tinta es el pincel, que se maneja básicamente como la acuarela y que se llama aguada, no obstante la técnica milenaria llamada caligrafía o escritura japonesa también se realiza con tinta y pincel sobre papel. Otras formas más utilitarias de usar la tinta es en tiralíneas (cargador de tinta) o rapidograph. La tinta junto al grafito son más bien técnicas de dibujo.

La tinta neutra es una técnica frecuente en la restauración de pintura mural. Se utiliza cuando el restaurador se encuentra con grandes pérdidas y desconoce como era el original. Consiste en aplicar un color uniforme en la zona perdida, que no moleste en exceso y que entone con el colorido general de la obra.

Cuando se emplean diversas técnicas en un mismo soporte. El "collage", por ejemplo, es una técnica artística (no pictórica por no ser pintada), se convierte en una técnica mixta cuando tiene intervenciones con gouache, óleo o tinta.
Como muestra de sus posibilidades artísticas, se cita la Técnica introducida por el pintor Carlos Benítez Campos desde principios de siglo, la cual consiste en pintar al óleo un acontecimiento cualquiera de la época, sobre el papel pegado de las noticias en prensa que lo publican.

Sería conveniente distinguir entre "procedimiento pictórico" y "técnica pictórica". Se entiende por "procedimiento pictórico" la unión de los elementos que constituyen el aglutinante o adhesivo, y los pigmentos. La forma de aplicar ese procedimiento pictórico se denomina "técnica pictórica".

El "décollage" designa a la técnica opuesta al "collage".



</doc>
<doc id="2833" url="https://es.wikipedia.org/wiki?curid=2833" title="Tarot (adivinación)">
Tarot (adivinación)

El tarot es una baraja de naipes a menudo utilizada como medio de consulta e interpretación de hechos (presentes, pasados o futuros), sueños, percepciones o estados emocionales que constituye, además, un tipo de cartomancia. Sus orígenes datan al menos del siglo XIV. La técnica se basa en la selección de cartas de una baraja especial, que luego son interpretadas por un lector, según el orden o disposición en que han sido seleccionadas o repartidas.
La baraja de tarot está compuesta por 78 cartas, divididas en arcanos mayores y menores. La palabra «arcano» proviene del latín "arcanum", que significa "misterio" o "secreto".

Las cartas de los arcanos mayores, 22 en total, son:

Esta última es la única carta que, según el tipo de naipe o su edición, puede no estar numerada o bien corresponderle el número cero. Asimismo, los 22 Arcanos Mayores se conocen como "triunfos" ("atouts", en francés; "atutti" en italiano), lo que significa "por encima de todo".

Los arcanos menores son un conjunto de 56 cartas divididas en 4 palos de 14 cartas: espadas, copas, bastos y oros, como en la baraja española, pero del modelo más antiguo: cartas numeradas del uno al diez, más los personajes de la corte: "sota", "caballo" (o "caballero"), "reina" y "rey"; los que se cree que eran los cuatro niveles sociales durante los tiempos medievales: la nobleza, simbolizada por las espadas; los campesinos, por los bastos; el clero, por las copas, y los comerciantes, por los oros.

El diseño de los naipes es diverso, aunque existen diseños clásicos como el del tarot de Marsella (finales del siglo XVII), que ha servido como guía en la elaboración de las figuras y su simbología. Una baraja muy popular y actualmente la más reconocida es el Rider-Waite-Smith Tarot, (o Rider-Waite o simplemente Rider), ideado en 1910 por Arthur Edward Waite, elaborado por su discípula Pamela Colman Smith e impreso por la Rider Company. Otra baraja común es el Book of Thoth Tarot, ideado entre 1938 y 1942 por el mago inglés Aleister Crowley y realizado por su discípula Frieda Harris; esta baraja se publicó en 1944, en blanco y negro, junto con "El Libro de Thoth", que explica la simbología y uso, pero fue editado con sus colores originales sólo hasta 1977, en Nueva York, por US Games Systems y Samuel Weiser.

Las primeras referencias al tarot aparecen en el siglo XV en Italia. La baraja más antigua es el tarot del duque de Milán, Filippo María Visconti (1412-1447), hoy día en la Biblioteca de la Universidad Yale. Es conocida actualmente como la baraja Visconti-Sforza, posiblemente para celebrar el casamiento de su hija Bianca Maria con su sucesor el futuro duque Francisco I Sforza.

De acuerdo al historiador italiano Giordano Berti, algunas imágenes del tarot de Filippo María Visconti son iguales a las de otra baraja diseñada por el duque en 1415: el juego Los XVI Héroes.

En estudios realizados por ocultistas de los siglos XVIII y XIX, como Antoine Court de Gebelin, Eliphas Levi y el doctor Gérard Encausse (Papus), se intenta demostrar la conexión existente entre el tarot y la cábala, así como con el simbolismo egipcio.

Según plantean los investigadores Daniel Rodes y Encarna Sánchez, el origen del tarot habría que buscarlo entre los cátaros medievales y la cultura occitana, cuya filosofía encaja perfectamente en la idea básica del juego de tarot.

Así, la presencia de una papisa, la importancia de los personajes femeninos y claras referencias a un cristianismo distinto al de la ortodoxia romana harían pensar en un uso original del tarot como una transmisión de un conocimiento filosófico, si bien con el paso del tiempo pasarían a ser usadas como un sistema adivinatorio. Pero la papisa fue, en realidad, un símbolo de la fe cristiana, como demuestran numerosas obras de arte de la Edad Media.

Otros autores afirman que los gitanos, en su deambular por los países europeos, promovieron el tarot como un sistema adivinatorio. Hay, de hecho, quien sostiene que el tarot logró sobrevivir a la Inquisición, ya que los gitanos no representaban objetivos prioritarios de la jurisdicción inquisitorial, por los que ellos, sus conocidas prácticas esotéricas y sus efectos personales consiguieron zafarse de la persecución y la hoguera y llegar hasta nuestros días. Pero es cierto que los gitanos llegaron a Europa cuando el tarot era ya conocido. Por otra parte, el tarot se juega en Italia desde el siglo XV, y en el siglo siguiente se propagó en muchas regiones de Europa: en primer lugar Francia, después Suiza, Bélgica, Alemania y Austria. La adivinación con el tarot aparece con seguridad en Italia y Francia en el siglo XVIII.

A fines del siglo XVIII y comienzos del XIX las cartas del tarot fueron asociadas al misticismo y a la magia. La tradición comenzó en 1781, cuando Antoine Court de Gébelin, un clérigo suizo y francmasón, publicó "Le Monde Primitif", un estudio especulativo sobre el simbolismo religioso antiguo y sus remanentes en el mundo moderno. De Gébelin argumentaba que el simbolismo del tarot de Marsella representaba los misterios de Isis y Thoth. Gébelin más tarde afirmó que el nombre "tarot" venía de los vocablos egipcios "tar", que significa "real", y "ro", que significa "camino", y que el tarot representaba, por lo tanto, un "camino real" a la sabiduría.

Gébelin arguyó estos y similares puntos de vista en forma dogmática; no presentó evidencias para sostener sus argumentos. Además, Gébelin escribió antes de que Champollion hubiera descifrado los jeroglíficos egipcios. Los modernos egiptólogos nada encontraron en el lenguaje egipcio que sustentara las fantasiosas etimologías de Gébelin, pero estos descubrimientos llegaron demasiado tarde. Cuando se dispuso de los auténticos textos egipcios, ya estaba firmemente establecida la identificación de las cartas del tarot con el "Libro de Thot" egipcio en la práctica ocultista.

Aunque las cartas del tarot se usaban para predecir la fortuna en Bolonia, en el siglo XVIII, fueron publicadas originalmente como un método de adivinación por Jean-Baptiste Alliette, también llamado "Etteilla", un ocultista francés que revirtió las letras de su nombre y trabajó como adivino poco antes de la revolución francesa. Etteilla diseñó el primer mazo de tarot esotérico y añadió atribuciones astrológicas y motivos "egipcios" a varias cartas, alterando muchos de los diseños marselleses y añadiendo significados adivinatorios en el texto de las cartas. Los mazos de Etteilla, aunque ahora eclipsados por los ilustrados de Smith y Waite y el mazo "Thoth" de Aleister Crowley, aún se encuentran disponible. 

Más tarde, Marie-Anne Le Normand popularizó la adivinación y la profecía durante el reinado de Napoleón I. Esto se debió en parte a la influencia que tuvo sobre Josefina de Beauharnais, la primera esposa de Napoleón. Sin embargo, ésta no usaba el tarot habitualmente.

El interés en el tarot para la adivinación a cargo de otros ocultistas llegó después, durante el auge de los herméticos, de la década de 1840, en la cual (entre otros) estuvo involucrado Víctor Hugo. La idea de las cartas como clave mística fue desarrollada posteriormente por Eliphas Lévi y pasó al mundo de habla inglesa por la Orden Hermética del Alba Dorada. Lévi, y no Etteilla, es considerado por algunos el verdadero fundador de las escuelas más contemporáneas de tarot; su "Dogme et Ritual de la Haute Magie", de 1854, introdujo una interpretación de las cartas que las relacionaba con la Cábala. Aunque Lévi aceptó las afirmaciones de Court de Gébelin sobre un origen egipcio de los símbolos de las cartas, rechazó las innovaciones de Eteilla y su mazo alterado y arregló en su lugar un sistema que relacionaba al tarot, especialmente al tarot de Marsella, con la cábala y con los cuatro elementos de la alquimia. Por otro lado, algunos significados adivinatorios de Etteilla todavía son usados por algunos lectores de tarot.

La lectura del tarot se enmarca en la creencia de que las cartas pueden ser usadas para comprender situaciones actuales y futuras de la persona consultante. Algunos dicen que las cartas son guiadas por una fuerza espiritual como guía, mientras otros creen que las cartas los ayudan en introducirse a un inconsciente colectivo. Uno de los métodos más utilizados son las tiradas, que consisten en voltear un número de cartas que previamente han sido barajadas al azar y repartidas en un cierto orden boca abajo, y darle una interpretación (valor o significado) a cada carta según la posición relativa en la que se encuentre sobre la mesa y en relación con las cartas adyacentes, y el tarotista formula su interpretación sobre su significado. Existen además programas de cómputo o aplicaciones para Facebook o teléfono móvil que replican las tiradas con cartas.

Existen distintas configuraciones utilizadas para las tiradas:

Aun cuando es conocido el interés que el psiquiatra suizo Carl Gustav Jung mostró por diversas mancias, como el "I Ching", la astrología o el significado del tarot, no escribió obra o tratado alguno sobre este último, y se ciñó a esporádicas alusiones contenidas en sus obras completas. 

Como suele ser habitual, serán realmente sus discípulos quienes desarrollarán y amplificarán los fundamentos arquetípicos junguianos, así como su principio de sincronicidad en el tarot, de entre los cuales destaca la analista Sallie Nichols y su obra "Jung y el tarot. Un viaje arquetípico". En ella se reitera ya desde el mismo prólogo el uso que hace Nichols de la obra junguiana para desarrollar su propia propuesta del tarot, integrando psicología analítica y dicha mancia.

En virtud del principio de sincronicidad por él postulado, la psique humana es capaz de intuir el presente, el pasado y el futuro del "continuum" espacio-temporal en el momento de la tirada de cartas; dicho de otro modo, en el momento de la echada de cartas, las imágenes simbólico-arquetípicas resultantes de la tirada mantienen una relación sincronizada con acontecimientos pasados, presentes y futuros. Desde la perspectiva junguiana, las cartas del tarot se ven a su vez como representantes simbólico-arquetípicos de tipos fundamentales de personas o situaciones del inconsciente colectivo. La carta del Emperador, por ejemplo, representa posiblemente la figura del patriarca o del padre, la autoridad en el plano temporal en general, mientras que la carta del Papa representa la autoridad en el plano espiritual, la sabiduría teológica, etcétera.

Algunas escuelas del pensamiento oculto y del estudio de los símbolos, como la Orden Hermética del Alba Dorada, consideran el tarot como un libro de texto y un artilugio mnemotécnico para sus enseñanzas. Ésta puede ser la causa de que la palabra "arcanos" (o "arcana") sirve para describir dos secciones del mazo del tarot: "arcana" es la forma plural de la palabra latina "arcanum", que significa "cerrado" o "escondido".

Cada carta tiene una asignación de significados arbitraria. Los mismos están relacionados con los grandes arquetipos universales (en este sentido, los significados pueden ser solo alusiones para dar flexibilidad en la interpretación). El conjunto de los significados de cada carta forma un universo semántico, rico en interpretación (filosófica, situacional). 

Cada carta de un tarot cuenta con una ilustración que sirve como referencia memorística, en la cual es importante la selección de iconos y colores, ya que cada color tiene un valor simbólico (por ejemplo, azul-espiritualidad).

Ligado al número de cartas, hay toda una tradición acerca del significado de cada número.

La tradición divide el tarot en: espadas (elemento aire, pensamiento e inteligencia), bastos (elemento fuego, vida, pasiones), copas (elemento agua,
amor y sentimientos) y oros o pentáculos (elemento tierra, naturaleza, materia, lo económico).

Aunque este elemento no es forzoso, es importante para reutilizar el aprendizaje de otros tarots.

Desde el punto de vista científico, es imposible conocer los hechos futuros a través de método adivinatorio alguno. Tampoco es posible describir una situación actual sin disponer de información.




</doc>
<doc id="2834" url="https://es.wikipedia.org/wiki?curid=2834" title="Honda (arma)">
Honda (arma)

La honda es una de las armas más antiguas de la Humanidad. Consiste básicamente en dos cuerdas o correas en cuyos extremos se sujeta un receptáculo flexible desde el que se dispara un proyectil. Agarrado el artilugio por los otros dos extremos opuestos, se voltea de manera que el proyectil adquiera velocidad y después se suelta una de las cuerdas para liberarlo, alcanzando este gran distancia y poder de impacto. Los materiales empleados en su construcción son muy diversos, tradicionalmente cuero, fibras textiles, tendones, crin, etc. Los proyectiles pueden ser piedras naturales redondeadas, o labradas con bastante precisión, arcilla cocida o secada al sol, plomo moldeado, etc.

En el contexto de usos de entretenimiento y juegos infantiles, se emplea equivocadamente el término «honda» para designar a lo que en varios países de Latinoamérica se llama resortera y gomera, y en España, tirachinas. Sin embargo, este último es un instrumento de reciente aparición, ligada al uso del caucho.

El origen de la honda se remonta a los tiempos prehistóricos, quizás al final del Paleolítico, en el que se usaría exclusivamente como arma de caza. Pero las evidencias arqueológicas de su existencia corresponden ya a la época del Neolítico, cuando aparecen en el área de Oriente Próximo grandes cantidades de proyectiles de arcilla cocida, asociados a usos bélicos.

Como herramienta asociada al pastoreo la honda se usaría desde el Neolítico hasta nuestros días.

En épocas clásicas, además del famoso uso de David contra Goliat, la honda fue usada por griegos, cartagineses, romanos, etc.

Fueron famosos en todo el orbe antiguo los honderos baleares, que eran contratados como mercenarios por los diferentes ejércitos de la Antigüedad. Eran entrenados desde la infancia en la destreza con la honda y llevaban tres tipos de distinta longitud, según la distancia de lanzamiento. Se decía que su precisión y potencia no tenían parangón.

El uso de proyectiles de plomo, inventado por los griegos, haría de la honda un arma temible dada su mayor potencia de impacto y alcance; a esto se unía el pequeño tamaño de los proyectiles, que eran capaces de penetrar en el cuerpo a la manera de una bala, y lo mismo que ella eran invisibles por el aire. Como arma de guerra, la honda se utilizaría todavía durante toda la Edad Media, llegando a convivir incluso con los primitivos cañones.

Los incas ya conocían el uso de la honda, a la que denominaban "huaraca", ellos difundieron el uso de esta arma para la caza de pequeños animales o aves. Los diaguitas y los mapuches adoptaron esta arma gracias al contacto con los incas. Hoy en día, aún es usada en los pueblos indígenas altiplánicos.



</doc>
<doc id="2837" url="https://es.wikipedia.org/wiki?curid=2837" title="Tomaso Albinoni">
Tomaso Albinoni

Tomaso Giovanni Albinoni (Venecia, 30 de junio de 1669-ibídem, 17 de enero de 1751) fue un compositor italiano del Barroco. En su época fue célebre como compositor de ópera, pero actualmente es conocido sobre todo por su música instrumental, parte de la cual se graba con regularidad. El "Adagio en sol menor" constituye su obra más difundida pese a que, en realidad, se trata de una obra apócrifa compuesta en el siglo XX por el musicólogo y especialista en su obra Remo Giazotto.

Era hijo de Antonio Albinoni (1634-1709), un rico comerciante de papel en Venecia. Estudió violín y canto. Se sabe relativamente poco de su vida, si se tiene en cuenta su importancia contemporánea como compositor y el hecho de que vivió durante un período relativamente bien documentado. En 1694 dedicó su Opus 1 a su compatriota veneciano cardenal Pietro Ottoboni (sobrino-nieto del papa Alejandro VIII). Ottoboni era un mecenas de otros compositores en Roma, como Arcangelo Corelli. Es probable que Albinoni fuera contratado en 1700 como violinista por Fernando Carlo, duque de Mantua, a quien le dedicó su colección de piezas instrumentales Opus 2. En 1701 escribió sus muy populares suites Opus 3, y dedicó tal colección al Gran Duque Fernando III de Toscana.

En 1705 se casó. Antonino Biffi, el "maestro di cappella" de San Marcos de Venecia fue testigo de su boda, y evidentemente era amigo de Albinoni. Sin embargo, no parece que Albinoni tuviera ninguna otra relación con ese establecimiento que tanto destacaba musicalmente en Venecia. Logró su temprana fama como compositor de ópera en muchas ciudades de Italia, incluyendo Venecia, Génova, Bolonia, Mantua, Udine, Piacenza y Nápoles. Durante esta época compuso abundante música instrumental: antes de 1705, escribió sobre todo sonatas en trío y conciertos para violín, pero entre esa fecha y 1719 se dedicó más a sonatas para solo y conciertos para oboe.

A diferencia de la mayor parte de los compositores de su época, parece que nunca buscó un puesto en la Iglesia o en la corte, pero lo cierto es que era un hombre independiente con recursos propios. En 1722, Maximiliano II Manuel de Baviera, a quien Albinoni había dedicado un conjunto de doce conciertos, le invitó a dirigir dos de sus óperas en Múnich.

Alrededor de 1740, una colección de sonatas para violín se publicó en Francia como una obra póstuma, y los eruditos supusieron durante mucho tiempo que ello significaba que Albinoni había muerto para entonces. Sin embargo, parece que siguió viviendo en Venecia sin que haya llegado hasta nosotros ninguna composición en este último período de su vida. Un archivo de la parroquia de San Bernabé indica que Tomaso Albinoni falleció en 1751 «a la edad de 79 años», de diabetes.

Escribió una cincuentena de óperas, de las cuales 28 se representaron en Venecia entre 1723 y 1740, pero actualmente es más conocido por su música instrumental, especialmente sus conciertos para oboe.

Su música instrumental atrajo la atención de Johann Sebastian Bach, quien escribió al menos dos fugas sobre temas de Albinoni y utilizó constantemente sus bajos como ejercicios de armonía para sus alumnos.

Parte de la obra de Albinoni se perdió durante el bombardeo de Dresde durante la Segunda Guerra Mundial, con la destrucción de la Biblioteca estatal de Dresde, así que se sabe poco de su vida y su música posterior a mediados de los años 1720.

Su fama se incrementó en gran medida con la publicación del conocido como Adagio de Albinoni, compuesto en 1945 por el musicólogo italiano Remo Giazotto. Publicado por primera vez en 1958 por la editorial Casa Ricordi, el editor lanzó como argumento de venta que el autor se había basado en unos fragmentos de un movimiento lento de una sonata a trío para cuerdas y órgano de Tomaso Albinoni presumiblemente encontrados en las ruinas de la Biblioteca de Dresde tras los bombardeos de la ciudad acaecidos en la Segunda Guerra Mundial. La obra fue compuesta por Giazotto basándose supuestamente en el fragmento que pudo rescatar de la partitura original, en el que apenas se apreciaba el bajo continuo y seis compases de melodía. Sin embargo, una prueba seria de la existencia de tales fragmentos no ha sido nunca encontrada; por el contrario la «Staatsbibliothek Dresden» ha desmentido formalmente tenerlas en su colección de partituras.




El "Adagio en sol menor" de Remo Giazotto, atribuido a Albinoni, ha logrado tal fama que se transcribe normalmente para otros instrumentos y se usa en la cultura popular, por ejemplo, en la banda sonora de películas como "Gallipoli", de 1981, que se ambienta en 1915–1916 durante la batalla del mismo nombre de la Primera Guerra Mundial, y programas de televisión y en anuncios.

En el álbum del grupo The Doors, "An American Prayer", Jim Morrison recita poesía con lo que parece ser un arreglo musical adaptado del "Adagio en sol menor" tocado en el fondo de «Feast of Friends». La obra de Yngwie Malmsteen "Icarus Dream Suite Op. 4" se inspira y se basa principalmente en el Adagio.

Entre otros cantantes actuales que han utilizado el "Adagio" como base para sus baladas se encuentran: Camilo Sesto, Lara Fabian, Ricardo Montaner, Sarah Brightman y Rosa López.




</doc>
<doc id="2839" url="https://es.wikipedia.org/wiki?curid=2839" title="Topología">
Topología

La topología (del griego τόπος, 'lugar', y λόγος, 'estudio') es la rama de las matemáticas dedicada al estudio de aquellas propiedades de los cuerpos geométricos que permanecen inalteradas por transformaciones continuas. Es una disciplina que estudia las propiedades de los espacios topológicos y las funciones continuas. La topología se interesa por conceptos como "proximidad", "número de agujeros", el tipo de "consistencia" (o "textura") que presenta un objeto, comparar objetos y clasificar múltiples atributos donde destacan conectividad, compacidad, metricidad o metrizabilidad, entre otros.

Los matemáticos usan la palabra "topología" con dos sentidos: informalmente es el sentido arriba especificado, y de manera formal es la referencia a una cierta familia de subconjuntos de un conjunto dado, familia que cumple unas reglas sobre la unión y la intersección —este segundo sentido puede verse desarrollado en el artículo espacio topológico—.

Coloquialmente, se presenta a la topología como la «geometría de la página de goma (chicle)». Esto hace referencia a que, en la geometría euclídea, dos objetos serán equivalentes mientras podamos transformar uno en otro mediante isometrías (rotaciones, traslaciones, reflexiones, etc.), es decir, mediante transformaciones que conservan las medidas de ángulo, área, longitud, volumen y otras.

En topología, dos objetos son equivalentes en un sentido mucho más amplio. Han de tener el mismo número de "trozos", "huecos", "intersecciones", etc. En topología está permitido doblar, estirar, encoger, retorcer, etc., los objetos, pero siempre que se haga sin romper ni separar lo que estaba unido, ni pegar lo que estaba separado. Por ejemplo, un triángulo es topológicamente lo mismo que una circunferencia, ya que podemos transformar uno en otra de forma continua, sin romper ni pegar. Pero una circunferencia no es lo mismo que un segmento, ya que habría que partirla (o pegarla) por algún punto.

Esta es la razón de que se la llame la «geometría de la página de goma», porque es como si estuviéramos estudiando geometría sobre un papel de goma que pudiera contraerse, estirarse, etc.

Un chiste habitual entre los topólogos (los matemáticos que se dedican a la topología) es que «un topólogo es una persona incapaz de distinguir una taza de una rosquilla». Pero esta visión, aunque muy intuitiva e ingeniosa, es sesgada y parcial. Por un lado, puede llevar a pensar que la topología trata solo de objetos y conceptos geométricos, siendo más bien al contrario, es la geometría la que trata con un cierto tipo de objetos topológicos. Por otro lado, en muchos casos es imposible dar una imagen o interpretación intuitiva de problemas topológicos o incluso de algunos conceptos. Es frecuente entre los estudiantes primerizos escuchar que «no entienden la topología» y que no les gusta esa rama; generalmente se debe a que se mantienen en esta actitud gráfica. Por último, la topología se nutre también en buena medida de conceptos cuya inspiración se encuentra en el análisis matemático. Se puede decir que casi la totalidad de los conceptos e ideas de esta rama son conceptos e ideas topológicas.

Observemos un plano del metro de Madrid. En él están representadas las estaciones y las líneas de metro que las unen, pero no es "geométricamente" exacto. La curvatura de las líneas de metro no coincide, ni su longitud a escala, ni la posición relativa de las estaciones... Pero aun así es un plano perfectamente útil. Sin embargo, este plano es exacto en cierto sentido pues representa fielmente cierto tipo de información, la única que necesitamos para decidir nuestro camino por la red de metro: "información topológica".

Históricamente, las primeras ideas topológicas conciernen al concepto de límite y al de completitud de un espacio métrico, y se manifestaron principalmente en la crisis de los inconmesurables de los pitagóricos, ante la aparición de números reales no racionales. El primer acercamiento concreto al concepto de límite y también al de integral aparece en el método de exhaución de Arquímedes. La aparición del análisis matemático en el siglo XVII puso en evidencia la necesidad de formalizar los conceptos de proximidad y continuidad, y la incapacidad de la geometría para tratar este tema. Fue precisamente la fundamentación del cálculo infinitesimal, así como los intentos de formalizar el concepto de variedad en Geometría lo que llevó a la aparición de la topología, a finales del siglo XIX y principios del XX.

Se suele fechar el origen de la topología con la resolución por parte de Euler del problema de los puentes de Königsberg, en 1735. Ciertamente, la resolución de Euler del problema utiliza una forma de pensar totalmente topológica, y la solución del problema nos lleva a la característica de Euler, el primer invariante de la topología algebraica, pero sería muy arriesgado y arbitrario fechar en ese momento la aparición de la topología. La situación es exactamente análoga a la del cálculo del área de la elipse por Arquímedes.

El término "topología" fue usado por primera vez por Johann Benedict Listing, en 1836 en una carta a su antiguo profesor de la escuela primaria, Müller, y posteriormente en su libro "Vorstudien zur Topologie" ('Estudios previos a la topología'), publicado en 1847. Anteriormente se la denominaba "analysis situs". Maurice Fréchet introdujo el concepto de espacio métrico en 1906.

En el artículo Glosario de topología se encuentra una colección de términos topológicos con su significado. Aquí y ahora nos limitaremos a dar algunas nociones básicas.

Como hemos dicho, el concepto fundamental de la topología es la "relación de proximidad", que puede parecer ambigua y subjetiva. El gran logro de la topología es dar una formulación precisa, objetiva y útil de este concepto. Para ello tomamos un conjunto de referencia formula_1, que será el ambiente en el que nos moveremos, y al que llamaremos espacio. Tomaremos un elemento cualquiera formula_2 de formula_1. A los elementos del espacio se les llama puntos, así que formula_2 será llamado punto, independientemente de que formula_2 sea una función, un vector, un conjunto, un ideal maximal en un anillo conmutativo y unitario... Un subconjunto formula_6 de formula_1 será un entorno de formula_2 si formula_6 incluye un conjunto abierto formula_10 de manera que formula_2 es elemento de formula_10. ¿Qué entenderemos por conjunto abierto? Aquí está el quid de la cuestión: una colección formula_13 de subconjuntos de formula_1 se dirá que es una topología sobre formula_1 si formula_1 es uno de los elementos de esa colección, si formula_17 es un elemento de la colección, si la unión de elementos de la colección da como resultado un elemento de la colección y si la intersección finita de elementos de la colección también es un elemento de la colección. A los elementos de la colección formula_13 se les denomina abiertos de la topología formula_13, y al par formula_20 se le denomina espacio topológico.

Las condiciones para que formula_13 sea topología sobre formula_1 son entonces estas:

Puede parecer extraño que de una definición tan altamente formal y conjuntista se obtenga una formulación precisa del concepto de proximidad. Lo primero que se observa es que sobre un mismo espacio formula_1 se pueden definir distintas topologías, generando entonces distintos espacios topológicos. Por otra parte, precisamente la manera en que quede determinada una topología sobre un conjunto (es decir, la elección del criterio que nos permita decidir si un conjunto dado es o no abierto) es lo que va a dar carácter "visualizable" o no a ese espacio topológico.

Una de las maneras más sencillas de determinar una topología es mediante una distancia o métrica, método que sólo es aplicable en algunos casos (si bien es cierto que muchos de los casos más intersantes de topologías en la Geometría y del Análisis Matemático pueden determinarse mediante alguna distancia). Una distancia sobre un conjunto formula_1 es una aplicación formula_28 que verifica las siguientes propiedades:

cualesquiera que sean formula_34.

Si tenemos definida una distancia sobre formula_1, diremos que la pareja
es un espacio métrico. Dado un espacio métrico formula_37, queda determinada una topología sobre formula_1 en la que los conjuntos abiertos son los subconjuntos formula_10 de formula_1 tales que cualquiera que sea el punto formula_2 de formula_10 existe un número formula_43 de tal manera que el conjunto formula_44 está totalmente incluido en formula_10. Al conjunto formula_44 se le denomina bola abierta de centro formula_2 y radio formula_48, y será precisamente un entorno del punto formula_2.

Como se ha apuntado antes, por desgracia no toda topología proviene de una distancia, es decir, existen espacios topológicos que no son espacios métricos. Cuando un espacio topológico es además espacio métrico (esto es, cuando dada una topología sobre un conjunto, puede definirse en ese conjunto una distancia de manera que la topología generada por la distancia coincida con la topología dada) se dice que el espacio topológico es metrizable. Un problema clásico en topología es el de determinar qué condiciones debe satisfacer un espacio topológico para que sea metrizable.

Se suelen considerar principalmente tres ramas:

Además de estas tres ramas, que podríamos decir propiamente topológicas, la implicación en mayor o menor medida en otras disciplinas matemáticas hacen que muchos consideren parte de la topología al análisis funcional, la teoría de la medida, la teoría de nudos (parte de la topología de dimensiones baja), la teoría de grupos topológicos, etc. Es fundamental su contribución a la teoría de grafos, análisis matemático, ecuaciones diferenciales, ecuaciones funcionales, variable compleja, geometría diferencial, geometría algebraica, álgebra conmutativa, estadística, teoría del caos, geometría fractal... Incluso tiene aplicaciones directas en biología, sociología, etc.

Constituye la base de los estudios en topología. En ella se desarrollan tópicos como lo que es un espacio topológico o los entornos de un punto.

Sea formula_1 un conjunto cualquiera y formula_51 el conjunto de sus partes. Una topología sobre formula_1 es un conjunto formula_53 que cumpla que formula_54, formula_55, si formula_56 entonces
formula_57, y que si formula_58 entonces formula_59. A los elementos de formula_13 se les denomina conjuntos abiertos. Al par formula_20 se le denomina espacio topológico. A los elementos de formula_1 se les suele denominar puntos.

Nótese que desde un primer momento hemos especificado que el conjunto formula_1 es cualquiera, no necesariamente un conjunto de naturaleza geométrica. La denominación de espacio (topológico) y
de punto se mantiene aun cuando formula_1 sea un conjunto de números, de funciones, de ecuaciones diferenciales, de figuras geométricas, de vectores, de conjuntos...

Como puede observarse, la definición es muy formal y general, y lo primero que se observa es que sobre un mismo conjunto pueden darse multitud de topologías distintas. Así es. Pero de momento,
los conceptos de conjunto abierto en formula_65 o en formula_66 o formula_67 cumplen las condiciones exigibles a una topología. Es precisamente el comprobar que otras familias de conjuntos en otros conjuntos de naturaleza no geométrica que comparten estas mismas propiedades (como en el conjunto de soluciones de una ecuación diferencial, o el conjunto de los ceros de los polinomios con coeficientes en los ideales en un anillo conmutativo, por ejemplo) lo que motiva esta definición. Así podremos aplicar a estos conjuntos las mismas (o parecidas) técnicas topológicas que aplicamos a los abiertos del plano, por ejemplo. La situación es análoga a la que se da en Álgebra Lineal cuando se pasa de trabajar en formula_66 o formula_67 a trabajar en espacios vectoriales arbitrarios.

En lo que sigue, formula_20 representará siempre un espacio topológico.

Ligado al concepto de conjunto abierto está el de conjunto cerrado. Un conjunto formula_71 se dice que es cerrado si su complementario formula_72 es un conjunto
abierto. Es importante observar que un conjunto que no es abierto no necesariamente ha de ser
cerrado, y un conjunto que no sea cerrado no necesariamente ha de ser abierto. Así, existen
conjuntos que son abiertos y cerrados a la vez, como formula_17, y pueden existir conjuntos que no sean ni abiertos ni cerrados.

Es inmediato comprobar que la intersección de cerrados es un conjunto cerrado, que la unión de
una cantidad finita de conjuntos cerrados es un conjunto cerrado, y que tanto formula_1 como formula_17 son conjuntos cerrados.

Si formula_76, el conjunto formula_77 es una topología para formula_78. Se dirá entonces que el espacio formula_79 es subespacio topológico del formula_20.

La noción de subespacio topológico se presenta de manera natural, y es el concepto análogo al de
subgrupo en Teoría de Grupos o al de subespacio vectorial en Álgebra Lineal.

Una propiedad relativa a espacios topológicos se dice que es hereditaria cuando si un
espacio la tiene, entonces también la tiene cualquiera de sus subespacios.

Una familia formula_81 se dice que es base (de la topología formula_13) si para cualquiera que sea el formula_83 existe un conjunto formula_84 de manera que formula_85.

No siempre es cómodo trabajar con una topología. A veces resulta más complicado establecer una
topología que una base de topología (como en espacios métricos). En cualquier caso, una base
es una manera muy cómoda de establecer una topología. Aún más sencillo es establecer una subbase,
que es una familia de conjuntos para la que el conjunto de sus intersecciones finitas forma una
base de topología. Uno de los casos más importantes de topología, la de los espacios métricos,
viene dado por una base, la del conjunto de bolas abiertas del espacio.

Un espacio topológico se dice que cumple el Segundo Axioma de Numerabilidad (IIAN) si existe alguna base de su topología que tenga cardinalidad numerable.

Sea formula_86 un conjunto cualquiera y sea formula_87 un punto arbitrario. Se dice que formula_88 es entorno de formula_2 si existe un conjunto abierto formula_90 de manera que formula_91. Todo conjunto abierto es entorno de todos sus puntos. Al conjunto de todos los entornos de un punto formula_2 se le denomina sistema de entornos de formula_2.

Obsérvese que no se ha exigido que un entorno sea un conjunto abierto. Los entornos abiertos son
un tipo de entornos muy útiles (sobre todo en Geometría y Análisis) y muy usados, tanto que en
muchas ocasiones se omite el calificativo "abierto". Esto es un abuso de lenguaje y debe
evitarse.

Una colección formula_94 de entornos de un mismo punto x se dice que es una base de entornos (o base local) de formula_2 si dado cualquier entorno formula_6 de formula_2 existe un formula_98 de manera que formula_99.

Se dice que un espacio topológico cumple el Primer Axioma de Numerabilidad (IAN) si cada punto del espacio tiene alguna base local de cardinal numerable.

Ahora podemos establecer una serie de definiciones de gran importancia, pues serán las piezas
básicas del estudio de la topología y constituirán la materia prima de los conceptos posteriores.

Un punto formula_100 se dirá que es un punto interior de formula_88 si existe un entorno formula_102 de formula_2 tal que formula_104. Así, el conjunto de los puntos interiores a formula_88 es un conjunto abierto, denominado Interior de A, denotado por Int (A) o también como formula_106. Es el mayor conjunto abierto incluido en A.

Un punto formula_107 se dirá que es un punto exterior a formula_88 si existe un entorno formula_102 de formula_110 tal que formula_102 ⊂ formula_112. Asimismo, el conjunto de los puntos exteriores a formula_88 es otro conjunto abierto, denominado Exterior de A y denotado por Ext (A).

Un punto formula_114 se dice que es un punto frontera de formula_88 si todo entorno formula_6 de formula_117 es tal que formula_118 y formula_119. Al conjunto de los punto frontera de formula_88 se le denomina Frontera de A y se denota por Fr(A). En otras palabras, todo entorno con centro en formula_117 tendrá elementos pertenecientes al conjunto formula_88 y otros elementos fuera del conjunto formula_88. La frontera de formula_88 es un conjunto cerrado.

Un punto formula_100 se dice que es un punto de adherencia de formula_88 si todo entorno formula_6 de formula_2 es tal que formula_129. Se hace pues evidente que todo punto interior y todo punto frontera es punto de adherencia. Al conjunto de los puntos de adherencia del conjunto formula_88 se le denomina adherencia o clausura de formula_88, y se denota por formula_132 o por formula_133. La
clausura de un conjunto formula_88 es un conjunto cerrado, y es el menor conjunto cerrado que contiene al conjunto.

Un punto formula_100 se dice que es un punto de acumulación de formula_88 si todo entorno formula_6 de formula_2 es tal que formula_139. Al conjunto de los puntos de acumulación de un conjunto se le denomina acumulación del conjunto, o conjunto derivado, y se le denota por formula_140 o por formula_141.

Un punto formula_100 se dice que es un punto de formula_143-acumulación de formula_88 si todo entorno formula_6 de formula_2 es tal que formula_147 es un conjunto infinito. Al conjunto de los puntos de
formula_148-acumulación de un conjunto se le denomina formula_148-acumulación del conjunto, o conjunto formula_148-derivado, y se le denota por formula_151 o por formula_152. Todo punto de formula_148-acumulación es punto de acumulación, y todo punto de acumulación es punto
de adherencia del mismo conjunto.

Un punto formula_100 se dice que es un punto aislado de formula_88 si existe algún entorno perforado formula_6 de formula_2 (es decir, un conjunto formula_158 de manera que formula_159 es un entorno de formula_2) de manera que formula_161. Al conjunto de los puntos aislados de formula_88 se le denomina conjunto de los puntos aislados de formula_88, y se le denota por formula_164. Todo punto aislado es punto frontera y también es punto de adherencia del mismo conjunto.

En topología son de una importancia capital los conjuntos interior y clausura de un conjunto. Su importancia radica en ser, respectivamente, el mayor abierto contenido en el conjunto y el menor cerrado que contiene al conjunto. El interior puede obtenerse también como la unión de todos los abiertos contenidos en el conjunto, y la clausura como la intersección de todos los cerrados que contienen al conjunto. Sin tanta importancia en topología pero de mucha en otras áreas de la Matemática son los conjuntos de acumulación, frontera y de los puntos aislados de un conjunto.

La idea de la convergencia es la de "aproximar" un objeto por otro, es decir, sustituir un objeto por otro que está próximo a él. Evidentemente, al hacerlo así se está cometiendo un error, error que en general dependerá de lo próximo que se encuentre el objeto sustituido del objeto sustituto. Para hacer esta sustitución de una manera sistemática, de forma que el error pueda ser elegido arbitrariamente pequeño, aparecen distintos tipos de conjuntos. Se obtiene así un proceso de sucesivas aproximaciones que, si todo va bien, terminarían llevándonos al objeto, aunque fuese después de un número infinito de aproximaciones. El más sencillo de estos conjuntos es una sucesión, es decir, una colección infinita (numerable) y ordenada de objetos, aunque con el mismo carácter de orden hay otros conjuntos que reflejan mejor el concepto de convergencia.

Es importante observar que la topología no trabaja con errores ni con aproximaciones. Eso entra en el ámbito del Análisis Numérico e incluso del Análisis Matemático. La topología lo que hace en este problema es aportar las herramientas básicas y los conceptos teóricos para afrontar correctamente el problema, siempre desde un punto de vista conceptual y cualitativo. Estudia qué es lo que debe entenderse cuando decimos que un conjunto (como puede ser una sucesión) se acerca a un objeto (que puede ser un punto, un conjunto, etcétera).

Una sucesión es una aplicación en un conjunto cuyo dominio es el conjunto de los números naturales. En particular, una sucesión en un espacio topológico formula_165 es una aplicación formula_166.

Una sucesión es el caso más sencillo de aplicación de dominio infinito.

Se dice que formula_100 es un punto límite de la sucesión formula_168, o bien que formula_168 converge al punto formula_170, si se cumple que, cualquiera que sea el entorno formula_171 de formula_170 existe un número natural formula_173 de tal manera que si formula_174 es otro número natural mayor o igual que formula_173 (o sea, formula_176) entonces se cumple que formula_177.

Hay que hacer dos observaciones sobre esto:

Un punto formula_100 es punto de aglomeración de la sucesión formula_168 si cualquiera que sea el entorno formula_6 de formula_2 se cumple que el conjunto formula_186 es infinito. Todo punto límite es punto de aglomeración, pero el recíproco no es cierto. Por ejemplo, los límites de oscilación de una sucesión no convergente de números reales (como por ejemplo la sucesión formula_187) son puntos de aglomeración, pero no son puntos límites (no existe límite para dicha sucesión, mientras que 1 y -1 son puntos de acumulación).

Otro concepto totalmente fundamental estudiado en esta rama es el de aplicación continua. Una aplicación formula_188 entre dos espacios topológicos se dice que es continua si dado cualquier conjunto formula_189 abierto en formula_190, el conjunto formula_191 es un conjunto abierto en formula_192.

Con la misma notación, si formula_100, diremos que formula_194 es continua en formula_195 cuando se obtiene que formula_196 es un entorno de formula_170, cualquiera que sea el entorno formula_198 de formula_199.

Es inmediato entonces comprobar que formula_194 es continua cuando y sólo cuando es continua en formula_100, cualquiera que sea éste, es decir, cuando y sólo cuando sea continua en cada uno de los puntos de su dominio.

Informalmente hablando, una aplicación es continua si transforma puntos que están cerca en puntos que están cerca, es decir, si respeta la "relación de cercanía". Esto además quiere decir que una función continua no "rompe" los que está unido y no "pega" lo que está separado.

Un conjunto se dice que es conexo si no puede expresarse como unión de dos abiertos disjuntos no vacíos.

Un conjunto formula_202 se dice que es conexo por caminos si todo par de puntos puede unirse mediante un camino, esto es, formula_203 continua de tal manera que formula_204 y formula_205. Todo conjunto conexo por caminos es conexo, pero no todo conjunto conexo es conexo por caminos (ver, por ejemplo, el seno del topólogo).

Estos conjuntos están "hechos de una pieza" (los conexos) o "hechos de manera que no tienen piezas totalmente sueltas" (los conexos por caminos). Naturalmente esto es sólo una manera de interpretarlos. Las piezas de un conjunto (los mayores subconjuntos conexos que contiene el conjunto) se denominan "componentes conexas". Por ejemplo, un puñado de arena sería un conjunto en el que las componentes conexas son cada granito de arena. Un espejo roto sería un conjunto en el que cada trozo de espejo es una componente conexa. Una bola de hierro es un conjunto con una sola componente conexa, es decir, un conjunto conexo. Una rejilla también es un conjunto conexo, formado por una sola componente conexa.

Existe otra noción de conexión, la conexión por arcos o arco conexión ligeramente más restrictiva que la conexión por caminos. Se exige que el camino sea un homeomorfismo sobre su imagen. Aun así, la conexión por arcos y por caminos coinciden sobre los espacios de Hausdorff.

Los conjuntos compactos son un tipo de conjunto mucho más difíciles de definir. Un espacio es compacto si para todo recubrimiento por abiertos (familia de abiertos cuya unión contiene al espacio total X) existe subrecubrimiento finito (familia finita de abiertos, formada sólo por conjuntos de la familia anterior, cuya unión contiene a X).

En un espacio métrico, un conjunto es compacto si cumple dos condiciones: es "cerrado", es decir contiene a todos sus puntos frontera; y es "acotado", es decir es posible trazar una bola que lo contenga. La compacidad es una propiedad muy importante en topología, así como en Geometría y en Análisis Matemático.

En cualquier espacio topológico, un conjunto cerrado dentro de un compacto, siempre es compacto. Además, en un espacio topológico de Hausdorff, un compacto siempre es cerrado.

Una topología sobre un conjunto es metrizable si es posible encontrar una distancia de forma que los abiertos para esa distancia sean exactamente los abiertos de la topología de partida. La metrizabilidad es también una propiedad muy deseable en un espacio topológico, pues nos permite dar una caracterización muy sencilla de los abiertos de la topología, además de implicar otras ciertas propiedades.

Las propiedades de separación son ciertas propiedades, cada una un grado más restrictiva que la anterior, que nos indican la "resolución" o "finura del grano" de una topología. Por ejemplo, la propiedad de separación T2 significa que para dos puntos distintos siempre pueden encontrarse entornos disjuntos (es decir que no se cortan).

Un conjunto es denso en el espacio si está "cerca de todos los puntos" de ese espacio. De manera más precisa, un conjunto es denso si su clausura topológica es todo el espacio. Equivalentemente, un conjunto es denso si su intersección con cualquier abierto no vacío del espacio es también no vacía. Un conjunto se dice que es separable si tiene algún subconjunto denso y numerable.

La topología producto nos proporciona una manera de dotar de una topología al producto cartesiano de varios espacios topológicos, de tal manera que se conserven buenas propiedades, en particular que las proyecciones sobre cada factor sean aplicaciones continuas y abiertas. La topología cociente nos proporciona una manera de dotar de una topología al cociente (espacio de clases) de un espacio por una relación de equivalencia, de manera que tenga el mayor número posible de conjuntos abiertos y sin embargo la proyección sea continua (es decir la imagen recíproca de cada abierto sea un abierto).

La topología algebraica estudia ciertas propiedades relacionadas con la conexión de un espacio, propiedades que podríamos describir como la "porosidad" de un espacio, la cantidad de boquetes que presenta. Para ello se vale de instrumentos algebraicos, fundamentalmente la teoría de grupos y el álgebra homológica, hasta tal punto que su desarrollo es totalmente algebraico.

En la topología algebraica se consideran una gran diversidad de problemas incluidos en la teoría de nudos por ejemplo, o en la teoría de homotopías y la teoría de homología.

Para comprender sucintamente estas cuestiones, volvamos a los ejemplos de conjuntos conexos. Según hemos dicho, una rejilla, una bola de hierro o una esponja son conjuntos conexos. Sin embargo todos entendemos que parece que no tienen el mismo «grados de conexión», por expresarlo de alguna manera. Mientras que una bola de hierro es maciza, una esponja y una rejilla tienen agujeros, e incluso parece claro que entre estos hay también una cierta diferencia. La homotopía y la homología tratan estas cuestiones.




</doc>
<doc id="2840" url="https://es.wikipedia.org/wiki?curid=2840" title="Testigos de Jehová">
Testigos de Jehová

Los testigos de Jehová son una denominación cristiana milenarista y restauracionista con creencias antitrinitarista distintas a las vertientes principales del cristianismo. Se consideran a sí mismos una restitución del cristianismo primitivo, creencia que se basa en su propio entendimiento de la Biblia, preferentemente de su "Traducción del Nuevo Mundo de las Santas Escrituras", y que tiene, según ellos, como propósito santificar el nombre de Jehová.

Su entidad jurídica, la Watch Tower Bible and Tract Society of Pennsylvania, fue fundada en 1881 por Charles Taze Russell, quien la presidió hasta su muerte, en 1916. Según sus publicaciones oficiales, en la actualidad es dirigida por un Cuerpo Gobernante desde su sede principal en Warwick (Nueva York). Este cuerpo gobernante se encarga de establecer la doctrina oficial de la congregación mundial.

Según sus propios datos, en 2016 sus publicaciones se distribuyeron en 240 países y territorios; contaban con 8.3 millones de publicadores activos y la asistencia anual a la «Conmemoración de la cena de Jesús» fue de 20.08 millones de personas.

Los Testigos de Jehová surgieron a partir de un grupo de cristianos restauracionistas, milenaristas y antitrinitarios pertenecientes al movimiento Estudiantes de la Biblia, el cual había sido organizado por el estadounidense Charles Taze Russell en los años 1870 en la localidad de Allegheny, Pensilvania. En julio de 1879 este movimiento publicó el primer número de la revista "Zion's Watch Tower and Herald of Christ's Presence", la que se continuó desarrollando y actualmente se ha constituido como la publicación más conocida de los Testigos de Jehová, bajo el nombre en castellano de "La Atalaya".

En 1881, Russell y un grupo de amigos del movimiento fundaron como entidad legal la Zion's Watch Tower Tract Society. Russell fue nombrado presidente en 1884, y la sociedad cambió más tarde su nombre por el de Watch Tower Bible and Tract Society of Pennsylvania, la que corresponde a la principal y más antigua entidad jurídica utilizada por los Testigos de Jehová. Desde entonces la sociedad comenzó a publicar y distribuir diversas publicaciones en distintos idiomas, relacionadas con sus creencias y su dios Jehová.

En 1909 la sede se trasladó hasta Brooklyn, Nueva York, donde permaneció hasta 2017, año en que mudaron su sede a Warwick (Nueva York). En 1914 se creó la primera entidad legal fuera de Estados Unidos, la sucursal International Bible Students Association en Gran Bretaña. Russell falleció dos años después. Para entonces, los estudiantes de la Biblia ya rechazaban doctrinas tales como la Santísima Trinidad, la inmortalidad del alma y el fuego del infierno, pero todavía celebraban la Navidad y aceptaban el símbolo de la cruz. Estas creencias, entre otras, se fueron rechazando progresivamente más adelante.

Tras el fallecimiento de Russell, la presidencia de la Watch Tower Bible and Tract Society of Pennsylvania fue asumida en 1917 por Joseph Franklin Rutherford. Durante su presidencia la organización aumentó considerablemente. Frente a las críticas de diversas denominaciones cristianas por sus principios doctrinarios, establecieron un cuerpo legal, mediante el que se obtuvieron fallos positivos en los tribunales de varios países que les dieron libertad de culto. También durante la presidencia de Rutherford se definió el nombre de «Testigos de Jehová», basados en el pasaje del canon bíblico Isaías 43:10, 11. El nombre se adoptó el 26 de julio de 1931, durante la asamblea de Columbus, Ohio, celebrada entre el 24 y el 30 de julio.

Entre 1942 y 1977, la presidencia de la Sociedad fue ejercida por Nathan Homer Knorr, quien ayudó a desarrollar los aspectos estructurales de la organización, fortaleció el Cuerpo Gobernante y creó diversas escuelas con el fin de estandarizar las tareas de evangelización, tales como la Escuela del Ministerio Teocrático (formación en oratoria) o la Escuela Bíblica de Galaad (formación misionera).

Entre 1977 y 1992 ejerció como presidente Frederick William Franz, quien era miembro del Cuerpo Gobernante y había sido vicepresidente desde 1945. Casi al término de su presidencia, en 1991, se levantaron las proscripciones de los Testigos de Jehová en Europa Oriental y África. Tras su muerte, Milton Henschel, antiguo asesor de Nathan Homer Knorr, asumió la presidencia hasta el año 2000, siendo reemplazado por Don Alden Adams, quien ejerce dicho cargo hasta hoy, sin ser miembro del Cuerpo Gobernante.

Los Testigos de Jehová basan sus creencias en la Biblia, libro que consideran como fuente exclusiva de referencia en asuntos doctrinales. Creen en Jehová como el único Dios, el cual no es omnipresente, y se identifican como seguidores de un único líder, Jesucristo, a quien consideran hijo de Dios pero no Dios en sí mismo, y a quien además identifican con el arcángel Miguel. Si bien aceptan a María como madre de Jesús y de sus hermanos, no la veneran ni la consideran madre de Dios. Creen en la Gran Apostasía y en el libre albedrío por sobre la predestinación. A diferencia de otras denominaciones cristianas, rechazan todas las doctrinas del Concilio de Nicea I y posteriores, entre ellas la Santísima Trinidad, el fuego del infierno y la inmortalidad inherente del alma. Realizan el bautismo por inmersión en agua, en el nombre del «Padre», del «Hijo» y del «Espíritu Santo» pero rechazan el bautismo de niños. No celebran la Navidad, la Pascua, los cumpleaños ni otras fiestas y costumbres que consideran incompatibles con el cristianismo por sus orígenes paganos. Tampoco consideran obligatorio el descanso semanal, pues argumentan que el feriado sabático de la ley mosaica estaba destinado exclusivamente a Israel. Son contrarios al ecumenismo y a las demás religiones y denominaciones cristianas las identifican con el apelativo de «Babilonia la Grande». En sus liturgias evitan el uso de imágenes y símbolos, no le ofrecen adoración a la cruz cristiana (creen que Cristo en realidad murió en un madero de tormento) ni creen en los dones milagrosos, los cuales consideran que terminaron tras la muerte de los doce apóstoles.

Los Testigos de Jehová creen que la parusía ya se produjo en 1914, de modo que desde entonces Cristo está presente de manera espiritual en la Tierra. Al mismo tiempo, creen que el armagedón está cerca, que el establecimiento del reino de Dios en la Tierra es la única manera de salvarse, y que solo 144 mil humanos, «los ungidos», irán al Cielo. Consideran que la sociedad secular actual está moralmente corrupta e influida por Satanás, por lo que sus miembros deben limitar su interacción social con las personas ajenas a su fe. Se suelen referir a su cuerpo de creencias como «la verdad» y consideran que ellos están «en la verdad». Los Testigos de Jehová, al igual que la mayoría de cristianos, consideran pecado e inmorales la masturbación, la fornicación, la homosexualidad, el aborto inducido y el espiritismo. Por faltas de conducta, los Testigos de Jehová pueden ser sometidos a diversas acciones disciplinarias, que pueden variar desde la «censura» hasta la expulsión.

Son conocidos por su predicación de casa en casa, donde distribuyen gratuitamente sus publicaciones, como "La Atalaya" y "¡Despertad!". También son conocidos por su oposición al servicio militar, su rechazo a los símbolos patrios y los nacionalismos. En general se declaran política y militarmente neutrales; si bien rechazan la violencia y el uso de armas, lo que en ocasiones provocó la persecución y matanza de sus miembros.

Los Testigos de Jehová creen en la creación divina y rechazan el naturalismo y la evolución biológica. Para ellos, la muerte y la vejez son una herencia del pecado original de Adán.

Piensan que Jesús no siguió la tradición judía de la no pronunciación del tetragramaton, sino que por el contrario, mandó santificarlo y darlo a conocer como el nombre de su padre (Mateo 6:9; Juan 12:28; 17:3,6,26).

Los Testigos de Jehová se reúnen semanalmente con sus respectivas congregaciones en los denominados salones del Reino. También se reúnen en asambleas anuales y en su celebración anual de la Conmemoración de la muerte de Jesús o Cena del Señor. Esta última es la única ceremonia que celebran. La realizan una vez al año en la fecha que corresponde al 14 de Nisán según el calendario lunar bíblico (marzo/abril), en la que recuerdan la muerte de Jesucristo y la analizan desde un punto de vista religioso.

Los Testigos de Jehová no creen en la transubstanciación, por lo que en estas ceremonias el pan y el vino tinto son solo elementos que representan simbólicamente el cuerpo y la sangre de Cristo.

En sus reuniones se interpretan canciones compuestas íntegramente por Testigos de Jehová. Entre sus cancioneros se encuentran "Cantando y acompañándose con música en su corazón" (1969), "Canten alabanzas a Jehová" (1986), "Cantemos a Jehová" (2009) y "Cantemos con Gozo a Jehová" (2017) todos ellos traducidos por la Watch Tower Society a numerosos idiomas.

Los Testigos de Jehová tienen prohibido hacerse transfusiones de sangre, incluso aunque de ello dependa sus vidas, lo que ha conducido a numerosas muertes evitables, incluyendo de niños. Desde 1961, la aceptación de transfusiones de sangre por parte de un miembro sin posterior arrepentimiento es causa de expulsión. Solo aceptan tratamientos alternativos.

Si bien históricamente rechazaban la vacunación y aceptación o donación de órganos, actualmente, es un asunto de decisión personal, mientras no incluya transfusiones de sangre.

Los Testigos de Jehová poseen dos servicios relacionados con este tema:

La congregación cristiana de los Testigos de Jehová es coordinada y dirigida a nivel mundial por un Cuerpo Gobernante, que además ejerce como la principal entidad legal de la corporación Watch Tower Bible and Tract Society of Pennsylvania, cuya sede central se encuentra actualmente en Warwick (Nueva York). Todos los miembros del Cuerpo Gobernante se consideran ungidos, y están por sobre el presidente de la asociación legal. Sus distintas sucursales son dirigidas a su vez por «comités de sucursal», los que están a cargo de un país o un grupo de países. Las sucursales se dividen a su vez en circuitos, compuestos por alrededor de veinte congregaciones que reciben regularmente visitas de los «superintendentes de circuito», para ayudarlas a organizar y ejecutar las predicaciones en sus territorios. Las congregaciones se reúnen en templos denominados «salones del Reino». Cada congregación tiene un «cuerpo de ancianos», a quienes se encomiendan diversas tareas de supervisión y pastoreo.

Su órgano legal en español es la Watch Tower Bible and Tract Society Incorporated (en castellano, «Atalaya, Biblias y Tratados Sociedad Anónima») que participa como casa editora y distribuidora.

Durante la presidencia de Nathan Homer Knorr (1942-1977), la junta directiva de la Sociedad pasó a formar parte del Cuerpo Gobernante, entidades que hasta entonces se consideraban equivalentes. El Cuerpo Gobernante se amplió a once miembros, mientras que la junta directiva se limitó a siete miembros. Desde esa fecha, el cargo de la presidencia en el Cuerpo Gobernante, a diferencia del de la presidencia de la Sociedad, es de rotación anual, por orden alfabético de los apellidos. El número de miembros del Cuerpo Gobernante se volvió a aumentar en 1974.

Hacia 2013, el Cuerpo Gobernante estaba compuesto por David H. Splane, Anthony Morris III, D. Mark Sanderson, Geoffrey W. Jackson, M. Stephen Lett, Samuel F. Herd y Gerrit Lösch y Guy H. Pierce. El actual presidente de la Watch Tower Bible and Tract Society of Pennsylvania, Don Alden Adams, no pertenece a los ungidos y por lo tanto tampoco forma parte del Cuerpo Gobernante. Tras la muerte de Guy H. Pierce, en 2018 se integró Kenneth Cook.

Cuando un testigo de Jehová comete lo que, de acuerdo con las creencias y normativas de la comunidad, es un pecado, éste es juzgado por un «comité judicial», el cual está conformado por tres o más «ancianos». El comité se reúne con el acusado para establecer la gravedad del pecado realizado. Si el «pecador» muestra arrepentimiento, se le aplica una «censura», es decir, una serie de sugerencias basadas en la Biblia que para ellos tienen como finalidad la reconciliación del acusado con Jehová. En caso de seguir estas sugerencias, la censura se hace pública a los demás miembros de la congregación, y se considera a la persona «censurada». Si, por el contrario, durante la etapa de censura el acusado no muestra arrepentimiento, entonces la persona es expulsada de la congregación y aislada de ésta. Una persona también puede desasociarse voluntariamente, en cuyo caso pasa a ser considerada como una persona expulsada de conocimiento público dentro de la congregación. En tales casos la persona pierde contacto con sus parientes Testigos que no viven bajo el mismo techo, y los miembros de la congregación no vuelven a saludarlo ni a tener contacto social con él.

El financiamiento de los Testigos de Jehová depende fundamentalmente de la corporación Watch Tower Bible and Tract Society of Pennsylvania, principalmente dedicada al negocio editorial, en el cual trabajan numerosos Testigos de Jehová de manera voluntaria, sin recibir remuneraciones. De acuerdo con el estudioso Wilbur Lingle, alrededor del 70% de sus ingresos provienen de sus millones de publicaciones que distribuyen anualmente a precios ligeramente superiores a los costos de impresión.

Rechazan el pago de diezmos o cuotas obligatorias o de membresía, tampoco pasan ofrenderos entre los congregados; pero si recogen donaciones en alcancías (recipientes) que ponen fijas en los Salónes del Reino y en coliseos y estadios cuando efectúan sus asambleas.

Desde sus inicios, tanto la sede central como las sucursales de los Testigos de Jehová han realizado una intensa actividad editorial evangelizadora, que incluye la publicación de numerosos textos al año, incluidas biblias, libros, folletos, tratados religiosos, vídeos y música, entre otros.

"La Atalaya", su revista más conocida, se comenzó a publicar en 1879, dos años antes de la fundación de la Watch Tower Bible and Tract Society of Pennsylvania. Mediante su nueva entidad legal, los Testigos de Jehová comenzaron a publicar su propia Biblia y otros tratados a partir de 1896, dejando de predicar con la "Biblia del rey Jacobo", que habían utilizado hasta entonces. En un comienzo utilizaron imprentas externas, y adquirieron en Estados Unidos los derechos de distintas versiones de la Biblia ya existentes:

En diciembre de 1926, "The Emphatic Diaglott" se convierte en la primera versión bíblica impresa directamente en las prensas de la Watch Tower Bible and Tract Society of Pennsylvania, ubicada en Brooklyn, Nueva York. Desde entonces comenzaron a imprimir diversas versiones de la Biblia independientes:
El hecho que los Testigos dispongan de su propia versión de la Biblia ha sido cuestionado, principalmente por diferencias con las traducciones tradicionales, o por basarse en un texto crítico. Los detractores y exmiembros de esta denominación cristiana, así como diversas agrupaciones religiosas, sostienen que el contenido de la "Traducción del Nuevo Mundo de las Santas Escrituras" ha sido alterado para apoyar las creencias particulares de los Testigos; a lo que los Testigos de Jehová responden que su traducción es precisa, exacta y literalmente ajustada a los manuscritos originales. Los Testigos de Jehová sostienen que en los pasajes criticados solo usan un estilo de traducción similar al de otras versiones reconocidas, constituyéndose en traducciones poco convencionales pero legítimas.

En la actualidad, las revistas "La Atalaya" y "¡Despertad!" son quincenales; la edición del día 1 de cada mes es pública, mientras que la del día 15 es para los miembros de la congregación. Ambas revistas se publican en papel, y desde 2012 también en formato electrónico.

Los Testigos de Jehová están presentes en un gran número de países, aunque no forman una proporción amplia de la población en ninguno de ellos. A diferencia de otras confesiones religiosas que contabilizan sus miembros por la asistencia anual a sus servicios o por sus miembros bautizados, los Testigos de Jehová cuentan como tales únicamente cuando son publicadores o predicadores activos.

El número de interesados o simpatizantes de su labor se muestra por la asistencia a su reunión anual, la Conmemoración de la muerte de Jesucristo. De acuerdo con sus propios datos, en 2016 asistieron 20 085 142 personas. De acuerdo con sus datos, las cifras actuales son las siguientes:

En marzo de 2014, el portal web de Testigos de los Jehová ocupó el primer puesto en sitios religiosos visitados en el mundo, según el contador Alexa.

Durante la Segunda Guerra Mundial, fueron el segundo colectivo religioso más perseguido por la Alemania nazi, después de los judíos. Se calcula que durante el holocausto murieron en total 1490 Testigos de Jehová, entre ellos 253 sentenciados a muerte. La primera cifra difiere a la del Círculo europeo de antiguos deportados e internados Testigos de Jehová, grupo de Testigos de Jehová que contabiliza unos 2500 muertos, además de aproximadamente diez mil encarcelados y deportados, y en un 97% el porcentaje de Testigos de Jehová alemanes perseguidos de una u otra forma por el nazismo. En los , los denominados "Bibelforscher" llevaban un triángulo púrpura cosida en la ropa como identificación. El 5 de octubre de 2006, el Museo del Holocausto de Washington D. C. ofreció un día dedicado a los Testigos de Jehová víctimas del nazismo.

Las creencias y prácticas de los Testigos de Jehová suelen ser criticadas por corrientes principales del cristianismo, miembros de la comunidad médica, exmiembros y otros tipos de organizaciones.

Distintos miembros de la confesión religiosa de los Testigos de Jehová han sido vinculados con casos de abuso sexual infantil. El comportamiento histórico de muchos de los líderes y ancianos de las congregaciones fue el «secreto», una especie de «código del silencio», que influyó en las víctimas para que no fueran a informar a las autoridades ni a la policía, y que prohibió además la discusión de estos asuntos dentro de la iglesia.
Solo en Australia se calculó en 2015 la cifra de más de mil menores agredidos. El fiscal local definió a los Testigos de Jehová «como una secta insular con reglas diseñadas para detener los informes sobre abusos sexuales».
Esta política le significó a la congregación una serie de demandas y pago de millonarias indemnizaciones.

En respuesta, la organización ha declarado que han desarrollado políticas de protección de menores para gestionar casos de abuso infantil cometidos por miembros de sus congregaciones. Detalles de estas políticas han sido emitidos en sus publicaciones y comunicados de prensa emitidos por su Oficina de Información Pública. Pese a lo anterior, la organización ha preferido pagar altas multas a cambio de retener información relacionada con los abusos.

Algunas acusaciones, desde el punto de vista social, son las siguientes:


Por otra parte, su objeción de conciencia al servicio militar y rechazo de los saludos a los símbolos patrios les ha generado conflictos con algunos gobiernos. En consecuencia, algunos Testigos de Jehová han sido perseguidos y sus actividades han sido prohibidas o restringidas en algunos países. Para el crítico religioso , los desafíos legales que han planteado ha influido en la legislación relacionada con los derechos civiles y políticos en algunos países.

Otro foco permanente de críticas es la construcción de la mansión Beth Sarim, construida supuestamente para el «inminente retorno de los patriarcas», la cual finalmente nunca tuvo el uso que se planteó en las publicaciones de la Sociedad Wachtower, sino que se empleó como residencia particular de Joseph Franklin Rutherford, el mismo que masificó la idea de la Segunda Venida a través de su libro de 1920, "Millones que ahora viven no morirán jamás", editado por la Sociedad Watchtower.

Algunas denominaciones cristianas critican el hecho que los Testigos de Jehová manejen su propia versión de la Biblia, "Traducción del Nuevo Mundo de las Santas Escrituras", pues presenta diferencias de traducción y omite algunos textos de las versiones más conocidas.

Otra crítica tiene que ver con sus profecías fallidas, las cuales han sido citadas explícita o implícitamente en sus publicaciones, en particular las relacionadas con los años 1914, 1918 1925 y 1975. En dichas ocasiones en que las profecías no se han cumplido, se han alterado doctrinas, o bien se ha justificado la falla mediante el término «revelaciones progresivas», que se interpretan como una conducción gradual de Dios a la comprensión más clara de la voluntad de sus seguidores.

Hasta 1914 los Testigos de Jehová esperaban la Segunda Venida. Como esta no se produjo, declararon que el término «parusía» en realidad significa «presencia» y no «llegada», que el Reino de Dios era espiritual y que el mundo estaba viviendo sus últimos días. Posteriormente, a través de publicaciones como "Millones que ahora viven no morirán jamás" y números de revistas, los años para el cumplimiento de esta profecía se fueron extendiendo cada vez en el tiempo, sin cumplirse en ninguno de los casos. La generación de 1914 pasó y murió completamente, por lo que desde 2010 en adelante al seguir sin cumplimiento se habla de la «generación traslapada» con lo que alargan en el tiempo la profecía.




</doc>
<doc id="2844" url="https://es.wikipedia.org/wiki?curid=2844" title="Tejido nervioso">
Tejido nervioso

Es un conjunto de células desarrollado a partir del ectodermo que es una capa celular más externa de un embrión animal ,este tejido nervioso percibe los estímulos internos y externos para transformarlos en impulsos nerviosos que es una onda oscilación eléctrica que recorre la membrana plasmática. Las modificaciones del tejido nervioso de la parte externa o interna de los estímulos sensoriales como la temperatura, la presión, la luz, los sonidos, el gusto, entre otros, porque son detectados, examinados y transmitidos por las células nerviosas. Además el tejido nervioso es el que se encarga de reunir las funciones motoras, las glándulas, los viscerales y de la mente humana.
El tejido nervioso comprende billones de neuronas y una incalculable cantidad de conexiones, que forma el sistema de comunicación neuronal. Las neuronas tienen receptores, elaborados en sus terminales, especializados para percibir diferentes tipos de estímulos ya sean mecánicos, químicos, térmicos, etc, y traducirlos en impulsos nerviosos que lo conducirán a los centros nerviosos. Estos impulsos se propagan sucesivamente a otras neuronas para procesamiento y transmisión a los centros más altos y percibir sensaciones o iniciar reacciones motoras.

Para llevar a cabo todas estas funciones, el sistema nervioso está organizado desde el punto de vista anatómico, en el sistema nervioso central (SNC) y el sistema nervioso periférico (SNP). El SNP se encuentra localizado fuera del SNC e incluye los 12 pares de nervios craneales (que nacen en el encéfalo), 31 pares de nervios raquídeos (que surgen de la médula espinal) y sus ganglios relacionados.

De manera complementaria, el componente motor se subdivide en:


En adición a las neuronas, el tejido nervioso contiene muchas otras células que se denominan en conjunto células gliales, que ni reciben ni transmiten impulso, su misión es apoyar a la célula principal: la neurona.

Las células del sistema nervioso se dividen en dos grandes categorías: neuronas y células gliales.
Se creía antes que estas eran las únicas células que no se reproducían, y cuando mueren no se podía reponer; sin embargo, hace poco se demostró que su capacidad regenerativa es extremadamente lenta, pero no nula. Se reconocen tres tipos de neuronas:


Uno de los propósitos de estas células era mantener a las neuronas unidas y en su lugar según Virchow. Ahora se sabe que es una de las varias funciones.
Las microglías son células pequeñas con núcleo alargado y con prolongaciones cortas e irregulares que tienen capacidad fagocitaria. Se originan en precursores de la médula ósea y alcanzan el sistema nervioso a través de la sangre; representan el sistema mononuclear fagocítico en el sistema nervioso central.

Contienen lisosomas y cuerpos residuales. Generalmente se la clasifica como célula de la neuroglia. Presentan el antígeno común leucocítico y el antígeno de histocompatibilidad clase II, propio de las células presentadoras de antígeno




</doc>
<doc id="2846" url="https://es.wikipedia.org/wiki?curid=2846" title="Río Tambopata">
Río Tambopata

El río Tambopata es un río que pertenece a la cuenca del Amazonas, un afluente del río Madre de Dios. Discurre casi totalmente por el Perú, por la región Madre de Dios y la región Puno, pero es un río internacional, ya que un corto tramo forma frontera con Bolivia (departamento de La Paz). Tiene una longitud de 402 km.

El río Tambopata nace por encima de los 3.900 m, en los cerros nevados que dominan el altiplano peruano-boliviano aproximadamente en las coordenadas () en el departamento de La Paz. Recorre 66 kilómetros por territorio boliviano hasta las coordenadas () donde pasa a formar frontera con el Perú en un tramo de 58 kilómetros hasta la afluencia del río Colorado donde se adentra en territorio peruano. En su descenso inicial hacia la llanura de la Amazonía forma rápidos y cascadas al discurrir por profundos valles y cañones. 

Por debajo de los 3.000 m atraviesa un ecosistema selvático casi permanentemente cubierto por nubes, conocido como el bosque de neblina o yungas. Al llegar a la llanura amazónica, por debajo de los 500 m, la selva de altura se convierte en la selva amazónica baja y el río Tambopata en un río tranquilo, ancho y sinuoso. Pasa por la ciudad de San Rafael y desemboca en el Madre de Dios en Puerto Maldonado (capital de la Región Madre de Dios, con unos 40.000 habitantes), frente al río de Las Piedras.

En su recorrido, este río atraviesa la Reserva Nacional Tambopata. Sus principales afluentes son el río Mosojhuaico y el río Carama.


</doc>
<doc id="2847" url="https://es.wikipedia.org/wiki?curid=2847" title="Taifa">
Taifa

Las taifas ( "ṭā'ifa", plural طوائف "ṭawā'if", palabra que significa "bando" o "facción") fueron pequeños reinos (ملوك الطوائف) en que se dividió el califato de Córdoba después del derrocamiento del califa Hisham III (de la dinastía omeya) y la abolición del califato en 1031. Posteriormente, tras el debilitamiento de los almorávides y los almohades, surgieron los llamados segundos (1144 y 1170) y terceros reinos de taifas (siglo XIII). El origen de todas las dinastías de las taifas era extranjero, salvo el de los Banu Qasi y los Banu Harún, que era muladí.

Desde que el califa Hisham II es obligado a abdicar en 1009 hasta el año de la abolición formal del califato en 1031 se suceden en el trono de Córdoba nueve califas, de las dinastías omeya y hamudí, en medio de un caos total que se refleja en la independencia paulatina de las taifas de Almería, Murcia, Alpuente, Arcos, Badajoz, Carmona, Denia, Granada, Huelva, Morón, Silves, Toledo, Tortosa, Valencia y Zaragoza. Cuando el último califa Hisham III es depuesto y proclamada en Córdoba la república, todas las "coras" (provincias) de al-Ándalus que aún no se habían independizado se autoproclaman independientes, regidas por clanes árabes, bereberes o eslavos.

En el trasfondo se hallaban problemas muy profundos. Por una parte, las luchas por el trono califal no hacían sino reproducir las luchas internas que siempre habían asolado el emirato y el califato por causas raciales (árabes, bereberes y muladíes o eslavos, estos últimos esclavos libertos del norte peninsular o de origen centroeuropeo). También influían la mayor o menor presencia de población mozárabe, el ansia independentista de las áreas con mayores recursos económicos y también la agobiante presión fiscal necesaria para financiar el coste de los esfuerzos bélicos.

Cada taifa se identificó al principio con una familia, clan o dinastía. Así surgen la taifa de los amiríes (descendientes de Almanzor) en Valencia; la de los tuyibíes en Zaragoza; la de los aftasíes en Badajoz; la de los birzalíes en Carmona; la de los ziríes en Granada; la de los hamudíes en Algeciras, Ceuta y Málaga; y la de los abadíes en Sevilla. Con el paso de los años, las taifas de Sevilla (que había conquistado toda la Andalucía occidental y parte de la oriental), Badajoz, Toledo y Zaragoza, constituían las potencias islámicas peninsulares.

Durante el apogeo de los reinos de taifas (siglo XI y después a mediados del siglo XII), los reyes de las taifas compitieron entre sí no sólo militarmente, sino sobre todo en prestigio. Para ello, trataron de patrocinar a los más prestigiosos poetas y artesanos.

Sin embargo, la disgregación del califato en múltiples taifas, las cuales podían subdividirse o concentrarse con el paso del tiempo, hizo evidente que sólo un poder político centralizado y unificado podía resistir el avance de los reinos cristianos del norte. Careciendo de las tropas necesarias, las taifas contrataban mercenarios para luchar contra sus vecinos o para oponerse a los reinos cristianos del norte. Incluso guerreros cristianos, como el propio Cid Campeador, sirvieron a reyes musulmanes, luchando incluso contra otros reyes cristianos. Sin embargo, esto no fue suficiente y los reinos cristianos aprovecharían la división musulmana y la debilidad de cada taifa individual para someterlas. Al principio el sometimiento era únicamente económico, forzando a las taifas a pagar un tributo anual, las "parias", a los monarcas cristianos. Sin embargo, la conquista de Toledo en 1085 por parte de Alfonso VI de León y Castilla hizo palpable que la amenaza cristiana podía acabar con los reinos musulmanes de la península. Ante tal amenaza, los reyes de las taifas pidieron ayuda al sultán almorávide del norte de África, Yúsuf ibn Tasufin, el cual pasó el estrecho asentándose en Algeciras y no sólo derrotó al rey leonés en la batalla de Zalaca (1086), sino que conquistó progresivamente todas las taifas.


Cuando el dominio almorávide empezó a decaer, surgieron los llamados segundos reinos de taifas (1144-1170), que fueron posteriormente sometidos por los almohades, que habían sucedido a los almorávides en su dominio del norte de África.


Tras el fin del periodo almohade, marcado por la batalla de las Navas de Tolosa (1212), hubo un corto periodo denominado terceros reinos de Taifas, que terminó en la primera mitad del siglo XIII con las conquistas cristianas en el Levante de Jaime I de Aragón (Valencia, 1236) y en Castilla de Fernando III el Santo (Córdoba, 1236 y Sevilla, 1248) y perduró en Granada con la fundación del reino nazarí, que no capituló hasta el 2 de enero de 1492, fecha que pone fin a la Reconquista.







</doc>
<doc id="2850" url="https://es.wikipedia.org/wiki?curid=2850" title="Teoría del Derecho">
Teoría del Derecho

La teoría del derecho o teoría general del derecho es la ciencia jurídica que estudia los elementos del derecho u ordenamiento jurídico existente en toda organización social y los fundamentos científicos y filosóficos que lo han permitido evolucionar hasta nuestros días.

La teoría del derecho tiene como objetivo fundamental el análisis y la determinación de los elementos básicos que conforman el derecho, entendido este como ordenamiento jurídico unitario, esto es, un conjunto de normas que conforman un solo derecho u ordenamiento jurídico en una sociedad o sociedades determinadas.

Solo a través de la comprensión del ordenamiento jurídico en su totalidad se pueden individualizar las características del fenómeno jurídico de las que habitualmente nos servimos para diferenciar al derecho de otros ordenamientos como son el moral y el de los usos sociales.

El estudio de los fundamentos del derecho se vale de disciplinas filosófico-jurídicas específicas, a saber:


La sociología del derecho, también llamada sociología jurídica, es aquella disciplina que estudia los problemas, las implicaciones, y todo aquello concerniente a las relaciones entre el derecho y la sociedad. A diferencia de la teoría del derecho y de la filosofía política, el principal problema u objeto de estudio de la sociología jurídica es el de la eficacia del derecho.

La teoría jurídica crítica se refiere a un movimiento en el pensamiento jurídico que aplica métodos propios de la teoría crítica (la Escuela de Frankfurt) al derecho. En términos generales, este pensamiento postula nociones tales como: El derecho es simplemente política. El lenguaje jurídico es un falso discurso que ayuda a perpetuar las jerarquías: hombres sobre mujeres, ricos sobre pobres, mayorías sobre minorías.

Pero, como se pasa en Brasil, ahora, hay perspectivas de Teoria Crítica muy diversas entre si, y que no están más tán conectadas a categorías filosóficas del siglo XX, sino del siglo XXI. Es un ejemplo, el trabajo de Teoria Crítica en la Teoria del Derecho, que se puede encontrar en la Teoria del Humanismo Realista, uma concepción muy latino-americana que se vuelve a la democracia, la justicia y los derechos humanos.

El derecho es un lenguaje que nos sirve para conocer la realidad jurídicamente considerada, misma que es una parte de la realidad universal física. Todo objeto es real si puede medirse en dimensiones matemáticas: volumen, peso, densidad, etc. Por tanto, el derecho, al hablar de la realidad social, se vuelve un metalenguaje, que a su vez es lenguaje objeto de la ciencia del derecho.
Cualquier orden jurídico es, por ende, un esquema de interpretación de la realidad que dice "qué es derecho; y es prescriptivo porque señala "qué debe hacer el hombre".

Por otra parte, la ciencia jurídica, a diferencia del derecho, es descriptiva y nos dice "cómo es" y "como funciona" el sistema normativo coactivo, su único objeto de estudio.
La teoría del derecho no debe ocuparse de nociones fuera de su objeto de estudio, tales como los valores o las causas sociales que motivan la creación de normas jurídicas. Dichas nociones son el ámbito de investigación de la ética y la sociología.
Es pertinente aclarar que basta con conocer las bases del lenguaje del derecho, su paradigma y reglas de creación y aplicación, para describirlo y proveer a su eficacia.



</doc>
<doc id="2851" url="https://es.wikipedia.org/wiki?curid=2851" title="Territorios del Noroeste">
Territorios del Noroeste

Territorios del Noroeste (en francés: "les Territoires du Nord-Ouest"; en inglés: "Northwest Territories") es uno de los tres territorios que, junto con las diez provincias, conforman las trece entidades federales de Canadá. Su capital y ciudad más poblada es Yellowknife. Está ubicado, como su nombre indica, al noroeste del pais, limitando al norte con el Océano Ártico, al este con Nunavut, al sureste con Manitoba, al sur con Saskatchewan, Alberta y Columbia Británica, y al oeste con Yukón. Con 42 514 habs. en 2008 es la tercera entidad menos poblada —por delante de Yukón y Nunavut, la menos poblada—, con 1 346 106 km², la tercera más extensa —por detrás de Nunavut y Quebec— y con 0,03 hab/km², la segunda menos densamente poblada, por detrás de Nunavut.

Algunos de sus rasgos geográficos incluyen el vasto Gran Lago del Oso y el Gran Lago del Esclavo, así como el inmenso río Mackenzie y los cañones del Parque Nacional Nahanni, un parque calificado como Área Natural Protegida de Canadá y declarado Patrimonio de la Humanidad por la Unesco. Las islas del territorio en el archipiélago ártico canadiense incluyen: isla de Banks, isla Borden, isla Prince Patrick, y partes de isla Victoria e isla Melville. El punto más alto es el monte Nirvana, cerca de la frontera con Yukón, con una elevación de 2773 m.

El territorio actual fue creado en junio de 1870, cuando la Compañía de la Bahía de Hudson transfirió la Tierra de Rupert y el Territorio Noroeste al gobierno de Canadá. Esta inmensa región comprendía toda Canadá no confederada excepto la Columbia Británica, la costa de los Grandes Lagos, el valle del río San Lorenzo y el tercio sur de Quebec, las Provincias marítimas de Canadá, Terranova y la costa de Labrador. No incluía el territorio británico de las Islas Árticas excepto la mitad del sur de la isla Baffin, que permanecieron bajo dominio británico directo hasta 1880.

Después de la transferencia, la extensión de los territorios fue disminuyendo gradualmente. La provincia de Manitoba se creó el 15 de julio de 1870, un cuadrado diminuto alrededor de Winnipeg, que posteriormente se amplió en 1881 a una región rectangular que forma el sur de la actual provincia. Cuando la Columbia Británica se unió a la Confederación Canadiense el 20 de julio de 1871, ya le había sido concedida (1866) la parte del "Territorio Noroeste" al sur de 60 grados norte y oeste de 120 grados oeste, un área que comprendía la mayor parte del "Territorio Stikine". En 1882, Regina, en el por entonces distrito de Assiniboia, se convirtió en la capital territorial; después de que Alberta y Saskatchewan se convirtieran en provincias en 1905, Regina se convirtió en la capital de la nueva provincia de Saskatchewan.

En 1876, el "Distrito de Keewatin", en el centro del territorio, se separó de éste. En 1882 y de nuevo en 1896, la parte restante fue dividida en los siguientes distritos (entre paréntesis se indica su correspondencia con las áreas actuales):


El Distrito de Keewatin (la actual Manitoba, noroeste de Ontario y sur de Nunavut) retornó a los Territorios del Noroeste en 1905.

Mientras tanto, Ontario fue ampliado hacia el noroeste en 1882. Quebec también fue ampliado, en 1898 y Yukón se convirtió en un territorio separado en el mismo año para encargarse de la "fiebre del oro" de Klondike, y liberar al gobierno de los Territorios del Noroeste de administrar el súbito incremento demográfico, de actividad económica y de la afluencia de no canadienses.

Las provincias de Alberta y Saskatchewan fueron creadas en 1905, y Manitoba, Ontario, y Quebec adquirieron de los Territorios del Noroeste la última parte sus actuales territorios en 1912, quedándose sólo con los distritos de Mackenzie, Franklin (que absorbió los remanentes de Ungava en 1920) y Keewatin. En 1925, las fronteras de los Territorios del Noroeste fueron ampliadas a todo lo largo del Polo Norte en el principio del sector, ampliando enormemente su territorio en el norte de los campos de hielo.

Los reducidos Territorios del Noroeste no fueron representados en la Cámara de los Comunes canadiense de 1907 a 1947 cuando el distrito electoral del río Yukón-Mackenzie fue creado. Esta dependencia sólo incluyó el Distrito de Mackenzie. El resto de los Territorios del Noroeste no tenía ninguna representación en la Cámara de los Comunes hasta 1962, cuando el distrito electoral de los Territorios del Noroeste fue creado en reconocimiento a los esquimales, que habían obtenido el derecho al voto en 1953.

En 1912 el Gobierno de Canadá renombró el territorio a Territorios del Noroeste, eliminando la forma escrita con guion ortográfico ("North-Western Territory"). Entre 1925 y 1999, los Territorios del Noroeste abarcaban 3.439.296 kilómetros cuadrados - mayor extensión que la India.
Finalmente, el 1 de abril de 1999, tres quintas partes del este de los Territorios del Noroeste (incluido todo el Distrito Keewatin y la mayor parte de los de Mackenzie y Franklin) se convirtieron en un territorio separado llamado Nunavut.

Hubo cierta discusión sobre cambiar el nombre de los Territorios del Noroeste después de la separación de Nunavut, posiblemente a un término de una lengua Aborigen. Una propuesta era "Denendeh" ("nuestra tierra" en el idioma de los Dene). La idea fue defendida por el antiguo primer ministro Stephen Kakfwi, entre otros. También, una popular emisora de radio comenzó a promover el cambio del nombre del territorio a ""Bob"". Finalmente, una encuesta llevada a cabo antes de la división mostró un amplio apoyo a conservar el nombre "Territorios del Noroeste". 

En idioma inuktitut, los Territorios del Noroeste se denominan ᓄᓇᑦᓯᐊᖅ ("Nunatsiaq"), "tierra hermosa".

Como territorio, los Territorios del Noroeste tienen algunos menos derechos que las provincias. Durante su mandato, el primer ministro Kakfwi presionó para obtener un acuerdo del gobierno federal con más derechos para el territorio, incluyendo que los ingresos por los recursos naturales del territorio reviertan al territorio. La devolución de poderes al territorio fue una cuestión en las 20ª elecciones generales del 2003, y lo ha sido desde que el territorio comenzó a elegir miembros en 1881.
El comisionado es el jefe del ejecutivo y es designado por el Gobernador en consejo de Canadá a recomendación del Ministro federal de Asuntos Indios y Desarrollo del Norte. La posición solía ser más administrativa y gubernamental pero con la delegación de cada vez más poderes a la Asamblea electa desde 1967, su posición se ha vuelto simbólica. Desde 1985 el comisionado ya no preside las reuniones del Consejo Ejecutivo (o gabinete) y el gobierno federal ha dado instrucciones a los comisarios de comportarse como un teniente gobernador provincial. A diferencia de los tenientes gobernadores, el comisionado de los Territorios del Noroeste no es un representante formal de la Reina de Canadá.

A diferencia de los gobiernos provinciales y de Yukón, el gobierno de los Territorios del Noroeste no tiene partidos políticos, excepto durante el período entre 1898 y 1905. Es un gobierno de consenso llamado Asamblea Legislativa. Este grupo está formado por un miembro decidido en cada uno de los diecinueve distritos electorales. Después de las elecciones generales, el nuevo parlamento decide a un primer ministro y portavoz por votación secreta. Siete MLA ("Member of Legislative Assembly", Miembro de la Asamblea Legislativa) son elegidos también como ministros, y el resto forma la oposición. Las elecciones generales más recientes del territorio fueron el 1 de octubre de 2007. El jefe de estado para los territorios es un comisionado designado por el gobierno federal. El comisionado tenía poderes gubernamentales plenos hasta 1980, momento en que los territorios obtuvieron un mayor autogobierno. La legislatura entonces comenzó a decidir un gabinete y un "Líder del Gobierno" posteriormente conocido como primer ministro ("Premier").

El actual primer ministro de los Territorios del Noroeste es Bob McLeod. El miembro del Parlamento por el distrito del "Ártico Oeste" ("Western Arctic"), el distrito que comprende los Territorios del Noroeste, es Dennis Bevington (Nuevo Partido Democrático). El comisionado de los Territorios del Noroeste es Tony Whitford.

El territorio disfruta de unos vastos recursos geológicos, que incluyen diamantes, oro, y gas natural. En particular, los diamantes de los Territorios del Noroeste son ofrecidos como una alternativa ética que alivie los riesgos de apoyar conflictos para adquirir los llamados "diamantes de guerra". Sin embargo, su explotación también ha provocado inquietudes ambientales.

La gran cantidad de recursos naturales y la escasa población dan a los Territorios del Noroeste el más alto PIB per cápita de todas las provincias y territorios de Canadá. De hecho, su PIB per cápita de 97.923 dólares lo clasificaría como primero en el mundo si fuera considerado como un país, por delante de Luxemburgo (con PBI nominal de aproximadamente 83.000C$).

Las explotaciones más destacadas son:


De acuerdo con el censo canadiense de 2001 los 10 grupos étnicos principales en los Territorios del Noroeste eran:

Población de los Territorios del Noroeste desde 1871

Notas:
(*): "El Territorio de Yukón fue cedido de los Territorios Noroestes en 1898."

(**): "Alberta y Saskatchewan fueron creados de partes de los Territorios del Noroeste en 1905."

(***): "Los datos hasta 1996 incluyen Nunavut. Los datos de 2001 no incluyen Nunavut."

(****): "Datos del censo de 2006."

"Fuente: Statistics Canada"

El gobierno elegido convirtió el francés en un idioma oficial en la Sección 11 del "Acta de los Territorios del Noroeste de 1877" que recibió el "Asentimiento Real" el 28 de abril de 1877. Antes de esto, el francés era un idioma oficial mientras los Territorios del Noroeste fueron administrados conforme al "Acta de Manitoba" entre 1870 y 1875. La cuestión se volvió candente por el teniente gobernador Joseph Royal al leer el Discurso del Trono en francés el 31 de octubre de 1888. La clamorosa protesta hizo que Royal leyera su segundo discurso del trono solo en inglés. El 28 de octubre de 1889, la cuestión quedó estancada cuando se tomó la "Resolución del Idioma", una moción que declaró que la asamblea no necesitaba el reconocimiento oficial de idiomas. La votación fue de 17 contra 2. Pero esto no duró, porque el gobierno federal se implicó, y advirtió al teniente gobernador Royal que comenzara a dar discursos en francés otra vez, y trató de legislar el bilingüismo oficial en el territorio de nuevo, a través de la Cámara de los Comunes canadiense. Sin embargo el proyecto de ley fue derrotado en la segunda lectura. La interferencia del gobierno de Canadá causó que miembros elegidos para la asamblea favorecieran al inglés como el único idioma oficial. El 19 de enero de 1892 el primer ministro territorial Frederick Haultain promovió una moción para que sólo el inglés fuera usado en la Asamblea. El movimiento pasó la división: 20 a favor, 4 en contra.

A principios de los años 1980, el gobierno de los Territorios del Noroeste estaba otra vez bajo la presión del gobierno federal para presentar de nuevo el francés como un idioma oficial. Algunos miembros nativos abandonaron la Asamblea, protestando por que no les permitirían utilizar su propio idioma. El consejo ejecutivo designó un comité especial de miembros de la Asamblea Legislativa para estudiar el tema. Estos decidieron que si el francés debía ser un idioma oficial, entonces también lo deberían ser los otros idiomas de los territorios.

La "Ley de Idiomas Oficiales de los Territorios del Noroeste" reconoce los siguientes once idiomas oficiales, que son más que cualquier otra división política en América:


Los residentes de los Territorios del Noroeste tienen el derecho de usar cualquiera de los susodichos idiomas en un tribunal territorial y en debates y procedimientos de la legislatura. Sin embargo, las leyes implican obligatoriedad jurídica sólo en sus versiones francesa e inglesa, y el gobierno sólo publica leyes y otros documentos en los otros idiomas oficiales del territorio cuando la legislatura lo demanda. Además, el acceso a servicios en cualquier idioma está limitado con instituciones y circunstancias donde hay demanda significativa de ese idioma o donde es razonable esperarlo dada la naturaleza de los servicios solicitados. En la práctica esto significa que los servicios en idioma inglés están universalmente disponibles, y no está garantizado que otros idiomas, incluido el francés, serán usados por cualquier servicio en particular del gobierno, excepto en los tribunales.

Los resultados del censo de 2006 mostraron una población de 41.464 habitantes.
De las 40.680 respuestas singulares del censo concernientes a la lengua materna, los idiomas con más respuestas fueron:
Había también 320 respuestas tanto del inglés como 'de un idioma no oficial'; 15 tanto de francés como de 'un idioma no oficial'; 45 tanto de inglés como de francés, y aproximadamente 400 personas no respondieron a la pregunta, o respondieron múltiples idiomas no oficiales o dieron una respuesta no enumerada. Los idiomas oficiales de los Territorios del Noroeste son mostrados en negrita (los datos mostrados son para el número de respuestas de idioma único y el porcentaje de respuestas de idioma único totales).

La Iglesia con mayor número de fieles es la Iglesia Católica con un 46,7%, seguida de la Iglesia Anglicana con un 14.9% y de la Iglesia Unida de Canadá con un 6%. Los territorios del Noroeste junto a Quebec, Nuevo Brunswick y la Isla del Príncipe Eduardo son las únicas entidades territoriales de Canadá en que la Iglesia Católica es el culto mayoritario.




</doc>
<doc id="2852" url="https://es.wikipedia.org/wiki?curid=2852" title="Terranova y Labrador">
Terranova y Labrador

Terranova y Labrador (en inglés: "Newfoundland and Labrador", ) es una de las diez provincias que, junto con los tres territorios, conforman las trece entidades federales de Canadá. Su capital es San Juan de Terranova. Ubicada al noreste del país, está formada por dos áreas distintas: Labrador —situado en la península homónima, limitando al norte con el mar homónimo y al oeste y sur con Quebec— y Terranova —una isla situada en el extremo este, que limita al norte y este con el océano Atlántico, y al oeste con el golfo de San Lorenzo. Con 405 512 km² es la cuarta entidad menos extensa —por delante de Nuevo Brunswick, Nueva Escocia e Isla del Príncipe Eduardo, la menos extensa— y con 1,2 hab/km², la cuarta menos densamente poblada, por delante de Yukón, Territorios del Noroeste y Nunavut, la menos densamente poblada.

Cuando el entonces Dominio de Terranova se unió a la confederación en 1949, la provincia se conoció como Terranova, pero desde 1964, el gobierno de la provincia se ha referido a sí mismo como el "Gobierno de Terranova y Labrador", y el 6 de diciembre de 2001, se aprobó una enmienda a la Constitución de Canadá para cambiar el nombre de la provincia a "Terranova y Labrador."
La población de la provincia se estima (en abril de 2008) en 508.270 habitantes.Terranova tiene sus propios dialectos de inglés, francés, irlandés y otras lenguas. El dialecto inglés de Labrador comparte mucho con el de Terranova. Por otra parte, Labrador tiene sus propios dialectos de innu-aimun e inuktitut.

Según el censo canadiense de 2001, el grupo étnico más grande de Terranova y Labrador son los ingleses (39.4%), seguido de los irlandeses (19.7%), escoceses (6.0%), franceses (5.5%), los españoles (2.09%) y las Naciones Originarias de Canadá (3.2%).

Población desde 1951

"Fuente: Statistics Canada"

Según el censo de 2006 contaba con una población de 505.469 habitantes. Para las 499.830 personas que respondieron a la pregunta concerniente a 'la lengua materna', las lenguas más comunes eran las siguientes:
Las cifras mostradas arriba son el número de respuestas que marcaron una única lengua y con dichos datos se ha hallado el porcentaje de las respuestas. Hubo también 435 respuestas que marcaron inglés y otra lengua no oficial, 30 que marcaron francés y otra lengua no oficial, 295 que marcaron inglés y francés, 10 que marcaron inglés, francés y otra lengua no oficial y 14.305 personas que no respondieron a la pregunta o que marcaron varias lenguas no oficiales.

Durante muchos años, Terranova y Labrador tuvo una economía deprimida. Tras el colapso de la pesquería del bacalao la provincia registró altas tasas de desempleo y la población disminuyó en alrededor de 60.000. Sin embargo, la creciente industrias minera y los recientes descubrimientos de petróleo en alta mar han empujado la economía provincial.

El PIB alcanzó los 28,1 mil millones de dólares canadienses, en comparación con 25,0 mil millones en 2009. El PIB per cápita en 2008 fue de 61.763 dólares canadienses, muy superior a la media nacional. Los servicios aportan más del 60% del PIB, especialmente los servicios financieros, cuidado de la salud y la administración pública. Otras importantes industrias son la minería, la producción de petróleo y la manufactura.
La explotación minera se centra en la obtención de hierro, níquel, cobre, zinc, plata y oro. La producción de petróleo fue de 110 millones de barriles, lo que elevó el PIB un 15%. Por otra parte, la industria pesquera sigue siendo una parte importante de la economía provincial ya que emplea aproximadamente 26.000 personas y contribuye con más de $ 440 millones de dólares canadienses al PIB. La cosecha combinada de peces como el bacalao, eglefino, halibut, el arenque y la caballa fue de 165.000 toneladas en 2010, valoradas en aproximadamente en $ 130 millones de dólares canadienses. Los mariscos, como cangrejos, camarones y almejas, representaron 215.000 toneladas, con un valor de $ 316 millones en el mismo año. El valor de los productos de la caza de focas fue de $ 55 millones.

La industria forestal es importante, ya que produce 462.000 toneladas de madera por año. El valor de las exportaciones de papel prensa es muy variable de año en año, dependiendo del precio del mercado mundial. La madera es producida por numerosos molinos en Terranova.

La acuicultura es una industria nueva para la provincia, que en 2006 produjo más de 10.000 toneladas de salmón del Atlántico, los mejillones y trucha arco iris de más de $ 50 millones. En Terranova se limita a las áreas al sur de San Juan, cerca de Deer Lake y en el Valle de Codroy. Se cultivan principalmente patatas, nabos, zanahorias y el repollo se cultiva para el consumo local. Arándanos silvestres y moras son cultivadas con fines comerciales y se utiliza en mermeladas y vino. La ganadería se reduce a la avícultura y a la producción lechera. Aparte del procesamiento de mariscos, la fabricación de papel y la refinación de petróleo, la industria se complementa con pequeñas industrias productoras de alimentos, la producción de cerveza y otras bebidas, y el calzado. El turismo también está en crecimiento.



</doc>
<doc id="2864" url="https://es.wikipedia.org/wiki?curid=2864" title="Toledo (desambiguación)">
Toledo (desambiguación)

Toledo puede referirse a:







</doc>
<doc id="2867" url="https://es.wikipedia.org/wiki?curid=2867" title="Tesla (unidad)">
Tesla (unidad)

</math>

El tesla (símbolo T), es la unidad de inducción magnética (o densidad de flujo magnético) del Sistema Internacional de Unidades (SI). Se define como una inducción magnética uniforme que, repartida normalmente sobre una superficie de un metro cuadrado, produce a través de esta superficie un flujo magnético total de un weber. Fue nombrada así en 1960 en honor al ingeniero e inventor Nikola Tesla.

Un tesla también se define como la inducción de un campo magnético que ejerce una fuerza de 1 N (newton) sobre una carga de 1 C (culombio) que se mueve a velocidad de 1 m/s dentro del campo y perpendicularmente a las líneas de inducción magnética.

Lo que es: 1 T = 1 N·s·m·C

La unidad equivalente en el Sistema Cegesimal de Unidades

A continuación una tabla de los múltiplos y submúltiplos del Sistema Internacional de Unidades.



</doc>
<doc id="2870" url="https://es.wikipedia.org/wiki?curid=2870" title="Tejido cartilaginoso">
Tejido cartilaginoso

El tejido cartilaginoso, o cartílago, es un tipo de tejido conectivo especializado, elástico, carente de vasos sanguíneos, formados principalmente por matriz extracelular y por células dispersas denominadas condrocitos. La parte exterior del cartílago, llamada pericondrio, es la encargada de brindar el soporte vital a los condrocitos.

El cartílago se encuentra revistiendo articulaciones, en las uniones entre las costillas y el esternón, como refuerzo en la tráquea y bronquios, en el oído externo y en el tabique nasal. También se encuentra en embriones de vertebrados y peces cartilaginosos.

Los cartílagos sirven para acomodar las superficies de los cóndilos femorales a las cavidades glenoideas de la tibia, para amortiguar los golpes al caminar y los saltos, para prevenir el desgaste por rozamiento y, por lo tanto, para permitir los movimientos de la articulación. Es una estructura de soporte y da cierta movilidad a las articulaciones.

Existen 3 tipos de tejido cartilaginoso:




Las células propias de este tejido se llaman condrocitos, los cuales provienen de los condroblastos presentes en el pericondrio. Poseen un rugoso bien desarrollado y un aparato de Golgi grande, así como muchas vesículas, las cuales son indicios de su actividad secretora. Los filamentos intermedios, compuestos por vimentina aparecen en abundancia. A menudo las células contienen glucógeno y no con poca frecuencia también inclusiones lipídicas, en parte grandes.

El cartílago hialino se encuentra cubierto externamente por una membrana fibrosa llamada pericondrio, excepto en los extremos articulares de los huesos y también donde se encuentra directamente debajo de la piel, es decir, las orejas y la nariz. Esta membrana contiene vasos que le proporcionan nutrición al cartílago.

Si se observa una delgada capa bajo el microscopio, se encontrará que está formado por células con forma redondeada o sin puntas, en grupos de dos o más en una matriz biológica granular o casi homogénea. Al microscopio óptico no se observa red fibrilar en la matriz extracelular, sin embargo al utilizar luz polarizada pueden visualizarse redes de fibrillas. Este efecto óptico es debido a que el índice de refracción de las fibrillas es similar al de la matriz en las que se encuentran inmersas.

La matriz extracelular es una red 3D muy compleja. Las células se encuentran en las cavidades de la matriz, llamada lagunas del cartílago;3 en torno a estas la matriz está dispuesta en líneas concéntricas, como si se hubiera formado en porciones sucesivas alrededor de las células del cartílago. Esto constituye la llamada cápsula del espacio.

Cada laguna está generalmente ocupada por una sola célula, pero durante la división de las células puede contener dos, cuatro u ocho células, lo que constituye un grupo isogénico o grupo de células isogénicas.4
El cartílago hialino también contiene condrocitos que son las células del cartílago que producen la matriz. La matriz hialina del cartílago se compone sobre todo de colágeno tipo II y sulfato de condroitina, los cuales también se encuentran en el cartílago elástico. Formado principalmente por fibrillas de colágeno tipo I. Posee condrocitos dispuestos en grupos. Existe pericondrio. Es el más abundante del cuerpo. Tiene un aspecto blanquecino azulado. Se encuentra en el esqueleto nasal, la laringe, la tráquea, los bronquios, los arcos costales (costillas) y los extremos articulares de los . Es avascular, nutriéndose por difusión a partir del líquido sinovial. Es de pocas fibras.

Los condrocitos producen la matriz amplia formada por colágeno de tipo II (forma fibrillas finas características), colágeno del tipo IX (une las fibrillas de tipo II), colágeno de tipo X (rodea células hipertróficas), colágeno de tipo XI (función desconocida), hialuronano y el agregado de proteoglucano unido a él. En especial, las cadenas de queratin sulfato y condroitin sulfato del agrecano fijan el agua, un requisito fundamental para la creación de la consistencia elástica característica del cartílago. El hialuronano y la gran cantidad de moléculas de agrecano unidas a el forman un complejo molecular gigante dado que puede alcanzar un tamaño de 3-4 mm. Estos agregados constituyen la mayor parte del cartílago y le imparten su consistencia cartilaginosa. La estabilidad morfológica del cartílago también tiene su origen en estos agregados. En la forma de cartílago articular este material singular puede soportar todo el peso del cuerpo. Un vínculo importante entre la matriz y las células cartilaginosas es la condronectina presente en la membrana de los condrocitos, una proteína semejante a la fibronectina.

Es característico que los condrocitos que han surgido de una célula progenitora por división mitótica se ubiquen en grupos pequeños contiguos (grupos celulares isógenos). En el entorno inmediato de las células cartilaginosas la matriz contiene glucosaminoglucanos muy sulfatados y recibe el nombre de matriz territorial. Esta matriz se tiñe de color azul violeta intenso en los preparados coloreados con H-E. El grupo de condrocitos y su matriz forman un territorio cartilaginoso (= condrona). La matriz entre los territorios se denomina matriz interterritorial. Carece de células y se tiñe de un tono pálido.

Los condrocitos están ubicados en espacios ("lagunas" o condroplastos) de la matriz cuya pared, o sea, el entorno inmediato de las células cartilaginosas también recibe el nombre de cápsula condrocítica. La región capsular con frecuencia posee una capa pericelular con función protectora especial contra la compresión y la tracción. En los preparados histológicos, los condrocitos suelen aparecer retraídos artificialmente dentro de sus lagunas.

El crecimiento ocurre por secreción de matriz en el interior de las piezas cartilaginosas (crecimiento intersticial o por intususcepción) o por formación nueva en la periferia (crecimiento por aposición).

El cartílago maduro no tiene nervios y en la mayoría de los casos es avascular. La nutrición de las células cartilaginosas ocurre por difusión a través de la matriz provista de agua en abundancia. El metabolismo es anaerobio en una proporción considerable.


</doc>
<doc id="2873" url="https://es.wikipedia.org/wiki?curid=2873" title="Tachyglossidae">
Tachyglossidae

Los taquiglósidos o equidnas (Tachyglossidae) son la única familia conocida del suborden Tachyglossa, donde se clasifican a los equidnas actuales y sus ancestros extintos. Estos mamíferos, similares en apariencia a los erizos, habitan en las islas de Nueva Guinea, Salawati, Australia, Tasmania y otras islas menores próximas a las costas de estas. Además de ser muy difíciles de encontrar, su rareza reside en que son los únicos mamíferos, junto con los ornitorrincos, que ponen huevos.

Los equidnas deben su nombre a la ninfa mitológica madre de todos los legendarios monstruos de la Grecia Clásica. Tienen el cuerpo cubierto de espinas, lo que unido a la dieta que llevan, mayoritariamente insectívora, y en algunos casos con predilección por las hormigas y termitas (mirmecofagia), les ha valido el nombre de «hormigueros espinosos».

En la actualidad se reconocen dos géneros vivientes, con solo cuatro especies:

Son animales de cuerpo compacto, cubierto de un denso pelaje del que sobresalen largas púas empleadas como método de defensa. Normalmente miden entre 35 y 45 centímetros de largo, con una cola de 10 centímetros, y un peso promedio de 2 a 7 kilogramos. Los machos son por regla general de mayor tamaño que las hembras.

El cráneo es largo y redondeado, la cara larga con la mandíbula inferior poco desarrollada, constituida por dos delgados y largos huesos. Su dieta, constituida por insectos y lombrices, determina un aparato bucal tubular de estrecha abertura, provisto de una larga lengua pegajosa que puede alcanzar 20 centímetros de longitud, con la que atrapan el alimento, que, al carecer de dientes, será triturado con unas espinas córneas situadas en el paladar al final de la boca. Para localizar los alimentos, además de un agudizado sentido del olfato, están dotados de electrorreceptores táctiles en el rostro con los que les resulta fácil hallar las colonias de hormigas y termitas.

Son poderosos excavadores que emplean pies y manos para construir galerías y oquedades o escarbar en la tierra en busca de alimento. Para ello sus extremidades poseen manos y pies cavadores dotados de poderosas uñas. El segundo dedo de las extremidades posteriores es más largo y lo emplean para rascarse y limpiarse el pelo y la piel.

Los machos y algunas hembras, poseen espolones tras la articulación de la rodilla, pero a diferencia de "Ornithorhynchus sp"., este animal no sintetiza ninguna sustancia tóxica, por lo que se desconoce la función real de los mismos.

Las hembras desarrollan un marsupio temporal mientras dura la incubación y la lactancia. El pene de los machos tiene cuatro cabezas, algo común entre reptiles pero raro en mamíferos. A pesar de ser mamífero, la cría del equidna nace a partir de huevos ya que es uno de los dos mamíferos ovíparos, junto al ornitorrinco ("Ornithorhynchus anatinus"), que existen en la Tierra.

A diferencia de lo que se cree, los equidnas no hibernan como respuesta al frío. El estado de torpor al que se ven sometidos algunos ejemplares aislados, parece estar relacionado más bien con un proceso digestivo anómalo.

Al contrario de lo previamente investigado, los equidnas sí entran en sueño REM, aunque sólo cuando la temperatura del ambiente está alrededor de los 25 °C. A temperaturas de 15 °C y 28 °C, el sueño REM se suprime.

La hembra pone un solo huevo. La incubación tarda diez días; el equidna joven succiona la leche de los poros de las dos glándulas mamarias (los monotremas no tienen pezones) y permanecen en la bolsa durante cuarenta y cinco a cincuenta días, en dicho tiempo comienzan a desarrollar las espinas. La madre cava una madriguera y deposita al pequeño, retornando cada cinco días para amamantarlo hasta el destete, que es a los siete meses.

Los equidnas machos tienen un pene tetracapitado, pero sólo dos de las cabezas se usan durante el apareamiento. Las otras dos cabezas "se cierran" y no crecen en tamaño. Las cabezas usadas se intercambian cada vez que el mamífero copula.




</doc>
<doc id="2875" url="https://es.wikipedia.org/wiki?curid=2875" title="Thylacinidae">
Thylacinidae

Los tilacínidos (Thylacinidae) son una familia de marsupiales carnívoros del orden Dasyuromorphia propios de la fauna australiana. Con un rango fósil que comprende el intervalo Oligoceno - Holoceno, su representante más reciente, extinto en 1936, es "Thylacinus cynocephalus", el tilacín o lobo marsupial. Se trata de animales poliprotodontos con una fórmula dentaria 4/3, 1/1, 3/3, 4/4 = 46.

Familia Thylacinidae †



</doc>
<doc id="2877" url="https://es.wikipedia.org/wiki?curid=2877" title="Tipo de dato">
Tipo de dato

En ciencias de la computación, un tipo de dato informático o simplemente tipo, es un atributo de los datos que indica al ordenador (y/o al programador/programadora) sobre la clase de datos que se va a manejar. Esto incluye imponer restricciones en los datos, como qué valores pueden tomar y qué operaciones se pueden realizar.

Los tipos de datos más comunes son: números enteros, números con signo (negativos), números de coma flotante (decimales), cadenas alfanuméricas (y unicodes), estados, etc.

Un tipo de dato es, en esencia, un espacio en memoria con restricciones. Por ejemplo, el tipo "int" representa, generalmente, un conjunto de enteros de 32 bits cuyo rango va desde el -2.147.483.648 al 2.147.483.647, así como las operaciones que se pueden realizar con los enteros, como son la suma, la resta, y la multiplicación. Los colores, por su parte, se representan como tres bytes denotando la cantidad de rojo, verde y azul, y una cadena de caracteres representando el nombre del color (en este caso, las operaciones permitidas incluyen la adición y la sustracción, pero no la multiplicación).

Este es un concepto propio de la informática, y más específicamente de los lenguajes de programación, aunque también se encuentra relacionado con nociones similares de la matemática y la lógica.

En un sentido amplio, un tipo de datos define un conjunto de valores y las operaciones sobre esos valores. Casi todos los lenguajes de programación explícitamente incluyen la notación del tipo de datos, aunque lenguajes diferentes pueden usar terminologías diferentes. La mayor parte de los lenguajes de programación permiten al programador definir tipos de datos adicionales, normalmente combinando múltiples elementos de otros tipos y definiendo las operaciones del nuevo tipo de dato. Por ejemplo, un programador puede crear un nuevo tipo de dato llamado "Persona", contemplando que el dato interpretado como Persona incluiya un nombre y una fecha de nacimiento.

Un tipo de dato puede ser también visto como una limitación impuesta en la interpretación de los datos en un sistema de tipificación, describiendo la representación, la interpretación y la estructura de los valores u objetos almacenados en la memoria del ordenador. El sistema de tipificación usa información de los tipos de datos para comprobar la verificación de los programas que acceden o manipulan los datos.

Los tipos de datos hacen referencia al tipo de información que se trabaja, donde la unidad mínima de almacenamiento es el dato, también se puede considerar como el rango de valores que puede tomar una variable durante la ejecución del programa. 

El tipo de dato caracter es un dígito individual el cual se puede representar como numéricos (0 al 9), letras (a-z) y símbolos (!"$&/\).

El tipo de dato carácter unicode es una "extensión" del tipo de dato cadena, permite ampliar los símbolos de escritura, provee exactamente hasta 65535 caracteres diferentes.

Nota: En el lenguaje java la codificación Unicode permite trabajar con todos los caracteres de distintos idiomas.

Este tipo de dato puede ser real o entero, dependiendo del tipo de dato que se vaya a utilizar.

Enteros: son los valores que no tienen punto decimal, pueden ser positivos o negativos y el cero.

Reales: estos caracteres almacenan números muy grandes que poseen parte entera y parte decimal.

estos serían sus rangos y tamaños ordenados

Este tipo de dato se emplea para valores lógicos, los podemos definir como datos comparativos dicha comparación devuelve resultados lógicos (Verdadero o Falso).

Los tipos compuestos se derivan de uno o más datos primitivos. A las distintas maneras de formar o combinar estos datos se les conocen con el nombre de “Estructura de datos”. Al combinarlo podemos crear un nuevo tipo, por ejemplo:

"array-de-enteros" es distinto al tipo "entero".


El lenguaje de programación Pascal permite declarar variables de tipo carácter (Cadena) y numérica.
Como se puede apreciar, todas las variables excepto la de tipo Cadena son de tipo numéricas (incluyendo Booleano).

El lenguaje de programación Java permite declarar variables de tipo primitivo, pero dada que los envoltorios de dichas funciones presentan muchas operaciones útiles, es más común hacer uso de las clases que las tratan.

No hay que confundir estos tipos de datos con los tipos de datos abstractos.

Los TDA siguen una interfaz que especifica que hace ese tipo de datos (la estructura de datos sería la implementación concreta). Formalmente se trata de un modelo matemático para tipos de datos que están definidos por su comportamiento o semántica. A nivel de usuario se puede ver como el esquema de los datos y operaciones para manipular los elementos que componen ese tipo de datos. La estructura de datos sería la representación concreta de los datos.




</doc>
<doc id="2882" url="https://es.wikipedia.org/wiki?curid=2882" title="Identificador de recursos uniforme">
Identificador de recursos uniforme

Un identificador de recursos uniforme o URI —del inglés "uniform resource identifier"— es una cadena de caracteres que identifica los recursos de una red de forma unívoca. La diferencia respecto a un localizador de recursos uniforme (URL) es que estos últimos hacen referencia a recursos que, de forma general, pueden variar en el tiempo.

Normalmente estos recursos son accesibles en una red o sistema. Los URI pueden ser localizador de recursos uniforme (URL), "uniform resource name" (URN), o ambos.

Un URI consta de las siguientes partes:


Aunque se acostumbra llamar URL a todas las direcciones web, URI es un identificador más completo
y por eso es recomendado su uso en lugar de la expresión URL.

Un URI se diferencia de un URL en que permite incluir en la dirección una subdirección, determinada por el “fragmento”.




</doc>
<doc id="2883" url="https://es.wikipedia.org/wiki?curid=2883" title="Unicode">
Unicode

Unicode es un estándar de codificación de caracteres diseñado para facilitar el tratamiento informático, transmisión y visualización de textos de múltiples lenguajes y disciplinas técnicas, además de textos clásicos de lenguas muertas. El término Unicode proviene de los tres objetivos perseguidos: universalidad, uniformidad y unicidad.

Unicode define cada carácter o símbolo mediante un nombre e identificador numérico, el "code point" (‘punto de código’). Además incluye otras informaciones para el uso correcto de cada carácter, como sistema de escritura, categoría, direccionalidad, mayúsculas y otros atributos. Unicode trata los caracteres alfabéticos, ideográficos y símbolos de forma equivalente, lo que significa que se pueden mezclar en un mismo texto sin utilizar de marcas o caracteres de control.

Este estándar es mantenido por el Unicode Technical Committee (UTC), integrado en el Consorcio Unicode, del que forman parte con distinto grado de implicación empresas como: Microsoft, Apple, Adobe, IBM, Oracle, SAP, Google, Facebook o Shopify, instituciones como la Universidad de Berkeley, o el Gobierno de la India y profesionales y académicos a título individual.
El Unicode Consortium mantiene estrecha relación con ISO/IEC, con la que mantiene desde 1991 el acuerdo de sincronizar sus estándares que contienen los mismos caracteres y puntos de código.

La creación de Unicode ha sido un ambicioso proyecto para reemplazar los esquemas de codificación de caracteres ya existentes, muchos de los cuales estaban muy limitados en tamaño y son incompatibles con entornos plurilingües. Unicode se ha convertido en el más extenso y completo esquema de codificación de caracteres, siendo el dominante en la internacionalización y adaptación local del software informático. El estándar ha sido aceptado en un número considerable de tecnologías recientes, como XML, Java y sistemas operativos modernos.

La descripción completa del estándar y las tablas de caracteres están disponibles en la página web oficial de Unicode . La referencia completa se publica, además, en forma de libro cada vez que se completa una nueva versión principal. La versión digital de este libro está disponible de forma gratuita. Las revisiones y adiciones se publican de forma independiente.

Unicode incluye todos los caracteres de uso común en la actualidad. La versión 5.1 contenía 100 713 caracteres provenientes de alfabetos, sistemas ideográficos y colecciones de símbolos (matemáticos, técnicos, musicales, iconos...). La cifra crece con cada versión.

Unicode incluye sistemas de escritura modernos como: árabe, braille, copto, cirílico, griego, sinogramas (hanja coreano, hanzi chino y kanji japonés), silabarios japoneses (hiragana y katakana), hebreo y latino; escrituras históricas extintas, para propósitos académicos, como por ejemplo: cuneiforme, griego antiguo, lineal B micénico, fenicio y rúnico. Entre los caracteres no alfabéticos incluidos en Unicode se encuentran símbolos musicales y matemáticos, fichas de juegos como el dominó, flechas, iconos etc.

Además, Unicode incluye los signos diacríticos como caracteres independientes que pueden ser combinados con otros caracteres y dispone de versiones predefinidas de la mayoría de letras con símbolos diacríticos en uso en la actualidad, como las vocales acentuadas del español.

Unicode es un estándar en constante evolución y se agregan nuevos caracteres continuamente. Se han descartado ciertos alfabetos, propuestos por distintas razones, como por ejemplo el alfabeto klingon.

Como ya se ha indicado, Unicode está sincronizado con el estándar ISO/IEC conocido como UCS o juego de caracteres universal. Desde un punto de vista técnico, incluye o es compatible con codificaciones anteriores como ASCII7 o ISO 8859-1, los estándares nacionales ANSI Z39.64, KS X 1001, JIS X 0208, JIS X 0212, JIS X 0213, GB 2312, GB 18030, HKSCS, y CNS 11643, codificaciones particulares de fabricantes de software como Apple, Adobe, Microsoft, IBM, etc. Además, Unicode reserva espacio para fabricantes de software que pueden crear extensiones para su propio uso.

El elemento básico del estándar Unicode es el carácter. Se considera un carácter al elemento más pequeño de un sistema de escritura con significado. El estándar Unicode codifica los caracteres esenciales ―grafemas― definiéndolos de forma abstracta y deja la representación visual (tamaño, dimensión, fuente o estilo) al software que lo trate, como procesadores de texto o navegadores web. Se incluyen letras, signos diacríticos, caracteres de puntuación, ideogramas, caracteres silábicos, caracteres de control y otros símbolos. Los caracteres se agrupan en alfabetos o sistemas de escritura. Se considera que son diferentes los caracteres de alfabetos distintos, aunque compartan forma y significación.

Los caracteres se identifican mediante un número o punto de código y su nombre o descripción. Cuando se ha asignado un código a un carácter, se dice que dicho carácter está codificado. El espacio para códigos tiene 1 114 112 posiciones posibles (0x10FFFF). Los puntos de código se representan utilizando notación hexadecimal agregando el prefijo U+. El valor hexadecimal se completa con ceros hasta 4 dígitos hexadecimales cuando es necesario; si es de longitud mayor que 4 dígitos no se agregan ceros.

Los bloques del espacio de códigos contienen puntos con la siguiente información:


Unicode incluye un mecanismo para formar caracteres y así extender el repertorio de compatibilidad con los símbolos existentes. Un carácter base se complementa con marcas: signos diacríticos, de puntuación o marcos. El tipo de cada carácter y sus atributos definen el papel que pueden jugar en una combinación. Por este motivo, puede haber varias opciones que representen el mismo carácter. Para facilitar la compatibilidad con codificaciones anteriores, se proporcionan caracteres precompuestos; en la definición de dichos caracteres se hace constar qué caracteres intervienen en la composición.

Un grupo de caracteres consecutivos, independientemente de su tipo, forma una secuencia. En caso de que varias secuencias representen el mismo conjunto de caracteres esenciales, el estándar no define una de ellas como 'correcta', sino que las considera equivalentes. Para poder identificar dichas equivalencias, Unicode define los mecanismos de "equivalencia canónica" y de "equivalencia de compatibilidad" basados en la obtención de formas normalizadas de las cadenas a comparar.

En el estándar Unicode, los ideogramas de Asia oriental (popularmente llamados «caracteres chinos») se denominan «ideogramas han». Estos ideogramas se desarrollaron en China y fueron adaptados por culturas próximas para su propio uso.
Japón, Corea y Vietnam desarrollaron sus propios sistemas alfabéticos o silábicos para usar en combinación con los símbolos chinos: hiragana y katakana (en Japón), hangul (en Corea) y yi (en Vietnam). La evolución natural de los sistemas de escritura y los distintos momentos de entrada de los caracteres en las distintas culturas han marcado diferencias en los ideogramas utilizados. Unicode considera las distintas versiones de los ideogramas como variantes de un mismo carácter abstracto, es decir, como resultado de la aplicación de un tipo de letra diferente en cada caso y considera las variantes nacionales como pertenecientes a un mismo sistema de escritura. La versión original del estándar se desarrolló a partir de los estándares industriales existentes en los países afectados.

El organismo encargado de desarrollar el repertorio de caracteres es el Ideographic Rapporteur Group (IRG). IRG es un grupo de trabajo integrado en ISO/IEC JTC1/SC2/WG2, incluyendo a China, Hong Kong, Macao, Taipei Computer Association, Singapur, Japón, Corea del Sur, Corea del Norte, Vietnam y Estados Unidos de América.

La base de datos de caracteres CJK se denomina Unihan y contiene, además, información auxiliar sobre significado, conversiones, datos necesarios para utilizarlos en los diferentes lenguajes que los utilizan. A continuación se muestran los bloques que describen este repertorio. IRG define los caracteres de los siete grupos unificados, los siguientes dos grupos contienen caracteres para compatibilidad con estándares anteriores.

Se admite que nunca se podrá finalizar la tarea de incluir ideogramas en el estándar debido, principalmente, a que la creación de nuevos ideogramas continúa. A fin de suplir eventuales carencias, Unicode ofrece un mecanismo que permite la representación de los símbolos que faltan denominado «secuencias de descripción ideográfica». Se basa en que en la práctica, la totalidad de los ideogramas se puede descomponer en piezas más pequeñas que, a su vez, son ideogramas. Aunque sea posible la representación de un símbolo mediante una secuencia, el estándar especifica que siempre que exista una versión codificada su uso debe ser preferente. No hay un método para la «descomposición canónica» de ideogramas ni algoritmos de equivalencia por lo que las operaciones sobre el texto, como búsqueda u ordenación, pueden fallar.

Unicode define 12 caracteres de control para la descripción de ideogramas representando distintas posibilidades de combinación espacial de otros caracteres han.

El estándar fue diseñado con los siguientes objetivos:


El conjunto de caracteres codificados por Unicode, es la UCD (unicode character database: base de datos de caracteres Unicode). Además de nombre y punto de código, incluye más información: alfabeto al que pertenece, nombre, clasificación, mayúsculas, orientación y otras formas de uso, variantes estandarizadas, reglas de combinación, etc.

Formalmente la base de datos se divide en planos y estos a su vez en áreas y bloques. Con excepciones, los caracteres codificados se agrupan en el espacio de códigos siguiendo categorías como alfabeto o sistema de escritura, de forma que caracteres relacionados se encuentren cerca en las tablas de codificación.

Por conveniencia se ha dividido el espacio de códigos en grandes grupos denominados "planos". Cada plano contiene un máximo de 65 535 caracteres. Dado un punto de código expresado en hexadecimal, los 4 últimos dígitos determinan la posición del carácter en el plano.

Los distintos planos se dividen en áreas de direccionamiento en función de los tipos generales que incluyen. Esta división es convencional, no reglada y puede variar con el tiempo. Las áreas se dividen, a su vez, en bloques. Los bloques están definidos normativamente y son rangos consecutivos del espacio de códigos. Los bloques se utilizan para formar las tablas impresas de caracteres pero no deben tomarse como definiciones de grupos significativos de caracteres.

Los puntos de código de Unicode se identifican por un número entero. Según su arquitectura, un ordenador utilizará unidades de 8, 16 o 32 bits para representar dichos enteros. Las "formas de codificación" de Unicode reglamentan la forma en que los puntos de código se transformarán en unidades tratables por el computador.

Unicode define tres formas de codificación bajo el nombre UTF (Unicode transformation format: formato de transformación Unicode):

Las "formas de codificación" se limitan a describir el modo en que se representan los puntos de código en formato inteligible por la máquina. A partir de las 3 formas identificadas se definen 7 esquemas de codificación.

Los "esquemas de codificación" tratan de la forma en que se serializa la información codificada. La seguridad en los intercambios de información entre sistemas heterogéneos requiere la implementación de sistemas que permitan determinar el orden correcto de los bits y bytes y garantizar que la reconstrucción de la información es correcta. Una diferencia fundamental entre procesadores es el orden de disposición de los bytes en palabras de 16 y 32 bits, lo que se denomina "endianness". Los esquemas de codificación deben garantizar que los extremos de una comunicación saben cómo interpretar la información recibida. A partir de las 3 formas de codificación se definen 7 esquemas. A pesar de que comparten nombres, no debe confundirse esquemas y formas de codificación.

Unicode define una marca especial, la marca de orden de bytes (BOM, "Byte Order Mark"), al inicio de un fichero o una comunicación para hacer explícita la ordenación de bytes. Cuando un protocolo superior especifica el orden de bytes, la marca no es necesaria y puede omitirse dando lugar a los esquemas de la lista anterior con sufijo "BE" o "LE". En los esquemas UTF-16 y UTF-32, que admiten BOM, si este no se especifica se asume que la ordenación de bytes es "big-endian".

La unidad de codificación en UTF-8 es el byte por lo que no necesita una indicación de orden de byte. El estándar ni requiere ni recomienda la utilización de BOM, pero lo admite como marca de que el texto es Unicode o como resultado de la conversión de otros esquemas.

El proyecto Unicode se inició a finales de 1987, tras conversaciones entre Joe Becker, Lee Collins y Mark Davis (ingenieros de las empresas Apple y Xerox). Como resultado de su colaboración, en agosto de 1988 se publicó el primer borrador de Unicode bajo el nombre de Unicode88.
En esta primera versión se consideraba que sólo se codificarían los caracteres necesarios para el uso moderno, por lo que se utilizaron códigos de 16 bits.

Durante el año 1989 se sumaron colaboradores de otras compañías como Microsoft o Sun Microsystems. El 3 de febrero de 1991 se formó el Consorcio Unicode, y en octubre de 1991 se publicó la primera versión del estándar. La segunda versión, que ya incluía la escritura ideográfica han se publicó en junio de 1992. A continuación se muestra una tabla con las distintas versiones del Estándar Unicode con sus adiciones o modificaciones más importantes.



</doc>
<doc id="2885" url="https://es.wikipedia.org/wiki?curid=2885" title="Unidad astronómica">
Unidad astronómica

La unidad astronómica (abreviada ua, au, UA o AU) es una unidad de longitud igual, por definición, a , y que equivale aproximadamente a la distancia media entre el planeta Tierra y el Sol. Esta definición está en vigor desde la asamblea general de la Unión Astronómica Internacional (UAI) del 31 de agosto de 2012, en la cual se dejó sin efecto la definición gaussiana usada desde 1976, que era «el radio de una órbita circular newtoniana y libre de perturbaciones alrededor del Sol descrita por una partícula de masa infinitesimal que se desplaza en promedio a 0,01720209895 radianes por día». 

El símbolo ua es el recomendado por la Oficina Internacional de Pesas y Medidas y por la norma internacional ISO 80000, mientras que au es el único considerado válido por la UAI, y el más común en los países angloparlantes. También es frecuente ver el símbolo escrito en mayúsculas, UA o AU, a pesar de que el Sistema Internacional de unidades utiliza letras mayúsculas solo para los símbolos de las unidades que llevan el nombre de una persona. 

El nombre proviene de los siglos XVI y XVII, cuando todavía no se calculaban con precisión las distancias absolutas entre los cuerpos del sistema solar, y solo se conocían las distancias relativas tomando como patrón la distancia media entre la Tierra y el Sol, que fue denominada unidad astronómica. Se llegó a afirmar que el día en que se midiera este valor, «se conocería el tamaño del universo».

Un antecedente directo de la unidad astronómica se puede encontrar directamente en las demostraciones de Nicolás Copérnico para su sistema heliocéntrico en el siglo XVI. En el tomo V de su libro "De Revolutionibus Orbium Coelestium" (1543) calculó, utilizando trigonometría, las distancias relativas entre los planetas conocidos entonces y el Sol, teniendo como base la distancia entre la Tierra y el Sol. Midiendo los ángulos entre la Tierra, el planeta y el Sol en los momentos en que estos forman un ángulo recto, es posible obtener la distancia Sol-planeta en unidades astronómicas. Esta fue una de sus demostraciones para probar que los planetas, incluida la Tierra, giraban alrededor del Sol (heliocentrismo), descartando el modelo de Claudio Ptolomeo que postulaba que la Tierra era el centro alrededor del cual giraban los planetas y el Sol (geocentrismo). Estableció así la primera escala relativa del sistema solar utilizando como patrón la distancia entre la Tierra y el Sol.

Posteriormente Johannes Kepler, basándose en las cuidadosas observaciones de Tycho Brahe, estableció las leyes del movimiento planetario, las cuales se conocen justamente como «leyes de Kepler». La tercera de estas leyes relaciona la distancia de cada planeta al Sol con el tiempo que tarda en recorrer su órbita (es decir el período orbital) y, como consecuencia, establece una escala relativa mejorada para el sistema solar: por ejemplo, basta con medir cuántos años tarda Saturno en orbitar el Sol para saber cuál es la distancia de Saturno al Sol en unidades astronómicas. 
Kepler estimó con muy buena precisión los tamaños de las órbitas planetarias; por ejemplo, fijó la distancia entre Mercurio y el Sol en 0,387 unidades astronómicas (el valor correcto es 0,389), y la distancia de Saturno al Sol en 9,510 unidades astronómicas (el valor correcto siendo 9,539). Sin embargo, ni Kepler ni ninguno de sus contemporáneos sabían cuánto valía esta unidad astronómica, y por tanto ignoraban completamente la escala real del sistema planetario conocido, que en aquel entonces se extendía hasta Saturno.

Partiendo de las leyes de Kepler, bastaba medir la distancia de un planeta cualquiera al Sol, o a la Tierra, para conocer la unidad astronómica. En 1659 Christian Huygens midió el ángulo que subtiende Marte en el cielo y, atribuyendo un valor al diámetro de este planeta, estimó que la unidad astronómica debía ser 160 millones de kilómetros, es decir siete veces mayor que lo estimado por Kepler, pero de hecho menos del 10 % por encima del valor real. Sin embargo esta medición no era aceptada ya que, como el mismo Huygens reconoció, todo dependía del valor que uno atribuyera al tamaño de Marte. Curiosamente, Huygens adivinó con notable exactitud el tamaño de Marte.

Se conocía otro método más fiable, pero que requería mediciones muy difíciles de realizar: el método de la paralaje. Si dos personas situadas en puntos alejados de la Tierra, digamos en París (Francia) y en Cayena (Guayana Francesa), observan simultáneamente la posición de un planeta en el cielo en relación a las estrellas de fondo, sus mediciones dan una pequeña diferencia que corresponde al ángulo que subtendería la línea París-Cayena vista desde el planeta. Conociendo este ángulo, y la distancia París-Cayena, se puede deducir el valor de la unidad astronómica. En la práctica existían tres dificultades: primero, no se conocían bien las distancias sobre la Tierra; segundo, la medición del tiempo no era lo suficientemente precisa como para permitir mediciones simultáneas entre puntos muy alejados; y tercero, la medición de la posición aparente del planeta en el cielo debía ser muy precisa. Pasó más de medio siglo antes de que fuera posible medir la paralaje de un planeta: en 1672 Jean Richer viajó a Cayena para medir la posición de Marte en el cielo en el mismo instante en que sus colegas en París hacían lo mismo. Richer y sus colegas estimaron el valor en 140 millones de kilómetros.

Con el tiempo se desarrollaron métodos más precisos y fiables para estimar la unidad astronómica; en particular, el propuesto por el matemático escocés James Gregory y por el astrónomo británico Edmund Halley (el mismo del cometa), se basa en mediciones del tránsito de Venus o Mercurio sobre el disco solar y fue empleado hasta principios del siglo XX. Las mediciones contemporáneas se hacen con técnicas láser o de radar y dan el valor 149 597 870 km, con un error aparente de uno o dos kilómetros.

Newton reformuló la tercera ley de Kepler. Un planeta de masa "m", orbitando el sol de masa "M", en una elipse con semi-eje mayor "a" y con un período sideral "T", verifica la ecuación

El matemático alemán Carl Friedrich Gauss (1777-1855) usó para sus cálculos de la dinámica del sistema solar como unidad de masa la masa solar, como unidad de tiempo el día solar medio y como unidad de distancia el semieje mayor de la órbita de la Tierra. Utilizando estas unidades, la ecuación anterior se escribe como

Donde "k" se conoce como la constante gravitacional gaussiana. Gauss utilizó los valores estimados en su época

Gauss reconoció que el problema con esta definición es que cuando se determinaran con mejor precisión el año sidéreo y la masa del Sol, el valor de "k" cambiaría. En 1938, la Unión Astronómica Internacional (UAI) adoptó el valor de la constante gravitacional gaussiana (y la unidad astronómica de ella derivada) como una definición en astronomía. Sin embargo, con la precisión de las medidas actuales, se sabe que el año sidéreo es 56 segundos más corto que el valor conocido en tiempos de Gauss, y que el semieje mayor de la órbita real de la Tierra es unos 17 km más pequeño que la unidad astronómica.

En la asamblea general de la Unión Astronómica Internacional de agosto de 2012 en Pekín se resolvió dejar sin efecto la definición gaussiana y darle a la unidad astronómica el valor actual de 149 597 870 700 metros.


Algunos factores de conversión:


La siguiente tabla muestra algunas distancias tomadas en unidades astronómicas. Incluye algunos ejemplos con distancias aproximadas porque son demasiado pequeños o están demasiado alejados. Las distancias van cambiando con el tiempo. También se puede ordenar según aumente la distancia.



</doc>
<doc id="2886" url="https://es.wikipedia.org/wiki?curid=2886" title="Urano">
Urano

El término Urano puede referirse a:



</doc>
<doc id="2887" url="https://es.wikipedia.org/wiki?curid=2887" title="Unix">
Unix

Unix (registrado oficialmente como UNIX®) es un sistema operativo portable, multitarea y multiusuario; desarrollado, en principio, en 1969, por un grupo de empleados de los laboratorios Bell de AT&T, entre los que figuran Dennis Ritchie, Ken Thompson y Douglas McIlroy.

El sistema, junto con todos los derechos fueron vendidos por AT&T a Novell, Inc. Esta vendió posteriormente el software a Santa Cruz Operation en 1995, y esta, a su vez, lo revendió a Caldera Software en 2001, empresa que después se convirtió en el grupo SCO. Sin embargo, Novell siempre argumentó que solo vendió los derechos de uso del software, pero que retuvo el "copyright" sobre "UNIX®". En 2010, y tras una larga batalla legal, ésta ha pasado nuevamente a ser propiedad de Novell.

Solo los sistemas totalmente compatibles y que se encuentran certificados por la especificación Single UNIX Specification pueden ser denominados "UNIX®" (otros reciben la denominación «similar a un sistema Unix» o «similar a Unix»). En ocasiones, suele usarse el término "Unix tradicional" para referirse a Unix o a un sistema operativo que cuenta con las características de UNIX Versión 7 o UNIX System V o unix versión 6. 

A finales de la década de 1960 el Instituto Tecnológico de Massachusetts, los Laboratorios Bell de AT&T y General Electric trabajaban en un sistema operativo experimental llamado Multics (Multiplexed Information and Computing Service), desarrollado para ejecutarse en una computadora central (mainframe) modelo GE-645. El objetivo del proyecto era desarrollar un gran sistema operativo interactivo que contase con muchas innovaciones, entre ellas mejoras en las políticas de seguridad. El proyecto consiguió dar a luz versiones para producción, pero las primeras versiones contaban con un pobre rendimiento.
Los laboratorios Bell de AT&T decidieron desvincularse y dedicar sus recursos a otros proyectos.

Uno de los programadores de los laboratorios Bell, Ken Thompson, siguió trabajando para la computadora GE-645 y escribió un juego llamado "Space Travel", (Viaje espacial). Sin embargo, descubrió que el juego era lento en la máquina de General Electric y resultaba realmente caro, algo así como 75 dólares de EE.UU. por cada partida.

De este modo, Thompson escribió nuevamente el programa, con ayuda de Dennis Ritchie, en lenguaje ensamblador, para que se ejecutase en una computadora DEC PDP-7. Esta experiencia, junto al trabajo que desarrolló para el proyecto Multics, condujo a Thompson a iniciar la creación de un nuevo sistema operativo para la DEC PDP-7. Thompson y Ritchie lideraron un grupo de programadores, entre ellos a Rudd Canaday, en los laboratorios Bell, para desarrollar tanto el sistema de ficheros como el sistema operativo multitarea en sí. A lo anterior, agregaron un intérprete de órdenes (o intérprete de comandos) y un pequeño conjunto de programas. El proyecto fue bautizado UNICS, como acrónimo Uniplexed Information and Computing System, pues solo prestaba servicios a dos usuarios (de acuerdo con Andrew Tanenbaum, era solo a un usuario). La autoría de esta sigla se le atribuye a Brian Kernighan, ya que era un "hack" de Multics. Dada la popularidad que tuvo un juego de palabras que consideraba a UNICS un sistema MULTICS castrado (pues "eunuchs", en inglés, es un homófono de UNICS), se cambió el nombre a UNIX, dando origen al legado que llega hasta nuestros días.

Hasta ese instante, no había existido apoyo económico por parte de los laboratorios Bell, pero eso cambió cuando el Grupo de Investigación en Ciencias de la Computación decidió utilizar UNIX en una máquina superior a la PDP-7. Thompson y Ritchie lograron cumplir con la solicitud de agregar herramientas que permitieran el procesamiento de textos a UNIX en una máquina PDP-11/20, y como consecuencia de ello consiguieron el apoyo económico de los laboratorios Bell. Fue así como por vez primera, en 1970, se habla oficialmente del sistema operativo UNIX ejecutado en una PDP-11/20. Se incluía en él un programa para dar formato a textos (runoff) y un editor de texto. Tanto el sistema operativo como los programas fueron escritos en el lenguaje ensamblador de la PDP-11/20. Este "sistema de procesamiento de texto" inicial, compuesto tanto por el sistema operativo como de runoff y el editor de texto, fue utilizado en los laboratorios Bell para procesar las solicitudes de patentes que ellos recibían. Pronto, runoff evolucionó hasta convertirse en troff, el primer programa de edición electrónica que permitía realizar composición tipográfica. El 3 de noviembre de 1971 Thomson y Ritchie publicaron un manual de programación de UNIX (título original en inglés: "UNIX Programmer's Manual").

En 1972 se tomó la decisión de escribir nuevamente UNIX, pero esta vez en el lenguaje de programación C. Este cambio significaba que UNIX podría ser fácilmente modificado para funcionar en otras computadoras (de esta manera, se volvía portable) y así otras variaciones podían ser desarrolladas por otros programadores. Ahora, el código era más conciso y compacto, lo que se tradujo en un aumento en la velocidad de desarrollo de UNIX. AT&T puso a UNIX a disposición de universidades y compañías, también al gobierno de los Estados Unidos, a través de licencias.
Una de estas licencias fue otorgada al Departamento de Computación de la Universidad de California, con sede en Berkeley. En 1975 esta institución desarrolló y publicó su propio sucedáneo de UNIX, conocida como "Berkeley Software Distribution" (BSD), que se convirtió en una fuerte competencia para la familia UNIX de AT&T.

Mientras tanto, AT&T creó una división comercial denominada "Unix Systems Laboratories" para la explotación comercial del sistema operativo. El desarrollo prosiguió, con la entrega de las versiones 4, 5 y 6 en el transcurso de 1975. Estas versiones incluían los "pipes" o "tuberías", lo que permitió dar al desarrollo una orientación modular respecto a la base del código, consiguiendo aumentar aún más la velocidad de desarrollo. Ya en 1978, cerca de 600 o más máquinas estaban ejecutándose con alguna de las distintas encarnaciones de UNIX.

La versión 7, la última versión del UNIX original con amplia distribución, entró en circulación en 1979. Las versiones 8, 9 y 10 se desarrollaron durante la década de 1980, pero su circulación se limitó a unas cuantas universidades, a pesar de que se publicaron los informes que describían el nuevo trabajo. Los resultados de esta investigación sirvieron de base para la creación de Plan 9 from Bell Labs, un nuevo sistema operativo portable y distribuido, diseñado para ser el sucesor de UNIX en investigación por los Laboratorios Bell.

AT&T entonces inició el desarrollo de UNIX System III, basado en la versión 7, como una variante de tinte comercial y así vendía el producto de manera directa. La primera versión del sistema III se lanzó en 1981. A pesar de lo anterior, la empresa subsidiaria Western Electric seguía vendiendo versiones antiguas de Unix basadas en las distintas versiones hasta la séptima. Para finalizar con la confusión con todas las versiones divergentes, AT&T decidió combinar varias versiones desarrolladas en distintas universidades y empresas, dando origen en 1983 al Unix System V Release 1. Esta versión presentó características tales como el editor Vi y la biblioteca curses, desarrolladas por Berkeley Software Distribution en la Universidad de California, Berkeley. También contaba con compatibilidad con las máquinas VAX de la compañía DEC.

Hacia 1991, un estudiante de ciencias de la computación de la Universidad de Helsinki, llamado Linus Torvalds desarrolló un núcleo para computadoras con arquitectura x86 de Intel que emulaba muchas de las funcionalidades de UNIX y lo lanzó en forma de código abierto en 1991, bajo el nombre de Linux. En 1992, el Proyecto GNU comenzó a utilizar el núcleo Linux junto a sus programas.

En 1993, la compañía Novell adquirió la división Unix Systems Laboratories de AT&T junto con su propiedad intelectual. Esto ocurrió en un momento delicado en el que "Unix Systems Laboratories" disputaba una demanda en los tribunales contra BSD por infracción de los derechos de copyright, revelación de secretos y violación de marca de mercado.

Aunque BSD ganó el juicio, Novell descubrió que gran parte del código de BSD fue copiada ilegalmente en UNIX System V. En realidad, la propiedad intelectual de Novell se reducía a unos cuantos archivos fuente. La correspondiente contra-demanda acabó en un acuerdo extrajudicial cuyos términos permanecen bajo secreto a petición de Novell.

A finales de 1993, Novell vendió su división UNIX comercial(es decir, la antigua Unix Systems Laboratories) a "Santa Cruz Operation" (SCO) reservándose, aparentemente, algunos derechos de propiedad intelectual sobre el software. Xinuos (antes UnXis) continúa la comercialización de System V en su producto UnixWare tras adquirir a SCO en abril de 2011.


Las interrelaciones entre estas familias son las siguientes, aproximadamente en orden cronológico:


UNIX® es una marca registrada de Novell, después de una disputa con The Open Group en Estados Unidos y otros países. Esta marca solo se puede aplicar a los sistemas operativos que cumplen la "Single Unix Specification" de esta organización y han pagado las regalías establecidas.

En la práctica, el término UNIX se utiliza en su acepción de familia. Se aplica también a sistemas multiusuario basados en POSIX (tales como GNU/Linux, Mac OS X [el cual, en su versión 10.5 ya ha alcanzado la certificación UNIX], FreeBSD, NetBSD, OpenBSD), los cuales no buscan la certificación UNIX por resultar cara para productos destinados al consumidor final o que se distribuyen libremente en Internet. En estos casos, el término se suele escribir como "UN*X", "UNIX*", "*NIX", o "*N?X". Para referirse a ellos (tanto a Unix, como a los sistema basados en Unix/POSIX) también se utiliza "Unixes", pero "Unices" (que trata la palabra "Unix" como un nombre latino de la tercera declinación) es asimismo popular.

A lo largo de la historia ha surgido una gran multitud de implementaciones comerciales de UNIX. Sin embargo, un conjunto reducido de productos ha consolidado el mercado y prevalece gracias a un continuo esfuerzo de desarrollo por parte de sus fabricantes. Los más importantes son:

Existen sistemas operativos basados en el núcleo Linux, y el conjunto de aplicaciones GNU (también denominado GNU/Linux), entre las más utilizadas encontramos:



También son populares los sistemas operativos descendientes del 4.4BSD:


Las siguientes implementaciones de UNIX tienen importancia desde el punto de vista histórico, no obstante, actualmente están en desuso:


Algunos comandos básicos de UNIX son:

Esta es una lista de los sesenta comandos de usuario de la sección 1 de la Primera Edición:

Otros comandos




Software sinfonía anónima Karol G

</doc>
<doc id="2888" url="https://es.wikipedia.org/wiki?curid=2888" title="Urticaceae">
Urticaceae

Urticaceae es una familia de plantas pertenecientes al orden Rosales.

Son plantas herbáceas, anuales o perennes, raras veces leñosas (en los trópicos), frecuentemente con pelos urticantes (cistolitos). Sin látex. Hojas simples, opuestas o alternas, con frecuencia estipuladas. Flores inconspicuas (verdosas), generalmente unisexuales, de disposición monoica o dioica, monoclamídeas, tetrámeras o pentámeras; gineceo súpero, unicarpelar (un estigma), con un óvulo; reunidas en inflorescencias axilares en panículas, cimas o amentos. Fruto en aquenio o núcula, unas 550 especies, propias sobre todo de las regiones cálidas.

La familia se subdivisa en 5 trubus:


</doc>
<doc id="2889" url="https://es.wikipedia.org/wiki?curid=2889" title="Ulmaceae">
Ulmaceae

Ulmaceae, las Ulmáceas, son una familia del Orden Rosales. 
Tienen hojas simples alternas, a menudo asimétricas, con estípulas prontamente caducas. Flores en su mayoría hermafroditas (hay plantas poliginas); monoclámideas; cáliz con 4 - 9 sépalos soldados; con 4 - 6 estambres episépalos; gineceo bicarpelar, sincárpico, con dos estigmas patentes; comúnmente agrupadas. Fruto en sámara o en drupa redondeada. Unas 140 especies de las zonas templadas subtropicales y tropicales del hemisferio norte, dos géneros en Europa.

El nombre de la familia Ulmaceae proviene desde 1815 Charles François Brisseau de Mirbel en "Elem. Physiol. Veg. Bot.", 2, S. 905. El género tipo es "Ulmus" L. La familia de las Ulmáceas durante mucho tiempo sólo tuvo dos subfamilias: Ulmoideae y Celtidoideae dentro del ordo Urticales. Estudios de genética molecular revelaron que las seis o siete familias y 2.600 especies del anterior ordo "Urticales" pertenecían al orden Rosales. Resultó que la subfamilia "Celtidoideae", con los géneros "Aphananthe, Celtis, Gironniera, Pteroceltis" y "Trema" no se asocian ya a la subfamilia "Ulmoideae", sino que esos géneros que se enmarcaban en la subfamilia "Celtidoideae" pertenecen en realidad a las "Cannabaceae".

Familias relacionadas dentro del orden Rosales:

En la familia de Ulmáceas hay actualmente sólo siete géneros con cerca de 35 especies


</doc>
<doc id="2890" url="https://es.wikipedia.org/wiki?curid=2890" title="Urticales">
Urticales

Urticales fue considerado como un orden de plantas herbáceas y leñosas. 

Tras la clasificación realizada por el sistema de clasificación APG III de 2009, las plantas correspondientes a este orden, incluyendo cuatro familias, pasaron a formar parte del orden Rosales y de la subclase Rosidae. La razón del cambio de subclase se debe al abandono del mismo taxón Hamamelidae, debido también al sistema APG III.
A las siguientes cuatro familias se las considera por tanto dentro del orden Rosales, y debido a la relación que poseen entre ellas tras estudiar los análisis filogenéticos de sus secuencias de ADN, se las denomina también como rósidas urticales.

Hay que destacar otras dos familias consideradas con anterioridad dentro de este taxón: Celtidaceae y Cecropiaceae:



Flores variables, hermafroditas y unisexuales; gineceo frecuentemente súpero, de carpelos abiertos, unilocular; en general anemófilas y solitarias, si hay inflorescencias, estas son aisladas y variables, raramente en amentos monoclamídeos. Los frutos son drupas denominadas también núculas. Chalazogamia hasta porogamia.


</doc>
